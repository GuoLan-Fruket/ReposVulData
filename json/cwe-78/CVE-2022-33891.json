{"index": 8556, "cve_id": "CVE-2022-33891", "cwe_id": ["CWE-77", "CWE-78"], "cve_language": "Python", "cve_description": "The Apache Spark UI offers the possibility to enable ACLs via the configuration option spark.acls.enable. With an authentication filter, this checks whether a user has access permissions to view or modify the application. If ACLs are enabled, a code path in HttpSecurityFilter can allow someone to perform impersonation by providing an arbitrary user name. A malicious user might then be able to reach a permission check function that will ultimately build a Unix shell command based on their input, and execute it. This will result in arbitrary shell command execution as the user Spark is currently running as. This affects Apache Spark versions 3.0.3 and earlier, versions 3.1.1 to 3.1.2, and versions 3.2.0 to 3.2.1.", "cvss": "8.8", "publish_date": "July 18, 2022", "AV": "NETWORK", "AC": "NETWORK", "PR": "LOW", "UI": "NONE", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "c83618e4e5fc092829a1f2a726f12fb832e802cc", "commit_message": "[SPARK-38992][CORE] Avoid using bash -c in ShellBasedGroupsMappingProvider\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to avoid using `bash -c` in `ShellBasedGroupsMappingProvider`. This could allow users a command injection.\n\n### Why are the changes needed?\n\nFor a security purpose.\n\n### Does this PR introduce _any_ user-facing change?\n\nVirtually no.\n\n### How was this patch tested?\n\nManually tested.\n\nCloses #36315 from HyukjinKwon/SPARK-38992.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>", "commit_date": "2022-04-22T10:01:05Z", "project": "apache/spark", "url": "https://api.github.com/repos/apache/spark/commits/c83618e4e5fc092829a1f2a726f12fb832e802cc", "html_url": "https://github.com/apache/spark/commit/c83618e4e5fc092829a1f2a726f12fb832e802cc", "windows_before": [{"commit_id": "8d59fdbacf28427f72dd30e5e7e135644a0f2190", "commit_date": "Fri Apr 22 12:54:48 2022 +0300", "commit_message": "[SPARK-38736][SQL][TESTS] Test the error classes: INVALID_ARRAY_INDEX & INVALID_ARRAY_INDEX_IN_ELEMENT_AT", "files_name": ["sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionAnsiErrorsSuite.scala"]}, {"commit_id": "e00d305053c98995efa990ffb2cf82cb281c71d8", "commit_date": "Fri Apr 22 14:34:01 2022 +0900", "commit_message": "[SPARK-38994][DOCS] Add an Python example of StreamingQueryListener", "files_name": ["docs/structured-streaming-programming-guide.md"]}, {"commit_id": "b8020de5f8fced11384798c5fa0e4f283f697f9b", "commit_date": "Fri Apr 22 08:08:50 2022 +0300", "commit_message": "[SPARK-38986][SQL] Prepend error class tag to error messages", "files_name": ["core/src/main/scala/org/apache/spark/ErrorInfo.scala", "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala", "core/src/test/scala/org/apache/spark/metrics/sink/GraphiteSinkSuite.scala", "core/src/test/scala/org/apache/spark/shuffle/sort/ShuffleExternalSorterSuite.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala", "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/date.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/datetime-parsing-invalid.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/decimalArithmeticOperations.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/literals.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/map.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/timestamp.sql.out", "sql/core/src/test/resources/sql-tests/results/columnresolution-negative.sql.out", "sql/core/src/test/resources/sql-tests/results/csv-functions.sql.out", "sql/core/src/test/resources/sql-tests/results/cte.sql.out", "sql/core/src/test/resources/sql-tests/results/date.sql.out", "sql/core/src/test/resources/sql-tests/results/datetime-formatting-invalid.sql.out", "sql/core/src/test/resources/sql-tests/results/datetime-legacy.sql.out", "sql/core/src/test/resources/sql-tests/results/datetime-parsing-invalid.sql.out", "sql/core/src/test/resources/sql-tests/results/describe-query.sql.out", "sql/core/src/test/resources/sql-tests/results/describe.sql.out", "sql/core/src/test/resources/sql-tests/results/group-analytics.sql.out", "sql/core/src/test/resources/sql-tests/results/group-by.sql.out", "sql/core/src/test/resources/sql-tests/results/grouping_set.sql.out", "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "sql/core/src/test/resources/sql-tests/results/join-lateral.sql.out", "sql/core/src/test/resources/sql-tests/results/json-functions.sql.out", "sql/core/src/test/resources/sql-tests/results/literals.sql.out", "sql/core/src/test/resources/sql-tests/results/natural-join.sql.out", "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/aggregates_part1.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/case.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/create_view.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/int4.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/join.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/select_having.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/select_implicit.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/union.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part2.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part3.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part4.sql.out", "sql/core/src/test/resources/sql-tests/results/query_regex_column.sql.out", "sql/core/src/test/resources/sql-tests/results/show-tables.sql.out", "sql/core/src/test/resources/sql-tests/results/subquery/negative-cases/invalid-correlation.sql.out", "sql/core/src/test/resources/sql-tests/results/table-aliases.sql.out", "sql/core/src/test/resources/sql-tests/results/timestamp.sql.out", "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp-ansi.sql.out", "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp.sql.out", "sql/core/src/test/resources/sql-tests/results/transform.sql.out", "sql/core/src/test/resources/sql-tests/results/typeCoercion/native/stringCastAndExpressions.sql.out", "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-aggregates_part1.sql.out", "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-case.sql.out", "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-join.sql.out", "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-select_having.sql.out", "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-select_implicit.sql.out", "sql/core/src/test/resources/sql-tests/results/udf/udf-group-analytics.sql.out", "sql/core/src/test/resources/sql-tests/results/udf/udf-group-by.sql.out", "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "sql/core/src/test/resources/sql-tests/results/window.sql.out", "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryErrorsSuiteBase.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionAnsiErrorsSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala"]}, {"commit_id": "2e4f4abf553cedec1fa8611b9494a01d24e6238a", "commit_date": "Fri Apr 22 12:30:34 2022 +0900", "commit_message": "[SPARK-38990][SQL] Avoid `NullPointerException` when evaluating date_trunc/trunc format as a bound reference", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala"]}, {"commit_id": "cbfa0513421d5e9e9b7410d7f86b8e25df4ae548", "commit_date": "Fri Apr 22 11:24:34 2022 +0800", "commit_message": "[SPARK-38974][SQL] Filter registered functions with a given database name in list functions", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala"]}, {"commit_id": "49d2f3c2458863eefd63c8ce38064757874ab4ad", "commit_date": "Fri Apr 22 11:12:32 2022 +0800", "commit_message": "[SPARK-38666][SQL] Add missing aggregate filter checks", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala"]}, {"commit_id": "f29f030d4b38c4dc6ee49defe6ad1fb870708c46", "commit_date": "Fri Apr 22 11:47:38 2022 +0900", "commit_message": "[SPARK-38938][PYTHON] Implement `inplace` and `columns` parameters of `Series.drop`", "files_name": ["python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst", "python/pyspark/pandas/series.py", "python/pyspark/pandas/tests/test_series.py"]}, {"commit_id": "05507db907c4f7845e86bde6a39226e1697ea638", "commit_date": "Fri Apr 22 11:46:39 2022 +0900", "commit_message": "[SPARK-38952][PYTHON] Implement `numeric_only` of `GroupBy.first` and `GroupBy.last`", "files_name": ["python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst", "python/pyspark/pandas/groupby.py", "python/pyspark/pandas/tests/test_groupby.py"]}, {"commit_id": "f3cc2814d4bc585dad92c9eca9a593d1617d27e9", "commit_date": "Fri Apr 22 11:43:46 2022 +0900", "commit_message": "[SPARK-38955][SQL] Disable lineSep option in 'from_csv' and 'schema_of_csv'", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala", "sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala"]}, {"commit_id": "7a58670e2e68ee4950cf62c2be236e00eb8fc44b", "commit_date": "Thu Apr 21 19:26:26 2022 -0700", "commit_message": "[MINOR][DOCS] Also remove Google Analytics from Spark release docs, per ASF policy", "files_name": ["docs/_layouts/global.html"]}, {"commit_id": "0b543e7480b6e414b23e02e6c805a33abc535c89", "commit_date": "Thu Apr 21 11:52:04 2022 -0700", "commit_message": "[SPARK-38950][SQL][FOLLOWUP] Fix java doc", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala"]}, {"commit_id": "11880d1e83061733325a448d91f41c565eb5f038", "commit_date": "Fri Apr 22 01:46:38 2022 +0800", "commit_message": "[SPARK-38984][SQL] Allow comparison between TimestampNTZ and Timestamp", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/AnsiTypeCoercion.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercionSuite.scala", "sql/core/src/test/resources/sql-tests/inputs/timestamp-ntz.sql", "sql/core/src/test/resources/sql-tests/results/timestamp-ntz.sql.out"]}, {"commit_id": "2e7633622e14392932fd17d41aef907bf3b4fa21", "commit_date": "Fri Apr 22 01:45:42 2022 +0800", "commit_message": "[SPARK-38980][SQL][TEST] Move error class tests requiring ANSI SQL mode to QueryExecutionAnsiErrorsSuite", "files_name": ["sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionAnsiErrorsSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala"]}, {"commit_id": "7221d754075656ce41edacb0fccc1cf89a62fc77", "commit_date": "Thu Apr 21 23:16:22 2022 +0800", "commit_message": "[SPARK-38950][SQL] Return Array of Predicate for SupportsPushDownCatalystFilters.pushedFilters", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala", "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileScanBuilder.scala", "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala", "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala"]}, {"commit_id": "43e610333fb78834a09cd82f3da32bad262564f3", "commit_date": "Thu Apr 21 21:53:28 2022 +0800", "commit_message": "[SPARK-38972][SQL] Support <param> in error-class messages", "files_name": ["core/src/main/resources/error/error-classes.json", "core/src/main/scala/org/apache/spark/ErrorInfo.scala"]}, {"commit_id": "bb5092b9af60afdceeccb239d14be660f77ae0ea", "commit_date": "Thu Apr 21 16:30:54 2022 +0800", "commit_message": "[SPARK-38916][CORE] Tasks not killed caused by race conditions between killTask() and launchTask()", "files_name": ["core/src/main/scala/org/apache/spark/executor/Executor.scala", "core/src/test/scala/org/apache/spark/executor/CoarseGrainedExecutorBackendSuite.scala", "core/src/test/scala/org/apache/spark/executor/ExecutorSuite.scala"]}, {"commit_id": "8fe5bca1773521d967b82a920c6881f081155bc3", "commit_date": "Thu Apr 21 16:19:20 2022 +0800", "commit_message": "[SPARK-38957][SQL] Use multipartIdentifier for parsing table-valued functions", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala"]}, {"commit_id": "36fc8bd185da99b64954ca0dd393b452fb788226", "commit_date": "Thu Apr 21 16:16:47 2022 +0800", "commit_message": "[SPARK-38432][SQL][FOLLOWUP] Fix problems in And/Or/Not to V2 Filter", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/sources/filters.scala", "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2PredicateSuite.scala"]}, {"commit_id": "4dc12eb54544a12ff7ddf078ca8bcec9471212c3", "commit_date": "Thu Apr 21 11:34:39 2022 +0900", "commit_message": "[SPARK-38936][SQL] Script transform feed thread should have name", "files_name": ["sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala"]}, {"commit_id": "f43c68cb38cb0556f2058be6d3a016083ef5152d", "commit_date": "Thu Apr 21 11:01:30 2022 +0900", "commit_message": "[SPARK-38581][PYTHON][DOCS] List of supported pandas APIs for pandas-on-Spark docs", "files_name": ["python/docs/source/user_guide/pandas_on_spark/index.rst", "python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst", "python/pyspark/pandas/missing/frame.py", "python/pyspark/pandas/missing/general_functions.py", "python/pyspark/pandas/missing/groupby.py", "python/pyspark/pandas/missing/indexes.py", "python/pyspark/pandas/missing/series.py", "python/pyspark/pandas/missing/window.py"]}, {"commit_id": "f54aee14cefb039b41e921e670b6c463ccb6ef53", "commit_date": "Thu Apr 21 09:22:38 2022 +0900", "commit_message": "[SPARK-38971][PYTHON][TESTS] Test anchor frame for in-place `Series.rename_axis`", "files_name": ["python/pyspark/pandas/tests/test_series.py"]}, {"commit_id": "1cc2d1641c23f028b5f175f80a695891ff13a6e2", "commit_date": "Wed Apr 20 09:48:09 2022 -0700", "commit_message": "[SPARK-38550][DOCS][FOLLOWUP] Improve the documentation of Diagnostic disk store", "files_name": ["core/src/main/scala/org/apache/spark/internal/config/Status.scala", "docs/configuration.md", "docs/monitoring.md"]}, {"commit_id": "488f3923c72f636c6ebb01c245609c51c3b68b67", "commit_date": "Thu Apr 21 00:37:28 2022 +0800", "commit_message": "[SPARK-38404][SQL] Improve CTE resolution when a nested CTE references an outer CTE", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala", "sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql", "sql/core/src/test/resources/sql-tests/results/cte-legacy.sql.out", "sql/core/src/test/resources/sql-tests/results/cte-nested.sql.out", "sql/core/src/test/resources/sql-tests/results/cte-nonlegacy.sql.out"]}, {"commit_id": "8acce8885964fcb5b23046bdac8a882d723f35cc", "commit_date": "Wed Apr 20 21:58:10 2022 +0800", "commit_message": "[SPARK-37681][SQL] Support ANSI Aggregate Function: regr_sxy", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/linearRegression.scala", "sql/core/src/test/resources/sql-functions/sql-expression-schema.md", "sql/core/src/test/resources/sql-tests/inputs/group-by.sql", "sql/core/src/test/resources/sql-tests/inputs/postgreSQL/aggregates_part1.sql", "sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-aggregates_part1.sql", "sql/core/src/test/resources/sql-tests/results/group-by.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/aggregates_part1.sql.out", "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-aggregates_part1.sql.out"]}, {"commit_id": "276bdbafe83a5c0b8425a20eb8101a630be8b752", "commit_date": "Wed Apr 20 21:41:58 2022 +0800", "commit_message": "[SPARK-38967][SQL] Turn \"spark.sql.ansi.strictIndexOperator\" into an internal configuration", "files_name": ["core/src/main/resources/error/error-classes.json", "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out"]}], "windows_after": [{"commit_id": "80929d6b549dfc61ade130a9d59dfa1abe72d681", "commit_date": "Fri Apr 22 18:17:04 2022 +0800", "commit_message": "[SPARK-38832][SQL][FOLLOWUP] Support propagate empty expression set for distinct key", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitor.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlanDistinctKeys.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitorSuite.scala"]}, {"commit_id": "ffaceac43293b14903f211d7e96a4c700a0bb4a4", "commit_date": "Fri Apr 22 08:22:59 2022 -0500", "commit_message": "[SPARK-38968][K8S] Remove a variable `hadoopConf` from `KerberosConfDriverFeatureStep`", "files_name": ["resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/KerberosConfDriverFeatureStep.scala"]}, {"commit_id": "86b8757c2c4bab6a0f7a700cf2c690cdd7f31eba", "commit_date": "Fri Apr 22 10:13:40 2022 -0700", "commit_message": "[SPARK-34960][SQL][DOCS][FOLLOWUP] Improve doc for DSv2 aggregate push down", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala", "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala"]}, {"commit_id": "f4a81ae6e631af27fc5eef81097b842d4e0e2e51", "commit_date": "Fri Apr 22 14:38:55 2022 -0500", "commit_message": "[SPARK-38973][SHUFFLE] Mark stage as merge finalized irrespective of its state", "files_name": ["core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala"]}, {"commit_id": "5e494d3de70c6e46f33addd751a227e6f9d5703f", "commit_date": "Fri Apr 22 23:07:01 2022 +0300", "commit_message": "[SPARK-38996][SQL] Use double quotes for types in error massages", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala", "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/date.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/datetime-parsing-invalid.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part2.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part3.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part4.sql.out", "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp-ansi.sql.out", "sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionAnsiErrorsSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala"]}, {"commit_id": "41c4f910c26dab648cb5f45c50b51dfd0a384739", "commit_date": "Fri Apr 22 23:12:04 2022 +0300", "commit_message": "[SPARK-38732][SQL][TESTS] Test the error class: INCOMPARABLE_PIVOT_COLUMN", "files_name": ["sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala"]}, {"commit_id": "0c9947dabcb71de414c97c0e60a1067e468f2642", "commit_date": "Fri Apr 22 14:11:47 2022 -0700", "commit_message": "[SPARK-38977][SQL] Fix schema pruning with correlated subqueries", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala", "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala"]}, {"commit_id": "ef94090de83acae6cdabce5e45c84e012200c531", "commit_date": "Sat Apr 23 09:11:45 2022 +0300", "commit_message": "[SPARK-38750][SQL][TESTS] Test the error class: SECOND_FUNCTION_ARGUMENT_NOT_INTEGER", "files_name": ["sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala"]}, {"commit_id": "3b81aaad1e9420fe537c7871e6ae337d5a34f881", "commit_date": "Sun Apr 24 10:34:38 2022 +0300", "commit_message": "[SPARK-38740][SQL][TESTS] Test the error class: INVALID_JSON_SCHEMA_MAPTYPE", "files_name": ["sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala"]}, {"commit_id": "7154fb989de9b0dfefb5dc8953ee63ab43dc414b", "commit_date": "Sun Apr 24 11:02:02 2022 -0500", "commit_message": "[SPARK-38885][BUILD][FOLLOWUP] Fix compile error on `Appleslicon/MacOs` and ensure `./dev/test-dependencies.sh` produce the same results on Linux and `Appleslicon/MacOs`", "files_name": ["common/network-common/pom.xml", "core/pom.xml", "pom.xml"]}, {"commit_id": "9440590a909d9222db838426c8e528ddec90e196", "commit_date": "Mon Apr 25 09:53:02 2022 +0900", "commit_message": "[SPARK-38948][TESTS] Fix `DiskRowQueue` leak in `PythonForeachWriterSuite`", "files_name": ["sql/core/src/test/scala/org/apache/spark/sql/execution/python/PythonForeachWriterSuite.scala"]}, {"commit_id": "5046b8c04cadca6605dd34b98b31850b643dfe45", "commit_date": "Mon Apr 25 11:46:34 2022 +0900", "commit_message": "[SPARK-38937][PYTHON] interpolate support param `limit_direction`", "files_name": ["python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst", "python/pyspark/pandas/frame.py", "python/pyspark/pandas/generic.py", "python/pyspark/pandas/series.py", "python/pyspark/pandas/tests/test_generic_functions.py"]}, {"commit_id": "e2930b8dc087e5a284b451c4cac6c1a2459b456d", "commit_date": "Mon Apr 25 13:49:15 2022 +0800", "commit_message": "[SPARK-38868][SQL] Don't propagate exceptions from filter predicate when optimizing outer joins", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OuterJoinEliminationSuite.scala"]}, {"commit_id": "8c800160b62657fa5ab16a69ab694360897468d6", "commit_date": "Mon Apr 25 15:44:20 2022 +0800", "commit_message": "[SPARK-38999][SQL] Refactor `FileSourceScanExec`: file scan physical node", "files_name": ["sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala"]}, {"commit_id": "f01bff971e36870e101b2f76195e0d380db64e0c", "commit_date": "Mon Apr 25 11:55:34 2022 +0300", "commit_message": "[SPARK-39007][SQL] Use double quotes for SQL configs in error messages", "files_name": ["core/src/main/resources/error/error-classes.json", "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/decimalArithmeticOperations.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/timestamp.sql.out", "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/case.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/select_having.sql.out", "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp-ansi.sql.out", "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-case.sql.out", "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-select_having.sql.out", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionAnsiErrorsSuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala"]}, {"commit_id": "efa70e4af9d73407b16b4877d1f0135dafa3899f", "commit_date": "Tue Apr 26 01:11:46 2022 +0800", "commit_message": "[SPARK-38939][SQL] Support DROP COLUMN [IF EXISTS] syntax", "files_name": ["sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4", "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableChange.java", "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala", "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/CatalogSuite.scala", "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "sql/core/src/test/scala/org/apache/spark/sql/connector/AlterTableTests.scala", "sql/core/src/test/scala/org/apache/spark/sql/connector/V2CommandsCaseSensitivitySuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala"]}, {"commit_id": "10a643c8af368cce131ef217f6ef610bf84f8b9c", "commit_date": "Mon Apr 25 20:25:56 2022 +0300", "commit_message": "[SPARK-39001][SQL][DOCS] Document which options are unsupported in CSV and JSON functions", "files_name": ["docs/sql-data-sources-csv.md", "docs/sql-data-sources-json.md"]}, {"commit_id": "da51dc7aa7674f158fb82f9f735af7d46f6a9399", "commit_date": "Mon Apr 25 21:53:17 2022 +0300", "commit_message": "[SPARK-38742][SQL][TESTS] Move the tests `MISSING_COLUMN` from SQLQuerySuite to QueryCompilationErrorsSuite", "files_name": ["sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala"]}, {"commit_id": "ce957e26e72d022b8fd9664bd19c431536302c36", "commit_date": "Tue Apr 26 08:51:54 2022 +0900", "commit_message": "[SPARK-39008][BUILD] Change ASF as a single author in Spark distribution", "files_name": ["R/pkg/DESCRIPTION", "R/pkg/pkgdown/_pkgdown_template.yml", "pom.xml"]}, {"commit_id": "e44ff986e20b4d7817901b2de5c01a11d0b9fd7d", "commit_date": "Tue Apr 26 10:55:44 2022 +0900", "commit_message": "[SPARK-38989][PYTHON] Implement `ignore_index` of `DataFrame/Series.sample`", "files_name": ["python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst", "python/pyspark/pandas/frame.py", "python/pyspark/pandas/series.py", "python/pyspark/pandas/tests/test_dataframe.py"]}, {"commit_id": "5056c6cc333982d39546f2acf9a889d102cc4ab3", "commit_date": "Tue Apr 26 10:57:11 2022 +0900", "commit_message": "[SPARK-39001][SQL][DOCS][FOLLOW-UP] Revert the doc changes for dropFieldIfAllNull, prefersDecimal and primitivesAsString (schema_of_json)", "files_name": ["docs/sql-data-sources-json.md"]}, {"commit_id": "148918b5469eb972b5d8e22d9b1cea5ca0721045", "commit_date": "Mon Apr 25 20:42:37 2022 -0700", "commit_message": "[SPARK-36664][CORE] Log time waiting for cluster resources", "files_name": ["core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala", "core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala", "core/src/main/scala/org/apache/spark/util/JsonProtocol.scala", "core/src/test/scala/org/apache/spark/scheduler/CoarseGrainedSchedulerBackendSuite.scala", "core/src/test/scala/org/apache/spark/scheduler/SparkListenerWithClusterSuite.scala", "core/src/test/scala/org/apache/spark/scheduler/dynalloc/ExecutorMonitorSuite.scala", "core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala"]}, {"commit_id": "028c472b72cdd0663c2d1e7769fee17e31e0abfb", "commit_date": "Tue Apr 26 13:15:41 2022 +0900", "commit_message": "[SPARK-39014][SQL] Respect `ignoreMissingFiles` from Data Source options in `InMemoryFileIndex`", "files_name": ["sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala", "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileIndexSuite.scala"]}, {"commit_id": "ac5ec646292c9738c29b5e0b5d29a95ce593dbc6", "commit_date": "Tue Apr 26 19:14:20 2022 +0900", "commit_message": "[SPARK-38821][PYTHON] Skip nsmall/nlarge nan test under pandas 1.4.[0,1,2]", "files_name": ["python/pyspark/pandas/tests/test_dataframe.py"]}, {"commit_id": "f24c9b0d135ce7ef4f219ab661a6b665663039f0", "commit_date": "Tue Apr 26 19:19:49 2022 +0900", "commit_message": "[SPARK-34079][SQL][FOLLOW-UP] Remove debug logging", "files_name": ["sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala"]}, {"commit_id": "f1286f324c032f9a875167fdbb265b4d495752c9", "commit_date": "Tue Apr 26 08:47:54 2022 -0500", "commit_message": "[SPARK-38944][CORE][SQL] Close `UnsafeSorterSpillReader` before `SpillableArrayIterator` throw `ConcurrentModificationException`", "files_name": ["core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java", "sql/core/src/main/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray.scala", "sql/core/src/test/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArraySuite.scala"]}, {"commit_id": "f66fde8a4f36f46c6b4e731ca9cc9f18ccf2e9dc", "commit_date": "Tue Apr 26 11:26:50 2022 -0500", "commit_message": "[SPARK-39016][TESTS] Fix compilation warnings related to \"`enum` will become a keyword in Scala 3\"", "files_name": ["connector/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala", "core/src/test/scala/org/apache/spark/internal/config/ConfigEntrySuite.scala"]}, {"commit_id": "ead45889278e8c5f71dc2ff2c7b020592e5e897f", "commit_date": "Tue Apr 26 22:06:07 2022 +0300", "commit_message": "[SPARK-39028][SQL] Use SparkDateTimeException when casting to datetime types failed", "files_name": ["sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/date.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/datetime-parsing-invalid.sql.out", "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part3.sql.out", "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp-ansi.sql.out"]}, {"commit_id": "97449d23a3b2232e14e63c6645919c5d93e4491c", "commit_date": "Tue Apr 26 15:52:22 2022 -0700", "commit_message": "[SPARK-39019][TESTS] Use `withTempPath` to clean up temporary data directory after `SPARK-37463: read/write Timestamp ntz to Orc with different time zone`", "files_name": ["sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala"]}, {"commit_id": "6b5a1f9df28262fa90d28dc15af67e8a37a9efcf", "commit_date": "Tue Apr 26 16:12:35 2022 -0700", "commit_message": "Revert \"[SPARK-38354][SQL] Add hash probes metric for shuffled hash join\"", "files_name": ["core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java", "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala", "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala", "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala", "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala"]}, {"commit_id": "3821d807a599a2d243465b4e443f1eb68251d432", "commit_date": "Wed Apr 27 10:10:11 2022 +0900", "commit_message": "[SPARK-39030][PYTHON] Rename sum to avoid shading the builtin Python function", "files_name": ["python/pyspark/pandas/base.py"]}, {"commit_id": "4fb4648ce3d7fab65ccfceb86cb6c839d0c921da", "commit_date": "Wed Apr 27 10:11:16 2022 +0900", "commit_message": "[SPARK-38879][PYTHON][TEST] Improve the test coverage for pyspark/rddsampler.py", "files_name": ["dev/sparktestsupport/modules.py", "python/pyspark/tests/test_rddsampler.py"]}, {"commit_id": "b8cacdf614affbda53e462531f8cb36911393782", "commit_date": "Wed Apr 27 10:11:58 2022 +0900", "commit_message": "[SPARK-39000][PYTHON] Convert bools to ints in basic statistical functions of GroupBy objects", "files_name": []}], "parents": [{"commit_id_before": "8d59fdbacf28427f72dd30e5e7e135644a0f2190", "url_before": "https://api.github.com/repos/apache/spark/commits/8d59fdbacf28427f72dd30e5e7e135644a0f2190", "html_url_before": "https://github.com/apache/spark/commit/8d59fdbacf28427f72dd30e5e7e135644a0f2190"}], "details": [{"raw_url": "https://github.com/apache/spark/raw/c83618e4e5fc092829a1f2a726f12fb832e802cc/core%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Fspark%2Fsecurity%2FShellBasedGroupsMappingProvider.scala", "code": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.spark.security\n\nimport org.apache.spark.internal.Logging\nimport org.apache.spark.util.Utils\n\n/**\n * This class is responsible for getting the groups for a particular user in Unix based\n * environments. This implementation uses the Unix Shell based id command to fetch the user groups\n * for the specified user. It does not cache the user groups as the invocations are expected\n * to be infrequent.\n */\n\nprivate[spark] class ShellBasedGroupsMappingProvider extends GroupMappingServiceProvider\n  with Logging {\n\n  private lazy val idPath = Utils.executeAndGetOutput(\"which\" :: \"id\" :: Nil).stripLineEnd\n\n  override def getGroups(username: String): Set[String] = {\n    val userGroups = getUnixGroups(username)\n    logDebug(\"User: \" + username + \" Groups: \" + userGroups.mkString(\",\"))\n    userGroups\n  }\n\n  // shells out a \"bash -c id -Gn username\" to get user groups\n  private def getUnixGroups(username: String): Set[String] = {\n    // we need to get rid of the trailing \"\\n\" from the result of command execution\n    Utils.executeAndGetOutput(idPath ::  \"-Gn\" :: username :: Nil).stripLineEnd.split(\" \").toSet\n  }\n}\n", "code_before": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.spark.security\n\nimport org.apache.spark.internal.Logging\nimport org.apache.spark.util.Utils\n\n/**\n * This class is responsible for getting the groups for a particular user in Unix based\n * environments. This implementation uses the Unix Shell based id command to fetch the user groups\n * for the specified user. It does not cache the user groups as the invocations are expected\n * to be infrequent.\n */\n\nprivate[spark] class ShellBasedGroupsMappingProvider extends GroupMappingServiceProvider\n  with Logging {\n\n  override def getGroups(username: String): Set[String] = {\n    val userGroups = getUnixGroups(username)\n    logDebug(\"User: \" + username + \" Groups: \" + userGroups.mkString(\",\"))\n    userGroups\n  }\n\n  // shells out a \"bash -c id -Gn username\" to get user groups\n  private def getUnixGroups(username: String): Set[String] = {\n    val cmdSeq = Seq(\"bash\", \"-c\", \"id -Gn \" + username)\n    // we need to get rid of the trailing \"\\n\" from the result of command execution\n    Utils.executeAndGetOutput(cmdSeq).stripLineEnd.split(\" \").toSet\n  }\n}\n", "patch": "@@ -30,6 +30,8 @@ import org.apache.spark.util.Utils\n private[spark] class ShellBasedGroupsMappingProvider extends GroupMappingServiceProvider\n   with Logging {\n \n+  private lazy val idPath = Utils.executeAndGetOutput(\"which\" :: \"id\" :: Nil).stripLineEnd\n+\n   override def getGroups(username: String): Set[String] = {\n     val userGroups = getUnixGroups(username)\n     logDebug(\"User: \" + username + \" Groups: \" + userGroups.mkString(\",\"))\n@@ -38,8 +40,7 @@ private[spark] class ShellBasedGroupsMappingProvider extends GroupMappingService\n \n   // shells out a \"bash -c id -Gn username\" to get user groups\n   private def getUnixGroups(username: String): Set[String] = {\n-    val cmdSeq = Seq(\"bash\", \"-c\", \"id -Gn \" + username)\n     // we need to get rid of the trailing \"\\n\" from the result of command execution\n-    Utils.executeAndGetOutput(cmdSeq).stripLineEnd.split(\" \").toSet\n+    Utils.executeAndGetOutput(idPath ::  \"-Gn\" :: username :: Nil).stripLineEnd.split(\" \").toSet\n   }\n }", "file_path": "files/2022_7/185", "file_language": "scala", "file_name": "core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
