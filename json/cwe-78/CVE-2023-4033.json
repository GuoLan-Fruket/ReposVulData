{"index": 11846, "cve_id": "CVE-2023-4033", "cwe_id": ["CWE-78"], "cve_language": "Python", "cve_description": "OS Command Injection in GitHub repository mlflow/mlflow prior to 2.6.0.", "cvss": "7.8", "publish_date": "July 31, 2023", "AV": "LOCAL", "AC": "LOCAL", "PR": "LOW", "UI": "NONE", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "6dde93758d42455cb90ef324407919ed67668b9b", "commit_message": "Fix potential issues with PyFuncBackend in cli (#9053)\n\nSigned-off-by: Serena Ruan <serena.rxy@gmail.com>", "commit_date": "2023-07-18T01:31:13Z", "project": "mlflow/mlflow", "url": "https://api.github.com/repos/mlflow/mlflow/commits/6dde93758d42455cb90ef324407919ed67668b9b", "html_url": "https://github.com/mlflow/mlflow/commit/6dde93758d42455cb90ef324407919ed67668b9b", "windows_before": [{"commit_id": "330bf0b38b07d4adf3fae99af1b44c8e857f514a", "commit_date": "Tue Jul 18 08:16:12 2023 +0900", "commit_message": "Remove unnecessary `openai_api_base` (#9100)", "files_name": ["docs/gateway_api_docs.py", "docs/source/gateway/index.rst", "examples/gateway/openai/config.yaml"]}, {"commit_id": "02ea9cb2d55878b995672b5e595e12756999c9f3", "commit_date": "Tue Jul 18 04:16:23 2023 +0530", "commit_message": "Updated ruff version to 0.0.278 (#9106)", "files_name": ["requirements/lint-requirements.txt"]}, {"commit_id": "580d614be2042c0236c757682c3ee48eed9cda0a", "commit_date": "Tue Jul 18 03:22:24 2023 +0530", "commit_message": "Removed unused `_user` variable (#9105)", "files_name": ["mlflow/server/auth/__init__.py"]}, {"commit_id": "d2ed085af26ebc4a97bd091dd7b358b1e49f1e8e", "commit_date": "Mon Jul 17 23:58:12 2023 +0900", "commit_message": "Use random experiment name in auth example (#9098)", "files_name": ["examples/auth/auth.py"]}, {"commit_id": "81308c1ab00ff26f1a7f69a01efb6917f11c5612", "commit_date": "Mon Jul 17 23:55:47 2023 +0900", "commit_message": "Improve supported provider models (#9101)", "files_name": ["docs/source/gateway/index.rst"]}, {"commit_id": "6dfe9b11f1f14b8c3c5f1d5ebf2fe374013ac2e0", "commit_date": "Mon Jul 17 20:53:20 2023 +0600", "commit_message": "fix: remove indentation in example yaml config for gateway (#9104)", "files_name": ["docs/source/gateway/index.rst"]}, {"commit_id": "1b384da05d4dca38f23f9090b93e053b9f4bbed7", "commit_date": "Mon Jul 17 17:21:06 2023 +0700", "commit_message": "Update requirements/lint-requirements.txt (#9097)", "files_name": ["requirements/lint-requirements.txt"]}, {"commit_id": "9d42f27c64d82815c1c92b75ab97406b76317e02", "commit_date": "Mon Jul 17 18:01:48 2023 +0800", "commit_message": "Fix sklearn flavor `_get_metric_name_list` function (#9095)", "files_name": ["mlflow/sklearn/__init__.py"]}, {"commit_id": "2d576b72925ed68ab8361c17a9b19da57a6d21e6", "commit_date": "Mon Jul 17 18:18:33 2023 +0900", "commit_message": "Run `python3 dev/update_mlflow_versions.py post-release...` (#9096)", "files_name": ["mlflow/R/mlflow/DESCRIPTION", "mlflow/java/client/pom.xml", "mlflow/java/pom.xml", "mlflow/java/scoring/pom.xml", "mlflow/java/scoring/src/main/java/org/mlflow/sagemaker/ScoringServer.java", "mlflow/java/scoring/src/test/java/org/mlflow/ScoringServerTest.java", "mlflow/java/spark/pom.xml", "mlflow/server/js/src/common/constants.tsx", "mlflow/version.py"]}, {"commit_id": "4bd7566f77d683ec0859e78e6d9a20423de92bec", "commit_date": "Mon Jul 17 16:31:31 2023 +0900", "commit_message": "Run `python3 dev/update_changelog.py --prev-version 2.4...` (#9061)", "files_name": ["CHANGELOG.md"]}, {"commit_id": "0235cc8799e7bb22b03cbe8bb5a73a8f50e0bb02", "commit_date": "Mon Jul 17 15:44:22 2023 +0900", "commit_message": "Fix `IntegrityError` handling in amind user creation (#9090)", "files_name": ["mlflow/server/auth/__init__.py", "mlflow/server/auth/sqlalchemy_store.py"]}, {"commit_id": "d137caf7946ff630f16b61c50d6c492b0f42d7d3", "commit_date": "Mon Jul 17 14:25:28 2023 +0900", "commit_message": "Run `blacken-docs` in autoformat workflow (#9082)", "files_name": [".github/workflows/autoformat.yml"]}, {"commit_id": "3bec11b86a259ec7cdb49ef222121ee72244eb48", "commit_date": "Mon Jul 17 14:16:26 2023 +0900", "commit_message": "Add missing `__init__.py` files (#9089)", "files_name": ["mlflow/server/auth/db/migrations/__init__.py", "mlflow/server/auth/db/migrations/versions/__init__.py"]}, {"commit_id": "aca99804bea0083878b681de7714f6e0372a0e9c", "commit_date": "Mon Jul 17 13:15:02 2023 +0900", "commit_message": "Fix the default value of `candidate_count` (#9088)", "files_name": ["mlflow/gateway/schemas/chat.py", "tests/gateway/providers/test_anthropic.py", "tests/gateway/providers/test_openai.py"]}, {"commit_id": "bc0d21e5999afad6a822030ee2544fbff9fa9e7a", "commit_date": "Sun Jul 16 20:28:56 2023 -0700", "commit_message": "Gateway: Improve documentation for query parameters (#9083)", "files_name": ["docs/source/gateway/index.rst"]}, {"commit_id": "e293a73b510c924cbca50b6337b6d6f9fd9f8f1b", "commit_date": "Fri Jul 14 21:22:18 2023 -0700", "commit_message": "Gateway docs tweaks / improvements (#9081)", "files_name": ["docs/source/gateway/index.rst"]}, {"commit_id": "5a8e4a16e78561e859260240315cede0a4bc41d1", "commit_date": "Sat Jul 15 11:15:49 2023 +0900", "commit_message": "Run gateway workflow on push (#9080)", "files_name": [".github/workflows/gateway.yml"]}, {"commit_id": "7ce907cab1d973ea4be9ab41562cdfabb69bb07f", "commit_date": "Sat Jul 15 10:52:39 2023 +0900", "commit_message": "Fix `texts` to `text` (#9079)", "files_name": ["docs/source/gateway/index.rst"]}, {"commit_id": "2cc56da462fe5526b26728bd029140761dccf88e", "commit_date": "Sat Jul 15 10:29:29 2023 +0900", "commit_message": "Fix curl examples in gateway docs (#9078)", "files_name": ["docs/source/gateway/index.rst"]}, {"commit_id": "8b1e8fc434202886fe0c96a0def91af0206a7eca", "commit_date": "Sat Jul 15 08:45:23 2023 +0900", "commit_message": "Skip gateway job if PR is a draft (#9076)", "files_name": [".github/workflows/gateway.yml"]}, {"commit_id": "23eec6ce656fc8d664ba038f474042a1c9593838", "commit_date": "Fri Jul 14 22:51:13 2023 +0900", "commit_message": "Remove unnecessary debug calls in auth (#9071)", "files_name": ["mlflow/server/auth/__init__.py"]}, {"commit_id": "203afbab11f47733476275f1462b33c6ebf8991d", "commit_date": "Fri Jul 14 21:45:36 2023 +0900", "commit_message": "Separate extras and package name in requirements YAML (#9074)", "files_name": [".github/actions/update-requirements/action.yml", ".github/workflows/requirements.yml", "dev/generate_requirements.py", "requirements/gateway-requirements.yaml"]}, {"commit_id": "47c58b5ff9007fd199b9c63f03582495bdc47d9f", "commit_date": "Fri Jul 14 21:15:04 2023 +0900", "commit_message": "Fix `_all_tables_exist` (#9075)", "files_name": ["mlflow/store/db/utils.py"]}, {"commit_id": "e373f791957c039e247a31ae28068368057c914a", "commit_date": "Fri Jul 14 20:49:55 2023 +0900", "commit_message": "Run `python3 dev/update_ml_package_versions.py` (#9073)", "files_name": ["mlflow/ml-package-versions.yml", "mlflow/ml_package_versions.py"]}, {"commit_id": "da7048a3d30218d832b9e00f5344c0b6398d05ee", "commit_date": "Fri Jul 14 20:10:49 2023 +0900", "commit_message": "Enable authentication on proxy artifact routes (#8975)", "files_name": ["mlflow/server/auth/__init__.py", "tests/server/auth/test_auth.py"]}, {"commit_id": "701423260e8147c0424445ce105c55fef6f494ee", "commit_date": "Fri Jul 14 19:55:31 2023 +0900", "commit_message": "Ignore IntegrityError when creating admin user (#9070)", "files_name": ["mlflow/server/auth/__init__.py"]}, {"commit_id": "1c75c4812bcbde979e3cf9696f5880d2f5a91acc", "commit_date": "Fri Jul 14 18:59:44 2023 +0900", "commit_message": "Run `python3 dev/update_pypi_package_index.py` (#9072)", "files_name": ["mlflow/pypi_package_index.json"]}, {"commit_id": "7a688bf4942c63608de91909aa4472ec1e6652b3", "commit_date": "Fri Jul 14 13:56:38 2023 +0900", "commit_message": "Database migration for mlflow auth (#9000)", "files_name": ["docs/source/auth/index.rst", "mlflow/server/auth/__main__.py", "mlflow/server/auth/cli.py", "mlflow/server/auth/db/__init__.py", "mlflow/server/auth/db/cli.py", "mlflow/server/auth/db/migrations/alembic.ini", "mlflow/server/auth/db/migrations/env.py", "mlflow/server/auth/db/migrations/script.py.mako", "mlflow/server/auth/db/migrations/versions/8606fa83a998_initial_migration.py", "mlflow/server/auth/db/utils.py", "mlflow/server/auth/sqlalchemy_store.py", "setup.py", "tests/server/auth/db/test_cli.py", "tests/server/auth/test_cli.py"]}, {"commit_id": "63778473bbcb45d96b43467a7a223dbd57824ada", "commit_date": "Thu Jul 13 23:23:03 2023 -0400", "commit_message": "[Gateway] Adjust client and server timeout settings (#9069)", "files_name": ["mlflow/gateway/client.py", "mlflow/gateway/constants.py", "mlflow/gateway/providers/utils.py", "mlflow/utils/rest_utils.py", "tests/gateway/providers/test_openai.py", "tests/gateway/test_client.py"]}, {"commit_id": "ae0bb515b16115bf2d0ab9d35c82659c2d5c57c1", "commit_date": "Thu Jul 13 23:24:17 2023 +0800", "commit_message": "Revert inf params (#9059)", "files_name": ["docs/source/conf.py", "examples/flower_classifier/image_pyfunc.py", "examples/pyfunc/train.py", "examples/sktime/flavor.py", "examples/transformers/simple.py", "mlflow/catboost.py", "mlflow/diviner.py", "mlflow/fastai/__init__.py", "mlflow/gluon/__init__.py", "mlflow/h2o.py", "mlflow/johnsnowlabs.py", "mlflow/langchain/__init__.py", "mlflow/lightgbm.py", "mlflow/models/evaluation/default_evaluator.py", "mlflow/models/model.py", "mlflow/models/signature.py", "mlflow/models/utils.py", "mlflow/onnx.py", "mlflow/openai/__init__.py", "mlflow/paddle/__init__.py", "mlflow/pmdarima.py", "mlflow/prophet.py", "mlflow/pyfunc/__init__.py", "mlflow/pyfunc/model.py", "mlflow/pytorch/__init__.py", "mlflow/recipes/utils/wrapped_recipe_model.py", "mlflow/sentence_transformers.py", "mlflow/shap.py", "mlflow/sklearn/__init__.py", "mlflow/spacy.py", "mlflow/spark.py", "mlflow/statsmodels.py", "mlflow/tensorflow/__init__.py", "mlflow/transformers.py", "mlflow/types/__init__.py", "mlflow/types/schema.py", "mlflow/types/utils.py", "mlflow/xgboost/__init__.py", "tests/db/check_migration.py", "tests/db/test_tracking_operations.py", "tests/evaluate/test_default_evaluator.py", "tests/evaluate/test_evaluation.py", "tests/openai/test_openai_model_export.py", "tests/pyfunc/test_model_export_with_class_and_artifacts.py", "tests/pyfunc/test_pyfunc_class_methods.py", "tests/pyfunc/test_pyfunc_schema_enforcement.py", "tests/pyfunc/test_spark.py", "tests/pytorch/test_pytorch_model_export.py", "tests/sentence_transformers/test_sentence_transformers_model_export.py", "tests/sklearn/test_sklearn_model_export.py", "tests/store/model_registry/test_file_store.py", "tests/test_cli.py", "tests/transformers/test_transformers_model_export.py", "tests/types/test_schema.py"]}, {"commit_id": "7a5ed130d6dc0ed6d348d259e16965a33a415d79", "commit_date": "Thu Jul 13 19:11:56 2023 +0900", "commit_message": "Fix default value of `MLFLOW_GATEWAY_URI` (#9063)", "files_name": ["mlflow/environment_variables.py"]}, {"commit_id": "aa8bdda76539994ed376cbb94d5c2084728b914c", "commit_date": "Thu Jul 13 19:11:29 2023 +0900", "commit_message": "Fix Gateway API documentation (#9062)", "files_name": ["docs/gateway_api_docs.py"]}, {"commit_id": "0f2ad0236e355b0816a06670eccf69f57551fa2d", "commit_date": "Thu Jul 13 15:59:11 2023 +0800", "commit_message": "fix validate_path_is_safe function on windows (#8999)", "files_name": ["mlflow/server/handlers.py", "tests/tracking/test_rest_tracking.py"]}, {"commit_id": "83c149f0b7f3e75b50df9ddcb93e92786e0122bd", "commit_date": "Thu Jul 13 02:31:57 2023 -0400", "commit_message": "[Gateway] disallow model argument to be passed during query (#9060)", "files_name": ["mlflow/gateway/providers/anthropic.py", "mlflow/gateway/providers/base.py", "mlflow/gateway/providers/cohere.py"]}], "windows_after": [{"commit_id": "de65bb96331b8fc3e06117b0e2cd349bd397f680", "commit_date": "Tue Jul 18 16:20:52 2023 +0900", "commit_message": "Add `make view` to quickly view documentation (#9103)", "files_name": ["docs/Makefile"]}, {"commit_id": "2682ebcbf41288680d8166eb6b77e4a6980089ec", "commit_date": "Tue Jul 18 10:52:52 2023 -0700", "commit_message": "Add sentence-transformers doc & example (#9047)", "files_name": ["docs/source/models.rst", "examples/sentence_transformers/simple.py", "tests/examples/test_examples.py"]}, {"commit_id": "dc5885a80c06981379d9b5379506c4fc746b5412", "commit_date": "Wed Jul 19 10:35:21 2023 +0800", "commit_message": "Clean CI disk for pytorch cross test on version >= 2 (#9112)", "files_name": [".github/workflows/cross-version-tests.yml"]}, {"commit_id": "f2fadddcbb653d4ec7f2ec3367b2f237d56c9f72", "commit_date": "Wed Jul 19 15:39:11 2023 +0700", "commit_message": "Update enable UP032 on pyproject.toml (#9118)", "files_name": ["dev/update_ml_package_versions.py", "mlflow/catboost.py", "mlflow/langchain/__init__.py", "mlflow/store/model_registry/dbmodels/models.py", "mlflow/store/tracking/dbmodels/models.py", "mlflow/store/tracking/file_store.py", "mlflow/store/tracking/sqlalchemy_store.py", "mlflow/tracking/client.py", "mlflow/types/utils.py", "mlflow/utils/__init__.py", "mlflow/utils/model_utils.py", "pyproject.toml", "tests/store/tracking/test_sqlalchemy_store.py"]}, {"commit_id": "2751953c9537d053d7bbe0258c9f411187c854a0", "commit_date": "Wed Jul 19 13:21:45 2023 +0100", "commit_message": "Docs model serve docker h (#9067)", "files_name": ["docs/source/models.rst"]}, {"commit_id": "7180af3be585931aa76419fa35a4268a1e3f7249", "commit_date": "Thu Jul 20 08:36:28 2023 +0900", "commit_message": "Document LangChain integration (#9108)", "files_name": ["docs/source/gateway/index.rst"]}, {"commit_id": "26eb4523c2ced23bbab776ccd91a5ce564d64f4f", "commit_date": "Thu Jul 20 16:31:41 2023 +0900", "commit_message": "Fix `type` to `route_type` in gateway config examples (#9125)", "files_name": ["docs/source/gateway/index.rst"]}, {"commit_id": "dad74eb0d4d8799cacb4fc954f80344741a9f6ec", "commit_date": "Thu Jul 20 17:08:55 2023 +0900", "commit_message": "Import gateway in `mlflow/__init__.py` for convenience  (#9124)", "files_name": ["mlflow/__init__.py"]}, {"commit_id": "4f76275317accb0b51770a4a83b8d1db2fda21f9", "commit_date": "Thu Jul 20 23:47:57 2023 +0800", "commit_message": "Fix infer_signature warning in sklearn (#9123)", "files_name": ["mlflow/sklearn/__init__.py", "tests/sklearn/test_sklearn_autolog.py"]}, {"commit_id": "106a306a5d6a85c2c1f55b7aa0fb5b01d5a1ee55", "commit_date": "Fri Jul 21 09:30:15 2023 +0900", "commit_message": "Drop `gluon` support (#9122)", "files_name": ["conftest.py", "docs/source/models.rst", "docs/source/python_api/mlflow.gluon.rst", "docs/source/tracking.rst", "examples/gluon/train.py", "mlflow/__init__.py", "mlflow/gluon/__init__.py", "mlflow/gluon/_autolog.py", "mlflow/ml-package-versions.yml", "mlflow/ml_package_versions.py", "mlflow/models/__init__.py", "mlflow/tracking/fluent.py", "mlflow/utils/autologging_utils/versioning.py", "tests/autologging/test_autologging_behaviors_integration.py", "tests/autologging/test_autologging_utils.py", "tests/check_mlflow_lazily_imports_ml_packages.py", "tests/gluon/test_gluon_autolog.py", "tests/gluon/test_gluon_model_export.py", "tests/gluon/utils.py", "tests/tracking/fluent/test_fluent_autolog.py"]}, {"commit_id": "f3db4dd23683bcaf9524cb5f9c8aab138c9d27b6", "commit_date": "Fri Jul 21 10:05:31 2023 +0900", "commit_message": "Add langchain integration in gateway doc (#9121)", "files_name": ["docs/source/gateway/index.rst"]}, {"commit_id": "3805d1f36ebc9ecdf3a9013fe4e46adc0bd2538c", "commit_date": "Fri Jul 21 11:55:59 2023 +0900", "commit_message": "Revert \"Drop `gluon` support\" (#9128)", "files_name": ["conftest.py", "docs/source/models.rst", "docs/source/python_api/mlflow.gluon.rst", "docs/source/tracking.rst", "examples/gluon/train.py", "mlflow/__init__.py", "mlflow/gluon/__init__.py", "mlflow/gluon/_autolog.py", "mlflow/ml-package-versions.yml", "mlflow/ml_package_versions.py", "mlflow/models/__init__.py", "mlflow/tracking/fluent.py", "mlflow/utils/autologging_utils/versioning.py", "tests/autologging/test_autologging_behaviors_integration.py", "tests/autologging/test_autologging_utils.py", "tests/check_mlflow_lazily_imports_ml_packages.py", "tests/gluon/test_gluon_autolog.py", "tests/gluon/test_gluon_model_export.py", "tests/gluon/utils.py", "tests/tracking/fluent/test_fluent_autolog.py"]}, {"commit_id": "f4ba3a9df13ba9f44077491baf3522d1dc4c8257", "commit_date": "Fri Jul 21 19:22:09 2023 +0800", "commit_message": "Fix whisper.py example (#9129)", "files_name": ["examples/transformers/whisper.py"]}, {"commit_id": "80cc1aaded8fd9c69d8477e4ba65550d158672fa", "commit_date": "Fri Jul 21 21:00:32 2023 +0900", "commit_message": "Run pylint and ruff if their config files are changed (#9131)", "files_name": [".github/workflows/master.yml", "pyproject.toml"]}, {"commit_id": "fc112a2af9b294ae9e58a22128cd1271aedc64b8", "commit_date": "Fri Jul 21 21:14:17 2023 +0900", "commit_message": "Bump pygments from 2.13.0 to 2.15.0 in /.devcontainer (#9126)", "files_name": [".devcontainer/requirements.txt"]}, {"commit_id": "d454525e319bb9deb7c93941b1b40e54f13b483b", "commit_date": "Fri Jul 21 17:44:45 2023 -0700", "commit_message": "Fix redundant docs page (#9135)", "files_name": ["docs/docs/source/python_api/mlflow.johnsnowlabs.rst"]}, {"commit_id": "22fb98ff1640e47cc2ad465c2f7e7b1a91644197", "commit_date": "Sat Jul 22 10:00:21 2023 +0900", "commit_message": "Remove tests for gluon (#9132)", "files_name": ["dev/set_matrix.py", "tests/autologging/test_autologging_behaviors_integration.py", "tests/autologging/test_autologging_utils.py", "tests/check_mlflow_lazily_imports_ml_packages.py", "tests/gluon/test_gluon_autolog.py", "tests/gluon/test_gluon_model_export.py", "tests/gluon/utils.py", "tests/tracking/fluent/test_fluent_autolog.py"]}, {"commit_id": "4d79ccc2821c94102e4e3797dad2531dbb6f117c", "commit_date": "Sat Jul 22 15:23:45 2023 +0900", "commit_message": "Autoformat pyhton files if `pyproject.toml` changes (#9136)", "files_name": [".github/workflows/autoformat.yml"]}, {"commit_id": "9d88ac5c1167d0b13e0a6986195b7669ee8bd9c1", "commit_date": "Sat Jul 22 17:29:15 2023 +0200", "commit_message": "Enable flake8 pytest style (#9051)", "files_name": ["pyproject.toml", "tests/artifacts/test_artifacts.py", "tests/autologging/test_autologging_behaviors_integration.py", "tests/autologging/test_autologging_utils.py", "tests/data/test_spark_dataset.py", "tests/entities/conftest.py", "tests/gateway/providers/test_provider_utils.py", "tests/gateway/test_gateway_config_parsing.py", "tests/gateway/test_utils.py", "tests/projects/test_databricks.py", "tests/recipes/test_ingest_step.py", "tests/recipes/test_train_step.py", "tests/sentence_transformers/test_sentence_transformers_model_export.py", "tests/server/test_handlers.py", "tests/server/test_init.py", "tests/server/test_prometheus_exporter.py", "tests/sklearn/test_sklearn_autolog.py", "tests/spark/autologging/datasource/test_spark_datasource_autologging_crossframework.py", "tests/spark/autologging/datasource/test_spark_datasource_autologging_unit.py", "tests/spark/autologging/ml/test_pyspark_ml_autologging.py", "tests/spark/test_spark_model_export.py", "tests/store/_unity_catalog/model_registry/test_unity_catalog_rest_store.py", "tests/store/artifact/test_azure_data_lake_artifact_repo.py", "tests/store/artifact/test_cli.py", "tests/store/artifact/test_databricks_artifact_repo.py", "tests/store/artifact/test_databricks_models_artifact_repo.py", "tests/store/artifact/test_dbfs_artifact_repo_delegation.py", "tests/store/artifact/test_dbfs_fuse_artifact_repo.py", "tests/store/artifact/test_dbfs_rest_artifact_repo.py", "tests/store/model_registry/test_file_store.py", "tests/store/tracking/test_file_store.py", "tests/store/tracking/test_sqlalchemy_store.py", "tests/store/tracking/test_sqlalchemy_store_schema.py", "tests/tracking/_model_registry/test_utils.py", "tests/tracking/test_rest_tracking.py", "tests/tracking/test_tracking.py", "tests/transformers/test_transformers_autolog.py", "tests/transformers/test_transformers_model_export.py", "tests/utils/test_proto_json_utils.py"]}, {"commit_id": "aabfa91bca0afb98a6b8803c3079e054e4be6fbf", "commit_date": "Sun Jul 23 07:12:20 2023 +0700", "commit_message": "Update ruff on requirements/lint-requirements.txt (#9120)", "files_name": ["mlflow/pyfunc/backend.py", "requirements/lint-requirements.txt", "tests/projects/test_projects.py", "tests/recipes/test_evaluate_step.py", "tests/recipes/test_predict_step.py", "tests/recipes/test_register_step.py", "tests/recipes/test_split_step.py", "tests/recipes/test_train_step.py", "tests/recipes/test_transform_step.py"]}, {"commit_id": "31fc811b02ff9636e12b8ef4238e438b0bde899f", "commit_date": "Sun Jul 23 08:34:34 2023 +0700", "commit_message": "Replace % formatting with f-string (#9139)", "files_name": ["examples/docker/train.py", "examples/multistep_workflow/etl_data.py", "examples/multistep_workflow/load_raw_data.py", "examples/paddle/train_high_level_api.py", "examples/paddle/train_low_level_api.py", "examples/pytorch/mnist_tensorboard_artifact.py", "examples/rapids/mlflow_project/src/rf_test/train_simple.py", "examples/shap/explainer_logging.py", "examples/sklearn_elasticnet_diabetes/linux/train_diabetes.py", "examples/sklearn_elasticnet_diabetes/osx/train_diabetes.py", "examples/sklearn_elasticnet_wine/train.py", "examples/sklearn_logistic_regression/train.py", "examples/spacy/train.py", "mlflow/cli.py", "mlflow/deployments/utils.py", "mlflow/entities/lifecycle_stage.py", "mlflow/experiments.py", "mlflow/models/utils.py"]}, {"commit_id": "153c7d72fb3a4822b7b32673ab51021ebc8f4329", "commit_date": "Mon Jul 24 08:34:38 2023 +0700", "commit_message": "python formatting f-string from script (#9141)", "files_name": ["mlflow/projects/__init__.py", "mlflow/projects/_project_spec.py", "mlflow/projects/databricks.py", "mlflow/projects/utils.py", "mlflow/recipes/steps/train.py", "mlflow/runs.py"]}], "parents": [{"commit_id_before": "330bf0b38b07d4adf3fae99af1b44c8e857f514a", "url_before": "https://api.github.com/repos/mlflow/mlflow/commits/330bf0b38b07d4adf3fae99af1b44c8e857f514a", "html_url_before": "https://github.com/mlflow/mlflow/commit/330bf0b38b07d4adf3fae99af1b44c8e857f514a"}], "details": [{"raw_url": "https://github.com/mlflow/mlflow/raw/6dde93758d42455cb90ef324407919ed67668b9b/mlflow%2Fpyfunc%2F_mlflow_pyfunc_backend_predict.py", "code": "\"\"\"\nThis script should be executed in a fresh python interpreter process using `subprocess`.\n\"\"\"\nimport argparse\n\nfrom mlflow.pyfunc.scoring_server import _predict\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-uri\", required=True)\n    parser.add_argument(\"--input-path\", required=False)\n    parser.add_argument(\"--output-path\", required=False)\n    parser.add_argument(\"--content-type\", required=True)\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    _predict(\n        model_uri=args.model_uri,\n        input_path=args.input_path if args.input_path else None,\n        output_path=args.output_path if args.output_path else None,\n        content_type=args.content_type,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "code_before": "", "patch": "@@ -0,0 +1,29 @@\n+\"\"\"\n+This script should be executed in a fresh python interpreter process using `subprocess`.\n+\"\"\"\n+import argparse\n+\n+from mlflow.pyfunc.scoring_server import _predict\n+\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--model-uri\", required=True)\n+    parser.add_argument(\"--input-path\", required=False)\n+    parser.add_argument(\"--output-path\", required=False)\n+    parser.add_argument(\"--content-type\", required=True)\n+    return parser.parse_args()\n+\n+\n+def main():\n+    args = parse_args()\n+    _predict(\n+        model_uri=args.model_uri,\n+        input_path=args.input_path if args.input_path else None,\n+        output_path=args.output_path if args.output_path else None,\n+        content_type=args.content_type,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()", "file_path": "files/2023_7/1", "file_language": "py", "file_name": "mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [], "function_after": [{"function": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-uri\", required=True)\n    parser.add_argument(\"--input-path\", required=False)\n    parser.add_argument(\"--output-path\", required=False)\n    parser.add_argument(\"--content-type\", required=True)\n    return parser.parse_args()", "target": 0}, {"function": "def main():\n    args = parse_args()\n    _predict(\n        model_uri=args.model_uri,\n        input_path=args.input_path if args.input_path else None,\n        output_path=args.output_path if args.output_path else None,\n        content_type=args.content_type,\n    )", "target": 0}]}, {"raw_url": "https://github.com/mlflow/mlflow/raw/6dde93758d42455cb90ef324407919ed67668b9b/mlflow%2Fpyfunc%2Fbackend.py", "code": "import logging\nimport os\nimport pathlib\nimport subprocess\nimport posixpath\nimport shlex\nimport sys\nimport warnings\nimport ctypes\nimport signal\nfrom pathlib import Path\n\nfrom mlflow.models import FlavorBackend\nfrom mlflow.models.docker_utils import (\n    _build_image,\n    _generate_dockerfile_content,\n    DISABLE_ENV_CREATION,\n    SETUP_MINICONDA,\n    SETUP_PYENV_AND_VIRTUALENV,\n    _get_mlflow_install_step,\n)\nfrom mlflow.models.container import ENABLE_MLSERVER\nfrom mlflow.pyfunc import ENV, scoring_server, mlserver, _extract_conda_env\n\nfrom mlflow.utils.conda import get_or_create_conda_env, get_conda_bin_executable\nfrom mlflow.tracking.artifact_utils import _download_artifact_from_uri\nfrom mlflow.utils import env_manager as _EnvManager\nfrom mlflow.pyfunc import _mlflow_pyfunc_backend_predict\nfrom mlflow.utils.file_utils import (\n    path_to_local_file_uri,\n    get_or_create_tmp_dir,\n    get_or_create_nfs_tmp_dir,\n)\nfrom mlflow.utils.environment import Environment\nfrom mlflow.utils.virtualenv import (\n    _get_or_create_virtualenv,\n    _get_pip_install_mlflow,\n)\nfrom mlflow.utils.nfs_on_spark import get_nfs_cache_root_dir\nfrom mlflow.utils.process import cache_return_value_per_process\nfrom mlflow.version import VERSION\n\n_logger = logging.getLogger(__name__)\n\n_IS_UNIX = os.name != \"nt\"\n_STDIN_SERVER_SCRIPT = Path(__file__).parent.joinpath(\"stdin_server.py\")\n\n\nclass PyFuncBackend(FlavorBackend):\n    \"\"\"\n    Flavor backend implementation for the generic python models.\n    \"\"\"\n\n    def __init__(\n        self,\n        config,\n        workers=1,\n        env_manager=_EnvManager.VIRTUALENV,\n        install_mlflow=False,\n        create_env_root_dir=False,\n        env_root_dir=None,\n        **kwargs,\n    ):\n        \"\"\"\n        :param env_root_dir: Root path for conda env. If None, use Conda's default environments\n                             directory. Note if this is set, conda package cache path becomes\n                             \"{env_root_dir}/conda_cache_pkgs\" instead of the global package cache\n                             path, and pip package cache path becomes\n                             \"{env_root_dir}/pip_cache_pkgs\" instead of the global package cache\n                             path.\n        \"\"\"\n        super().__init__(config=config, **kwargs)\n        self._nworkers = workers or 1\n        if env_manager == _EnvManager.CONDA and ENV not in config:\n            env_manager = _EnvManager.LOCAL\n        self._env_manager = env_manager\n        self._install_mlflow = install_mlflow\n        self._env_id = os.environ.get(\"MLFLOW_HOME\", VERSION) if install_mlflow else None\n        self._create_env_root_dir = create_env_root_dir\n        self._env_root_dir = env_root_dir\n        self._environment = None\n\n    def prepare_env(self, model_uri, capture_output=False):\n        if self._environment is not None:\n            return self._environment\n\n        @cache_return_value_per_process\n        def _get_or_create_env_root_dir(should_use_nfs):\n            if should_use_nfs:\n                root_tmp_dir = get_or_create_nfs_tmp_dir()\n            else:\n                root_tmp_dir = get_or_create_tmp_dir()\n\n            env_root_dir = os.path.join(root_tmp_dir, \"envs\")\n            os.makedirs(env_root_dir, exist_ok=True)\n            return env_root_dir\n\n        local_path = _download_artifact_from_uri(model_uri)\n        if self._create_env_root_dir:\n            if self._env_root_dir is not None:\n                raise Exception(\"env_root_dir can not be set when create_env_root_dir=True\")\n            nfs_root_dir = get_nfs_cache_root_dir()\n            env_root_dir = _get_or_create_env_root_dir(nfs_root_dir is not None)\n        else:\n            env_root_dir = self._env_root_dir\n\n        if self._env_manager == _EnvManager.VIRTUALENV:\n            activate_cmd = _get_or_create_virtualenv(\n                local_path,\n                self._env_id,\n                env_root_dir=env_root_dir,\n                capture_output=capture_output,\n            )\n            self._environment = Environment(activate_cmd)\n        elif self._env_manager == _EnvManager.CONDA:\n            conda_env_path = os.path.join(local_path, _extract_conda_env(self._config[ENV]))\n            self._environment = get_or_create_conda_env(\n                conda_env_path,\n                env_id=self._env_id,\n                capture_output=capture_output,\n                env_root_dir=env_root_dir,\n            )\n\n        elif self._env_manager == _EnvManager.LOCAL:\n            raise Exception(\"Prepare env should not be called with local env manager!\")\n        else:\n            raise Exception(f\"Unexpected env manager value '{self._env_manager}'\")\n\n        if self._install_mlflow:\n            self._environment.execute(_get_pip_install_mlflow())\n        else:\n            self._environment.execute('python -c \"\"')\n\n        return self._environment\n\n    def predict(self, model_uri, input_path, output_path, content_type):\n        \"\"\"\n        Generate predictions using generic python model saved with MLflow. The expected format of\n        the input JSON is the Mlflow scoring format.\n        Return the prediction results as a JSON.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n        # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure\n        # platform compatibility.\n        local_uri = path_to_local_file_uri(local_path)\n\n        if self._env_manager != _EnvManager.LOCAL:\n            predict_cmd = [\n                \"python\",\n                _mlflow_pyfunc_backend_predict.__file__,\n                \"--model-uri\",\n                str(local_uri),\n                \"--content-type\",\n                shlex.quote(str(content_type)),\n            ]\n            if input_path:\n                predict_cmd += [\"--input-path\", shlex.quote(str(input_path))]\n            if output_path:\n                predict_cmd += [\"--output-path\", shlex.quote(str(output_path))]\n            return self.prepare_env(local_path).execute(\" \".join(predict_cmd))\n        else:\n            scoring_server._predict(local_uri, input_path, output_path, content_type)\n\n    def serve(\n        self,\n        model_uri,\n        port,\n        host,\n        timeout,\n        enable_mlserver,\n        synchronous=True,\n        stdout=None,\n        stderr=None,\n    ):\n        \"\"\"\n        Serve pyfunc model locally.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n\n        server_implementation = mlserver if enable_mlserver else scoring_server\n        command, command_env = server_implementation.get_cmd(\n            local_path, port, host, timeout, self._nworkers\n        )\n\n        if sys.platform.startswith(\"linux\"):\n\n            def setup_sigterm_on_parent_death():\n                \"\"\"\n                Uses prctl to automatically send SIGTERM to the command process when its parent is\n                dead.\n\n                This handles the case when the parent is a PySpark worker process.\n                If a user cancels the PySpark job, the worker process gets killed, regardless of\n                PySpark daemon and worker reuse settings.\n                We use prctl to ensure the command process receives SIGTERM after spark job\n                cancellation.\n                The command process itself should handle SIGTERM properly.\n                This is a no-op on macOS because prctl is not supported.\n\n                Note:\n                When a pyspark job canceled, the UDF python process are killed by signal \"SIGKILL\",\n                This case neither \"atexit\" nor signal handler can capture SIGKILL signal.\n                prctl is the only way to capture SIGKILL signal.\n                \"\"\"\n                try:\n                    libc = ctypes.CDLL(\"libc.so.6\")\n                    # Set the parent process death signal of the command process to SIGTERM.\n                    libc.prctl(1, signal.SIGTERM)  # PR_SET_PDEATHSIG, see prctl.h\n                except OSError as e:\n                    # TODO: find approach for supporting MacOS/Windows system which does\n                    #  not support prctl.\n                    warnings.warn(f\"Setup libc.prctl PR_SET_PDEATHSIG failed, error {e!r}.\")\n\n        else:\n            setup_sigterm_on_parent_death = None\n\n        if _IS_UNIX:\n            # Add \"exec\" before the starting scoring server command, so that the scoring server\n            # process replaces the bash process, otherwise the scoring server process is created\n            # as a child process of the bash process.\n            # Note we in `mlflow.pyfunc.spark_udf`, use prctl PR_SET_PDEATHSIG to ensure scoring\n            # server process being killed when UDF process exit. The PR_SET_PDEATHSIG can only\n            # send signal to the bash process, if the scoring server process is created as a\n            # child process of the bash process, then it cannot receive the signal sent by prctl.\n            # TODO: For Windows, there's no equivalent things of Unix shell's exec. Windows also\n            #  does not support prctl. We need to find an approach to address it.\n            command = \"exec \" + command\n\n        if self._env_manager != _EnvManager.LOCAL:\n            return self.prepare_env(local_path).execute(\n                command,\n                command_env,\n                stdout=stdout,\n                stderr=stderr,\n                preexec_fn=setup_sigterm_on_parent_death,\n                synchronous=synchronous,\n            )\n        else:\n            _logger.info(\"=== Running command '%s'\", command)\n\n            if os.name != \"nt\":\n                command = [\"bash\", \"-c\", command]\n\n            child_proc = subprocess.Popen(\n                command,\n                env=command_env,\n                preexec_fn=setup_sigterm_on_parent_death,\n                stdout=stdout,\n                stderr=stderr,\n            )\n\n            if synchronous:\n                rc = child_proc.wait()\n                if rc != 0:\n                    raise Exception(\n                        f\"Command '{command}' returned non zero return code. Return code = {rc}\"\n                    )\n                return 0\n            else:\n                return child_proc\n\n    def serve_stdin(\n        self,\n        model_uri,\n        stdout=None,\n        stderr=None,\n    ):\n        local_path = _download_artifact_from_uri(model_uri)\n        return self.prepare_env(local_path).execute(\n            command=f\"python {_STDIN_SERVER_SCRIPT} --model-uri {local_path}\",\n            stdin=subprocess.PIPE,\n            stdout=stdout,\n            stderr=stderr,\n            synchronous=False,\n        )\n\n    def can_score_model(self):\n        if self._env_manager == _EnvManager.LOCAL:\n            # noconda => already in python and dependencies are assumed to be installed.\n            return True\n        conda_path = get_conda_bin_executable(\"conda\")\n        try:\n            p = subprocess.Popen(\n                [conda_path, \"--version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n            )\n            _, _ = p.communicate()\n            return p.wait() == 0\n        except FileNotFoundError:\n            # Can not find conda\n            return False\n\n    def generate_dockerfile(\n        self,\n        model_uri,\n        output_path=\"mlflow-dockerfile\",\n        install_mlflow=False,\n        mlflow_home=None,\n        enable_mlserver=False,\n    ):\n        copy_model_into_container = self.copy_model_into_container_wrapper(\n            model_uri, install_mlflow, enable_mlserver\n        )\n        pyfunc_entrypoint = _pyfunc_entrypoint(\n            self._env_manager, model_uri, install_mlflow, enable_mlserver\n        )\n\n        mlflow_home = os.path.abspath(mlflow_home) if mlflow_home else None\n\n        is_conda = self._env_manager == _EnvManager.CONDA\n        setup_miniconda = \"\"\n        setup_pyenv_and_virtualenv = \"\"\n\n        if is_conda:\n            setup_miniconda = SETUP_MINICONDA\n        else:\n            setup_pyenv_and_virtualenv = SETUP_PYENV_AND_VIRTUALENV\n\n        os.makedirs(output_path, exist_ok=True)\n\n        _logger.debug(\"Created all folders in path\", extra={\"output_directory\": output_path})\n        install_mlflow = _get_mlflow_install_step(output_path, mlflow_home)\n\n        custom_setup_steps = copy_model_into_container(output_path)\n\n        dockerfile_text = _generate_dockerfile_content(\n            setup_miniconda=setup_miniconda,\n            setup_pyenv_and_virtualenv=setup_pyenv_and_virtualenv,\n            install_mlflow=install_mlflow,\n            custom_setup_steps=custom_setup_steps,\n            entrypoint=pyfunc_entrypoint,\n        )\n        _logger.debug(\"generated dockerfile text\", extra={\"dockerfile\": dockerfile_text})\n\n        with open(os.path.join(output_path, \"Dockerfile\"), \"w\") as dockerfile:\n            dockerfile.write(dockerfile_text)\n\n    def build_image(\n        self, model_uri, image_name, install_mlflow=False, mlflow_home=None, enable_mlserver=False\n    ):\n        copy_model_into_container = self.copy_model_into_container_wrapper(\n            model_uri, install_mlflow, enable_mlserver\n        )\n        pyfunc_entrypoint = _pyfunc_entrypoint(\n            self._env_manager, model_uri, install_mlflow, enable_mlserver\n        )\n        _build_image(\n            image_name=image_name,\n            mlflow_home=mlflow_home,\n            env_manager=self._env_manager,\n            custom_setup_steps_hook=copy_model_into_container,\n            entrypoint=pyfunc_entrypoint,\n        )\n\n    def copy_model_into_container_wrapper(self, model_uri, install_mlflow, enable_mlserver):\n        def copy_model_into_container(dockerfile_context_dir):\n            # This function have to be included in another,\n            # since `_build_image` function in `docker_utils` accepts only\n            # single-argument function like this\n            model_cwd = os.path.join(dockerfile_context_dir, \"model_dir\")\n            pathlib.Path(model_cwd).mkdir(parents=True, exist_ok=True)\n            if model_uri:\n                model_path = _download_artifact_from_uri(model_uri, output_path=model_cwd)\n                return \"\"\"\n                    COPY {model_dir} /opt/ml/model\n                    RUN python -c \\\n                    'from mlflow.models.container import _install_pyfunc_deps;\\\n                    _install_pyfunc_deps(\\\n                        \"/opt/ml/model\", \\\n                        install_mlflow={install_mlflow}, \\\n                        enable_mlserver={enable_mlserver}, \\\n                        env_manager=\"{env_manager}\")'\n                    ENV {disable_env}=\"true\"\n                    ENV {ENABLE_MLSERVER}={enable_mlserver}\n                    \"\"\".format(\n                    disable_env=DISABLE_ENV_CREATION,\n                    model_dir=str(posixpath.join(\"model_dir\", os.path.basename(model_path))),\n                    install_mlflow=repr(install_mlflow),\n                    ENABLE_MLSERVER=ENABLE_MLSERVER,\n                    enable_mlserver=repr(enable_mlserver),\n                    env_manager=self._env_manager,\n                )\n            else:\n                return \"\"\"\n                    ENV {disable_env}=\"true\"\n                    ENV {ENABLE_MLSERVER}={enable_mlserver}\n                    \"\"\".format(\n                    disable_env=DISABLE_ENV_CREATION,\n                    ENABLE_MLSERVER=ENABLE_MLSERVER,\n                    enable_mlserver=repr(enable_mlserver),\n                )\n\n        return copy_model_into_container\n\n\ndef _pyfunc_entrypoint(env_manager, model_uri, install_mlflow, enable_mlserver):\n    if model_uri:\n        # The pyfunc image runs the same server as the Sagemaker image\n        pyfunc_entrypoint = (\n            'ENTRYPOINT [\"python\", \"-c\", \"from mlflow.models import container as C;'\n            f'C._serve({env_manager!r})\"]'\n        )\n    else:\n        entrypoint_code = \"; \".join(\n            [\n                \"from mlflow.models import container as C\",\n                \"from mlflow.models.container import _install_pyfunc_deps\",\n                (\n                    \"_install_pyfunc_deps(\"\n                    + '\"/opt/ml/model\", '\n                    + f\"install_mlflow={install_mlflow}, \"\n                    + f\"enable_mlserver={enable_mlserver}, \"\n                    + f'env_manager=\"{env_manager}\"'\n                    + \")\"\n                ),\n                f'C._serve(\"{env_manager}\")',\n            ]\n        )\n        pyfunc_entrypoint = 'ENTRYPOINT [\"python\", \"-c\", \"{entrypoint_code}\"]'.format(\n            entrypoint_code=entrypoint_code.replace('\"', '\\\\\"')\n        )\n\n    return pyfunc_entrypoint\n", "code_before": "import logging\nimport os\nimport pathlib\nimport subprocess\nimport posixpath\nimport sys\nimport warnings\nimport ctypes\nimport signal\nfrom pathlib import Path\n\nfrom mlflow.models import FlavorBackend\nfrom mlflow.models.docker_utils import (\n    _build_image,\n    _generate_dockerfile_content,\n    DISABLE_ENV_CREATION,\n    SETUP_MINICONDA,\n    SETUP_PYENV_AND_VIRTUALENV,\n    _get_mlflow_install_step,\n)\nfrom mlflow.models.container import ENABLE_MLSERVER\nfrom mlflow.pyfunc import ENV, scoring_server, mlserver, _extract_conda_env\n\nfrom mlflow.utils.conda import get_or_create_conda_env, get_conda_bin_executable\nfrom mlflow.tracking.artifact_utils import _download_artifact_from_uri\nfrom mlflow.utils import env_manager as _EnvManager\nfrom mlflow.utils.file_utils import (\n    path_to_local_file_uri,\n    get_or_create_tmp_dir,\n    get_or_create_nfs_tmp_dir,\n)\nfrom mlflow.utils.environment import Environment\nfrom mlflow.utils.virtualenv import (\n    _get_or_create_virtualenv,\n    _get_pip_install_mlflow,\n)\nfrom mlflow.utils.nfs_on_spark import get_nfs_cache_root_dir\nfrom mlflow.utils.process import cache_return_value_per_process\nfrom mlflow.version import VERSION\n\n_logger = logging.getLogger(__name__)\n\n_IS_UNIX = os.name != \"nt\"\n_STDIN_SERVER_SCRIPT = Path(__file__).parent.joinpath(\"stdin_server.py\")\n\n\nclass PyFuncBackend(FlavorBackend):\n    \"\"\"\n    Flavor backend implementation for the generic python models.\n    \"\"\"\n\n    def __init__(\n        self,\n        config,\n        workers=1,\n        env_manager=_EnvManager.VIRTUALENV,\n        install_mlflow=False,\n        create_env_root_dir=False,\n        env_root_dir=None,\n        **kwargs,\n    ):\n        \"\"\"\n        :param env_root_dir: Root path for conda env. If None, use Conda's default environments\n                             directory. Note if this is set, conda package cache path becomes\n                             \"{env_root_dir}/conda_cache_pkgs\" instead of the global package cache\n                             path, and pip package cache path becomes\n                             \"{env_root_dir}/pip_cache_pkgs\" instead of the global package cache\n                             path.\n        \"\"\"\n        super().__init__(config=config, **kwargs)\n        self._nworkers = workers or 1\n        if env_manager == _EnvManager.CONDA and ENV not in config:\n            env_manager = _EnvManager.LOCAL\n        self._env_manager = env_manager\n        self._install_mlflow = install_mlflow\n        self._env_id = os.environ.get(\"MLFLOW_HOME\", VERSION) if install_mlflow else None\n        self._create_env_root_dir = create_env_root_dir\n        self._env_root_dir = env_root_dir\n        self._environment = None\n\n    def prepare_env(self, model_uri, capture_output=False):\n        if self._environment is not None:\n            return self._environment\n\n        @cache_return_value_per_process\n        def _get_or_create_env_root_dir(should_use_nfs):\n            if should_use_nfs:\n                root_tmp_dir = get_or_create_nfs_tmp_dir()\n            else:\n                root_tmp_dir = get_or_create_tmp_dir()\n\n            env_root_dir = os.path.join(root_tmp_dir, \"envs\")\n            os.makedirs(env_root_dir, exist_ok=True)\n            return env_root_dir\n\n        local_path = _download_artifact_from_uri(model_uri)\n        if self._create_env_root_dir:\n            if self._env_root_dir is not None:\n                raise Exception(\"env_root_dir can not be set when create_env_root_dir=True\")\n            nfs_root_dir = get_nfs_cache_root_dir()\n            env_root_dir = _get_or_create_env_root_dir(nfs_root_dir is not None)\n        else:\n            env_root_dir = self._env_root_dir\n\n        if self._env_manager == _EnvManager.VIRTUALENV:\n            activate_cmd = _get_or_create_virtualenv(\n                local_path,\n                self._env_id,\n                env_root_dir=env_root_dir,\n                capture_output=capture_output,\n            )\n            self._environment = Environment(activate_cmd)\n        elif self._env_manager == _EnvManager.CONDA:\n            conda_env_path = os.path.join(local_path, _extract_conda_env(self._config[ENV]))\n            self._environment = get_or_create_conda_env(\n                conda_env_path,\n                env_id=self._env_id,\n                capture_output=capture_output,\n                env_root_dir=env_root_dir,\n            )\n\n        elif self._env_manager == _EnvManager.LOCAL:\n            raise Exception(\"Prepare env should not be called with local env manager!\")\n        else:\n            raise Exception(f\"Unexpected env manager value '{self._env_manager}'\")\n\n        if self._install_mlflow:\n            self._environment.execute(_get_pip_install_mlflow())\n        else:\n            self._environment.execute('python -c \"\"')\n\n        return self._environment\n\n    def predict(self, model_uri, input_path, output_path, content_type):\n        \"\"\"\n        Generate predictions using generic python model saved with MLflow. The expected format of\n        the input JSON is the Mlflow scoring format.\n        Return the prediction results as a JSON.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n        # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure\n        # platform compatibility.\n        local_uri = path_to_local_file_uri(local_path)\n\n        if self._env_manager != _EnvManager.LOCAL:\n            command = (\n                'python -c \"from mlflow.pyfunc.scoring_server import _predict; _predict('\n                \"model_uri={model_uri}, \"\n                \"input_path={input_path}, \"\n                \"output_path={output_path}, \"\n                \"content_type={content_type})\"\n                '\"'\n            ).format(\n                model_uri=repr(local_uri),\n                input_path=repr(input_path),\n                output_path=repr(output_path),\n                content_type=repr(content_type),\n            )\n            return self.prepare_env(local_path).execute(command)\n        else:\n            scoring_server._predict(local_uri, input_path, output_path, content_type)\n\n    def serve(\n        self,\n        model_uri,\n        port,\n        host,\n        timeout,\n        enable_mlserver,\n        synchronous=True,\n        stdout=None,\n        stderr=None,\n    ):\n        \"\"\"\n        Serve pyfunc model locally.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n\n        server_implementation = mlserver if enable_mlserver else scoring_server\n        command, command_env = server_implementation.get_cmd(\n            local_path, port, host, timeout, self._nworkers\n        )\n\n        if sys.platform.startswith(\"linux\"):\n\n            def setup_sigterm_on_parent_death():\n                \"\"\"\n                Uses prctl to automatically send SIGTERM to the command process when its parent is\n                dead.\n\n                This handles the case when the parent is a PySpark worker process.\n                If a user cancels the PySpark job, the worker process gets killed, regardless of\n                PySpark daemon and worker reuse settings.\n                We use prctl to ensure the command process receives SIGTERM after spark job\n                cancellation.\n                The command process itself should handle SIGTERM properly.\n                This is a no-op on macOS because prctl is not supported.\n\n                Note:\n                When a pyspark job canceled, the UDF python process are killed by signal \"SIGKILL\",\n                This case neither \"atexit\" nor signal handler can capture SIGKILL signal.\n                prctl is the only way to capture SIGKILL signal.\n                \"\"\"\n                try:\n                    libc = ctypes.CDLL(\"libc.so.6\")\n                    # Set the parent process death signal of the command process to SIGTERM.\n                    libc.prctl(1, signal.SIGTERM)  # PR_SET_PDEATHSIG, see prctl.h\n                except OSError as e:\n                    # TODO: find approach for supporting MacOS/Windows system which does\n                    #  not support prctl.\n                    warnings.warn(f\"Setup libc.prctl PR_SET_PDEATHSIG failed, error {e!r}.\")\n\n        else:\n            setup_sigterm_on_parent_death = None\n\n        if _IS_UNIX:\n            # Add \"exec\" before the starting scoring server command, so that the scoring server\n            # process replaces the bash process, otherwise the scoring server process is created\n            # as a child process of the bash process.\n            # Note we in `mlflow.pyfunc.spark_udf`, use prctl PR_SET_PDEATHSIG to ensure scoring\n            # server process being killed when UDF process exit. The PR_SET_PDEATHSIG can only\n            # send signal to the bash process, if the scoring server process is created as a\n            # child process of the bash process, then it cannot receive the signal sent by prctl.\n            # TODO: For Windows, there's no equivalent things of Unix shell's exec. Windows also\n            #  does not support prctl. We need to find an approach to address it.\n            command = \"exec \" + command\n\n        if self._env_manager != _EnvManager.LOCAL:\n            return self.prepare_env(local_path).execute(\n                command,\n                command_env,\n                stdout=stdout,\n                stderr=stderr,\n                preexec_fn=setup_sigterm_on_parent_death,\n                synchronous=synchronous,\n            )\n        else:\n            _logger.info(\"=== Running command '%s'\", command)\n\n            if os.name != \"nt\":\n                command = [\"bash\", \"-c\", command]\n\n            child_proc = subprocess.Popen(\n                command,\n                env=command_env,\n                preexec_fn=setup_sigterm_on_parent_death,\n                stdout=stdout,\n                stderr=stderr,\n            )\n\n            if synchronous:\n                rc = child_proc.wait()\n                if rc != 0:\n                    raise Exception(\n                        f\"Command '{command}' returned non zero return code. Return code = {rc}\"\n                    )\n                return 0\n            else:\n                return child_proc\n\n    def serve_stdin(\n        self,\n        model_uri,\n        stdout=None,\n        stderr=None,\n    ):\n        local_path = _download_artifact_from_uri(model_uri)\n        return self.prepare_env(local_path).execute(\n            command=f\"python {_STDIN_SERVER_SCRIPT} --model-uri {local_path}\",\n            stdin=subprocess.PIPE,\n            stdout=stdout,\n            stderr=stderr,\n            synchronous=False,\n        )\n\n    def can_score_model(self):\n        if self._env_manager == _EnvManager.LOCAL:\n            # noconda => already in python and dependencies are assumed to be installed.\n            return True\n        conda_path = get_conda_bin_executable(\"conda\")\n        try:\n            p = subprocess.Popen(\n                [conda_path, \"--version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n            )\n            _, _ = p.communicate()\n            return p.wait() == 0\n        except FileNotFoundError:\n            # Can not find conda\n            return False\n\n    def generate_dockerfile(\n        self,\n        model_uri,\n        output_path=\"mlflow-dockerfile\",\n        install_mlflow=False,\n        mlflow_home=None,\n        enable_mlserver=False,\n    ):\n        copy_model_into_container = self.copy_model_into_container_wrapper(\n            model_uri, install_mlflow, enable_mlserver\n        )\n        pyfunc_entrypoint = _pyfunc_entrypoint(\n            self._env_manager, model_uri, install_mlflow, enable_mlserver\n        )\n\n        mlflow_home = os.path.abspath(mlflow_home) if mlflow_home else None\n\n        is_conda = self._env_manager == _EnvManager.CONDA\n        setup_miniconda = \"\"\n        setup_pyenv_and_virtualenv = \"\"\n\n        if is_conda:\n            setup_miniconda = SETUP_MINICONDA\n        else:\n            setup_pyenv_and_virtualenv = SETUP_PYENV_AND_VIRTUALENV\n\n        os.makedirs(output_path, exist_ok=True)\n\n        _logger.debug(\"Created all folders in path\", extra={\"output_directory\": output_path})\n        install_mlflow = _get_mlflow_install_step(output_path, mlflow_home)\n\n        custom_setup_steps = copy_model_into_container(output_path)\n\n        dockerfile_text = _generate_dockerfile_content(\n            setup_miniconda=setup_miniconda,\n            setup_pyenv_and_virtualenv=setup_pyenv_and_virtualenv,\n            install_mlflow=install_mlflow,\n            custom_setup_steps=custom_setup_steps,\n            entrypoint=pyfunc_entrypoint,\n        )\n        _logger.debug(\"generated dockerfile text\", extra={\"dockerfile\": dockerfile_text})\n\n        with open(os.path.join(output_path, \"Dockerfile\"), \"w\") as dockerfile:\n            dockerfile.write(dockerfile_text)\n\n    def build_image(\n        self, model_uri, image_name, install_mlflow=False, mlflow_home=None, enable_mlserver=False\n    ):\n        copy_model_into_container = self.copy_model_into_container_wrapper(\n            model_uri, install_mlflow, enable_mlserver\n        )\n        pyfunc_entrypoint = _pyfunc_entrypoint(\n            self._env_manager, model_uri, install_mlflow, enable_mlserver\n        )\n        _build_image(\n            image_name=image_name,\n            mlflow_home=mlflow_home,\n            env_manager=self._env_manager,\n            custom_setup_steps_hook=copy_model_into_container,\n            entrypoint=pyfunc_entrypoint,\n        )\n\n    def copy_model_into_container_wrapper(self, model_uri, install_mlflow, enable_mlserver):\n        def copy_model_into_container(dockerfile_context_dir):\n            # This function have to be included in another,\n            # since `_build_image` function in `docker_utils` accepts only\n            # single-argument function like this\n            model_cwd = os.path.join(dockerfile_context_dir, \"model_dir\")\n            pathlib.Path(model_cwd).mkdir(parents=True, exist_ok=True)\n            if model_uri:\n                model_path = _download_artifact_from_uri(model_uri, output_path=model_cwd)\n                return \"\"\"\n                    COPY {model_dir} /opt/ml/model\n                    RUN python -c \\\n                    'from mlflow.models.container import _install_pyfunc_deps;\\\n                    _install_pyfunc_deps(\\\n                        \"/opt/ml/model\", \\\n                        install_mlflow={install_mlflow}, \\\n                        enable_mlserver={enable_mlserver}, \\\n                        env_manager=\"{env_manager}\")'\n                    ENV {disable_env}=\"true\"\n                    ENV {ENABLE_MLSERVER}={enable_mlserver}\n                    \"\"\".format(\n                    disable_env=DISABLE_ENV_CREATION,\n                    model_dir=str(posixpath.join(\"model_dir\", os.path.basename(model_path))),\n                    install_mlflow=repr(install_mlflow),\n                    ENABLE_MLSERVER=ENABLE_MLSERVER,\n                    enable_mlserver=repr(enable_mlserver),\n                    env_manager=self._env_manager,\n                )\n            else:\n                return \"\"\"\n                    ENV {disable_env}=\"true\"\n                    ENV {ENABLE_MLSERVER}={enable_mlserver}\n                    \"\"\".format(\n                    disable_env=DISABLE_ENV_CREATION,\n                    ENABLE_MLSERVER=ENABLE_MLSERVER,\n                    enable_mlserver=repr(enable_mlserver),\n                )\n\n        return copy_model_into_container\n\n\ndef _pyfunc_entrypoint(env_manager, model_uri, install_mlflow, enable_mlserver):\n    if model_uri:\n        # The pyfunc image runs the same server as the Sagemaker image\n        pyfunc_entrypoint = (\n            'ENTRYPOINT [\"python\", \"-c\", \"from mlflow.models import container as C;'\n            f'C._serve({env_manager!r})\"]'\n        )\n    else:\n        entrypoint_code = \"; \".join(\n            [\n                \"from mlflow.models import container as C\",\n                \"from mlflow.models.container import _install_pyfunc_deps\",\n                (\n                    \"_install_pyfunc_deps(\"\n                    + '\"/opt/ml/model\", '\n                    + f\"install_mlflow={install_mlflow}, \"\n                    + f\"enable_mlserver={enable_mlserver}, \"\n                    + f'env_manager=\"{env_manager}\"'\n                    + \")\"\n                ),\n                f'C._serve(\"{env_manager}\")',\n            ]\n        )\n        pyfunc_entrypoint = 'ENTRYPOINT [\"python\", \"-c\", \"{entrypoint_code}\"]'.format(\n            entrypoint_code=entrypoint_code.replace('\"', '\\\\\"')\n        )\n\n    return pyfunc_entrypoint\n", "patch": "@@ -3,6 +3,7 @@\n import pathlib\n import subprocess\n import posixpath\n+import shlex\n import sys\n import warnings\n import ctypes\n@@ -24,6 +25,7 @@\n from mlflow.utils.conda import get_or_create_conda_env, get_conda_bin_executable\n from mlflow.tracking.artifact_utils import _download_artifact_from_uri\n from mlflow.utils import env_manager as _EnvManager\n+from mlflow.pyfunc import _mlflow_pyfunc_backend_predict\n from mlflow.utils.file_utils import (\n     path_to_local_file_uri,\n     get_or_create_tmp_dir,\n@@ -143,20 +145,19 @@ def predict(self, model_uri, input_path, output_path, content_type):\n         local_uri = path_to_local_file_uri(local_path)\n \n         if self._env_manager != _EnvManager.LOCAL:\n-            command = (\n-                'python -c \"from mlflow.pyfunc.scoring_server import _predict; _predict('\n-                \"model_uri={model_uri}, \"\n-                \"input_path={input_path}, \"\n-                \"output_path={output_path}, \"\n-                \"content_type={content_type})\"\n-                '\"'\n-            ).format(\n-                model_uri=repr(local_uri),\n-                input_path=repr(input_path),\n-                output_path=repr(output_path),\n-                content_type=repr(content_type),\n-            )\n-            return self.prepare_env(local_path).execute(command)\n+            predict_cmd = [\n+                \"python\",\n+                _mlflow_pyfunc_backend_predict.__file__,\n+                \"--model-uri\",\n+                str(local_uri),\n+                \"--content-type\",\n+                shlex.quote(str(content_type)),\n+            ]\n+            if input_path:\n+                predict_cmd += [\"--input-path\", shlex.quote(str(input_path))]\n+            if output_path:\n+                predict_cmd += [\"--output-path\", shlex.quote(str(output_path))]\n+            return self.prepare_env(local_path).execute(\" \".join(predict_cmd))\n         else:\n             scoring_server._predict(local_uri, input_path, output_path, content_type)\n ", "file_path": "files/2023_7/2", "file_language": "py", "file_name": "mlflow/pyfunc/backend.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 1, "static": {"rats": [false, []], "semgrep": [true, ["       python.sqlalchemy.security.sqlalchemy-execute-raw-query.sqlalchemy-execute-raw-query          \n          Avoiding SQL string concatenation: untrusted input concatenated with raw SQL query can     \n          result in SQL Injection. In order to execute raw query safely, prepared statement should be\n          used. SQLAlchemy provides TextualSQL to easily used prepared statement with named          \n          parameters. For complex SQL composition, use SQL Expression Language or Schema Definition  \n          Language. In most cases, SQLAlchemy ORM will be a better option.                           \n          Details: https://sg.run/2b1L                                                               \n          159\u2506 return self.prepare_env(local_path).execute(command)", "       python.lang.security.audit.formatted-sql-query.formatted-sql-query          \n          Detected possible formatted SQL query. Use parameterized queries instead.\n          Details: https://sg.run/EkWw                                             \n\n          159\u2506 return self.prepare_env(local_path).execute(command)"]]}, "target": 1, "function_before": [{"function": "class PyFuncBackend(FlavorBackend):\n    \"\"\"\n    Flavor backend implementation for the generic python models.\n    \"\"\"\n\n    def __init__(\n        self,\n        config,\n        workers=1,\n        env_manager=_EnvManager.VIRTUALENV,\n        install_mlflow=False,\n        create_env_root_dir=False,\n        env_root_dir=None,\n        **kwargs,\n    ):\n        \"\"\"\n        :param env_root_dir: Root path for conda env. If None, use Conda's default environments\n                             directory. Note if this is set, conda package cache path becomes\n                             \"{env_root_dir}/conda_cache_pkgs\" instead of the global package cache\n                             path, and pip package cache path becomes\n                             \"{env_root_dir}/pip_cache_pkgs\" instead of the global package cache\n                             path.\n        \"\"\"\n        super().__init__(config=config, **kwargs)\n        self._nworkers = workers or 1\n        if env_manager == _EnvManager.CONDA and ENV not in config:\n            env_manager = _EnvManager.LOCAL\n        self._env_manager = env_manager\n        self._install_mlflow = install_mlflow\n        self._env_id = os.environ.get(\"MLFLOW_HOME\", VERSION) if install_mlflow else None\n        self._create_env_root_dir = create_env_root_dir\n        self._env_root_dir = env_root_dir\n        self._environment = None\n\n    def prepare_env(self, model_uri, capture_output=False):\n        if self._environment is not None:\n            return self._environment\n\n        @cache_return_value_per_process\n        def _get_or_create_env_root_dir(should_use_nfs):\n            if should_use_nfs:\n                root_tmp_dir = get_or_create_nfs_tmp_dir()\n            else:\n                root_tmp_dir = get_or_create_tmp_dir()\n\n            env_root_dir = os.path.join(root_tmp_dir, \"envs\")\n            os.makedirs(env_root_dir, exist_ok=True)\n            return env_root_dir\n\n        local_path = _download_artifact_from_uri(model_uri)\n        if self._create_env_root_dir:\n            if self._env_root_dir is not None:\n                raise Exception(\"env_root_dir can not be set when create_env_root_dir=True\")\n            nfs_root_dir = get_nfs_cache_root_dir()\n            env_root_dir = _get_or_create_env_root_dir(nfs_root_dir is not None)\n        else:\n            env_root_dir = self._env_root_dir\n\n        if self._env_manager == _EnvManager.VIRTUALENV:\n            activate_cmd = _get_or_create_virtualenv(\n                local_path,\n                self._env_id,\n                env_root_dir=env_root_dir,\n                capture_output=capture_output,\n            )\n            self._environment = Environment(activate_cmd)\n        elif self._env_manager == _EnvManager.CONDA:\n            conda_env_path = os.path.join(local_path, _extract_conda_env(self._config[ENV]))\n            self._environment = get_or_create_conda_env(\n                conda_env_path,\n                env_id=self._env_id,\n                capture_output=capture_output,\n                env_root_dir=env_root_dir,\n            )\n\n        elif self._env_manager == _EnvManager.LOCAL:\n            raise Exception(\"Prepare env should not be called with local env manager!\")\n        else:\n            raise Exception(f\"Unexpected env manager value '{self._env_manager}'\")\n\n        if self._install_mlflow:\n            self._environment.execute(_get_pip_install_mlflow())\n        else:\n            self._environment.execute('python -c \"\"')\n\n        return self._environment\n\n    def predict(self, model_uri, input_path, output_path, content_type):\n        \"\"\"\n        Generate predictions using generic python model saved with MLflow. The expected format of\n        the input JSON is the Mlflow scoring format.\n        Return the prediction results as a JSON.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n        # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure\n        # platform compatibility.\n        local_uri = path_to_local_file_uri(local_path)\n\n        if self._env_manager != _EnvManager.LOCAL:\n            command = (\n                'python -c \"from mlflow.pyfunc.scoring_server import _predict; _predict('\n                \"model_uri={model_uri}, \"\n                \"input_path={input_path}, \"\n                \"output_path={output_path}, \"\n                \"content_type={content_type})\"\n                '\"'\n            ).format(\n                model_uri=repr(local_uri),\n                input_path=repr(input_path),\n                output_path=repr(output_path),\n                content_type=repr(content_type),\n            )\n            return self.prepare_env(local_path).execute(command)\n        else:\n            scoring_server._predict(local_uri, input_path, output_path, content_type)\n\n    def serve(\n        self,\n        model_uri,\n        port,\n        host,\n        timeout,\n        enable_mlserver,\n        synchronous=True,\n        stdout=None,\n        stderr=None,\n    ):\n        \"\"\"\n        Serve pyfunc model locally.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n\n        server_implementation = mlserver if enable_mlserver else scoring_server\n        command, command_env = server_implementation.get_cmd(\n            local_path, port, host, timeout, self._nworkers\n        )\n\n        if sys.platform.startswith(\"linux\"):\n\n            def setup_sigterm_on_parent_death():\n                \"\"\"\n                Uses prctl to automatically send SIGTERM to the command process when its parent is\n                dead.\n\n                This handles the case when the parent is a PySpark worker process.\n                If a user cancels the PySpark job, the worker process gets killed, regardless of\n                PySpark daemon and worker reuse settings.\n                We use prctl to ensure the command process receives SIGTERM after spark job\n                cancellation.\n                The command process itself should handle SIGTERM properly.\n                This is a no-op on macOS because prctl is not supported.\n\n                Note:\n                When a pyspark job canceled, the UDF python process are killed by signal \"SIGKILL\",\n                This case neither \"atexit\" nor signal handler can capture SIGKILL signal.\n                prctl is the only way to capture SIGKILL signal.\n                \"\"\"\n                try:\n                    libc = ctypes.CDLL(\"libc.so.6\")\n                    # Set the parent process death signal of the command process to SIGTERM.\n                    libc.prctl(1, signal.SIGTERM)  # PR_SET_PDEATHSIG, see prctl.h\n                except OSError as e:\n                    # TODO: find approach for supporting MacOS/Windows system which does\n                    #  not support prctl.\n                    warnings.warn(f\"Setup libc.prctl PR_SET_PDEATHSIG failed, error {e!r}.\")\n\n        else:\n            setup_sigterm_on_parent_death = None\n\n        if _IS_UNIX:\n            # Add \"exec\" before the starting scoring server command, so that the scoring server\n            # process replaces the bash process, otherwise the scoring server process is created\n            # as a child process of the bash process.\n            # Note we in `mlflow.pyfunc.spark_udf`, use prctl PR_SET_PDEATHSIG to ensure scoring\n            # server process being killed when UDF process exit. The PR_SET_PDEATHSIG can only\n            # send signal to the bash process, if the scoring server process is created as a\n            # child process of the bash process, then it cannot receive the signal sent by prctl.\n            # TODO: For Windows, there's no equivalent things of Unix shell's exec. Windows also\n            #  does not support prctl. We need to find an approach to address it.\n            command = \"exec \" + command\n\n        if self._env_manager != _EnvManager.LOCAL:\n            return self.prepare_env(local_path).execute(\n                command,\n                command_env,\n                stdout=stdout,\n                stderr=stderr,\n                preexec_fn=setup_sigterm_on_parent_death,\n                synchronous=synchronous,\n            )\n        else:\n            _logger.info(\"=== Running command '%s'\", command)\n\n            if os.name != \"nt\":\n                command = [\"bash\", \"-c\", command]\n\n            child_proc = subprocess.Popen(\n                command,\n                env=command_env,\n                preexec_fn=setup_sigterm_on_parent_death,\n                stdout=stdout,\n                stderr=stderr,\n            )\n\n            if synchronous:\n                rc = child_proc.wait()\n                if rc != 0:\n                    raise Exception(\n                        f\"Command '{command}' returned non zero return code. Return code = {rc}\"\n                    )\n                return 0\n            else:\n                return child_proc\n\n    def serve_stdin(\n        self,\n        model_uri,\n        stdout=None,\n        stderr=None,\n    ):\n        local_path = _download_artifact_from_uri(model_uri)\n        return self.prepare_env(local_path).execute(\n            command=f\"python {_STDIN_SERVER_SCRIPT} --model-uri {local_path}\",\n            stdin=subprocess.PIPE,\n            stdout=stdout,\n            stderr=stderr,\n            synchronous=False,\n        )\n\n    def can_score_model(self):\n        if self._env_manager == _EnvManager.LOCAL:\n            # noconda => already in python and dependencies are assumed to be installed.\n            return True\n        conda_path = get_conda_bin_executable(\"conda\")\n        try:\n            p = subprocess.Popen(\n                [conda_path, \"--version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n            )\n            _, _ = p.communicate()\n            return p.wait() == 0\n        except FileNotFoundError:\n            # Can not find conda\n            return False\n\n    def generate_dockerfile(\n        self,\n        model_uri,\n        output_path=\"mlflow-dockerfile\",\n        install_mlflow=False,\n        mlflow_home=None,\n        enable_mlserver=False,\n    ):\n        copy_model_into_container = self.copy_model_into_container_wrapper(\n            model_uri, install_mlflow, enable_mlserver\n        )\n        pyfunc_entrypoint = _pyfunc_entrypoint(\n            self._env_manager, model_uri, install_mlflow, enable_mlserver\n        )\n\n        mlflow_home = os.path.abspath(mlflow_home) if mlflow_home else None\n\n        is_conda = self._env_manager == _EnvManager.CONDA\n        setup_miniconda = \"\"\n        setup_pyenv_and_virtualenv = \"\"\n\n        if is_conda:\n            setup_miniconda = SETUP_MINICONDA\n        else:\n            setup_pyenv_and_virtualenv = SETUP_PYENV_AND_VIRTUALENV\n\n        os.makedirs(output_path, exist_ok=True)\n\n        _logger.debug(\"Created all folders in path\", extra={\"output_directory\": output_path})\n        install_mlflow = _get_mlflow_install_step(output_path, mlflow_home)\n\n        custom_setup_steps = copy_model_into_container(output_path)\n\n        dockerfile_text = _generate_dockerfile_content(\n            setup_miniconda=setup_miniconda,\n            setup_pyenv_and_virtualenv=setup_pyenv_and_virtualenv,\n            install_mlflow=install_mlflow,\n            custom_setup_steps=custom_setup_steps,\n            entrypoint=pyfunc_entrypoint,\n        )\n        _logger.debug(\"generated dockerfile text\", extra={\"dockerfile\": dockerfile_text})\n\n        with open(os.path.join(output_path, \"Dockerfile\"), \"w\") as dockerfile:\n            dockerfile.write(dockerfile_text)\n\n    def build_image(\n        self, model_uri, image_name, install_mlflow=False, mlflow_home=None, enable_mlserver=False\n    ):\n        copy_model_into_container = self.copy_model_into_container_wrapper(\n            model_uri, install_mlflow, enable_mlserver\n        )\n        pyfunc_entrypoint = _pyfunc_entrypoint(\n            self._env_manager, model_uri, install_mlflow, enable_mlserver\n        )\n        _build_image(\n            image_name=image_name,\n            mlflow_home=mlflow_home,\n            env_manager=self._env_manager,\n            custom_setup_steps_hook=copy_model_into_container,\n            entrypoint=pyfunc_entrypoint,\n        )\n\n    def copy_model_into_container_wrapper(self, model_uri, install_mlflow, enable_mlserver):\n        def copy_model_into_container(dockerfile_context_dir):\n            # This function have to be included in another,\n            # since `_build_image` function in `docker_utils` accepts only\n            # single-argument function like this\n            model_cwd = os.path.join(dockerfile_context_dir, \"model_dir\")\n            pathlib.Path(model_cwd).mkdir(parents=True, exist_ok=True)\n            if model_uri:\n                model_path = _download_artifact_from_uri(model_uri, output_path=model_cwd)\n                return \"\"\"\n                    COPY {model_dir} /opt/ml/model\n                    RUN python -c \\\n                    'from mlflow.models.container import _install_pyfunc_deps;\\\n                    _install_pyfunc_deps(\\\n                        \"/opt/ml/model\", \\\n                        install_mlflow={install_mlflow}, \\\n                        enable_mlserver={enable_mlserver}, \\\n                        env_manager=\"{env_manager}\")'\n                    ENV {disable_env}=\"true\"\n                    ENV {ENABLE_MLSERVER}={enable_mlserver}\n                    \"\"\".format(\n                    disable_env=DISABLE_ENV_CREATION,\n                    model_dir=str(posixpath.join(\"model_dir\", os.path.basename(model_path))),\n                    install_mlflow=repr(install_mlflow),\n                    ENABLE_MLSERVER=ENABLE_MLSERVER,\n                    enable_mlserver=repr(enable_mlserver),\n                    env_manager=self._env_manager,\n                )\n            else:\n                return \"\"\"\n                    ENV {disable_env}=\"true\"\n                    ENV {ENABLE_MLSERVER}={enable_mlserver}\n                    \"\"\".format(\n                    disable_env=DISABLE_ENV_CREATION,\n                    ENABLE_MLSERVER=ENABLE_MLSERVER,\n                    enable_mlserver=repr(enable_mlserver),\n                )\n\n        return copy_model_into_container", "target": 1, "line": "@@  -143,20 +145,19  @@ def predict(self, model_uri, input_path, output_path, content_type):\n         local_uri = path_to_local_file_uri(local_path)\n \n         if self._env_manager != _EnvManager.LOCAL:\n-            command = (\n-                'python -c \"from mlflow.pyfunc.scoring_server import _predict; _predict('\n-                \"model_uri={model_uri}, \"\n-                \"input_path={input_path}, \"\n-                \"output_path={output_path}, \"\n-                \"content_type={content_type})\"\n-                '\"'\n-            ).format(\n-                model_uri=repr(local_uri),\n-                input_path=repr(input_path),\n-                output_path=repr(output_path),\n-                content_type=repr(content_type),\n-            )\n-            return self.prepare_env(local_path).execute(command)\n+            predict_cmd = [\n+                \"python\",\n+                _mlflow_pyfunc_backend_predict.__file__,\n+                \"--model-uri\",\n+                str(local_uri),\n+                \"--content-type\",\n+                shlex.quote(str(content_type)),\n+            ]\n+            if input_path:\n+                predict_cmd += [\"--input-path\", shlex.quote(str(input_path))]\n+            if output_path:\n+                predict_cmd += [\"--output-path\", shlex.quote(str(output_path))]\n+            return self.prepare_env(local_path).execute(\" \".join(predict_cmd))\n         else:\n             scoring_server._predict(local_uri, input_path, output_path, content_type)\n "}, {"function": "def _pyfunc_entrypoint(env_manager, model_uri, install_mlflow, enable_mlserver):\n    if model_uri:\n        # The pyfunc image runs the same server as the Sagemaker image\n        pyfunc_entrypoint = (\n            'ENTRYPOINT [\"python\", \"-c\", \"from mlflow.models import container as C;'\n            f'C._serve({env_manager!r})\"]'\n        )\n    else:\n        entrypoint_code = \"; \".join(\n            [\n                \"from mlflow.models import container as C\",\n                \"from mlflow.models.container import _install_pyfunc_deps\",\n                (\n                    \"_install_pyfunc_deps(\"\n                    + '\"/opt/ml/model\", '\n                    + f\"install_mlflow={install_mlflow}, \"\n                    + f\"enable_mlserver={enable_mlserver}, \"\n                    + f'env_manager=\"{env_manager}\"'\n                    + \")\"\n                ),\n                f'C._serve(\"{env_manager}\")',\n            ]\n        )\n        pyfunc_entrypoint = 'ENTRYPOINT [\"python\", \"-c\", \"{entrypoint_code}\"]'.format(\n            entrypoint_code=entrypoint_code.replace('\"', '\\\\\"')\n        )\n\n    return pyfunc_entrypoint", "target": 0}], "function_after": [{"function": "class PyFuncBackend(FlavorBackend):\n    \"\"\"\n    Flavor backend implementation for the generic python models.\n    \"\"\"\n\n    def __init__(\n        self,\n        config,\n        workers=1,\n        env_manager=_EnvManager.VIRTUALENV,\n        install_mlflow=False,\n        create_env_root_dir=False,\n        env_root_dir=None,\n        **kwargs,\n    ):\n        \"\"\"\n        :param env_root_dir: Root path for conda env. If None, use Conda's default environments\n                             directory. Note if this is set, conda package cache path becomes\n                             \"{env_root_dir}/conda_cache_pkgs\" instead of the global package cache\n                             path, and pip package cache path becomes\n                             \"{env_root_dir}/pip_cache_pkgs\" instead of the global package cache\n                             path.\n        \"\"\"\n        super().__init__(config=config, **kwargs)\n        self._nworkers = workers or 1\n        if env_manager == _EnvManager.CONDA and ENV not in config:\n            env_manager = _EnvManager.LOCAL\n        self._env_manager = env_manager\n        self._install_mlflow = install_mlflow\n        self._env_id = os.environ.get(\"MLFLOW_HOME\", VERSION) if install_mlflow else None\n        self._create_env_root_dir = create_env_root_dir\n        self._env_root_dir = env_root_dir\n        self._environment = None\n\n    def prepare_env(self, model_uri, capture_output=False):\n        if self._environment is not None:\n            return self._environment\n\n        @cache_return_value_per_process\n        def _get_or_create_env_root_dir(should_use_nfs):\n            if should_use_nfs:\n                root_tmp_dir = get_or_create_nfs_tmp_dir()\n            else:\n                root_tmp_dir = get_or_create_tmp_dir()\n\n            env_root_dir = os.path.join(root_tmp_dir, \"envs\")\n            os.makedirs(env_root_dir, exist_ok=True)\n            return env_root_dir\n\n        local_path = _download_artifact_from_uri(model_uri)\n        if self._create_env_root_dir:\n            if self._env_root_dir is not None:\n                raise Exception(\"env_root_dir can not be set when create_env_root_dir=True\")\n            nfs_root_dir = get_nfs_cache_root_dir()\n            env_root_dir = _get_or_create_env_root_dir(nfs_root_dir is not None)\n        else:\n            env_root_dir = self._env_root_dir\n\n        if self._env_manager == _EnvManager.VIRTUALENV:\n            activate_cmd = _get_or_create_virtualenv(\n                local_path,\n                self._env_id,\n                env_root_dir=env_root_dir,\n                capture_output=capture_output,\n            )\n            self._environment = Environment(activate_cmd)\n        elif self._env_manager == _EnvManager.CONDA:\n            conda_env_path = os.path.join(local_path, _extract_conda_env(self._config[ENV]))\n            self._environment = get_or_create_conda_env(\n                conda_env_path,\n                env_id=self._env_id,\n                capture_output=capture_output,\n                env_root_dir=env_root_dir,\n            )\n\n        elif self._env_manager == _EnvManager.LOCAL:\n            raise Exception(\"Prepare env should not be called with local env manager!\")\n        else:\n            raise Exception(f\"Unexpected env manager value '{self._env_manager}'\")\n\n        if self._install_mlflow:\n            self._environment.execute(_get_pip_install_mlflow())\n        else:\n            self._environment.execute('python -c \"\"')\n\n        return self._environment\n\n    def predict(self, model_uri, input_path, output_path, content_type):\n        \"\"\"\n        Generate predictions using generic python model saved with MLflow. The expected format of\n        the input JSON is the Mlflow scoring format.\n        Return the prediction results as a JSON.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n        # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure\n        # platform compatibility.\n        local_uri = path_to_local_file_uri(local_path)\n\n        if self._env_manager != _EnvManager.LOCAL:\n            predict_cmd = [\n                \"python\",\n                _mlflow_pyfunc_backend_predict.__file__,\n                \"--model-uri\",\n                str(local_uri),\n                \"--content-type\",\n                shlex.quote(str(content_type)),\n            ]\n            if input_path:\n                predict_cmd += [\"--input-path\", shlex.quote(str(input_path))]\n            if output_path:\n                predict_cmd += [\"--output-path\", shlex.quote(str(output_path))]\n            return self.prepare_env(local_path).execute(\" \".join(predict_cmd))\n        else:\n            scoring_server._predict(local_uri, input_path, output_path, content_type)\n\n    def serve(\n        self,\n        model_uri,\n        port,\n        host,\n        timeout,\n        enable_mlserver,\n        synchronous=True,\n        stdout=None,\n        stderr=None,\n    ):\n        \"\"\"\n        Serve pyfunc model locally.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n\n        server_implementation = mlserver if enable_mlserver else scoring_server\n        command, command_env = server_implementation.get_cmd(\n            local_path, port, host, timeout, self._nworkers\n        )\n\n        if sys.platform.startswith(\"linux\"):\n\n            def setup_sigterm_on_parent_death():\n                \"\"\"\n                Uses prctl to automatically send SIGTERM to the command process when its parent is\n                dead.\n\n                This handles the case when the parent is a PySpark worker process.\n                If a user cancels the PySpark job, the worker process gets killed, regardless of\n                PySpark daemon and worker reuse settings.\n                We use prctl to ensure the command process receives SIGTERM after spark job\n                cancellation.\n                The command process itself should handle SIGTERM properly.\n                This is a no-op on macOS because prctl is not supported.\n\n                Note:\n                When a pyspark job canceled, the UDF python process are killed by signal \"SIGKILL\",\n                This case neither \"atexit\" nor signal handler can capture SIGKILL signal.\n                prctl is the only way to capture SIGKILL signal.\n                \"\"\"\n                try:\n                    libc = ctypes.CDLL(\"libc.so.6\")\n                    # Set the parent process death signal of the command process to SIGTERM.\n                    libc.prctl(1, signal.SIGTERM)  # PR_SET_PDEATHSIG, see prctl.h\n                except OSError as e:\n                    # TODO: find approach for supporting MacOS/Windows system which does\n                    #  not support prctl.\n                    warnings.warn(f\"Setup libc.prctl PR_SET_PDEATHSIG failed, error {e!r}.\")\n\n        else:\n            setup_sigterm_on_parent_death = None\n\n        if _IS_UNIX:\n            # Add \"exec\" before the starting scoring server command, so that the scoring server\n            # process replaces the bash process, otherwise the scoring server process is created\n            # as a child process of the bash process.\n            # Note we in `mlflow.pyfunc.spark_udf`, use prctl PR_SET_PDEATHSIG to ensure scoring\n            # server process being killed when UDF process exit. The PR_SET_PDEATHSIG can only\n            # send signal to the bash process, if the scoring server process is created as a\n            # child process of the bash process, then it cannot receive the signal sent by prctl.\n            # TODO: For Windows, there's no equivalent things of Unix shell's exec. Windows also\n            #  does not support prctl. We need to find an approach to address it.\n            command = \"exec \" + command\n\n        if self._env_manager != _EnvManager.LOCAL:\n            return self.prepare_env(local_path).execute(\n                command,\n                command_env,\n                stdout=stdout,\n                stderr=stderr,\n                preexec_fn=setup_sigterm_on_parent_death,\n                synchronous=synchronous,\n            )\n        else:\n            _logger.info(\"=== Running command '%s'\", command)\n\n            if os.name != \"nt\":\n                command = [\"bash\", \"-c\", command]\n\n            child_proc = subprocess.Popen(\n                command,\n                env=command_env,\n                preexec_fn=setup_sigterm_on_parent_death,\n                stdout=stdout,\n                stderr=stderr,\n            )\n\n            if synchronous:\n                rc = child_proc.wait()\n                if rc != 0:\n                    raise Exception(\n                        f\"Command '{command}' returned non zero return code. Return code = {rc}\"\n                    )\n                return 0\n            else:\n                return child_proc\n\n    def serve_stdin(\n        self,\n        model_uri,\n        stdout=None,\n        stderr=None,\n    ):\n        local_path = _download_artifact_from_uri(model_uri)\n        return self.prepare_env(local_path).execute(\n            command=f\"python {_STDIN_SERVER_SCRIPT} --model-uri {local_path}\",\n            stdin=subprocess.PIPE,\n            stdout=stdout,\n            stderr=stderr,\n            synchronous=False,\n        )\n\n    def can_score_model(self):\n        if self._env_manager == _EnvManager.LOCAL:\n            # noconda => already in python and dependencies are assumed to be installed.\n            return True\n        conda_path = get_conda_bin_executable(\"conda\")\n        try:\n            p = subprocess.Popen(\n                [conda_path, \"--version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n            )\n            _, _ = p.communicate()\n            return p.wait() == 0\n        except FileNotFoundError:\n            # Can not find conda\n            return False\n\n    def generate_dockerfile(\n        self,\n        model_uri,\n        output_path=\"mlflow-dockerfile\",\n        install_mlflow=False,\n        mlflow_home=None,\n        enable_mlserver=False,\n    ):\n        copy_model_into_container = self.copy_model_into_container_wrapper(\n            model_uri, install_mlflow, enable_mlserver\n        )\n        pyfunc_entrypoint = _pyfunc_entrypoint(\n            self._env_manager, model_uri, install_mlflow, enable_mlserver\n        )\n\n        mlflow_home = os.path.abspath(mlflow_home) if mlflow_home else None\n\n        is_conda = self._env_manager == _EnvManager.CONDA\n        setup_miniconda = \"\"\n        setup_pyenv_and_virtualenv = \"\"\n\n        if is_conda:\n            setup_miniconda = SETUP_MINICONDA\n        else:\n            setup_pyenv_and_virtualenv = SETUP_PYENV_AND_VIRTUALENV\n\n        os.makedirs(output_path, exist_ok=True)\n\n        _logger.debug(\"Created all folders in path\", extra={\"output_directory\": output_path})\n        install_mlflow = _get_mlflow_install_step(output_path, mlflow_home)\n\n        custom_setup_steps = copy_model_into_container(output_path)\n\n        dockerfile_text = _generate_dockerfile_content(\n            setup_miniconda=setup_miniconda,\n            setup_pyenv_and_virtualenv=setup_pyenv_and_virtualenv,\n            install_mlflow=install_mlflow,\n            custom_setup_steps=custom_setup_steps,\n            entrypoint=pyfunc_entrypoint,\n        )\n        _logger.debug(\"generated dockerfile text\", extra={\"dockerfile\": dockerfile_text})\n\n        with open(os.path.join(output_path, \"Dockerfile\"), \"w\") as dockerfile:\n            dockerfile.write(dockerfile_text)\n\n    def build_image(\n        self, model_uri, image_name, install_mlflow=False, mlflow_home=None, enable_mlserver=False\n    ):\n        copy_model_into_container = self.copy_model_into_container_wrapper(\n            model_uri, install_mlflow, enable_mlserver\n        )\n        pyfunc_entrypoint = _pyfunc_entrypoint(\n            self._env_manager, model_uri, install_mlflow, enable_mlserver\n        )\n        _build_image(\n            image_name=image_name,\n            mlflow_home=mlflow_home,\n            env_manager=self._env_manager,\n            custom_setup_steps_hook=copy_model_into_container,\n            entrypoint=pyfunc_entrypoint,\n        )\n\n    def copy_model_into_container_wrapper(self, model_uri, install_mlflow, enable_mlserver):\n        def copy_model_into_container(dockerfile_context_dir):\n            # This function have to be included in another,\n            # since `_build_image` function in `docker_utils` accepts only\n            # single-argument function like this\n            model_cwd = os.path.join(dockerfile_context_dir, \"model_dir\")\n            pathlib.Path(model_cwd).mkdir(parents=True, exist_ok=True)\n            if model_uri:\n                model_path = _download_artifact_from_uri(model_uri, output_path=model_cwd)\n                return \"\"\"\n                    COPY {model_dir} /opt/ml/model\n                    RUN python -c \\\n                    'from mlflow.models.container import _install_pyfunc_deps;\\\n                    _install_pyfunc_deps(\\\n                        \"/opt/ml/model\", \\\n                        install_mlflow={install_mlflow}, \\\n                        enable_mlserver={enable_mlserver}, \\\n                        env_manager=\"{env_manager}\")'\n                    ENV {disable_env}=\"true\"\n                    ENV {ENABLE_MLSERVER}={enable_mlserver}\n                    \"\"\".format(\n                    disable_env=DISABLE_ENV_CREATION,\n                    model_dir=str(posixpath.join(\"model_dir\", os.path.basename(model_path))),\n                    install_mlflow=repr(install_mlflow),\n                    ENABLE_MLSERVER=ENABLE_MLSERVER,\n                    enable_mlserver=repr(enable_mlserver),\n                    env_manager=self._env_manager,\n                )\n            else:\n                return \"\"\"\n                    ENV {disable_env}=\"true\"\n                    ENV {ENABLE_MLSERVER}={enable_mlserver}\n                    \"\"\".format(\n                    disable_env=DISABLE_ENV_CREATION,\n                    ENABLE_MLSERVER=ENABLE_MLSERVER,\n                    enable_mlserver=repr(enable_mlserver),\n                )\n\n        return copy_model_into_container", "target": 0}, {"function": "def _pyfunc_entrypoint(env_manager, model_uri, install_mlflow, enable_mlserver):\n    if model_uri:\n        # The pyfunc image runs the same server as the Sagemaker image\n        pyfunc_entrypoint = (\n            'ENTRYPOINT [\"python\", \"-c\", \"from mlflow.models import container as C;'\n            f'C._serve({env_manager!r})\"]'\n        )\n    else:\n        entrypoint_code = \"; \".join(\n            [\n                \"from mlflow.models import container as C\",\n                \"from mlflow.models.container import _install_pyfunc_deps\",\n                (\n                    \"_install_pyfunc_deps(\"\n                    + '\"/opt/ml/model\", '\n                    + f\"install_mlflow={install_mlflow}, \"\n                    + f\"enable_mlserver={enable_mlserver}, \"\n                    + f'env_manager=\"{env_manager}\"'\n                    + \")\"\n                ),\n                f'C._serve(\"{env_manager}\")',\n            ]\n        )\n        pyfunc_entrypoint = 'ENTRYPOINT [\"python\", \"-c\", \"{entrypoint_code}\"]'.format(\n            entrypoint_code=entrypoint_code.replace('\"', '\\\\\"')\n        )\n\n    return pyfunc_entrypoint", "target": 0}]}, {"raw_url": "https://github.com/mlflow/mlflow/raw/6dde93758d42455cb90ef324407919ed67668b9b/mlflow%2Fpyfunc%2Fscoring_server%2F__init__.py", "code": "\"\"\"\nScoring server for python model format.\nThe passed int model is expected to have function:\n   predict(pandas.Dataframe) -> pandas.DataFrame\n\nInput, expected in text/csv or application/json format,\nis parsed into pandas.DataFrame and passed to the model.\n\nDefines four endpoints:\n    /ping used for health check\n    /health (same as /ping)\n    /version used for getting the mlflow version\n    /invocations used for scoring\n\"\"\"\nfrom typing import Tuple, Dict\nimport flask\nimport json\nimport logging\nimport os\nimport shlex\nimport sys\nimport traceback\n\nfrom mlflow.environment_variables import MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT\n\n# NB: We need to be careful what we import form mlflow here. Scoring server is used from within\n# model's conda environment. The version of mlflow doing the serving (outside) and the version of\n# mlflow in the model's conda environment (inside) can differ. We should therefore keep mlflow\n# dependencies to the minimum here.\n# ALl of the mlflow dependencies below need to be backwards compatible.\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.types import Schema\nfrom mlflow.utils import reraise\nfrom mlflow.utils.file_utils import path_to_local_file_uri\nfrom mlflow.utils.os import is_windows\nfrom mlflow.utils.proto_json_utils import (\n    NumpyEncoder,\n    dataframe_from_parsed_json,\n    _get_jsonable_obj,\n    parse_tf_serving_input,\n)\nfrom mlflow.version import VERSION\n\ntry:\n    from mlflow.pyfunc import load_model, PyFuncModel\nexcept ImportError:\n    from mlflow.pyfunc import load_pyfunc as load_model\nfrom mlflow.protos.databricks_pb2 import BAD_REQUEST, INVALID_PARAMETER_VALUE\nfrom mlflow.server.handlers import catch_mlflow_exception\nfrom io import StringIO\n\n_SERVER_MODEL_PATH = \"__pyfunc_model_path__\"\n\nCONTENT_TYPE_CSV = \"text/csv\"\nCONTENT_TYPE_JSON = \"application/json\"\n\nCONTENT_TYPES = [\n    CONTENT_TYPE_CSV,\n    CONTENT_TYPE_JSON,\n]\n\n_logger = logging.getLogger(__name__)\n\nDF_RECORDS = \"dataframe_records\"\nDF_SPLIT = \"dataframe_split\"\nINSTANCES = \"instances\"\nINPUTS = \"inputs\"\n\nSUPPORTED_FORMATS = {DF_RECORDS, DF_SPLIT, INSTANCES, INPUTS}\n\nREQUIRED_INPUT_FORMAT = (\n    f\"The input must be a JSON dictionary with exactly one of the input fields {SUPPORTED_FORMATS}\"\n)\nSCORING_PROTOCOL_CHANGE_INFO = (\n    \"IMPORTANT: The MLflow Model scoring protocol has changed in MLflow version 2.0. If you are\"\n    \" seeing this error, you are likely using an outdated scoring request format. To resolve the\"\n    \" error, either update your request format or adjust your MLflow Model's requirements file to\"\n    \" specify an older version of MLflow (for example, change the 'mlflow' requirement specifier\"\n    \" to 'mlflow==1.30.0'). If you are making a request using the MLflow client\"\n    \" (e.g. via `mlflow.pyfunc.spark_udf()`), upgrade your MLflow client to a version >= 2.0 in\"\n    \" order to use the new request format. For more information about the updated MLflow\"\n    \" Model scoring protocol in MLflow 2.0, see\"\n    \" https://mlflow.org/docs/latest/models.html#deploy-mlflow-models.\"\n)\n\n\ndef infer_and_parse_json_input(json_input, schema: Schema = None):\n    \"\"\"\n    :param json_input: A JSON-formatted string representation of TF serving input or a Pandas\n                       DataFrame, or a stream containing such a string representation.\n    :param schema: Optional schema specification to be used during parsing.\n    \"\"\"\n    if isinstance(json_input, dict):\n        decoded_input = json_input\n    else:\n        try:\n            decoded_input = json.loads(json_input)\n        except json.decoder.JSONDecodeError as ex:\n            raise MlflowException(\n                message=(\n                    \"Failed to parse input from JSON. Ensure that input is a valid JSON\"\n                    f\" formatted string. Error: '{ex}'. Input: \\n{json_input}\\n\"\n                ),\n                error_code=BAD_REQUEST,\n            )\n    if isinstance(decoded_input, dict):\n        format_keys = set(decoded_input.keys()).intersection(SUPPORTED_FORMATS)\n        if len(format_keys) != 1:\n            message = f\"Received dictionary with input fields: {list(decoded_input.keys())}\"\n            raise MlflowException(\n                message=f\"{REQUIRED_INPUT_FORMAT}. {message}. {SCORING_PROTOCOL_CHANGE_INFO}\",\n                error_code=BAD_REQUEST,\n            )\n        input_format = format_keys.pop()\n        if input_format in (INSTANCES, INPUTS):\n            return parse_tf_serving_input(decoded_input, schema=schema)\n\n        elif input_format in (DF_SPLIT, DF_RECORDS):\n            # NB: skip the dataframe_ prefix\n            pandas_orient = input_format[10:]\n            return dataframe_from_parsed_json(\n                decoded_input[input_format], pandas_orient=pandas_orient, schema=schema\n            )\n    elif isinstance(decoded_input, list):\n        message = \"Received a list\"\n        raise MlflowException(\n            message=f\"{REQUIRED_INPUT_FORMAT}. {message}. {SCORING_PROTOCOL_CHANGE_INFO}\",\n            error_code=BAD_REQUEST,\n        )\n    else:\n        message = f\"Received unexpected input type '{type(decoded_input)}'\"\n        raise MlflowException(\n            message=f\"{REQUIRED_INPUT_FORMAT}. {message}.\", error_code=BAD_REQUEST\n        )\n\n\ndef parse_csv_input(csv_input, schema: Schema = None):\n    \"\"\"\n    :param csv_input: A CSV-formatted string representation of a Pandas DataFrame, or a stream\n                      containing such a string representation.\n    :param schema: Optional schema specification to be used during parsing.\n    \"\"\"\n    import pandas as pd\n\n    try:\n        if schema is None:\n            return pd.read_csv(csv_input)\n        else:\n            dtypes = dict(zip(schema.input_names(), schema.pandas_types()))\n            return pd.read_csv(csv_input, dtype=dtypes)\n    except Exception:\n        _handle_serving_error(\n            error_message=(\n                \"Failed to parse input as a Pandas DataFrame. Ensure that the input is\"\n                \" a valid CSV-formatted Pandas DataFrame produced using the\"\n                \" `pandas.DataFrame.to_csv()` method.\"\n            ),\n            error_code=BAD_REQUEST,\n        )\n\n\ndef predictions_to_json(raw_predictions, output, metadata=None):\n    if metadata and \"predictions\" in metadata:\n        raise MlflowException(\n            \"metadata cannot contain 'predictions' key\", error_code=INVALID_PARAMETER_VALUE\n        )\n    predictions = _get_jsonable_obj(raw_predictions, pandas_orient=\"records\")\n    return json.dump({\"predictions\": predictions, **(metadata or {})}, output, cls=NumpyEncoder)\n\n\ndef _handle_serving_error(error_message, error_code, include_traceback=True):\n    \"\"\"\n    Logs information about an exception thrown by model inference code that is currently being\n    handled and reraises it with the specified error message. The exception stack trace\n    is also included in the reraised error message.\n\n    :param error_message: A message for the reraised exception.\n    :param error_code: An appropriate error code for the reraised exception. This should be one of\n                       the codes listed in the `mlflow.protos.databricks_pb2` proto.\n    :param include_traceback: Whether to include the current traceback in the returned error.\n    \"\"\"\n    if include_traceback:\n        traceback_buf = StringIO()\n        traceback.print_exc(file=traceback_buf)\n        traceback_str = traceback_buf.getvalue()\n        e = MlflowException(message=error_message, error_code=error_code, stack_trace=traceback_str)\n    else:\n        e = MlflowException(message=error_message, error_code=error_code)\n    reraise(MlflowException, e)\n\n\ndef init(model: PyFuncModel):\n    \"\"\"\n    Initialize the server. Loads pyfunc model from the path.\n    \"\"\"\n    app = flask.Flask(__name__)\n    input_schema = model.metadata.get_input_schema()\n\n    @app.route(\"/ping\", methods=[\"GET\"])\n    @app.route(\"/health\", methods=[\"GET\"])\n    def ping():\n        \"\"\"\n        Determine if the container is working and healthy.\n        We declare it healthy if we can load the model successfully.\n        \"\"\"\n        health = model is not None\n        status = 200 if health else 404\n        return flask.Response(response=\"\\n\", status=status, mimetype=\"application/json\")\n\n    @app.route(\"/version\", methods=[\"GET\"])\n    def version():\n        \"\"\"\n        Returns the current mlflow version.\n        \"\"\"\n        return flask.Response(response=VERSION, status=200, mimetype=\"application/json\")\n\n    @app.route(\"/invocations\", methods=[\"POST\"])\n    @catch_mlflow_exception\n    def transformation():\n        \"\"\"\n        Do an inference on a single batch of data. In this sample server,\n        we take data as CSV or json, convert it to a Pandas DataFrame or Numpy,\n        generate predictions and convert them back to json.\n        \"\"\"\n\n        # Content-Type can include other attributes like CHARSET\n        # Content-type RFC: https://datatracker.ietf.org/doc/html/rfc2045#section-5.1\n        # TODO: Suport \";\" in quoted parameter values\n        type_parts = flask.request.content_type.split(\";\")\n        type_parts = list(map(str.strip, type_parts))\n        mime_type = type_parts[0]\n        parameter_value_pairs = type_parts[1:]\n        parameter_values = {}\n        for parameter_value_pair in parameter_value_pairs:\n            (key, _, value) = parameter_value_pair.partition(\"=\")\n            parameter_values[key] = value\n\n        charset = parameter_values.get(\"charset\", \"utf-8\").lower()\n        if charset != \"utf-8\":\n            return flask.Response(\n                response=\"The scoring server only supports UTF-8\",\n                status=415,\n                mimetype=\"text/plain\",\n            )\n\n        unexpected_content_parameters = set(parameter_values.keys()).difference({\"charset\"})\n        if unexpected_content_parameters:\n            return flask.Response(\n                response=(\n                    f\"Unrecognized content type parameters: \"\n                    f\"{', '.join(unexpected_content_parameters)}. \"\n                    f\"{SCORING_PROTOCOL_CHANGE_INFO}\"\n                ),\n                status=415,\n                mimetype=\"text/plain\",\n            )\n        # Convert from CSV to pandas\n        if mime_type == CONTENT_TYPE_CSV:\n            data = flask.request.data.decode(\"utf-8\")\n            csv_input = StringIO(data)\n            data = parse_csv_input(csv_input=csv_input, schema=input_schema)\n        elif mime_type == CONTENT_TYPE_JSON:\n            json_str = flask.request.data.decode(\"utf-8\")\n            data = infer_and_parse_json_input(json_str, input_schema)\n        else:\n            return flask.Response(\n                response=(\n                    \"This predictor only supports the following content types:\"\n                    f\" Types: {CONTENT_TYPES}.\"\n                    f\" Got '{flask.request.content_type}'.\"\n                ),\n                status=415,\n                mimetype=\"text/plain\",\n            )\n\n        # Do the prediction\n        try:\n            raw_predictions = model.predict(data)\n        except MlflowException as e:\n            raise e\n        except Exception:\n            raise MlflowException(\n                message=(\n                    \"Encountered an unexpected error while evaluating the model. Verify\"\n                    \" that the serialized input Dataframe is compatible with the model for\"\n                    \" inference.\"\n                ),\n                error_code=BAD_REQUEST,\n                stack_trace=traceback.format_exc(),\n            )\n        result = StringIO()\n        predictions_to_json(raw_predictions, result)\n        return flask.Response(response=result.getvalue(), status=200, mimetype=\"application/json\")\n\n    return app\n\n\ndef _predict(model_uri, input_path, output_path, content_type):\n    pyfunc_model = load_model(model_uri)\n\n    if content_type == \"json\":\n        if input_path is None:\n            input_str = sys.stdin.read()\n        else:\n            with open(input_path) as f:\n                input_str = f.read()\n        df = infer_and_parse_json_input(input_str)\n    elif content_type == \"csv\":\n        if input_path is not None:\n            df = parse_csv_input(input_path)\n        else:\n            df = parse_csv_input(sys.stdin)\n    else:\n        raise Exception(f\"Unknown content type '{content_type}'\")\n\n    if output_path is None:\n        predictions_to_json(pyfunc_model.predict(df), sys.stdout)\n    else:\n        with open(output_path, \"w\") as fout:\n            predictions_to_json(pyfunc_model.predict(df), fout)\n\n\ndef _serve(model_uri, port, host):\n    pyfunc_model = load_model(model_uri)\n    init(pyfunc_model).run(port=port, host=host)\n\n\ndef get_cmd(\n    model_uri: str, port: int = None, host: int = None, timeout: int = None, nworkers: int = None\n) -> Tuple[str, Dict[str, str]]:\n    local_uri = path_to_local_file_uri(model_uri)\n    timeout = timeout or MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT.get()\n\n    # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure\n    # platform compatibility.\n    if not is_windows():\n        args = [f\"--timeout={timeout}\"]\n        if port and host:\n            address = shlex.quote(f\"{host}:{port}\")\n            args.append(f\"-b {address}\")\n        elif host:\n            args.append(f\"-b {shlex.quote(host)}\")\n\n        if nworkers:\n            args.append(f\"-w {nworkers}\")\n\n        command = (\n            f\"gunicorn {' '.join(args)} ${{GUNICORN_CMD_ARGS}}\"\n            \" -- mlflow.pyfunc.scoring_server.wsgi:app\"\n        )\n    else:\n        args = []\n        if host:\n            args.append(f\"--host={shlex.quote(host)}\")\n\n        if port:\n            args.append(f\"--port={port}\")\n\n        command = (\n            f\"waitress-serve {' '.join(args)} \"\n            \"--ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app\"\n        )\n\n    command_env = os.environ.copy()\n    command_env[_SERVER_MODEL_PATH] = local_uri\n\n    return command, command_env\n", "code_before": "\"\"\"\nScoring server for python model format.\nThe passed int model is expected to have function:\n   predict(pandas.Dataframe) -> pandas.DataFrame\n\nInput, expected in text/csv or application/json format,\nis parsed into pandas.DataFrame and passed to the model.\n\nDefines four endpoints:\n    /ping used for health check\n    /health (same as /ping)\n    /version used for getting the mlflow version\n    /invocations used for scoring\n\"\"\"\nfrom typing import Tuple, Dict\nimport flask\nimport json\nimport logging\nimport os\nimport sys\nimport traceback\n\nfrom mlflow.environment_variables import MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT\n\n# NB: We need to be careful what we import form mlflow here. Scoring server is used from within\n# model's conda environment. The version of mlflow doing the serving (outside) and the version of\n# mlflow in the model's conda environment (inside) can differ. We should therefore keep mlflow\n# dependencies to the minimum here.\n# ALl of the mlflow dependencies below need to be backwards compatible.\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.types import Schema\nfrom mlflow.utils import reraise\nfrom mlflow.utils.file_utils import path_to_local_file_uri\nfrom mlflow.utils.proto_json_utils import (\n    NumpyEncoder,\n    dataframe_from_parsed_json,\n    _get_jsonable_obj,\n    parse_tf_serving_input,\n)\nfrom mlflow.version import VERSION\n\ntry:\n    from mlflow.pyfunc import load_model, PyFuncModel\nexcept ImportError:\n    from mlflow.pyfunc import load_pyfunc as load_model\nfrom mlflow.protos.databricks_pb2 import BAD_REQUEST, INVALID_PARAMETER_VALUE\nfrom mlflow.server.handlers import catch_mlflow_exception\nfrom io import StringIO\n\n_SERVER_MODEL_PATH = \"__pyfunc_model_path__\"\n\nCONTENT_TYPE_CSV = \"text/csv\"\nCONTENT_TYPE_JSON = \"application/json\"\n\nCONTENT_TYPES = [\n    CONTENT_TYPE_CSV,\n    CONTENT_TYPE_JSON,\n]\n\n_logger = logging.getLogger(__name__)\n\nDF_RECORDS = \"dataframe_records\"\nDF_SPLIT = \"dataframe_split\"\nINSTANCES = \"instances\"\nINPUTS = \"inputs\"\n\nSUPPORTED_FORMATS = {DF_RECORDS, DF_SPLIT, INSTANCES, INPUTS}\n\nREQUIRED_INPUT_FORMAT = (\n    f\"The input must be a JSON dictionary with exactly one of the input fields {SUPPORTED_FORMATS}\"\n)\nSCORING_PROTOCOL_CHANGE_INFO = (\n    \"IMPORTANT: The MLflow Model scoring protocol has changed in MLflow version 2.0. If you are\"\n    \" seeing this error, you are likely using an outdated scoring request format. To resolve the\"\n    \" error, either update your request format or adjust your MLflow Model's requirements file to\"\n    \" specify an older version of MLflow (for example, change the 'mlflow' requirement specifier\"\n    \" to 'mlflow==1.30.0'). If you are making a request using the MLflow client\"\n    \" (e.g. via `mlflow.pyfunc.spark_udf()`), upgrade your MLflow client to a version >= 2.0 in\"\n    \" order to use the new request format. For more information about the updated MLflow\"\n    \" Model scoring protocol in MLflow 2.0, see\"\n    \" https://mlflow.org/docs/latest/models.html#deploy-mlflow-models.\"\n)\n\n\ndef infer_and_parse_json_input(json_input, schema: Schema = None):\n    \"\"\"\n    :param json_input: A JSON-formatted string representation of TF serving input or a Pandas\n                       DataFrame, or a stream containing such a string representation.\n    :param schema: Optional schema specification to be used during parsing.\n    \"\"\"\n    if isinstance(json_input, dict):\n        decoded_input = json_input\n    else:\n        try:\n            decoded_input = json.loads(json_input)\n        except json.decoder.JSONDecodeError as ex:\n            raise MlflowException(\n                message=(\n                    \"Failed to parse input from JSON. Ensure that input is a valid JSON\"\n                    f\" formatted string. Error: '{ex}'. Input: \\n{json_input}\\n\"\n                ),\n                error_code=BAD_REQUEST,\n            )\n    if isinstance(decoded_input, dict):\n        format_keys = set(decoded_input.keys()).intersection(SUPPORTED_FORMATS)\n        if len(format_keys) != 1:\n            message = f\"Received dictionary with input fields: {list(decoded_input.keys())}\"\n            raise MlflowException(\n                message=f\"{REQUIRED_INPUT_FORMAT}. {message}. {SCORING_PROTOCOL_CHANGE_INFO}\",\n                error_code=BAD_REQUEST,\n            )\n        input_format = format_keys.pop()\n        if input_format in (INSTANCES, INPUTS):\n            return parse_tf_serving_input(decoded_input, schema=schema)\n\n        elif input_format in (DF_SPLIT, DF_RECORDS):\n            # NB: skip the dataframe_ prefix\n            pandas_orient = input_format[10:]\n            return dataframe_from_parsed_json(\n                decoded_input[input_format], pandas_orient=pandas_orient, schema=schema\n            )\n    elif isinstance(decoded_input, list):\n        message = \"Received a list\"\n        raise MlflowException(\n            message=f\"{REQUIRED_INPUT_FORMAT}. {message}. {SCORING_PROTOCOL_CHANGE_INFO}\",\n            error_code=BAD_REQUEST,\n        )\n    else:\n        message = f\"Received unexpected input type '{type(decoded_input)}'\"\n        raise MlflowException(\n            message=f\"{REQUIRED_INPUT_FORMAT}. {message}.\", error_code=BAD_REQUEST\n        )\n\n\ndef parse_csv_input(csv_input, schema: Schema = None):\n    \"\"\"\n    :param csv_input: A CSV-formatted string representation of a Pandas DataFrame, or a stream\n                      containing such a string representation.\n    :param schema: Optional schema specification to be used during parsing.\n    \"\"\"\n    import pandas as pd\n\n    try:\n        if schema is None:\n            return pd.read_csv(csv_input)\n        else:\n            dtypes = dict(zip(schema.input_names(), schema.pandas_types()))\n            return pd.read_csv(csv_input, dtype=dtypes)\n    except Exception:\n        _handle_serving_error(\n            error_message=(\n                \"Failed to parse input as a Pandas DataFrame. Ensure that the input is\"\n                \" a valid CSV-formatted Pandas DataFrame produced using the\"\n                \" `pandas.DataFrame.to_csv()` method.\"\n            ),\n            error_code=BAD_REQUEST,\n        )\n\n\ndef predictions_to_json(raw_predictions, output, metadata=None):\n    if metadata and \"predictions\" in metadata:\n        raise MlflowException(\n            \"metadata cannot contain 'predictions' key\", error_code=INVALID_PARAMETER_VALUE\n        )\n    predictions = _get_jsonable_obj(raw_predictions, pandas_orient=\"records\")\n    return json.dump({\"predictions\": predictions, **(metadata or {})}, output, cls=NumpyEncoder)\n\n\ndef _handle_serving_error(error_message, error_code, include_traceback=True):\n    \"\"\"\n    Logs information about an exception thrown by model inference code that is currently being\n    handled and reraises it with the specified error message. The exception stack trace\n    is also included in the reraised error message.\n\n    :param error_message: A message for the reraised exception.\n    :param error_code: An appropriate error code for the reraised exception. This should be one of\n                       the codes listed in the `mlflow.protos.databricks_pb2` proto.\n    :param include_traceback: Whether to include the current traceback in the returned error.\n    \"\"\"\n    if include_traceback:\n        traceback_buf = StringIO()\n        traceback.print_exc(file=traceback_buf)\n        traceback_str = traceback_buf.getvalue()\n        e = MlflowException(message=error_message, error_code=error_code, stack_trace=traceback_str)\n    else:\n        e = MlflowException(message=error_message, error_code=error_code)\n    reraise(MlflowException, e)\n\n\ndef init(model: PyFuncModel):\n    \"\"\"\n    Initialize the server. Loads pyfunc model from the path.\n    \"\"\"\n    app = flask.Flask(__name__)\n    input_schema = model.metadata.get_input_schema()\n\n    @app.route(\"/ping\", methods=[\"GET\"])\n    @app.route(\"/health\", methods=[\"GET\"])\n    def ping():\n        \"\"\"\n        Determine if the container is working and healthy.\n        We declare it healthy if we can load the model successfully.\n        \"\"\"\n        health = model is not None\n        status = 200 if health else 404\n        return flask.Response(response=\"\\n\", status=status, mimetype=\"application/json\")\n\n    @app.route(\"/version\", methods=[\"GET\"])\n    def version():\n        \"\"\"\n        Returns the current mlflow version.\n        \"\"\"\n        return flask.Response(response=VERSION, status=200, mimetype=\"application/json\")\n\n    @app.route(\"/invocations\", methods=[\"POST\"])\n    @catch_mlflow_exception\n    def transformation():\n        \"\"\"\n        Do an inference on a single batch of data. In this sample server,\n        we take data as CSV or json, convert it to a Pandas DataFrame or Numpy,\n        generate predictions and convert them back to json.\n        \"\"\"\n\n        # Content-Type can include other attributes like CHARSET\n        # Content-type RFC: https://datatracker.ietf.org/doc/html/rfc2045#section-5.1\n        # TODO: Suport \";\" in quoted parameter values\n        type_parts = flask.request.content_type.split(\";\")\n        type_parts = list(map(str.strip, type_parts))\n        mime_type = type_parts[0]\n        parameter_value_pairs = type_parts[1:]\n        parameter_values = {}\n        for parameter_value_pair in parameter_value_pairs:\n            (key, _, value) = parameter_value_pair.partition(\"=\")\n            parameter_values[key] = value\n\n        charset = parameter_values.get(\"charset\", \"utf-8\").lower()\n        if charset != \"utf-8\":\n            return flask.Response(\n                response=\"The scoring server only supports UTF-8\",\n                status=415,\n                mimetype=\"text/plain\",\n            )\n\n        unexpected_content_parameters = set(parameter_values.keys()).difference({\"charset\"})\n        if unexpected_content_parameters:\n            return flask.Response(\n                response=(\n                    f\"Unrecognized content type parameters: \"\n                    f\"{', '.join(unexpected_content_parameters)}. \"\n                    f\"{SCORING_PROTOCOL_CHANGE_INFO}\"\n                ),\n                status=415,\n                mimetype=\"text/plain\",\n            )\n        # Convert from CSV to pandas\n        if mime_type == CONTENT_TYPE_CSV:\n            data = flask.request.data.decode(\"utf-8\")\n            csv_input = StringIO(data)\n            data = parse_csv_input(csv_input=csv_input, schema=input_schema)\n        elif mime_type == CONTENT_TYPE_JSON:\n            json_str = flask.request.data.decode(\"utf-8\")\n            data = infer_and_parse_json_input(json_str, input_schema)\n        else:\n            return flask.Response(\n                response=(\n                    \"This predictor only supports the following content types:\"\n                    f\" Types: {CONTENT_TYPES}.\"\n                    f\" Got '{flask.request.content_type}'.\"\n                ),\n                status=415,\n                mimetype=\"text/plain\",\n            )\n\n        # Do the prediction\n        try:\n            raw_predictions = model.predict(data)\n        except MlflowException as e:\n            raise e\n        except Exception:\n            raise MlflowException(\n                message=(\n                    \"Encountered an unexpected error while evaluating the model. Verify\"\n                    \" that the serialized input Dataframe is compatible with the model for\"\n                    \" inference.\"\n                ),\n                error_code=BAD_REQUEST,\n                stack_trace=traceback.format_exc(),\n            )\n        result = StringIO()\n        predictions_to_json(raw_predictions, result)\n        return flask.Response(response=result.getvalue(), status=200, mimetype=\"application/json\")\n\n    return app\n\n\ndef _predict(model_uri, input_path, output_path, content_type):\n    pyfunc_model = load_model(model_uri)\n\n    if content_type == \"json\":\n        if input_path is None:\n            input_str = sys.stdin.read()\n        else:\n            with open(input_path) as f:\n                input_str = f.read()\n        df = infer_and_parse_json_input(input_str)\n    elif content_type == \"csv\":\n        if input_path is not None:\n            df = parse_csv_input(input_path)\n        else:\n            df = parse_csv_input(sys.stdin)\n    else:\n        raise Exception(f\"Unknown content type '{content_type}'\")\n\n    if output_path is None:\n        predictions_to_json(pyfunc_model.predict(df), sys.stdout)\n    else:\n        with open(output_path, \"w\") as fout:\n            predictions_to_json(pyfunc_model.predict(df), fout)\n\n\ndef _serve(model_uri, port, host):\n    pyfunc_model = load_model(model_uri)\n    init(pyfunc_model).run(port=port, host=host)\n\n\ndef get_cmd(\n    model_uri: str, port: int = None, host: int = None, timeout: int = None, nworkers: int = None\n) -> Tuple[str, Dict[str, str]]:\n    local_uri = path_to_local_file_uri(model_uri)\n    timeout = timeout or MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT.get()\n    # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure\n    # platform compatibility.\n    if os.name != \"nt\":\n        args = [f\"--timeout={timeout}\"]\n        if port and host:\n            args.append(f\"-b {host}:{port}\")\n        elif host:\n            args.append(f\"-b {host}\")\n\n        if nworkers:\n            args.append(f\"-w {nworkers}\")\n\n        command = (\n            f\"gunicorn {' '.join(args)} ${{GUNICORN_CMD_ARGS}}\"\n            \" -- mlflow.pyfunc.scoring_server.wsgi:app\"\n        )\n    else:\n        args = []\n        if host:\n            args.append(f\"--host={host}\")\n\n        if port:\n            args.append(f\"--port={port}\")\n\n        command = (\n            f\"waitress-serve {' '.join(args)} \"\n            \"--ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app\"\n        )\n\n    command_env = os.environ.copy()\n    command_env[_SERVER_MODEL_PATH] = local_uri\n\n    return command, command_env\n", "patch": "@@ -17,6 +17,7 @@\n import json\n import logging\n import os\n+import shlex\n import sys\n import traceback\n \n@@ -31,6 +32,7 @@\n from mlflow.types import Schema\n from mlflow.utils import reraise\n from mlflow.utils.file_utils import path_to_local_file_uri\n+from mlflow.utils.os import is_windows\n from mlflow.utils.proto_json_utils import (\n     NumpyEncoder,\n     dataframe_from_parsed_json,\n@@ -328,14 +330,16 @@ def get_cmd(\n ) -> Tuple[str, Dict[str, str]]:\n     local_uri = path_to_local_file_uri(model_uri)\n     timeout = timeout or MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT.get()\n+\n     # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure\n     # platform compatibility.\n-    if os.name != \"nt\":\n+    if not is_windows():\n         args = [f\"--timeout={timeout}\"]\n         if port and host:\n-            args.append(f\"-b {host}:{port}\")\n+            address = shlex.quote(f\"{host}:{port}\")\n+            args.append(f\"-b {address}\")\n         elif host:\n-            args.append(f\"-b {host}\")\n+            args.append(f\"-b {shlex.quote(host)}\")\n \n         if nworkers:\n             args.append(f\"-w {nworkers}\")\n@@ -347,7 +351,7 @@ def get_cmd(\n     else:\n         args = []\n         if host:\n-            args.append(f\"--host={host}\")\n+            args.append(f\"--host={shlex.quote(host)}\")\n \n         if port:\n             args.append(f\"--port={port}\")", "file_path": "files/2023_7/3", "file_language": "py", "file_name": "mlflow/pyfunc/scoring_server/__init__.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/mlflow/mlflow/raw/6dde93758d42455cb90ef324407919ed67668b9b/tests%2Fmodels%2Ftest_cli.py", "code": "import json\nimport os\nimport subprocess\nimport sys\nimport warnings\nfrom pathlib import Path\n\nfrom click.testing import CliRunner\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport re\nimport sklearn\nimport sklearn.datasets\nimport sklearn.neighbors\n\nfrom unittest import mock\n\n\nimport mlflow\nimport mlflow.sklearn\nfrom mlflow.models.flavor_backend_registry import get_flavor_backend\n\nfrom mlflow.utils.conda import _get_conda_env_name\n\nimport mlflow.models.cli as models_cli\n\nfrom mlflow.environment_variables import MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.protos.databricks_pb2 import ErrorCode, BAD_REQUEST\nfrom mlflow.pyfunc.backend import PyFuncBackend\nfrom mlflow.pyfunc.scoring_server import (\n    CONTENT_TYPE_JSON,\n    CONTENT_TYPE_CSV,\n)\nfrom mlflow.utils.file_utils import TempDir\nfrom mlflow.utils.environment import _mlflow_conda_env\nfrom mlflow.utils import env_manager as _EnvManager\nfrom mlflow.utils import PYTHON_VERSION\nfrom mlflow.utils.process import ShellCommandException\nfrom tests.helper_functions import (\n    pyfunc_build_image,\n    pyfunc_serve_from_docker_image,\n    pyfunc_serve_from_docker_image_with_env_override,\n    RestEndpoint,\n    get_safe_port,\n    pyfunc_serve_and_score_model,\n    PROTOBUF_REQUIREMENT,\n    pyfunc_generate_dockerfile,\n)\n\n# NB: for now, windows tests do not have conda available.\nno_conda = [\"--env-manager\", \"local\"] if sys.platform == \"win32\" else []\n\n# NB: need to install mlflow since the pip version does not have mlflow models cli.\ninstall_mlflow = [\"--install-mlflow\"] if not no_conda else []\n\nextra_options = no_conda + install_mlflow\ngunicorn_options = \"--timeout 60 -w 5\"\n\n\ndef env_with_tracking_uri():\n    return {**os.environ, \"MLFLOW_TRACKING_URI\": mlflow.get_tracking_uri()}\n\n\n@pytest.fixture(scope=\"module\")\ndef iris_data():\n    iris = sklearn.datasets.load_iris()\n    x = iris.data[:, :2]\n    y = iris.target\n    return x, y\n\n\n@pytest.fixture(scope=\"module\")\ndef sk_model(iris_data):\n    x, y = iris_data\n    knn_model = sklearn.neighbors.KNeighborsClassifier()\n    knn_model.fit(x, y)\n    return knn_model\n\n\n@pytest.mark.allow_infer_pip_requirements_fallback\ndef test_mlflow_is_not_installed_unless_specified():\n    if no_conda:\n        pytest.skip(\"This test requires conda.\")\n    with TempDir(chdr=True) as tmp:\n        fake_model_path = tmp.path(\"fake_model\")\n        mlflow.pyfunc.save_model(fake_model_path, loader_module=__name__)\n        # Overwrite the logged `conda.yaml` to remove mlflow.\n        _mlflow_conda_env(path=os.path.join(fake_model_path, \"conda.yaml\"), install_mlflow=False)\n        # The following should fail because there should be no mlflow in the env:\n        prc = subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                fake_model_path,\n                \"--env-manager\",\n                \"conda\",\n            ],\n            stderr=subprocess.PIPE,\n            cwd=tmp.path(\"\"),\n            check=False,\n            text=True,\n            env=env_with_tracking_uri(),\n        )\n        assert prc.returncode != 0\n        if PYTHON_VERSION.startswith(\"3\"):\n            assert \"ModuleNotFoundError: No module named 'mlflow'\" in prc.stderr\n        else:\n            assert \"ImportError: No module named mlflow.pyfunc.scoring_server\" in prc.stderr\n\n\ndef test_model_with_no_deployable_flavors_fails_pollitely():\n    from mlflow.models import Model\n\n    with TempDir(chdr=True) as tmp:\n        m = Model(\n            artifact_path=None,\n            run_id=None,\n            utc_time_created=\"now\",\n            flavors={\"some\": {}, \"useless\": {}, \"flavors\": {}},\n        )\n        os.mkdir(tmp.path(\"model\"))\n        m.save(tmp.path(\"model\", \"MLmodel\"))\n        # The following should fail because there should be no suitable flavor\n        prc = subprocess.run(\n            [\"mlflow\", \"models\", \"predict\", \"-m\", tmp.path(\"model\")],\n            stderr=subprocess.PIPE,\n            cwd=tmp.path(\"\"),\n            check=False,\n            text=True,\n            env=env_with_tracking_uri(),\n        )\n        assert \"No suitable flavor backend was found for the model.\" in prc.stderr\n\n\ndef test_serve_gunicorn_opts(iris_data, sk_model):\n    if sys.platform == \"win32\":\n        pytest.skip(\"This test requires gunicorn which is not available on windows.\")\n    with mlflow.start_run() as active_run:\n        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"imlegit\")\n        run_id = active_run.info.run_id\n\n    model_uris = [\n        \"models:/{name}/{stage}\".format(name=\"imlegit\", stage=\"None\"),\n        f\"runs:/{run_id}/model\",\n    ]\n    for model_uri in model_uris:\n        with TempDir() as tpm:\n            output_file_path = tpm.path(\"stoudt\")\n            with open(output_file_path, \"w\") as output_file:\n                x, _ = iris_data\n                scoring_response = pyfunc_serve_and_score_model(\n                    model_uri,\n                    pd.DataFrame(x),\n                    content_type=CONTENT_TYPE_JSON,\n                    stdout=output_file,\n                    extra_args=[\"-w\", \"3\"],\n                )\n            with open(output_file_path) as output_file:\n                stdout = output_file.read()\n        actual = pd.read_json(scoring_response.content.decode(\"utf-8\"), orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n        expected_command_pattern = re.compile(\n            \"gunicorn.*-w 3.*mlflow.pyfunc.scoring_server.wsgi:app\"\n        )\n        assert expected_command_pattern.search(stdout) is not None\n\n\ndef test_predict(iris_data, sk_model):\n    with TempDir(chdr=True) as tmp:\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n        model_registry_uri = \"models:/impredicting/None\"\n        input_json_path = tmp.path(\"input.json\")\n        input_csv_path = tmp.path(\"input.csv\")\n        output_json_path = tmp.path(\"output.json\")\n        x, _ = iris_data\n        with open(input_json_path, \"w\") as f:\n            json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n\n        pd.DataFrame(x).to_csv(input_csv_path, index=False)\n\n        # Test with no conda & model registry URI\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_registry_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"--env-manager\",\n                \"local\",\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # With conda + --install-mlflow\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # explicit json format with default orient (should be split)\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # explicit json format with orient==split\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # read from stdin, write to stdout.\n        prc = subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            input=Path(input_json_path).read_text(),\n            stdout=subprocess.PIPE,\n            env=env_with_tracking_uri(),\n            text=True,\n            check=True,\n        )\n        actual = pd.read_json(prc.stdout, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # NB: We do not test orient=records here because records may loose column ordering.\n        # orient == records is tested in other test with simpler model.\n\n        # csv\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_csv_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"csv\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n\ndef test_predict_check_content_type(iris_data, sk_model, tmp_path):\n    with mlflow.start_run():\n        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n    model_registry_uri = \"models:/impredicting/None\"\n    input_json_path = tmp_path / \"input.json\"\n    input_csv_path = tmp_path / \"input.csv\"\n    output_json_path = tmp_path / \"output.json\"\n\n    x, _ = iris_data\n    with input_json_path.open(\"w\") as f:\n        json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n\n    pd.DataFrame(x).to_csv(input_csv_path, index=False)\n\n    # Throw errors for invalid content_type\n    prc = subprocess.run(\n        [\n            \"mlflow\",\n            \"models\",\n            \"predict\",\n            \"-m\",\n            model_registry_uri,\n            \"-i\",\n            input_json_path,\n            \"-o\",\n            output_json_path,\n            \"-t\",\n            \"invalid\",\n            \"--env-manager\",\n            \"local\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env_with_tracking_uri(),\n        check=False,\n    )\n    assert prc.returncode != 0\n    assert \"Unknown content type\" in prc.stderr.decode(\"utf-8\")\n\n\ndef test_predict_check_input_path(iris_data, sk_model, tmp_path):\n    with mlflow.start_run():\n        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n    model_registry_uri = \"models:/impredicting/None\"\n    input_json_path = tmp_path / \"input with space.json\"\n    input_csv_path = tmp_path / \"input.csv\"\n    output_json_path = tmp_path / \"output.json\"\n\n    x, _ = iris_data\n    with input_json_path.open(\"w\") as f:\n        json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n\n    pd.DataFrame(x).to_csv(input_csv_path, index=False)\n\n    # Valid input path with space\n    prc = subprocess.run(\n        [\n            \"mlflow\",\n            \"models\",\n            \"predict\",\n            \"-m\",\n            model_registry_uri,\n            \"-i\",\n            f\"{input_json_path}\",\n            \"-o\",\n            output_json_path,\n            \"--env-manager\",\n            \"local\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env_with_tracking_uri(),\n        check=False,\n        text=True,\n    )\n    assert prc.returncode == 0\n\n    # Throw errors for invalid input_path\n    prc = subprocess.run(\n        [\n            \"mlflow\",\n            \"models\",\n            \"predict\",\n            \"-m\",\n            model_registry_uri,\n            \"-i\",\n            f'{input_json_path}\"; echo ThisIsABug! \"',\n            \"-o\",\n            output_json_path,\n            \"--env-manager\",\n            \"local\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env_with_tracking_uri(),\n        check=False,\n        text=True,\n    )\n    assert prc.returncode != 0\n    assert \"ThisIsABug!\" not in prc.stdout\n    assert \"FileNotFoundError\" in prc.stderr\n\n    prc = subprocess.run(\n        [\n            \"mlflow\",\n            \"models\",\n            \"predict\",\n            \"-m\",\n            model_registry_uri,\n            \"-i\",\n            f'{input_csv_path}\"; echo ThisIsABug! \"',\n            \"-o\",\n            output_json_path,\n            \"-t\",\n            \"csv\",\n            \"--env-manager\",\n            \"local\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env_with_tracking_uri(),\n        check=False,\n        text=True,\n    )\n    assert prc.returncode != 0\n    assert \"ThisIsABug!\" not in prc.stdout\n    assert \"FileNotFoundError\" in prc.stderr\n\n\ndef test_predict_check_output_path(iris_data, sk_model, tmp_path):\n    with mlflow.start_run():\n        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n    model_registry_uri = \"models:/impredicting/None\"\n    input_json_path = tmp_path / \"input.json\"\n    input_csv_path = tmp_path / \"input.csv\"\n    output_json_path = tmp_path / \"output.json\"\n\n    x, _ = iris_data\n    with input_json_path.open(\"w\") as f:\n        json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n\n    pd.DataFrame(x).to_csv(input_csv_path, index=False)\n\n    prc = subprocess.run(\n        [\n            \"mlflow\",\n            \"models\",\n            \"predict\",\n            \"-m\",\n            model_registry_uri,\n            \"-i\",\n            input_json_path,\n            \"-o\",\n            f'{output_json_path}\"; echo ThisIsABug! \"',\n            \"--env-manager\",\n            \"local\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env_with_tracking_uri(),\n        check=False,\n        text=True,\n    )\n    assert prc.returncode == 0\n    assert \"ThisIsABug!\" not in prc.stdout\n\n\ndef test_prepare_env_passes(sk_model):\n    if no_conda:\n        pytest.skip(\"This test requires conda.\")\n\n    with TempDir(chdr=True):\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(sk_model, \"model\")\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n\n        # With conda\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n\n        # Should be idempotent\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n\n\ndef test_prepare_env_fails(sk_model):\n    if no_conda:\n        pytest.skip(\"This test requires conda.\")\n\n    with TempDir(chdr=True):\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(\n                sk_model, \"model\", pip_requirements=[\"does-not-exist-dep==abc\"]\n            )\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n\n        # With conda - should fail due to bad conda environment.\n        prc = subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=False,\n        )\n        assert prc.returncode != 0\n\n\n@pytest.mark.parametrize(\"enable_mlserver\", [True, False])\ndef test_generate_dockerfile(sk_model, enable_mlserver, tmp_path):\n    with mlflow.start_run() as active_run:\n        if enable_mlserver:\n            mlflow.sklearn.log_model(\n                sk_model, \"model\", extra_pip_requirements=[PROTOBUF_REQUIREMENT]\n            )\n        else:\n            mlflow.sklearn.log_model(sk_model, \"model\")\n        model_uri = f\"runs:/{active_run.info.run_id}/model\"\n    extra_args = [\"--install-mlflow\"]\n    if enable_mlserver:\n        extra_args.append(\"--enable-mlserver\")\n\n    output_directory = tmp_path.joinpath(\"output_directory\")\n    pyfunc_generate_dockerfile(\n        output_directory,\n        model_uri,\n        extra_args=extra_args,\n        env=env_with_tracking_uri(),\n    )\n    assert output_directory.is_dir()\n    assert output_directory.joinpath(\"Dockerfile\").exists()\n    assert output_directory.joinpath(\"model_dir\").is_dir()\n    # Assert file is not empty\n    assert output_directory.joinpath(\"Dockerfile\").stat().st_size != 0\n\n\n@pytest.mark.parametrize(\"enable_mlserver\", [True, False])\ndef test_build_docker(iris_data, sk_model, enable_mlserver):\n    with mlflow.start_run() as active_run:\n        if enable_mlserver:\n            mlflow.sklearn.log_model(\n                sk_model, \"model\", extra_pip_requirements=[PROTOBUF_REQUIREMENT]\n            )\n        else:\n            mlflow.sklearn.log_model(sk_model, \"model\")\n        model_uri = f\"runs:/{active_run.info.run_id}/model\"\n\n    x, _ = iris_data\n    df = pd.DataFrame(x)\n\n    extra_args = [\"--install-mlflow\"]\n    if enable_mlserver:\n        extra_args.append(\"--enable-mlserver\")\n\n    image_name = pyfunc_build_image(\n        model_uri,\n        extra_args=extra_args,\n        env=env_with_tracking_uri(),\n    )\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image(image_name, host_port)\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model, enable_mlserver)\n\n\ndef test_build_docker_virtualenv(iris_data, sk_model):\n    with mlflow.start_run():\n        model_info = mlflow.sklearn.log_model(sk_model, \"model\")\n\n    x, _ = iris_data\n    df = pd.DataFrame(iris_data[0])\n\n    extra_args = [\"--install-mlflow\", \"--env-manager\", \"virtualenv\"]\n    image_name = pyfunc_build_image(\n        model_info.model_uri,\n        extra_args=extra_args,\n        env=env_with_tracking_uri(),\n    )\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image(image_name, host_port)\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model)\n\n\n@pytest.mark.parametrize(\"enable_mlserver\", [True, False])\ndef test_build_docker_with_env_override(iris_data, sk_model, enable_mlserver):\n    with mlflow.start_run() as active_run:\n        if enable_mlserver:\n            mlflow.sklearn.log_model(\n                sk_model, \"model\", extra_pip_requirements=[PROTOBUF_REQUIREMENT]\n            )\n        else:\n            mlflow.sklearn.log_model(sk_model, \"model\")\n        model_uri = f\"runs:/{active_run.info.run_id}/model\"\n    x, _ = iris_data\n    df = pd.DataFrame(x)\n\n    extra_args = [\"--install-mlflow\"]\n    if enable_mlserver:\n        extra_args.append(\"--enable-mlserver\")\n\n    image_name = pyfunc_build_image(\n        model_uri,\n        extra_args=extra_args,\n        env=env_with_tracking_uri(),\n    )\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image_with_env_override(\n        image_name, host_port, gunicorn_options\n    )\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model, enable_mlserver)\n\n\ndef test_build_docker_without_model_uri(iris_data, sk_model, tmp_path):\n    model_path = tmp_path.joinpath(\"model\")\n    mlflow.sklearn.save_model(sk_model, model_path)\n    image_name = pyfunc_build_image(model_uri=None)\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image_with_env_override(\n        image_name,\n        host_port,\n        gunicorn_options,\n        extra_docker_run_options=[\"-v\", f\"{model_path}:/opt/ml/model\"],\n    )\n    x = iris_data[0]\n    df = pd.DataFrame(x)\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model)\n\n\ndef _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model, enable_mlserver=False):\n    with RestEndpoint(proc=scoring_proc, port=host_port, validate_version=False) as endpoint:\n        for content_type in [CONTENT_TYPE_JSON, CONTENT_TYPE_CSV]:\n            scoring_response = endpoint.invoke(df, content_type)\n            assert scoring_response.status_code == 200, (\n                \"Failed to serve prediction, got response %s\" % scoring_response.text\n            )\n            np.testing.assert_array_equal(\n                np.array(json.loads(scoring_response.text)[\"predictions\"]), sk_model.predict(x)\n            )\n        # Try examples of bad input, verify we get a non-200 status code\n        for content_type in [CONTENT_TYPE_JSON, CONTENT_TYPE_CSV, CONTENT_TYPE_JSON]:\n            scoring_response = endpoint.invoke(data=\"\", content_type=content_type)\n            expected_status_code = 500 if enable_mlserver else 400\n            assert scoring_response.status_code == expected_status_code, (\n                f\"Expected server failure with error code {expected_status_code}, \"\n                f\"got response with status code {scoring_response.status_code} \"\n                f\"and body {scoring_response.text}\"\n            )\n\n            if enable_mlserver:\n                # MLServer returns a different set of errors.\n                # Skip these assertions until this issue gets tackled:\n                # https://github.com/SeldonIO/MLServer/issues/360)\n                continue\n\n            scoring_response_dict = json.loads(scoring_response.content)\n            assert \"error_code\" in scoring_response_dict\n            assert scoring_response_dict[\"error_code\"] == ErrorCode.Name(BAD_REQUEST)\n            assert \"message\" in scoring_response_dict\n\n\ndef test_env_manager_warning_for_use_of_conda(monkeypatch):\n    with mock.patch(\"mlflow.models.cli.get_flavor_backend\") as mock_get_flavor_backend:\n        with pytest.warns(UserWarning, match=r\"Use of conda is discouraged\"):\n            CliRunner().invoke(\n                models_cli.serve,\n                [\"--model-uri\", \"model\", \"--env-manager\", \"conda\"],\n                catch_exceptions=False,\n            )\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            monkeypatch.setenv(MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING.name, \"TRUE\")\n            CliRunner().invoke(\n                models_cli.serve,\n                [\"--model-uri\", \"model\", \"--env-manager\", \"conda\"],\n                catch_exceptions=False,\n            )\n\n        assert mock_get_flavor_backend.call_count == 2\n\n\ndef test_env_manager_unsupported_value():\n    with pytest.raises(MlflowException, match=r\"Invalid value for `env_manager`\"):\n        CliRunner().invoke(\n            models_cli.serve,\n            [\"--model-uri\", \"model\", \"--env-manager\", \"abc\"],\n            catch_exceptions=False,\n        )\n\n\ndef test_host_invalid_value():\n    class MyModel(mlflow.pyfunc.PythonModel):\n        def predict(self, ctx, model_input):\n            return model_input\n\n    with mlflow.start_run():\n        model_info = mlflow.pyfunc.log_model(\n            python_model=MyModel(), artifact_path=\"test_model\", registered_model_name=\"model\"\n        )\n\n    with mock.patch(\"mlflow.models.cli.get_flavor_backend\", return_value=PyFuncBackend({})):\n        with pytest.raises(ShellCommandException, match=r\"Non-zero exit code: 1\"):\n            CliRunner().invoke(\n                models_cli.serve,\n                [\"--model-uri\", model_info.model_uri, \"--host\", \"localhost & echo BUG\"],\n                catch_exceptions=False,\n            )\n\n\ndef test_change_conda_env_root_location(tmp_path, sk_model):\n    env_root1_path = tmp_path / \"root1\"\n    env_root1_path.mkdir()\n\n    env_root2_path = tmp_path / \"root2\"\n    env_root2_path.mkdir()\n\n    model1_path = tmp_path / \"model1\"\n    mlflow.sklearn.save_model(sk_model, str(model1_path), pip_requirements=[\"scikit-learn==1.0.1\"])\n\n    model2_path = tmp_path / \"model2\"\n    mlflow.sklearn.save_model(sk_model, str(model2_path), pip_requirements=[\"scikit-learn==1.0.2\"])\n\n    env_path_set = set()\n    for env_root_path, model_path, sklearn_ver in [\n        (env_root1_path, model1_path, \"1.0.1\"),\n        (\n            env_root2_path,\n            model1_path,\n            \"1.0.1\",\n        ),  # test the same env created in different env root path.\n        (\n            env_root1_path,\n            model2_path,\n            \"1.0.2\",\n        ),  # test different env created in the same env root path.\n    ]:\n        env = get_flavor_backend(\n            str(model_path),\n            env_manager=_EnvManager.CONDA,\n            install_mlflow=False,\n            env_root_dir=str(env_root_path),\n        ).prepare_env(model_uri=str(model_path))\n\n        conda_env_name = _get_conda_env_name(\n            str(model_path / \"conda.yaml\"), env_root_dir=env_root_path\n        )\n        env_path = env_root_path / \"conda_envs\" / conda_env_name\n        assert env_path.exists()\n        env_path_set.add(str(env_path))\n\n        python_exec_path = str(env_path / \"bin\" / \"python\")\n\n        # Test execution of command under the correct activated python env.\n        env.execute(\n            command=f\"python -c \\\"import sys; assert sys.executable == '{python_exec_path}'; \"\n            f\"import sklearn; assert sklearn.__version__ == '{sklearn_ver}'\\\"\",\n        )\n\n    assert len(env_path_set) == 3\n", "code_before": "import json\nimport os\nimport subprocess\nimport sys\nimport warnings\nfrom pathlib import Path\n\nfrom click.testing import CliRunner\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport re\nimport sklearn\nimport sklearn.datasets\nimport sklearn.neighbors\n\nfrom unittest import mock\n\n\nimport mlflow\nimport mlflow.sklearn\nfrom mlflow.models.flavor_backend_registry import get_flavor_backend\n\nfrom mlflow.utils.conda import _get_conda_env_name\n\nimport mlflow.models.cli as models_cli\n\nfrom mlflow.environment_variables import MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.protos.databricks_pb2 import ErrorCode, BAD_REQUEST\nfrom mlflow.pyfunc.scoring_server import (\n    CONTENT_TYPE_JSON,\n    CONTENT_TYPE_CSV,\n)\nfrom mlflow.utils.file_utils import TempDir\nfrom mlflow.utils.environment import _mlflow_conda_env\nfrom mlflow.utils import env_manager as _EnvManager\nfrom mlflow.utils import PYTHON_VERSION\nfrom tests.helper_functions import (\n    pyfunc_build_image,\n    pyfunc_serve_from_docker_image,\n    pyfunc_serve_from_docker_image_with_env_override,\n    RestEndpoint,\n    get_safe_port,\n    pyfunc_serve_and_score_model,\n    PROTOBUF_REQUIREMENT,\n    pyfunc_generate_dockerfile,\n)\n\n# NB: for now, windows tests do not have conda available.\nno_conda = [\"--env-manager\", \"local\"] if sys.platform == \"win32\" else []\n\n# NB: need to install mlflow since the pip version does not have mlflow models cli.\ninstall_mlflow = [\"--install-mlflow\"] if not no_conda else []\n\nextra_options = no_conda + install_mlflow\ngunicorn_options = \"--timeout 60 -w 5\"\n\n\ndef env_with_tracking_uri():\n    return {**os.environ, \"MLFLOW_TRACKING_URI\": mlflow.get_tracking_uri()}\n\n\n@pytest.fixture(scope=\"module\")\ndef iris_data():\n    iris = sklearn.datasets.load_iris()\n    x = iris.data[:, :2]\n    y = iris.target\n    return x, y\n\n\n@pytest.fixture(scope=\"module\")\ndef sk_model(iris_data):\n    x, y = iris_data\n    knn_model = sklearn.neighbors.KNeighborsClassifier()\n    knn_model.fit(x, y)\n    return knn_model\n\n\n@pytest.mark.allow_infer_pip_requirements_fallback\ndef test_mlflow_is_not_installed_unless_specified():\n    if no_conda:\n        pytest.skip(\"This test requires conda.\")\n    with TempDir(chdr=True) as tmp:\n        fake_model_path = tmp.path(\"fake_model\")\n        mlflow.pyfunc.save_model(fake_model_path, loader_module=__name__)\n        # Overwrite the logged `conda.yaml` to remove mlflow.\n        _mlflow_conda_env(path=os.path.join(fake_model_path, \"conda.yaml\"), install_mlflow=False)\n        # The following should fail because there should be no mlflow in the env:\n        prc = subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                fake_model_path,\n                \"--env-manager\",\n                \"conda\",\n            ],\n            stderr=subprocess.PIPE,\n            cwd=tmp.path(\"\"),\n            check=False,\n            text=True,\n            env=env_with_tracking_uri(),\n        )\n        assert prc.returncode != 0\n        if PYTHON_VERSION.startswith(\"3\"):\n            assert \"ModuleNotFoundError: No module named 'mlflow'\" in prc.stderr\n        else:\n            assert \"ImportError: No module named mlflow.pyfunc.scoring_server\" in prc.stderr\n\n\ndef test_model_with_no_deployable_flavors_fails_pollitely():\n    from mlflow.models import Model\n\n    with TempDir(chdr=True) as tmp:\n        m = Model(\n            artifact_path=None,\n            run_id=None,\n            utc_time_created=\"now\",\n            flavors={\"some\": {}, \"useless\": {}, \"flavors\": {}},\n        )\n        os.mkdir(tmp.path(\"model\"))\n        m.save(tmp.path(\"model\", \"MLmodel\"))\n        # The following should fail because there should be no suitable flavor\n        prc = subprocess.run(\n            [\"mlflow\", \"models\", \"predict\", \"-m\", tmp.path(\"model\")],\n            stderr=subprocess.PIPE,\n            cwd=tmp.path(\"\"),\n            check=False,\n            text=True,\n            env=env_with_tracking_uri(),\n        )\n        assert \"No suitable flavor backend was found for the model.\" in prc.stderr\n\n\ndef test_serve_gunicorn_opts(iris_data, sk_model):\n    if sys.platform == \"win32\":\n        pytest.skip(\"This test requires gunicorn which is not available on windows.\")\n    with mlflow.start_run() as active_run:\n        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"imlegit\")\n        run_id = active_run.info.run_id\n\n    model_uris = [\n        \"models:/{name}/{stage}\".format(name=\"imlegit\", stage=\"None\"),\n        f\"runs:/{run_id}/model\",\n    ]\n    for model_uri in model_uris:\n        with TempDir() as tpm:\n            output_file_path = tpm.path(\"stoudt\")\n            with open(output_file_path, \"w\") as output_file:\n                x, _ = iris_data\n                scoring_response = pyfunc_serve_and_score_model(\n                    model_uri,\n                    pd.DataFrame(x),\n                    content_type=CONTENT_TYPE_JSON,\n                    stdout=output_file,\n                    extra_args=[\"-w\", \"3\"],\n                )\n            with open(output_file_path) as output_file:\n                stdout = output_file.read()\n        actual = pd.read_json(scoring_response.content.decode(\"utf-8\"), orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n        expected_command_pattern = re.compile(\n            \"gunicorn.*-w 3.*mlflow.pyfunc.scoring_server.wsgi:app\"\n        )\n        assert expected_command_pattern.search(stdout) is not None\n\n\ndef test_predict(iris_data, sk_model):\n    with TempDir(chdr=True) as tmp:\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n        model_registry_uri = \"models:/{name}/{stage}\".format(name=\"impredicting\", stage=\"None\")\n        input_json_path = tmp.path(\"input.json\")\n        input_csv_path = tmp.path(\"input.csv\")\n        output_json_path = tmp.path(\"output.json\")\n        x, _ = iris_data\n        with open(input_json_path, \"w\") as f:\n            json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n\n        pd.DataFrame(x).to_csv(input_csv_path, index=False)\n\n        # Test with no conda & model registry URI\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_registry_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"--env-manager\",\n                \"local\",\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # With conda + --install-mlflow\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # explicit json format with default orient (should be split)\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # explicit json format with orient==split\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # read from stdin, write to stdout.\n        prc = subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            input=Path(input_json_path).read_text(),\n            stdout=subprocess.PIPE,\n            env=env_with_tracking_uri(),\n            text=True,\n            check=True,\n        )\n        actual = pd.read_json(prc.stdout, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # NB: We do not test orient=records here because records may loose column ordering.\n        # orient == records is tested in other test with simpler model.\n\n        # csv\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_csv_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"csv\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n\ndef test_prepare_env_passes(sk_model):\n    if no_conda:\n        pytest.skip(\"This test requires conda.\")\n\n    with TempDir(chdr=True):\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(sk_model, \"model\")\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n\n        # With conda\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n\n        # Should be idempotent\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n\n\ndef test_prepare_env_fails(sk_model):\n    if no_conda:\n        pytest.skip(\"This test requires conda.\")\n\n    with TempDir(chdr=True):\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(\n                sk_model, \"model\", pip_requirements=[\"does-not-exist-dep==abc\"]\n            )\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n\n        # With conda - should fail due to bad conda environment.\n        prc = subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=False,\n        )\n        assert prc.returncode != 0\n\n\n@pytest.mark.parametrize(\"enable_mlserver\", [True, False])\ndef test_generate_dockerfile(sk_model, enable_mlserver, tmp_path):\n    with mlflow.start_run() as active_run:\n        if enable_mlserver:\n            mlflow.sklearn.log_model(\n                sk_model, \"model\", extra_pip_requirements=[PROTOBUF_REQUIREMENT]\n            )\n        else:\n            mlflow.sklearn.log_model(sk_model, \"model\")\n        model_uri = f\"runs:/{active_run.info.run_id}/model\"\n    extra_args = [\"--install-mlflow\"]\n    if enable_mlserver:\n        extra_args.append(\"--enable-mlserver\")\n\n    output_directory = tmp_path.joinpath(\"output_directory\")\n    pyfunc_generate_dockerfile(\n        output_directory,\n        model_uri,\n        extra_args=extra_args,\n        env=env_with_tracking_uri(),\n    )\n    assert output_directory.is_dir()\n    assert output_directory.joinpath(\"Dockerfile\").exists()\n    assert output_directory.joinpath(\"model_dir\").is_dir()\n    # Assert file is not empty\n    assert output_directory.joinpath(\"Dockerfile\").stat().st_size != 0\n\n\n@pytest.mark.parametrize(\"enable_mlserver\", [True, False])\ndef test_build_docker(iris_data, sk_model, enable_mlserver):\n    with mlflow.start_run() as active_run:\n        if enable_mlserver:\n            mlflow.sklearn.log_model(\n                sk_model, \"model\", extra_pip_requirements=[PROTOBUF_REQUIREMENT]\n            )\n        else:\n            mlflow.sklearn.log_model(sk_model, \"model\")\n        model_uri = f\"runs:/{active_run.info.run_id}/model\"\n\n    x, _ = iris_data\n    df = pd.DataFrame(x)\n\n    extra_args = [\"--install-mlflow\"]\n    if enable_mlserver:\n        extra_args.append(\"--enable-mlserver\")\n\n    image_name = pyfunc_build_image(\n        model_uri,\n        extra_args=extra_args,\n        env=env_with_tracking_uri(),\n    )\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image(image_name, host_port)\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model, enable_mlserver)\n\n\ndef test_build_docker_virtualenv(iris_data, sk_model):\n    with mlflow.start_run():\n        model_info = mlflow.sklearn.log_model(sk_model, \"model\")\n\n    x, _ = iris_data\n    df = pd.DataFrame(iris_data[0])\n\n    extra_args = [\"--install-mlflow\", \"--env-manager\", \"virtualenv\"]\n    image_name = pyfunc_build_image(\n        model_info.model_uri,\n        extra_args=extra_args,\n        env=env_with_tracking_uri(),\n    )\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image(image_name, host_port)\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model)\n\n\n@pytest.mark.parametrize(\"enable_mlserver\", [True, False])\ndef test_build_docker_with_env_override(iris_data, sk_model, enable_mlserver):\n    with mlflow.start_run() as active_run:\n        if enable_mlserver:\n            mlflow.sklearn.log_model(\n                sk_model, \"model\", extra_pip_requirements=[PROTOBUF_REQUIREMENT]\n            )\n        else:\n            mlflow.sklearn.log_model(sk_model, \"model\")\n        model_uri = f\"runs:/{active_run.info.run_id}/model\"\n    x, _ = iris_data\n    df = pd.DataFrame(x)\n\n    extra_args = [\"--install-mlflow\"]\n    if enable_mlserver:\n        extra_args.append(\"--enable-mlserver\")\n\n    image_name = pyfunc_build_image(\n        model_uri,\n        extra_args=extra_args,\n        env=env_with_tracking_uri(),\n    )\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image_with_env_override(\n        image_name, host_port, gunicorn_options\n    )\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model, enable_mlserver)\n\n\ndef test_build_docker_without_model_uri(iris_data, sk_model, tmp_path):\n    model_path = tmp_path.joinpath(\"model\")\n    mlflow.sklearn.save_model(sk_model, model_path)\n    image_name = pyfunc_build_image(model_uri=None)\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image_with_env_override(\n        image_name,\n        host_port,\n        gunicorn_options,\n        extra_docker_run_options=[\"-v\", f\"{model_path}:/opt/ml/model\"],\n    )\n    x = iris_data[0]\n    df = pd.DataFrame(x)\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model)\n\n\ndef _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model, enable_mlserver=False):\n    with RestEndpoint(proc=scoring_proc, port=host_port, validate_version=False) as endpoint:\n        for content_type in [CONTENT_TYPE_JSON, CONTENT_TYPE_CSV]:\n            scoring_response = endpoint.invoke(df, content_type)\n            assert scoring_response.status_code == 200, (\n                \"Failed to serve prediction, got response %s\" % scoring_response.text\n            )\n            np.testing.assert_array_equal(\n                np.array(json.loads(scoring_response.text)[\"predictions\"]), sk_model.predict(x)\n            )\n        # Try examples of bad input, verify we get a non-200 status code\n        for content_type in [CONTENT_TYPE_JSON, CONTENT_TYPE_CSV, CONTENT_TYPE_JSON]:\n            scoring_response = endpoint.invoke(data=\"\", content_type=content_type)\n            expected_status_code = 500 if enable_mlserver else 400\n            assert scoring_response.status_code == expected_status_code, (\n                f\"Expected server failure with error code {expected_status_code}, \"\n                f\"got response with status code {scoring_response.status_code} \"\n                f\"and body {scoring_response.text}\"\n            )\n\n            if enable_mlserver:\n                # MLServer returns a different set of errors.\n                # Skip these assertions until this issue gets tackled:\n                # https://github.com/SeldonIO/MLServer/issues/360)\n                continue\n\n            scoring_response_dict = json.loads(scoring_response.content)\n            assert \"error_code\" in scoring_response_dict\n            assert scoring_response_dict[\"error_code\"] == ErrorCode.Name(BAD_REQUEST)\n            assert \"message\" in scoring_response_dict\n\n\ndef test_env_manager_warning_for_use_of_conda(monkeypatch):\n    with mock.patch(\"mlflow.models.cli.get_flavor_backend\") as mock_get_flavor_backend:\n        with pytest.warns(UserWarning, match=r\"Use of conda is discouraged\"):\n            CliRunner().invoke(\n                models_cli.serve,\n                [\"--model-uri\", \"model\", \"--env-manager\", \"conda\"],\n                catch_exceptions=False,\n            )\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            monkeypatch.setenv(MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING.name, \"TRUE\")\n            CliRunner().invoke(\n                models_cli.serve,\n                [\"--model-uri\", \"model\", \"--env-manager\", \"conda\"],\n                catch_exceptions=False,\n            )\n\n        assert mock_get_flavor_backend.call_count == 2\n\n\ndef test_env_manager_unsupported_value():\n    with pytest.raises(MlflowException, match=r\"Invalid value for `env_manager`\"):\n        CliRunner().invoke(\n            models_cli.serve,\n            [\"--model-uri\", \"model\", \"--env-manager\", \"abc\"],\n            catch_exceptions=False,\n        )\n\n\ndef test_change_conda_env_root_location(tmp_path, sk_model):\n    env_root1_path = tmp_path / \"root1\"\n    env_root1_path.mkdir()\n\n    env_root2_path = tmp_path / \"root2\"\n    env_root2_path.mkdir()\n\n    model1_path = tmp_path / \"model1\"\n    mlflow.sklearn.save_model(sk_model, str(model1_path), pip_requirements=[\"scikit-learn==1.0.1\"])\n\n    model2_path = tmp_path / \"model2\"\n    mlflow.sklearn.save_model(sk_model, str(model2_path), pip_requirements=[\"scikit-learn==1.0.2\"])\n\n    env_path_set = set()\n    for env_root_path, model_path, sklearn_ver in [\n        (env_root1_path, model1_path, \"1.0.1\"),\n        (\n            env_root2_path,\n            model1_path,\n            \"1.0.1\",\n        ),  # test the same env created in different env root path.\n        (\n            env_root1_path,\n            model2_path,\n            \"1.0.2\",\n        ),  # test different env created in the same env root path.\n    ]:\n        env = get_flavor_backend(\n            str(model_path),\n            env_manager=_EnvManager.CONDA,\n            install_mlflow=False,\n            env_root_dir=str(env_root_path),\n        ).prepare_env(model_uri=str(model_path))\n\n        conda_env_name = _get_conda_env_name(\n            str(model_path / \"conda.yaml\"), env_root_dir=env_root_path\n        )\n        env_path = env_root_path / \"conda_envs\" / conda_env_name\n        assert env_path.exists()\n        env_path_set.add(str(env_path))\n\n        python_exec_path = str(env_path / \"bin\" / \"python\")\n\n        # Test execution of command under the correct activated python env.\n        env.execute(\n            command=f\"python -c \\\"import sys; assert sys.executable == '{python_exec_path}'; \"\n            f\"import sklearn; assert sklearn.__version__ == '{sklearn_ver}'\\\"\",\n        )\n\n    assert len(env_path_set) == 3\n", "patch": "@@ -28,6 +28,7 @@\n from mlflow.environment_variables import MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING\n from mlflow.exceptions import MlflowException\n from mlflow.protos.databricks_pb2 import ErrorCode, BAD_REQUEST\n+from mlflow.pyfunc.backend import PyFuncBackend\n from mlflow.pyfunc.scoring_server import (\n     CONTENT_TYPE_JSON,\n     CONTENT_TYPE_CSV,\n@@ -36,6 +37,7 @@\n from mlflow.utils.environment import _mlflow_conda_env\n from mlflow.utils import env_manager as _EnvManager\n from mlflow.utils import PYTHON_VERSION\n+from mlflow.utils.process import ShellCommandException\n from tests.helper_functions import (\n     pyfunc_build_image,\n     pyfunc_serve_from_docker_image,\n@@ -174,7 +176,7 @@ def test_predict(iris_data, sk_model):\n         with mlflow.start_run() as active_run:\n             mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n             model_uri = f\"runs:/{active_run.info.run_id}/model\"\n-        model_registry_uri = \"models:/{name}/{stage}\".format(name=\"impredicting\", stage=\"None\")\n+        model_registry_uri = \"models:/impredicting/None\"\n         input_json_path = tmp.path(\"input.json\")\n         input_csv_path = tmp.path(\"input.csv\")\n         output_json_path = tmp.path(\"output.json\")\n@@ -331,6 +333,173 @@ def test_predict(iris_data, sk_model):\n         assert all(expected == actual)\n \n \n+def test_predict_check_content_type(iris_data, sk_model, tmp_path):\n+    with mlflow.start_run():\n+        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n+    model_registry_uri = \"models:/impredicting/None\"\n+    input_json_path = tmp_path / \"input.json\"\n+    input_csv_path = tmp_path / \"input.csv\"\n+    output_json_path = tmp_path / \"output.json\"\n+\n+    x, _ = iris_data\n+    with input_json_path.open(\"w\") as f:\n+        json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n+\n+    pd.DataFrame(x).to_csv(input_csv_path, index=False)\n+\n+    # Throw errors for invalid content_type\n+    prc = subprocess.run(\n+        [\n+            \"mlflow\",\n+            \"models\",\n+            \"predict\",\n+            \"-m\",\n+            model_registry_uri,\n+            \"-i\",\n+            input_json_path,\n+            \"-o\",\n+            output_json_path,\n+            \"-t\",\n+            \"invalid\",\n+            \"--env-manager\",\n+            \"local\",\n+        ],\n+        stdout=subprocess.PIPE,\n+        stderr=subprocess.PIPE,\n+        env=env_with_tracking_uri(),\n+        check=False,\n+    )\n+    assert prc.returncode != 0\n+    assert \"Unknown content type\" in prc.stderr.decode(\"utf-8\")\n+\n+\n+def test_predict_check_input_path(iris_data, sk_model, tmp_path):\n+    with mlflow.start_run():\n+        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n+    model_registry_uri = \"models:/impredicting/None\"\n+    input_json_path = tmp_path / \"input with space.json\"\n+    input_csv_path = tmp_path / \"input.csv\"\n+    output_json_path = tmp_path / \"output.json\"\n+\n+    x, _ = iris_data\n+    with input_json_path.open(\"w\") as f:\n+        json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n+\n+    pd.DataFrame(x).to_csv(input_csv_path, index=False)\n+\n+    # Valid input path with space\n+    prc = subprocess.run(\n+        [\n+            \"mlflow\",\n+            \"models\",\n+            \"predict\",\n+            \"-m\",\n+            model_registry_uri,\n+            \"-i\",\n+            f\"{input_json_path}\",\n+            \"-o\",\n+            output_json_path,\n+            \"--env-manager\",\n+            \"local\",\n+        ],\n+        stdout=subprocess.PIPE,\n+        stderr=subprocess.PIPE,\n+        env=env_with_tracking_uri(),\n+        check=False,\n+        text=True,\n+    )\n+    assert prc.returncode == 0\n+\n+    # Throw errors for invalid input_path\n+    prc = subprocess.run(\n+        [\n+            \"mlflow\",\n+            \"models\",\n+            \"predict\",\n+            \"-m\",\n+            model_registry_uri,\n+            \"-i\",\n+            f'{input_json_path}\"; echo ThisIsABug! \"',\n+            \"-o\",\n+            output_json_path,\n+            \"--env-manager\",\n+            \"local\",\n+        ],\n+        stdout=subprocess.PIPE,\n+        stderr=subprocess.PIPE,\n+        env=env_with_tracking_uri(),\n+        check=False,\n+        text=True,\n+    )\n+    assert prc.returncode != 0\n+    assert \"ThisIsABug!\" not in prc.stdout\n+    assert \"FileNotFoundError\" in prc.stderr\n+\n+    prc = subprocess.run(\n+        [\n+            \"mlflow\",\n+            \"models\",\n+            \"predict\",\n+            \"-m\",\n+            model_registry_uri,\n+            \"-i\",\n+            f'{input_csv_path}\"; echo ThisIsABug! \"',\n+            \"-o\",\n+            output_json_path,\n+            \"-t\",\n+            \"csv\",\n+            \"--env-manager\",\n+            \"local\",\n+        ],\n+        stdout=subprocess.PIPE,\n+        stderr=subprocess.PIPE,\n+        env=env_with_tracking_uri(),\n+        check=False,\n+        text=True,\n+    )\n+    assert prc.returncode != 0\n+    assert \"ThisIsABug!\" not in prc.stdout\n+    assert \"FileNotFoundError\" in prc.stderr\n+\n+\n+def test_predict_check_output_path(iris_data, sk_model, tmp_path):\n+    with mlflow.start_run():\n+        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n+    model_registry_uri = \"models:/impredicting/None\"\n+    input_json_path = tmp_path / \"input.json\"\n+    input_csv_path = tmp_path / \"input.csv\"\n+    output_json_path = tmp_path / \"output.json\"\n+\n+    x, _ = iris_data\n+    with input_json_path.open(\"w\") as f:\n+        json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n+\n+    pd.DataFrame(x).to_csv(input_csv_path, index=False)\n+\n+    prc = subprocess.run(\n+        [\n+            \"mlflow\",\n+            \"models\",\n+            \"predict\",\n+            \"-m\",\n+            model_registry_uri,\n+            \"-i\",\n+            input_json_path,\n+            \"-o\",\n+            f'{output_json_path}\"; echo ThisIsABug! \"',\n+            \"--env-manager\",\n+            \"local\",\n+        ],\n+        stdout=subprocess.PIPE,\n+        stderr=subprocess.PIPE,\n+        env=env_with_tracking_uri(),\n+        check=False,\n+        text=True,\n+    )\n+    assert prc.returncode == 0\n+    assert \"ThisIsABug!\" not in prc.stdout\n+\n+\n def test_prepare_env_passes(sk_model):\n     if no_conda:\n         pytest.skip(\"This test requires conda.\")\n@@ -574,6 +743,25 @@ def test_env_manager_unsupported_value():\n         )\n \n \n+def test_host_invalid_value():\n+    class MyModel(mlflow.pyfunc.PythonModel):\n+        def predict(self, ctx, model_input):\n+            return model_input\n+\n+    with mlflow.start_run():\n+        model_info = mlflow.pyfunc.log_model(\n+            python_model=MyModel(), artifact_path=\"test_model\", registered_model_name=\"model\"\n+        )\n+\n+    with mock.patch(\"mlflow.models.cli.get_flavor_backend\", return_value=PyFuncBackend({})):\n+        with pytest.raises(ShellCommandException, match=r\"Non-zero exit code: 1\"):\n+            CliRunner().invoke(\n+                models_cli.serve,\n+                [\"--model-uri\", model_info.model_uri, \"--host\", \"localhost & echo BUG\"],\n+                catch_exceptions=False,\n+            )\n+\n+\n def test_change_conda_env_root_location(tmp_path, sk_model):\n     env_root1_path = tmp_path / \"root1\"\n     env_root1_path.mkdir()", "file_path": "files/2023_7/4", "file_language": "py", "file_name": "tests/models/test_cli.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "def env_with_tracking_uri():\n    return {**os.environ, \"MLFLOW_TRACKING_URI\": mlflow.get_tracking_uri()}", "target": 0}, {"function": "def test_model_with_no_deployable_flavors_fails_pollitely():\n    from mlflow.models import Model\n\n    with TempDir(chdr=True) as tmp:\n        m = Model(\n            artifact_path=None,\n            run_id=None,\n            utc_time_created=\"now\",\n            flavors={\"some\": {}, \"useless\": {}, \"flavors\": {}},\n        )\n        os.mkdir(tmp.path(\"model\"))\n        m.save(tmp.path(\"model\", \"MLmodel\"))\n        # The following should fail because there should be no suitable flavor\n        prc = subprocess.run(\n            [\"mlflow\", \"models\", \"predict\", \"-m\", tmp.path(\"model\")],\n            stderr=subprocess.PIPE,\n            cwd=tmp.path(\"\"),\n            check=False,\n            text=True,\n            env=env_with_tracking_uri(),\n        )\n        assert \"No suitable flavor backend was found for the model.\" in prc.stderr", "target": 0}, {"function": "def test_serve_gunicorn_opts(iris_data, sk_model):\n    if sys.platform == \"win32\":\n        pytest.skip(\"This test requires gunicorn which is not available on windows.\")\n    with mlflow.start_run() as active_run:\n        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"imlegit\")\n        run_id = active_run.info.run_id\n\n    model_uris = [\n        \"models:/{name}/{stage}\".format(name=\"imlegit\", stage=\"None\"),\n        f\"runs:/{run_id}/model\",\n    ]\n    for model_uri in model_uris:\n        with TempDir() as tpm:\n            output_file_path = tpm.path(\"stoudt\")\n            with open(output_file_path, \"w\") as output_file:\n                x, _ = iris_data\n                scoring_response = pyfunc_serve_and_score_model(\n                    model_uri,\n                    pd.DataFrame(x),\n                    content_type=CONTENT_TYPE_JSON,\n                    stdout=output_file,\n                    extra_args=[\"-w\", \"3\"],\n                )\n            with open(output_file_path) as output_file:\n                stdout = output_file.read()\n        actual = pd.read_json(scoring_response.content.decode(\"utf-8\"), orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n        expected_command_pattern = re.compile(\n            \"gunicorn.*-w 3.*mlflow.pyfunc.scoring_server.wsgi:app\"\n        )\n        assert expected_command_pattern.search(stdout) is not None", "target": 0}, {"function": "def test_predict(iris_data, sk_model):\n    with TempDir(chdr=True) as tmp:\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n        model_registry_uri = \"models:/{name}/{stage}\".format(name=\"impredicting\", stage=\"None\")\n        input_json_path = tmp.path(\"input.json\")\n        input_csv_path = tmp.path(\"input.csv\")\n        output_json_path = tmp.path(\"output.json\")\n        x, _ = iris_data\n        with open(input_json_path, \"w\") as f:\n            json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n\n        pd.DataFrame(x).to_csv(input_csv_path, index=False)\n\n        # Test with no conda & model registry URI\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_registry_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"--env-manager\",\n                \"local\",\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # With conda + --install-mlflow\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # explicit json format with default orient (should be split)\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # explicit json format with orient==split\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # read from stdin, write to stdout.\n        prc = subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            input=Path(input_json_path).read_text(),\n            stdout=subprocess.PIPE,\n            env=env_with_tracking_uri(),\n            text=True,\n            check=True,\n        )\n        actual = pd.read_json(prc.stdout, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # NB: We do not test orient=records here because records may loose column ordering.\n        # orient == records is tested in other test with simpler model.\n\n        # csv\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_csv_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"csv\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)", "target": 0}, {"function": "def test_prepare_env_passes(sk_model):\n    if no_conda:\n        pytest.skip(\"This test requires conda.\")\n\n    with TempDir(chdr=True):\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(sk_model, \"model\")\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n\n        # With conda\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n\n        # Should be idempotent\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )", "target": 0}, {"function": "def test_prepare_env_fails(sk_model):\n    if no_conda:\n        pytest.skip(\"This test requires conda.\")\n\n    with TempDir(chdr=True):\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(\n                sk_model, \"model\", pip_requirements=[\"does-not-exist-dep==abc\"]\n            )\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n\n        # With conda - should fail due to bad conda environment.\n        prc = subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=False,\n        )\n        assert prc.returncode != 0", "target": 0}, {"function": "def test_build_docker_virtualenv(iris_data, sk_model):\n    with mlflow.start_run():\n        model_info = mlflow.sklearn.log_model(sk_model, \"model\")\n\n    x, _ = iris_data\n    df = pd.DataFrame(iris_data[0])\n\n    extra_args = [\"--install-mlflow\", \"--env-manager\", \"virtualenv\"]\n    image_name = pyfunc_build_image(\n        model_info.model_uri,\n        extra_args=extra_args,\n        env=env_with_tracking_uri(),\n    )\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image(image_name, host_port)\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model)", "target": 0}, {"function": "def test_build_docker_without_model_uri(iris_data, sk_model, tmp_path):\n    model_path = tmp_path.joinpath(\"model\")\n    mlflow.sklearn.save_model(sk_model, model_path)\n    image_name = pyfunc_build_image(model_uri=None)\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image_with_env_override(\n        image_name,\n        host_port,\n        gunicorn_options,\n        extra_docker_run_options=[\"-v\", f\"{model_path}:/opt/ml/model\"],\n    )\n    x = iris_data[0]\n    df = pd.DataFrame(x)\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model)", "target": 0}, {"function": "def _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model, enable_mlserver=False):\n    with RestEndpoint(proc=scoring_proc, port=host_port, validate_version=False) as endpoint:\n        for content_type in [CONTENT_TYPE_JSON, CONTENT_TYPE_CSV]:\n            scoring_response = endpoint.invoke(df, content_type)\n            assert scoring_response.status_code == 200, (\n                \"Failed to serve prediction, got response %s\" % scoring_response.text\n            )\n            np.testing.assert_array_equal(\n                np.array(json.loads(scoring_response.text)[\"predictions\"]), sk_model.predict(x)\n            )\n        # Try examples of bad input, verify we get a non-200 status code\n        for content_type in [CONTENT_TYPE_JSON, CONTENT_TYPE_CSV, CONTENT_TYPE_JSON]:\n            scoring_response = endpoint.invoke(data=\"\", content_type=content_type)\n            expected_status_code = 500 if enable_mlserver else 400\n            assert scoring_response.status_code == expected_status_code, (\n                f\"Expected server failure with error code {expected_status_code}, \"\n                f\"got response with status code {scoring_response.status_code} \"\n                f\"and body {scoring_response.text}\"\n            )\n\n            if enable_mlserver:\n                # MLServer returns a different set of errors.\n                # Skip these assertions until this issue gets tackled:\n                # https://github.com/SeldonIO/MLServer/issues/360)\n                continue\n\n            scoring_response_dict = json.loads(scoring_response.content)\n            assert \"error_code\" in scoring_response_dict\n            assert scoring_response_dict[\"error_code\"] == ErrorCode.Name(BAD_REQUEST)\n            assert \"message\" in scoring_response_dict", "target": 0}, {"function": "def test_env_manager_warning_for_use_of_conda(monkeypatch):\n    with mock.patch(\"mlflow.models.cli.get_flavor_backend\") as mock_get_flavor_backend:\n        with pytest.warns(UserWarning, match=r\"Use of conda is discouraged\"):\n            CliRunner().invoke(\n                models_cli.serve,\n                [\"--model-uri\", \"model\", \"--env-manager\", \"conda\"],\n                catch_exceptions=False,\n            )\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            monkeypatch.setenv(MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING.name, \"TRUE\")\n            CliRunner().invoke(\n                models_cli.serve,\n                [\"--model-uri\", \"model\", \"--env-manager\", \"conda\"],\n                catch_exceptions=False,\n            )\n\n        assert mock_get_flavor_backend.call_count == 2", "target": 0}, {"function": "def test_env_manager_unsupported_value():\n    with pytest.raises(MlflowException, match=r\"Invalid value for `env_manager`\"):\n        CliRunner().invoke(\n            models_cli.serve,\n            [\"--model-uri\", \"model\", \"--env-manager\", \"abc\"],\n            catch_exceptions=False,\n        )", "target": 0}, {"function": "def test_change_conda_env_root_location(tmp_path, sk_model):\n    env_root1_path = tmp_path / \"root1\"\n    env_root1_path.mkdir()\n\n    env_root2_path = tmp_path / \"root2\"\n    env_root2_path.mkdir()\n\n    model1_path = tmp_path / \"model1\"\n    mlflow.sklearn.save_model(sk_model, str(model1_path), pip_requirements=[\"scikit-learn==1.0.1\"])\n\n    model2_path = tmp_path / \"model2\"\n    mlflow.sklearn.save_model(sk_model, str(model2_path), pip_requirements=[\"scikit-learn==1.0.2\"])\n\n    env_path_set = set()\n    for env_root_path, model_path, sklearn_ver in [\n        (env_root1_path, model1_path, \"1.0.1\"),\n        (\n            env_root2_path,\n            model1_path,\n            \"1.0.1\",\n        ),  # test the same env created in different env root path.\n        (\n            env_root1_path,\n            model2_path,\n            \"1.0.2\",\n        ),  # test different env created in the same env root path.\n    ]:\n        env = get_flavor_backend(\n            str(model_path),\n            env_manager=_EnvManager.CONDA,\n            install_mlflow=False,\n            env_root_dir=str(env_root_path),\n        ).prepare_env(model_uri=str(model_path))\n\n        conda_env_name = _get_conda_env_name(\n            str(model_path / \"conda.yaml\"), env_root_dir=env_root_path\n        )\n        env_path = env_root_path / \"conda_envs\" / conda_env_name\n        assert env_path.exists()\n        env_path_set.add(str(env_path))\n\n        python_exec_path = str(env_path / \"bin\" / \"python\")\n\n        # Test execution of command under the correct activated python env.\n        env.execute(\n            command=f\"python -c \\\"import sys; assert sys.executable == '{python_exec_path}'; \"\n            f\"import sklearn; assert sklearn.__version__ == '{sklearn_ver}'\\\"\",\n        )\n\n    assert len(env_path_set) == 3", "target": 0}], "function_after": [{"function": "def env_with_tracking_uri():\n    return {**os.environ, \"MLFLOW_TRACKING_URI\": mlflow.get_tracking_uri()}", "target": 0}, {"function": "def test_model_with_no_deployable_flavors_fails_pollitely():\n    from mlflow.models import Model\n\n    with TempDir(chdr=True) as tmp:\n        m = Model(\n            artifact_path=None,\n            run_id=None,\n            utc_time_created=\"now\",\n            flavors={\"some\": {}, \"useless\": {}, \"flavors\": {}},\n        )\n        os.mkdir(tmp.path(\"model\"))\n        m.save(tmp.path(\"model\", \"MLmodel\"))\n        # The following should fail because there should be no suitable flavor\n        prc = subprocess.run(\n            [\"mlflow\", \"models\", \"predict\", \"-m\", tmp.path(\"model\")],\n            stderr=subprocess.PIPE,\n            cwd=tmp.path(\"\"),\n            check=False,\n            text=True,\n            env=env_with_tracking_uri(),\n        )\n        assert \"No suitable flavor backend was found for the model.\" in prc.stderr", "target": 0}, {"function": "def test_serve_gunicorn_opts(iris_data, sk_model):\n    if sys.platform == \"win32\":\n        pytest.skip(\"This test requires gunicorn which is not available on windows.\")\n    with mlflow.start_run() as active_run:\n        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"imlegit\")\n        run_id = active_run.info.run_id\n\n    model_uris = [\n        \"models:/{name}/{stage}\".format(name=\"imlegit\", stage=\"None\"),\n        f\"runs:/{run_id}/model\",\n    ]\n    for model_uri in model_uris:\n        with TempDir() as tpm:\n            output_file_path = tpm.path(\"stoudt\")\n            with open(output_file_path, \"w\") as output_file:\n                x, _ = iris_data\n                scoring_response = pyfunc_serve_and_score_model(\n                    model_uri,\n                    pd.DataFrame(x),\n                    content_type=CONTENT_TYPE_JSON,\n                    stdout=output_file,\n                    extra_args=[\"-w\", \"3\"],\n                )\n            with open(output_file_path) as output_file:\n                stdout = output_file.read()\n        actual = pd.read_json(scoring_response.content.decode(\"utf-8\"), orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n        expected_command_pattern = re.compile(\n            \"gunicorn.*-w 3.*mlflow.pyfunc.scoring_server.wsgi:app\"\n        )\n        assert expected_command_pattern.search(stdout) is not None", "target": 0}, {"function": "def test_predict(iris_data, sk_model):\n    with TempDir(chdr=True) as tmp:\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n        model_registry_uri = \"models:/impredicting/None\"\n        input_json_path = tmp.path(\"input.json\")\n        input_csv_path = tmp.path(\"input.csv\")\n        output_json_path = tmp.path(\"output.json\")\n        x, _ = iris_data\n        with open(input_json_path, \"w\") as f:\n            json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n\n        pd.DataFrame(x).to_csv(input_csv_path, index=False)\n\n        # Test with no conda & model registry URI\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_registry_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"--env-manager\",\n                \"local\",\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # With conda + --install-mlflow\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # explicit json format with default orient (should be split)\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # explicit json format with orient==split\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_json_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # read from stdin, write to stdout.\n        prc = subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-t\",\n                \"json\",\n                *extra_options,\n            ],\n            input=Path(input_json_path).read_text(),\n            stdout=subprocess.PIPE,\n            env=env_with_tracking_uri(),\n            text=True,\n            check=True,\n        )\n        actual = pd.read_json(prc.stdout, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)\n\n        # NB: We do not test orient=records here because records may loose column ordering.\n        # orient == records is tested in other test with simpler model.\n\n        # csv\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"predict\",\n                \"-m\",\n                model_uri,\n                \"-i\",\n                input_csv_path,\n                \"-o\",\n                output_json_path,\n                \"-t\",\n                \"csv\",\n                *extra_options,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n        actual = pd.read_json(output_json_path, orient=\"records\")\n        actual = actual[actual.columns[0]].values\n        expected = sk_model.predict(x)\n        assert all(expected == actual)", "target": 0}, {"function": "def test_predict_check_content_type(iris_data, sk_model, tmp_path):\n    with mlflow.start_run():\n        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n    model_registry_uri = \"models:/impredicting/None\"\n    input_json_path = tmp_path / \"input.json\"\n    input_csv_path = tmp_path / \"input.csv\"\n    output_json_path = tmp_path / \"output.json\"\n\n    x, _ = iris_data\n    with input_json_path.open(\"w\") as f:\n        json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n\n    pd.DataFrame(x).to_csv(input_csv_path, index=False)\n\n    # Throw errors for invalid content_type\n    prc = subprocess.run(\n        [\n            \"mlflow\",\n            \"models\",\n            \"predict\",\n            \"-m\",\n            model_registry_uri,\n            \"-i\",\n            input_json_path,\n            \"-o\",\n            output_json_path,\n            \"-t\",\n            \"invalid\",\n            \"--env-manager\",\n            \"local\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env_with_tracking_uri(),\n        check=False,\n    )\n    assert prc.returncode != 0\n    assert \"Unknown content type\" in prc.stderr.decode(\"utf-8\")", "target": 0}, {"function": "def test_predict_check_input_path(iris_data, sk_model, tmp_path):\n    with mlflow.start_run():\n        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n    model_registry_uri = \"models:/impredicting/None\"\n    input_json_path = tmp_path / \"input with space.json\"\n    input_csv_path = tmp_path / \"input.csv\"\n    output_json_path = tmp_path / \"output.json\"\n\n    x, _ = iris_data\n    with input_json_path.open(\"w\") as f:\n        json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n\n    pd.DataFrame(x).to_csv(input_csv_path, index=False)\n\n    # Valid input path with space\n    prc = subprocess.run(\n        [\n            \"mlflow\",\n            \"models\",\n            \"predict\",\n            \"-m\",\n            model_registry_uri,\n            \"-i\",\n            f\"{input_json_path}\",\n            \"-o\",\n            output_json_path,\n            \"--env-manager\",\n            \"local\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env_with_tracking_uri(),\n        check=False,\n        text=True,\n    )\n    assert prc.returncode == 0\n\n    # Throw errors for invalid input_path\n    prc = subprocess.run(\n        [\n            \"mlflow\",\n            \"models\",\n            \"predict\",\n            \"-m\",\n            model_registry_uri,\n            \"-i\",\n            f'{input_json_path}\"; echo ThisIsABug! \"',\n            \"-o\",\n            output_json_path,\n            \"--env-manager\",\n            \"local\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env_with_tracking_uri(),\n        check=False,\n        text=True,\n    )\n    assert prc.returncode != 0\n    assert \"ThisIsABug!\" not in prc.stdout\n    assert \"FileNotFoundError\" in prc.stderr\n\n    prc = subprocess.run(\n        [\n            \"mlflow\",\n            \"models\",\n            \"predict\",\n            \"-m\",\n            model_registry_uri,\n            \"-i\",\n            f'{input_csv_path}\"; echo ThisIsABug! \"',\n            \"-o\",\n            output_json_path,\n            \"-t\",\n            \"csv\",\n            \"--env-manager\",\n            \"local\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env_with_tracking_uri(),\n        check=False,\n        text=True,\n    )\n    assert prc.returncode != 0\n    assert \"ThisIsABug!\" not in prc.stdout\n    assert \"FileNotFoundError\" in prc.stderr", "target": 0}, {"function": "def test_predict_check_output_path(iris_data, sk_model, tmp_path):\n    with mlflow.start_run():\n        mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")\n    model_registry_uri = \"models:/impredicting/None\"\n    input_json_path = tmp_path / \"input.json\"\n    input_csv_path = tmp_path / \"input.csv\"\n    output_json_path = tmp_path / \"output.json\"\n\n    x, _ = iris_data\n    with input_json_path.open(\"w\") as f:\n        json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)\n\n    pd.DataFrame(x).to_csv(input_csv_path, index=False)\n\n    prc = subprocess.run(\n        [\n            \"mlflow\",\n            \"models\",\n            \"predict\",\n            \"-m\",\n            model_registry_uri,\n            \"-i\",\n            input_json_path,\n            \"-o\",\n            f'{output_json_path}\"; echo ThisIsABug! \"',\n            \"--env-manager\",\n            \"local\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env_with_tracking_uri(),\n        check=False,\n        text=True,\n    )\n    assert prc.returncode == 0\n    assert \"ThisIsABug!\" not in prc.stdout", "target": 0}, {"function": "def test_prepare_env_passes(sk_model):\n    if no_conda:\n        pytest.skip(\"This test requires conda.\")\n\n    with TempDir(chdr=True):\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(sk_model, \"model\")\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n\n        # With conda\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )\n\n        # Should be idempotent\n        subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=True,\n        )", "target": 0}, {"function": "def test_prepare_env_fails(sk_model):\n    if no_conda:\n        pytest.skip(\"This test requires conda.\")\n\n    with TempDir(chdr=True):\n        with mlflow.start_run() as active_run:\n            mlflow.sklearn.log_model(\n                sk_model, \"model\", pip_requirements=[\"does-not-exist-dep==abc\"]\n            )\n            model_uri = f\"runs:/{active_run.info.run_id}/model\"\n\n        # With conda - should fail due to bad conda environment.\n        prc = subprocess.run(\n            [\n                \"mlflow\",\n                \"models\",\n                \"prepare-env\",\n                \"-m\",\n                model_uri,\n            ],\n            env=env_with_tracking_uri(),\n            check=False,\n        )\n        assert prc.returncode != 0", "target": 0}, {"function": "def test_build_docker_virtualenv(iris_data, sk_model):\n    with mlflow.start_run():\n        model_info = mlflow.sklearn.log_model(sk_model, \"model\")\n\n    x, _ = iris_data\n    df = pd.DataFrame(iris_data[0])\n\n    extra_args = [\"--install-mlflow\", \"--env-manager\", \"virtualenv\"]\n    image_name = pyfunc_build_image(\n        model_info.model_uri,\n        extra_args=extra_args,\n        env=env_with_tracking_uri(),\n    )\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image(image_name, host_port)\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model)", "target": 0}, {"function": "def test_build_docker_without_model_uri(iris_data, sk_model, tmp_path):\n    model_path = tmp_path.joinpath(\"model\")\n    mlflow.sklearn.save_model(sk_model, model_path)\n    image_name = pyfunc_build_image(model_uri=None)\n    host_port = get_safe_port()\n    scoring_proc = pyfunc_serve_from_docker_image_with_env_override(\n        image_name,\n        host_port,\n        gunicorn_options,\n        extra_docker_run_options=[\"-v\", f\"{model_path}:/opt/ml/model\"],\n    )\n    x = iris_data[0]\n    df = pd.DataFrame(x)\n    _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model)", "target": 0}, {"function": "def _validate_with_rest_endpoint(scoring_proc, host_port, df, x, sk_model, enable_mlserver=False):\n    with RestEndpoint(proc=scoring_proc, port=host_port, validate_version=False) as endpoint:\n        for content_type in [CONTENT_TYPE_JSON, CONTENT_TYPE_CSV]:\n            scoring_response = endpoint.invoke(df, content_type)\n            assert scoring_response.status_code == 200, (\n                \"Failed to serve prediction, got response %s\" % scoring_response.text\n            )\n            np.testing.assert_array_equal(\n                np.array(json.loads(scoring_response.text)[\"predictions\"]), sk_model.predict(x)\n            )\n        # Try examples of bad input, verify we get a non-200 status code\n        for content_type in [CONTENT_TYPE_JSON, CONTENT_TYPE_CSV, CONTENT_TYPE_JSON]:\n            scoring_response = endpoint.invoke(data=\"\", content_type=content_type)\n            expected_status_code = 500 if enable_mlserver else 400\n            assert scoring_response.status_code == expected_status_code, (\n                f\"Expected server failure with error code {expected_status_code}, \"\n                f\"got response with status code {scoring_response.status_code} \"\n                f\"and body {scoring_response.text}\"\n            )\n\n            if enable_mlserver:\n                # MLServer returns a different set of errors.\n                # Skip these assertions until this issue gets tackled:\n                # https://github.com/SeldonIO/MLServer/issues/360)\n                continue\n\n            scoring_response_dict = json.loads(scoring_response.content)\n            assert \"error_code\" in scoring_response_dict\n            assert scoring_response_dict[\"error_code\"] == ErrorCode.Name(BAD_REQUEST)\n            assert \"message\" in scoring_response_dict", "target": 0}, {"function": "def test_env_manager_warning_for_use_of_conda(monkeypatch):\n    with mock.patch(\"mlflow.models.cli.get_flavor_backend\") as mock_get_flavor_backend:\n        with pytest.warns(UserWarning, match=r\"Use of conda is discouraged\"):\n            CliRunner().invoke(\n                models_cli.serve,\n                [\"--model-uri\", \"model\", \"--env-manager\", \"conda\"],\n                catch_exceptions=False,\n            )\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            monkeypatch.setenv(MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING.name, \"TRUE\")\n            CliRunner().invoke(\n                models_cli.serve,\n                [\"--model-uri\", \"model\", \"--env-manager\", \"conda\"],\n                catch_exceptions=False,\n            )\n\n        assert mock_get_flavor_backend.call_count == 2", "target": 0}, {"function": "def test_env_manager_unsupported_value():\n    with pytest.raises(MlflowException, match=r\"Invalid value for `env_manager`\"):\n        CliRunner().invoke(\n            models_cli.serve,\n            [\"--model-uri\", \"model\", \"--env-manager\", \"abc\"],\n            catch_exceptions=False,\n        )", "target": 0}, {"function": "def test_host_invalid_value():\n    class MyModel(mlflow.pyfunc.PythonModel):\n        def predict(self, ctx, model_input):\n            return model_input\n\n    with mlflow.start_run():\n        model_info = mlflow.pyfunc.log_model(\n            python_model=MyModel(), artifact_path=\"test_model\", registered_model_name=\"model\"\n        )\n\n    with mock.patch(\"mlflow.models.cli.get_flavor_backend\", return_value=PyFuncBackend({})):\n        with pytest.raises(ShellCommandException, match=r\"Non-zero exit code: 1\"):\n            CliRunner().invoke(\n                models_cli.serve,\n                [\"--model-uri\", model_info.model_uri, \"--host\", \"localhost & echo BUG\"],\n                catch_exceptions=False,\n            )", "target": 0}, {"function": "def test_change_conda_env_root_location(tmp_path, sk_model):\n    env_root1_path = tmp_path / \"root1\"\n    env_root1_path.mkdir()\n\n    env_root2_path = tmp_path / \"root2\"\n    env_root2_path.mkdir()\n\n    model1_path = tmp_path / \"model1\"\n    mlflow.sklearn.save_model(sk_model, str(model1_path), pip_requirements=[\"scikit-learn==1.0.1\"])\n\n    model2_path = tmp_path / \"model2\"\n    mlflow.sklearn.save_model(sk_model, str(model2_path), pip_requirements=[\"scikit-learn==1.0.2\"])\n\n    env_path_set = set()\n    for env_root_path, model_path, sklearn_ver in [\n        (env_root1_path, model1_path, \"1.0.1\"),\n        (\n            env_root2_path,\n            model1_path,\n            \"1.0.1\",\n        ),  # test the same env created in different env root path.\n        (\n            env_root1_path,\n            model2_path,\n            \"1.0.2\",\n        ),  # test different env created in the same env root path.\n    ]:\n        env = get_flavor_backend(\n            str(model_path),\n            env_manager=_EnvManager.CONDA,\n            install_mlflow=False,\n            env_root_dir=str(env_root_path),\n        ).prepare_env(model_uri=str(model_path))\n\n        conda_env_name = _get_conda_env_name(\n            str(model_path / \"conda.yaml\"), env_root_dir=env_root_path\n        )\n        env_path = env_root_path / \"conda_envs\" / conda_env_name\n        assert env_path.exists()\n        env_path_set.add(str(env_path))\n\n        python_exec_path = str(env_path / \"bin\" / \"python\")\n\n        # Test execution of command under the correct activated python env.\n        env.execute(\n            command=f\"python -c \\\"import sys; assert sys.executable == '{python_exec_path}'; \"\n            f\"import sklearn; assert sklearn.__version__ == '{sklearn_ver}'\\\"\",\n        )\n\n    assert len(env_path_set) == 3", "target": 0}]}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
