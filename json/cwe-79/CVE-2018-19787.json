{"index": 3255, "cve_id": "CVE-2018-19787", "cwe_id": ["CWE-79"], "cve_language": "Python", "cve_description": "An issue was discovered in lxml before 4.2.5. lxml/html/clean.py in the lxml.html.clean module does not remove javascript: URLs that use escaping, allowing a remote attacker to conduct XSS attacks, as demonstrated by \"j a v a s c r i p t:\" in Internet Explorer. This is a similar issue to CVE-2014-3146.", "cvss": "6.1", "publish_date": "December 2, 2018", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "REQUIRED", "S": "CHANGED", "C": "LOW", "I": "LOW", "A": "NONE", "commit_id": "6be1d081b49c97cfd7b3fbd934a193b668629109", "commit_message": "Fix: make the cleaner also remove javascript URLs that use escaping.", "commit_date": "2018-09-09T14:44:17Z", "project": "lxml/lxml", "url": "https://api.github.com/repos/lxml/lxml/commits/6be1d081b49c97cfd7b3fbd934a193b668629109", "html_url": "https://github.com/lxml/lxml/commit/6be1d081b49c97cfd7b3fbd934a193b668629109", "windows_before": [{"commit_id": "085ccc2c5d3da1cdf0c10246f7bcd6e2bd5fcb2d", "commit_date": "Sun Aug 26 16:37:25 2018 +0200", "commit_message": "Merge pull request #271 from hugovk/rm-3.3", "files_name": ["ae02899b108ec247c3f3401321fc71527ddb2cc5 - Sun Aug 26 17:21:43 2018 +0300 : Drop support for EOL Python 3.3", ".appveyor.yml", ".travis.yml", "INSTALL.txt", "setup.py", "src/lxml/tests/test_elementtree.py", "tox.ini"]}, {"commit_id": "eb4d8c4423c4b8a881f99dd6aaba40828798b34a", "commit_date": "Sun Aug 26 15:49:59 2018 +0200", "commit_message": "Merge pull request #270 from hugovk/rm-2.6", "files_name": ["9ac32de2352912e52dea7c5bd825d99100d22171 - Sun Aug 26 15:58:05 2018 +0300 : Remove ununsed imports", "src/lxml/html/diff.py", "src/lxml/html/tests/test_autolink.py", "src/lxml/html/tests/test_basic.py", "src/lxml/html/tests/test_clean.py", "src/lxml/html/tests/test_diff.py", "src/lxml/html/tests/test_feedparser_data.py", "src/lxml/html/tests/test_formfill.py", "src/lxml/html/tests/test_forms.py", "src/lxml/html/tests/test_rewritelinks.py", "src/lxml/html/tests/test_xhtml.py", "src/lxml/html/tests/transform_feedparser_data.py", "src/lxml/tests/test_doctestcompare.py", "src/lxml/tests/test_external_document.py", "src/lxml/tests/test_pyclasslookup.py"]}, {"commit_id": "1e10b9dd4b1cba9d624f08dc5f7730c79ff63ced", "commit_date": "Sun Aug 26 16:10:15 2018 +0300", "commit_message": "Keep parentheses", "files_name": ["benchmark/benchbase.py"]}, {"commit_id": "7063ee19a13facad087b8b1e886a1f7efc7887c2", "commit_date": "Sun Aug 26 16:08:19 2018 +0300", "commit_message": "Add newer Python versions", "files_name": ["tox.ini"]}, {"commit_id": "dcdf7b7191f7d56e0dcdf2096bb6269c7fecccd1", "commit_date": "Sun Aug 26 16:07:46 2018 +0300", "commit_message": "Keep on same line", "files_name": ["src/lxml/tests/test_isoschematron.py"]}, {"commit_id": "6b8edfac28921f189ed70616d4eded44af885db4", "commit_date": "Sun Aug 26 16:05:04 2018 +0300", "commit_message": "Start a new line for the items", "files_name": ["src/lxml/tests/test_incremental_xmlfile.py"]}, {"commit_id": "7b417ec179641097716985c6db06736fa98ecd14", "commit_date": "Sun Aug 26 16:03:04 2018 +0300", "commit_message": "Use set comprehension", "files_name": ["src/lxml/tests/test_incremental_xmlfile.py"]}, {"commit_id": "22feab429af1ab67fe9b91772804c1959e88877a", "commit_date": "Sun Aug 26 15:22:11 2018 +0300", "commit_message": "Add newlines for dict's keys", "files_name": ["src/lxml/tests/selftest.py"]}, {"commit_id": "a6c7f49dd3ee3d16723142905db7fdd9de2554ed", "commit_date": "Sun Aug 26 15:14:37 2018 +0300", "commit_message": "Use tempfile.NamedTemporaryFile directly", "files_name": ["src/lxml/html/tests/test_html5parser.py", "src/lxml/tests/test_io.py"]}, {"commit_id": "af5005967be29aadbd7258ec9e9a90a9445650cb", "commit_date": "Sun Aug 26 15:10:17 2018 +0300", "commit_message": "Min version of LIBXML_VERSION is now 2.7", "files_name": ["src/lxml/html/tests/test_clean.py", "src/lxml/tests/test_etree.py"]}, {"commit_id": "37f87ef29780db7db998e9e17a3281720455e244", "commit_date": "Sun Aug 26 15:08:34 2018 +0300", "commit_message": "'assert False' more readable than 'assert 0'", "files_name": ["src/lxml/classlookup.pxi", "src/lxml/html/diff.py"]}, {"commit_id": "e3ab04c0671bdaaead31cae5e3eb317e2892caf8", "commit_date": "Sun Aug 26 15:06:39 2018 +0300", "commit_message": "Revert \"Replace mutable default argument\"", "files_name": ["DD.py", "src/lxml/html/clean.py", "src/lxml/isoschematron/__init__.py"]}, {"commit_id": "38b89d1d0a5f38ec347ce6193ccd1038bc25bbea", "commit_date": "Sun Aug 26 15:05:29 2018 +0300", "commit_message": "Remove redundant '= None'", "files_name": ["src/lxml/etree.pyx"]}, {"commit_id": "6359bb0ca0fc8f86854f0fef248e467be086d0a9", "commit_date": "Sun Aug 26 15:04:46 2018 +0300", "commit_message": "Split lines for clarity", "files_name": ["src/lxml/doctestcompare.py"]}, {"commit_id": "0d146b06e26cc4ae6ba6aa16708de9a867ba47f5", "commit_date": "Sun Aug 26 15:04:00 2018 +0300", "commit_message": "Simplify isinstance", "files_name": ["src/lxml/apihelpers.pxi"]}, {"commit_id": "171eaaa30a0ac0f572c932ed04d5029af53b6bd1", "commit_date": "Sun Aug 26 08:59:30 2018 +0200", "commit_message": "Fix typo in test file.", "files_name": ["src/lxml/html/tests/test_html5parser.py"]}, {"commit_id": "9375f791c9f1934c10a127294446bdb2c39fc3ae", "commit_date": "Sun Aug 26 08:59:30 2018 +0200", "commit_message": "Fix typo in test file.", "files_name": ["src/lxml/html/tests/test_html5parser.py"]}, {"commit_id": "1bb1c7e22fcb04a9148531490f0aabcbf67ae233", "commit_date": "Sat Aug 25 21:15:58 2018 +0300", "commit_message": "Remove unnecessary backslash", "files_name": ["src/lxml/apihelpers.pxi"]}, {"commit_id": "5674dd2c1e29b98026350ab27163a2b06187be46", "commit_date": "Sat Aug 25 21:14:49 2018 +0300", "commit_message": "Remove redundant parentheses", "files_name": ["DD.py", "benchmark/benchbase.py", "buildlibxml.py", "src/lxml/etree.pyx", "src/lxml/html/diff.py", "src/lxml/objectify.pyx", "src/lxml/parser.pxi", "src/lxml/sax.py", "src/lxml/serializer.pxi", "src/lxml/tests/test_etree.py", "src/lxml/tests/test_isoschematron.py", "src/lxml/tests/test_objectify.py", "src/lxml/tests/test_threading.py", "src/lxml/xmlid.pxi"]}, {"commit_id": "2692f36d8f6fce77bd90d2ee4b28bdc0119691dc", "commit_date": "Sat Aug 25 21:13:53 2018 +0300", "commit_message": "Replace list creation with list literal", "files_name": ["src/lxml/doctestcompare.py"]}, {"commit_id": "8e8fd0d05d22655a20e4d4814796c7e1c8e04986", "commit_date": "Sat Aug 25 21:12:02 2018 +0300", "commit_message": "Replace function call with set literal", "files_name": ["src/lxml/html/clean.py", "src/lxml/html/tests/test_select.py", "src/lxml/tests/test_elementtree.py", "src/lxml/tests/test_incremental_xmlfile.py"]}, {"commit_id": "5703e6de18be851fc60b7e4edec83c95ba066c5a", "commit_date": "Sat Aug 25 21:11:19 2018 +0300", "commit_message": "Replace dictionary creation with dictionary literal", "files_name": ["src/lxml/tests/selftest.py"]}, {"commit_id": "92faebc0efa332c39a94d90d4ab7eb1a82233c4b", "commit_date": "Sat Aug 25 21:10:48 2018 +0300", "commit_message": "Replace mutable default argument", "files_name": ["DD.py", "src/lxml/html/clean.py", "src/lxml/isoschematron/__init__.py"]}, {"commit_id": "29b9f09bb7fcb73edee0de939630f71665d75e47", "commit_date": "Sat Aug 25 21:09:41 2018 +0300", "commit_message": "Compare None using 'is'/'is not' instead of equality operators", "files_name": ["DD.py", "src/lxml/tests/selftest2.py", "src/lxml/tests/test_elementtree.py"]}, {"commit_id": "3c9475c4fe34ba70382100a8a2a441a550b35e48", "commit_date": "Sat Aug 25 21:06:45 2018 +0300", "commit_message": "Simplify Boolean expression", "files_name": ["DD.py"]}, {"commit_id": "beaa4eb8904b9209d75d98059b5b92b26fdfebe3", "commit_date": "Sat Aug 25 20:53:59 2018 +0300", "commit_message": "Remove redundant code for Python <= 2.6", "files_name": ["INSTALL.txt", "doc/api.txt", "doc/xpathxslt.txt", "setup.py", "src/lxml/apihelpers.pxi", "src/lxml/etree.pyx", "src/lxml/html/clean.py", "src/lxml/html/tests/test_autolink.py", "src/lxml/html/tests/test_basic.py", "src/lxml/html/tests/test_clean.py", "src/lxml/html/tests/test_diff.py", "src/lxml/html/tests/test_feedparser_data.py", "src/lxml/html/tests/test_formfill.py", "src/lxml/html/tests/test_forms.py", "src/lxml/html/tests/test_html5parser.py", "src/lxml/html/tests/test_rewritelinks.py", "src/lxml/includes/etree_defs.h", "src/lxml/python.pxd", "src/lxml/tests/dummy_http_server.py", "src/lxml/tests/test_doctestcompare.py", "src/lxml/tests/test_etree.py", "src/lxml/tests/test_external_document.py", "src/lxml/tests/test_http_io.py", "src/lxml/tests/test_io.py", "src/lxml/tests/test_objectify.py", "test.py", "tools/manylinux/build-wheels.sh", "tox.ini"]}, {"commit_id": "9b9136b622ed7ccabb3da76a1902fc366e1c1cbe", "commit_date": "Thu Aug 9 14:32:54 2018 +0200", "commit_message": "Make .nsmap available in XSLT extensions.", "files_name": ["src/lxml/readonlytree.pxi", "src/lxml/tests/test_xslt.py"]}, {"commit_id": "396ded1558c4ea7a3723be994c76304b7c5edff8", "commit_date": "Sat Aug 4 14:27:20 2018 +0200", "commit_message": "Show ccache stats in travis after using it in the build.", "files_name": [".travis.yml"]}, {"commit_id": "6aab5999b284abbdd993023be8c25963e981348c", "commit_date": "Sat Aug 4 14:20:04 2018 +0200", "commit_message": "Speed up travis build by not making it wait for Py3.7 (xenial).", "files_name": [".travis.yml"]}, {"commit_id": "6f39772279f6eff007fe24116fedbd7bbfa03c5a", "commit_date": "Sat Aug 4 14:18:27 2018 +0200", "commit_message": "Try to actually enable ccache.", "files_name": [".travis.yml"]}, {"commit_id": "f9c25c4b08b350ddd9bfece0c6be74b1afd0fd9c", "commit_date": "Sat Aug 4 14:01:17 2018 +0200", "commit_message": "Try to reverse the travis matrix build order.", "files_name": [".travis.yml"]}, {"commit_id": "ae38f441413a2c949b48c7d5ba9b2bd1b55db2ec", "commit_date": "Sat Aug 4 14:00:28 2018 +0200", "commit_message": "Remove non-working Python setup from travis build matrix.", "files_name": [".travis.yml"]}, {"commit_id": "54c2fc5b7af9ad5f96f75cc713ddb7fd7ce8a152", "commit_date": "Sat Aug 4 13:58:20 2018 +0200", "commit_message": "Repair travis build matrix setup by removing duplicate matrix config.", "files_name": [".travis.yml"]}, {"commit_id": "f3c02650a793dd5520966a2661f79e2064d53422", "commit_date": "Sat Aug 4 13:56:55 2018 +0200", "commit_message": "Revert most changes in travis build matrix.", "files_name": [".travis.yml"]}, {"commit_id": "e4bac4d6187fb0ac088f504cc3eb9a6d4c93dd3a", "commit_date": "Sat Aug 4 13:47:51 2018 +0200", "commit_message": "Try to fix travis build setup for Py3.7+.", "files_name": [".travis.yml"]}, {"commit_id": "e27156d55b1c9ecc90013837b35d4c58e0ad9827", "commit_date": "Sat Aug 4 13:42:37 2018 +0200", "commit_message": "Try to fix travis build setup for Py3.8 and Py3.7.", "files_name": [".travis.yml"]}, {"commit_id": "e6f8bf938ca3e2c844bf82ae169c33f67fbf60b6", "commit_date": "Sat Aug 4 13:16:46 2018 +0200", "commit_message": "Make sure ccache is available in travis build and use the same setup for Py3.8 as for Py3.7.", "files_name": [".travis.yml"]}, {"commit_id": "aed0ae2a9fe8007ed21f2fb34515ebcc0dd54096", "commit_date": "Sat Aug 4 13:05:32 2018 +0200", "commit_message": "Enable ccache for travis builds.", "files_name": [".travis.yml"]}, {"commit_id": "acef361ca80ff9afd828d91c98ea91c92f9d09af", "commit_date": "Sat Aug 4 12:56:14 2018 +0200", "commit_message": "Make test more resilient against changes in latest libxslt releases.", "files_name": ["src/lxml/tests/test_threading.py"]}, {"commit_id": "810d3ce99aaf9701670f8149c280a6557d50ee29", "commit_date": "Sat Aug 4 11:16:14 2018 +0200", "commit_message": "Use a fixed libxslt version for the static builds in travis since the latest beta is problematic.", "files_name": [".travis.yml"]}, {"commit_id": "736b8b79bf8c09ec2351e6133e72117f60b67a02", "commit_date": "Sat Aug 4 11:14:15 2018 +0200", "commit_message": "Revert to using Ubuntu trusty in travis by default since the xenial farm still seems to be really small/slow.", "files_name": [".travis.yml"]}], "windows_after": [{"commit_id": "1dee355e83b1f524de7a772a8da941a186036bc2", "commit_date": "Sun Sep 9 17:16:33 2018 +0200", "commit_message": "Py3 syntax fix in helper script.", "files_name": ["doc/rest2html.py"]}, {"commit_id": "3f3082e0a67851cde26a48da3d1f4b75d8aa07ec", "commit_date": "Sun Sep 9 17:19:58 2018 +0200", "commit_message": "Merge branch lxml-4.2 into master.", "files_name": ["617c10eb870e6261d7457b899aff8987562d3071 - Sun Sep 9 18:04:41 2018 +0200 : Do not try to run tests in wheel building script since it leads to problems with the library import.", "tools/manylinux/build-wheels.sh"]}, {"commit_id": "f677d68f863c9c112f4facfdb1d15212c4464dcb", "commit_date": "Sun Sep 9 18:04:41 2018 +0200", "commit_message": "Do not try to run tests in wheel building script since it leads to problems with the library import.", "files_name": ["tools/manylinux/build-wheels.sh"]}, {"commit_id": "ebf4bf8759f7023f62fdd85f7fe6aebeb4da5f7f", "commit_date": "Sun Sep 9 18:05:50 2018 +0200", "commit_message": "Merge branch lxml-4.2 into master.", "files_name": ["2178791ff027a4fc5eb01b8ba2fa36383091685c - Fri Sep 14 00:08:03 2018 +0200 : LP#1792388: Add missing test file to sdist.", "MANIFEST.in"]}, {"commit_id": "8f5d34fe5192e86c7abc36c53f5b912a8f2da099", "commit_date": "Sat Sep 15 11:56:22 2018 +0200", "commit_message": "Fix broken link.", "files_name": ["doc/intro.txt"]}, {"commit_id": "01a107bb1e04f93a966e13a4e83dceca272d1ae7", "commit_date": "Sat Sep 15 13:22:12 2018 +0200", "commit_message": "Provide more information on download errors in static build script.", "files_name": ["buildlibxml.py"]}, {"commit_id": "de326abde764fd0969d59601cd103fc8eea46487", "commit_date": "Sat Sep 29 14:43:15 2018 +0200", "commit_message": "Fix import warnings in Py3.6+ by switching to absolute imports.", "files_name": ["CHANGES.txt", "src/lxml/_elementpath.py", "src/lxml/builder.py", "src/lxml/html/clean.py", "src/lxml/html/diff.py", "src/lxml/sax.py"]}, {"commit_id": "55a9595b983e00583bff0f0417228751abadb2ef", "commit_date": "Sat Sep 29 14:44:51 2018 +0200", "commit_message": "Merge lxml-4.2 branch into master.", "files_name": ["ff3003712733b707766919191880bf67f1d5003b - Mon Oct 8 09:45:13 2018 +0200 : doc: fix 2 links lxml-source-howto.txt", "doc/lxml-source-howto.txt"]}, {"commit_id": "4c5f71ba5b6826d0f1e3c84576cb277088d1d6e4", "commit_date": "Sat Oct 13 21:36:15 2018 +0300", "commit_message": "Fix broken link FAQ page", "files_name": ["doc/FAQ.txt"]}, {"commit_id": "c73f588d08475e82a2cd54184ddc966e68e8ab99", "commit_date": "Sat Oct 13 20:52:48 2018 +0200", "commit_message": "Merge pull request #275 from bzz/patch-2", "files_name": ["f884405b4a67555bece4922311f8c0e986dd4208 - Sun Oct 14 14:44:18 2018 +0300 : Updates description", "doc/FAQ.txt"]}, {"commit_id": "1f78df1f83481528b066e177b0b0171f495ec591", "commit_date": "Sun Oct 14 13:52:11 2018 +0200", "commit_message": "Merge pull request #276 from AndreyErmilov/master", "files_name": ["035d48a84deea73323991a919c864dc8ea854886 - Mon Jun 11 13:26:43 2018 +0200 : Let ElementTreeProducer use the available namespaces", "CHANGES.txt", "src/lxml/sax.py", "src/lxml/tests/test_sax.py"]}, {"commit_id": "2d92c1edc1dbf5c3eee7206011725453faa04b20", "commit_date": "Wed Oct 17 16:32:00 2018 +0200", "commit_message": "Merge branch 'master' into master", "files_name": ["d5c69a40df483ed60e75ebcb27e493c51e10873d - Sat Oct 20 19:06:10 2018 +0200 : Speed up ascii/non-ascii string detection in isutf8() and funicode() helper functions.", "src/lxml/apihelpers.pxi", "src/lxml/serializer.pxi"]}, {"commit_id": "68cf93c4827ea74e46d2aa6809011f96ed9c689a", "commit_date": "Wed Oct 24 21:16:26 2018 +0200", "commit_message": "LP#1799755: Fix ABC imports from collections package to resolve a DeprecationWarning in Py3.7.", "files_name": ["CHANGES.txt", "src/lxml/html/__init__.py", "src/lxml/html/_setmixin.py"]}, {"commit_id": "71919ff169ab137bcc0d6df776046ac8ccc54595", "commit_date": "Wed Oct 24 21:16:26 2018 +0200", "commit_message": "LP#1799755: Fix ABC imports from collections package to resolve a DeprecationWarning in Py3.7.", "files_name": ["CHANGES.txt", "src/lxml/html/__init__.py", "src/lxml/html/_setmixin.py"]}, {"commit_id": "2ea6f97c5758b80d6a8394724c36091234fc9191", "commit_date": "Fri Nov 16 18:08:19 2018 +0100", "commit_message": "Clarify docstring: passing 'unicode' as encoding name into tostring() is more common than passing the unicode/str function.", "files_name": ["src/lxml/etree.pyx"]}, {"commit_id": "8c8e6136cd35f12ad0b90e8265eb13c5ea58e29b", "commit_date": "Thu Nov 22 13:26:17 2018 +0100", "commit_message": "New and improved namespace handling for the saxifier", "files_name": ["CHANGES.txt", "src/lxml/sax.py", "src/lxml/tests/test_sax.py"]}, {"commit_id": "51308a28ac6e4e5ec7e014932a1ef39c1f99c5de", "commit_date": "Fri Nov 23 15:18:50 2018 +0100", "commit_message": "Merge branch 'master' into master", "files_name": ["00d8bcaa72fdc881d70edf7e35145f2dfcb1117a - Fri Nov 23 19:27:29 2018 +0100 : Fix signature of helper function to avoid C compiler warnings.", "src/lxml/xpath.pxi"]}, {"commit_id": "5a444c238f526edaa1319e9f0852d18332079aa8", "commit_date": "Fri Nov 23 20:04:45 2018 +0100", "commit_message": "Update iso-schematron to 2013 (latest) version, now MIT licensed.", "files_name": ["CHANGES.txt", "src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_abstract_expand.xsl", "src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/readme.txt"]}, {"commit_id": "92901bd2b2ff9280df4c9d5ae720e390dfb4da18", "commit_date": "Fri Nov 23 20:50:00 2018 +0100", "commit_message": "Update ISO-Schematron RNG schema to 2016 specification from http://standards.iso.org/ittf/PubliclyAvailableStandards/c055982_ISO_IEC_19757-3_2016.zip", "files_name": ["src/lxml/isoschematron/resources/rng/iso-schematron.rng"]}, {"commit_id": "4980b025bd84af6f0254db93a982a67ca23fc79e", "commit_date": "Fri Nov 23 20:57:12 2018 +0100", "commit_message": "Make <properties> tag in ISO-Schematron RNG optional, diverging from the 2016 version of the standard.", "files_name": ["src/lxml/isoschematron/resources/rng/iso-schematron.rng"]}, {"commit_id": "d7e033506d28af5c9208a7d292406068827ebcef", "commit_date": "Fri Nov 23 20:58:09 2018 +0100", "commit_message": "Simplify RNG parsing in ISO-Schematron setup code.", "files_name": ["src/lxml/isoschematron/__init__.py"]}, {"commit_id": "82601a09d015bc3e7a4090223fcbb9a5d5d4590d", "commit_date": "Fri Nov 23 21:01:00 2018 +0100", "commit_message": "Update changelog.", "files_name": ["CHANGES.txt"]}, {"commit_id": "e08620788d739d98a869e068a0f79af04ea4ef48", "commit_date": "Fri Nov 23 22:02:29 2018 +0100", "commit_message": "Use older libxml2 version 2.9.8 in travis tests as the latest pre-release 2.9.9-rc1 has a RelaxNG bug.", "files_name": [".travis.yml"]}, {"commit_id": "6ce2892aa33a329ff89abdf582ecae02cf55105b", "commit_date": "Fri Nov 23 22:09:30 2018 +0100", "commit_message": "Merge branch 'master' into master", "files_name": ["2d7c2f8063d1c2279482729f8020eb28b2b09040 - Fri Nov 23 22:17:17 2018 +0100 : Add \"libs\" download directory to hg-ignored files.", ".hgignore"]}, {"commit_id": "579a4b061a5faee91e05e8fb18699ec4d88934eb", "commit_date": "Fri Nov 23 22:17:58 2018 +0100", "commit_message": "Start caching libs/ download directory to avoid re-downloading the dependencies all the time.", "files_name": [".travis.yml"]}, {"commit_id": "488286e179fc9b31df1570b4bca8d1ec9b1e4031", "commit_date": "Mon Nov 26 19:25:03 2018 +0100", "commit_message": "Further updates to the namespace changes when saxifying", "files_name": ["src/lxml/sax.py"]}, {"commit_id": "9d91c1e602dcffa2a4b08c69a33f7ef4e75bde46", "commit_date": "Sun Dec 2 12:59:19 2018 +0100", "commit_message": "Update changelog.", "files_name": ["CHANGES.txt"]}, {"commit_id": "1c0a657e5b6860c9a3ffd679080102f1428b6f10", "commit_date": "Sun Dec 2 12:59:45 2018 +0100", "commit_message": "Merge branch lxml-4.2 into master", "files_name": ["4432378cfc6d7bddb4cf9cac324606b9cae8647d - Sun Dec 2 14:05:43 2018 +0100 : Increase minimum required lib versions to what actually compiles and tests correctly these days. Add a travis setup with the minimal required lib versions.", ".travis.yml", "INSTALL.txt"]}, {"commit_id": "9ecef44311afe7082fdba124d5c1a688442b1854", "commit_date": "Sun Dec 2 15:40:50 2018 +0100", "commit_message": "Merge pull request #267 from regebro/master", "files_name": ["10ce94b0a7db3470792e2e0fdd180e6f1ba52212 - Sun Dec 2 16:18:39 2018 +0100 : Cleanups for #267: avoid failure on min([]), tune some code constructs for faster compilation.", "src/lxml/sax.py"]}, {"commit_id": "6c2d46e785abb939a5cc9a0d752241d54da46683", "commit_date": "Sun Dec 2 16:20:37 2018 +0100", "commit_message": "Speed up sax.py by converting ElementTreeProducer into an extension type and inlining its internal method calls.", "files_name": ["src/lxml/sax.pxd"]}, {"commit_id": "9057bd1c3495ea1ed7b0569949ef7481fc1dc350", "commit_date": "Sun Dec 2 17:26:45 2018 +0100", "commit_message": "Set explicit Cython language levels for compiled modules (Cython suggests to make them explicit).", "files_name": ["src/lxml/_elementpath.py", "src/lxml/builder.pxd", "src/lxml/builder.py", "src/lxml/etree.pyx", "src/lxml/html/clean.py", "src/lxml/html/diff.py", "src/lxml/objectify.pyx", "src/lxml/sax.pxd", "src/lxml/sax.py"]}, {"commit_id": "f365016531d73186bead3daf6337a397585a1732", "commit_date": "Sun Dec 2 17:28:07 2018 +0100", "commit_message": "Fix command in make target.", "files_name": ["Makefile"]}, {"commit_id": "013c309b604021839ef99b36d601aa6f8323db28", "commit_date": "Sun Dec 2 17:55:43 2018 +0100", "commit_message": "Fix compile problem due to language_level=3: \"basestring\" must still refer to \"str/unicode\" in Py2.", "files_name": ["src/lxml/builder.py"]}, {"commit_id": "d211622bdcc40c63b542a53411069885b0789f17", "commit_date": "Sun Dec 2 18:27:13 2018 +0100", "commit_message": "Actually use \"language_level=2\" everywhere for better Py2 compatibility.", "files_name": ["src/lxml/_elementpath.py", "src/lxml/builder.pxd", "src/lxml/html/clean.py", "src/lxml/sax.pxd", "src/lxml/sax.py"]}, {"commit_id": "38ce4d5e783809ab4c60139d1d4f178b96592fd6", "commit_date": "Sun Dec 2 18:36:02 2018 +0100", "commit_message": "Simplify ccache usage by relying on its aliases being in the path before gcc.", "files_name": [".travis.yml"]}, {"commit_id": "b23b4090e2279553bb63dac8ba23626ecadcdd38", "commit_date": "Sun Dec 2 18:40:54 2018 +0100", "commit_message": "Update changelog.", "files_name": ["CHANGES.txt"]}, {"commit_id": "24706601a30a5915e7799f83738c82cd47dd7c78", "commit_date": "Sun Dec 2 18:44:52 2018 +0100", "commit_message": "Use newest Cython (0.29 is required for Py3.7 support).", "files_name": ["doc/build.txt", "requirements.txt"]}, {"commit_id": "b767e9c398bcf0a0f1d5db7e291b5363547b2f0b", "commit_date": "Sun Dec 2 19:06:04 2018 +0100", "commit_message": "Update changelog.", "files_name": ["CHANGES.txt"]}, {"commit_id": "1dd26eb772abd58ae3aea596800ed0cd612cf145", "commit_date": "Wed Jan 2 18:15:09 2019 +0100", "commit_message": "Prepare release of 4.2.6.", "files_name": ["CHANGES.txt", "doc/main.txt", "version.txt"]}, {"commit_id": "c7bdc8c0e32e48c6730332e6b629ccb351c0f91b", "commit_date": "Wed Jan 2 18:23:24 2019 +0100", "commit_message": "Merge lxml-4.2 branch into master.", "files_name": ["c2324cf5832d8e2347751940a0205c46775e5f86 - Wed Jan 2 21:56:16 2019 +0100 : Py3 fix in PDF docs builder script.", "doc/rest2latex.py"]}, {"commit_id": "d255d4aed7db4d2c86aa2cca8cc25b1b3236ff61", "commit_date": "Fri Jan 4 15:13:04 2019 +0100", "commit_message": "Removed leftover comment from changelog.", "files_name": ["CHANGES.txt"]}, {"commit_id": "fa6e7f975129b68e70dace876b62b7b683df3df4", "commit_date": "Fri Jan 4 15:15:46 2019 +0100", "commit_message": "Increase default libxml2/libxslt versions to 2.9.9 and 1.1.33.", "files_name": [".travis.yml", "Makefile"]}, {"commit_id": "925a6fb21bdfdd17c1e3fa8d28922b95f19ee8b2", "commit_date": "Fri Jan 4 16:06:25 2019 +0100", "commit_message": "Use http(s) download URLs for build libraries instead of FTP, since it's much safer and also more reliable on travis.", "files_name": ["buildlibxml.py"]}, {"commit_id": "1da2827263dcd756014d0ded78ca5fb780341a99", "commit_date": "Fri Jan 4 16:18:20 2019 +0100", "commit_message": "Prioritise non-static builds in travis to get faster responsiveness.", "files_name": [".travis.yml"]}, {"commit_id": "b4a7df9ab43d6ecc653711948b39e3366b48eae4", "commit_date": "Fri Jan 4 16:24:28 2019 +0100", "commit_message": "Also show ccache stats after the test run, in case more files were compiled.", "files_name": [".travis.yml"]}, {"commit_id": "7303cadd01b81fceb40f74148a5b9b6178936768", "commit_date": "Fri Jan 4 16:29:32 2019 +0100", "commit_message": "Prepare release of lxml 4.3.0.", "files_name": ["CHANGES.txt", "doc/main.txt", "version.txt"]}, {"commit_id": "201b712edf0478e6a94ace984c1e8435bf3bc3c3", "commit_date": "Tue Feb 5 21:31:02 2019 +0100", "commit_message": "LP#1814522: Fix a crash when appending a child subtree that contains unsubstituted entity references. This is a work-around for a (supposed) bug in libxml2 (https://gitlab.gnome.org/GNOME/libxml2/issues/42), which crashes by running into an infinite recursive loop while traversing the child nodes of the entity reference. A lucky side effect is that the previously duplicated cleanup traversal to a) update the .doc pointers in libxml2 and b) update the dict names in lxml is now replaced by a single traversal, which should speed things up for large subtrees.", "files_name": ["CHANGES.txt", "src/lxml/apihelpers.pxi", "src/lxml/includes/tree.pxd", "src/lxml/proxy.pxi", "src/lxml/tests/test_etree.py"]}, {"commit_id": "fc0a4d3cfe410dc3483ada551781203a95167964", "commit_date": "Wed Feb 6 21:15:11 2019 +0100", "commit_message": "Run tests in appveyor.", "files_name": [".appveyor.yml"]}, {"commit_id": "10ee3839744ff41eca4737ee1fc44db4fc8470e9", "commit_date": "Wed Feb 6 21:19:17 2019 +0100", "commit_message": "First build, *then* run the tests in appveyor. Also reorder the Python versions to get faster feedback on the most important ones.", "files_name": [".appveyor.yml"]}, {"commit_id": "9a6db11a42f3239f3f2c1c4386f3fbe7eb924d9d", "commit_date": "Wed Feb 6 21:22:17 2019 +0100", "commit_message": "Rename appveyor script to more common name without leading dot.", "files_name": ["appveyor.yml"]}], "parents": [{"commit_id_before": "1f534e2b957c0ea537c42d87fc262cb7069f0b1c", "url_before": "https://api.github.com/repos/lxml/lxml/commits/1f534e2b957c0ea537c42d87fc262cb7069f0b1c", "html_url_before": "https://github.com/lxml/lxml/commit/1f534e2b957c0ea537c42d87fc262cb7069f0b1c"}], "details": [{"raw_url": "https://github.com/lxml/lxml/raw/6be1d081b49c97cfd7b3fbd934a193b668629109/src%2Flxml%2Fhtml%2Fclean.py", "code": "\"\"\"A cleanup tool for HTML.\n\nRemoves unwanted tags and content.  See the `Cleaner` class for\ndetails.\n\"\"\"\n\nimport re\nimport copy\ntry:\n    from urlparse import urlsplit\n    from urllib import unquote_plus\nexcept ImportError:\n    # Python 3\n    from urllib.parse import urlsplit, unquote_plus\nfrom lxml import etree\nfrom lxml.html import defs\nfrom lxml.html import fromstring, XHTML_NAMESPACE\nfrom lxml.html import xhtml_to_html, _transform_result\n\ntry:\n    unichr\nexcept NameError:\n    # Python 3\n    unichr = chr\ntry:\n    unicode\nexcept NameError:\n    # Python 3\n    unicode = str\ntry:\n    bytes\nexcept NameError:\n    # Python < 2.6\n    bytes = str\ntry:\n    basestring\nexcept NameError:\n    basestring = (str, bytes)\n\n\n__all__ = ['clean_html', 'clean', 'Cleaner', 'autolink', 'autolink_html',\n           'word_break', 'word_break_html']\n\n# Look at http://code.sixapart.com/trac/livejournal/browser/trunk/cgi-bin/cleanhtml.pl\n#   Particularly the CSS cleaning; most of the tag cleaning is integrated now\n# I have multiple kinds of schemes searched; but should schemes be\n#   whitelisted instead?\n# max height?\n# remove images?  Also in CSS?  background attribute?\n# Some way to whitelist object, iframe, etc (e.g., if you want to\n#   allow *just* embedded YouTube movies)\n# Log what was deleted and why?\n# style=\"behavior: ...\" might be bad in IE?\n# Should we have something for just <meta http-equiv>?  That's the worst of the\n#   metas.\n# UTF-7 detections?  Example:\n#     <HEAD><META HTTP-EQUIV=\"CONTENT-TYPE\" CONTENT=\"text/html; charset=UTF-7\"> </HEAD>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\n#   you don't always have to have the charset set, if the page has no charset\n#   and there's UTF7-like code in it.\n# Look at these tests: http://htmlpurifier.org/live/smoketests/xssAttacks.php\n\n\n# This is an IE-specific construct you can have in a stylesheet to\n# run some Javascript:\n_css_javascript_re = re.compile(\n    r'expression\\s*\\(.*?\\)', re.S|re.I)\n\n# Do I have to worry about @\\nimport?\n_css_import_re = re.compile(\n    r'@\\s*import', re.I)\n\n# All kinds of schemes besides just javascript: that can cause\n# execution:\n_is_image_dataurl = re.compile(\n    r'^data:image/.+;base64', re.I).search\n_is_possibly_malicious_scheme = re.compile(\n    r'(?:javascript|jscript|livescript|vbscript|data|about|mocha):',\n    re.I).search\ndef _is_javascript_scheme(s):\n    if _is_image_dataurl(s):\n        return None\n    return _is_possibly_malicious_scheme(s)\n\n_substitute_whitespace = re.compile(r'[\\s\\x00-\\x08\\x0B\\x0C\\x0E-\\x19]+').sub\n# FIXME: should data: be blocked?\n\n# FIXME: check against: http://msdn2.microsoft.com/en-us/library/ms537512.aspx\n_conditional_comment_re = re.compile(\n    r'\\[if[\\s\\n\\r]+.*?][\\s\\n\\r]*>', re.I|re.S)\n\n_find_styled_elements = etree.XPath(\n    \"descendant-or-self::*[@style]\")\n\n_find_external_links = etree.XPath(\n    (\"descendant-or-self::a  [normalize-space(@href) and substring(normalize-space(@href),1,1) != '#'] |\"\n     \"descendant-or-self::x:a[normalize-space(@href) and substring(normalize-space(@href),1,1) != '#']\"),\n    namespaces={'x':XHTML_NAMESPACE})\n\n\nclass Cleaner(object):\n    \"\"\"\n    Instances cleans the document of each of the possible offending\n    elements.  The cleaning is controlled by attributes; you can\n    override attributes in a subclass, or set them in the constructor.\n\n    ``scripts``:\n        Removes any ``<script>`` tags.\n\n    ``javascript``:\n        Removes any Javascript, like an ``onclick`` attribute. Also removes stylesheets\n        as they could contain Javascript.\n\n    ``comments``:\n        Removes any comments.\n\n    ``style``:\n        Removes any style tags.\n\n    ``inline_style``\n        Removes any style attributes.  Defaults to the value of the ``style`` option.\n\n    ``links``:\n        Removes any ``<link>`` tags\n\n    ``meta``:\n        Removes any ``<meta>`` tags\n\n    ``page_structure``:\n        Structural parts of a page: ``<head>``, ``<html>``, ``<title>``.\n\n    ``processing_instructions``:\n        Removes any processing instructions.\n\n    ``embedded``:\n        Removes any embedded objects (flash, iframes)\n\n    ``frames``:\n        Removes any frame-related tags\n\n    ``forms``:\n        Removes any form tags\n\n    ``annoying_tags``:\n        Tags that aren't *wrong*, but are annoying.  ``<blink>`` and ``<marquee>``\n\n    ``remove_tags``:\n        A list of tags to remove.  Only the tags will be removed,\n        their content will get pulled up into the parent tag.\n\n    ``kill_tags``:\n        A list of tags to kill.  Killing also removes the tag's content,\n        i.e. the whole subtree, not just the tag itself.\n\n    ``allow_tags``:\n        A list of tags to include (default include all).\n\n    ``remove_unknown_tags``:\n        Remove any tags that aren't standard parts of HTML.\n\n    ``safe_attrs_only``:\n        If true, only include 'safe' attributes (specifically the list\n        from the feedparser HTML sanitisation web site).\n\n    ``safe_attrs``:\n        A set of attribute names to override the default list of attributes\n        considered 'safe' (when safe_attrs_only=True).\n\n    ``add_nofollow``:\n        If true, then any <a> tags will have ``rel=\"nofollow\"`` added to them.\n\n    ``host_whitelist``:\n        A list or set of hosts that you can use for embedded content\n        (for content like ``<object>``, ``<link rel=\"stylesheet\">``, etc).\n        You can also implement/override the method\n        ``allow_embedded_url(el, url)`` or ``allow_element(el)`` to\n        implement more complex rules for what can be embedded.\n        Anything that passes this test will be shown, regardless of\n        the value of (for instance) ``embedded``.\n\n        Note that this parameter might not work as intended if you do not\n        make the links absolute before doing the cleaning.\n\n        Note that you may also need to set ``whitelist_tags``.\n\n    ``whitelist_tags``:\n        A set of tags that can be included with ``host_whitelist``.\n        The default is ``iframe`` and ``embed``; you may wish to\n        include other tags like ``script``, or you may want to\n        implement ``allow_embedded_url`` for more control.  Set to None to\n        include all tags.\n\n    This modifies the document *in place*.\n    \"\"\"\n\n    scripts = True\n    javascript = True\n    comments = True\n    style = False\n    inline_style = None\n    links = True\n    meta = True\n    page_structure = True\n    processing_instructions = True\n    embedded = True\n    frames = True\n    forms = True\n    annoying_tags = True\n    remove_tags = None\n    allow_tags = None\n    kill_tags = None\n    remove_unknown_tags = True\n    safe_attrs_only = True\n    safe_attrs = defs.safe_attrs\n    add_nofollow = False\n    host_whitelist = ()\n    whitelist_tags = set(['iframe', 'embed'])\n\n    def __init__(self, **kw):\n        for name, value in kw.items():\n            if not hasattr(self, name):\n                raise TypeError(\n                    \"Unknown parameter: %s=%r\" % (name, value))\n            setattr(self, name, value)\n        if self.inline_style is None and 'inline_style' not in kw:\n            self.inline_style = self.style\n\n    # Used to lookup the primary URL for a given tag that is up for\n    # removal:\n    _tag_link_attrs = dict(\n        script='src',\n        link='href',\n        # From: http://java.sun.com/j2se/1.4.2/docs/guide/misc/applet.html\n        # From what I can tell, both attributes can contain a link:\n        applet=['code', 'object'],\n        iframe='src',\n        embed='src',\n        layer='src',\n        # FIXME: there doesn't really seem like a general way to figure out what\n        # links an <object> tag uses; links often go in <param> tags with values\n        # that we don't really know.  You'd have to have knowledge about specific\n        # kinds of plugins (probably keyed off classid), and match against those.\n        ##object=?,\n        # FIXME: not looking at the action currently, because it is more complex\n        # than than -- if you keep the form, you should keep the form controls.\n        ##form='action',\n        a='href',\n        )\n\n    def __call__(self, doc):\n        \"\"\"\n        Cleans the document.\n        \"\"\"\n        if hasattr(doc, 'getroot'):\n            # ElementTree instance, instead of an element\n            doc = doc.getroot()\n        # convert XHTML to HTML\n        xhtml_to_html(doc)\n        # Normalize a case that IE treats <image> like <img>, and that\n        # can confuse either this step or later steps.\n        for el in doc.iter('image'):\n            el.tag = 'img'\n        if not self.comments:\n            # Of course, if we were going to kill comments anyway, we don't\n            # need to worry about this\n            self.kill_conditional_comments(doc)\n\n        kill_tags = set(self.kill_tags or ())\n        remove_tags = set(self.remove_tags or ())\n        allow_tags = set(self.allow_tags or ())\n\n        if self.scripts:\n            kill_tags.add('script')\n        if self.safe_attrs_only:\n            safe_attrs = set(self.safe_attrs)\n            for el in doc.iter(etree.Element):\n                attrib = el.attrib\n                for aname in attrib.keys():\n                    if aname not in safe_attrs:\n                        del attrib[aname]\n        if self.javascript:\n            if not (self.safe_attrs_only and\n                    self.safe_attrs == defs.safe_attrs):\n                # safe_attrs handles events attributes itself\n                for el in doc.iter(etree.Element):\n                    attrib = el.attrib\n                    for aname in attrib.keys():\n                        if aname.startswith('on'):\n                            del attrib[aname]\n            doc.rewrite_links(self._remove_javascript_link,\n                              resolve_base_href=False)\n            # If we're deleting style then we don't have to remove JS links\n            # from styles, otherwise...\n            if not self.inline_style:\n                for el in _find_styled_elements(doc):\n                    old = el.get('style')\n                    new = _css_javascript_re.sub('', old)\n                    new = _css_import_re.sub('', new)\n                    if self._has_sneaky_javascript(new):\n                        # Something tricky is going on...\n                        del el.attrib['style']\n                    elif new != old:\n                        el.set('style', new)\n            if not self.style:\n                for el in list(doc.iter('style')):\n                    if el.get('type', '').lower().strip() == 'text/javascript':\n                        el.drop_tree()\n                        continue\n                    old = el.text or ''\n                    new = _css_javascript_re.sub('', old)\n                    # The imported CSS can do anything; we just can't allow:\n                    new = _css_import_re.sub('', old)\n                    if self._has_sneaky_javascript(new):\n                        # Something tricky is going on...\n                        el.text = '/* deleted */'\n                    elif new != old:\n                        el.text = new\n        if self.comments or self.processing_instructions:\n            # FIXME: why either?  I feel like there's some obscure reason\n            # because you can put PIs in comments...?  But I've already\n            # forgotten it\n            kill_tags.add(etree.Comment)\n        if self.processing_instructions:\n            kill_tags.add(etree.ProcessingInstruction)\n        if self.style:\n            kill_tags.add('style')\n        if self.inline_style:\n            etree.strip_attributes(doc, 'style')\n        if self.links:\n            kill_tags.add('link')\n        elif self.style or self.javascript:\n            # We must get rid of included stylesheets if Javascript is not\n            # allowed, as you can put Javascript in them\n            for el in list(doc.iter('link')):\n                if 'stylesheet' in el.get('rel', '').lower():\n                    # Note this kills alternate stylesheets as well\n                    if not self.allow_element(el):\n                        el.drop_tree()\n        if self.meta:\n            kill_tags.add('meta')\n        if self.page_structure:\n            remove_tags.update(('head', 'html', 'title'))\n        if self.embedded:\n            # FIXME: is <layer> really embedded?\n            # We should get rid of any <param> tags not inside <applet>;\n            # These are not really valid anyway.\n            for el in list(doc.iter('param')):\n                found_parent = False\n                parent = el.getparent()\n                while parent is not None and parent.tag not in ('applet', 'object'):\n                    parent = parent.getparent()\n                if parent is None:\n                    el.drop_tree()\n            kill_tags.update(('applet',))\n            # The alternate contents that are in an iframe are a good fallback:\n            remove_tags.update(('iframe', 'embed', 'layer', 'object', 'param'))\n        if self.frames:\n            # FIXME: ideally we should look at the frame links, but\n            # generally frames don't mix properly with an HTML\n            # fragment anyway.\n            kill_tags.update(defs.frame_tags)\n        if self.forms:\n            remove_tags.add('form')\n            kill_tags.update(('button', 'input', 'select', 'textarea'))\n        if self.annoying_tags:\n            remove_tags.update(('blink', 'marquee'))\n\n        _remove = []\n        _kill = []\n        for el in doc.iter():\n            if el.tag in kill_tags:\n                if self.allow_element(el):\n                    continue\n                _kill.append(el)\n            elif el.tag in remove_tags:\n                if self.allow_element(el):\n                    continue\n                _remove.append(el)\n\n        if _remove and _remove[0] == doc:\n            # We have to drop the parent-most tag, which we can't\n            # do.  Instead we'll rewrite it:\n            el = _remove.pop(0)\n            el.tag = 'div'\n            el.attrib.clear()\n        elif _kill and _kill[0] == doc:\n            # We have to drop the parent-most element, which we can't\n            # do.  Instead we'll clear it:\n            el = _kill.pop(0)\n            if el.tag != 'html':\n                el.tag = 'div'\n            el.clear()\n\n        _kill.reverse() # start with innermost tags\n        for el in _kill:\n            el.drop_tree()\n        for el in _remove:\n            el.drop_tag()\n\n        if self.remove_unknown_tags:\n            if allow_tags:\n                raise ValueError(\n                    \"It does not make sense to pass in both allow_tags and remove_unknown_tags\")\n            allow_tags = set(defs.tags)\n        if allow_tags:\n            bad = []\n            for el in doc.iter():\n                if el.tag not in allow_tags:\n                    bad.append(el)\n            if bad:\n                if bad[0] is doc:\n                    el = bad.pop(0)\n                    el.tag = 'div'\n                    el.attrib.clear()\n                for el in bad:\n                    el.drop_tag()\n        if self.add_nofollow:\n            for el in _find_external_links(doc):\n                if not self.allow_follow(el):\n                    rel = el.get('rel')\n                    if rel:\n                        if ('nofollow' in rel\n                                and ' nofollow ' in (' %s ' % rel)):\n                            continue\n                        rel = '%s nofollow' % rel\n                    else:\n                        rel = 'nofollow'\n                    el.set('rel', rel)\n\n    def allow_follow(self, anchor):\n        \"\"\"\n        Override to suppress rel=\"nofollow\" on some anchors.\n        \"\"\"\n        return False\n\n    def allow_element(self, el):\n        if el.tag not in self._tag_link_attrs:\n            return False\n        attr = self._tag_link_attrs[el.tag]\n        if isinstance(attr, (list, tuple)):\n            for one_attr in attr:\n                url = el.get(one_attr)\n                if not url:\n                    return False\n                if not self.allow_embedded_url(el, url):\n                    return False\n            return True\n        else:\n            url = el.get(attr)\n            if not url:\n                return False\n            return self.allow_embedded_url(el, url)\n\n    def allow_embedded_url(self, el, url):\n        if (self.whitelist_tags is not None\n            and el.tag not in self.whitelist_tags):\n            return False\n        scheme, netloc, path, query, fragment = urlsplit(url)\n        netloc = netloc.lower().split(':', 1)[0]\n        if scheme not in ('http', 'https'):\n            return False\n        if netloc in self.host_whitelist:\n            return True\n        return False\n\n    def kill_conditional_comments(self, doc):\n        \"\"\"\n        IE conditional comments basically embed HTML that the parser\n        doesn't normally see.  We can't allow anything like that, so\n        we'll kill any comments that could be conditional.\n        \"\"\"\n        bad = []\n        self._kill_elements(\n            doc, lambda el: _conditional_comment_re.search(el.text),\n            etree.Comment)                \n\n    def _kill_elements(self, doc, condition, iterate=None):\n        bad = []\n        for el in doc.iter(iterate):\n            if condition(el):\n                bad.append(el)\n        for el in bad:\n            el.drop_tree()\n\n    def _remove_javascript_link(self, link):\n        # links like \"j a v a s c r i p t:\" might be interpreted in IE\n        new = _substitute_whitespace('', unquote_plus(link))\n        if _is_javascript_scheme(new):\n            # FIXME: should this be None to delete?\n            return ''\n        return link\n\n    _substitute_comments = re.compile(r'/\\*.*?\\*/', re.S).sub\n\n    def _has_sneaky_javascript(self, style):\n        \"\"\"\n        Depending on the browser, stuff like ``e x p r e s s i o n(...)``\n        can get interpreted, or ``expre/* stuff */ssion(...)``.  This\n        checks for attempt to do stuff like this.\n\n        Typically the response will be to kill the entire style; if you\n        have just a bit of Javascript in the style another rule will catch\n        that and remove only the Javascript from the style; this catches\n        more sneaky attempts.\n        \"\"\"\n        style = self._substitute_comments('', style)\n        style = style.replace('\\\\', '')\n        style = _substitute_whitespace('', style)\n        style = style.lower()\n        if 'javascript:' in style:\n            return True\n        if 'expression(' in style:\n            return True\n        return False\n\n    def clean_html(self, html):\n        result_type = type(html)\n        if isinstance(html, basestring):\n            doc = fromstring(html)\n        else:\n            doc = copy.deepcopy(html)\n        self(doc)\n        return _transform_result(result_type, doc)\n\nclean = Cleaner()\nclean_html = clean.clean_html\n\n############################################################\n## Autolinking\n############################################################\n\n_link_regexes = [\n    re.compile(r'(?P<body>https?://(?P<host>[a-z0-9._-]+)(?:/[/\\-_.,a-z0-9%&?;=~]*)?(?:\\([/\\-_.,a-z0-9%&?;=~]*\\))?)', re.I),\n    # This is conservative, but autolinking can be a bit conservative:\n    re.compile(r'mailto:(?P<body>[a-z0-9._-]+@(?P<host>[a-z0-9_.-]+[a-z]))', re.I),\n    ]\n\n_avoid_elements = ['textarea', 'pre', 'code', 'head', 'select', 'a']\n\n_avoid_hosts = [\n    re.compile(r'^localhost', re.I),\n    re.compile(r'\\bexample\\.(?:com|org|net)$', re.I),\n    re.compile(r'^127\\.0\\.0\\.1$'),\n    ]\n\n_avoid_classes = ['nolink']\n\ndef autolink(el, link_regexes=_link_regexes,\n             avoid_elements=_avoid_elements,\n             avoid_hosts=_avoid_hosts,\n             avoid_classes=_avoid_classes):\n    \"\"\"\n    Turn any URLs into links.\n\n    It will search for links identified by the given regular\n    expressions (by default mailto and http(s) links).\n\n    It won't link text in an element in avoid_elements, or an element\n    with a class in avoid_classes.  It won't link to anything with a\n    host that matches one of the regular expressions in avoid_hosts\n    (default localhost and 127.0.0.1).\n\n    If you pass in an element, the element's tail will not be\n    substituted, only the contents of the element.\n    \"\"\"\n    if el.tag in avoid_elements:\n        return\n    class_name = el.get('class')\n    if class_name:\n        class_name = class_name.split()\n        for match_class in avoid_classes:\n            if match_class in class_name:\n                return\n    for child in list(el):\n        autolink(child, link_regexes=link_regexes,\n                 avoid_elements=avoid_elements,\n                 avoid_hosts=avoid_hosts,\n                 avoid_classes=avoid_classes)\n        if child.tail:\n            text, tail_children = _link_text(\n                child.tail, link_regexes, avoid_hosts, factory=el.makeelement)\n            if tail_children:\n                child.tail = text\n                index = el.index(child)\n                el[index+1:index+1] = tail_children\n    if el.text:\n        text, pre_children = _link_text(\n            el.text, link_regexes, avoid_hosts, factory=el.makeelement)\n        if pre_children:\n            el.text = text\n            el[:0] = pre_children\n\ndef _link_text(text, link_regexes, avoid_hosts, factory):\n    leading_text = ''\n    links = []\n    last_pos = 0\n    while 1:\n        best_match, best_pos = None, None\n        for regex in link_regexes:\n            regex_pos = last_pos\n            while 1:\n                match = regex.search(text, pos=regex_pos)\n                if match is None:\n                    break\n                host = match.group('host')\n                for host_regex in avoid_hosts:\n                    if host_regex.search(host):\n                        regex_pos = match.end()\n                        break\n                else:\n                    break\n            if match is None:\n                continue\n            if best_pos is None or match.start() < best_pos:\n                best_match = match\n                best_pos = match.start()\n        if best_match is None:\n            # No more matches\n            if links:\n                assert not links[-1].tail\n                links[-1].tail = text\n            else:\n                assert not leading_text\n                leading_text = text\n            break\n        link = best_match.group(0)\n        end = best_match.end()\n        if link.endswith('.') or link.endswith(','):\n            # These punctuation marks shouldn't end a link\n            end -= 1\n            link = link[:-1]\n        prev_text = text[:best_match.start()]\n        if links:\n            assert not links[-1].tail\n            links[-1].tail = prev_text\n        else:\n            assert not leading_text\n            leading_text = prev_text\n        anchor = factory('a')\n        anchor.set('href', link)\n        body = best_match.group('body')\n        if not body:\n            body = link\n        if body.endswith('.') or body.endswith(','):\n            body = body[:-1]\n        anchor.text = body\n        links.append(anchor)\n        text = text[end:]\n    return leading_text, links\n                \ndef autolink_html(html, *args, **kw):\n    result_type = type(html)\n    if isinstance(html, basestring):\n        doc = fromstring(html)\n    else:\n        doc = copy.deepcopy(html)\n    autolink(doc, *args, **kw)\n    return _transform_result(result_type, doc)\n\nautolink_html.__doc__ = autolink.__doc__\n\n############################################################\n## Word wrapping\n############################################################\n\n_avoid_word_break_elements = ['pre', 'textarea', 'code']\n_avoid_word_break_classes = ['nobreak']\n\ndef word_break(el, max_width=40,\n               avoid_elements=_avoid_word_break_elements,\n               avoid_classes=_avoid_word_break_classes,\n               break_character=unichr(0x200b)):\n    \"\"\"\n    Breaks any long words found in the body of the text (not attributes).\n\n    Doesn't effect any of the tags in avoid_elements, by default\n    ``<textarea>`` and ``<pre>``\n\n    Breaks words by inserting &#8203;, which is a unicode character\n    for Zero Width Space character.  This generally takes up no space\n    in rendering, but does copy as a space, and in monospace contexts\n    usually takes up space.\n\n    See http://www.cs.tut.fi/~jkorpela/html/nobr.html for a discussion\n    \"\"\"\n    # Character suggestion of &#8203 comes from:\n    #   http://www.cs.tut.fi/~jkorpela/html/nobr.html\n    if el.tag in _avoid_word_break_elements:\n        return\n    class_name = el.get('class')\n    if class_name:\n        dont_break = False\n        class_name = class_name.split()\n        for avoid in avoid_classes:\n            if avoid in class_name:\n                dont_break = True\n                break\n        if dont_break:\n            return\n    if el.text:\n        el.text = _break_text(el.text, max_width, break_character)\n    for child in el:\n        word_break(child, max_width=max_width,\n                   avoid_elements=avoid_elements,\n                   avoid_classes=avoid_classes,\n                   break_character=break_character)\n        if child.tail:\n            child.tail = _break_text(child.tail, max_width, break_character)\n\ndef word_break_html(html, *args, **kw):\n    result_type = type(html)\n    doc = fromstring(html)\n    word_break(doc, *args, **kw)\n    return _transform_result(result_type, doc)\n\ndef _break_text(text, max_width, break_character):\n    words = text.split()\n    for word in words:\n        if len(word) > max_width:\n            replacement = _insert_break(word, max_width, break_character)\n            text = text.replace(word, replacement)\n    return text\n\n_break_prefer_re = re.compile(r'[^a-z]', re.I)\n\ndef _insert_break(word, width, break_character):\n    orig_word = word\n    result = ''\n    while len(word) > width:\n        start = word[:width]\n        breaks = list(_break_prefer_re.finditer(start))\n        if breaks:\n            last_break = breaks[-1]\n            # Only walk back up to 10 characters to find a nice break:\n            if last_break.end() > width-10:\n                # FIXME: should the break character be at the end of the\n                # chunk, or the beginning of the next chunk?\n                start = word[:last_break.end()]\n        result += start + break_character\n        word = word[len(start):]\n    result += word\n    return result\n    \n", "code_before": "\"\"\"A cleanup tool for HTML.\n\nRemoves unwanted tags and content.  See the `Cleaner` class for\ndetails.\n\"\"\"\n\nimport re\nimport copy\ntry:\n    from urlparse import urlsplit\nexcept ImportError:\n    # Python 3\n    from urllib.parse import urlsplit\nfrom lxml import etree\nfrom lxml.html import defs\nfrom lxml.html import fromstring, XHTML_NAMESPACE\nfrom lxml.html import xhtml_to_html, _transform_result\n\ntry:\n    unichr\nexcept NameError:\n    # Python 3\n    unichr = chr\ntry:\n    unicode\nexcept NameError:\n    # Python 3\n    unicode = str\ntry:\n    bytes\nexcept NameError:\n    # Python < 2.6\n    bytes = str\ntry:\n    basestring\nexcept NameError:\n    basestring = (str, bytes)\n\n\n__all__ = ['clean_html', 'clean', 'Cleaner', 'autolink', 'autolink_html',\n           'word_break', 'word_break_html']\n\n# Look at http://code.sixapart.com/trac/livejournal/browser/trunk/cgi-bin/cleanhtml.pl\n#   Particularly the CSS cleaning; most of the tag cleaning is integrated now\n# I have multiple kinds of schemes searched; but should schemes be\n#   whitelisted instead?\n# max height?\n# remove images?  Also in CSS?  background attribute?\n# Some way to whitelist object, iframe, etc (e.g., if you want to\n#   allow *just* embedded YouTube movies)\n# Log what was deleted and why?\n# style=\"behavior: ...\" might be bad in IE?\n# Should we have something for just <meta http-equiv>?  That's the worst of the\n#   metas.\n# UTF-7 detections?  Example:\n#     <HEAD><META HTTP-EQUIV=\"CONTENT-TYPE\" CONTENT=\"text/html; charset=UTF-7\"> </HEAD>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\n#   you don't always have to have the charset set, if the page has no charset\n#   and there's UTF7-like code in it.\n# Look at these tests: http://htmlpurifier.org/live/smoketests/xssAttacks.php\n\n\n# This is an IE-specific construct you can have in a stylesheet to\n# run some Javascript:\n_css_javascript_re = re.compile(\n    r'expression\\s*\\(.*?\\)', re.S|re.I)\n\n# Do I have to worry about @\\nimport?\n_css_import_re = re.compile(\n    r'@\\s*import', re.I)\n\n# All kinds of schemes besides just javascript: that can cause\n# execution:\n_is_image_dataurl = re.compile(\n    r'^data:image/.+;base64', re.I).search\n_is_possibly_malicious_scheme = re.compile(\n    r'(?:javascript|jscript|livescript|vbscript|data|about|mocha):',\n    re.I).search\ndef _is_javascript_scheme(s):\n    if _is_image_dataurl(s):\n        return None\n    return _is_possibly_malicious_scheme(s)\n\n_substitute_whitespace = re.compile(r'[\\s\\x00-\\x08\\x0B\\x0C\\x0E-\\x19]+').sub\n# FIXME: should data: be blocked?\n\n# FIXME: check against: http://msdn2.microsoft.com/en-us/library/ms537512.aspx\n_conditional_comment_re = re.compile(\n    r'\\[if[\\s\\n\\r]+.*?][\\s\\n\\r]*>', re.I|re.S)\n\n_find_styled_elements = etree.XPath(\n    \"descendant-or-self::*[@style]\")\n\n_find_external_links = etree.XPath(\n    (\"descendant-or-self::a  [normalize-space(@href) and substring(normalize-space(@href),1,1) != '#'] |\"\n     \"descendant-or-self::x:a[normalize-space(@href) and substring(normalize-space(@href),1,1) != '#']\"),\n    namespaces={'x':XHTML_NAMESPACE})\n\n\nclass Cleaner(object):\n    \"\"\"\n    Instances cleans the document of each of the possible offending\n    elements.  The cleaning is controlled by attributes; you can\n    override attributes in a subclass, or set them in the constructor.\n\n    ``scripts``:\n        Removes any ``<script>`` tags.\n\n    ``javascript``:\n        Removes any Javascript, like an ``onclick`` attribute. Also removes stylesheets\n        as they could contain Javascript.\n\n    ``comments``:\n        Removes any comments.\n\n    ``style``:\n        Removes any style tags.\n\n    ``inline_style``\n        Removes any style attributes.  Defaults to the value of the ``style`` option.\n\n    ``links``:\n        Removes any ``<link>`` tags\n\n    ``meta``:\n        Removes any ``<meta>`` tags\n\n    ``page_structure``:\n        Structural parts of a page: ``<head>``, ``<html>``, ``<title>``.\n\n    ``processing_instructions``:\n        Removes any processing instructions.\n\n    ``embedded``:\n        Removes any embedded objects (flash, iframes)\n\n    ``frames``:\n        Removes any frame-related tags\n\n    ``forms``:\n        Removes any form tags\n\n    ``annoying_tags``:\n        Tags that aren't *wrong*, but are annoying.  ``<blink>`` and ``<marquee>``\n\n    ``remove_tags``:\n        A list of tags to remove.  Only the tags will be removed,\n        their content will get pulled up into the parent tag.\n\n    ``kill_tags``:\n        A list of tags to kill.  Killing also removes the tag's content,\n        i.e. the whole subtree, not just the tag itself.\n\n    ``allow_tags``:\n        A list of tags to include (default include all).\n\n    ``remove_unknown_tags``:\n        Remove any tags that aren't standard parts of HTML.\n\n    ``safe_attrs_only``:\n        If true, only include 'safe' attributes (specifically the list\n        from the feedparser HTML sanitisation web site).\n\n    ``safe_attrs``:\n        A set of attribute names to override the default list of attributes\n        considered 'safe' (when safe_attrs_only=True).\n\n    ``add_nofollow``:\n        If true, then any <a> tags will have ``rel=\"nofollow\"`` added to them.\n\n    ``host_whitelist``:\n        A list or set of hosts that you can use for embedded content\n        (for content like ``<object>``, ``<link rel=\"stylesheet\">``, etc).\n        You can also implement/override the method\n        ``allow_embedded_url(el, url)`` or ``allow_element(el)`` to\n        implement more complex rules for what can be embedded.\n        Anything that passes this test will be shown, regardless of\n        the value of (for instance) ``embedded``.\n\n        Note that this parameter might not work as intended if you do not\n        make the links absolute before doing the cleaning.\n\n        Note that you may also need to set ``whitelist_tags``.\n\n    ``whitelist_tags``:\n        A set of tags that can be included with ``host_whitelist``.\n        The default is ``iframe`` and ``embed``; you may wish to\n        include other tags like ``script``, or you may want to\n        implement ``allow_embedded_url`` for more control.  Set to None to\n        include all tags.\n\n    This modifies the document *in place*.\n    \"\"\"\n\n    scripts = True\n    javascript = True\n    comments = True\n    style = False\n    inline_style = None\n    links = True\n    meta = True\n    page_structure = True\n    processing_instructions = True\n    embedded = True\n    frames = True\n    forms = True\n    annoying_tags = True\n    remove_tags = None\n    allow_tags = None\n    kill_tags = None\n    remove_unknown_tags = True\n    safe_attrs_only = True\n    safe_attrs = defs.safe_attrs\n    add_nofollow = False\n    host_whitelist = ()\n    whitelist_tags = set(['iframe', 'embed'])\n\n    def __init__(self, **kw):\n        for name, value in kw.items():\n            if not hasattr(self, name):\n                raise TypeError(\n                    \"Unknown parameter: %s=%r\" % (name, value))\n            setattr(self, name, value)\n        if self.inline_style is None and 'inline_style' not in kw:\n            self.inline_style = self.style\n\n    # Used to lookup the primary URL for a given tag that is up for\n    # removal:\n    _tag_link_attrs = dict(\n        script='src',\n        link='href',\n        # From: http://java.sun.com/j2se/1.4.2/docs/guide/misc/applet.html\n        # From what I can tell, both attributes can contain a link:\n        applet=['code', 'object'],\n        iframe='src',\n        embed='src',\n        layer='src',\n        # FIXME: there doesn't really seem like a general way to figure out what\n        # links an <object> tag uses; links often go in <param> tags with values\n        # that we don't really know.  You'd have to have knowledge about specific\n        # kinds of plugins (probably keyed off classid), and match against those.\n        ##object=?,\n        # FIXME: not looking at the action currently, because it is more complex\n        # than than -- if you keep the form, you should keep the form controls.\n        ##form='action',\n        a='href',\n        )\n\n    def __call__(self, doc):\n        \"\"\"\n        Cleans the document.\n        \"\"\"\n        if hasattr(doc, 'getroot'):\n            # ElementTree instance, instead of an element\n            doc = doc.getroot()\n        # convert XHTML to HTML\n        xhtml_to_html(doc)\n        # Normalize a case that IE treats <image> like <img>, and that\n        # can confuse either this step or later steps.\n        for el in doc.iter('image'):\n            el.tag = 'img'\n        if not self.comments:\n            # Of course, if we were going to kill comments anyway, we don't\n            # need to worry about this\n            self.kill_conditional_comments(doc)\n\n        kill_tags = set(self.kill_tags or ())\n        remove_tags = set(self.remove_tags or ())\n        allow_tags = set(self.allow_tags or ())\n\n        if self.scripts:\n            kill_tags.add('script')\n        if self.safe_attrs_only:\n            safe_attrs = set(self.safe_attrs)\n            for el in doc.iter(etree.Element):\n                attrib = el.attrib\n                for aname in attrib.keys():\n                    if aname not in safe_attrs:\n                        del attrib[aname]\n        if self.javascript:\n            if not (self.safe_attrs_only and\n                    self.safe_attrs == defs.safe_attrs):\n                # safe_attrs handles events attributes itself\n                for el in doc.iter(etree.Element):\n                    attrib = el.attrib\n                    for aname in attrib.keys():\n                        if aname.startswith('on'):\n                            del attrib[aname]\n            doc.rewrite_links(self._remove_javascript_link,\n                              resolve_base_href=False)\n            # If we're deleting style then we don't have to remove JS links\n            # from styles, otherwise...\n            if not self.inline_style:\n                for el in _find_styled_elements(doc):\n                    old = el.get('style')\n                    new = _css_javascript_re.sub('', old)\n                    new = _css_import_re.sub('', new)\n                    if self._has_sneaky_javascript(new):\n                        # Something tricky is going on...\n                        del el.attrib['style']\n                    elif new != old:\n                        el.set('style', new)\n            if not self.style:\n                for el in list(doc.iter('style')):\n                    if el.get('type', '').lower().strip() == 'text/javascript':\n                        el.drop_tree()\n                        continue\n                    old = el.text or ''\n                    new = _css_javascript_re.sub('', old)\n                    # The imported CSS can do anything; we just can't allow:\n                    new = _css_import_re.sub('', old)\n                    if self._has_sneaky_javascript(new):\n                        # Something tricky is going on...\n                        el.text = '/* deleted */'\n                    elif new != old:\n                        el.text = new\n        if self.comments or self.processing_instructions:\n            # FIXME: why either?  I feel like there's some obscure reason\n            # because you can put PIs in comments...?  But I've already\n            # forgotten it\n            kill_tags.add(etree.Comment)\n        if self.processing_instructions:\n            kill_tags.add(etree.ProcessingInstruction)\n        if self.style:\n            kill_tags.add('style')\n        if self.inline_style:\n            etree.strip_attributes(doc, 'style')\n        if self.links:\n            kill_tags.add('link')\n        elif self.style or self.javascript:\n            # We must get rid of included stylesheets if Javascript is not\n            # allowed, as you can put Javascript in them\n            for el in list(doc.iter('link')):\n                if 'stylesheet' in el.get('rel', '').lower():\n                    # Note this kills alternate stylesheets as well\n                    if not self.allow_element(el):\n                        el.drop_tree()\n        if self.meta:\n            kill_tags.add('meta')\n        if self.page_structure:\n            remove_tags.update(('head', 'html', 'title'))\n        if self.embedded:\n            # FIXME: is <layer> really embedded?\n            # We should get rid of any <param> tags not inside <applet>;\n            # These are not really valid anyway.\n            for el in list(doc.iter('param')):\n                found_parent = False\n                parent = el.getparent()\n                while parent is not None and parent.tag not in ('applet', 'object'):\n                    parent = parent.getparent()\n                if parent is None:\n                    el.drop_tree()\n            kill_tags.update(('applet',))\n            # The alternate contents that are in an iframe are a good fallback:\n            remove_tags.update(('iframe', 'embed', 'layer', 'object', 'param'))\n        if self.frames:\n            # FIXME: ideally we should look at the frame links, but\n            # generally frames don't mix properly with an HTML\n            # fragment anyway.\n            kill_tags.update(defs.frame_tags)\n        if self.forms:\n            remove_tags.add('form')\n            kill_tags.update(('button', 'input', 'select', 'textarea'))\n        if self.annoying_tags:\n            remove_tags.update(('blink', 'marquee'))\n\n        _remove = []\n        _kill = []\n        for el in doc.iter():\n            if el.tag in kill_tags:\n                if self.allow_element(el):\n                    continue\n                _kill.append(el)\n            elif el.tag in remove_tags:\n                if self.allow_element(el):\n                    continue\n                _remove.append(el)\n\n        if _remove and _remove[0] == doc:\n            # We have to drop the parent-most tag, which we can't\n            # do.  Instead we'll rewrite it:\n            el = _remove.pop(0)\n            el.tag = 'div'\n            el.attrib.clear()\n        elif _kill and _kill[0] == doc:\n            # We have to drop the parent-most element, which we can't\n            # do.  Instead we'll clear it:\n            el = _kill.pop(0)\n            if el.tag != 'html':\n                el.tag = 'div'\n            el.clear()\n\n        _kill.reverse() # start with innermost tags\n        for el in _kill:\n            el.drop_tree()\n        for el in _remove:\n            el.drop_tag()\n\n        if self.remove_unknown_tags:\n            if allow_tags:\n                raise ValueError(\n                    \"It does not make sense to pass in both allow_tags and remove_unknown_tags\")\n            allow_tags = set(defs.tags)\n        if allow_tags:\n            bad = []\n            for el in doc.iter():\n                if el.tag not in allow_tags:\n                    bad.append(el)\n            if bad:\n                if bad[0] is doc:\n                    el = bad.pop(0)\n                    el.tag = 'div'\n                    el.attrib.clear()\n                for el in bad:\n                    el.drop_tag()\n        if self.add_nofollow:\n            for el in _find_external_links(doc):\n                if not self.allow_follow(el):\n                    rel = el.get('rel')\n                    if rel:\n                        if ('nofollow' in rel\n                                and ' nofollow ' in (' %s ' % rel)):\n                            continue\n                        rel = '%s nofollow' % rel\n                    else:\n                        rel = 'nofollow'\n                    el.set('rel', rel)\n\n    def allow_follow(self, anchor):\n        \"\"\"\n        Override to suppress rel=\"nofollow\" on some anchors.\n        \"\"\"\n        return False\n\n    def allow_element(self, el):\n        if el.tag not in self._tag_link_attrs:\n            return False\n        attr = self._tag_link_attrs[el.tag]\n        if isinstance(attr, (list, tuple)):\n            for one_attr in attr:\n                url = el.get(one_attr)\n                if not url:\n                    return False\n                if not self.allow_embedded_url(el, url):\n                    return False\n            return True\n        else:\n            url = el.get(attr)\n            if not url:\n                return False\n            return self.allow_embedded_url(el, url)\n\n    def allow_embedded_url(self, el, url):\n        if (self.whitelist_tags is not None\n            and el.tag not in self.whitelist_tags):\n            return False\n        scheme, netloc, path, query, fragment = urlsplit(url)\n        netloc = netloc.lower().split(':', 1)[0]\n        if scheme not in ('http', 'https'):\n            return False\n        if netloc in self.host_whitelist:\n            return True\n        return False\n\n    def kill_conditional_comments(self, doc):\n        \"\"\"\n        IE conditional comments basically embed HTML that the parser\n        doesn't normally see.  We can't allow anything like that, so\n        we'll kill any comments that could be conditional.\n        \"\"\"\n        bad = []\n        self._kill_elements(\n            doc, lambda el: _conditional_comment_re.search(el.text),\n            etree.Comment)                \n\n    def _kill_elements(self, doc, condition, iterate=None):\n        bad = []\n        for el in doc.iter(iterate):\n            if condition(el):\n                bad.append(el)\n        for el in bad:\n            el.drop_tree()\n\n    def _remove_javascript_link(self, link):\n        # links like \"j a v a s c r i p t:\" might be interpreted in IE\n        new = _substitute_whitespace('', link)\n        if _is_javascript_scheme(new):\n            # FIXME: should this be None to delete?\n            return ''\n        return link\n\n    _substitute_comments = re.compile(r'/\\*.*?\\*/', re.S).sub\n\n    def _has_sneaky_javascript(self, style):\n        \"\"\"\n        Depending on the browser, stuff like ``e x p r e s s i o n(...)``\n        can get interpreted, or ``expre/* stuff */ssion(...)``.  This\n        checks for attempt to do stuff like this.\n\n        Typically the response will be to kill the entire style; if you\n        have just a bit of Javascript in the style another rule will catch\n        that and remove only the Javascript from the style; this catches\n        more sneaky attempts.\n        \"\"\"\n        style = self._substitute_comments('', style)\n        style = style.replace('\\\\', '')\n        style = _substitute_whitespace('', style)\n        style = style.lower()\n        if 'javascript:' in style:\n            return True\n        if 'expression(' in style:\n            return True\n        return False\n\n    def clean_html(self, html):\n        result_type = type(html)\n        if isinstance(html, basestring):\n            doc = fromstring(html)\n        else:\n            doc = copy.deepcopy(html)\n        self(doc)\n        return _transform_result(result_type, doc)\n\nclean = Cleaner()\nclean_html = clean.clean_html\n\n############################################################\n## Autolinking\n############################################################\n\n_link_regexes = [\n    re.compile(r'(?P<body>https?://(?P<host>[a-z0-9._-]+)(?:/[/\\-_.,a-z0-9%&?;=~]*)?(?:\\([/\\-_.,a-z0-9%&?;=~]*\\))?)', re.I),\n    # This is conservative, but autolinking can be a bit conservative:\n    re.compile(r'mailto:(?P<body>[a-z0-9._-]+@(?P<host>[a-z0-9_.-]+[a-z]))', re.I),\n    ]\n\n_avoid_elements = ['textarea', 'pre', 'code', 'head', 'select', 'a']\n\n_avoid_hosts = [\n    re.compile(r'^localhost', re.I),\n    re.compile(r'\\bexample\\.(?:com|org|net)$', re.I),\n    re.compile(r'^127\\.0\\.0\\.1$'),\n    ]\n\n_avoid_classes = ['nolink']\n\ndef autolink(el, link_regexes=_link_regexes,\n             avoid_elements=_avoid_elements,\n             avoid_hosts=_avoid_hosts,\n             avoid_classes=_avoid_classes):\n    \"\"\"\n    Turn any URLs into links.\n\n    It will search for links identified by the given regular\n    expressions (by default mailto and http(s) links).\n\n    It won't link text in an element in avoid_elements, or an element\n    with a class in avoid_classes.  It won't link to anything with a\n    host that matches one of the regular expressions in avoid_hosts\n    (default localhost and 127.0.0.1).\n\n    If you pass in an element, the element's tail will not be\n    substituted, only the contents of the element.\n    \"\"\"\n    if el.tag in avoid_elements:\n        return\n    class_name = el.get('class')\n    if class_name:\n        class_name = class_name.split()\n        for match_class in avoid_classes:\n            if match_class in class_name:\n                return\n    for child in list(el):\n        autolink(child, link_regexes=link_regexes,\n                 avoid_elements=avoid_elements,\n                 avoid_hosts=avoid_hosts,\n                 avoid_classes=avoid_classes)\n        if child.tail:\n            text, tail_children = _link_text(\n                child.tail, link_regexes, avoid_hosts, factory=el.makeelement)\n            if tail_children:\n                child.tail = text\n                index = el.index(child)\n                el[index+1:index+1] = tail_children\n    if el.text:\n        text, pre_children = _link_text(\n            el.text, link_regexes, avoid_hosts, factory=el.makeelement)\n        if pre_children:\n            el.text = text\n            el[:0] = pre_children\n\ndef _link_text(text, link_regexes, avoid_hosts, factory):\n    leading_text = ''\n    links = []\n    last_pos = 0\n    while 1:\n        best_match, best_pos = None, None\n        for regex in link_regexes:\n            regex_pos = last_pos\n            while 1:\n                match = regex.search(text, pos=regex_pos)\n                if match is None:\n                    break\n                host = match.group('host')\n                for host_regex in avoid_hosts:\n                    if host_regex.search(host):\n                        regex_pos = match.end()\n                        break\n                else:\n                    break\n            if match is None:\n                continue\n            if best_pos is None or match.start() < best_pos:\n                best_match = match\n                best_pos = match.start()\n        if best_match is None:\n            # No more matches\n            if links:\n                assert not links[-1].tail\n                links[-1].tail = text\n            else:\n                assert not leading_text\n                leading_text = text\n            break\n        link = best_match.group(0)\n        end = best_match.end()\n        if link.endswith('.') or link.endswith(','):\n            # These punctuation marks shouldn't end a link\n            end -= 1\n            link = link[:-1]\n        prev_text = text[:best_match.start()]\n        if links:\n            assert not links[-1].tail\n            links[-1].tail = prev_text\n        else:\n            assert not leading_text\n            leading_text = prev_text\n        anchor = factory('a')\n        anchor.set('href', link)\n        body = best_match.group('body')\n        if not body:\n            body = link\n        if body.endswith('.') or body.endswith(','):\n            body = body[:-1]\n        anchor.text = body\n        links.append(anchor)\n        text = text[end:]\n    return leading_text, links\n                \ndef autolink_html(html, *args, **kw):\n    result_type = type(html)\n    if isinstance(html, basestring):\n        doc = fromstring(html)\n    else:\n        doc = copy.deepcopy(html)\n    autolink(doc, *args, **kw)\n    return _transform_result(result_type, doc)\n\nautolink_html.__doc__ = autolink.__doc__\n\n############################################################\n## Word wrapping\n############################################################\n\n_avoid_word_break_elements = ['pre', 'textarea', 'code']\n_avoid_word_break_classes = ['nobreak']\n\ndef word_break(el, max_width=40,\n               avoid_elements=_avoid_word_break_elements,\n               avoid_classes=_avoid_word_break_classes,\n               break_character=unichr(0x200b)):\n    \"\"\"\n    Breaks any long words found in the body of the text (not attributes).\n\n    Doesn't effect any of the tags in avoid_elements, by default\n    ``<textarea>`` and ``<pre>``\n\n    Breaks words by inserting &#8203;, which is a unicode character\n    for Zero Width Space character.  This generally takes up no space\n    in rendering, but does copy as a space, and in monospace contexts\n    usually takes up space.\n\n    See http://www.cs.tut.fi/~jkorpela/html/nobr.html for a discussion\n    \"\"\"\n    # Character suggestion of &#8203 comes from:\n    #   http://www.cs.tut.fi/~jkorpela/html/nobr.html\n    if el.tag in _avoid_word_break_elements:\n        return\n    class_name = el.get('class')\n    if class_name:\n        dont_break = False\n        class_name = class_name.split()\n        for avoid in avoid_classes:\n            if avoid in class_name:\n                dont_break = True\n                break\n        if dont_break:\n            return\n    if el.text:\n        el.text = _break_text(el.text, max_width, break_character)\n    for child in el:\n        word_break(child, max_width=max_width,\n                   avoid_elements=avoid_elements,\n                   avoid_classes=avoid_classes,\n                   break_character=break_character)\n        if child.tail:\n            child.tail = _break_text(child.tail, max_width, break_character)\n\ndef word_break_html(html, *args, **kw):\n    result_type = type(html)\n    doc = fromstring(html)\n    word_break(doc, *args, **kw)\n    return _transform_result(result_type, doc)\n\ndef _break_text(text, max_width, break_character):\n    words = text.split()\n    for word in words:\n        if len(word) > max_width:\n            replacement = _insert_break(word, max_width, break_character)\n            text = text.replace(word, replacement)\n    return text\n\n_break_prefer_re = re.compile(r'[^a-z]', re.I)\n\ndef _insert_break(word, width, break_character):\n    orig_word = word\n    result = ''\n    while len(word) > width:\n        start = word[:width]\n        breaks = list(_break_prefer_re.finditer(start))\n        if breaks:\n            last_break = breaks[-1]\n            # Only walk back up to 10 characters to find a nice break:\n            if last_break.end() > width-10:\n                # FIXME: should the break character be at the end of the\n                # chunk, or the beginning of the next chunk?\n                start = word[:last_break.end()]\n        result += start + break_character\n        word = word[len(start):]\n    result += word\n    return result\n    \n", "patch": "@@ -8,9 +8,10 @@\n import copy\n try:\n     from urlparse import urlsplit\n+    from urllib import unquote_plus\n except ImportError:\n     # Python 3\n-    from urllib.parse import urlsplit\n+    from urllib.parse import urlsplit, unquote_plus\n from lxml import etree\n from lxml.html import defs\n from lxml.html import fromstring, XHTML_NAMESPACE\n@@ -482,7 +483,7 @@ def _kill_elements(self, doc, condition, iterate=None):\n \n     def _remove_javascript_link(self, link):\n         # links like \"j a v a s c r i p t:\" might be interpreted in IE\n-        new = _substitute_whitespace('', link)\n+        new = _substitute_whitespace('', unquote_plus(link))\n         if _is_javascript_scheme(new):\n             # FIXME: should this be None to delete?\n             return ''", "file_path": "files/2018_12\\245", "file_language": "py", "file_name": "src/lxml/html/clean.py", "outdated_file_modify": 1, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/lxml/lxml/raw/6be1d081b49c97cfd7b3fbd934a193b668629109/src%2Flxml%2Fhtml%2Ftests%2Ftest_clean.txt", "code": ">>> import re\n>>> from lxml.html import fromstring, tostring\n>>> from lxml.html.clean import clean, clean_html, Cleaner\n>>> from lxml.html import usedoctest\n\n>>> doc = '''<html>\n...   <head>\n...     <script type=\"text/javascript\" src=\"evil-site\"></script>\n...     <link rel=\"alternate\" type=\"text/rss\" src=\"evil-rss\">\n...     <link rel=\"alternate\" type=\"text/rss\" href=\"http://example.com\">\n...     <link rel=\"stylesheet\" type=\"text/rss\" href=\"http://example.com\">\n...     <style>\n...       body {background-image: url(javascript:do_evil)};\n...       div {background-image: url(data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==)};\n...       div {color: expression(evil)};\n...     </style>\n...   </head>\n...   <body onload=\"evil_function()\">\n...     <!-- I am interpreted for EVIL! -->\n...     <a href=\"javascript:evil_function()\">a link</a>\n...     <a href=\"j\\x01a\\x02v\\x03a\\x04s\\x05c\\x06r\\x07i\\x0Ep t%20:evil_function()\">a control char link</a>\n...     <a href=\"data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==\">data</a>\n...     <a href=\"#\" onclick=\"evil_function()\">another link</a>\n...     <p onclick=\"evil_function()\">a paragraph</p>\n...     <div style=\"display: none\">secret EVIL!</div>\n...     <object> of EVIL! </object>\n...     <iframe src=\"evil-site\"></iframe>\n...     <form action=\"evil-site\">\n...       Password: <input type=\"password\" name=\"password\">\n...     </form>\n...     <a href=\"evil-site\">spam spam SPAM!</a>\n...     <a href=\"http://example.com\" rel=\"author\">Author</a>\n...     <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n...     <img src=\"evil!\">\n...   </body>\n... </html>'''\n\n>>> print(re.sub('[\\x00-\\x07\\x0E]', '', doc))\n<html>\n  <head>\n    <script type=\"text/javascript\" src=\"evil-site\"></script>\n    <link rel=\"alternate\" type=\"text/rss\" src=\"evil-rss\">\n    <link rel=\"alternate\" type=\"text/rss\" href=\"http://example.com\">\n    <link rel=\"stylesheet\" type=\"text/rss\" href=\"http://example.com\">\n    <style>\n      body {background-image: url(javascript:do_evil)};\n      div {background-image: url(data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==)};\n      div {color: expression(evil)};\n    </style>\n  </head>\n  <body onload=\"evil_function()\">\n    <!-- I am interpreted for EVIL! -->\n    <a href=\"javascript:evil_function()\">a link</a>\n    <a href=\"javascrip t%20:evil_function()\">a control char link</a>\n    <a href=\"data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==\">data</a>\n    <a href=\"#\" onclick=\"evil_function()\">another link</a>\n    <p onclick=\"evil_function()\">a paragraph</p>\n    <div style=\"display: none\">secret EVIL!</div>\n    <object> of EVIL! </object>\n    <iframe src=\"evil-site\"></iframe>\n    <form action=\"evil-site\">\n      Password: <input type=\"password\" name=\"password\">\n    </form>\n    <a href=\"evil-site\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n\n>>> print(tostring(fromstring(doc)).decode(\"utf-8\"))\n<html>\n  <head>\n    <script type=\"text/javascript\" src=\"evil-site\"></script>\n    <link rel=\"alternate\" type=\"text/rss\" src=\"evil-rss\">\n    <link rel=\"alternate\" type=\"text/rss\" href=\"http://example.com\">\n    <link rel=\"stylesheet\" type=\"text/rss\" href=\"http://example.com\">\n    <style>\n      body {background-image: url(javascript:do_evil)};\n      div {background-image: url(data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==)};\n      div {color: expression(evil)};\n    </style>\n  </head>\n  <body onload=\"evil_function()\">\n    <!-- I am interpreted for EVIL! -->\n    <a href=\"javascript:evil_function()\">a link</a>\n    <a href=\"javascrip%20t%20:evil_function()\">a control char link</a>\n    <a href=\"data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==\">data</a>\n    <a href=\"#\" onclick=\"evil_function()\">another link</a>\n    <p onclick=\"evil_function()\">a paragraph</p>\n    <div style=\"display: none\">secret EVIL!</div>\n    <object> of EVIL! </object>\n    <iframe src=\"evil-site\"></iframe>\n    <form action=\"evil-site\">\n      Password: <input type=\"password\" name=\"password\">\n    </form>\n    <a href=\"evil-site\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n\n>>> print(Cleaner(page_structure=False, safe_attrs_only=False).clean_html(doc))\n<html>\n  <head>\n    <style>/* deleted */</style>\n  </head>\n  <body>\n    <a href=\"\">a link</a>\n    <a href=\"\">a control char link</a>\n    <a href=\"\">data</a>\n    <a href=\"#\">another link</a>\n    <p>a paragraph</p>\n    <div style=\"display: none\">secret EVIL!</div>\n    of EVIL!\n    Password:\n    <a href=\"evil-site\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n\n>>> print(Cleaner(style=True, inline_style=True, links=True, add_nofollow=True, page_structure=False, safe_attrs_only=False).clean_html(doc))\n<html>\n  <head>\n  </head>\n  <body>\n    <a href=\"\">a link</a>\n    <a href=\"\">a control char link</a>\n    <a href=\"\">data</a>\n    <a href=\"#\">another link</a>\n    <p>a paragraph</p>\n    <div>secret EVIL!</div>\n    of EVIL!\n    Password:\n    <a href=\"evil-site\" rel=\"nofollow\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author nofollow\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n\n>>> print(Cleaner(style=True, inline_style=False, links=True, add_nofollow=True, page_structure=False, safe_attrs_only=False).clean_html(doc))\n<html>\n  <head>\n  </head>\n  <body>\n    <a href=\"\">a link</a>\n    <a href=\"\">a control char link</a>\n    <a href=\"\">data</a>\n    <a href=\"#\">another link</a>\n    <p>a paragraph</p>\n    <div style=\"display: none\">secret EVIL!</div>\n    of EVIL!\n    Password:\n    <a href=\"evil-site\" rel=\"nofollow\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author nofollow\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n\n>>> print(Cleaner(links=False, page_structure=False, javascript=True, host_whitelist=['example.com'], whitelist_tags=None).clean_html(doc))\n<html>\n  <head>\n    <link rel=\"alternate\" type=\"text/rss\" src=\"evil-rss\">\n    <link rel=\"alternate\" type=\"text/rss\" href=\"http://example.com\">\n    <link rel=\"stylesheet\" type=\"text/rss\" href=\"http://example.com\">\n    <style>/* deleted */</style>\n  </head>\n  <body>\n    <a href=\"\">a link</a>\n    <a href=\"\">a control char link</a>\n    <a href=\"\">data</a>\n    <a href=\"#\">another link</a>\n    <p>a paragraph</p>\n    <div>secret EVIL!</div>\n    of EVIL!\n    Password:\n    <a href=\"evil-site\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n", "code_before": ">>> import re\n>>> from lxml.html import fromstring, tostring\n>>> from lxml.html.clean import clean, clean_html, Cleaner\n>>> from lxml.html import usedoctest\n\n>>> doc = '''<html>\n...   <head>\n...     <script type=\"text/javascript\" src=\"evil-site\"></script>\n...     <link rel=\"alternate\" type=\"text/rss\" src=\"evil-rss\">\n...     <link rel=\"alternate\" type=\"text/rss\" href=\"http://example.com\">\n...     <link rel=\"stylesheet\" type=\"text/rss\" href=\"http://example.com\">\n...     <style>\n...       body {background-image: url(javascript:do_evil)};\n...       div {background-image: url(data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==)};\n...       div {color: expression(evil)};\n...     </style>\n...   </head>\n...   <body onload=\"evil_function()\">\n...     <!-- I am interpreted for EVIL! -->\n...     <a href=\"javascript:evil_function()\">a link</a>\n...     <a href=\"j\\x01a\\x02v\\x03a\\x04s\\x05c\\x06r\\x07i\\x0Ep t:evil_function()\">a control char link</a>\n...     <a href=\"data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==\">data</a>\n...     <a href=\"#\" onclick=\"evil_function()\">another link</a>\n...     <p onclick=\"evil_function()\">a paragraph</p>\n...     <div style=\"display: none\">secret EVIL!</div>\n...     <object> of EVIL! </object>\n...     <iframe src=\"evil-site\"></iframe>\n...     <form action=\"evil-site\">\n...       Password: <input type=\"password\" name=\"password\">\n...     </form>\n...     <a href=\"evil-site\">spam spam SPAM!</a>\n...     <a href=\"http://example.com\" rel=\"author\">Author</a>\n...     <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n...     <img src=\"evil!\">\n...   </body>\n... </html>'''\n\n>>> print(re.sub('[\\x00-\\x07\\x0E]', '', doc))\n<html>\n  <head>\n    <script type=\"text/javascript\" src=\"evil-site\"></script>\n    <link rel=\"alternate\" type=\"text/rss\" src=\"evil-rss\">\n    <link rel=\"alternate\" type=\"text/rss\" href=\"http://example.com\">\n    <link rel=\"stylesheet\" type=\"text/rss\" href=\"http://example.com\">\n    <style>\n      body {background-image: url(javascript:do_evil)};\n      div {background-image: url(data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==)};\n      div {color: expression(evil)};\n    </style>\n  </head>\n  <body onload=\"evil_function()\">\n    <!-- I am interpreted for EVIL! -->\n    <a href=\"javascript:evil_function()\">a link</a>\n    <a href=\"javascrip t:evil_function()\">a control char link</a>\n    <a href=\"data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==\">data</a>\n    <a href=\"#\" onclick=\"evil_function()\">another link</a>\n    <p onclick=\"evil_function()\">a paragraph</p>\n    <div style=\"display: none\">secret EVIL!</div>\n    <object> of EVIL! </object>\n    <iframe src=\"evil-site\"></iframe>\n    <form action=\"evil-site\">\n      Password: <input type=\"password\" name=\"password\">\n    </form>\n    <a href=\"evil-site\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n\n>>> print(tostring(fromstring(doc)).decode(\"utf-8\"))\n<html>\n  <head>\n    <script type=\"text/javascript\" src=\"evil-site\"></script>\n    <link rel=\"alternate\" type=\"text/rss\" src=\"evil-rss\">\n    <link rel=\"alternate\" type=\"text/rss\" href=\"http://example.com\">\n    <link rel=\"stylesheet\" type=\"text/rss\" href=\"http://example.com\">\n    <style>\n      body {background-image: url(javascript:do_evil)};\n      div {background-image: url(data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==)};\n      div {color: expression(evil)};\n    </style>\n  </head>\n  <body onload=\"evil_function()\">\n    <!-- I am interpreted for EVIL! -->\n    <a href=\"javascript:evil_function()\">a link</a>\n    <a href=\"javascrip%20t:evil_function()\">a control char link</a>\n    <a href=\"data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==\">data</a>\n    <a href=\"#\" onclick=\"evil_function()\">another link</a>\n    <p onclick=\"evil_function()\">a paragraph</p>\n    <div style=\"display: none\">secret EVIL!</div>\n    <object> of EVIL! </object>\n    <iframe src=\"evil-site\"></iframe>\n    <form action=\"evil-site\">\n      Password: <input type=\"password\" name=\"password\">\n    </form>\n    <a href=\"evil-site\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n\n>>> print(Cleaner(page_structure=False, safe_attrs_only=False).clean_html(doc))\n<html>\n  <head>\n    <style>/* deleted */</style>\n  </head>\n  <body>\n    <a href=\"\">a link</a>\n    <a href=\"\">a control char link</a>\n    <a href=\"\">data</a>\n    <a href=\"#\">another link</a>\n    <p>a paragraph</p>\n    <div style=\"display: none\">secret EVIL!</div>\n    of EVIL!\n    Password:\n    <a href=\"evil-site\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n\n>>> print(Cleaner(style=True, inline_style=True, links=True, add_nofollow=True, page_structure=False, safe_attrs_only=False).clean_html(doc))\n<html>\n  <head>\n  </head>\n  <body>\n    <a href=\"\">a link</a>\n    <a href=\"\">a control char link</a>\n    <a href=\"\">data</a>\n    <a href=\"#\">another link</a>\n    <p>a paragraph</p>\n    <div>secret EVIL!</div>\n    of EVIL!\n    Password:\n    <a href=\"evil-site\" rel=\"nofollow\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author nofollow\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n\n>>> print(Cleaner(style=True, inline_style=False, links=True, add_nofollow=True, page_structure=False, safe_attrs_only=False).clean_html(doc))\n<html>\n  <head>\n  </head>\n  <body>\n    <a href=\"\">a link</a>\n    <a href=\"\">a control char link</a>\n    <a href=\"\">data</a>\n    <a href=\"#\">another link</a>\n    <p>a paragraph</p>\n    <div style=\"display: none\">secret EVIL!</div>\n    of EVIL!\n    Password:\n    <a href=\"evil-site\" rel=\"nofollow\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author nofollow\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n\n>>> print(Cleaner(links=False, page_structure=False, javascript=True, host_whitelist=['example.com'], whitelist_tags=None).clean_html(doc))\n<html>\n  <head>\n    <link rel=\"alternate\" type=\"text/rss\" src=\"evil-rss\">\n    <link rel=\"alternate\" type=\"text/rss\" href=\"http://example.com\">\n    <link rel=\"stylesheet\" type=\"text/rss\" href=\"http://example.com\">\n    <style>/* deleted */</style>\n  </head>\n  <body>\n    <a href=\"\">a link</a>\n    <a href=\"\">a control char link</a>\n    <a href=\"\">data</a>\n    <a href=\"#\">another link</a>\n    <p>a paragraph</p>\n    <div>secret EVIL!</div>\n    of EVIL!\n    Password:\n    <a href=\"evil-site\">spam spam SPAM!</a>\n    <a href=\"http://example.com\" rel=\"author\">Author</a>\n    <a href=\"http://example.com\" rel=\"nofollow\">Text</a>\n    <img src=\"evil!\">\n  </body>\n</html>\n", "patch": "@@ -18,7 +18,7 @@\n ...   <body onload=\"evil_function()\">\n ...     <!-- I am interpreted for EVIL! -->\n ...     <a href=\"javascript:evil_function()\">a link</a>\n-...     <a href=\"j\\x01a\\x02v\\x03a\\x04s\\x05c\\x06r\\x07i\\x0Ep t:evil_function()\">a control char link</a>\n+...     <a href=\"j\\x01a\\x02v\\x03a\\x04s\\x05c\\x06r\\x07i\\x0Ep t%20:evil_function()\">a control char link</a>\n ...     <a href=\"data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==\">data</a>\n ...     <a href=\"#\" onclick=\"evil_function()\">another link</a>\n ...     <p onclick=\"evil_function()\">a paragraph</p>\n@@ -51,7 +51,7 @@\n   <body onload=\"evil_function()\">\n     <!-- I am interpreted for EVIL! -->\n     <a href=\"javascript:evil_function()\">a link</a>\n-    <a href=\"javascrip t:evil_function()\">a control char link</a>\n+    <a href=\"javascrip t%20:evil_function()\">a control char link</a>\n     <a href=\"data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==\">data</a>\n     <a href=\"#\" onclick=\"evil_function()\">another link</a>\n     <p onclick=\"evil_function()\">a paragraph</p>\n@@ -84,7 +84,7 @@\n   <body onload=\"evil_function()\">\n     <!-- I am interpreted for EVIL! -->\n     <a href=\"javascript:evil_function()\">a link</a>\n-    <a href=\"javascrip%20t:evil_function()\">a control char link</a>\n+    <a href=\"javascrip%20t%20:evil_function()\">a control char link</a>\n     <a href=\"data:text/html;base64,PHNjcmlwdD5hbGVydCgidGVzdCIpOzwvc2NyaXB0Pg==\">data</a>\n     <a href=\"#\" onclick=\"evil_function()\">another link</a>\n     <p onclick=\"evil_function()\">a paragraph</p>", "file_path": "files/2018_12\\246", "file_language": "txt", "file_name": "src/lxml/html/tests/test_clean.txt", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
