{"index": 10654, "cve_id": "CVE-2021-23980", "cwe_id": ["CWE-79"], "cve_language": "Python", "cve_description": "A mutation XSS affects users calling bleach.clean with all of: svg or math in the allowed tags p or br in allowed tags style, title, noscript, script, textarea, noframes, iframe, or xmp in allowed tags the keyword argument strip_comments=False Note: none of the above tags are in the default allowed tags and strip_comments defaults to True.", "cvss": "6.1", "publish_date": "February 16, 2023", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "REQUIRED", "S": "CHANGED", "C": "LOW", "I": "LOW", "A": "NONE", "commit_id": "1334134d34397966a7f7cfebd38639e9ba2c680e", "commit_message": "sanitizer: escape HTML comments\n\nfixes: bug 1689399 / GHSA vv2x-vrpj-qqpq", "commit_date": "2021-02-01T17:05:38Z", "project": "mozilla/bleach", "url": "https://api.github.com/repos/mozilla/bleach/commits/1334134d34397966a7f7cfebd38639e9ba2c680e", "html_url": "https://github.com/mozilla/bleach/commit/1334134d34397966a7f7cfebd38639e9ba2c680e", "windows_before": [{"commit_id": "c045a8b2a02bfb77bb9cacd5d3e5926c056074d2", "commit_date": "Tue Jan 26 17:39:43 2021 -0500", "commit_message": "Merge pull request #581 from mozilla/nit-fixes", "files_name": ["491abb06ce89012d852f4c5ab3aff8f572532611 - Tue Jan 26 16:57:53 2021 -0500 : fix typo s/vnedoring/vendoring/", "scripts/vendor_verify.sh"]}, {"commit_id": "10b1c5dda8ebceffce1d8f7d66d4b309b4f8c0cf", "commit_date": "Tue Jan 26 16:34:34 2021 -0500", "commit_message": "vendor: add html5lib-1.1.dist-info/REQUESTED", "files_name": ["bleach/_vendor/html5lib-1.1.dist-info/REQUESTED"]}, {"commit_id": "cd838c3b527021f2780d77718488fa03d81f08e3", "commit_date": "Tue Jan 26 09:42:31 2021 -0500", "commit_message": "Merge pull request #579 from mozilla/validate-convert-entity-code-points", "files_name": ["612b8080ada0fba45f0575bfcd4f3a0bda7bfaca - Mon Jan 25 17:47:06 2021 -0500 : Update for v3.2.3 release", "CHANGES", "bleach/__init__.py", "tests_website/index.html"]}, {"commit_id": "6879f6a67058c0d5977a8aa580b6338c9d34ff0e", "commit_date": "Mon Jan 25 17:39:42 2021 -0500", "commit_message": "html5lib_shim: validate unicode points for convert_entity", "files_name": ["bleach/html5lib_shim.py", "tests/test_html5lib_shim.py"]}, {"commit_id": "90cb80be961aaf650ebc65b2ba2b789a2e9b129f", "commit_date": "Wed Jan 20 14:00:09 2021 -0500", "commit_message": "Update for v3.2.2 release", "files_name": ["CHANGES", "CONTRIBUTORS", "bleach/__init__.py", "tests_website/index.html"]}, {"commit_id": "c66edfdc44cb037a1070305e8aa8c8ab6b4c09f6", "commit_date": "Wed Jan 20 13:52:02 2021 -0500", "commit_message": "Merge pull request #572 from mozilla/fix-indexerror", "files_name": ["eb8aebd45737ffba8abd23ba88b14dede47f35af - Wed Jan 20 12:59:20 2021 -0500 : linkify: fix IndexError in convert_entity", "bleach/html5lib_shim.py", "tests/test_linkify.py"]}, {"commit_id": "a4ed4d2e3781b01ca9fea6db0e647ff0b20b7ea6", "commit_date": "Tue Jan 5 12:06:38 2021 -0500", "commit_message": "tests_website: bump to current bleach version", "files_name": ["tests_website/index.html"]}, {"commit_id": "0f5b25c17a3149b0ac47813165572785d099d7cb", "commit_date": "Tue Jan 5 12:06:12 2021 -0500", "commit_message": "docs: replace travis refs in dev docs and tox comment", "files_name": ["docs/dev.rst", "tox.ini"]}, {"commit_id": "1d13e9f7610cb4ecf068cadd647a7c6afd6dbc66", "commit_date": "Tue Jan 5 12:05:36 2021 -0500", "commit_message": "docs: replace travis ci badges with GHA", "files_name": ["README.rst", "tests_website/index.html"]}, {"commit_id": "caed8b5b74b1c7dfc35d0cf9228f14e2178c421d", "commit_date": "Tue Jan 5 11:54:23 2021 -0500", "commit_message": "ci: remove travis", "files_name": [".travis.yml"]}, {"commit_id": "49b3e4388a01de8eec487601822d8985da8b15dc", "commit_date": "Mon Sep 14 17:56:34 2020 +0300", "commit_message": "Update and simplify GHA config", "files_name": [".github/workflows/lint.yml", ".github/workflows/test.yml"]}, {"commit_id": "29dc1fcf4138d22d1ccad4741723969f4a44cc81", "commit_date": "Tue Jan 28 11:36:39 2020 +0200", "commit_message": "Lint on GitHub Actions", "files_name": [".github/workflows/lint.yml"]}, {"commit_id": "263675d84aee9050f9ec0e4abbca6a12efc019ef", "commit_date": "Tue Jan 28 11:29:48 2020 +0200", "commit_message": "Test on GitHub Actions", "files_name": [".github/workflows/test.yml"]}, {"commit_id": "f5971aa57461cce00d6dd474e901bcbd00f240f1", "commit_date": "Fri Sep 18 09:14:33 2020 -0400", "commit_message": "Update for v3.2.1 release", "files_name": ["CHANGES", "bleach/__init__.py"]}, {"commit_id": "38122ef260e64c240bfd645408b01136df277ead", "commit_date": "Thu Sep 17 10:19:26 2020 -0400", "commit_message": "suppress html5lib sanitizer deprecation warnings", "files_name": ["bleach/html5lib_shim.py", "bleach/sanitizer.py"]}, {"commit_id": "c4a98da74fc2d1f975e133d6b56fd338a1048b46", "commit_date": "Thu Sep 17 09:56:25 2020 -0400", "commit_message": "upgrade html5lib deprecation warnings to errors", "files_name": ["setup.cfg"]}, {"commit_id": "56d4a756bba795cd43f10b73f6dd76a9de0d47e4", "commit_date": "Wed Sep 16 13:21:43 2020 -0400", "commit_message": "flake8 ignore E203 whitespace before :", "files_name": ["setup.cfg"]}, {"commit_id": "da0eb4b1d50c75469c16894c05b44f8665084ebd", "commit_date": "Wed Sep 16 12:59:58 2020 -0400", "commit_message": "format bleach code and tests ignoring _vendor", "files_name": ["bleach/__init__.py", "bleach/callbacks.py", "bleach/html5lib_shim.py", "bleach/linkifier.py", "bleach/sanitizer.py", "bleach/utils.py", "tests/test_callbacks.py", "tests/test_clean.py", "tests/test_css.py", "tests/test_html5lib_shim.py", "tests/test_linkify.py", "tests/test_unicode.py", "tests/test_utils.py", "tests_website/data_to_json.py", "tests_website/open_test_page.py", "tests_website/server.py"]}, {"commit_id": "b0485609a5abc117b9b389b24dc4b8382b92dbc2", "commit_date": "Wed Sep 16 12:58:52 2020 -0400", "commit_message": "add format-check to tox and CI", "files_name": [".travis.yml", "tox.ini"]}, {"commit_id": "103c16de43ad6ad00fb5686fcb9a5f48eaf7d8e8", "commit_date": "Wed Sep 16 12:58:34 2020 -0400", "commit_message": "scripts: add format and format-check test modes", "files_name": ["scripts/run_tests.sh"]}, {"commit_id": "1f93bbe20426db24a31fe9e4cb1bfdc8e724a76c", "commit_date": "Wed Sep 16 12:25:32 2020 -0400", "commit_message": "update CHANGES, CONTRIBUTORS, and set dev version", "files_name": ["CHANGES", "CONTRIBUTORS", "bleach/__init__.py"]}, {"commit_id": "d5656069c3596fe88128254df252fc3b6c917a36", "commit_date": "Mon Jan 6 14:43:55 2020 -0800", "commit_message": "add DEFAULT_CALLBACKS to linkifier", "files_name": ["bleach/linkifier.py", "docs/linkify.rst", "tests/test_linkify.py"]}, {"commit_id": "692b15f6f81b69ef07544ab7f650b3004f5aaaea", "commit_date": "Wed Sep 16 12:06:29 2020 -0400", "commit_message": "Update issue templates", "files_name": [".github/ISSUE_TEMPLATE/bug-report.md", ".github/ISSUE_TEMPLATE/feature-request.md"]}, {"commit_id": "29a6f722d9956c136be869fdb43bf238b5a12390", "commit_date": "Wed Sep 16 11:16:41 2020 -0400", "commit_message": "add tests for void tags #488", "files_name": ["tests/test_clean.py"]}, {"commit_id": "17dceafa0037fc45feae573bff0904b2ec1e185a", "commit_date": "Wed Sep 16 09:22:15 2020 -0400", "commit_message": "Update for v3.2.0 release", "files_name": ["CHANGES", "SECURITY.md", "bleach/__init__.py"]}, {"commit_id": "b54c78e084b41be8fda8483f72740710377edd26", "commit_date": "Tue Sep 15 14:14:05 2020 -0400", "commit_message": "vendor: add pip version used to vendor readme", "files_name": ["bleach/_vendor/README.rst"]}, {"commit_id": "fecb496418d6dbf76f4dd06b687e9f60e92bc7ca", "commit_date": "Tue Sep 15 14:13:52 2020 -0400", "commit_message": "add hashin to requirements-dev.txt", "files_name": ["requirements-dev.txt"]}, {"commit_id": "7c111706d53a8064190e6d5f233a523ca9d41d14", "commit_date": "Tue Sep 15 13:36:13 2020 -0400", "commit_message": "update changelog, contributors, and set dev version", "files_name": ["CHANGES", "CONTRIBUTORS", "bleach/__init__.py"]}, {"commit_id": "26f7b1627e96c0e2f585cb4ca8f3295917d851ff", "commit_date": "Tue Sep 15 13:31:39 2020 -0400", "commit_message": "suppress html5lib deprecation warnings", "files_name": ["bleach/html5lib_shim.py", "bleach/sanitizer.py", "setup.cfg"]}, {"commit_id": "4d3ef668eb0cc5883724e761d71d8581d38eeaff", "commit_date": "Tue Sep 15 12:54:53 2020 -0400", "commit_message": "update invalid attr name sanitization for html5lib 1.1", "files_name": ["bleach/html5lib_shim.py"]}, {"commit_id": "a9ff437d71daf8ed8ac95a4dec050fb045e0537e", "commit_date": "Tue Sep 8 17:06:54 2020 -0400", "commit_message": "vendor: update vendored html5lib to v1.1", "files_name": ["bleach/_vendor/html5lib-1.0.1.dist-info/DESCRIPTION.rst", "bleach/_vendor/html5lib-1.0.1.dist-info/RECORD", "bleach/_vendor/html5lib-1.0.1.dist-info/metadata.json", "bleach/_vendor/html5lib-1.1.dist-info/AUTHORS.rst", "bleach/_vendor/html5lib-1.1.dist-info/INSTALLER", "bleach/_vendor/html5lib-1.1.dist-info/LICENSE", "bleach/_vendor/html5lib-1.1.dist-info/METADATA", "bleach/_vendor/html5lib-1.1.dist-info/RECORD", "bleach/_vendor/html5lib-1.1.dist-info/WHEEL", "bleach/_vendor/html5lib-1.1.dist-info/top_level.txt", "bleach/_vendor/html5lib/__init__.py", "bleach/_vendor/html5lib/_ihatexml.py", "bleach/_vendor/html5lib/_inputstream.py", "bleach/_vendor/html5lib/_tokenizer.py", "bleach/_vendor/html5lib/_trie/__init__.py", "bleach/_vendor/html5lib/_trie/_base.py", "bleach/_vendor/html5lib/_trie/datrie.py", "bleach/_vendor/html5lib/_utils.py", "bleach/_vendor/html5lib/constants.py", "bleach/_vendor/html5lib/filters/sanitizer.py", "bleach/_vendor/html5lib/html5parser.py", "bleach/_vendor/html5lib/serializer.py", "bleach/_vendor/html5lib/treebuilders/base.py", "bleach/_vendor/html5lib/treebuilders/dom.py", "bleach/_vendor/html5lib/treebuilders/etree.py", "bleach/_vendor/html5lib/treebuilders/etree_lxml.py", "bleach/_vendor/html5lib/treewalkers/__init__.py", "bleach/_vendor/html5lib/treewalkers/etree.py", "bleach/_vendor/html5lib/treewalkers/etree_lxml.py", "bleach/_vendor/vendor.txt"]}, {"commit_id": "2d41050033ca9187912ec2ce92caed2fa9056653", "commit_date": "Fri Jul 24 15:11:22 2020 +0100", "commit_message": "update tests_website terminology", "files_name": ["tests_website/index.html"]}, {"commit_id": "366e79ed5152cb6108af57bfd0dfcb53ec97aa92", "commit_date": "Wed Apr 29 14:21:39 2020 -0400", "commit_message": "Update for v3.1.5 release", "files_name": ["CHANGES", "CONTRIBUTORS", "bleach/__init__.py"]}, {"commit_id": "b7240abe93c34338a0315e224341e1f8e7411458", "commit_date": "Tue Apr 28 14:29:24 2020 -0700", "commit_message": "Replace setuptools dependency with packaging.", "files_name": ["bleach/__init__.py", "setup.py"]}, {"commit_id": "2df82b5e61af5f597bb479396853f020ab15134d", "commit_date": "Thu Mar 26 10:46:57 2020 -0400", "commit_message": "fix long line lint failure bleach/sanitizer.py L601", "files_name": ["bleach/sanitizer.py"]}, {"commit_id": "6e74a5027b57055cdaeb040343d32934121392a7", "commit_date": "Mon Mar 23 17:07:49 2020 -0400", "commit_message": "Update for v3.1.4 release", "files_name": ["CHANGES", "bleach/__init__.py"]}, {"commit_id": "d6018f2539d271963c3e7f54f36ef11900363c69", "commit_date": "Mon Mar 23 13:46:36 2020 -0400", "commit_message": "fix bug 1623633", "files_name": ["bleach/sanitizer.py", "tests/test_css.py"]}, {"commit_id": "fc77027e67cc04aff6f4d4885358705f98ad20f4", "commit_date": "Tue Mar 17 11:24:30 2020 -0400", "commit_message": "Merge branch 'v3.1.0-branch'", "files_name": ["e4b1c50e098c33f82c862a34bb2a40f9c4458f46 - Tue Mar 17 11:16:06 2020 -0400 : Update for v3.1.3 release", "CHANGES", "CONTRIBUTORS", "bleach/__init__.py"]}, {"commit_id": "59cc502cee44bd18adc78619e6baed7a108c3ba1", "commit_date": "Wed Mar 11 16:17:57 2020 -0400", "commit_message": "Update for v3.1.2 release", "files_name": ["CHANGES"]}], "windows_after": [{"commit_id": "842fcb4a05e59d9a22dafb8c51865ee79d753c03", "commit_date": "Fri Jan 29 11:22:55 2021 -0500", "commit_message": "Update for v3.3.0 release", "files_name": ["CHANGES", "SECURITY.md", "bleach/__init__.py", "tests_website/index.html"]}, {"commit_id": "79b7a3c5e56a09d1d323a5006afa59b56162eb13", "commit_date": "Mon Feb 1 12:11:15 2021 -0500", "commit_message": "Merge pull request from GHSA-vv2x-vrpj-qqpq", "files_name": ["d398c89e54ced6b1039d3677689707456ba42dec - Tue Feb 2 11:13:09 2021 -0500 : tests: add tests for more eject tags for GHSA-vv2x-vrpj-qqpq", "tests/test_clean.py"]}, {"commit_id": "c48bf509cf66d8cfda554ce62427e8d6f7bca594", "commit_date": "Tue Feb 2 11:27:26 2021 -0500", "commit_message": "Merge pull request #585 from mozilla/bug-1689399-test-more-eject-tags", "files_name": ["45f3de7f3cc45397e00cf7be4e841b7eb3498a42 - Sat Feb 20 14:07:39 2021 +0700 : Remove duplicated h1 in changelog", "CHANGES"]}, {"commit_id": "3e5d6aa375677821aaf249127e44ac51a815cf2b", "commit_date": "Mon Feb 22 09:35:15 2021 -0500", "commit_message": "Merge pull request #586 from McSinyx/change-head-dup", "files_name": ["8da3105370ccdb1be3fecd26e3c82361dfbc6819 - Thu Jun 24 15:21:45 2021 -0400 : Update bug-report.md", ".github/ISSUE_TEMPLATE/bug-report.md"]}, {"commit_id": "ee775e22b6d501cc94c7c5aa581ce3aacd58c7fd", "commit_date": "Thu Jun 24 16:08:03 2021 -0400", "commit_message": "Update tox.ini", "files_name": ["tox.ini"]}, {"commit_id": "1c16d17ba7991c06a78a567dc73256cdd432eb91", "commit_date": "Wed Jul 14 12:12:27 2021 -0400", "commit_message": "scripts: fail tox vendorverify target when tree diff fails", "files_name": ["scripts/vendor_verify.sh"]}, {"commit_id": "599226634f31130db6fed4dcef483e6194ae12fd", "commit_date": "Wed Jul 14 12:18:54 2021 -0400", "commit_message": "tox: bump lint and vendorverify python to 3.8", "files_name": ["tox.ini"]}, {"commit_id": "ef0a2fafa748c6ab17ecce1cb9b4c008c393abfd", "commit_date": "Wed Jul 14 12:38:28 2021 -0400", "commit_message": "docs: add vendorverify and artifact verification to release process", "files_name": ["docs/dev.rst"]}, {"commit_id": "bf4c67a9623a34d5935399cc791fd284e55b9b00", "commit_date": "Wed Jul 14 17:52:00 2021 +0100", "commit_message": "Fix attribute name in linkify docs.", "files_name": ["docs/linkify.rst"]}, {"commit_id": "76021fab822650d09320f9dc6f8b0896e4bb3c97", "commit_date": "Wed Jul 14 13:00:28 2021 -0400", "commit_message": "Merge pull request #601 from mozilla/fix-598-verify-vendor", "files_name": ["cf313fa3fe390412dbf350523479ac48f2debe42 - Wed Jul 14 15:02:47 2021 -0400 : Update for v3.3.1 release", "CHANGES", "CONTRIBUTORS", "bleach/__init__.py"]}, {"commit_id": "99579a9540734ef4b46fb8ec757fd7b511cee394", "commit_date": "Wed Jul 14 15:08:01 2021 -0400", "commit_message": "Merge pull request #602 from mozilla/prepare-3.3.1", "files_name": ["ea1849a72adfad76f3a8836d72eb3ab65ea45a4e - Wed Jul 14 15:20:43 2021 -0400 : Update dev.rst", "docs/dev.rst"]}, {"commit_id": "45144d9a2aa60c440ea81188562173e962c2d631", "commit_date": "Tue Aug 3 12:41:12 2021 -0400", "commit_message": "Merge pull request #600 from CheesyFeet/patch-1", "files_name": ["e96c8d7147b34090c140cc619be7c4b024164ec6 - Thu Nov 5 12:46:10 2020 -0500 : drop EOL pythons from tox, setup.py, and ci", ".github/workflows/test.yml", "setup.py", "tox.ini"]}, {"commit_id": "ae196a3fd4da6db488421ed23889090628289ea1", "commit_date": "Thu Nov 5 13:09:10 2020 -0500", "commit_message": "tests_website: remove six, bump version, update open script", "files_name": ["tests_website/index.html", "tests_website/open_test_page.py", "tests_website/server.py"]}, {"commit_id": "23d1397b66bdf2034c67d8a5a14f501e49cf2160", "commit_date": "Thu Nov 5 13:00:32 2020 -0500", "commit_message": "drop security support for bleach 3.x versions", "files_name": ["SECURITY.md"]}, {"commit_id": "783029f75baed2cf647604bf568498f509b6e1de", "commit_date": "Thu Nov 5 12:59:20 2020 -0500", "commit_message": "remove six usage, __future__.unicode_literals, compat functions", "files_name": ["bleach/__init__.py", "bleach/callbacks.py", "bleach/html5lib_shim.py", "bleach/linkifier.py", "bleach/sanitizer.py", "bleach/utils.py", "docs/clean.rst", "docs/linkify.rst", "tests/test_callbacks.py", "tests/test_clean.py", "tests/test_css.py", "tests/test_html5lib_shim.py", "tests/test_linkify.py", "tests/test_unicode.py"]}, {"commit_id": "4d8a2e04fd88e4d501cb8524c66cecb1ee65c659", "commit_date": "Mon Jan 25 12:07:31 2021 -0500", "commit_message": "drop object from class definitions", "files_name": ["bleach/html5lib_shim.py", "bleach/linkifier.py", "bleach/sanitizer.py"]}, {"commit_id": "275a51c305943154969c1d8bcb482a89cda16504", "commit_date": "Thu Nov 5 13:01:35 2020 -0500", "commit_message": "bump major version and update releasedate", "files_name": ["bleach/__init__.py"]}, {"commit_id": "5537128215f43d278ebecd2d4c9e398c8f77936b", "commit_date": "Thu Nov 5 13:12:56 2020 -0500", "commit_message": "update CHANGES and CONTRIBUTORS", "files_name": ["CHANGES", "CONTRIBUTORS"]}, {"commit_id": "ad0004f682655a541ae05b726161b2a359d301a5", "commit_date": "Tue Aug 3 13:17:51 2021 -0400", "commit_message": "Merge pull request #605 from mozilla/4.0.0-branch", "files_name": ["289ab557671cbae5ee086f474a8cf7a9a183f959 - Tue Aug 3 13:59:38 2021 -0400 : docs: rename default branch to main", "docs/dev.rst"]}, {"commit_id": "29e2e48aca7d7483d4e50c53315bf77acf8e9e7a", "commit_date": "Tue Aug 3 14:23:38 2021 -0400", "commit_message": "document vendoring and unvendoring versioning", "files_name": ["README.rst", "bleach/_vendor/README.rst", "docs/dev.rst"]}, {"commit_id": "50382cc013564acf41a4d87b1a62475d49eb9da3", "commit_date": "Tue Aug 3 14:37:56 2021 -0400", "commit_message": "Merge pull request #606 from mozilla/update-docs", "files_name": ["b4c2606bfb3ac3befd9c602a04dd99a8376d1c43 - Thu Aug 5 10:37:22 2021 -0400 : ci: drop older ubuntu versions; add latest", ".github/workflows/test.yml"]}, {"commit_id": "793d08a092e6db39fdd8e352781ce963cef1e4c5", "commit_date": "Thu Aug 5 10:50:24 2021 -0400", "commit_message": "Merge pull request #607 from mozilla/ci-update-ubuntu", "files_name": ["27a1c02879708c2447b4676984f1bb549fefa37c - Fri Aug 6 14:54:19 2021 -0400 : docs: replace deprecated highlightlang directive with highlight", "docs/clean.rst", "docs/linkify.rst"]}, {"commit_id": "879d07cff7eaa66ca03e4dfe41b0e8754f7e8fb3", "commit_date": "Fri Aug 6 15:55:20 2021 -0400", "commit_message": "docs: add _static dir with placeholder dotfile", "files_name": ["docs/_static/.gitignore"]}, {"commit_id": "f648354ba34b49f6ec4d5ec9b0bbe1904fab7514", "commit_date": "Fri Aug 6 16:00:15 2021 -0400", "commit_message": "Merge pull request #609 from mozilla/fix-594-docbuild-warnings", "files_name": ["84451282132955ffa6941b3cb0ff77a31ff0d749 - Fri Nov 6 10:57:24 2020 -0500 : add black and mypy to dev requirements for cpython", "requirements-dev.txt", "tox.ini"]}, {"commit_id": "014d5911508a973f379afb4b566a1df23f13b675", "commit_date": "Thu Oct 29 11:42:11 2020 -0400", "commit_message": "ci: test against python 3.9", "files_name": [".github/workflows/test.yml", "tox.ini"]}, {"commit_id": "8a1ea67577089bf6f5fd6736cead9492161b7fc0", "commit_date": "Thu Nov 5 10:17:24 2020 -0500", "commit_message": "ci: bump python to 3.9 for lint, vendorverify, and format-check jobs", "files_name": [".github/workflows/lint.yml", "tox.ini"]}, {"commit_id": "7838d76ab89f1764f0ebdfe9d4fdf11f914fdd1e", "commit_date": "Wed Aug 4 15:38:16 2021 -0400", "commit_message": "scripts: black format and format-check for py36", "files_name": ["bleach/sanitizer.py", "scripts/run_tests.sh"]}, {"commit_id": "4f0cebbb0c9e48f3185354f5eb88687fa1e29958", "commit_date": "Thu Aug 5 10:52:24 2021 -0400", "commit_message": "vendor: rename install script", "files_name": ["bleach/_vendor/README.rst", "bleach/_vendor/vendor_install.sh", "scripts/vendor_verify.sh"]}, {"commit_id": "b04b95eb89a3199b02fa113d2def3267cf68992c", "commit_date": "Thu Aug 5 11:06:31 2021 -0400", "commit_message": "user vendor install script in vendor verify", "files_name": ["bleach/_vendor/vendor_install.sh", "scripts/vendor_verify.sh"]}, {"commit_id": "9023f7f6e1b945a4c6c72de0f390b53cd75ffd74", "commit_date": "Thu Aug 5 11:16:30 2021 -0400", "commit_message": "vendor: add Python 3.6.14 urllib.parse", "files_name": ["bleach/_vendor/parse.py", "bleach/_vendor/parse.py.SHA256SUM", "bleach/_vendor/vendor_install.sh"]}, {"commit_id": "1033d4d045a938968b1b71d28399bc922a5aea09", "commit_date": "Thu Nov 5 10:28:07 2020 -0500", "commit_message": "sanitizer: use urlparse from vendored CPython 3.6.14 urllib.parse", "files_name": ["bleach/sanitizer.py", "tests/test_clean.py"]}, {"commit_id": "931b24eda2970134bf8510b747b6f58f8caa02e4", "commit_date": "Wed Jan 27 11:02:41 2021 -0500", "commit_message": "Update for v4.1.0 release", "files_name": ["CHANGES", "bleach/__init__.py", "setup.py"]}, {"commit_id": "e4718bdda1c43849a8dfbda1b2915e8a6594aab2", "commit_date": "Wed Aug 25 10:44:22 2021 -0400", "commit_message": "Merge pull request #565 from mozilla/fix-536-3.9-urlparse-changes", "files_name": ["f8f4a0ae0de5aaab0a6e534c45879c65bc10f71c - Fri Aug 6 14:50:17 2021 -0400 : pin dev requirements with pip-compile from pip-tools", ".github/workflows/test.yml", "requirements-dev.in", "requirements-dev.txt"]}, {"commit_id": "756246741d3713437ecaeab99f811f9f52a8bbe2", "commit_date": "Wed Aug 25 12:18:19 2021 -0400", "commit_message": "Merge pull request #608 from mozilla/pin-dev-reqs", "files_name": ["248749ad56e2e72db211e121a9538a3fb89674f5 - Wed Aug 25 14:51:11 2021 -0400 : update dev docs and SECURITY.md", "SECURITY.md", "docs/dev.rst"]}, {"commit_id": "0882d7da774f8c9dad1115cac3d2e3ccdfec83f0", "commit_date": "Thu Aug 26 10:00:39 2021 -0400", "commit_message": "Merge pull request #613 from mozilla/update-docs", "files_name": ["954db62b526ef9e4205c2d8c83ac014992c77d3b - Wed Sep 8 13:22:30 2021 +0300 : Add support for Python 3.10", ".github/workflows/test.yml", "setup.py", "tox.ini"]}, {"commit_id": "1e6c10e29ded1a6c3e7ca64fdfb0ab5fef157065", "commit_date": "Wed Sep 8 09:22:27 2021 -0400", "commit_message": "Merge pull request #614 from hugovk/add-3.10", "files_name": ["4ce3b35a3b9ae51b92d0cbea146cfec0d70bd9df - Mon Oct 4 11:56:29 2021 -0400 : ci: drop -dev from 3.10 test matrix", ".github/workflows/test.yml"]}, {"commit_id": "a362171055af36996dc1cedb64c6b3f68359db27", "commit_date": "Tue Oct 5 10:21:18 2021 -0400", "commit_message": "Merge pull request #616 from mozilla/g-k-patch-1", "files_name": ["bd1d85fcca070dff0e5a859a64a3bc0c64cd876e - Thu Dec 30 14:33:11 2021 +0000 : Add notes on migrating from html5lib sanitization", "docs/migrating-from-html5lib-sanitizer.rst"]}, {"commit_id": "a0bc127fe7c3d70cd84b89307a0456810f54d31d", "commit_date": "Thu Jan 6 06:43:01 2022 +0000", "commit_message": "Accept Will's rewording.", "files_name": ["docs/migrating-from-html5lib-sanitizer.rst"]}, {"commit_id": "4dec3c3dd22e3b13e7f1d093b2a952b43d6b62c5", "commit_date": "Thu Jan 6 06:53:07 2022 +0000", "commit_message": "word wrap at 80 chars", "files_name": ["docs/migrating-from-html5lib-sanitizer.rst"]}, {"commit_id": "b92c763cb986d1d032a3dae6a388b7a958fd6bb0", "commit_date": "Mon Feb 7 11:50:21 2022 -0500", "commit_message": "Merge pull request #625 from sferencik/main", "files_name": ["00218ca65db72b670b1e0faba26a59326cfba656 - Mon Feb 7 20:44:10 2022 -0500 : Remove CODE_OF_CONDUCT.rst (#619)", "CODE_OF_CONDUCT.rst", "docs/dev.rst"]}, {"commit_id": "31f50c79a2a720d6ccf012d486d42eaa1cb75d96", "commit_date": "Mon Feb 7 21:26:17 2022 -0500", "commit_message": "Merge pull request #634 from willkg/619-coc", "files_name": ["bd628ce2ef7a7a9e531951c1f0e7cf1b820fcef9 - Mon Feb 7 21:47:15 2022 -0500 : Fix doctest linting issues with migrating chapter (#636)", "docs/index.rst", "docs/migrating.rst"]}, {"commit_id": "498af1383d89f91e19415ad31eac7097a5dbbafd", "commit_date": "Mon Feb 7 22:11:42 2022 -0500", "commit_message": "Fix pip-tools problem and rework requirements (#636)", "files_name": [".github/workflows/lint.yml", ".github/workflows/test.yml", "requirements-dev.in", "requirements-dev.txt", "scripts/run_tests.sh"]}, {"commit_id": "f175b7ced07c72949b13a26bb4076cac45377004", "commit_date": "Tue Feb 8 20:18:37 2022 -0500", "commit_message": "Merge pull request #637 from willkg/636-ci", "files_name": []}], "parents": [{"commit_id_before": "c045a8b2a02bfb77bb9cacd5d3e5926c056074d2", "url_before": "https://api.github.com/repos/mozilla/bleach/commits/c045a8b2a02bfb77bb9cacd5d3e5926c056074d2", "html_url_before": "https://github.com/mozilla/bleach/commit/c045a8b2a02bfb77bb9cacd5d3e5926c056074d2"}], "details": [{"raw_url": "https://github.com/mozilla/bleach/raw/1334134d34397966a7f7cfebd38639e9ba2c680e/bleach%2Fhtml5lib_shim.py", "code": "# flake8: noqa\n\"\"\"\nShim module between Bleach and html5lib. This makes it easier to upgrade the\nhtml5lib library without having to change a lot of code.\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport re\nimport string\nimport warnings\n\nimport six\n\n# ignore html5lib deprecation warnings to use bleach; we are bleach\n# apply before we import submodules that import html5lib\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"html5lib's sanitizer is deprecated\",\n    category=DeprecationWarning,\n    module=\"bleach._vendor.html5lib\",\n)\n\nfrom bleach._vendor.html5lib import (  # noqa: E402 module level import not at top of file\n    HTMLParser,\n    getTreeWalker,\n)\nfrom bleach._vendor.html5lib import (\n    constants,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib.constants import (  # noqa: E402 module level import not at top of file\n    namespaces,\n    prefixes,\n)\nfrom bleach._vendor.html5lib.constants import (\n    _ReparseException as ReparseException,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib.filters.base import (\n    Filter,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib.filters.sanitizer import (\n    allowed_protocols,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib.filters.sanitizer import (\n    Filter as SanitizerFilter,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib._inputstream import (\n    HTMLInputStream,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib.serializer import (\n    escape,\n    HTMLSerializer,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib._tokenizer import (\n    attributeMap,\n    HTMLTokenizer,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib._trie import (\n    Trie,\n)  # noqa: E402 module level import not at top of file\n\n\n#: Map of entity name to expanded entity\nENTITIES = constants.entities\n\n#: Trie of html entity string -> character representation\nENTITIES_TRIE = Trie(ENTITIES)\n\n#: Token type constants--these never change\nTAG_TOKEN_TYPES = {\n    constants.tokenTypes[\"StartTag\"],\n    constants.tokenTypes[\"EndTag\"],\n    constants.tokenTypes[\"EmptyTag\"],\n}\nCHARACTERS_TYPE = constants.tokenTypes[\"Characters\"]\nPARSEERROR_TYPE = constants.tokenTypes[\"ParseError\"]\n\n\n#: List of valid HTML tags, from WHATWG HTML Living Standard as of 2018-10-17\n#: https://html.spec.whatwg.org/multipage/indices.html#elements-3\nHTML_TAGS = [\n    \"a\",\n    \"abbr\",\n    \"address\",\n    \"area\",\n    \"article\",\n    \"aside\",\n    \"audio\",\n    \"b\",\n    \"base\",\n    \"bdi\",\n    \"bdo\",\n    \"blockquote\",\n    \"body\",\n    \"br\",\n    \"button\",\n    \"canvas\",\n    \"caption\",\n    \"cite\",\n    \"code\",\n    \"col\",\n    \"colgroup\",\n    \"data\",\n    \"datalist\",\n    \"dd\",\n    \"del\",\n    \"details\",\n    \"dfn\",\n    \"dialog\",\n    \"div\",\n    \"dl\",\n    \"dt\",\n    \"em\",\n    \"embed\",\n    \"fieldset\",\n    \"figcaption\",\n    \"figure\",\n    \"footer\",\n    \"form\",\n    \"h1\",\n    \"h2\",\n    \"h3\",\n    \"h4\",\n    \"h5\",\n    \"h6\",\n    \"head\",\n    \"header\",\n    \"hgroup\",\n    \"hr\",\n    \"html\",\n    \"i\",\n    \"iframe\",\n    \"img\",\n    \"input\",\n    \"ins\",\n    \"kbd\",\n    \"keygen\",\n    \"label\",\n    \"legend\",\n    \"li\",\n    \"link\",\n    \"map\",\n    \"mark\",\n    \"menu\",\n    \"meta\",\n    \"meter\",\n    \"nav\",\n    \"noscript\",\n    \"object\",\n    \"ol\",\n    \"optgroup\",\n    \"option\",\n    \"output\",\n    \"p\",\n    \"param\",\n    \"picture\",\n    \"pre\",\n    \"progress\",\n    \"q\",\n    \"rp\",\n    \"rt\",\n    \"ruby\",\n    \"s\",\n    \"samp\",\n    \"script\",\n    \"section\",\n    \"select\",\n    \"slot\",\n    \"small\",\n    \"source\",\n    \"span\",\n    \"strong\",\n    \"style\",\n    \"sub\",\n    \"summary\",\n    \"sup\",\n    \"table\",\n    \"tbody\",\n    \"td\",\n    \"template\",\n    \"textarea\",\n    \"tfoot\",\n    \"th\",\n    \"thead\",\n    \"time\",\n    \"title\",\n    \"tr\",\n    \"track\",\n    \"u\",\n    \"ul\",\n    \"var\",\n    \"video\",\n    \"wbr\",\n]\n\n\nclass InputStreamWithMemory(object):\n    \"\"\"Wraps an HTMLInputStream to remember characters since last <\n\n    This wraps existing HTMLInputStream classes to keep track of the stream\n    since the last < which marked an open tag state.\n\n    \"\"\"\n\n    def __init__(self, inner_stream):\n        self._inner_stream = inner_stream\n        self.reset = self._inner_stream.reset\n        self.position = self._inner_stream.position\n        self._buffer = []\n\n    @property\n    def errors(self):\n        return self._inner_stream.errors\n\n    @property\n    def charEncoding(self):\n        return self._inner_stream.charEncoding\n\n    @property\n    def changeEncoding(self):\n        return self._inner_stream.changeEncoding\n\n    def char(self):\n        c = self._inner_stream.char()\n        # char() can return None if EOF, so ignore that\n        if c:\n            self._buffer.append(c)\n        return c\n\n    def charsUntil(self, characters, opposite=False):\n        chars = self._inner_stream.charsUntil(characters, opposite=opposite)\n        self._buffer.extend(list(chars))\n        return chars\n\n    def unget(self, char):\n        if self._buffer:\n            self._buffer.pop(-1)\n        return self._inner_stream.unget(char)\n\n    def get_tag(self):\n        \"\"\"Returns the stream history since last '<'\n\n        Since the buffer starts at the last '<' as as seen by tagOpenState(),\n        we know that everything from that point to when this method is called\n        is the \"tag\" that is being tokenized.\n\n        \"\"\"\n        return six.text_type(\"\").join(self._buffer)\n\n    def start_tag(self):\n        \"\"\"Resets stream history to just '<'\n\n        This gets called by tagOpenState() which marks a '<' that denotes an\n        open tag. Any time we see that, we reset the buffer.\n\n        \"\"\"\n        self._buffer = [\"<\"]\n\n\nclass BleachHTMLTokenizer(HTMLTokenizer):\n    \"\"\"Tokenizer that doesn't consume character entities\"\"\"\n\n    def __init__(self, consume_entities=False, **kwargs):\n        super(BleachHTMLTokenizer, self).__init__(**kwargs)\n\n        self.consume_entities = consume_entities\n\n        # Wrap the stream with one that remembers the history\n        self.stream = InputStreamWithMemory(self.stream)\n\n    def __iter__(self):\n        last_error_token = None\n\n        for token in super(BleachHTMLTokenizer, self).__iter__():\n            if last_error_token is not None:\n                if (\n                    last_error_token[\"data\"] == \"invalid-character-in-attribute-name\"\n                    and token[\"type\"] in TAG_TOKEN_TYPES\n                    and token.get(\"data\")\n                ):\n                    # token[\"data\"] is an html5lib attributeMap\n                    # (OrderedDict 3.7+ and dict otherwise)\n                    # of attr name to attr value\n                    #\n                    # Remove attribute names that have ', \" or < in them\n                    # because those characters are invalid for attribute names.\n                    token[\"data\"] = attributeMap(\n                        (attr_name, attr_value)\n                        for attr_name, attr_value in token[\"data\"].items()\n                        if (\n                            '\"' not in attr_name\n                            and \"'\" not in attr_name\n                            and \"<\" not in attr_name\n                        )\n                    )\n                    last_error_token = None\n                    yield token\n\n                elif (\n                    last_error_token[\"data\"] == \"expected-closing-tag-but-got-char\"\n                    and self.parser.tags is not None\n                    and token[\"data\"].lower().strip() not in self.parser.tags\n                ):\n                    # We've got either a malformed tag or a pseudo-tag or\n                    # something that html5lib wants to turn into a malformed\n                    # comment which Bleach clean() will drop so we interfere\n                    # with the token stream to handle it more correctly.\n                    #\n                    # If this is an allowed tag, it's malformed and we just let\n                    # the html5lib parser deal with it--we don't enter into this\n                    # block.\n                    #\n                    # If this is not an allowed tag, then we convert it to\n                    # characters and it'll get escaped in the sanitizer.\n                    token[\"data\"] = self.stream.get_tag()\n                    token[\"type\"] = CHARACTERS_TYPE\n\n                    last_error_token = None\n                    yield token\n\n                elif token[\"type\"] == PARSEERROR_TYPE:\n                    # If the token is a parse error, then let the last_error_token\n                    # go, and make token the new last_error_token\n                    yield last_error_token\n                    last_error_token = token\n\n                else:\n                    yield last_error_token\n                    yield token\n                    last_error_token = None\n\n                continue\n\n            # If the token is a ParseError, we hold on to it so we can get the\n            # next token and potentially fix it.\n            if token[\"type\"] == PARSEERROR_TYPE:\n                last_error_token = token\n                continue\n\n            yield token\n\n        if last_error_token:\n            yield last_error_token\n\n    def consumeEntity(self, allowedChar=None, fromAttribute=False):\n        # If this tokenizer is set to consume entities, then we can let the\n        # superclass do its thing.\n        if self.consume_entities:\n            return super(BleachHTMLTokenizer, self).consumeEntity(\n                allowedChar, fromAttribute\n            )\n\n        # If this tokenizer is set to not consume entities, then we don't want\n        # to consume and convert them, so this overrides the html5lib tokenizer's\n        # consumeEntity so that it's now a no-op.\n        #\n        # However, when that gets called, it's consumed an &, so we put that back in\n        # the stream.\n        if fromAttribute:\n            self.currentToken[\"data\"][-1][1] += \"&\"\n\n        else:\n            self.tokenQueue.append({\"type\": CHARACTERS_TYPE, \"data\": \"&\"})\n\n    def tagOpenState(self):\n        # This state marks a < that is either a StartTag, EndTag, EmptyTag,\n        # or ParseError. In all cases, we want to drop any stream history\n        # we've collected so far and we do that by calling start_tag() on\n        # the input stream wrapper.\n        self.stream.start_tag()\n        return super(BleachHTMLTokenizer, self).tagOpenState()\n\n    def emitCurrentToken(self):\n        token = self.currentToken\n\n        if (\n            self.parser.tags is not None\n            and token[\"type\"] in TAG_TOKEN_TYPES\n            and token[\"name\"].lower() not in self.parser.tags\n        ):\n            # If this is a start/end/empty tag for a tag that's not in our\n            # allowed list, then it gets stripped or escaped. In both of these\n            # cases it gets converted to a Characters token.\n            if self.parser.strip:\n                # If we're stripping the token, we just throw in an empty\n                # string token.\n                new_data = \"\"\n\n            else:\n                # If we're escaping the token, we want to escape the exact\n                # original string. Since tokenizing also normalizes data\n                # and this is a tag-like thing, we've lost some information.\n                # So we go back through the stream to get the original\n                # string and use that.\n                new_data = self.stream.get_tag()\n\n            new_token = {\"type\": CHARACTERS_TYPE, \"data\": new_data}\n\n            self.currentToken = new_token\n            self.tokenQueue.append(new_token)\n            self.state = self.dataState\n            return\n\n        super(BleachHTMLTokenizer, self).emitCurrentToken()\n\n\nclass BleachHTMLParser(HTMLParser):\n    \"\"\"Parser that uses BleachHTMLTokenizer\"\"\"\n\n    def __init__(self, tags, strip, consume_entities, **kwargs):\n        \"\"\"\n        :arg tags: list of allowed tags--everything else is either stripped or\n            escaped; if None, then this doesn't look at tags at all\n        :arg strip: whether to strip disallowed tags (True) or escape them (False);\n            if tags=None, then this doesn't have any effect\n        :arg consume_entities: whether to consume entities (default behavior) or\n            leave them as is when tokenizing (BleachHTMLTokenizer-added behavior)\n\n        \"\"\"\n        self.tags = [tag.lower() for tag in tags] if tags is not None else None\n        self.strip = strip\n        self.consume_entities = consume_entities\n        super(BleachHTMLParser, self).__init__(**kwargs)\n\n    def _parse(\n        self, stream, innerHTML=False, container=\"div\", scripting=True, **kwargs\n    ):\n        # set scripting=True to parse <noscript> as though JS is enabled to\n        # match the expected context in browsers\n        #\n        # https://html.spec.whatwg.org/multipage/scripting.html#the-noscript-element\n        #\n        # Override HTMLParser so we can swap out the tokenizer for our own.\n        self.innerHTMLMode = innerHTML\n        self.container = container\n        self.scripting = scripting\n        self.tokenizer = BleachHTMLTokenizer(\n            stream=stream, consume_entities=self.consume_entities, parser=self, **kwargs\n        )\n        self.reset()\n\n        try:\n            self.mainLoop()\n        except ReparseException:\n            self.reset()\n            self.mainLoop()\n\n\ndef convert_entity(value):\n    \"\"\"Convert an entity (minus the & and ; part) into what it represents\n\n    This handles numeric, hex, and text entities.\n\n    :arg value: the string (minus the ``&`` and ``;`` part) to convert\n\n    :returns: unicode character or None if it's an ambiguous ampersand that\n        doesn't match a character entity\n\n    \"\"\"\n    if value[0] == \"#\":\n        if len(value) < 2:\n            return None\n\n        if value[1] in (\"x\", \"X\"):\n            # hex-encoded code point\n            int_as_string, base = value[2:], 16\n        else:\n            # decimal code point\n            int_as_string, base = value[1:], 10\n\n        if int_as_string == \"\":\n            return None\n\n        code_point = int(int_as_string, base)\n        if 0 < code_point < 0x110000:\n            return six.unichr(code_point)\n        else:\n            return None\n\n    return ENTITIES.get(value, None)\n\n\ndef convert_entities(text):\n    \"\"\"Converts all found entities in the text\n\n    :arg text: the text to convert entities in\n\n    :returns: unicode text with converted entities\n\n    \"\"\"\n    if \"&\" not in text:\n        return text\n\n    new_text = []\n    for part in next_possible_entity(text):\n        if not part:\n            continue\n\n        if part.startswith(\"&\"):\n            entity = match_entity(part)\n            if entity is not None:\n                converted = convert_entity(entity)\n\n                # If it's not an ambiguous ampersand, then replace with the\n                # unicode character. Otherwise, we leave the entity in.\n                if converted is not None:\n                    new_text.append(converted)\n                    remainder = part[len(entity) + 2 :]\n                    if part:\n                        new_text.append(remainder)\n                    continue\n\n        new_text.append(part)\n\n    return \"\".join(new_text)\n\n\ndef match_entity(stream):\n    \"\"\"Returns first entity in stream or None if no entity exists\n\n    Note: For Bleach purposes, entities must start with a \"&\" and end with\n    a \";\". This ignoresambiguous character entities that have no \";\" at the\n    end.\n\n    :arg stream: the character stream\n\n    :returns: ``None`` or the entity string without \"&\" or \";\"\n\n    \"\"\"\n    # Nix the & at the beginning\n    if stream[0] != \"&\":\n        raise ValueError('Stream should begin with \"&\"')\n\n    stream = stream[1:]\n\n    stream = list(stream)\n    possible_entity = \"\"\n    end_characters = \"<&=;\" + string.whitespace\n\n    # Handle number entities\n    if stream and stream[0] == \"#\":\n        possible_entity = \"#\"\n        stream.pop(0)\n\n        if stream and stream[0] in (\"x\", \"X\"):\n            allowed = \"0123456789abcdefABCDEF\"\n            possible_entity += stream.pop(0)\n        else:\n            allowed = \"0123456789\"\n\n        # FIXME(willkg): Do we want to make sure these are valid number\n        # entities? This doesn't do that currently.\n        while stream and stream[0] not in end_characters:\n            c = stream.pop(0)\n            if c not in allowed:\n                break\n            possible_entity += c\n\n        if possible_entity and stream and stream[0] == \";\":\n            return possible_entity\n        return None\n\n    # Handle character entities\n    while stream and stream[0] not in end_characters:\n        c = stream.pop(0)\n        if not ENTITIES_TRIE.has_keys_with_prefix(possible_entity):\n            break\n        possible_entity += c\n\n    if possible_entity and stream and stream[0] == \";\":\n        return possible_entity\n\n    return None\n\n\nAMP_SPLIT_RE = re.compile(\"(&)\")\n\n\ndef next_possible_entity(text):\n    \"\"\"Takes a text and generates a list of possible entities\n\n    :arg text: the text to look at\n\n    :returns: generator where each part (except the first) starts with an\n        \"&\"\n\n    \"\"\"\n    for i, part in enumerate(AMP_SPLIT_RE.split(text)):\n        if i == 0:\n            yield part\n        elif i % 2 == 0:\n            yield \"&\" + part\n\n\nclass BleachHTMLSerializer(HTMLSerializer):\n    \"\"\"HTMLSerializer that undoes & -> &amp; in attributes and sets\n    escape_rcdata to True\n    \"\"\"\n\n    # per the HTMLSerializer.__init__ docstring:\n    #\n    # Whether to escape characters that need to be\n    # escaped within normal elements within rcdata elements such as\n    # style.\n    #\n    escape_rcdata = True\n\n    def escape_base_amp(self, stoken):\n        \"\"\"Escapes just bare & in HTML attribute values\"\"\"\n        # First, undo escaping of &. We need to do this because html5lib's\n        # HTMLSerializer expected the tokenizer to consume all the character\n        # entities and convert them to their respective characters, but the\n        # BleachHTMLTokenizer doesn't do that. For example, this fixes\n        # &amp;entity; back to &entity; .\n        stoken = stoken.replace(\"&amp;\", \"&\")\n\n        # However, we do want all bare & that are not marking character\n        # entities to be changed to &amp;, so let's do that carefully here.\n        for part in next_possible_entity(stoken):\n            if not part:\n                continue\n\n            if part.startswith(\"&\"):\n                entity = match_entity(part)\n                # Only leave entities in that are not ambiguous. If they're\n                # ambiguous, then we escape the ampersand.\n                if entity is not None and convert_entity(entity) is not None:\n                    yield \"&\" + entity + \";\"\n\n                    # Length of the entity plus 2--one for & at the beginning\n                    # and one for ; at the end\n                    part = part[len(entity) + 2 :]\n                    if part:\n                        yield part\n                    continue\n\n            yield part.replace(\"&\", \"&amp;\")\n\n    def serialize(self, treewalker, encoding=None):\n        \"\"\"Wrap HTMLSerializer.serialize and conver & to &amp; in attribute values\n\n        Note that this converts & to &amp; in attribute values where the & isn't\n        already part of an unambiguous character entity.\n\n        \"\"\"\n        in_tag = False\n        after_equals = False\n\n        for stoken in super(BleachHTMLSerializer, self).serialize(treewalker, encoding):\n            if in_tag:\n                if stoken == \">\":\n                    in_tag = False\n\n                elif after_equals:\n                    if stoken != '\"':\n                        for part in self.escape_base_amp(stoken):\n                            yield part\n\n                        after_equals = False\n                        continue\n\n                elif stoken == \"=\":\n                    after_equals = True\n\n                yield stoken\n            else:\n                if stoken.startswith(\"<\"):\n                    in_tag = True\n                yield stoken\n", "code_before": "# flake8: noqa\n\"\"\"\nShim module between Bleach and html5lib. This makes it easier to upgrade the\nhtml5lib library without having to change a lot of code.\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport re\nimport string\nimport warnings\n\nimport six\n\n# ignore html5lib deprecation warnings to use bleach; we are bleach\n# apply before we import submodules that import html5lib\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"html5lib's sanitizer is deprecated\",\n    category=DeprecationWarning,\n    module=\"bleach._vendor.html5lib\",\n)\n\nfrom bleach._vendor.html5lib import (  # noqa: E402 module level import not at top of file\n    HTMLParser,\n    getTreeWalker,\n)\nfrom bleach._vendor.html5lib import (\n    constants,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib.constants import (  # noqa: E402 module level import not at top of file\n    namespaces,\n    prefixes,\n)\nfrom bleach._vendor.html5lib.constants import (\n    _ReparseException as ReparseException,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib.filters.base import (\n    Filter,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib.filters.sanitizer import (\n    allowed_protocols,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib.filters.sanitizer import (\n    Filter as SanitizerFilter,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib._inputstream import (\n    HTMLInputStream,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib.serializer import (\n    HTMLSerializer,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib._tokenizer import (\n    attributeMap,\n    HTMLTokenizer,\n)  # noqa: E402 module level import not at top of file\nfrom bleach._vendor.html5lib._trie import (\n    Trie,\n)  # noqa: E402 module level import not at top of file\n\n\n#: Map of entity name to expanded entity\nENTITIES = constants.entities\n\n#: Trie of html entity string -> character representation\nENTITIES_TRIE = Trie(ENTITIES)\n\n#: Token type constants--these never change\nTAG_TOKEN_TYPES = {\n    constants.tokenTypes[\"StartTag\"],\n    constants.tokenTypes[\"EndTag\"],\n    constants.tokenTypes[\"EmptyTag\"],\n}\nCHARACTERS_TYPE = constants.tokenTypes[\"Characters\"]\nPARSEERROR_TYPE = constants.tokenTypes[\"ParseError\"]\n\n\n#: List of valid HTML tags, from WHATWG HTML Living Standard as of 2018-10-17\n#: https://html.spec.whatwg.org/multipage/indices.html#elements-3\nHTML_TAGS = [\n    \"a\",\n    \"abbr\",\n    \"address\",\n    \"area\",\n    \"article\",\n    \"aside\",\n    \"audio\",\n    \"b\",\n    \"base\",\n    \"bdi\",\n    \"bdo\",\n    \"blockquote\",\n    \"body\",\n    \"br\",\n    \"button\",\n    \"canvas\",\n    \"caption\",\n    \"cite\",\n    \"code\",\n    \"col\",\n    \"colgroup\",\n    \"data\",\n    \"datalist\",\n    \"dd\",\n    \"del\",\n    \"details\",\n    \"dfn\",\n    \"dialog\",\n    \"div\",\n    \"dl\",\n    \"dt\",\n    \"em\",\n    \"embed\",\n    \"fieldset\",\n    \"figcaption\",\n    \"figure\",\n    \"footer\",\n    \"form\",\n    \"h1\",\n    \"h2\",\n    \"h3\",\n    \"h4\",\n    \"h5\",\n    \"h6\",\n    \"head\",\n    \"header\",\n    \"hgroup\",\n    \"hr\",\n    \"html\",\n    \"i\",\n    \"iframe\",\n    \"img\",\n    \"input\",\n    \"ins\",\n    \"kbd\",\n    \"keygen\",\n    \"label\",\n    \"legend\",\n    \"li\",\n    \"link\",\n    \"map\",\n    \"mark\",\n    \"menu\",\n    \"meta\",\n    \"meter\",\n    \"nav\",\n    \"noscript\",\n    \"object\",\n    \"ol\",\n    \"optgroup\",\n    \"option\",\n    \"output\",\n    \"p\",\n    \"param\",\n    \"picture\",\n    \"pre\",\n    \"progress\",\n    \"q\",\n    \"rp\",\n    \"rt\",\n    \"ruby\",\n    \"s\",\n    \"samp\",\n    \"script\",\n    \"section\",\n    \"select\",\n    \"slot\",\n    \"small\",\n    \"source\",\n    \"span\",\n    \"strong\",\n    \"style\",\n    \"sub\",\n    \"summary\",\n    \"sup\",\n    \"table\",\n    \"tbody\",\n    \"td\",\n    \"template\",\n    \"textarea\",\n    \"tfoot\",\n    \"th\",\n    \"thead\",\n    \"time\",\n    \"title\",\n    \"tr\",\n    \"track\",\n    \"u\",\n    \"ul\",\n    \"var\",\n    \"video\",\n    \"wbr\",\n]\n\n\nclass InputStreamWithMemory(object):\n    \"\"\"Wraps an HTMLInputStream to remember characters since last <\n\n    This wraps existing HTMLInputStream classes to keep track of the stream\n    since the last < which marked an open tag state.\n\n    \"\"\"\n\n    def __init__(self, inner_stream):\n        self._inner_stream = inner_stream\n        self.reset = self._inner_stream.reset\n        self.position = self._inner_stream.position\n        self._buffer = []\n\n    @property\n    def errors(self):\n        return self._inner_stream.errors\n\n    @property\n    def charEncoding(self):\n        return self._inner_stream.charEncoding\n\n    @property\n    def changeEncoding(self):\n        return self._inner_stream.changeEncoding\n\n    def char(self):\n        c = self._inner_stream.char()\n        # char() can return None if EOF, so ignore that\n        if c:\n            self._buffer.append(c)\n        return c\n\n    def charsUntil(self, characters, opposite=False):\n        chars = self._inner_stream.charsUntil(characters, opposite=opposite)\n        self._buffer.extend(list(chars))\n        return chars\n\n    def unget(self, char):\n        if self._buffer:\n            self._buffer.pop(-1)\n        return self._inner_stream.unget(char)\n\n    def get_tag(self):\n        \"\"\"Returns the stream history since last '<'\n\n        Since the buffer starts at the last '<' as as seen by tagOpenState(),\n        we know that everything from that point to when this method is called\n        is the \"tag\" that is being tokenized.\n\n        \"\"\"\n        return six.text_type(\"\").join(self._buffer)\n\n    def start_tag(self):\n        \"\"\"Resets stream history to just '<'\n\n        This gets called by tagOpenState() which marks a '<' that denotes an\n        open tag. Any time we see that, we reset the buffer.\n\n        \"\"\"\n        self._buffer = [\"<\"]\n\n\nclass BleachHTMLTokenizer(HTMLTokenizer):\n    \"\"\"Tokenizer that doesn't consume character entities\"\"\"\n\n    def __init__(self, consume_entities=False, **kwargs):\n        super(BleachHTMLTokenizer, self).__init__(**kwargs)\n\n        self.consume_entities = consume_entities\n\n        # Wrap the stream with one that remembers the history\n        self.stream = InputStreamWithMemory(self.stream)\n\n    def __iter__(self):\n        last_error_token = None\n\n        for token in super(BleachHTMLTokenizer, self).__iter__():\n            if last_error_token is not None:\n                if (\n                    last_error_token[\"data\"] == \"invalid-character-in-attribute-name\"\n                    and token[\"type\"] in TAG_TOKEN_TYPES\n                    and token.get(\"data\")\n                ):\n                    # token[\"data\"] is an html5lib attributeMap\n                    # (OrderedDict 3.7+ and dict otherwise)\n                    # of attr name to attr value\n                    #\n                    # Remove attribute names that have ', \" or < in them\n                    # because those characters are invalid for attribute names.\n                    token[\"data\"] = attributeMap(\n                        (attr_name, attr_value)\n                        for attr_name, attr_value in token[\"data\"].items()\n                        if (\n                            '\"' not in attr_name\n                            and \"'\" not in attr_name\n                            and \"<\" not in attr_name\n                        )\n                    )\n                    last_error_token = None\n                    yield token\n\n                elif (\n                    last_error_token[\"data\"] == \"expected-closing-tag-but-got-char\"\n                    and self.parser.tags is not None\n                    and token[\"data\"].lower().strip() not in self.parser.tags\n                ):\n                    # We've got either a malformed tag or a pseudo-tag or\n                    # something that html5lib wants to turn into a malformed\n                    # comment which Bleach clean() will drop so we interfere\n                    # with the token stream to handle it more correctly.\n                    #\n                    # If this is an allowed tag, it's malformed and we just let\n                    # the html5lib parser deal with it--we don't enter into this\n                    # block.\n                    #\n                    # If this is not an allowed tag, then we convert it to\n                    # characters and it'll get escaped in the sanitizer.\n                    token[\"data\"] = self.stream.get_tag()\n                    token[\"type\"] = CHARACTERS_TYPE\n\n                    last_error_token = None\n                    yield token\n\n                elif token[\"type\"] == PARSEERROR_TYPE:\n                    # If the token is a parse error, then let the last_error_token\n                    # go, and make token the new last_error_token\n                    yield last_error_token\n                    last_error_token = token\n\n                else:\n                    yield last_error_token\n                    yield token\n                    last_error_token = None\n\n                continue\n\n            # If the token is a ParseError, we hold on to it so we can get the\n            # next token and potentially fix it.\n            if token[\"type\"] == PARSEERROR_TYPE:\n                last_error_token = token\n                continue\n\n            yield token\n\n        if last_error_token:\n            yield last_error_token\n\n    def consumeEntity(self, allowedChar=None, fromAttribute=False):\n        # If this tokenizer is set to consume entities, then we can let the\n        # superclass do its thing.\n        if self.consume_entities:\n            return super(BleachHTMLTokenizer, self).consumeEntity(\n                allowedChar, fromAttribute\n            )\n\n        # If this tokenizer is set to not consume entities, then we don't want\n        # to consume and convert them, so this overrides the html5lib tokenizer's\n        # consumeEntity so that it's now a no-op.\n        #\n        # However, when that gets called, it's consumed an &, so we put that back in\n        # the stream.\n        if fromAttribute:\n            self.currentToken[\"data\"][-1][1] += \"&\"\n\n        else:\n            self.tokenQueue.append({\"type\": CHARACTERS_TYPE, \"data\": \"&\"})\n\n    def tagOpenState(self):\n        # This state marks a < that is either a StartTag, EndTag, EmptyTag,\n        # or ParseError. In all cases, we want to drop any stream history\n        # we've collected so far and we do that by calling start_tag() on\n        # the input stream wrapper.\n        self.stream.start_tag()\n        return super(BleachHTMLTokenizer, self).tagOpenState()\n\n    def emitCurrentToken(self):\n        token = self.currentToken\n\n        if (\n            self.parser.tags is not None\n            and token[\"type\"] in TAG_TOKEN_TYPES\n            and token[\"name\"].lower() not in self.parser.tags\n        ):\n            # If this is a start/end/empty tag for a tag that's not in our\n            # allowed list, then it gets stripped or escaped. In both of these\n            # cases it gets converted to a Characters token.\n            if self.parser.strip:\n                # If we're stripping the token, we just throw in an empty\n                # string token.\n                new_data = \"\"\n\n            else:\n                # If we're escaping the token, we want to escape the exact\n                # original string. Since tokenizing also normalizes data\n                # and this is a tag-like thing, we've lost some information.\n                # So we go back through the stream to get the original\n                # string and use that.\n                new_data = self.stream.get_tag()\n\n            new_token = {\"type\": CHARACTERS_TYPE, \"data\": new_data}\n\n            self.currentToken = new_token\n            self.tokenQueue.append(new_token)\n            self.state = self.dataState\n            return\n\n        super(BleachHTMLTokenizer, self).emitCurrentToken()\n\n\nclass BleachHTMLParser(HTMLParser):\n    \"\"\"Parser that uses BleachHTMLTokenizer\"\"\"\n\n    def __init__(self, tags, strip, consume_entities, **kwargs):\n        \"\"\"\n        :arg tags: list of allowed tags--everything else is either stripped or\n            escaped; if None, then this doesn't look at tags at all\n        :arg strip: whether to strip disallowed tags (True) or escape them (False);\n            if tags=None, then this doesn't have any effect\n        :arg consume_entities: whether to consume entities (default behavior) or\n            leave them as is when tokenizing (BleachHTMLTokenizer-added behavior)\n\n        \"\"\"\n        self.tags = [tag.lower() for tag in tags] if tags is not None else None\n        self.strip = strip\n        self.consume_entities = consume_entities\n        super(BleachHTMLParser, self).__init__(**kwargs)\n\n    def _parse(\n        self, stream, innerHTML=False, container=\"div\", scripting=True, **kwargs\n    ):\n        # set scripting=True to parse <noscript> as though JS is enabled to\n        # match the expected context in browsers\n        #\n        # https://html.spec.whatwg.org/multipage/scripting.html#the-noscript-element\n        #\n        # Override HTMLParser so we can swap out the tokenizer for our own.\n        self.innerHTMLMode = innerHTML\n        self.container = container\n        self.scripting = scripting\n        self.tokenizer = BleachHTMLTokenizer(\n            stream=stream, consume_entities=self.consume_entities, parser=self, **kwargs\n        )\n        self.reset()\n\n        try:\n            self.mainLoop()\n        except ReparseException:\n            self.reset()\n            self.mainLoop()\n\n\ndef convert_entity(value):\n    \"\"\"Convert an entity (minus the & and ; part) into what it represents\n\n    This handles numeric, hex, and text entities.\n\n    :arg value: the string (minus the ``&`` and ``;`` part) to convert\n\n    :returns: unicode character or None if it's an ambiguous ampersand that\n        doesn't match a character entity\n\n    \"\"\"\n    if value[0] == \"#\":\n        if len(value) < 2:\n            return None\n\n        if value[1] in (\"x\", \"X\"):\n            # hex-encoded code point\n            int_as_string, base = value[2:], 16\n        else:\n            # decimal code point\n            int_as_string, base = value[1:], 10\n\n        if int_as_string == \"\":\n            return None\n\n        code_point = int(int_as_string, base)\n        if 0 < code_point < 0x110000:\n            return six.unichr(code_point)\n        else:\n            return None\n\n    return ENTITIES.get(value, None)\n\n\ndef convert_entities(text):\n    \"\"\"Converts all found entities in the text\n\n    :arg text: the text to convert entities in\n\n    :returns: unicode text with converted entities\n\n    \"\"\"\n    if \"&\" not in text:\n        return text\n\n    new_text = []\n    for part in next_possible_entity(text):\n        if not part:\n            continue\n\n        if part.startswith(\"&\"):\n            entity = match_entity(part)\n            if entity is not None:\n                converted = convert_entity(entity)\n\n                # If it's not an ambiguous ampersand, then replace with the\n                # unicode character. Otherwise, we leave the entity in.\n                if converted is not None:\n                    new_text.append(converted)\n                    remainder = part[len(entity) + 2 :]\n                    if part:\n                        new_text.append(remainder)\n                    continue\n\n        new_text.append(part)\n\n    return \"\".join(new_text)\n\n\ndef match_entity(stream):\n    \"\"\"Returns first entity in stream or None if no entity exists\n\n    Note: For Bleach purposes, entities must start with a \"&\" and end with\n    a \";\". This ignoresambiguous character entities that have no \";\" at the\n    end.\n\n    :arg stream: the character stream\n\n    :returns: ``None`` or the entity string without \"&\" or \";\"\n\n    \"\"\"\n    # Nix the & at the beginning\n    if stream[0] != \"&\":\n        raise ValueError('Stream should begin with \"&\"')\n\n    stream = stream[1:]\n\n    stream = list(stream)\n    possible_entity = \"\"\n    end_characters = \"<&=;\" + string.whitespace\n\n    # Handle number entities\n    if stream and stream[0] == \"#\":\n        possible_entity = \"#\"\n        stream.pop(0)\n\n        if stream and stream[0] in (\"x\", \"X\"):\n            allowed = \"0123456789abcdefABCDEF\"\n            possible_entity += stream.pop(0)\n        else:\n            allowed = \"0123456789\"\n\n        # FIXME(willkg): Do we want to make sure these are valid number\n        # entities? This doesn't do that currently.\n        while stream and stream[0] not in end_characters:\n            c = stream.pop(0)\n            if c not in allowed:\n                break\n            possible_entity += c\n\n        if possible_entity and stream and stream[0] == \";\":\n            return possible_entity\n        return None\n\n    # Handle character entities\n    while stream and stream[0] not in end_characters:\n        c = stream.pop(0)\n        if not ENTITIES_TRIE.has_keys_with_prefix(possible_entity):\n            break\n        possible_entity += c\n\n    if possible_entity and stream and stream[0] == \";\":\n        return possible_entity\n\n    return None\n\n\nAMP_SPLIT_RE = re.compile(\"(&)\")\n\n\ndef next_possible_entity(text):\n    \"\"\"Takes a text and generates a list of possible entities\n\n    :arg text: the text to look at\n\n    :returns: generator where each part (except the first) starts with an\n        \"&\"\n\n    \"\"\"\n    for i, part in enumerate(AMP_SPLIT_RE.split(text)):\n        if i == 0:\n            yield part\n        elif i % 2 == 0:\n            yield \"&\" + part\n\n\nclass BleachHTMLSerializer(HTMLSerializer):\n    \"\"\"HTMLSerializer that undoes & -> &amp; in attributes and sets\n    escape_rcdata to True\n    \"\"\"\n\n    # per the HTMLSerializer.__init__ docstring:\n    #\n    # Whether to escape characters that need to be\n    # escaped within normal elements within rcdata elements such as\n    # style.\n    #\n    escape_rcdata = True\n\n    def escape_base_amp(self, stoken):\n        \"\"\"Escapes just bare & in HTML attribute values\"\"\"\n        # First, undo escaping of &. We need to do this because html5lib's\n        # HTMLSerializer expected the tokenizer to consume all the character\n        # entities and convert them to their respective characters, but the\n        # BleachHTMLTokenizer doesn't do that. For example, this fixes\n        # &amp;entity; back to &entity; .\n        stoken = stoken.replace(\"&amp;\", \"&\")\n\n        # However, we do want all bare & that are not marking character\n        # entities to be changed to &amp;, so let's do that carefully here.\n        for part in next_possible_entity(stoken):\n            if not part:\n                continue\n\n            if part.startswith(\"&\"):\n                entity = match_entity(part)\n                # Only leave entities in that are not ambiguous. If they're\n                # ambiguous, then we escape the ampersand.\n                if entity is not None and convert_entity(entity) is not None:\n                    yield \"&\" + entity + \";\"\n\n                    # Length of the entity plus 2--one for & at the beginning\n                    # and one for ; at the end\n                    part = part[len(entity) + 2 :]\n                    if part:\n                        yield part\n                    continue\n\n            yield part.replace(\"&\", \"&amp;\")\n\n    def serialize(self, treewalker, encoding=None):\n        \"\"\"Wrap HTMLSerializer.serialize and conver & to &amp; in attribute values\n\n        Note that this converts & to &amp; in attribute values where the & isn't\n        already part of an unambiguous character entity.\n\n        \"\"\"\n        in_tag = False\n        after_equals = False\n\n        for stoken in super(BleachHTMLSerializer, self).serialize(treewalker, encoding):\n            if in_tag:\n                if stoken == \">\":\n                    in_tag = False\n\n                elif after_equals:\n                    if stoken != '\"':\n                        for part in self.escape_base_amp(stoken):\n                            yield part\n\n                        after_equals = False\n                        continue\n\n                elif stoken == \"=\":\n                    after_equals = True\n\n                yield stoken\n            else:\n                if stoken.startswith(\"<\"):\n                    in_tag = True\n                yield stoken\n", "patch": "@@ -48,6 +48,7 @@\n     HTMLInputStream,\n )  # noqa: E402 module level import not at top of file\n from bleach._vendor.html5lib.serializer import (\n+    escape,\n     HTMLSerializer,\n )  # noqa: E402 module level import not at top of file\n from bleach._vendor.html5lib._tokenizer import (", "file_path": "files/2023_2/760", "file_language": "py", "file_name": "bleach/html5lib_shim.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/mozilla/bleach/raw/1334134d34397966a7f7cfebd38639e9ba2c680e/bleach%2Fsanitizer.py", "code": "from __future__ import unicode_literals\n\nfrom itertools import chain\nimport re\nimport warnings\n\nimport six\nfrom six.moves.urllib.parse import urlparse\nfrom xml.sax.saxutils import unescape\n\nfrom bleach import html5lib_shim\nfrom bleach.utils import alphabetize_attributes, force_unicode\n\n\n#: List of allowed tags\nALLOWED_TAGS = [\n    \"a\",\n    \"abbr\",\n    \"acronym\",\n    \"b\",\n    \"blockquote\",\n    \"code\",\n    \"em\",\n    \"i\",\n    \"li\",\n    \"ol\",\n    \"strong\",\n    \"ul\",\n]\n\n\n#: Map of allowed attributes by tag\nALLOWED_ATTRIBUTES = {\n    \"a\": [\"href\", \"title\"],\n    \"abbr\": [\"title\"],\n    \"acronym\": [\"title\"],\n}\n\n#: List of allowed styles\nALLOWED_STYLES = []\n\n#: List of allowed protocols\nALLOWED_PROTOCOLS = [\"http\", \"https\", \"mailto\"]\n\n#: Invisible characters--0 to and including 31 except 9 (tab), 10 (lf), and 13 (cr)\nINVISIBLE_CHARACTERS = \"\".join(\n    [chr(c) for c in chain(range(0, 9), range(11, 13), range(14, 32))]\n)\n\n#: Regexp for characters that are invisible\nINVISIBLE_CHARACTERS_RE = re.compile(\"[\" + INVISIBLE_CHARACTERS + \"]\", re.UNICODE)\n\n#: String to replace invisible characters with. This can be a character, a\n#: string, or even a function that takes a Python re matchobj\nINVISIBLE_REPLACEMENT_CHAR = \"?\"\n\n\nclass Cleaner(object):\n    \"\"\"Cleaner for cleaning HTML fragments of malicious content\n\n    This cleaner is a security-focused function whose sole purpose is to remove\n    malicious content from a string such that it can be displayed as content in\n    a web page.\n\n    To use::\n\n        from bleach.sanitizer import Cleaner\n\n        cleaner = Cleaner()\n\n        for text in all_the_yucky_things:\n            sanitized = cleaner.clean(text)\n\n    .. Note::\n\n       This cleaner is not designed to use to transform content to be used in\n       non-web-page contexts.\n\n    .. Warning::\n\n       This cleaner is not thread-safe--the html parser has internal state.\n       Create a separate cleaner per thread!\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        tags=ALLOWED_TAGS,\n        attributes=ALLOWED_ATTRIBUTES,\n        styles=ALLOWED_STYLES,\n        protocols=ALLOWED_PROTOCOLS,\n        strip=False,\n        strip_comments=True,\n        filters=None,\n    ):\n        \"\"\"Initializes a Cleaner\n\n        :arg list tags: allowed list of tags; defaults to\n            ``bleach.sanitizer.ALLOWED_TAGS``\n\n        :arg dict attributes: allowed attributes; can be a callable, list or dict;\n            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``\n\n        :arg list styles: allowed list of css styles; defaults to\n            ``bleach.sanitizer.ALLOWED_STYLES``\n\n        :arg list protocols: allowed list of protocols for links; defaults\n            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``\n\n        :arg bool strip: whether or not to strip disallowed elements\n\n        :arg bool strip_comments: whether or not to strip HTML comments\n\n        :arg list filters: list of html5lib Filter classes to pass streamed content through\n\n            .. seealso:: http://html5lib.readthedocs.io/en/latest/movingparts.html#filters\n\n            .. Warning::\n\n               Using filters changes the output of ``bleach.Cleaner.clean``.\n               Make sure the way the filters change the output are secure.\n\n        \"\"\"\n        self.tags = tags\n        self.attributes = attributes\n        self.styles = styles\n        self.protocols = protocols\n        self.strip = strip\n        self.strip_comments = strip_comments\n        self.filters = filters or []\n\n        self.parser = html5lib_shim.BleachHTMLParser(\n            tags=self.tags,\n            strip=self.strip,\n            consume_entities=False,\n            namespaceHTMLElements=False,\n        )\n        self.walker = html5lib_shim.getTreeWalker(\"etree\")\n        self.serializer = html5lib_shim.BleachHTMLSerializer(\n            quote_attr_values=\"always\",\n            omit_optional_tags=False,\n            escape_lt_in_attrs=True,\n            # We want to leave entities as they are without escaping or\n            # resolving or expanding\n            resolve_entities=False,\n            # Bleach has its own sanitizer, so don't use the html5lib one\n            sanitize=False,\n            # Bleach sanitizer alphabetizes already, so don't use the html5lib one\n            alphabetical_attributes=False,\n        )\n\n    def clean(self, text):\n        \"\"\"Cleans text and returns sanitized result as unicode\n\n        :arg str text: text to be cleaned\n\n        :returns: sanitized text as unicode\n\n        :raises TypeError: if ``text`` is not a text type\n\n        \"\"\"\n        if not isinstance(text, six.string_types):\n            message = (\n                \"argument cannot be of '{name}' type, must be of text type\".format(\n                    name=text.__class__.__name__\n                )\n            )\n            raise TypeError(message)\n\n        if not text:\n            return \"\"\n\n        text = force_unicode(text)\n\n        dom = self.parser.parseFragment(text)\n        filtered = BleachSanitizerFilter(\n            source=self.walker(dom),\n            # Bleach-sanitizer-specific things\n            attributes=self.attributes,\n            strip_disallowed_elements=self.strip,\n            strip_html_comments=self.strip_comments,\n            # html5lib-sanitizer things\n            allowed_elements=self.tags,\n            allowed_css_properties=self.styles,\n            allowed_protocols=self.protocols,\n            allowed_svg_properties=[],\n        )\n\n        # Apply any filters after the BleachSanitizerFilter\n        for filter_class in self.filters:\n            filtered = filter_class(source=filtered)\n\n        return self.serializer.render(filtered)\n\n\ndef attribute_filter_factory(attributes):\n    \"\"\"Generates attribute filter function for the given attributes value\n\n    The attributes value can take one of several shapes. This returns a filter\n    function appropriate to the attributes value. One nice thing about this is\n    that there's less if/then shenanigans in the ``allow_token`` method.\n\n    \"\"\"\n    if callable(attributes):\n        return attributes\n\n    if isinstance(attributes, dict):\n\n        def _attr_filter(tag, attr, value):\n            if tag in attributes:\n                attr_val = attributes[tag]\n                if callable(attr_val):\n                    return attr_val(tag, attr, value)\n\n                if attr in attr_val:\n                    return True\n\n            if \"*\" in attributes:\n                attr_val = attributes[\"*\"]\n                if callable(attr_val):\n                    return attr_val(tag, attr, value)\n\n                return attr in attr_val\n\n            return False\n\n        return _attr_filter\n\n    if isinstance(attributes, list):\n\n        def _attr_filter(tag, attr, value):\n            return attr in attributes\n\n        return _attr_filter\n\n    raise ValueError(\"attributes needs to be a callable, a list or a dict\")\n\n\nclass BleachSanitizerFilter(html5lib_shim.SanitizerFilter):\n    \"\"\"html5lib Filter that sanitizes text\n\n    This filter can be used anywhere html5lib filters can be used.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        source,\n        attributes=ALLOWED_ATTRIBUTES,\n        strip_disallowed_elements=False,\n        strip_html_comments=True,\n        **kwargs\n    ):\n        \"\"\"Creates a BleachSanitizerFilter instance\n\n        :arg Treewalker source: stream\n\n        :arg list tags: allowed list of tags; defaults to\n            ``bleach.sanitizer.ALLOWED_TAGS``\n\n        :arg dict attributes: allowed attributes; can be a callable, list or dict;\n            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``\n\n        :arg list styles: allowed list of css styles; defaults to\n            ``bleach.sanitizer.ALLOWED_STYLES``\n\n        :arg list protocols: allowed list of protocols for links; defaults\n            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``\n\n        :arg bool strip_disallowed_elements: whether or not to strip disallowed\n            elements\n\n        :arg bool strip_html_comments: whether or not to strip HTML comments\n\n        \"\"\"\n        self.attr_filter = attribute_filter_factory(attributes)\n        self.strip_disallowed_elements = strip_disallowed_elements\n        self.strip_html_comments = strip_html_comments\n\n        # filter out html5lib deprecation warnings to use bleach from BleachSanitizerFilter init\n        warnings.filterwarnings(\n            \"ignore\",\n            message=\"html5lib's sanitizer is deprecated\",\n            category=DeprecationWarning,\n            module=\"bleach._vendor.html5lib\",\n        )\n        return super(BleachSanitizerFilter, self).__init__(source, **kwargs)\n\n    def sanitize_stream(self, token_iterator):\n        for token in token_iterator:\n            ret = self.sanitize_token(token)\n\n            if not ret:\n                continue\n\n            if isinstance(ret, list):\n                for subtoken in ret:\n                    yield subtoken\n            else:\n                yield ret\n\n    def merge_characters(self, token_iterator):\n        \"\"\"Merge consecutive Characters tokens in a stream\"\"\"\n        characters_buffer = []\n\n        for token in token_iterator:\n            if characters_buffer:\n                if token[\"type\"] == \"Characters\":\n                    characters_buffer.append(token)\n                    continue\n                else:\n                    # Merge all the characters tokens together into one and then\n                    # operate on it.\n                    new_token = {\n                        \"data\": \"\".join(\n                            [char_token[\"data\"] for char_token in characters_buffer]\n                        ),\n                        \"type\": \"Characters\",\n                    }\n                    characters_buffer = []\n                    yield new_token\n\n            elif token[\"type\"] == \"Characters\":\n                characters_buffer.append(token)\n                continue\n\n            yield token\n\n        new_token = {\n            \"data\": \"\".join([char_token[\"data\"] for char_token in characters_buffer]),\n            \"type\": \"Characters\",\n        }\n        yield new_token\n\n    def __iter__(self):\n        return self.merge_characters(\n            self.sanitize_stream(html5lib_shim.Filter.__iter__(self))\n        )\n\n    def sanitize_token(self, token):\n        \"\"\"Sanitize a token either by HTML-encoding or dropping.\n\n        Unlike sanitizer.Filter, allowed_attributes can be a dict of {'tag':\n        ['attribute', 'pairs'], 'tag': callable}.\n\n        Here callable is a function with two arguments of attribute name and\n        value. It should return true of false.\n\n        Also gives the option to strip tags instead of encoding.\n\n        :arg dict token: token to sanitize\n\n        :returns: token or list of tokens\n\n        \"\"\"\n        token_type = token[\"type\"]\n        if token_type in [\"StartTag\", \"EndTag\", \"EmptyTag\"]:\n            if token[\"name\"] in self.allowed_elements:\n                return self.allow_token(token)\n\n            elif self.strip_disallowed_elements:\n                return None\n\n            else:\n                if \"data\" in token:\n                    # Alphabetize the attributes before calling .disallowed_token()\n                    # so that the resulting string is stable\n                    token[\"data\"] = alphabetize_attributes(token[\"data\"])\n                return self.disallowed_token(token)\n\n        elif token_type == \"Comment\":\n            if not self.strip_html_comments:\n                # call lxml.sax.saxutils to escape &, <, and > in addition to \" and '\n                token[\"data\"] = html5lib_shim.escape(\n                    token[\"data\"], entities={'\"': \"&quot;\", \"'\": \"&#x27;\"}\n                )\n                return token\n            else:\n                return None\n\n        elif token_type == \"Characters\":\n            return self.sanitize_characters(token)\n\n        else:\n            return token\n\n    def sanitize_characters(self, token):\n        \"\"\"Handles Characters tokens\n\n        Our overridden tokenizer doesn't do anything with entities. However,\n        that means that the serializer will convert all ``&`` in Characters\n        tokens to ``&amp;``.\n\n        Since we don't want that, we extract entities here and convert them to\n        Entity tokens so the serializer will let them be.\n\n        :arg token: the Characters token to work on\n\n        :returns: a list of tokens\n\n        \"\"\"\n        data = token.get(\"data\", \"\")\n\n        if not data:\n            return token\n\n        data = INVISIBLE_CHARACTERS_RE.sub(INVISIBLE_REPLACEMENT_CHAR, data)\n        token[\"data\"] = data\n\n        # If there isn't a & in the data, we can return now\n        if \"&\" not in data:\n            return token\n\n        new_tokens = []\n\n        # For each possible entity that starts with a \"&\", we try to extract an\n        # actual entity and re-tokenize accordingly\n        for part in html5lib_shim.next_possible_entity(data):\n            if not part:\n                continue\n\n            if part.startswith(\"&\"):\n                entity = html5lib_shim.match_entity(part)\n                if entity is not None:\n                    if entity == \"amp\":\n                        # LinkifyFilter can't match urls across token boundaries\n                        # which is problematic with &amp; since that shows up in\n                        # querystrings all the time. This special-cases &amp;\n                        # and converts it to a & and sticks it in as a\n                        # Characters token. It'll get merged with surrounding\n                        # tokens in the BleachSanitizerfilter.__iter__ and\n                        # escaped in the serializer.\n                        new_tokens.append({\"type\": \"Characters\", \"data\": \"&\"})\n                    else:\n                        new_tokens.append({\"type\": \"Entity\", \"name\": entity})\n\n                    # Length of the entity plus 2--one for & at the beginning\n                    # and one for ; at the end\n                    remainder = part[len(entity) + 2 :]\n                    if remainder:\n                        new_tokens.append({\"type\": \"Characters\", \"data\": remainder})\n                    continue\n\n            new_tokens.append({\"type\": \"Characters\", \"data\": part})\n\n        return new_tokens\n\n    def sanitize_uri_value(self, value, allowed_protocols):\n        \"\"\"Checks a uri value to see if it's allowed\n\n        :arg value: the uri value to sanitize\n        :arg allowed_protocols: list of allowed protocols\n\n        :returns: allowed value or None\n\n        \"\"\"\n        # NOTE(willkg): This transforms the value into one that's easier to\n        # match and verify, but shouldn't get returned since it's vastly\n        # different than the original value.\n\n        # Convert all character entities in the value\n        new_value = html5lib_shim.convert_entities(value)\n\n        # Nix backtick, space characters, and control characters\n        new_value = re.sub(r\"[`\\000-\\040\\177-\\240\\s]+\", \"\", new_value)\n\n        # Remove REPLACEMENT characters\n        new_value = new_value.replace(\"\\ufffd\", \"\")\n\n        # Lowercase it--this breaks the value, but makes it easier to match\n        # against\n        new_value = new_value.lower()\n\n        try:\n            # Drop attributes with uri values that have protocols that aren't\n            # allowed\n            parsed = urlparse(new_value)\n        except ValueError:\n            # URI is impossible to parse, therefore it's not allowed\n            return None\n\n        if parsed.scheme:\n            # If urlparse found a scheme, check that\n            if parsed.scheme in allowed_protocols:\n                return value\n\n        else:\n            # Allow uris that are just an anchor\n            if new_value.startswith(\"#\"):\n                return value\n\n            # Handle protocols that urlparse doesn't recognize like \"myprotocol\"\n            if \":\" in new_value and new_value.split(\":\")[0] in allowed_protocols:\n                return value\n\n            # If there's no protocol/scheme specified, then assume it's \"http\"\n            # and see if that's allowed\n            if \"http\" in allowed_protocols:\n                return value\n\n        return None\n\n    def allow_token(self, token):\n        \"\"\"Handles the case where we're allowing the tag\"\"\"\n        if \"data\" in token:\n            # Loop through all the attributes and drop the ones that are not\n            # allowed, are unsafe or break other rules. Additionally, fix\n            # attribute values that need fixing.\n            #\n            # At the end of this loop, we have the final set of attributes\n            # we're keeping.\n            attrs = {}\n            for namespaced_name, val in token[\"data\"].items():\n                namespace, name = namespaced_name\n\n                # Drop attributes that are not explicitly allowed\n                #\n                # NOTE(willkg): We pass in the attribute name--not a namespaced\n                # name.\n                if not self.attr_filter(token[\"name\"], name, val):\n                    continue\n\n                # Drop attributes with uri values that use a disallowed protocol\n                # Sanitize attributes with uri values\n                if namespaced_name in self.attr_val_is_uri:\n                    new_value = self.sanitize_uri_value(val, self.allowed_protocols)\n                    if new_value is None:\n                        continue\n                    val = new_value\n\n                # Drop values in svg attrs with non-local IRIs\n                if namespaced_name in self.svg_attr_val_allows_ref:\n                    new_val = re.sub(r\"url\\s*\\(\\s*[^#\\s][^)]+?\\)\", \" \", unescape(val))\n                    new_val = new_val.strip()\n                    if not new_val:\n                        continue\n\n                    else:\n                        # Replace the val with the unescaped version because\n                        # it's a iri\n                        val = new_val\n\n                # Drop href and xlink:href attr for svg elements with non-local IRIs\n                if (None, token[\"name\"]) in self.svg_allow_local_href:\n                    if namespaced_name in [\n                        (None, \"href\"),\n                        (html5lib_shim.namespaces[\"xlink\"], \"href\"),\n                    ]:\n                        if re.search(r\"^\\s*[^#\\s]\", val):\n                            continue\n\n                # If it's a style attribute, sanitize it\n                if namespaced_name == (None, \"style\"):\n                    val = self.sanitize_css(val)\n\n                # At this point, we want to keep the attribute, so add it in\n                attrs[namespaced_name] = val\n\n            token[\"data\"] = alphabetize_attributes(attrs)\n\n        return token\n\n    def disallowed_token(self, token):\n        token_type = token[\"type\"]\n        if token_type == \"EndTag\":\n            token[\"data\"] = \"</%s>\" % token[\"name\"]\n\n        elif token[\"data\"]:\n            assert token_type in (\"StartTag\", \"EmptyTag\")\n            attrs = []\n            for (ns, name), v in token[\"data\"].items():\n                # If we end up with a namespace, but no name, switch them so we\n                # have a valid name to use.\n                if ns and not name:\n                    ns, name = name, ns\n\n                # Figure out namespaced name if the namespace is appropriate\n                # and exists; if the ns isn't in prefixes, then drop it.\n                if ns is None or ns not in html5lib_shim.prefixes:\n                    namespaced_name = name\n                else:\n                    namespaced_name = \"%s:%s\" % (html5lib_shim.prefixes[ns], name)\n\n                attrs.append(\n                    ' %s=\"%s\"'\n                    % (\n                        namespaced_name,\n                        # NOTE(willkg): HTMLSerializer escapes attribute values\n                        # already, so if we do it here (like HTMLSerializer does),\n                        # then we end up double-escaping.\n                        v,\n                    )\n                )\n            token[\"data\"] = \"<%s%s>\" % (token[\"name\"], \"\".join(attrs))\n\n        else:\n            token[\"data\"] = \"<%s>\" % token[\"name\"]\n\n        if token.get(\"selfClosing\"):\n            token[\"data\"] = token[\"data\"][:-1] + \"/>\"\n\n        token[\"type\"] = \"Characters\"\n\n        del token[\"name\"]\n        return token\n\n    def sanitize_css(self, style):\n        \"\"\"Sanitizes css in style tags\"\"\"\n        # Convert entities in the style so that it can be parsed as CSS\n        style = html5lib_shim.convert_entities(style)\n\n        # Drop any url values before we do anything else\n        style = re.compile(r\"url\\s*\\(\\s*[^\\s)]+?\\s*\\)\\s*\").sub(\" \", style)\n\n        # The gauntlet of sanitization\n\n        # Validate the css in the style tag and if it's not valid, then drop\n        # the whole thing.\n        parts = style.split(\";\")\n        gauntlet = re.compile(\n            r\"\"\"^(  # consider a style attribute value as composed of:\n[/:,#%!.\\s\\w]    # a non-newline character\n|\\w-\\w           # 3 characters in the form \\w-\\w\n|'[\\s\\w]+'\\s*    # a single quoted string of [\\s\\w]+ with trailing space\n|\"[\\s\\w]+\"       # a double quoted string of [\\s\\w]+\n|\\([\\d,%\\.\\s]+\\) # a parenthesized string of one or more digits, commas, periods, ...\n)*$\"\"\",  # ... percent signs, or whitespace e.g. from 'color: hsl(30,100%,50%)'\n            flags=re.U | re.VERBOSE,\n        )\n\n        for part in parts:\n            if not gauntlet.match(part):\n                return \"\"\n\n        if not re.match(r\"^\\s*([-\\w]+\\s*:[^:;]*(;\\s*|$))*$\", style):\n            return \"\"\n\n        clean = []\n        for prop, value in re.findall(r\"([-\\w]+)\\s*:\\s*([^:;]*)\", style):\n            if not value:\n                continue\n\n            if prop.lower() in self.allowed_css_properties:\n                clean.append(prop + \": \" + value + \";\")\n\n            elif prop.lower() in self.allowed_svg_properties:\n                clean.append(prop + \": \" + value + \";\")\n\n        return \" \".join(clean)\n", "code_before": "from __future__ import unicode_literals\n\nfrom itertools import chain\nimport re\nimport warnings\n\nimport six\nfrom six.moves.urllib.parse import urlparse\nfrom xml.sax.saxutils import unescape\n\nfrom bleach import html5lib_shim\nfrom bleach.utils import alphabetize_attributes, force_unicode\n\n\n#: List of allowed tags\nALLOWED_TAGS = [\n    \"a\",\n    \"abbr\",\n    \"acronym\",\n    \"b\",\n    \"blockquote\",\n    \"code\",\n    \"em\",\n    \"i\",\n    \"li\",\n    \"ol\",\n    \"strong\",\n    \"ul\",\n]\n\n\n#: Map of allowed attributes by tag\nALLOWED_ATTRIBUTES = {\n    \"a\": [\"href\", \"title\"],\n    \"abbr\": [\"title\"],\n    \"acronym\": [\"title\"],\n}\n\n#: List of allowed styles\nALLOWED_STYLES = []\n\n#: List of allowed protocols\nALLOWED_PROTOCOLS = [\"http\", \"https\", \"mailto\"]\n\n#: Invisible characters--0 to and including 31 except 9 (tab), 10 (lf), and 13 (cr)\nINVISIBLE_CHARACTERS = \"\".join(\n    [chr(c) for c in chain(range(0, 9), range(11, 13), range(14, 32))]\n)\n\n#: Regexp for characters that are invisible\nINVISIBLE_CHARACTERS_RE = re.compile(\"[\" + INVISIBLE_CHARACTERS + \"]\", re.UNICODE)\n\n#: String to replace invisible characters with. This can be a character, a\n#: string, or even a function that takes a Python re matchobj\nINVISIBLE_REPLACEMENT_CHAR = \"?\"\n\n\nclass Cleaner(object):\n    \"\"\"Cleaner for cleaning HTML fragments of malicious content\n\n    This cleaner is a security-focused function whose sole purpose is to remove\n    malicious content from a string such that it can be displayed as content in\n    a web page.\n\n    To use::\n\n        from bleach.sanitizer import Cleaner\n\n        cleaner = Cleaner()\n\n        for text in all_the_yucky_things:\n            sanitized = cleaner.clean(text)\n\n    .. Note::\n\n       This cleaner is not designed to use to transform content to be used in\n       non-web-page contexts.\n\n    .. Warning::\n\n       This cleaner is not thread-safe--the html parser has internal state.\n       Create a separate cleaner per thread!\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        tags=ALLOWED_TAGS,\n        attributes=ALLOWED_ATTRIBUTES,\n        styles=ALLOWED_STYLES,\n        protocols=ALLOWED_PROTOCOLS,\n        strip=False,\n        strip_comments=True,\n        filters=None,\n    ):\n        \"\"\"Initializes a Cleaner\n\n        :arg list tags: allowed list of tags; defaults to\n            ``bleach.sanitizer.ALLOWED_TAGS``\n\n        :arg dict attributes: allowed attributes; can be a callable, list or dict;\n            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``\n\n        :arg list styles: allowed list of css styles; defaults to\n            ``bleach.sanitizer.ALLOWED_STYLES``\n\n        :arg list protocols: allowed list of protocols for links; defaults\n            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``\n\n        :arg bool strip: whether or not to strip disallowed elements\n\n        :arg bool strip_comments: whether or not to strip HTML comments\n\n        :arg list filters: list of html5lib Filter classes to pass streamed content through\n\n            .. seealso:: http://html5lib.readthedocs.io/en/latest/movingparts.html#filters\n\n            .. Warning::\n\n               Using filters changes the output of ``bleach.Cleaner.clean``.\n               Make sure the way the filters change the output are secure.\n\n        \"\"\"\n        self.tags = tags\n        self.attributes = attributes\n        self.styles = styles\n        self.protocols = protocols\n        self.strip = strip\n        self.strip_comments = strip_comments\n        self.filters = filters or []\n\n        self.parser = html5lib_shim.BleachHTMLParser(\n            tags=self.tags,\n            strip=self.strip,\n            consume_entities=False,\n            namespaceHTMLElements=False,\n        )\n        self.walker = html5lib_shim.getTreeWalker(\"etree\")\n        self.serializer = html5lib_shim.BleachHTMLSerializer(\n            quote_attr_values=\"always\",\n            omit_optional_tags=False,\n            escape_lt_in_attrs=True,\n            # We want to leave entities as they are without escaping or\n            # resolving or expanding\n            resolve_entities=False,\n            # Bleach has its own sanitizer, so don't use the html5lib one\n            sanitize=False,\n            # Bleach sanitizer alphabetizes already, so don't use the html5lib one\n            alphabetical_attributes=False,\n        )\n\n    def clean(self, text):\n        \"\"\"Cleans text and returns sanitized result as unicode\n\n        :arg str text: text to be cleaned\n\n        :returns: sanitized text as unicode\n\n        :raises TypeError: if ``text`` is not a text type\n\n        \"\"\"\n        if not isinstance(text, six.string_types):\n            message = (\n                \"argument cannot be of '{name}' type, must be of text type\".format(\n                    name=text.__class__.__name__\n                )\n            )\n            raise TypeError(message)\n\n        if not text:\n            return \"\"\n\n        text = force_unicode(text)\n\n        dom = self.parser.parseFragment(text)\n        filtered = BleachSanitizerFilter(\n            source=self.walker(dom),\n            # Bleach-sanitizer-specific things\n            attributes=self.attributes,\n            strip_disallowed_elements=self.strip,\n            strip_html_comments=self.strip_comments,\n            # html5lib-sanitizer things\n            allowed_elements=self.tags,\n            allowed_css_properties=self.styles,\n            allowed_protocols=self.protocols,\n            allowed_svg_properties=[],\n        )\n\n        # Apply any filters after the BleachSanitizerFilter\n        for filter_class in self.filters:\n            filtered = filter_class(source=filtered)\n\n        return self.serializer.render(filtered)\n\n\ndef attribute_filter_factory(attributes):\n    \"\"\"Generates attribute filter function for the given attributes value\n\n    The attributes value can take one of several shapes. This returns a filter\n    function appropriate to the attributes value. One nice thing about this is\n    that there's less if/then shenanigans in the ``allow_token`` method.\n\n    \"\"\"\n    if callable(attributes):\n        return attributes\n\n    if isinstance(attributes, dict):\n\n        def _attr_filter(tag, attr, value):\n            if tag in attributes:\n                attr_val = attributes[tag]\n                if callable(attr_val):\n                    return attr_val(tag, attr, value)\n\n                if attr in attr_val:\n                    return True\n\n            if \"*\" in attributes:\n                attr_val = attributes[\"*\"]\n                if callable(attr_val):\n                    return attr_val(tag, attr, value)\n\n                return attr in attr_val\n\n            return False\n\n        return _attr_filter\n\n    if isinstance(attributes, list):\n\n        def _attr_filter(tag, attr, value):\n            return attr in attributes\n\n        return _attr_filter\n\n    raise ValueError(\"attributes needs to be a callable, a list or a dict\")\n\n\nclass BleachSanitizerFilter(html5lib_shim.SanitizerFilter):\n    \"\"\"html5lib Filter that sanitizes text\n\n    This filter can be used anywhere html5lib filters can be used.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        source,\n        attributes=ALLOWED_ATTRIBUTES,\n        strip_disallowed_elements=False,\n        strip_html_comments=True,\n        **kwargs\n    ):\n        \"\"\"Creates a BleachSanitizerFilter instance\n\n        :arg Treewalker source: stream\n\n        :arg list tags: allowed list of tags; defaults to\n            ``bleach.sanitizer.ALLOWED_TAGS``\n\n        :arg dict attributes: allowed attributes; can be a callable, list or dict;\n            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``\n\n        :arg list styles: allowed list of css styles; defaults to\n            ``bleach.sanitizer.ALLOWED_STYLES``\n\n        :arg list protocols: allowed list of protocols for links; defaults\n            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``\n\n        :arg bool strip_disallowed_elements: whether or not to strip disallowed\n            elements\n\n        :arg bool strip_html_comments: whether or not to strip HTML comments\n\n        \"\"\"\n        self.attr_filter = attribute_filter_factory(attributes)\n        self.strip_disallowed_elements = strip_disallowed_elements\n        self.strip_html_comments = strip_html_comments\n\n        # filter out html5lib deprecation warnings to use bleach from BleachSanitizerFilter init\n        warnings.filterwarnings(\n            \"ignore\",\n            message=\"html5lib's sanitizer is deprecated\",\n            category=DeprecationWarning,\n            module=\"bleach._vendor.html5lib\",\n        )\n        return super(BleachSanitizerFilter, self).__init__(source, **kwargs)\n\n    def sanitize_stream(self, token_iterator):\n        for token in token_iterator:\n            ret = self.sanitize_token(token)\n\n            if not ret:\n                continue\n\n            if isinstance(ret, list):\n                for subtoken in ret:\n                    yield subtoken\n            else:\n                yield ret\n\n    def merge_characters(self, token_iterator):\n        \"\"\"Merge consecutive Characters tokens in a stream\"\"\"\n        characters_buffer = []\n\n        for token in token_iterator:\n            if characters_buffer:\n                if token[\"type\"] == \"Characters\":\n                    characters_buffer.append(token)\n                    continue\n                else:\n                    # Merge all the characters tokens together into one and then\n                    # operate on it.\n                    new_token = {\n                        \"data\": \"\".join(\n                            [char_token[\"data\"] for char_token in characters_buffer]\n                        ),\n                        \"type\": \"Characters\",\n                    }\n                    characters_buffer = []\n                    yield new_token\n\n            elif token[\"type\"] == \"Characters\":\n                characters_buffer.append(token)\n                continue\n\n            yield token\n\n        new_token = {\n            \"data\": \"\".join([char_token[\"data\"] for char_token in characters_buffer]),\n            \"type\": \"Characters\",\n        }\n        yield new_token\n\n    def __iter__(self):\n        return self.merge_characters(\n            self.sanitize_stream(html5lib_shim.Filter.__iter__(self))\n        )\n\n    def sanitize_token(self, token):\n        \"\"\"Sanitize a token either by HTML-encoding or dropping.\n\n        Unlike sanitizer.Filter, allowed_attributes can be a dict of {'tag':\n        ['attribute', 'pairs'], 'tag': callable}.\n\n        Here callable is a function with two arguments of attribute name and\n        value. It should return true of false.\n\n        Also gives the option to strip tags instead of encoding.\n\n        :arg dict token: token to sanitize\n\n        :returns: token or list of tokens\n\n        \"\"\"\n        token_type = token[\"type\"]\n        if token_type in [\"StartTag\", \"EndTag\", \"EmptyTag\"]:\n            if token[\"name\"] in self.allowed_elements:\n                return self.allow_token(token)\n\n            elif self.strip_disallowed_elements:\n                return None\n\n            else:\n                if \"data\" in token:\n                    # Alphabetize the attributes before calling .disallowed_token()\n                    # so that the resulting string is stable\n                    token[\"data\"] = alphabetize_attributes(token[\"data\"])\n                return self.disallowed_token(token)\n\n        elif token_type == \"Comment\":\n            if not self.strip_html_comments:\n                return token\n            else:\n                return None\n\n        elif token_type == \"Characters\":\n            return self.sanitize_characters(token)\n\n        else:\n            return token\n\n    def sanitize_characters(self, token):\n        \"\"\"Handles Characters tokens\n\n        Our overridden tokenizer doesn't do anything with entities. However,\n        that means that the serializer will convert all ``&`` in Characters\n        tokens to ``&amp;``.\n\n        Since we don't want that, we extract entities here and convert them to\n        Entity tokens so the serializer will let them be.\n\n        :arg token: the Characters token to work on\n\n        :returns: a list of tokens\n\n        \"\"\"\n        data = token.get(\"data\", \"\")\n\n        if not data:\n            return token\n\n        data = INVISIBLE_CHARACTERS_RE.sub(INVISIBLE_REPLACEMENT_CHAR, data)\n        token[\"data\"] = data\n\n        # If there isn't a & in the data, we can return now\n        if \"&\" not in data:\n            return token\n\n        new_tokens = []\n\n        # For each possible entity that starts with a \"&\", we try to extract an\n        # actual entity and re-tokenize accordingly\n        for part in html5lib_shim.next_possible_entity(data):\n            if not part:\n                continue\n\n            if part.startswith(\"&\"):\n                entity = html5lib_shim.match_entity(part)\n                if entity is not None:\n                    if entity == \"amp\":\n                        # LinkifyFilter can't match urls across token boundaries\n                        # which is problematic with &amp; since that shows up in\n                        # querystrings all the time. This special-cases &amp;\n                        # and converts it to a & and sticks it in as a\n                        # Characters token. It'll get merged with surrounding\n                        # tokens in the BleachSanitizerfilter.__iter__ and\n                        # escaped in the serializer.\n                        new_tokens.append({\"type\": \"Characters\", \"data\": \"&\"})\n                    else:\n                        new_tokens.append({\"type\": \"Entity\", \"name\": entity})\n\n                    # Length of the entity plus 2--one for & at the beginning\n                    # and one for ; at the end\n                    remainder = part[len(entity) + 2 :]\n                    if remainder:\n                        new_tokens.append({\"type\": \"Characters\", \"data\": remainder})\n                    continue\n\n            new_tokens.append({\"type\": \"Characters\", \"data\": part})\n\n        return new_tokens\n\n    def sanitize_uri_value(self, value, allowed_protocols):\n        \"\"\"Checks a uri value to see if it's allowed\n\n        :arg value: the uri value to sanitize\n        :arg allowed_protocols: list of allowed protocols\n\n        :returns: allowed value or None\n\n        \"\"\"\n        # NOTE(willkg): This transforms the value into one that's easier to\n        # match and verify, but shouldn't get returned since it's vastly\n        # different than the original value.\n\n        # Convert all character entities in the value\n        new_value = html5lib_shim.convert_entities(value)\n\n        # Nix backtick, space characters, and control characters\n        new_value = re.sub(r\"[`\\000-\\040\\177-\\240\\s]+\", \"\", new_value)\n\n        # Remove REPLACEMENT characters\n        new_value = new_value.replace(\"\\ufffd\", \"\")\n\n        # Lowercase it--this breaks the value, but makes it easier to match\n        # against\n        new_value = new_value.lower()\n\n        try:\n            # Drop attributes with uri values that have protocols that aren't\n            # allowed\n            parsed = urlparse(new_value)\n        except ValueError:\n            # URI is impossible to parse, therefore it's not allowed\n            return None\n\n        if parsed.scheme:\n            # If urlparse found a scheme, check that\n            if parsed.scheme in allowed_protocols:\n                return value\n\n        else:\n            # Allow uris that are just an anchor\n            if new_value.startswith(\"#\"):\n                return value\n\n            # Handle protocols that urlparse doesn't recognize like \"myprotocol\"\n            if \":\" in new_value and new_value.split(\":\")[0] in allowed_protocols:\n                return value\n\n            # If there's no protocol/scheme specified, then assume it's \"http\"\n            # and see if that's allowed\n            if \"http\" in allowed_protocols:\n                return value\n\n        return None\n\n    def allow_token(self, token):\n        \"\"\"Handles the case where we're allowing the tag\"\"\"\n        if \"data\" in token:\n            # Loop through all the attributes and drop the ones that are not\n            # allowed, are unsafe or break other rules. Additionally, fix\n            # attribute values that need fixing.\n            #\n            # At the end of this loop, we have the final set of attributes\n            # we're keeping.\n            attrs = {}\n            for namespaced_name, val in token[\"data\"].items():\n                namespace, name = namespaced_name\n\n                # Drop attributes that are not explicitly allowed\n                #\n                # NOTE(willkg): We pass in the attribute name--not a namespaced\n                # name.\n                if not self.attr_filter(token[\"name\"], name, val):\n                    continue\n\n                # Drop attributes with uri values that use a disallowed protocol\n                # Sanitize attributes with uri values\n                if namespaced_name in self.attr_val_is_uri:\n                    new_value = self.sanitize_uri_value(val, self.allowed_protocols)\n                    if new_value is None:\n                        continue\n                    val = new_value\n\n                # Drop values in svg attrs with non-local IRIs\n                if namespaced_name in self.svg_attr_val_allows_ref:\n                    new_val = re.sub(r\"url\\s*\\(\\s*[^#\\s][^)]+?\\)\", \" \", unescape(val))\n                    new_val = new_val.strip()\n                    if not new_val:\n                        continue\n\n                    else:\n                        # Replace the val with the unescaped version because\n                        # it's a iri\n                        val = new_val\n\n                # Drop href and xlink:href attr for svg elements with non-local IRIs\n                if (None, token[\"name\"]) in self.svg_allow_local_href:\n                    if namespaced_name in [\n                        (None, \"href\"),\n                        (html5lib_shim.namespaces[\"xlink\"], \"href\"),\n                    ]:\n                        if re.search(r\"^\\s*[^#\\s]\", val):\n                            continue\n\n                # If it's a style attribute, sanitize it\n                if namespaced_name == (None, \"style\"):\n                    val = self.sanitize_css(val)\n\n                # At this point, we want to keep the attribute, so add it in\n                attrs[namespaced_name] = val\n\n            token[\"data\"] = alphabetize_attributes(attrs)\n\n        return token\n\n    def disallowed_token(self, token):\n        token_type = token[\"type\"]\n        if token_type == \"EndTag\":\n            token[\"data\"] = \"</%s>\" % token[\"name\"]\n\n        elif token[\"data\"]:\n            assert token_type in (\"StartTag\", \"EmptyTag\")\n            attrs = []\n            for (ns, name), v in token[\"data\"].items():\n                # If we end up with a namespace, but no name, switch them so we\n                # have a valid name to use.\n                if ns and not name:\n                    ns, name = name, ns\n\n                # Figure out namespaced name if the namespace is appropriate\n                # and exists; if the ns isn't in prefixes, then drop it.\n                if ns is None or ns not in html5lib_shim.prefixes:\n                    namespaced_name = name\n                else:\n                    namespaced_name = \"%s:%s\" % (html5lib_shim.prefixes[ns], name)\n\n                attrs.append(\n                    ' %s=\"%s\"'\n                    % (\n                        namespaced_name,\n                        # NOTE(willkg): HTMLSerializer escapes attribute values\n                        # already, so if we do it here (like HTMLSerializer does),\n                        # then we end up double-escaping.\n                        v,\n                    )\n                )\n            token[\"data\"] = \"<%s%s>\" % (token[\"name\"], \"\".join(attrs))\n\n        else:\n            token[\"data\"] = \"<%s>\" % token[\"name\"]\n\n        if token.get(\"selfClosing\"):\n            token[\"data\"] = token[\"data\"][:-1] + \"/>\"\n\n        token[\"type\"] = \"Characters\"\n\n        del token[\"name\"]\n        return token\n\n    def sanitize_css(self, style):\n        \"\"\"Sanitizes css in style tags\"\"\"\n        # Convert entities in the style so that it can be parsed as CSS\n        style = html5lib_shim.convert_entities(style)\n\n        # Drop any url values before we do anything else\n        style = re.compile(r\"url\\s*\\(\\s*[^\\s)]+?\\s*\\)\\s*\").sub(\" \", style)\n\n        # The gauntlet of sanitization\n\n        # Validate the css in the style tag and if it's not valid, then drop\n        # the whole thing.\n        parts = style.split(\";\")\n        gauntlet = re.compile(\n            r\"\"\"^(  # consider a style attribute value as composed of:\n[/:,#%!.\\s\\w]    # a non-newline character\n|\\w-\\w           # 3 characters in the form \\w-\\w\n|'[\\s\\w]+'\\s*    # a single quoted string of [\\s\\w]+ with trailing space\n|\"[\\s\\w]+\"       # a double quoted string of [\\s\\w]+\n|\\([\\d,%\\.\\s]+\\) # a parenthesized string of one or more digits, commas, periods, ...\n)*$\"\"\",  # ... percent signs, or whitespace e.g. from 'color: hsl(30,100%,50%)'\n            flags=re.U | re.VERBOSE,\n        )\n\n        for part in parts:\n            if not gauntlet.match(part):\n                return \"\"\n\n        if not re.match(r\"^\\s*([-\\w]+\\s*:[^:;]*(;\\s*|$))*$\", style):\n            return \"\"\n\n        clean = []\n        for prop, value in re.findall(r\"([-\\w]+)\\s*:\\s*([^:;]*)\", style):\n            if not value:\n                continue\n\n            if prop.lower() in self.allowed_css_properties:\n                clean.append(prop + \": \" + value + \";\")\n\n            elif prop.lower() in self.allowed_svg_properties:\n                clean.append(prop + \": \" + value + \";\")\n\n        return \" \".join(clean)\n", "patch": "@@ -371,6 +371,10 @@ def sanitize_token(self, token):\n \n         elif token_type == \"Comment\":\n             if not self.strip_html_comments:\n+                # call lxml.sax.saxutils to escape &, <, and > in addition to \" and '\n+                token[\"data\"] = html5lib_shim.escape(\n+                    token[\"data\"], entities={'\"': \"&quot;\", \"'\": \"&#x27;\"}\n+                )\n                 return token\n             else:\n                 return None", "file_path": "files/2023_2/761", "file_language": "py", "file_name": "bleach/sanitizer.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/mozilla/bleach/raw/1334134d34397966a7f7cfebd38639e9ba2c680e/tests%2Ftest_clean.py", "code": "from __future__ import unicode_literals\n\nimport os\n\nimport pytest\n\nfrom bleach import clean\nfrom bleach.html5lib_shim import Filter\nfrom bleach.sanitizer import Cleaner\nfrom bleach._vendor.html5lib.constants import rcdataElements\n\n\ndef test_clean_idempotent():\n    \"\"\"Make sure that applying the filter twice doesn't change anything.\"\"\"\n    dirty = \"<span>invalid & </span> < extra http://link.com<em>\"\n    assert clean(clean(dirty)) == clean(dirty)\n\n\ndef test_only_text_is_cleaned():\n    some_text = \"text\"\n    some_type = int\n    no_type = None\n\n    assert clean(some_text) == some_text\n\n    with pytest.raises(TypeError) as e:\n        clean(some_type)\n    assert \"argument cannot be of 'type' type\" in str(e.value)\n\n    with pytest.raises(TypeError) as e:\n        clean(no_type)\n    assert \"NoneType\" in str(e.value)\n\n\ndef test_empty():\n    assert clean(\"\") == \"\"\n\n\ndef test_content_has_no_html():\n    assert clean(\"no html string\") == \"no html string\"\n\n\n@pytest.mark.parametrize(\n    \"data, expected\",\n    [\n        (\"an <strong>allowed</strong> tag\", \"an <strong>allowed</strong> tag\"),\n        (\"another <em>good</em> tag\", \"another <em>good</em> tag\"),\n    ],\n)\ndef test_content_has_allowed_html(data, expected):\n    assert clean(data) == expected\n\n\ndef test_html_is_lowercased():\n    assert (\n        clean('<A HREF=\"http://example.com\">foo</A>')\n        == '<a href=\"http://example.com\">foo</a>'\n    )\n\n\ndef test_invalid_uri_does_not_raise_error():\n    assert clean('<a href=\"http://example.com]\">text</a>') == \"<a>text</a>\"\n\n\n@pytest.mark.parametrize(\n    \"data, should_strip, expected\",\n    [\n        # Regular comment\n        (\"<!-- this is a comment -->\", True, \"\"),\n        # Open comment with no close comment bit\n        (\"<!-- open comment\", True, \"\"),\n        (\"<!--open comment\", True, \"\"),\n        (\"<!-- open comment\", False, \"<!-- open comment-->\"),\n        (\"<!--open comment\", False, \"<!--open comment-->\"),\n        # Comment with text to the right\n        (\"<!-- comment -->text\", True, \"text\"),\n        (\"<!--comment-->text\", True, \"text\"),\n        (\"<!-- comment -->text\", False, \"<!-- comment -->text\"),\n        (\"<!--comment-->text\", False, \"<!--comment-->text\"),\n        # Comment with text to the left\n        (\"text<!-- comment -->\", True, \"text\"),\n        (\"text<!--comment-->\", True, \"text\"),\n        (\"text<!-- comment -->\", False, \"text<!-- comment -->\"),\n        (\"text<!--comment-->\", False, \"text<!--comment-->\"),\n    ],\n)\ndef test_comments(data, should_strip, expected):\n    assert clean(data, strip_comments=should_strip) == expected\n\n\ndef test_invalid_char_in_tag():\n    assert (\n        clean('<script/xss src=\"http://xx.com/xss.js\"></script>')\n        == '&lt;script/xss src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n    )\n    assert (\n        clean('<script/src=\"http://xx.com/xss.js\"></script>')\n        == '&lt;script/src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n    )\n\n\ndef test_unclosed_tag():\n    assert clean(\"a <em>fixed tag\") == \"a <em>fixed tag</em>\"\n    assert (\n        clean(\"<script src=http://xx.com/xss.js<b>\")\n        == \"&lt;script src=http://xx.com/xss.js&lt;b&gt;\"\n    )\n    assert (\n        clean('<script src=\"http://xx.com/xss.js\"<b>')\n        == '&lt;script src=\"http://xx.com/xss.js\"&lt;b&gt;'\n    )\n    assert (\n        clean('<script src=\"http://xx.com/xss.js\" <b>')\n        == '&lt;script src=\"http://xx.com/xss.js\" &lt;b&gt;'\n    )\n\n\ndef test_nested_script_tag():\n    assert (\n        clean(\"<<script>script>evil()<</script>/script>\")\n        == \"&lt;&lt;script&gt;script&gt;evil()&lt;&lt;/script&gt;/script&gt;\"\n    )\n    assert (\n        clean(\"<<x>script>evil()<</x>/script>\")\n        == \"&lt;&lt;x&gt;script&gt;evil()&lt;&lt;/x&gt;/script&gt;\"\n    )\n    assert (\n        clean(\"<script<script>>evil()</script</script>>\")\n        == \"&lt;script&lt;script&gt;&gt;evil()&lt;/script&lt;/script&gt;&gt;\"\n    )\n\n\n@pytest.mark.parametrize(\n    \"text, expected\",\n    [\n        (\"an & entity\", \"an &amp; entity\"),\n        (\"an < entity\", \"an &lt; entity\"),\n        (\"tag < <em>and</em> entity\", \"tag &lt; <em>and</em> entity\"),\n    ],\n)\ndef test_bare_entities_get_escaped_correctly(text, expected):\n    assert clean(text) == expected\n\n\n@pytest.mark.parametrize(\n    \"text, expected\",\n    [\n        # Test character entities\n        (\"&amp;\", \"&amp;\"),\n        (\"&nbsp;\", \"&nbsp;\"),\n        (\"&nbsp; test string &nbsp;\", \"&nbsp; test string &nbsp;\"),\n        (\"&lt;em&gt;strong&lt;/em&gt;\", \"&lt;em&gt;strong&lt;/em&gt;\"),\n        # Test character entity at beginning of string\n        (\"&amp;is cool\", \"&amp;is cool\"),\n        # Test it at the end of the string\n        (\"cool &amp;\", \"cool &amp;\"),\n        # Test bare ampersands and entities at beginning\n        (\"&&amp; is cool\", \"&amp;&amp; is cool\"),\n        # Test entities and bare ampersand at end\n        (\"&amp; is cool &amp;&\", \"&amp; is cool &amp;&amp;\"),\n        # Test missing semi-colon means we don't treat it like an entity\n        (\"this &amp that\", \"this &amp;amp that\"),\n        # Test a thing that looks like a character entity, but isn't because it's\n        # missing a ; (&current)\n        (\n            \"http://example.com?active=true&current=true\",\n            \"http://example.com?active=true&amp;current=true\",\n        ),\n        # Test character entities in attribute values are left alone\n        ('<a href=\"?art&amp;copy\">foo</a>', '<a href=\"?art&amp;copy\">foo</a>'),\n        ('<a href=\"?this=&gt;that\">foo</a>', '<a href=\"?this=&gt;that\">foo</a>'),\n        # Ambiguous ampersands get escaped in attributes\n        (\n            '<a href=\"http://example.com/&xx;\">foo</a>',\n            '<a href=\"http://example.com/&amp;xx;\">foo</a>',\n        ),\n        (\n            '<a href=\"http://example.com?active=true&current=true\">foo</a>',\n            '<a href=\"http://example.com?active=true&amp;current=true\">foo</a>',\n        ),\n        # Ambiguous ampersands in text are not escaped\n        (\"&xx;\", \"&xx;\"),\n        # Test numeric entities\n        (\"&#39;\", \"&#39;\"),\n        (\"&#34;\", \"&#34;\"),\n        (\"&#123;\", \"&#123;\"),\n        (\"&#x0007b;\", \"&#x0007b;\"),\n        (\"&#x0007B;\", \"&#x0007B;\"),\n        # Test non-numeric entities\n        (\"&#\", \"&amp;#\"),\n        (\"&#<\", \"&amp;#&lt;\"),\n        # html5lib tokenizer unescapes character entities, so these would become '\n        # and \" which makes it possible to break out of html attributes.\n        #\n        # Verify that clean() doesn't unescape entities.\n        (\"&#39;&#34;\", \"&#39;&#34;\"),\n    ],\n)\ndef test_character_entities_handling(text, expected):\n    assert clean(text) == expected\n\n\n@pytest.mark.parametrize(\n    \"data, kwargs, expected\",\n    [\n        # All tags are allowed, so it strips nothing\n        (\n            \"a test <em>with</em> <b>html</b> tags\",\n            {},\n            \"a test <em>with</em> <b>html</b> tags\",\n        ),\n        # img tag is disallowed, so it's stripped\n        (\n            'a test <em>with</em> <img src=\"http://example.com/\"> <b>html</b> tags',\n            {},\n            \"a test <em>with</em>  <b>html</b> tags\",\n        ),\n        # a tag is disallowed, so it's stripped\n        (\n            '<p><a href=\"http://example.com/\">link text</a></p>',\n            {\"tags\": [\"p\"]},\n            \"<p>link text</p>\",\n        ),\n        # Test nested disallowed tag\n        (\n            \"<p><span>multiply <span>nested <span>text</span></span></span></p>\",\n            {\"tags\": [\"p\"]},\n            \"<p>multiply nested text</p>\",\n        ),\n        # (#271)\n        (\"<ul><li><script></li></ul>\", {\"tags\": [\"ul\", \"li\"]}, \"<ul><li></li></ul>\"),\n        # Test disallowed tag that's deep in the tree\n        (\n            '<p><a href=\"http://example.com/\"><img src=\"http://example.com/\"></a></p>',\n            {\"tags\": [\"a\", \"p\"]},\n            '<p><a href=\"http://example.com/\"></a></p>',\n        ),\n        # Test isindex -- the parser expands this to a prompt (#279)\n        (\"<isindex>\", {}, \"\"),\n        # Test non-tags that are well-formed HTML (#280)\n        (\"Yeah right <sarcasm/>\", {}, \"Yeah right \"),\n        (\"<sarcasm>\", {}, \"\"),\n        (\"</sarcasm>\", {}, \"\"),\n        # These are non-tags, but also \"malformed\" so they don't get treated like\n        # tags and stripped\n        (\"</ sarcasm>\", {}, \"&lt;/ sarcasm&gt;\"),\n        (\"</ sarcasm >\", {}, \"&lt;/ sarcasm &gt;\"),\n        (\"Foo <bar@example.com>\", {}, \"Foo \"),\n        (\"Favorite movie: <name of movie>\", {}, \"Favorite movie: \"),\n        (\"</3\", {}, \"&lt;/3\"),\n    ],\n)\ndef test_stripping_tags(data, kwargs, expected):\n    assert clean(data, strip=True, **kwargs) == expected\n    assert clean(\"  \" + data + \"  \", strip=True, **kwargs) == \"  \" + expected + \"  \"\n    assert (\n        clean(\"abc \" + data + \" def\", strip=True, **kwargs)\n        == \"abc \" + expected + \" def\"\n    )\n\n\n@pytest.mark.parametrize(\n    \"data, expected\",\n    [\n        # Disallowed tag is escaped\n        (\n            \"<img src=\\\"javascript:alert('XSS');\\\">\",\n            \"&lt;img src=\\\"javascript:alert('XSS');\\\"&gt;\",\n        ),\n        # Test with parens\n        (\"<script>safe()</script>\", \"&lt;script&gt;safe()&lt;/script&gt;\"),\n        # Test with braces\n        (\"<style>body{}</style>\", \"&lt;style&gt;body{}&lt;/style&gt;\"),\n        # Test nested disallow tags (#271)\n        (\"<ul><li><script></li></ul>\", \"<ul><li>&lt;script&gt;</li></ul>\"),\n        # Test isindex -- the parser expands this to a prompt (#279)\n        (\"<isindex>\", \"&lt;isindex&gt;\"),\n        # Test non-tags (#280)\n        (\"<sarcasm/>\", \"&lt;sarcasm/&gt;\"),\n        (\"<sarcasm>\", \"&lt;sarcasm&gt;\"),\n        (\"</sarcasm>\", \"&lt;/sarcasm&gt;\"),\n        (\"</ sarcasm>\", \"&lt;/ sarcasm&gt;\"),\n        (\"</ sarcasm >\", \"&lt;/ sarcasm &gt;\"),\n        (\"</3\", \"&lt;/3\"),\n        (\"<bar@example.com>\", \"&lt;bar@example.com&gt;\"),\n        (\"Favorite movie: <name of movie>\", \"Favorite movie: &lt;name of movie&gt;\"),\n    ],\n)\ndef test_escaping_tags(data, expected):\n    assert clean(data, strip=False) == expected\n    assert clean(\"  \" + data + \"  \", strip=False) == \"  \" + expected + \"  \"\n    assert clean(\"abc \" + data + \" def\", strip=False) == \"abc \" + expected + \" def\"\n\n\n@pytest.mark.parametrize(\n    \"data, expected\",\n    [\n        (\"<scri<script>pt>alert(1)</scr</script>ipt>\", \"pt&gt;alert(1)ipt&gt;\"),\n        (\"<scri<scri<script>pt>pt>alert(1)</script>\", \"pt&gt;pt&gt;alert(1)\"),\n    ],\n)\ndef test_stripping_tags_is_safe(data, expected):\n    \"\"\"Test stripping tags shouldn't result in malicious content\"\"\"\n    assert clean(data, strip=True) == expected\n\n\ndef test_allowed_styles():\n    \"\"\"Test allowed styles\"\"\"\n    ATTRS = [\"style\"]\n    STYLE = [\"color\"]\n\n    assert clean('<b style=\"top:0\"></b>', attributes=ATTRS) == '<b style=\"\"></b>'\n\n    text = '<b style=\"color: blue;\"></b>'\n    assert clean(text, attributes=ATTRS, styles=STYLE) == text\n\n    text = '<b style=\"top: 0; color: blue;\"></b>'\n    assert clean(text, attributes=ATTRS, styles=STYLE) == '<b style=\"color: blue;\"></b>'\n\n\ndef test_href_with_wrong_tag():\n    assert clean('<em href=\"fail\">no link</em>') == \"<em>no link</em>\"\n\n\ndef test_disallowed_attr():\n    IMG = [\n        \"img\",\n    ]\n    IMG_ATTR = [\"src\"]\n\n    assert clean('<a onclick=\"evil\" href=\"test\">test</a>') == '<a href=\"test\">test</a>'\n    assert (\n        clean('<img onclick=\"evil\" src=\"test\" />', tags=IMG, attributes=IMG_ATTR)\n        == '<img src=\"test\">'\n    )\n    assert (\n        clean('<img href=\"invalid\" src=\"test\" />', tags=IMG, attributes=IMG_ATTR)\n        == '<img src=\"test\">'\n    )\n\n\ndef test_unquoted_attr_values_are_quoted():\n    assert (\n        clean(\"<abbr title=mytitle>myabbr</abbr>\")\n        == '<abbr title=\"mytitle\">myabbr</abbr>'\n    )\n\n\ndef test_unquoted_event_handler_attr_value():\n    assert (\n        clean('<a href=\"http://xx.com\" onclick=foo()>xx.com</a>')\n        == '<a href=\"http://xx.com\">xx.com</a>'\n    )\n\n\ndef test_invalid_filter_attr():\n    IMG = [\n        \"img\",\n    ]\n    IMG_ATTR = {\n        \"img\": lambda tag, name, val: name == \"src\" and val == \"http://example.com/\"\n    }\n\n    assert (\n        clean(\n            '<img onclick=\"evil\" src=\"http://example.com/\" />',\n            tags=IMG,\n            attributes=IMG_ATTR,\n        )\n        == '<img src=\"http://example.com/\">'\n    )\n    assert (\n        clean(\n            '<img onclick=\"evil\" src=\"http://badhost.com/\" />',\n            tags=IMG,\n            attributes=IMG_ATTR,\n        )\n        == \"<img>\"\n    )\n\n\ndef test_poster_attribute():\n    \"\"\"Poster attributes should not allow javascript.\"\"\"\n    tags = [\"video\"]\n    attrs = {\"video\": [\"poster\"]}\n\n    test = '<video poster=\"javascript:alert(1)\"></video>'\n    assert clean(test, tags=tags, attributes=attrs) == \"<video></video>\"\n\n    ok = '<video poster=\"/foo.png\"></video>'\n    assert clean(ok, tags=tags, attributes=attrs) == ok\n\n\ndef test_attributes_callable():\n    \"\"\"Verify attributes can take a callable\"\"\"\n    ATTRS = lambda tag, name, val: name == \"title\"\n    TAGS = [\"a\"]\n\n    text = '<a href=\"/foo\" title=\"blah\">example</a>'\n    assert clean(text, tags=TAGS, attributes=ATTRS) == '<a title=\"blah\">example</a>'\n\n\ndef test_attributes_wildcard():\n    \"\"\"Verify attributes[*] works\"\"\"\n    ATTRS = {\n        \"*\": [\"id\"],\n        \"img\": [\"src\"],\n    }\n    TAGS = [\"img\", \"em\"]\n\n    text = (\n        'both <em id=\"foo\" style=\"color: black\">can</em> have <img id=\"bar\" src=\"foo\"/>'\n    )\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS)\n        == 'both <em id=\"foo\">can</em> have <img id=\"bar\" src=\"foo\">'\n    )\n\n\ndef test_attributes_wildcard_callable():\n    \"\"\"Verify attributes[*] callable works\"\"\"\n    ATTRS = {\"*\": lambda tag, name, val: name == \"title\"}\n    TAGS = [\"a\"]\n\n    assert (\n        clean('<a href=\"/foo\" title=\"blah\">example</a>', tags=TAGS, attributes=ATTRS)\n        == '<a title=\"blah\">example</a>'\n    )\n\n\ndef test_attributes_tag_callable():\n    \"\"\"Verify attributes[tag] callable works\"\"\"\n\n    def img_test(tag, name, val):\n        return name == \"src\" and val.startswith(\"https\")\n\n    ATTRS = {\n        \"img\": img_test,\n    }\n    TAGS = [\"img\"]\n\n    text = 'foo <img src=\"http://example.com\" alt=\"blah\"> baz'\n    assert clean(text, tags=TAGS, attributes=ATTRS) == \"foo <img> baz\"\n    text = 'foo <img src=\"https://example.com\" alt=\"blah\"> baz'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS)\n        == 'foo <img src=\"https://example.com\"> baz'\n    )\n\n\ndef test_attributes_tag_list():\n    \"\"\"Verify attributes[tag] list works\"\"\"\n    ATTRS = {\"a\": [\"title\"]}\n    TAGS = [\"a\"]\n\n    assert (\n        clean('<a href=\"/foo\" title=\"blah\">example</a>', tags=TAGS, attributes=ATTRS)\n        == '<a title=\"blah\">example</a>'\n    )\n\n\ndef test_attributes_list():\n    \"\"\"Verify attributes list works\"\"\"\n    ATTRS = [\"title\"]\n    TAGS = [\"a\"]\n\n    text = '<a href=\"/foo\" title=\"blah\">example</a>'\n    assert clean(text, tags=TAGS, attributes=ATTRS) == '<a title=\"blah\">example</a>'\n\n\n@pytest.mark.parametrize(\n    \"data, kwargs, expected\",\n    [\n        # javascript: is not allowed by default\n        (\"<a href=\\\"javascript:alert('XSS')\\\">xss</a>\", {}, \"<a>xss</a>\"),\n        # File protocol is not allowed by default\n        ('<a href=\"file:///tmp/foo\">foo</a>', {}, \"<a>foo</a>\"),\n        # Specified protocols are allowed\n        (\n            '<a href=\"myprotocol://more_text\">allowed href</a>',\n            {\"protocols\": [\"myprotocol\"]},\n            '<a href=\"myprotocol://more_text\">allowed href</a>',\n        ),\n        # Unspecified protocols are not allowed\n        (\n            '<a href=\"http://example.com\">invalid href</a>',\n            {\"protocols\": [\"myprotocol\"]},\n            \"<a>invalid href</a>\",\n        ),\n        # Anchors are ok\n        (\n            '<a href=\"#example.com\">foo</a>',\n            {\"protocols\": []},\n            '<a href=\"#example.com\">foo</a>',\n        ),\n        # Allow implicit http if allowed\n        (\n            '<a href=\"example.com\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"example.com\">valid</a>',\n        ),\n        (\n            '<a href=\"example.com:8000\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"example.com:8000\">valid</a>',\n        ),\n        (\n            '<a href=\"localhost\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"localhost\">valid</a>',\n        ),\n        (\n            '<a href=\"localhost:8000\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"localhost:8000\">valid</a>',\n        ),\n        (\n            '<a href=\"192.168.100.100\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"192.168.100.100\">valid</a>',\n        ),\n        (\n            '<a href=\"192.168.100.100:8000\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"192.168.100.100:8000\">valid</a>',\n        ),\n        # Disallow implicit http if disallowed\n        ('<a href=\"example.com\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        ('<a href=\"example.com:8000\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        ('<a href=\"localhost\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        ('<a href=\"localhost:8000\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        ('<a href=\"192.168.100.100\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        ('<a href=\"192.168.100.100:8000\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        # Disallowed protocols with sneaky character entities\n        ('<a href=\"javas&#x09;cript:alert(1)\">alert</a>', {}, \"<a>alert</a>\"),\n        ('<a href=\"&#14;javascript:alert(1)\">alert</a>', {}, \"<a>alert</a>\"),\n        # Checking the uri should change it at all\n        (\n            '<a href=\"http://example.com/?foo&nbsp;bar\">foo</a>',\n            {},\n            '<a href=\"http://example.com/?foo&nbsp;bar\">foo</a>',\n        ),\n    ],\n)\ndef test_uri_value_allowed_protocols(data, kwargs, expected):\n    assert clean(data, **kwargs) == expected\n\n\ndef test_svg_attr_val_allows_ref():\n    \"\"\"Unescape values in svg attrs that allow url references\"\"\"\n    # Local IRI, so keep it\n    TAGS = [\"svg\", \"rect\"]\n    ATTRS = {\n        \"rect\": [\"fill\"],\n    }\n\n    text = '<svg><rect fill=\"url(#foo)\" /></svg>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS)\n        == '<svg><rect fill=\"url(#foo)\"></rect></svg>'\n    )\n\n    # Non-local IRI, so drop it\n    TAGS = [\"svg\", \"rect\"]\n    ATTRS = {\n        \"rect\": [\"fill\"],\n    }\n    text = '<svg><rect fill=\"url(http://example.com#foo)\" /></svg>'\n    assert clean(text, tags=TAGS, attributes=ATTRS) == \"<svg><rect></rect></svg>\"\n\n\n@pytest.mark.parametrize(\n    \"text, expected\",\n    [\n        (\n            '<svg><pattern id=\"patt1\" href=\"#patt2\"></pattern></svg>',\n            '<svg><pattern href=\"#patt2\" id=\"patt1\"></pattern></svg>',\n        ),\n        (\n            '<svg><pattern id=\"patt1\" xlink:href=\"#patt2\"></pattern></svg>',\n            # NOTE(willkg): Bug in html5lib serializer drops the xlink part\n            '<svg><pattern id=\"patt1\" href=\"#patt2\"></pattern></svg>',\n        ),\n    ],\n)\ndef test_svg_allow_local_href(text, expected):\n    \"\"\"Keep local hrefs for svg elements\"\"\"\n    TAGS = [\"svg\", \"pattern\"]\n    ATTRS = {\n        \"pattern\": [\"id\", \"href\"],\n    }\n    assert clean(text, tags=TAGS, attributes=ATTRS) == expected\n\n\n@pytest.mark.parametrize(\n    \"text, expected\",\n    [\n        (\n            '<svg><pattern id=\"patt1\" href=\"https://example.com/patt\"></pattern></svg>',\n            '<svg><pattern id=\"patt1\"></pattern></svg>',\n        ),\n        (\n            '<svg><pattern id=\"patt1\" xlink:href=\"https://example.com/patt\"></pattern></svg>',\n            '<svg><pattern id=\"patt1\"></pattern></svg>',\n        ),\n    ],\n)\ndef test_svg_allow_local_href_nonlocal(text, expected):\n    \"\"\"Drop non-local hrefs for svg elements\"\"\"\n    TAGS = [\"svg\", \"pattern\"]\n    ATTRS = {\n        \"pattern\": [\"id\", \"href\"],\n    }\n    assert clean(text, tags=TAGS, attributes=ATTRS) == expected\n\n\n@pytest.mark.parametrize(\n    \"data, expected\",\n    [\n        # Convert bell\n        (\"1\\a23\", \"1?23\"),\n        # Convert backpsace\n        (\"1\\b23\", \"1?23\"),\n        # Convert formfeed\n        (\"1\\v23\", \"1?23\"),\n        # Convert vertical tab\n        (\"1\\f23\", \"1?23\"),\n        # Convert a bunch of characters in a string\n        (\"import y\\bose\\bm\\bi\\bt\\be\\b\", \"import y?ose?m?i?t?e?\"),\n    ],\n)\ndef test_invisible_characters(data, expected):\n    assert clean(data) == expected\n\n\ndef test_nonexistent_namespace():\n    # Issue #352 involved this string kicking up a KeyError since the \"c\"\n    # namespace didn't exist. After the fixes for Bleach 3.0, this no longer\n    # goes through the HTML parser as a tag, so it doesn't tickle the bad\n    # namespace code.\n    assert clean(\"<d {c}>\") == \"&lt;d {c}&gt;\"\n\n\n@pytest.mark.parametrize(\n    \"tag\",\n    [\n        \"area\",\n        \"base\",\n        \"br\",\n        \"embed\",\n        \"hr\",\n        \"img\",\n        \"input\",\n        pytest.param(\n            \"keygen\",\n            marks=pytest.mark.xfail(\n                reason=\"https://github.com/mozilla/bleach/issues/488\"\n            ),\n        ),\n        \"link\",\n        \"meta\",\n        \"param\",\n        \"source\",\n        pytest.param(\n            \"menuitem\",\n            marks=pytest.mark.xfail(\n                reason=\"https://github.com/mozilla/bleach/issues/488\"\n            ),\n        ),\n        \"track\",\n        pytest.param(\n            \"wbr\",\n            marks=pytest.mark.xfail(\n                reason=\"https://github.com/mozilla/bleach/issues/488\"\n            ),\n        ),\n    ],\n)\ndef test_self_closing_tags_self_close(tag):\n    assert clean(\"<%s>\" % tag, tags=[tag]) == \"<%s>\" % tag\n\n\n# tags that get content passed through (i.e. parsed with parseRCDataRawtext)\n_raw_tags = [\n    \"title\",\n    \"textarea\",\n    \"script\",\n    \"style\",\n    \"noembed\",\n    \"noframes\",\n    \"iframe\",\n    \"xmp\",\n]\n\n\n@pytest.mark.parametrize(\n    \"raw_tag, data, expected\",\n    [\n        (\n            raw_tag,\n            \"<noscript><%s></noscript><img src=x onerror=alert(1) />\" % raw_tag,\n            \"<noscript>&lt;%s&gt;</noscript>&lt;img src=x onerror=alert(1) /&gt;\"\n            % raw_tag,\n        )\n        for raw_tag in _raw_tags\n    ],\n)\ndef test_noscript_rawtag_(raw_tag, data, expected):\n    # refs: bug 1615315 / GHSA-q65m-pv3f-wr5r\n    assert clean(data, tags=[\"noscript\", raw_tag]) == expected\n\n\n@pytest.mark.parametrize(\n    \"namespace_tag, rc_data_element_tag, data, expected\",\n    [\n        (\n            namespace_tag,\n            rc_data_element_tag,\n            \"<%s><%s><img src=x onerror=alert(1)>\"\n            % (namespace_tag, rc_data_element_tag),\n            \"<%s><%s>&lt;img src=x onerror=alert(1)&gt;</%s></%s>\"\n            % (namespace_tag, rc_data_element_tag, rc_data_element_tag, namespace_tag),\n        )\n        for namespace_tag in [\"math\", \"svg\"]\n        # https://dev.w3.org/html5/html-author/#rcdata-elements\n        # https://html.spec.whatwg.org/index.html#parsing-html-fragments\n        # in html5lib: 'style', 'script', 'xmp', 'iframe', 'noembed', 'noframes', and 'noscript'\n        for rc_data_element_tag in rcdataElements\n    ],\n)\ndef test_namespace_rc_data_element_strip_false(\n    namespace_tag, rc_data_element_tag, data, expected\n):\n    # refs: bug 1621692 / GHSA-m6xf-fq7q-8743\n    #\n    # browsers will pull the img out of the namespace and rc data tag resulting in XSS\n    assert (\n        clean(data, tags=[namespace_tag, rc_data_element_tag], strip=False) == expected\n    )\n\n\n@pytest.mark.parametrize(\n    \"namespace_tag, end_tag, data, expected\",\n    [\n        (\n            \"math\",\n            \"p\",\n            \"<math></p><style><!--</style><img src/onerror=alert(1)>\",\n            \"<math><p></p><style><!--&lt;/style&gt;&lt;img src/onerror=alert(1)&gt;--></style></math>\",\n        ),\n        (\n            \"math\",\n            \"br\",\n            \"<math></br><style><!--</style><img src/onerror=alert(1)>\",\n            \"<math><br><style><!--&lt;/style&gt;&lt;img src/onerror=alert(1)&gt;--></style></math>\",\n        ),\n        (\n            \"svg\",\n            \"p\",\n            \"<svg></p><style><!--</style><img src/onerror=alert(1)>\",\n            \"<svg><p></p><style><!--&lt;/style&gt;&lt;img src/onerror=alert(1)&gt;--></style></svg>\",\n        ),\n        (\n            \"svg\",\n            \"br\",\n            \"<svg></br><style><!--</style><img src/onerror=alert(1)>\",\n            \"<svg><br><style><!--&lt;/style&gt;&lt;img src/onerror=alert(1)&gt;--></style></svg>\",\n        ),\n    ],\n)\ndef test_html_comments_escaped(namespace_tag, end_tag, data, expected):\n    # refs: bug 1689399 / GHSA-vv2x-vrpj-qqpq\n    #\n    # p and br can be just an end tag (e.g. </p> == <p></p>)\n    #\n    # In browsers:\n    #\n    # * img and other tags break out of the svg or math namespace (e.g. <svg><img></svg> == <svg><img></svg>)\n    # * style does not (e.g. <svg><style></svg> == <svg><style></style></svg>)\n    # * the breaking tag ejects trailing elements (e.g. <svg><img><style></style></svg> == <svg></svg><img><style></style>)\n    #\n    # the ejected elements can trigger XSS\n    assert (\n        clean(data, tags=[namespace_tag, end_tag, \"style\"], strip_comments=False)\n        == expected\n    )\n\n\ndef get_ids_and_tests():\n    \"\"\"Retrieves regression tests from data/ directory\n\n    :returns: list of ``(id, filedata)`` tuples\n\n    \"\"\"\n    datadir = os.path.join(os.path.dirname(__file__), \"data\")\n    tests = [\n        os.path.join(datadir, fn) for fn in os.listdir(datadir) if fn.endswith(\".test\")\n    ]\n    # Sort numerically which makes it easier to iterate through them\n    tests.sort(key=lambda x: int(os.path.basename(x).split(\".\", 1)[0]))\n\n    testcases = []\n    for fn in tests:\n        with open(fn) as fp:\n            data = fp.read()\n        testcases.append((os.path.basename(fn), data))\n\n    return testcases\n\n\n_regression_ids_and_tests = get_ids_and_tests()\n_regression_ids = [item[0] for item in _regression_ids_and_tests]\n_regression_tests = [item[1] for item in _regression_ids_and_tests]\n\n\n@pytest.mark.parametrize(\"test_case\", _regression_tests, ids=_regression_ids)\ndef test_regressions(test_case):\n    \"\"\"Regression tests for clean so we can see if there are issues\"\"\"\n    test_data, expected = test_case.split(\"\\n--\\n\")\n\n    # NOTE(willkg): This strips input and expected which makes it easier to\n    # maintain the files. If there comes a time when the input needs whitespace\n    # at the beginning or end, then we'll have to figure out something else.\n    test_data = test_data.strip()\n    expected = expected.strip()\n\n    assert clean(test_data) == expected\n\n\nclass TestCleaner:\n    def test_basics(self):\n        TAGS = [\"span\", \"br\"]\n        ATTRS = {\"span\": [\"style\"]}\n\n        cleaner = Cleaner(tags=TAGS, attributes=ATTRS)\n\n        assert (\n            cleaner.clean('a <br/><span style=\"color:red\">test</span>')\n            == 'a <br><span style=\"\">test</span>'\n        )\n\n    def test_filters(self):\n        # Create a Filter that changes all the attr values to \"moo\"\n        class MooFilter(Filter):\n            def __iter__(self):\n                for token in Filter.__iter__(self):\n                    if token[\"type\"] in [\"StartTag\", \"EmptyTag\"] and token[\"data\"]:\n                        for attr, value in token[\"data\"].items():\n                            token[\"data\"][attr] = \"moo\"\n\n                    yield token\n\n        ATTRS = {\"img\": [\"rel\", \"src\"]}\n        TAGS = [\"img\"]\n\n        cleaner = Cleaner(tags=TAGS, attributes=ATTRS, filters=[MooFilter])\n\n        dirty = 'this is cute! <img src=\"http://example.com/puppy.jpg\" rel=\"nofollow\">'\n        assert cleaner.clean(dirty) == 'this is cute! <img rel=\"moo\" src=\"moo\">'\n", "code_before": "from __future__ import unicode_literals\n\nimport os\n\nimport pytest\n\nfrom bleach import clean\nfrom bleach.html5lib_shim import Filter\nfrom bleach.sanitizer import Cleaner\nfrom bleach._vendor.html5lib.constants import rcdataElements\n\n\ndef test_clean_idempotent():\n    \"\"\"Make sure that applying the filter twice doesn't change anything.\"\"\"\n    dirty = \"<span>invalid & </span> < extra http://link.com<em>\"\n    assert clean(clean(dirty)) == clean(dirty)\n\n\ndef test_only_text_is_cleaned():\n    some_text = \"text\"\n    some_type = int\n    no_type = None\n\n    assert clean(some_text) == some_text\n\n    with pytest.raises(TypeError) as e:\n        clean(some_type)\n    assert \"argument cannot be of 'type' type\" in str(e.value)\n\n    with pytest.raises(TypeError) as e:\n        clean(no_type)\n    assert \"NoneType\" in str(e.value)\n\n\ndef test_empty():\n    assert clean(\"\") == \"\"\n\n\ndef test_content_has_no_html():\n    assert clean(\"no html string\") == \"no html string\"\n\n\n@pytest.mark.parametrize(\n    \"data, expected\",\n    [\n        (\"an <strong>allowed</strong> tag\", \"an <strong>allowed</strong> tag\"),\n        (\"another <em>good</em> tag\", \"another <em>good</em> tag\"),\n    ],\n)\ndef test_content_has_allowed_html(data, expected):\n    assert clean(data) == expected\n\n\ndef test_html_is_lowercased():\n    assert (\n        clean('<A HREF=\"http://example.com\">foo</A>')\n        == '<a href=\"http://example.com\">foo</a>'\n    )\n\n\ndef test_invalid_uri_does_not_raise_error():\n    assert clean('<a href=\"http://example.com]\">text</a>') == \"<a>text</a>\"\n\n\n@pytest.mark.parametrize(\n    \"data, should_strip, expected\",\n    [\n        # Regular comment\n        (\"<!-- this is a comment -->\", True, \"\"),\n        # Open comment with no close comment bit\n        (\"<!-- open comment\", True, \"\"),\n        (\"<!--open comment\", True, \"\"),\n        (\"<!-- open comment\", False, \"<!-- open comment-->\"),\n        (\"<!--open comment\", False, \"<!--open comment-->\"),\n        # Comment with text to the right\n        (\"<!-- comment -->text\", True, \"text\"),\n        (\"<!--comment-->text\", True, \"text\"),\n        (\"<!-- comment -->text\", False, \"<!-- comment -->text\"),\n        (\"<!--comment-->text\", False, \"<!--comment-->text\"),\n        # Comment with text to the left\n        (\"text<!-- comment -->\", True, \"text\"),\n        (\"text<!--comment-->\", True, \"text\"),\n        (\"text<!-- comment -->\", False, \"text<!-- comment -->\"),\n        (\"text<!--comment-->\", False, \"text<!--comment-->\"),\n    ],\n)\ndef test_comments(data, should_strip, expected):\n    assert clean(data, strip_comments=should_strip) == expected\n\n\ndef test_invalid_char_in_tag():\n    assert (\n        clean('<script/xss src=\"http://xx.com/xss.js\"></script>')\n        == '&lt;script/xss src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n    )\n    assert (\n        clean('<script/src=\"http://xx.com/xss.js\"></script>')\n        == '&lt;script/src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n    )\n\n\ndef test_unclosed_tag():\n    assert clean(\"a <em>fixed tag\") == \"a <em>fixed tag</em>\"\n    assert (\n        clean(\"<script src=http://xx.com/xss.js<b>\")\n        == \"&lt;script src=http://xx.com/xss.js&lt;b&gt;\"\n    )\n    assert (\n        clean('<script src=\"http://xx.com/xss.js\"<b>')\n        == '&lt;script src=\"http://xx.com/xss.js\"&lt;b&gt;'\n    )\n    assert (\n        clean('<script src=\"http://xx.com/xss.js\" <b>')\n        == '&lt;script src=\"http://xx.com/xss.js\" &lt;b&gt;'\n    )\n\n\ndef test_nested_script_tag():\n    assert (\n        clean(\"<<script>script>evil()<</script>/script>\")\n        == \"&lt;&lt;script&gt;script&gt;evil()&lt;&lt;/script&gt;/script&gt;\"\n    )\n    assert (\n        clean(\"<<x>script>evil()<</x>/script>\")\n        == \"&lt;&lt;x&gt;script&gt;evil()&lt;&lt;/x&gt;/script&gt;\"\n    )\n    assert (\n        clean(\"<script<script>>evil()</script</script>>\")\n        == \"&lt;script&lt;script&gt;&gt;evil()&lt;/script&lt;/script&gt;&gt;\"\n    )\n\n\n@pytest.mark.parametrize(\n    \"text, expected\",\n    [\n        (\"an & entity\", \"an &amp; entity\"),\n        (\"an < entity\", \"an &lt; entity\"),\n        (\"tag < <em>and</em> entity\", \"tag &lt; <em>and</em> entity\"),\n    ],\n)\ndef test_bare_entities_get_escaped_correctly(text, expected):\n    assert clean(text) == expected\n\n\n@pytest.mark.parametrize(\n    \"text, expected\",\n    [\n        # Test character entities\n        (\"&amp;\", \"&amp;\"),\n        (\"&nbsp;\", \"&nbsp;\"),\n        (\"&nbsp; test string &nbsp;\", \"&nbsp; test string &nbsp;\"),\n        (\"&lt;em&gt;strong&lt;/em&gt;\", \"&lt;em&gt;strong&lt;/em&gt;\"),\n        # Test character entity at beginning of string\n        (\"&amp;is cool\", \"&amp;is cool\"),\n        # Test it at the end of the string\n        (\"cool &amp;\", \"cool &amp;\"),\n        # Test bare ampersands and entities at beginning\n        (\"&&amp; is cool\", \"&amp;&amp; is cool\"),\n        # Test entities and bare ampersand at end\n        (\"&amp; is cool &amp;&\", \"&amp; is cool &amp;&amp;\"),\n        # Test missing semi-colon means we don't treat it like an entity\n        (\"this &amp that\", \"this &amp;amp that\"),\n        # Test a thing that looks like a character entity, but isn't because it's\n        # missing a ; (&current)\n        (\n            \"http://example.com?active=true&current=true\",\n            \"http://example.com?active=true&amp;current=true\",\n        ),\n        # Test character entities in attribute values are left alone\n        ('<a href=\"?art&amp;copy\">foo</a>', '<a href=\"?art&amp;copy\">foo</a>'),\n        ('<a href=\"?this=&gt;that\">foo</a>', '<a href=\"?this=&gt;that\">foo</a>'),\n        # Ambiguous ampersands get escaped in attributes\n        (\n            '<a href=\"http://example.com/&xx;\">foo</a>',\n            '<a href=\"http://example.com/&amp;xx;\">foo</a>',\n        ),\n        (\n            '<a href=\"http://example.com?active=true&current=true\">foo</a>',\n            '<a href=\"http://example.com?active=true&amp;current=true\">foo</a>',\n        ),\n        # Ambiguous ampersands in text are not escaped\n        (\"&xx;\", \"&xx;\"),\n        # Test numeric entities\n        (\"&#39;\", \"&#39;\"),\n        (\"&#34;\", \"&#34;\"),\n        (\"&#123;\", \"&#123;\"),\n        (\"&#x0007b;\", \"&#x0007b;\"),\n        (\"&#x0007B;\", \"&#x0007B;\"),\n        # Test non-numeric entities\n        (\"&#\", \"&amp;#\"),\n        (\"&#<\", \"&amp;#&lt;\"),\n        # html5lib tokenizer unescapes character entities, so these would become '\n        # and \" which makes it possible to break out of html attributes.\n        #\n        # Verify that clean() doesn't unescape entities.\n        (\"&#39;&#34;\", \"&#39;&#34;\"),\n    ],\n)\ndef test_character_entities_handling(text, expected):\n    assert clean(text) == expected\n\n\n@pytest.mark.parametrize(\n    \"data, kwargs, expected\",\n    [\n        # All tags are allowed, so it strips nothing\n        (\n            \"a test <em>with</em> <b>html</b> tags\",\n            {},\n            \"a test <em>with</em> <b>html</b> tags\",\n        ),\n        # img tag is disallowed, so it's stripped\n        (\n            'a test <em>with</em> <img src=\"http://example.com/\"> <b>html</b> tags',\n            {},\n            \"a test <em>with</em>  <b>html</b> tags\",\n        ),\n        # a tag is disallowed, so it's stripped\n        (\n            '<p><a href=\"http://example.com/\">link text</a></p>',\n            {\"tags\": [\"p\"]},\n            \"<p>link text</p>\",\n        ),\n        # Test nested disallowed tag\n        (\n            \"<p><span>multiply <span>nested <span>text</span></span></span></p>\",\n            {\"tags\": [\"p\"]},\n            \"<p>multiply nested text</p>\",\n        ),\n        # (#271)\n        (\"<ul><li><script></li></ul>\", {\"tags\": [\"ul\", \"li\"]}, \"<ul><li></li></ul>\"),\n        # Test disallowed tag that's deep in the tree\n        (\n            '<p><a href=\"http://example.com/\"><img src=\"http://example.com/\"></a></p>',\n            {\"tags\": [\"a\", \"p\"]},\n            '<p><a href=\"http://example.com/\"></a></p>',\n        ),\n        # Test isindex -- the parser expands this to a prompt (#279)\n        (\"<isindex>\", {}, \"\"),\n        # Test non-tags that are well-formed HTML (#280)\n        (\"Yeah right <sarcasm/>\", {}, \"Yeah right \"),\n        (\"<sarcasm>\", {}, \"\"),\n        (\"</sarcasm>\", {}, \"\"),\n        # These are non-tags, but also \"malformed\" so they don't get treated like\n        # tags and stripped\n        (\"</ sarcasm>\", {}, \"&lt;/ sarcasm&gt;\"),\n        (\"</ sarcasm >\", {}, \"&lt;/ sarcasm &gt;\"),\n        (\"Foo <bar@example.com>\", {}, \"Foo \"),\n        (\"Favorite movie: <name of movie>\", {}, \"Favorite movie: \"),\n        (\"</3\", {}, \"&lt;/3\"),\n    ],\n)\ndef test_stripping_tags(data, kwargs, expected):\n    assert clean(data, strip=True, **kwargs) == expected\n    assert clean(\"  \" + data + \"  \", strip=True, **kwargs) == \"  \" + expected + \"  \"\n    assert (\n        clean(\"abc \" + data + \" def\", strip=True, **kwargs)\n        == \"abc \" + expected + \" def\"\n    )\n\n\n@pytest.mark.parametrize(\n    \"data, expected\",\n    [\n        # Disallowed tag is escaped\n        (\n            \"<img src=\\\"javascript:alert('XSS');\\\">\",\n            \"&lt;img src=\\\"javascript:alert('XSS');\\\"&gt;\",\n        ),\n        # Test with parens\n        (\"<script>safe()</script>\", \"&lt;script&gt;safe()&lt;/script&gt;\"),\n        # Test with braces\n        (\"<style>body{}</style>\", \"&lt;style&gt;body{}&lt;/style&gt;\"),\n        # Test nested disallow tags (#271)\n        (\"<ul><li><script></li></ul>\", \"<ul><li>&lt;script&gt;</li></ul>\"),\n        # Test isindex -- the parser expands this to a prompt (#279)\n        (\"<isindex>\", \"&lt;isindex&gt;\"),\n        # Test non-tags (#280)\n        (\"<sarcasm/>\", \"&lt;sarcasm/&gt;\"),\n        (\"<sarcasm>\", \"&lt;sarcasm&gt;\"),\n        (\"</sarcasm>\", \"&lt;/sarcasm&gt;\"),\n        (\"</ sarcasm>\", \"&lt;/ sarcasm&gt;\"),\n        (\"</ sarcasm >\", \"&lt;/ sarcasm &gt;\"),\n        (\"</3\", \"&lt;/3\"),\n        (\"<bar@example.com>\", \"&lt;bar@example.com&gt;\"),\n        (\"Favorite movie: <name of movie>\", \"Favorite movie: &lt;name of movie&gt;\"),\n    ],\n)\ndef test_escaping_tags(data, expected):\n    assert clean(data, strip=False) == expected\n    assert clean(\"  \" + data + \"  \", strip=False) == \"  \" + expected + \"  \"\n    assert clean(\"abc \" + data + \" def\", strip=False) == \"abc \" + expected + \" def\"\n\n\n@pytest.mark.parametrize(\n    \"data, expected\",\n    [\n        (\"<scri<script>pt>alert(1)</scr</script>ipt>\", \"pt&gt;alert(1)ipt&gt;\"),\n        (\"<scri<scri<script>pt>pt>alert(1)</script>\", \"pt&gt;pt&gt;alert(1)\"),\n    ],\n)\ndef test_stripping_tags_is_safe(data, expected):\n    \"\"\"Test stripping tags shouldn't result in malicious content\"\"\"\n    assert clean(data, strip=True) == expected\n\n\ndef test_allowed_styles():\n    \"\"\"Test allowed styles\"\"\"\n    ATTRS = [\"style\"]\n    STYLE = [\"color\"]\n\n    assert clean('<b style=\"top:0\"></b>', attributes=ATTRS) == '<b style=\"\"></b>'\n\n    text = '<b style=\"color: blue;\"></b>'\n    assert clean(text, attributes=ATTRS, styles=STYLE) == text\n\n    text = '<b style=\"top: 0; color: blue;\"></b>'\n    assert clean(text, attributes=ATTRS, styles=STYLE) == '<b style=\"color: blue;\"></b>'\n\n\ndef test_href_with_wrong_tag():\n    assert clean('<em href=\"fail\">no link</em>') == \"<em>no link</em>\"\n\n\ndef test_disallowed_attr():\n    IMG = [\n        \"img\",\n    ]\n    IMG_ATTR = [\"src\"]\n\n    assert clean('<a onclick=\"evil\" href=\"test\">test</a>') == '<a href=\"test\">test</a>'\n    assert (\n        clean('<img onclick=\"evil\" src=\"test\" />', tags=IMG, attributes=IMG_ATTR)\n        == '<img src=\"test\">'\n    )\n    assert (\n        clean('<img href=\"invalid\" src=\"test\" />', tags=IMG, attributes=IMG_ATTR)\n        == '<img src=\"test\">'\n    )\n\n\ndef test_unquoted_attr_values_are_quoted():\n    assert (\n        clean(\"<abbr title=mytitle>myabbr</abbr>\")\n        == '<abbr title=\"mytitle\">myabbr</abbr>'\n    )\n\n\ndef test_unquoted_event_handler_attr_value():\n    assert (\n        clean('<a href=\"http://xx.com\" onclick=foo()>xx.com</a>')\n        == '<a href=\"http://xx.com\">xx.com</a>'\n    )\n\n\ndef test_invalid_filter_attr():\n    IMG = [\n        \"img\",\n    ]\n    IMG_ATTR = {\n        \"img\": lambda tag, name, val: name == \"src\" and val == \"http://example.com/\"\n    }\n\n    assert (\n        clean(\n            '<img onclick=\"evil\" src=\"http://example.com/\" />',\n            tags=IMG,\n            attributes=IMG_ATTR,\n        )\n        == '<img src=\"http://example.com/\">'\n    )\n    assert (\n        clean(\n            '<img onclick=\"evil\" src=\"http://badhost.com/\" />',\n            tags=IMG,\n            attributes=IMG_ATTR,\n        )\n        == \"<img>\"\n    )\n\n\ndef test_poster_attribute():\n    \"\"\"Poster attributes should not allow javascript.\"\"\"\n    tags = [\"video\"]\n    attrs = {\"video\": [\"poster\"]}\n\n    test = '<video poster=\"javascript:alert(1)\"></video>'\n    assert clean(test, tags=tags, attributes=attrs) == \"<video></video>\"\n\n    ok = '<video poster=\"/foo.png\"></video>'\n    assert clean(ok, tags=tags, attributes=attrs) == ok\n\n\ndef test_attributes_callable():\n    \"\"\"Verify attributes can take a callable\"\"\"\n    ATTRS = lambda tag, name, val: name == \"title\"\n    TAGS = [\"a\"]\n\n    text = '<a href=\"/foo\" title=\"blah\">example</a>'\n    assert clean(text, tags=TAGS, attributes=ATTRS) == '<a title=\"blah\">example</a>'\n\n\ndef test_attributes_wildcard():\n    \"\"\"Verify attributes[*] works\"\"\"\n    ATTRS = {\n        \"*\": [\"id\"],\n        \"img\": [\"src\"],\n    }\n    TAGS = [\"img\", \"em\"]\n\n    text = (\n        'both <em id=\"foo\" style=\"color: black\">can</em> have <img id=\"bar\" src=\"foo\"/>'\n    )\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS)\n        == 'both <em id=\"foo\">can</em> have <img id=\"bar\" src=\"foo\">'\n    )\n\n\ndef test_attributes_wildcard_callable():\n    \"\"\"Verify attributes[*] callable works\"\"\"\n    ATTRS = {\"*\": lambda tag, name, val: name == \"title\"}\n    TAGS = [\"a\"]\n\n    assert (\n        clean('<a href=\"/foo\" title=\"blah\">example</a>', tags=TAGS, attributes=ATTRS)\n        == '<a title=\"blah\">example</a>'\n    )\n\n\ndef test_attributes_tag_callable():\n    \"\"\"Verify attributes[tag] callable works\"\"\"\n\n    def img_test(tag, name, val):\n        return name == \"src\" and val.startswith(\"https\")\n\n    ATTRS = {\n        \"img\": img_test,\n    }\n    TAGS = [\"img\"]\n\n    text = 'foo <img src=\"http://example.com\" alt=\"blah\"> baz'\n    assert clean(text, tags=TAGS, attributes=ATTRS) == \"foo <img> baz\"\n    text = 'foo <img src=\"https://example.com\" alt=\"blah\"> baz'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS)\n        == 'foo <img src=\"https://example.com\"> baz'\n    )\n\n\ndef test_attributes_tag_list():\n    \"\"\"Verify attributes[tag] list works\"\"\"\n    ATTRS = {\"a\": [\"title\"]}\n    TAGS = [\"a\"]\n\n    assert (\n        clean('<a href=\"/foo\" title=\"blah\">example</a>', tags=TAGS, attributes=ATTRS)\n        == '<a title=\"blah\">example</a>'\n    )\n\n\ndef test_attributes_list():\n    \"\"\"Verify attributes list works\"\"\"\n    ATTRS = [\"title\"]\n    TAGS = [\"a\"]\n\n    text = '<a href=\"/foo\" title=\"blah\">example</a>'\n    assert clean(text, tags=TAGS, attributes=ATTRS) == '<a title=\"blah\">example</a>'\n\n\n@pytest.mark.parametrize(\n    \"data, kwargs, expected\",\n    [\n        # javascript: is not allowed by default\n        (\"<a href=\\\"javascript:alert('XSS')\\\">xss</a>\", {}, \"<a>xss</a>\"),\n        # File protocol is not allowed by default\n        ('<a href=\"file:///tmp/foo\">foo</a>', {}, \"<a>foo</a>\"),\n        # Specified protocols are allowed\n        (\n            '<a href=\"myprotocol://more_text\">allowed href</a>',\n            {\"protocols\": [\"myprotocol\"]},\n            '<a href=\"myprotocol://more_text\">allowed href</a>',\n        ),\n        # Unspecified protocols are not allowed\n        (\n            '<a href=\"http://example.com\">invalid href</a>',\n            {\"protocols\": [\"myprotocol\"]},\n            \"<a>invalid href</a>\",\n        ),\n        # Anchors are ok\n        (\n            '<a href=\"#example.com\">foo</a>',\n            {\"protocols\": []},\n            '<a href=\"#example.com\">foo</a>',\n        ),\n        # Allow implicit http if allowed\n        (\n            '<a href=\"example.com\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"example.com\">valid</a>',\n        ),\n        (\n            '<a href=\"example.com:8000\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"example.com:8000\">valid</a>',\n        ),\n        (\n            '<a href=\"localhost\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"localhost\">valid</a>',\n        ),\n        (\n            '<a href=\"localhost:8000\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"localhost:8000\">valid</a>',\n        ),\n        (\n            '<a href=\"192.168.100.100\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"192.168.100.100\">valid</a>',\n        ),\n        (\n            '<a href=\"192.168.100.100:8000\">valid</a>',\n            {\"protocols\": [\"http\"]},\n            '<a href=\"192.168.100.100:8000\">valid</a>',\n        ),\n        # Disallow implicit http if disallowed\n        ('<a href=\"example.com\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        ('<a href=\"example.com:8000\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        ('<a href=\"localhost\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        ('<a href=\"localhost:8000\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        ('<a href=\"192.168.100.100\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        ('<a href=\"192.168.100.100:8000\">foo</a>', {\"protocols\": []}, \"<a>foo</a>\"),\n        # Disallowed protocols with sneaky character entities\n        ('<a href=\"javas&#x09;cript:alert(1)\">alert</a>', {}, \"<a>alert</a>\"),\n        ('<a href=\"&#14;javascript:alert(1)\">alert</a>', {}, \"<a>alert</a>\"),\n        # Checking the uri should change it at all\n        (\n            '<a href=\"http://example.com/?foo&nbsp;bar\">foo</a>',\n            {},\n            '<a href=\"http://example.com/?foo&nbsp;bar\">foo</a>',\n        ),\n    ],\n)\ndef test_uri_value_allowed_protocols(data, kwargs, expected):\n    assert clean(data, **kwargs) == expected\n\n\ndef test_svg_attr_val_allows_ref():\n    \"\"\"Unescape values in svg attrs that allow url references\"\"\"\n    # Local IRI, so keep it\n    TAGS = [\"svg\", \"rect\"]\n    ATTRS = {\n        \"rect\": [\"fill\"],\n    }\n\n    text = '<svg><rect fill=\"url(#foo)\" /></svg>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS)\n        == '<svg><rect fill=\"url(#foo)\"></rect></svg>'\n    )\n\n    # Non-local IRI, so drop it\n    TAGS = [\"svg\", \"rect\"]\n    ATTRS = {\n        \"rect\": [\"fill\"],\n    }\n    text = '<svg><rect fill=\"url(http://example.com#foo)\" /></svg>'\n    assert clean(text, tags=TAGS, attributes=ATTRS) == \"<svg><rect></rect></svg>\"\n\n\n@pytest.mark.parametrize(\n    \"text, expected\",\n    [\n        (\n            '<svg><pattern id=\"patt1\" href=\"#patt2\"></pattern></svg>',\n            '<svg><pattern href=\"#patt2\" id=\"patt1\"></pattern></svg>',\n        ),\n        (\n            '<svg><pattern id=\"patt1\" xlink:href=\"#patt2\"></pattern></svg>',\n            # NOTE(willkg): Bug in html5lib serializer drops the xlink part\n            '<svg><pattern id=\"patt1\" href=\"#patt2\"></pattern></svg>',\n        ),\n    ],\n)\ndef test_svg_allow_local_href(text, expected):\n    \"\"\"Keep local hrefs for svg elements\"\"\"\n    TAGS = [\"svg\", \"pattern\"]\n    ATTRS = {\n        \"pattern\": [\"id\", \"href\"],\n    }\n    assert clean(text, tags=TAGS, attributes=ATTRS) == expected\n\n\n@pytest.mark.parametrize(\n    \"text, expected\",\n    [\n        (\n            '<svg><pattern id=\"patt1\" href=\"https://example.com/patt\"></pattern></svg>',\n            '<svg><pattern id=\"patt1\"></pattern></svg>',\n        ),\n        (\n            '<svg><pattern id=\"patt1\" xlink:href=\"https://example.com/patt\"></pattern></svg>',\n            '<svg><pattern id=\"patt1\"></pattern></svg>',\n        ),\n    ],\n)\ndef test_svg_allow_local_href_nonlocal(text, expected):\n    \"\"\"Drop non-local hrefs for svg elements\"\"\"\n    TAGS = [\"svg\", \"pattern\"]\n    ATTRS = {\n        \"pattern\": [\"id\", \"href\"],\n    }\n    assert clean(text, tags=TAGS, attributes=ATTRS) == expected\n\n\n@pytest.mark.parametrize(\n    \"data, expected\",\n    [\n        # Convert bell\n        (\"1\\a23\", \"1?23\"),\n        # Convert backpsace\n        (\"1\\b23\", \"1?23\"),\n        # Convert formfeed\n        (\"1\\v23\", \"1?23\"),\n        # Convert vertical tab\n        (\"1\\f23\", \"1?23\"),\n        # Convert a bunch of characters in a string\n        (\"import y\\bose\\bm\\bi\\bt\\be\\b\", \"import y?ose?m?i?t?e?\"),\n    ],\n)\ndef test_invisible_characters(data, expected):\n    assert clean(data) == expected\n\n\ndef test_nonexistent_namespace():\n    # Issue #352 involved this string kicking up a KeyError since the \"c\"\n    # namespace didn't exist. After the fixes for Bleach 3.0, this no longer\n    # goes through the HTML parser as a tag, so it doesn't tickle the bad\n    # namespace code.\n    assert clean(\"<d {c}>\") == \"&lt;d {c}&gt;\"\n\n\n@pytest.mark.parametrize(\n    \"tag\",\n    [\n        \"area\",\n        \"base\",\n        \"br\",\n        \"embed\",\n        \"hr\",\n        \"img\",\n        \"input\",\n        pytest.param(\n            \"keygen\",\n            marks=pytest.mark.xfail(\n                reason=\"https://github.com/mozilla/bleach/issues/488\"\n            ),\n        ),\n        \"link\",\n        \"meta\",\n        \"param\",\n        \"source\",\n        pytest.param(\n            \"menuitem\",\n            marks=pytest.mark.xfail(\n                reason=\"https://github.com/mozilla/bleach/issues/488\"\n            ),\n        ),\n        \"track\",\n        pytest.param(\n            \"wbr\",\n            marks=pytest.mark.xfail(\n                reason=\"https://github.com/mozilla/bleach/issues/488\"\n            ),\n        ),\n    ],\n)\ndef test_self_closing_tags_self_close(tag):\n    assert clean(\"<%s>\" % tag, tags=[tag]) == \"<%s>\" % tag\n\n\n# tags that get content passed through (i.e. parsed with parseRCDataRawtext)\n_raw_tags = [\n    \"title\",\n    \"textarea\",\n    \"script\",\n    \"style\",\n    \"noembed\",\n    \"noframes\",\n    \"iframe\",\n    \"xmp\",\n]\n\n\n@pytest.mark.parametrize(\n    \"raw_tag, data, expected\",\n    [\n        (\n            raw_tag,\n            \"<noscript><%s></noscript><img src=x onerror=alert(1) />\" % raw_tag,\n            \"<noscript>&lt;%s&gt;</noscript>&lt;img src=x onerror=alert(1) /&gt;\"\n            % raw_tag,\n        )\n        for raw_tag in _raw_tags\n    ],\n)\ndef test_noscript_rawtag_(raw_tag, data, expected):\n    # refs: bug 1615315 / GHSA-q65m-pv3f-wr5r\n    assert clean(data, tags=[\"noscript\", raw_tag]) == expected\n\n\n@pytest.mark.parametrize(\n    \"namespace_tag, rc_data_element_tag, data, expected\",\n    [\n        (\n            namespace_tag,\n            rc_data_element_tag,\n            \"<%s><%s><img src=x onerror=alert(1)>\"\n            % (namespace_tag, rc_data_element_tag),\n            \"<%s><%s>&lt;img src=x onerror=alert(1)&gt;</%s></%s>\"\n            % (namespace_tag, rc_data_element_tag, rc_data_element_tag, namespace_tag),\n        )\n        for namespace_tag in [\"math\", \"svg\"]\n        # https://dev.w3.org/html5/html-author/#rcdata-elements\n        # https://html.spec.whatwg.org/index.html#parsing-html-fragments\n        # in html5lib: 'style', 'script', 'xmp', 'iframe', 'noembed', 'noframes', and 'noscript'\n        for rc_data_element_tag in rcdataElements\n    ],\n)\ndef test_namespace_rc_data_element_strip_false(\n    namespace_tag, rc_data_element_tag, data, expected\n):\n    # refs: bug 1621692 / GHSA-m6xf-fq7q-8743\n    #\n    # browsers will pull the img out of the namespace and rc data tag resulting in XSS\n    assert (\n        clean(data, tags=[namespace_tag, rc_data_element_tag], strip=False) == expected\n    )\n\n\ndef get_ids_and_tests():\n    \"\"\"Retrieves regression tests from data/ directory\n\n    :returns: list of ``(id, filedata)`` tuples\n\n    \"\"\"\n    datadir = os.path.join(os.path.dirname(__file__), \"data\")\n    tests = [\n        os.path.join(datadir, fn) for fn in os.listdir(datadir) if fn.endswith(\".test\")\n    ]\n    # Sort numerically which makes it easier to iterate through them\n    tests.sort(key=lambda x: int(os.path.basename(x).split(\".\", 1)[0]))\n\n    testcases = []\n    for fn in tests:\n        with open(fn) as fp:\n            data = fp.read()\n        testcases.append((os.path.basename(fn), data))\n\n    return testcases\n\n\n_regression_ids_and_tests = get_ids_and_tests()\n_regression_ids = [item[0] for item in _regression_ids_and_tests]\n_regression_tests = [item[1] for item in _regression_ids_and_tests]\n\n\n@pytest.mark.parametrize(\"test_case\", _regression_tests, ids=_regression_ids)\ndef test_regressions(test_case):\n    \"\"\"Regression tests for clean so we can see if there are issues\"\"\"\n    test_data, expected = test_case.split(\"\\n--\\n\")\n\n    # NOTE(willkg): This strips input and expected which makes it easier to\n    # maintain the files. If there comes a time when the input needs whitespace\n    # at the beginning or end, then we'll have to figure out something else.\n    test_data = test_data.strip()\n    expected = expected.strip()\n\n    assert clean(test_data) == expected\n\n\nclass TestCleaner:\n    def test_basics(self):\n        TAGS = [\"span\", \"br\"]\n        ATTRS = {\"span\": [\"style\"]}\n\n        cleaner = Cleaner(tags=TAGS, attributes=ATTRS)\n\n        assert (\n            cleaner.clean('a <br/><span style=\"color:red\">test</span>')\n            == 'a <br><span style=\"\">test</span>'\n        )\n\n    def test_filters(self):\n        # Create a Filter that changes all the attr values to \"moo\"\n        class MooFilter(Filter):\n            def __iter__(self):\n                for token in Filter.__iter__(self):\n                    if token[\"type\"] in [\"StartTag\", \"EmptyTag\"] and token[\"data\"]:\n                        for attr, value in token[\"data\"].items():\n                            token[\"data\"][attr] = \"moo\"\n\n                    yield token\n\n        ATTRS = {\"img\": [\"rel\", \"src\"]}\n        TAGS = [\"img\"]\n\n        cleaner = Cleaner(tags=TAGS, attributes=ATTRS, filters=[MooFilter])\n\n        dirty = 'this is cute! <img src=\"http://example.com/puppy.jpg\" rel=\"nofollow\">'\n        assert cleaner.clean(dirty) == 'this is cute! <img rel=\"moo\" src=\"moo\">'\n", "patch": "@@ -739,6 +739,53 @@ def test_namespace_rc_data_element_strip_false(\n     )\n \n \n+@pytest.mark.parametrize(\n+    \"namespace_tag, end_tag, data, expected\",\n+    [\n+        (\n+            \"math\",\n+            \"p\",\n+            \"<math></p><style><!--</style><img src/onerror=alert(1)>\",\n+            \"<math><p></p><style><!--&lt;/style&gt;&lt;img src/onerror=alert(1)&gt;--></style></math>\",\n+        ),\n+        (\n+            \"math\",\n+            \"br\",\n+            \"<math></br><style><!--</style><img src/onerror=alert(1)>\",\n+            \"<math><br><style><!--&lt;/style&gt;&lt;img src/onerror=alert(1)&gt;--></style></math>\",\n+        ),\n+        (\n+            \"svg\",\n+            \"p\",\n+            \"<svg></p><style><!--</style><img src/onerror=alert(1)>\",\n+            \"<svg><p></p><style><!--&lt;/style&gt;&lt;img src/onerror=alert(1)&gt;--></style></svg>\",\n+        ),\n+        (\n+            \"svg\",\n+            \"br\",\n+            \"<svg></br><style><!--</style><img src/onerror=alert(1)>\",\n+            \"<svg><br><style><!--&lt;/style&gt;&lt;img src/onerror=alert(1)&gt;--></style></svg>\",\n+        ),\n+    ],\n+)\n+def test_html_comments_escaped(namespace_tag, end_tag, data, expected):\n+    # refs: bug 1689399 / GHSA-vv2x-vrpj-qqpq\n+    #\n+    # p and br can be just an end tag (e.g. </p> == <p></p>)\n+    #\n+    # In browsers:\n+    #\n+    # * img and other tags break out of the svg or math namespace (e.g. <svg><img></svg> == <svg><img></svg>)\n+    # * style does not (e.g. <svg><style></svg> == <svg><style></style></svg>)\n+    # * the breaking tag ejects trailing elements (e.g. <svg><img><style></style></svg> == <svg></svg><img><style></style>)\n+    #\n+    # the ejected elements can trigger XSS\n+    assert (\n+        clean(data, tags=[namespace_tag, end_tag, \"style\"], strip_comments=False)\n+        == expected\n+    )\n+\n+\n def get_ids_and_tests():\n     \"\"\"Retrieves regression tests from data/ directory\n ", "file_path": "files/2023_2/762", "file_language": "py", "file_name": "tests/test_clean.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
