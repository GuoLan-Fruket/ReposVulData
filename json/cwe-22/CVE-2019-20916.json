{"index": 4971, "cve_id": "CVE-2019-20916", "cwe_id": ["CWE-22"], "cve_language": "Python", "cve_description": "The pip package before 19.2 for Python allows Directory Traversal when a URL is given in an install command, because a Content-Disposition header can have ../ in a filename, as demonstrated by overwriting the /root/.ssh/authorized_keys file. This occurs in _download_http_url in _internal/download.py.", "cvss": "7.5", "publish_date": "September 4, 2020", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "NONE", "S": "UNCHANGED", "C": "NONE", "I": "HIGH", "A": "NONE", "commit_id": "a4c735b14a62f9cb864533808ac63936704f2ace", "commit_message": "FIX #6413 pip install <url> allow directory traversal", "commit_date": "2019-06-11T07:22:29Z", "project": "gzpan123/pip", "url": "https://api.github.com/repos/gzpan123/pip/commits/a4c735b14a62f9cb864533808ac63936704f2ace", "html_url": "https://github.com/gzpan123/pip/commit/a4c735b14a62f9cb864533808ac63936704f2ace", "windows_before": [{"commit_id": "9c8b2ea759e5c7410eb46e91ec90b6bc9368bb93", "commit_date": "Fri Jun 7 13:55:37 2019 -0700", "commit_message": "Merge pull request #6577 from cjerdonek/issue-5369-download-python-version", "files_name": ["9dc245857978fe778c00f505ab941a1707652e4a - Fri Jun 7 12:45:14 2019 +0530 : Merge pull request #6552 from pypa/rename-auto-lock-label", "8dbf88dff7ffa3a2a81d450549a117f181d073c1 - Thu Jun 6 01:17:18 2019 -0700 : Update pip-download to respect --python-version.", "news/5369.bugfix", "src/pip/_internal/commands/download.py", "src/pip/_internal/index.py", "src/pip/_internal/legacy_resolve.py", "src/pip/_internal/utils/packaging.py", "tests/functional/test_download.py", "tests/unit/test_index.py"]}, {"commit_id": "e6b1070e145a5e95aca5d2c08aa3dce32922742e", "commit_date": "Tue Jun 4 20:28:12 2019 -0700", "commit_message": "Add normalize_version_info() with tests.", "files_name": ["src/pip/_internal/utils/misc.py", "tests/unit/test_utils.py"]}, {"commit_id": "107258da0b23acf44c18377387a1d4b21657ef8d", "commit_date": "Wed Jun 5 06:36:32 2019 +0530", "commit_message": "Document issue tracker triage as a contribution area (#6569)", "files_name": ["docs/html/development/contributing.rst"]}, {"commit_id": "ea864f1defddee60dac56c94b9a6e4d40461dbde", "commit_date": "Mon Jun 3 10:40:21 2019 -0700", "commit_message": "Merge pull request #6567 from cjerdonek/path-to-url-imports", "files_name": ["99228e503a34d2cf158b9af186c3694fa7768882 - Mon Jun 3 02:31:40 2019 -0700 : Import path_to_url() from utils/misc.py instead of download.py.", "src/pip/_internal/cache.py", "src/pip/_internal/index.py", "src/pip/_internal/models/link.py", "src/pip/_internal/req/constructors.py", "src/pip/_internal/wheel.py", "tests/unit/test_download.py", "tests/unit/test_req.py"]}, {"commit_id": "ce46f8524e194ea81c47dbf8a547698f12e61329", "commit_date": "Mon Jun 3 02:10:43 2019 -0700", "commit_message": "Merge pull request #6545 from cjerdonek/vcs-imports", "files_name": ["5c89643d3d675082dbe5cda0e1ef0f8b10cb6b6e - Tue May 28 11:09:00 2019 -0700 : Remove importing from vcs in pip/_internal/__init__.py.", "src/pip/_internal/__init__.py", "src/pip/_internal/download.py", "src/pip/_internal/index.py", "src/pip/_internal/vcs/__init__.py", "src/pip/_internal/vcs/bazaar.py", "src/pip/_internal/vcs/git.py", "src/pip/_internal/vcs/mercurial.py", "src/pip/_internal/vcs/subversion.py", "src/pip/_internal/vcs/versioncontrol.py", "tests/functional/test_install_vcs_svn.py", "tests/unit/test_vcs.py"]}, {"commit_id": "dd720118b70f6a1ebe3f6d4c2f8899a2531eb51b", "commit_date": "Sun Jun 2 21:32:12 2019 +0200", "commit_message": "Disable comment for lock-bot on auto-locking", "files_name": [".github/lock.yml"]}, {"commit_id": "b490eb673e96c2db85040bda9d5e896c173fc46d", "commit_date": "Sat Jun 1 00:14:06 2019 +0500", "commit_message": "Add a link to pip's dev documentation from README (#6556)", "files_name": ["README.rst"]}, {"commit_id": "83d813c01e8debe7fd2bee4af106da06c915d3fc", "commit_date": "Wed May 29 23:03:27 2019 -0400", "commit_message": "Updated comment-matching regex to be Jython-compatible (#5959)", "files_name": ["0cb8f60e50df9de0491f5aa98ac9d3e5ca70ec0e - Tue May 28 03:46:50 2019 -0700 : Merge pull request #6539 from cjerdonek/package-finder-version-info", "798d814629440d73256d12d0e5dc2544310af342 - Sat May 25 11:48:51 2019 -0700 : Change PackageFinder to use Tuple[int, ...] instead of List[str] for --python-version.", "src/pip/_internal/cli/base_command.py", "src/pip/_internal/cli/cmdoptions.py", "src/pip/_internal/commands/download.py", "src/pip/_internal/commands/install.py", "src/pip/_internal/index.py", "src/pip/_internal/pep425tags.py", "tests/unit/test_cmdoptions.py", "tests/unit/test_index.py", "tests/unit/test_pep425tags.py"]}, {"commit_id": "fada348b0085d71da5d3161a868a6fcc41da4bd9", "commit_date": "Sat May 25 09:55:49 2019 -0700", "commit_message": "Make all Option callbacks start with the same prefix.", "files_name": ["src/pip/_internal/cli/cmdoptions.py"]}, {"commit_id": "929fefdbae2ddddfd325204b43434310a8600115", "commit_date": "Tue May 28 10:28:18 2019 +0100", "commit_message": "Add --path to pip freeze to support --target installations (#6450)", "files_name": ["news/6404.feature", "src/pip/_internal/commands/freeze.py", "src/pip/_internal/operations/freeze.py", "src/pip/_internal/utils/misc.py", "tests/functional/test_freeze.py"]}, {"commit_id": "9fdff90ee61f7c461d3fd1c177535ac528e3238d", "commit_date": "Mon May 27 23:32:03 2019 -0400", "commit_message": "Rename the label applied to auto-locked issues", "files_name": [".github/lock.yml"]}, {"commit_id": "d3b9942a2478978f971853e9c906b157786bdb23", "commit_date": "Mon May 27 16:54:01 2019 -0400", "commit_message": "Merge pull request #5458 from pradyunsg/bots", "files_name": ["6599f990962f8566b9093649f34449d41c999b6d - Mon May 27 16:47:06 2019 -0400 : Rename 3662.bugfix to 3662.trivial", "news/3662.trivial"]}, {"commit_id": "24e9e4aa37dfd3ef6a4a027273c56469ccda5f70", "commit_date": "Mon May 27 16:38:17 2019 -0400", "commit_message": "Display install instructions for pre-releases (#5921)", "files_name": ["a1ad4b7c410df4d28f6133118399807427be6f95 - Mon May 27 16:33:45 2019 -0400 : Improve deprecation warnings (#6549)", "5802a63799495a0142158b0b1b36be8158c3b130 - Mon May 27 13:59:48 2019 -0400 : :newspaper:", "news/6549.feature"]}, {"commit_id": "2ff13e4340f263122d429dbf957297b64657222f", "commit_date": "Sun May 26 08:21:51 2019 -0400", "commit_message": "Use a loop instead of multiple similar conditionals", "files_name": ["src/pip/_internal/utils/deprecation.py"]}, {"commit_id": "bf728499beee85947d7c29a94ee8f4983d4a08a0", "commit_date": "Sat May 25 21:56:20 2019 -0400", "commit_message": ":art:", "files_name": ["src/pip/_internal/utils/deprecation.py"]}, {"commit_id": "995ef743ec99b91ff1895511dcc77f1ca722c38f", "commit_date": "Sat May 25 21:52:49 2019 -0400", "commit_message": "Add tests for the deprecated helper", "files_name": ["tests/unit/test_utils.py"]}, {"commit_id": "2d450936c6644a6f3d5d69953571046730c686ae", "commit_date": "Sun May 26 21:53:59 2019 -0700", "commit_message": "Move path_to_url() from download.py to misc.py.", "files_name": ["src/pip/_internal/download.py", "src/pip/_internal/utils/misc.py", "tests/unit/test_download.py", "tests/unit/test_utils.py"]}, {"commit_id": "30855ff9b75251bfa4fcfdde284dfbda7aff5f3a", "commit_date": "Sun May 26 16:17:22 2019 -0700", "commit_message": "Merge pull request #6542 from cjerdonek/issue-5082-missing-metadata-error", "files_name": ["6178f9681ba415064258a246e9010d6b3482665a - Sun May 26 14:10:09 2019 -0700 : Merge pull request #6540 from cjerdonek/issue-6121-incompatible-wheel-message", "cd5bd2cd520c4f834a229d7b3125d4e179fcf806 - Sun May 26 12:29:34 2019 -0700 : Improve error message if METADATA or PKG-INFO metadata is None.", "news/5082.feature", "src/pip/_internal/exceptions.py", "src/pip/_internal/utils/packaging.py", "tests/unit/test_legacy_resolve.py"]}, {"commit_id": "a9a9cfd98cc1a51bdc4016718d9b2554cd391929", "commit_date": "Sat May 25 20:31:41 2019 -0700", "commit_message": "Improve the debug log message when installing an incompatible wheel.", "files_name": ["news/6121.feature", "src/pip/_internal/index.py", "src/pip/_internal/wheel.py", "tests/functional/test_install_config.py", "tests/unit/test_finder.py", "tests/unit/test_index.py", "tests/unit/test_wheel.py"]}, {"commit_id": "2e777204469f4574e7baeb4b515e90f121af76d6", "commit_date": "Tue Jan 29 14:39:02 2019 +0530", "commit_message": "Mention gone_in version in deprecation messages", "files_name": ["src/pip/_internal/utils/deprecation.py"]}, {"commit_id": "f44344f1220bce5cd942641f832cb407ed477f06", "commit_date": "Sat May 25 14:47:51 2019 -0700", "commit_message": "Merge pull request #6515 from johnthagen/svn-interactive-final", "files_name": ["98a77a9317c2e5b5823556a258496fda3a8285e8 - Sat May 25 11:17:23 2019 -0700 : Merge pull request #6538 from cjerdonek/issue-6513-freeze-requirement-error", "c197b118165829e355d553c56fc9837baf9ea080 - Sat May 25 13:36:29 2019 -0400 : Move Resolver to a legacy_resolve module (#6535)", "920b95fe4909906f2ab7230e491e20464e976809 - Sat May 25 13:36:03 2019 -0400 : Add test with non-trivial RevOptions input", "tests/functional/test_install_vcs_svn.py"]}, {"commit_id": "ae03f49bd5098d32b16766464661b37ed99d015b", "commit_date": "Sat May 25 13:31:03 2019 -0400", "commit_message": "Fold obtain and export tests into TestSubversionArgs TestCase", "files_name": ["tests/functional/test_install_vcs_svn.py"]}, {"commit_id": "287aa4b7bf88c7ccc237a68165ce29511e3193fa", "commit_date": "Sat May 25 13:29:49 2019 -0400", "commit_message": "Merge pull request #6008 from jaraco/bugfix/4106-distutils-option-error-target-prefix-conflict", "files_name": ["c90e632f357d2cb8f070dae0cd2ae59ecedec7c0 - Sat May 25 13:23:43 2019 -0400 : Simplify test method names", "tests/functional/test_install_vcs_svn.py"]}, {"commit_id": "8de73a130e136c060cd16d57a506b735e223f7b3", "commit_date": "Sat May 25 13:22:53 2019 -0400", "commit_message": "Use bare strings in test assertion", "files_name": ["tests/functional/test_install_vcs_svn.py"]}, {"commit_id": "d5c4fcc39c7d05833d76f6985372353d6d618fac", "commit_date": "Sat May 25 12:44:45 2019 -0400", "commit_message": "Rename test_{resolve -> legacy_resolve}.py", "files_name": ["tests/unit/test_legacy_resolve.py"]}, {"commit_id": "b05c66722e02bbdbef0aa328bc19b881f309ece3", "commit_date": "Fri May 24 20:30:13 2019 -0400", "commit_message": "Move Resolver to a legacy_resolve module", "files_name": ["src/pip/_internal/commands/download.py", "src/pip/_internal/commands/install.py", "src/pip/_internal/commands/wheel.py", "src/pip/_internal/legacy_resolve.py", "tests/unit/test_req.py", "tests/unit/test_resolve.py"]}, {"commit_id": "77da032c913079a0afe96156cb6a5deb7e480d65", "commit_date": "Sat May 25 12:09:25 2019 -0400", "commit_message": "Make _check_dist_requires_python() parallel _check_link_requires_python(), and add more complete tests (#6528)", "files_name": ["7c42f0ed8e583f5706fece3df98512026fe5be5e - Sat May 25 00:31:44 2019 -0700 : Include more details in a pip freeze warning message.", "news/6513.bugfix", "src/pip/_internal/operations/freeze.py", "tests/functional/test_freeze.py"]}, {"commit_id": "f82ea77217460ab485676fc743119ad4adcb4037", "commit_date": "Thu May 23 22:59:30 2019 -0700", "commit_message": "Simplify the _check_dist_requires_python() call site.", "files_name": ["src/pip/_internal/index.py", "src/pip/_internal/resolve.py", "src/pip/_internal/utils/packaging.py", "tests/functional/test_install.py", "tests/unit/test_resolve.py"]}, {"commit_id": "ba539093754bc96dcdb7f4a48911deffcbcc8725", "commit_date": "Fri May 24 13:57:09 2019 -0700", "commit_message": "Merge pull request #6522 from theacodes/add-pip-ci", "files_name": ["03ad0421ee32e1f99d0d34c6ba445f091eb258a5 - Wed May 22 13:40:01 2019 -0700 : Check for explicit ``PIP_IS_CI`` environment variable to report automated installs to Warehouse.", "news/5499.feature", "src/pip/_internal/download.py", "tests/unit/test_download.py"]}, {"commit_id": "a4aff3b7ffa41b81c7cad564084d84fd9699c932", "commit_date": "Fri May 24 07:32:43 2019 -0400", "commit_message": "Use a TestCase to remove some duplication across similar tests", "files_name": ["tests/functional/test_install_vcs_svn.py"]}, {"commit_id": "cc70cf5ba3e6ba043dd04027aa80c9b8e71455e7", "commit_date": "Thu May 23 00:03:56 2019 -0700", "commit_message": "Move check_dist_requires_python() to resolve.py.", "files_name": ["src/pip/_internal/resolve.py", "src/pip/_internal/utils/packaging.py", "tests/unit/test_resolve.py", "tests/unit/test_utils.py"]}, {"commit_id": "128730f3a124d50ed7e95adc40687043fb90b28d", "commit_date": "Thu May 23 07:50:46 2019 -0400", "commit_message": "Add tests for fetch_new, switch, and update", "files_name": ["tests/functional/test_install_vcs_svn.py"]}, {"commit_id": "eb3db3b032c6b97113a10e6f1e86bc038c49c9ba", "commit_date": "Thu May 23 07:38:22 2019 -0400", "commit_message": "Add type hints to methods to improve documentation and type safety", "files_name": ["src/pip/_internal/vcs/subversion.py"]}, {"commit_id": "b9c83940981fccf6a03200f5f9c8df7b0b650e03", "commit_date": "Thu May 23 07:37:37 2019 -0400", "commit_message": "Fix PEP8 error", "files_name": ["tests/functional/test_install_vcs_svn.py"]}, {"commit_id": "6c5da6bcfa28d4ec61596c2067851875dee69a71", "commit_date": "Thu May 23 07:27:10 2019 -0400", "commit_message": "Remove unnecessary pytest.mark.network from tests that use mocked svn invocation", "files_name": ["tests/functional/test_install_vcs_svn.py"]}, {"commit_id": "034e483f23e6edae46f979090f3bd86ee3bbb71c", "commit_date": "Thu May 23 07:20:31 2019 -0400", "commit_message": "Merge remote-tracking branch 'origin/svn-interactive-final' into svn-interactive-final", "files_name": ["5100f81e4739197a5268c0822594eede9742ae20 - Thu May 23 07:20:25 2019 -0400 : Improve news file", "news/6386.bugfix"]}, {"commit_id": "eeb74aeb2958a7f71e3125811cffcbfca002d243", "commit_date": "Wed May 22 22:38:20 2019 -0700", "commit_message": "Merge pull request #6518 from cjerdonek/issue-6371-ignore-requires-python", "files_name": ["5528d353d21d51db497d6b38ed59f2c7149952c9 - Tue May 21 12:03:37 2019 -0700 : Add some tests of CandidateEvaluator.evaluate_link().", "tests/unit/test_index.py"]}, {"commit_id": "ad2b07898df8d62e9abd642c965f3dc5f38c965e", "commit_date": "Tue May 21 11:10:59 2019 -0700", "commit_message": "Fix pip-install to respect --ignore-requires-python.", "files_name": ["news/6371.bugfix"]}], "windows_after": [], "parents": [{"commit_id_before": "9c8b2ea759e5c7410eb46e91ec90b6bc9368bb93", "url_before": "https://api.github.com/repos/gzpan123/pip/commits/9c8b2ea759e5c7410eb46e91ec90b6bc9368bb93", "html_url_before": "https://github.com/gzpan123/pip/commit/9c8b2ea759e5c7410eb46e91ec90b6bc9368bb93"}], "details": [{"raw_url": "https://github.com/gzpan123/pip/raw/a4c735b14a62f9cb864533808ac63936704f2ace/news%2F6413.bugfix", "code": "Prevent ``pip install <url>`` from permitting directory traversal if e.g.\na malicious server sends a ``Content-Disposition`` header with a filename\ncontaining ``../`` or ``..\\\\``.\n", "code_before": "", "patch": "@@ -0,0 +1,3 @@\n+Prevent ``pip install <url>`` from permitting directory traversal if e.g.\n+a malicious server sends a ``Content-Disposition`` header with a filename\n+containing ``../`` or ``..\\\\``.", "file_path": "files/2020_9/384", "file_language": "bugfix", "file_name": "news/6413.bugfix", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/gzpan123/pip/raw/a4c735b14a62f9cb864533808ac63936704f2ace/src%2Fpip%2F_internal%2Fdownload.py", "code": "from __future__ import absolute_import\n\nimport cgi\nimport email.utils\nimport json\nimport logging\nimport mimetypes\nimport os\nimport platform\nimport re\nimport shutil\nimport sys\n\nfrom pip._vendor import requests, urllib3\nfrom pip._vendor.cachecontrol import CacheControlAdapter\nfrom pip._vendor.cachecontrol.caches import FileCache\nfrom pip._vendor.lockfile import LockError\nfrom pip._vendor.requests.adapters import BaseAdapter, HTTPAdapter\nfrom pip._vendor.requests.auth import AuthBase, HTTPBasicAuth\nfrom pip._vendor.requests.models import CONTENT_CHUNK_SIZE, Response\nfrom pip._vendor.requests.structures import CaseInsensitiveDict\nfrom pip._vendor.requests.utils import get_netrc_auth\n# NOTE: XMLRPC Client is not annotated in typeshed as on 2017-07-17, which is\n#       why we ignore the type on this import\nfrom pip._vendor.six.moves import xmlrpc_client  # type: ignore\nfrom pip._vendor.six.moves.urllib import parse as urllib_parse\nfrom pip._vendor.six.moves.urllib import request as urllib_request\nfrom pip._vendor.urllib3.util import IS_PYOPENSSL\n\nimport pip\nfrom pip._internal.exceptions import HashMismatch, InstallationError\nfrom pip._internal.locations import write_delete_marker_file\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.utils.encoding import auto_decode\nfrom pip._internal.utils.filesystem import check_path_owner\nfrom pip._internal.utils.glibc import libc_ver\nfrom pip._internal.utils.misc import (\n    ARCHIVE_EXTENSIONS, ask, ask_input, ask_password, ask_path_exists,\n    backup_dir, consume, display_path, format_size, get_installed_version,\n    path_to_url, remove_auth_from_url, rmtree, split_auth_netloc_from_url,\n    splitext, unpack_file,\n)\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._internal.utils.typing import MYPY_CHECK_RUNNING\nfrom pip._internal.utils.ui import DownloadProgressProvider\nfrom pip._internal.vcs import vcs\n\nif MYPY_CHECK_RUNNING:\n    from typing import (\n        Optional, Tuple, Dict, IO, Text, Union\n    )\n    from optparse import Values\n    from pip._internal.models.link import Link\n    from pip._internal.utils.hashes import Hashes\n    from pip._internal.vcs.versioncontrol import AuthInfo, VersionControl\n\ntry:\n    import ssl  # noqa\nexcept ImportError:\n    ssl = None\n\n\nHAS_TLS = (ssl is not None) or IS_PYOPENSSL\n\n__all__ = ['get_file_content',\n           'is_url', 'url_to_path', 'path_to_url',\n           'is_archive_file', 'unpack_vcs_link',\n           'unpack_file_url', 'is_vcs_url', 'is_file_url',\n           'unpack_http_url', 'unpack_url',\n           'parse_content_disposition', 'sanitize_content_filename']\n\n\nlogger = logging.getLogger(__name__)\n\n\ntry:\n    import keyring  # noqa\nexcept ImportError:\n    keyring = None\nexcept Exception as exc:\n    logger.warning(\"Keyring is skipped due to an exception: %s\",\n                   str(exc))\n    keyring = None\n\n# These are environment variables present when running under various\n# CI systems.  For each variable, some CI systems that use the variable\n# are indicated.  The collection was chosen so that for each of a number\n# of popular systems, at least one of the environment variables is used.\n# This list is used to provide some indication of and lower bound for\n# CI traffic to PyPI.  Thus, it is okay if the list is not comprehensive.\n# For more background, see: https://github.com/pypa/pip/issues/5499\nCI_ENVIRONMENT_VARIABLES = (\n    # Azure Pipelines\n    'BUILD_BUILDID',\n    # Jenkins\n    'BUILD_ID',\n    # AppVeyor, CircleCI, Codeship, Gitlab CI, Shippable, Travis CI\n    'CI',\n    # Explicit environment variable.\n    'PIP_IS_CI',\n)\n\n\ndef looks_like_ci():\n    # type: () -> bool\n    \"\"\"\n    Return whether it looks like pip is running under CI.\n    \"\"\"\n    # We don't use the method of checking for a tty (e.g. using isatty())\n    # because some CI systems mimic a tty (e.g. Travis CI).  Thus that\n    # method doesn't provide definitive information in either direction.\n    return any(name in os.environ for name in CI_ENVIRONMENT_VARIABLES)\n\n\ndef user_agent():\n    \"\"\"\n    Return a string representing the user agent.\n    \"\"\"\n    data = {\n        \"installer\": {\"name\": \"pip\", \"version\": pip.__version__},\n        \"python\": platform.python_version(),\n        \"implementation\": {\n            \"name\": platform.python_implementation(),\n        },\n    }\n\n    if data[\"implementation\"][\"name\"] == 'CPython':\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'PyPy':\n        if sys.pypy_version_info.releaselevel == 'final':\n            pypy_version_info = sys.pypy_version_info[:3]\n        else:\n            pypy_version_info = sys.pypy_version_info\n        data[\"implementation\"][\"version\"] = \".\".join(\n            [str(x) for x in pypy_version_info]\n        )\n    elif data[\"implementation\"][\"name\"] == 'Jython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'IronPython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n\n    if sys.platform.startswith(\"linux\"):\n        from pip._vendor import distro\n        distro_infos = dict(filter(\n            lambda x: x[1],\n            zip([\"name\", \"version\", \"id\"], distro.linux_distribution()),\n        ))\n        libc = dict(filter(\n            lambda x: x[1],\n            zip([\"lib\", \"version\"], libc_ver()),\n        ))\n        if libc:\n            distro_infos[\"libc\"] = libc\n        if distro_infos:\n            data[\"distro\"] = distro_infos\n\n    if sys.platform.startswith(\"darwin\") and platform.mac_ver()[0]:\n        data[\"distro\"] = {\"name\": \"macOS\", \"version\": platform.mac_ver()[0]}\n\n    if platform.system():\n        data.setdefault(\"system\", {})[\"name\"] = platform.system()\n\n    if platform.release():\n        data.setdefault(\"system\", {})[\"release\"] = platform.release()\n\n    if platform.machine():\n        data[\"cpu\"] = platform.machine()\n\n    if HAS_TLS:\n        data[\"openssl_version\"] = ssl.OPENSSL_VERSION\n\n    setuptools_version = get_installed_version(\"setuptools\")\n    if setuptools_version is not None:\n        data[\"setuptools_version\"] = setuptools_version\n\n    # Use None rather than False so as not to give the impression that\n    # pip knows it is not being run under CI.  Rather, it is a null or\n    # inconclusive result.  Also, we include some value rather than no\n    # value to make it easier to know that the check has been run.\n    data[\"ci\"] = True if looks_like_ci() else None\n\n    user_data = os.environ.get(\"PIP_USER_AGENT_USER_DATA\")\n    if user_data is not None:\n        data[\"user_data\"] = user_data\n\n    return \"{data[installer][name]}/{data[installer][version]} {json}\".format(\n        data=data,\n        json=json.dumps(data, separators=(\",\", \":\"), sort_keys=True),\n    )\n\n\ndef _get_keyring_auth(url, username):\n    \"\"\"Return the tuple auth for a given url from keyring.\"\"\"\n    if not url or not keyring:\n        return None\n\n    try:\n        try:\n            get_credential = keyring.get_credential\n        except AttributeError:\n            pass\n        else:\n            logger.debug(\"Getting credentials from keyring for %s\", url)\n            cred = get_credential(url, username)\n            if cred is not None:\n                return cred.username, cred.password\n            return None\n\n        if username:\n            logger.debug(\"Getting password from keyring for %s\", url)\n            password = keyring.get_password(url, username)\n            if password:\n                return username, password\n\n    except Exception as exc:\n        logger.warning(\"Keyring is skipped due to an exception: %s\",\n                       str(exc))\n\n\nclass MultiDomainBasicAuth(AuthBase):\n\n    def __init__(self, prompting=True, index_urls=None):\n        # type: (bool, Optional[Values]) -> None\n        self.prompting = prompting\n        self.index_urls = index_urls\n        self.passwords = {}  # type: Dict[str, AuthInfo]\n        # When the user is prompted to enter credentials and keyring is\n        # available, we will offer to save them. If the user accepts,\n        # this value is set to the credentials they entered. After the\n        # request authenticates, the caller should call\n        # ``save_credentials`` to save these.\n        self._credentials_to_save = None   # type: Tuple[str, str, str]\n\n    def _get_index_url(self, url):\n        \"\"\"Return the original index URL matching the requested URL.\n\n        Cached or dynamically generated credentials may work against\n        the original index URL rather than just the netloc.\n\n        The provided url should have had its username and password\n        removed already. If the original index url had credentials then\n        they will be included in the return value.\n\n        Returns None if no matching index was found, or if --no-index\n        was specified by the user.\n        \"\"\"\n        if not url or not self.index_urls:\n            return None\n\n        for u in self.index_urls:\n            prefix = remove_auth_from_url(u).rstrip(\"/\") + \"/\"\n            if url.startswith(prefix):\n                return u\n\n    def _get_new_credentials(self, original_url, allow_netrc=True,\n                             allow_keyring=True):\n        \"\"\"Find and return credentials for the specified URL.\"\"\"\n        # Split the credentials and netloc from the url.\n        url, netloc, url_user_password = split_auth_netloc_from_url(\n            original_url)\n\n        # Start with the credentials embedded in the url\n        username, password = url_user_password\n        if username is not None and password is not None:\n            logger.debug(\"Found credentials in url for %s\", netloc)\n            return url_user_password\n\n        # Find a matching index url for this request\n        index_url = self._get_index_url(url)\n        if index_url:\n            # Split the credentials from the url.\n            index_info = split_auth_netloc_from_url(index_url)\n            if index_info:\n                index_url, _, index_url_user_password = index_info\n                logger.debug(\"Found index url %s\", index_url)\n\n        # If an index URL was found, try its embedded credentials\n        if index_url and index_url_user_password[0] is not None:\n            username, password = index_url_user_password\n            if username is not None and password is not None:\n                logger.debug(\"Found credentials in index url for %s\", netloc)\n                return index_url_user_password\n\n        # Get creds from netrc if we still don't have them\n        if allow_netrc:\n            netrc_auth = get_netrc_auth(original_url)\n            if netrc_auth:\n                logger.debug(\"Found credentials in netrc for %s\", netloc)\n                return netrc_auth\n\n        # If we don't have a password and keyring is available, use it.\n        if allow_keyring:\n            # The index url is more specific than the netloc, so try it first\n            kr_auth = (_get_keyring_auth(index_url, username) or\n                       _get_keyring_auth(netloc, username))\n            if kr_auth:\n                logger.debug(\"Found credentials in keyring for %s\", netloc)\n                return kr_auth\n\n        return None, None\n\n    def _get_url_and_credentials(self, original_url):\n        \"\"\"Return the credentials to use for the provided URL.\n\n        If allowed, netrc and keyring may be used to obtain the\n        correct credentials.\n\n        Returns (url_without_credentials, username, password). Note\n        that even if the original URL contains credentials, this\n        function may return a different username and password.\n        \"\"\"\n        url, netloc, _ = split_auth_netloc_from_url(original_url)\n\n        # Use any stored credentials that we have for this netloc\n        username, password = self.passwords.get(netloc, (None, None))\n\n        # If nothing cached, acquire new credentials without prompting\n        # the user (e.g. from netrc, keyring, or similar).\n        if username is None or password is None:\n            username, password = self._get_new_credentials(original_url)\n\n        if username is not None and password is not None:\n            # Store the username and password\n            self.passwords[netloc] = (username, password)\n\n        return url, username, password\n\n    def __call__(self, req):\n        # Get credentials for this request\n        url, username, password = self._get_url_and_credentials(req.url)\n\n        # Set the url of the request to the url without any credentials\n        req.url = url\n\n        if username is not None and password is not None:\n            # Send the basic auth with this request\n            req = HTTPBasicAuth(username, password)(req)\n\n        # Attach a hook to handle 401 responses\n        req.register_hook(\"response\", self.handle_401)\n\n        return req\n\n    # Factored out to allow for easy patching in tests\n    def _prompt_for_password(self, netloc):\n        username = ask_input(\"User for %s: \" % netloc)\n        if not username:\n            return None, None\n        auth = _get_keyring_auth(netloc, username)\n        if auth:\n            return auth[0], auth[1], False\n        password = ask_password(\"Password: \")\n        return username, password, True\n\n    # Factored out to allow for easy patching in tests\n    def _should_save_password_to_keyring(self):\n        if not keyring:\n            return False\n        return ask(\"Save credentials to keyring [y/N]: \", [\"y\", \"n\"]) == \"y\"\n\n    def handle_401(self, resp, **kwargs):\n        # We only care about 401 responses, anything else we want to just\n        #   pass through the actual response\n        if resp.status_code != 401:\n            return resp\n\n        # We are not able to prompt the user so simply return the response\n        if not self.prompting:\n            return resp\n\n        parsed = urllib_parse.urlparse(resp.url)\n\n        # Prompt the user for a new username and password\n        username, password, save = self._prompt_for_password(parsed.netloc)\n\n        # Store the new username and password to use for future requests\n        self._credentials_to_save = None\n        if username is not None and password is not None:\n            self.passwords[parsed.netloc] = (username, password)\n\n            # Prompt to save the password to keyring\n            if save and self._should_save_password_to_keyring():\n                self._credentials_to_save = (parsed.netloc, username, password)\n\n        # Consume content and release the original connection to allow our new\n        #   request to reuse the same one.\n        resp.content\n        resp.raw.release_conn()\n\n        # Add our new username and password to the request\n        req = HTTPBasicAuth(username or \"\", password or \"\")(resp.request)\n        req.register_hook(\"response\", self.warn_on_401)\n\n        # On successful request, save the credentials that were used to\n        # keyring. (Note that if the user responded \"no\" above, this member\n        # is not set and nothing will be saved.)\n        if self._credentials_to_save:\n            req.register_hook(\"response\", self.save_credentials)\n\n        # Send our new request\n        new_resp = resp.connection.send(req, **kwargs)\n        new_resp.history.append(resp)\n\n        return new_resp\n\n    def warn_on_401(self, resp, **kwargs):\n        \"\"\"Response callback to warn about incorrect credentials.\"\"\"\n        if resp.status_code == 401:\n            logger.warning('401 Error, Credentials not correct for %s',\n                           resp.request.url)\n\n    def save_credentials(self, resp, **kwargs):\n        \"\"\"Response callback to save credentials on success.\"\"\"\n        assert keyring is not None, \"should never reach here without keyring\"\n        if not keyring:\n            return\n\n        creds = self._credentials_to_save\n        self._credentials_to_save = None\n        if creds and resp.status_code < 400:\n            try:\n                logger.info('Saving credentials to keyring')\n                keyring.set_password(*creds)\n            except Exception:\n                logger.exception('Failed to save credentials')\n\n\nclass LocalFSAdapter(BaseAdapter):\n\n    def send(self, request, stream=None, timeout=None, verify=None, cert=None,\n             proxies=None):\n        pathname = url_to_path(request.url)\n\n        resp = Response()\n        resp.status_code = 200\n        resp.url = request.url\n\n        try:\n            stats = os.stat(pathname)\n        except OSError as exc:\n            resp.status_code = 404\n            resp.raw = exc\n        else:\n            modified = email.utils.formatdate(stats.st_mtime, usegmt=True)\n            content_type = mimetypes.guess_type(pathname)[0] or \"text/plain\"\n            resp.headers = CaseInsensitiveDict({\n                \"Content-Type\": content_type,\n                \"Content-Length\": stats.st_size,\n                \"Last-Modified\": modified,\n            })\n\n            resp.raw = open(pathname, \"rb\")\n            resp.close = resp.raw.close\n\n        return resp\n\n    def close(self):\n        pass\n\n\nclass SafeFileCache(FileCache):\n    \"\"\"\n    A file based cache which is safe to use even when the target directory may\n    not be accessible or writable.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(SafeFileCache, self).__init__(*args, **kwargs)\n\n        # Check to ensure that the directory containing our cache directory\n        # is owned by the user current executing pip. If it does not exist\n        # we will check the parent directory until we find one that does exist.\n        # If it is not owned by the user executing pip then we will disable\n        # the cache and log a warning.\n        if not check_path_owner(self.directory):\n            logger.warning(\n                \"The directory '%s' or its parent directory is not owned by \"\n                \"the current user and the cache has been disabled. Please \"\n                \"check the permissions and owner of that directory. If \"\n                \"executing pip with sudo, you may want sudo's -H flag.\",\n                self.directory,\n            )\n\n            # Set our directory to None to disable the Cache\n            self.directory = None\n\n    def get(self, *args, **kwargs):\n        # If we don't have a directory, then the cache should be a no-op.\n        if self.directory is None:\n            return\n\n        try:\n            return super(SafeFileCache, self).get(*args, **kwargs)\n        except (LockError, OSError, IOError):\n            # We intentionally silence this error, if we can't access the cache\n            # then we can just skip caching and process the request as if\n            # caching wasn't enabled.\n            pass\n\n    def set(self, *args, **kwargs):\n        # If we don't have a directory, then the cache should be a no-op.\n        if self.directory is None:\n            return\n\n        try:\n            return super(SafeFileCache, self).set(*args, **kwargs)\n        except (LockError, OSError, IOError):\n            # We intentionally silence this error, if we can't access the cache\n            # then we can just skip caching and process the request as if\n            # caching wasn't enabled.\n            pass\n\n    def delete(self, *args, **kwargs):\n        # If we don't have a directory, then the cache should be a no-op.\n        if self.directory is None:\n            return\n\n        try:\n            return super(SafeFileCache, self).delete(*args, **kwargs)\n        except (LockError, OSError, IOError):\n            # We intentionally silence this error, if we can't access the cache\n            # then we can just skip caching and process the request as if\n            # caching wasn't enabled.\n            pass\n\n\nclass InsecureHTTPAdapter(HTTPAdapter):\n\n    def cert_verify(self, conn, url, verify, cert):\n        conn.cert_reqs = 'CERT_NONE'\n        conn.ca_certs = None\n\n\nclass PipSession(requests.Session):\n\n    timeout = None  # type: Optional[int]\n\n    def __init__(self, *args, **kwargs):\n        retries = kwargs.pop(\"retries\", 0)\n        cache = kwargs.pop(\"cache\", None)\n        insecure_hosts = kwargs.pop(\"insecure_hosts\", [])\n        index_urls = kwargs.pop(\"index_urls\", None)\n\n        super(PipSession, self).__init__(*args, **kwargs)\n\n        # Attach our User Agent to the request\n        self.headers[\"User-Agent\"] = user_agent()\n\n        # Attach our Authentication handler to the session\n        self.auth = MultiDomainBasicAuth(index_urls=index_urls)\n\n        # Create our urllib3.Retry instance which will allow us to customize\n        # how we handle retries.\n        retries = urllib3.Retry(\n            # Set the total number of retries that a particular request can\n            # have.\n            total=retries,\n\n            # A 503 error from PyPI typically means that the Fastly -> Origin\n            # connection got interrupted in some way. A 503 error in general\n            # is typically considered a transient error so we'll go ahead and\n            # retry it.\n            # A 500 may indicate transient error in Amazon S3\n            # A 520 or 527 - may indicate transient error in CloudFlare\n            status_forcelist=[500, 503, 520, 527],\n\n            # Add a small amount of back off between failed requests in\n            # order to prevent hammering the service.\n            backoff_factor=0.25,\n        )\n\n        # We want to _only_ cache responses on securely fetched origins. We do\n        # this because we can't validate the response of an insecurely fetched\n        # origin, and we don't want someone to be able to poison the cache and\n        # require manual eviction from the cache to fix it.\n        if cache:\n            secure_adapter = CacheControlAdapter(\n                cache=SafeFileCache(cache, use_dir_lock=True),\n                max_retries=retries,\n            )\n        else:\n            secure_adapter = HTTPAdapter(max_retries=retries)\n\n        # Our Insecure HTTPAdapter disables HTTPS validation. It does not\n        # support caching (see above) so we'll use it for all http:// URLs as\n        # well as any https:// host that we've marked as ignoring TLS errors\n        # for.\n        insecure_adapter = InsecureHTTPAdapter(max_retries=retries)\n\n        self.mount(\"https://\", secure_adapter)\n        self.mount(\"http://\", insecure_adapter)\n\n        # Enable file:// urls\n        self.mount(\"file://\", LocalFSAdapter())\n\n        # We want to use a non-validating adapter for any requests which are\n        # deemed insecure.\n        for host in insecure_hosts:\n            self.mount(\"https://{}/\".format(host), insecure_adapter)\n\n    def request(self, method, url, *args, **kwargs):\n        # Allow setting a default timeout on a session\n        kwargs.setdefault(\"timeout\", self.timeout)\n\n        # Dispatch the actual request\n        return super(PipSession, self).request(method, url, *args, **kwargs)\n\n\ndef get_file_content(url, comes_from=None, session=None):\n    # type: (str, Optional[str], Optional[PipSession]) -> Tuple[str, Text]\n    \"\"\"Gets the content of a file; it may be a filename, file: URL, or\n    http: URL.  Returns (location, content).  Content is unicode.\n\n    :param url:         File path or url.\n    :param comes_from:  Origin description of requirements.\n    :param session:     Instance of pip.download.PipSession.\n    \"\"\"\n    if session is None:\n        raise TypeError(\n            \"get_file_content() missing 1 required keyword argument: 'session'\"\n        )\n\n    match = _scheme_re.search(url)\n    if match:\n        scheme = match.group(1).lower()\n        if (scheme == 'file' and comes_from and\n                comes_from.startswith('http')):\n            raise InstallationError(\n                'Requirements file %s references URL %s, which is local'\n                % (comes_from, url))\n        if scheme == 'file':\n            path = url.split(':', 1)[1]\n            path = path.replace('\\\\', '/')\n            match = _url_slash_drive_re.match(path)\n            if match:\n                path = match.group(1) + ':' + path.split('|', 1)[1]\n            path = urllib_parse.unquote(path)\n            if path.startswith('/'):\n                path = '/' + path.lstrip('/')\n            url = path\n        else:\n            # FIXME: catch some errors\n            resp = session.get(url)\n            resp.raise_for_status()\n            return resp.url, resp.text\n    try:\n        with open(url, 'rb') as f:\n            content = auto_decode(f.read())\n    except IOError as exc:\n        raise InstallationError(\n            'Could not open requirements file: %s' % str(exc)\n        )\n    return url, content\n\n\n_scheme_re = re.compile(r'^(http|https|file):', re.I)\n_url_slash_drive_re = re.compile(r'/*([a-z])\\|', re.I)\n\n\ndef is_url(name):\n    # type: (Union[str, Text]) -> bool\n    \"\"\"Returns true if the name looks like a URL\"\"\"\n    if ':' not in name:\n        return False\n    scheme = name.split(':', 1)[0].lower()\n    return scheme in ['http', 'https', 'file', 'ftp'] + vcs.all_schemes\n\n\ndef url_to_path(url):\n    # type: (str) -> str\n    \"\"\"\n    Convert a file: URL to a path.\n    \"\"\"\n    assert url.startswith('file:'), (\n        \"You can only turn file: urls into filenames (not %r)\" % url)\n\n    _, netloc, path, _, _ = urllib_parse.urlsplit(url)\n\n    if not netloc or netloc == 'localhost':\n        # According to RFC 8089, same as empty authority.\n        netloc = ''\n    elif sys.platform == 'win32':\n        # If we have a UNC path, prepend UNC share notation.\n        netloc = '\\\\\\\\' + netloc\n    else:\n        raise ValueError(\n            'non-local file URIs are not supported on this platform: %r'\n            % url\n        )\n\n    path = urllib_request.url2pathname(netloc + path)\n    return path\n\n\ndef is_archive_file(name):\n    # type: (str) -> bool\n    \"\"\"Return True if `name` is a considered as an archive file.\"\"\"\n    ext = splitext(name)[1].lower()\n    if ext in ARCHIVE_EXTENSIONS:\n        return True\n    return False\n\n\ndef unpack_vcs_link(link, location):\n    vcs_backend = _get_used_vcs_backend(link)\n    vcs_backend.unpack(location, url=link.url)\n\n\ndef _get_used_vcs_backend(link):\n    # type: (Link) -> Optional[VersionControl]\n    \"\"\"\n    Return a VersionControl object or None.\n    \"\"\"\n    for vcs_backend in vcs.backends:\n        if link.scheme in vcs_backend.schemes:\n            return vcs_backend\n    return None\n\n\ndef is_vcs_url(link):\n    # type: (Link) -> bool\n    return bool(_get_used_vcs_backend(link))\n\n\ndef is_file_url(link):\n    # type: (Link) -> bool\n    return link.url.lower().startswith('file:')\n\n\ndef is_dir_url(link):\n    # type: (Link) -> bool\n    \"\"\"Return whether a file:// Link points to a directory.\n\n    ``link`` must not have any other scheme but file://. Call is_file_url()\n    first.\n\n    \"\"\"\n    link_path = url_to_path(link.url_without_fragment)\n    return os.path.isdir(link_path)\n\n\ndef _progress_indicator(iterable, *args, **kwargs):\n    return iterable\n\n\ndef _download_url(\n    resp,  # type: Response\n    link,  # type: Link\n    content_file,  # type: IO\n    hashes,  # type: Hashes\n    progress_bar  # type: str\n):\n    # type: (...) -> None\n    try:\n        total_length = int(resp.headers['content-length'])\n    except (ValueError, KeyError, TypeError):\n        total_length = 0\n\n    cached_resp = getattr(resp, \"from_cache\", False)\n    if logger.getEffectiveLevel() > logging.INFO:\n        show_progress = False\n    elif cached_resp:\n        show_progress = False\n    elif total_length > (40 * 1000):\n        show_progress = True\n    elif not total_length:\n        show_progress = True\n    else:\n        show_progress = False\n\n    show_url = link.show_url\n\n    def resp_read(chunk_size):\n        try:\n            # Special case for urllib3.\n            for chunk in resp.raw.stream(\n                    chunk_size,\n                    # We use decode_content=False here because we don't\n                    # want urllib3 to mess with the raw bytes we get\n                    # from the server. If we decompress inside of\n                    # urllib3 then we cannot verify the checksum\n                    # because the checksum will be of the compressed\n                    # file. This breakage will only occur if the\n                    # server adds a Content-Encoding header, which\n                    # depends on how the server was configured:\n                    # - Some servers will notice that the file isn't a\n                    #   compressible file and will leave the file alone\n                    #   and with an empty Content-Encoding\n                    # - Some servers will notice that the file is\n                    #   already compressed and will leave the file\n                    #   alone and will add a Content-Encoding: gzip\n                    #   header\n                    # - Some servers won't notice anything at all and\n                    #   will take a file that's already been compressed\n                    #   and compress it again and set the\n                    #   Content-Encoding: gzip header\n                    #\n                    # By setting this not to decode automatically we\n                    # hope to eliminate problems with the second case.\n                    decode_content=False):\n                yield chunk\n        except AttributeError:\n            # Standard file-like object.\n            while True:\n                chunk = resp.raw.read(chunk_size)\n                if not chunk:\n                    break\n                yield chunk\n\n    def written_chunks(chunks):\n        for chunk in chunks:\n            content_file.write(chunk)\n            yield chunk\n\n    progress_indicator = _progress_indicator\n\n    if link.netloc == PyPI.netloc:\n        url = show_url\n    else:\n        url = link.url_without_fragment\n\n    if show_progress:  # We don't show progress on cached responses\n        progress_indicator = DownloadProgressProvider(progress_bar,\n                                                      max=total_length)\n        if total_length:\n            logger.info(\"Downloading %s (%s)\", url, format_size(total_length))\n        else:\n            logger.info(\"Downloading %s\", url)\n    elif cached_resp:\n        logger.info(\"Using cached %s\", url)\n    else:\n        logger.info(\"Downloading %s\", url)\n\n    logger.debug('Downloading from URL %s', link)\n\n    downloaded_chunks = written_chunks(\n        progress_indicator(\n            resp_read(CONTENT_CHUNK_SIZE),\n            CONTENT_CHUNK_SIZE\n        )\n    )\n    if hashes:\n        hashes.check_against_chunks(downloaded_chunks)\n    else:\n        consume(downloaded_chunks)\n\n\ndef _copy_file(filename, location, link):\n    copy = True\n    download_location = os.path.join(location, link.filename)\n    if os.path.exists(download_location):\n        response = ask_path_exists(\n            'The file %s exists. (i)gnore, (w)ipe, (b)ackup, (a)abort' %\n            display_path(download_location), ('i', 'w', 'b', 'a'))\n        if response == 'i':\n            copy = False\n        elif response == 'w':\n            logger.warning('Deleting %s', display_path(download_location))\n            os.remove(download_location)\n        elif response == 'b':\n            dest_file = backup_dir(download_location)\n            logger.warning(\n                'Backing up %s to %s',\n                display_path(download_location),\n                display_path(dest_file),\n            )\n            shutil.move(download_location, dest_file)\n        elif response == 'a':\n            sys.exit(-1)\n    if copy:\n        shutil.copy(filename, download_location)\n        logger.info('Saved %s', display_path(download_location))\n\n\ndef unpack_http_url(\n    link,  # type: Link\n    location,  # type: str\n    download_dir=None,  # type: Optional[str]\n    session=None,  # type: Optional[PipSession]\n    hashes=None,  # type: Optional[Hashes]\n    progress_bar=\"on\"  # type: str\n):\n    # type: (...) -> None\n    if session is None:\n        raise TypeError(\n            \"unpack_http_url() missing 1 required keyword argument: 'session'\"\n        )\n\n    with TempDirectory(kind=\"unpack\") as temp_dir:\n        # If a download dir is specified, is the file already downloaded there?\n        already_downloaded_path = None\n        if download_dir:\n            already_downloaded_path = _check_download_dir(link,\n                                                          download_dir,\n                                                          hashes)\n\n        if already_downloaded_path:\n            from_path = already_downloaded_path\n            content_type = mimetypes.guess_type(from_path)[0]\n        else:\n            # let's download to a tmp dir\n            from_path, content_type = _download_http_url(link,\n                                                         session,\n                                                         temp_dir.path,\n                                                         hashes,\n                                                         progress_bar)\n\n        # unpack the archive to the build dir location. even when only\n        # downloading archives, they have to be unpacked to parse dependencies\n        unpack_file(from_path, location, content_type, link)\n\n        # a download dir is specified; let's copy the archive there\n        if download_dir and not already_downloaded_path:\n            _copy_file(from_path, download_dir, link)\n\n        if not already_downloaded_path:\n            os.unlink(from_path)\n\n\ndef unpack_file_url(\n    link,  # type: Link\n    location,  # type: str\n    download_dir=None,  # type: Optional[str]\n    hashes=None  # type: Optional[Hashes]\n):\n    # type: (...) -> None\n    \"\"\"Unpack link into location.\n\n    If download_dir is provided and link points to a file, make a copy\n    of the link file inside download_dir.\n    \"\"\"\n    link_path = url_to_path(link.url_without_fragment)\n\n    # If it's a url to a local directory\n    if is_dir_url(link):\n        if os.path.isdir(location):\n            rmtree(location)\n        shutil.copytree(link_path, location, symlinks=True)\n        if download_dir:\n            logger.info('Link is a directory, ignoring download_dir')\n        return\n\n    # If --require-hashes is off, `hashes` is either empty, the\n    # link's embedded hash, or MissingHashes; it is required to\n    # match. If --require-hashes is on, we are satisfied by any\n    # hash in `hashes` matching: a URL-based or an option-based\n    # one; no internet-sourced hash will be in `hashes`.\n    if hashes:\n        hashes.check_against_path(link_path)\n\n    # If a download dir is specified, is the file already there and valid?\n    already_downloaded_path = None\n    if download_dir:\n        already_downloaded_path = _check_download_dir(link,\n                                                      download_dir,\n                                                      hashes)\n\n    if already_downloaded_path:\n        from_path = already_downloaded_path\n    else:\n        from_path = link_path\n\n    content_type = mimetypes.guess_type(from_path)[0]\n\n    # unpack the archive to the build dir location. even when only downloading\n    # archives, they have to be unpacked to parse dependencies\n    unpack_file(from_path, location, content_type, link)\n\n    # a download dir is specified and not already downloaded\n    if download_dir and not already_downloaded_path:\n        _copy_file(from_path, download_dir, link)\n\n\nclass PipXmlrpcTransport(xmlrpc_client.Transport):\n    \"\"\"Provide a `xmlrpclib.Transport` implementation via a `PipSession`\n    object.\n    \"\"\"\n\n    def __init__(self, index_url, session, use_datetime=False):\n        xmlrpc_client.Transport.__init__(self, use_datetime)\n        index_parts = urllib_parse.urlparse(index_url)\n        self._scheme = index_parts.scheme\n        self._session = session\n\n    def request(self, host, handler, request_body, verbose=False):\n        parts = (self._scheme, host, handler, None, None, None)\n        url = urllib_parse.urlunparse(parts)\n        try:\n            headers = {'Content-Type': 'text/xml'}\n            response = self._session.post(url, data=request_body,\n                                          headers=headers, stream=True)\n            response.raise_for_status()\n            self.verbose = verbose\n            return self.parse_response(response.raw)\n        except requests.HTTPError as exc:\n            logger.critical(\n                \"HTTP error %s while getting %s\",\n                exc.response.status_code, url,\n            )\n            raise\n\n\ndef unpack_url(\n    link,  # type: Optional[Link]\n    location,  # type: Optional[str]\n    download_dir=None,  # type: Optional[str]\n    only_download=False,  # type: bool\n    session=None,  # type: Optional[PipSession]\n    hashes=None,  # type: Optional[Hashes]\n    progress_bar=\"on\"  # type: str\n):\n    # type: (...) -> None\n    \"\"\"Unpack link.\n       If link is a VCS link:\n         if only_download, export into download_dir and ignore location\n          else unpack into location\n       for other types of link:\n         - unpack into location\n         - if download_dir, copy the file into download_dir\n         - if only_download, mark location for deletion\n\n    :param hashes: A Hashes object, one of whose embedded hashes must match,\n        or HashMismatch will be raised. If the Hashes is empty, no matches are\n        required, and unhashable types of requirements (like VCS ones, which\n        would ordinarily raise HashUnsupported) are allowed.\n    \"\"\"\n    # non-editable vcs urls\n    if is_vcs_url(link):\n        unpack_vcs_link(link, location)\n\n    # file urls\n    elif is_file_url(link):\n        unpack_file_url(link, location, download_dir, hashes=hashes)\n\n    # http urls\n    else:\n        if session is None:\n            session = PipSession()\n\n        unpack_http_url(\n            link,\n            location,\n            download_dir,\n            session,\n            hashes=hashes,\n            progress_bar=progress_bar\n        )\n    if only_download:\n        write_delete_marker_file(location)\n\n\ndef sanitize_content_filename(filename):\n    # type: (str) -> str\n    \"\"\"\n    Sanitize the \"filename\" value from a Content-Disposition header.\n    \"\"\"\n    return os.path.basename(filename)\n\n\ndef parse_content_disposition(content_disposition, default_filename):\n    # type: (str, str) -> str\n    \"\"\"\n    Parse the \"filename\" value from a Content-Disposition header, and\n    return the default filename if the result is empty.\n    \"\"\"\n    _type, params = cgi.parse_header(content_disposition)\n    filename = params.get('filename')\n    if filename:\n        # We need to sanitize the filename to prevent directory traversal\n        # in case the filename contains \"..\" path parts.\n        filename = sanitize_content_filename(filename)\n    return filename or default_filename\n\n\ndef _download_http_url(\n    link,  # type: Link\n    session,  # type: PipSession\n    temp_dir,  # type: str\n    hashes,  # type: Hashes\n    progress_bar  # type: str\n):\n    # type: (...) -> Tuple[str, str]\n    \"\"\"Download link url into temp_dir using provided session\"\"\"\n    target_url = link.url.split('#', 1)[0]\n    try:\n        resp = session.get(\n            target_url,\n            # We use Accept-Encoding: identity here because requests\n            # defaults to accepting compressed responses. This breaks in\n            # a variety of ways depending on how the server is configured.\n            # - Some servers will notice that the file isn't a compressible\n            #   file and will leave the file alone and with an empty\n            #   Content-Encoding\n            # - Some servers will notice that the file is already\n            #   compressed and will leave the file alone and will add a\n            #   Content-Encoding: gzip header\n            # - Some servers won't notice anything at all and will take\n            #   a file that's already been compressed and compress it again\n            #   and set the Content-Encoding: gzip header\n            # By setting this to request only the identity encoding We're\n            # hoping to eliminate the third case. Hopefully there does not\n            # exist a server which when given a file will notice it is\n            # already compressed and that you're not asking for a\n            # compressed file and will then decompress it before sending\n            # because if that's the case I don't think it'll ever be\n            # possible to make this work.\n            headers={\"Accept-Encoding\": \"identity\"},\n            stream=True,\n        )\n        resp.raise_for_status()\n    except requests.HTTPError as exc:\n        logger.critical(\n            \"HTTP error %s while getting %s\", exc.response.status_code, link,\n        )\n        raise\n\n    content_type = resp.headers.get('content-type', '')\n    filename = link.filename  # fallback\n    # Have a look at the Content-Disposition header for a better guess\n    content_disposition = resp.headers.get('content-disposition')\n    if content_disposition:\n        filename = parse_content_disposition(content_disposition, filename)\n    ext = splitext(filename)[1]\n    if not ext:\n        ext = mimetypes.guess_extension(content_type)\n        if ext:\n            filename += ext\n    if not ext and link.url != resp.url:\n        ext = os.path.splitext(resp.url)[1]\n        if ext:\n            filename += ext\n    file_path = os.path.join(temp_dir, filename)\n    with open(file_path, 'wb') as content_file:\n        _download_url(resp, link, content_file, hashes, progress_bar)\n    return file_path, content_type\n\n\ndef _check_download_dir(link, download_dir, hashes):\n    # type: (Link, str, Hashes) -> Optional[str]\n    \"\"\" Check download_dir for previously downloaded file with correct hash\n        If a correct file is found return its path else None\n    \"\"\"\n    download_path = os.path.join(download_dir, link.filename)\n    if os.path.exists(download_path):\n        # If already downloaded, does its hash match?\n        logger.info('File was already downloaded %s', download_path)\n        if hashes:\n            try:\n                hashes.check_against_path(download_path)\n            except HashMismatch:\n                logger.warning(\n                    'Previously-downloaded file %s has bad hash. '\n                    'Re-downloading.',\n                    download_path\n                )\n                os.unlink(download_path)\n                return None\n        return download_path\n    return None\n", "code_before": "from __future__ import absolute_import\n\nimport cgi\nimport email.utils\nimport json\nimport logging\nimport mimetypes\nimport os\nimport platform\nimport re\nimport shutil\nimport sys\n\nfrom pip._vendor import requests, urllib3\nfrom pip._vendor.cachecontrol import CacheControlAdapter\nfrom pip._vendor.cachecontrol.caches import FileCache\nfrom pip._vendor.lockfile import LockError\nfrom pip._vendor.requests.adapters import BaseAdapter, HTTPAdapter\nfrom pip._vendor.requests.auth import AuthBase, HTTPBasicAuth\nfrom pip._vendor.requests.models import CONTENT_CHUNK_SIZE, Response\nfrom pip._vendor.requests.structures import CaseInsensitiveDict\nfrom pip._vendor.requests.utils import get_netrc_auth\n# NOTE: XMLRPC Client is not annotated in typeshed as on 2017-07-17, which is\n#       why we ignore the type on this import\nfrom pip._vendor.six.moves import xmlrpc_client  # type: ignore\nfrom pip._vendor.six.moves.urllib import parse as urllib_parse\nfrom pip._vendor.six.moves.urllib import request as urllib_request\nfrom pip._vendor.urllib3.util import IS_PYOPENSSL\n\nimport pip\nfrom pip._internal.exceptions import HashMismatch, InstallationError\nfrom pip._internal.locations import write_delete_marker_file\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.utils.encoding import auto_decode\nfrom pip._internal.utils.filesystem import check_path_owner\nfrom pip._internal.utils.glibc import libc_ver\nfrom pip._internal.utils.misc import (\n    ARCHIVE_EXTENSIONS, ask, ask_input, ask_password, ask_path_exists,\n    backup_dir, consume, display_path, format_size, get_installed_version,\n    path_to_url, remove_auth_from_url, rmtree, split_auth_netloc_from_url,\n    splitext, unpack_file,\n)\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._internal.utils.typing import MYPY_CHECK_RUNNING\nfrom pip._internal.utils.ui import DownloadProgressProvider\nfrom pip._internal.vcs import vcs\n\nif MYPY_CHECK_RUNNING:\n    from typing import (\n        Optional, Tuple, Dict, IO, Text, Union\n    )\n    from optparse import Values\n    from pip._internal.models.link import Link\n    from pip._internal.utils.hashes import Hashes\n    from pip._internal.vcs.versioncontrol import AuthInfo, VersionControl\n\ntry:\n    import ssl  # noqa\nexcept ImportError:\n    ssl = None\n\n\nHAS_TLS = (ssl is not None) or IS_PYOPENSSL\n\n__all__ = ['get_file_content',\n           'is_url', 'url_to_path', 'path_to_url',\n           'is_archive_file', 'unpack_vcs_link',\n           'unpack_file_url', 'is_vcs_url', 'is_file_url',\n           'unpack_http_url', 'unpack_url']\n\n\nlogger = logging.getLogger(__name__)\n\n\ntry:\n    import keyring  # noqa\nexcept ImportError:\n    keyring = None\nexcept Exception as exc:\n    logger.warning(\"Keyring is skipped due to an exception: %s\",\n                   str(exc))\n    keyring = None\n\n# These are environment variables present when running under various\n# CI systems.  For each variable, some CI systems that use the variable\n# are indicated.  The collection was chosen so that for each of a number\n# of popular systems, at least one of the environment variables is used.\n# This list is used to provide some indication of and lower bound for\n# CI traffic to PyPI.  Thus, it is okay if the list is not comprehensive.\n# For more background, see: https://github.com/pypa/pip/issues/5499\nCI_ENVIRONMENT_VARIABLES = (\n    # Azure Pipelines\n    'BUILD_BUILDID',\n    # Jenkins\n    'BUILD_ID',\n    # AppVeyor, CircleCI, Codeship, Gitlab CI, Shippable, Travis CI\n    'CI',\n    # Explicit environment variable.\n    'PIP_IS_CI',\n)\n\n\ndef looks_like_ci():\n    # type: () -> bool\n    \"\"\"\n    Return whether it looks like pip is running under CI.\n    \"\"\"\n    # We don't use the method of checking for a tty (e.g. using isatty())\n    # because some CI systems mimic a tty (e.g. Travis CI).  Thus that\n    # method doesn't provide definitive information in either direction.\n    return any(name in os.environ for name in CI_ENVIRONMENT_VARIABLES)\n\n\ndef user_agent():\n    \"\"\"\n    Return a string representing the user agent.\n    \"\"\"\n    data = {\n        \"installer\": {\"name\": \"pip\", \"version\": pip.__version__},\n        \"python\": platform.python_version(),\n        \"implementation\": {\n            \"name\": platform.python_implementation(),\n        },\n    }\n\n    if data[\"implementation\"][\"name\"] == 'CPython':\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'PyPy':\n        if sys.pypy_version_info.releaselevel == 'final':\n            pypy_version_info = sys.pypy_version_info[:3]\n        else:\n            pypy_version_info = sys.pypy_version_info\n        data[\"implementation\"][\"version\"] = \".\".join(\n            [str(x) for x in pypy_version_info]\n        )\n    elif data[\"implementation\"][\"name\"] == 'Jython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'IronPython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n\n    if sys.platform.startswith(\"linux\"):\n        from pip._vendor import distro\n        distro_infos = dict(filter(\n            lambda x: x[1],\n            zip([\"name\", \"version\", \"id\"], distro.linux_distribution()),\n        ))\n        libc = dict(filter(\n            lambda x: x[1],\n            zip([\"lib\", \"version\"], libc_ver()),\n        ))\n        if libc:\n            distro_infos[\"libc\"] = libc\n        if distro_infos:\n            data[\"distro\"] = distro_infos\n\n    if sys.platform.startswith(\"darwin\") and platform.mac_ver()[0]:\n        data[\"distro\"] = {\"name\": \"macOS\", \"version\": platform.mac_ver()[0]}\n\n    if platform.system():\n        data.setdefault(\"system\", {})[\"name\"] = platform.system()\n\n    if platform.release():\n        data.setdefault(\"system\", {})[\"release\"] = platform.release()\n\n    if platform.machine():\n        data[\"cpu\"] = platform.machine()\n\n    if HAS_TLS:\n        data[\"openssl_version\"] = ssl.OPENSSL_VERSION\n\n    setuptools_version = get_installed_version(\"setuptools\")\n    if setuptools_version is not None:\n        data[\"setuptools_version\"] = setuptools_version\n\n    # Use None rather than False so as not to give the impression that\n    # pip knows it is not being run under CI.  Rather, it is a null or\n    # inconclusive result.  Also, we include some value rather than no\n    # value to make it easier to know that the check has been run.\n    data[\"ci\"] = True if looks_like_ci() else None\n\n    user_data = os.environ.get(\"PIP_USER_AGENT_USER_DATA\")\n    if user_data is not None:\n        data[\"user_data\"] = user_data\n\n    return \"{data[installer][name]}/{data[installer][version]} {json}\".format(\n        data=data,\n        json=json.dumps(data, separators=(\",\", \":\"), sort_keys=True),\n    )\n\n\ndef _get_keyring_auth(url, username):\n    \"\"\"Return the tuple auth for a given url from keyring.\"\"\"\n    if not url or not keyring:\n        return None\n\n    try:\n        try:\n            get_credential = keyring.get_credential\n        except AttributeError:\n            pass\n        else:\n            logger.debug(\"Getting credentials from keyring for %s\", url)\n            cred = get_credential(url, username)\n            if cred is not None:\n                return cred.username, cred.password\n            return None\n\n        if username:\n            logger.debug(\"Getting password from keyring for %s\", url)\n            password = keyring.get_password(url, username)\n            if password:\n                return username, password\n\n    except Exception as exc:\n        logger.warning(\"Keyring is skipped due to an exception: %s\",\n                       str(exc))\n\n\nclass MultiDomainBasicAuth(AuthBase):\n\n    def __init__(self, prompting=True, index_urls=None):\n        # type: (bool, Optional[Values]) -> None\n        self.prompting = prompting\n        self.index_urls = index_urls\n        self.passwords = {}  # type: Dict[str, AuthInfo]\n        # When the user is prompted to enter credentials and keyring is\n        # available, we will offer to save them. If the user accepts,\n        # this value is set to the credentials they entered. After the\n        # request authenticates, the caller should call\n        # ``save_credentials`` to save these.\n        self._credentials_to_save = None   # type: Tuple[str, str, str]\n\n    def _get_index_url(self, url):\n        \"\"\"Return the original index URL matching the requested URL.\n\n        Cached or dynamically generated credentials may work against\n        the original index URL rather than just the netloc.\n\n        The provided url should have had its username and password\n        removed already. If the original index url had credentials then\n        they will be included in the return value.\n\n        Returns None if no matching index was found, or if --no-index\n        was specified by the user.\n        \"\"\"\n        if not url or not self.index_urls:\n            return None\n\n        for u in self.index_urls:\n            prefix = remove_auth_from_url(u).rstrip(\"/\") + \"/\"\n            if url.startswith(prefix):\n                return u\n\n    def _get_new_credentials(self, original_url, allow_netrc=True,\n                             allow_keyring=True):\n        \"\"\"Find and return credentials for the specified URL.\"\"\"\n        # Split the credentials and netloc from the url.\n        url, netloc, url_user_password = split_auth_netloc_from_url(\n            original_url)\n\n        # Start with the credentials embedded in the url\n        username, password = url_user_password\n        if username is not None and password is not None:\n            logger.debug(\"Found credentials in url for %s\", netloc)\n            return url_user_password\n\n        # Find a matching index url for this request\n        index_url = self._get_index_url(url)\n        if index_url:\n            # Split the credentials from the url.\n            index_info = split_auth_netloc_from_url(index_url)\n            if index_info:\n                index_url, _, index_url_user_password = index_info\n                logger.debug(\"Found index url %s\", index_url)\n\n        # If an index URL was found, try its embedded credentials\n        if index_url and index_url_user_password[0] is not None:\n            username, password = index_url_user_password\n            if username is not None and password is not None:\n                logger.debug(\"Found credentials in index url for %s\", netloc)\n                return index_url_user_password\n\n        # Get creds from netrc if we still don't have them\n        if allow_netrc:\n            netrc_auth = get_netrc_auth(original_url)\n            if netrc_auth:\n                logger.debug(\"Found credentials in netrc for %s\", netloc)\n                return netrc_auth\n\n        # If we don't have a password and keyring is available, use it.\n        if allow_keyring:\n            # The index url is more specific than the netloc, so try it first\n            kr_auth = (_get_keyring_auth(index_url, username) or\n                       _get_keyring_auth(netloc, username))\n            if kr_auth:\n                logger.debug(\"Found credentials in keyring for %s\", netloc)\n                return kr_auth\n\n        return None, None\n\n    def _get_url_and_credentials(self, original_url):\n        \"\"\"Return the credentials to use for the provided URL.\n\n        If allowed, netrc and keyring may be used to obtain the\n        correct credentials.\n\n        Returns (url_without_credentials, username, password). Note\n        that even if the original URL contains credentials, this\n        function may return a different username and password.\n        \"\"\"\n        url, netloc, _ = split_auth_netloc_from_url(original_url)\n\n        # Use any stored credentials that we have for this netloc\n        username, password = self.passwords.get(netloc, (None, None))\n\n        # If nothing cached, acquire new credentials without prompting\n        # the user (e.g. from netrc, keyring, or similar).\n        if username is None or password is None:\n            username, password = self._get_new_credentials(original_url)\n\n        if username is not None and password is not None:\n            # Store the username and password\n            self.passwords[netloc] = (username, password)\n\n        return url, username, password\n\n    def __call__(self, req):\n        # Get credentials for this request\n        url, username, password = self._get_url_and_credentials(req.url)\n\n        # Set the url of the request to the url without any credentials\n        req.url = url\n\n        if username is not None and password is not None:\n            # Send the basic auth with this request\n            req = HTTPBasicAuth(username, password)(req)\n\n        # Attach a hook to handle 401 responses\n        req.register_hook(\"response\", self.handle_401)\n\n        return req\n\n    # Factored out to allow for easy patching in tests\n    def _prompt_for_password(self, netloc):\n        username = ask_input(\"User for %s: \" % netloc)\n        if not username:\n            return None, None\n        auth = _get_keyring_auth(netloc, username)\n        if auth:\n            return auth[0], auth[1], False\n        password = ask_password(\"Password: \")\n        return username, password, True\n\n    # Factored out to allow for easy patching in tests\n    def _should_save_password_to_keyring(self):\n        if not keyring:\n            return False\n        return ask(\"Save credentials to keyring [y/N]: \", [\"y\", \"n\"]) == \"y\"\n\n    def handle_401(self, resp, **kwargs):\n        # We only care about 401 responses, anything else we want to just\n        #   pass through the actual response\n        if resp.status_code != 401:\n            return resp\n\n        # We are not able to prompt the user so simply return the response\n        if not self.prompting:\n            return resp\n\n        parsed = urllib_parse.urlparse(resp.url)\n\n        # Prompt the user for a new username and password\n        username, password, save = self._prompt_for_password(parsed.netloc)\n\n        # Store the new username and password to use for future requests\n        self._credentials_to_save = None\n        if username is not None and password is not None:\n            self.passwords[parsed.netloc] = (username, password)\n\n            # Prompt to save the password to keyring\n            if save and self._should_save_password_to_keyring():\n                self._credentials_to_save = (parsed.netloc, username, password)\n\n        # Consume content and release the original connection to allow our new\n        #   request to reuse the same one.\n        resp.content\n        resp.raw.release_conn()\n\n        # Add our new username and password to the request\n        req = HTTPBasicAuth(username or \"\", password or \"\")(resp.request)\n        req.register_hook(\"response\", self.warn_on_401)\n\n        # On successful request, save the credentials that were used to\n        # keyring. (Note that if the user responded \"no\" above, this member\n        # is not set and nothing will be saved.)\n        if self._credentials_to_save:\n            req.register_hook(\"response\", self.save_credentials)\n\n        # Send our new request\n        new_resp = resp.connection.send(req, **kwargs)\n        new_resp.history.append(resp)\n\n        return new_resp\n\n    def warn_on_401(self, resp, **kwargs):\n        \"\"\"Response callback to warn about incorrect credentials.\"\"\"\n        if resp.status_code == 401:\n            logger.warning('401 Error, Credentials not correct for %s',\n                           resp.request.url)\n\n    def save_credentials(self, resp, **kwargs):\n        \"\"\"Response callback to save credentials on success.\"\"\"\n        assert keyring is not None, \"should never reach here without keyring\"\n        if not keyring:\n            return\n\n        creds = self._credentials_to_save\n        self._credentials_to_save = None\n        if creds and resp.status_code < 400:\n            try:\n                logger.info('Saving credentials to keyring')\n                keyring.set_password(*creds)\n            except Exception:\n                logger.exception('Failed to save credentials')\n\n\nclass LocalFSAdapter(BaseAdapter):\n\n    def send(self, request, stream=None, timeout=None, verify=None, cert=None,\n             proxies=None):\n        pathname = url_to_path(request.url)\n\n        resp = Response()\n        resp.status_code = 200\n        resp.url = request.url\n\n        try:\n            stats = os.stat(pathname)\n        except OSError as exc:\n            resp.status_code = 404\n            resp.raw = exc\n        else:\n            modified = email.utils.formatdate(stats.st_mtime, usegmt=True)\n            content_type = mimetypes.guess_type(pathname)[0] or \"text/plain\"\n            resp.headers = CaseInsensitiveDict({\n                \"Content-Type\": content_type,\n                \"Content-Length\": stats.st_size,\n                \"Last-Modified\": modified,\n            })\n\n            resp.raw = open(pathname, \"rb\")\n            resp.close = resp.raw.close\n\n        return resp\n\n    def close(self):\n        pass\n\n\nclass SafeFileCache(FileCache):\n    \"\"\"\n    A file based cache which is safe to use even when the target directory may\n    not be accessible or writable.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(SafeFileCache, self).__init__(*args, **kwargs)\n\n        # Check to ensure that the directory containing our cache directory\n        # is owned by the user current executing pip. If it does not exist\n        # we will check the parent directory until we find one that does exist.\n        # If it is not owned by the user executing pip then we will disable\n        # the cache and log a warning.\n        if not check_path_owner(self.directory):\n            logger.warning(\n                \"The directory '%s' or its parent directory is not owned by \"\n                \"the current user and the cache has been disabled. Please \"\n                \"check the permissions and owner of that directory. If \"\n                \"executing pip with sudo, you may want sudo's -H flag.\",\n                self.directory,\n            )\n\n            # Set our directory to None to disable the Cache\n            self.directory = None\n\n    def get(self, *args, **kwargs):\n        # If we don't have a directory, then the cache should be a no-op.\n        if self.directory is None:\n            return\n\n        try:\n            return super(SafeFileCache, self).get(*args, **kwargs)\n        except (LockError, OSError, IOError):\n            # We intentionally silence this error, if we can't access the cache\n            # then we can just skip caching and process the request as if\n            # caching wasn't enabled.\n            pass\n\n    def set(self, *args, **kwargs):\n        # If we don't have a directory, then the cache should be a no-op.\n        if self.directory is None:\n            return\n\n        try:\n            return super(SafeFileCache, self).set(*args, **kwargs)\n        except (LockError, OSError, IOError):\n            # We intentionally silence this error, if we can't access the cache\n            # then we can just skip caching and process the request as if\n            # caching wasn't enabled.\n            pass\n\n    def delete(self, *args, **kwargs):\n        # If we don't have a directory, then the cache should be a no-op.\n        if self.directory is None:\n            return\n\n        try:\n            return super(SafeFileCache, self).delete(*args, **kwargs)\n        except (LockError, OSError, IOError):\n            # We intentionally silence this error, if we can't access the cache\n            # then we can just skip caching and process the request as if\n            # caching wasn't enabled.\n            pass\n\n\nclass InsecureHTTPAdapter(HTTPAdapter):\n\n    def cert_verify(self, conn, url, verify, cert):\n        conn.cert_reqs = 'CERT_NONE'\n        conn.ca_certs = None\n\n\nclass PipSession(requests.Session):\n\n    timeout = None  # type: Optional[int]\n\n    def __init__(self, *args, **kwargs):\n        retries = kwargs.pop(\"retries\", 0)\n        cache = kwargs.pop(\"cache\", None)\n        insecure_hosts = kwargs.pop(\"insecure_hosts\", [])\n        index_urls = kwargs.pop(\"index_urls\", None)\n\n        super(PipSession, self).__init__(*args, **kwargs)\n\n        # Attach our User Agent to the request\n        self.headers[\"User-Agent\"] = user_agent()\n\n        # Attach our Authentication handler to the session\n        self.auth = MultiDomainBasicAuth(index_urls=index_urls)\n\n        # Create our urllib3.Retry instance which will allow us to customize\n        # how we handle retries.\n        retries = urllib3.Retry(\n            # Set the total number of retries that a particular request can\n            # have.\n            total=retries,\n\n            # A 503 error from PyPI typically means that the Fastly -> Origin\n            # connection got interrupted in some way. A 503 error in general\n            # is typically considered a transient error so we'll go ahead and\n            # retry it.\n            # A 500 may indicate transient error in Amazon S3\n            # A 520 or 527 - may indicate transient error in CloudFlare\n            status_forcelist=[500, 503, 520, 527],\n\n            # Add a small amount of back off between failed requests in\n            # order to prevent hammering the service.\n            backoff_factor=0.25,\n        )\n\n        # We want to _only_ cache responses on securely fetched origins. We do\n        # this because we can't validate the response of an insecurely fetched\n        # origin, and we don't want someone to be able to poison the cache and\n        # require manual eviction from the cache to fix it.\n        if cache:\n            secure_adapter = CacheControlAdapter(\n                cache=SafeFileCache(cache, use_dir_lock=True),\n                max_retries=retries,\n            )\n        else:\n            secure_adapter = HTTPAdapter(max_retries=retries)\n\n        # Our Insecure HTTPAdapter disables HTTPS validation. It does not\n        # support caching (see above) so we'll use it for all http:// URLs as\n        # well as any https:// host that we've marked as ignoring TLS errors\n        # for.\n        insecure_adapter = InsecureHTTPAdapter(max_retries=retries)\n\n        self.mount(\"https://\", secure_adapter)\n        self.mount(\"http://\", insecure_adapter)\n\n        # Enable file:// urls\n        self.mount(\"file://\", LocalFSAdapter())\n\n        # We want to use a non-validating adapter for any requests which are\n        # deemed insecure.\n        for host in insecure_hosts:\n            self.mount(\"https://{}/\".format(host), insecure_adapter)\n\n    def request(self, method, url, *args, **kwargs):\n        # Allow setting a default timeout on a session\n        kwargs.setdefault(\"timeout\", self.timeout)\n\n        # Dispatch the actual request\n        return super(PipSession, self).request(method, url, *args, **kwargs)\n\n\ndef get_file_content(url, comes_from=None, session=None):\n    # type: (str, Optional[str], Optional[PipSession]) -> Tuple[str, Text]\n    \"\"\"Gets the content of a file; it may be a filename, file: URL, or\n    http: URL.  Returns (location, content).  Content is unicode.\n\n    :param url:         File path or url.\n    :param comes_from:  Origin description of requirements.\n    :param session:     Instance of pip.download.PipSession.\n    \"\"\"\n    if session is None:\n        raise TypeError(\n            \"get_file_content() missing 1 required keyword argument: 'session'\"\n        )\n\n    match = _scheme_re.search(url)\n    if match:\n        scheme = match.group(1).lower()\n        if (scheme == 'file' and comes_from and\n                comes_from.startswith('http')):\n            raise InstallationError(\n                'Requirements file %s references URL %s, which is local'\n                % (comes_from, url))\n        if scheme == 'file':\n            path = url.split(':', 1)[1]\n            path = path.replace('\\\\', '/')\n            match = _url_slash_drive_re.match(path)\n            if match:\n                path = match.group(1) + ':' + path.split('|', 1)[1]\n            path = urllib_parse.unquote(path)\n            if path.startswith('/'):\n                path = '/' + path.lstrip('/')\n            url = path\n        else:\n            # FIXME: catch some errors\n            resp = session.get(url)\n            resp.raise_for_status()\n            return resp.url, resp.text\n    try:\n        with open(url, 'rb') as f:\n            content = auto_decode(f.read())\n    except IOError as exc:\n        raise InstallationError(\n            'Could not open requirements file: %s' % str(exc)\n        )\n    return url, content\n\n\n_scheme_re = re.compile(r'^(http|https|file):', re.I)\n_url_slash_drive_re = re.compile(r'/*([a-z])\\|', re.I)\n\n\ndef is_url(name):\n    # type: (Union[str, Text]) -> bool\n    \"\"\"Returns true if the name looks like a URL\"\"\"\n    if ':' not in name:\n        return False\n    scheme = name.split(':', 1)[0].lower()\n    return scheme in ['http', 'https', 'file', 'ftp'] + vcs.all_schemes\n\n\ndef url_to_path(url):\n    # type: (str) -> str\n    \"\"\"\n    Convert a file: URL to a path.\n    \"\"\"\n    assert url.startswith('file:'), (\n        \"You can only turn file: urls into filenames (not %r)\" % url)\n\n    _, netloc, path, _, _ = urllib_parse.urlsplit(url)\n\n    if not netloc or netloc == 'localhost':\n        # According to RFC 8089, same as empty authority.\n        netloc = ''\n    elif sys.platform == 'win32':\n        # If we have a UNC path, prepend UNC share notation.\n        netloc = '\\\\\\\\' + netloc\n    else:\n        raise ValueError(\n            'non-local file URIs are not supported on this platform: %r'\n            % url\n        )\n\n    path = urllib_request.url2pathname(netloc + path)\n    return path\n\n\ndef is_archive_file(name):\n    # type: (str) -> bool\n    \"\"\"Return True if `name` is a considered as an archive file.\"\"\"\n    ext = splitext(name)[1].lower()\n    if ext in ARCHIVE_EXTENSIONS:\n        return True\n    return False\n\n\ndef unpack_vcs_link(link, location):\n    vcs_backend = _get_used_vcs_backend(link)\n    vcs_backend.unpack(location, url=link.url)\n\n\ndef _get_used_vcs_backend(link):\n    # type: (Link) -> Optional[VersionControl]\n    \"\"\"\n    Return a VersionControl object or None.\n    \"\"\"\n    for vcs_backend in vcs.backends:\n        if link.scheme in vcs_backend.schemes:\n            return vcs_backend\n    return None\n\n\ndef is_vcs_url(link):\n    # type: (Link) -> bool\n    return bool(_get_used_vcs_backend(link))\n\n\ndef is_file_url(link):\n    # type: (Link) -> bool\n    return link.url.lower().startswith('file:')\n\n\ndef is_dir_url(link):\n    # type: (Link) -> bool\n    \"\"\"Return whether a file:// Link points to a directory.\n\n    ``link`` must not have any other scheme but file://. Call is_file_url()\n    first.\n\n    \"\"\"\n    link_path = url_to_path(link.url_without_fragment)\n    return os.path.isdir(link_path)\n\n\ndef _progress_indicator(iterable, *args, **kwargs):\n    return iterable\n\n\ndef _download_url(\n    resp,  # type: Response\n    link,  # type: Link\n    content_file,  # type: IO\n    hashes,  # type: Hashes\n    progress_bar  # type: str\n):\n    # type: (...) -> None\n    try:\n        total_length = int(resp.headers['content-length'])\n    except (ValueError, KeyError, TypeError):\n        total_length = 0\n\n    cached_resp = getattr(resp, \"from_cache\", False)\n    if logger.getEffectiveLevel() > logging.INFO:\n        show_progress = False\n    elif cached_resp:\n        show_progress = False\n    elif total_length > (40 * 1000):\n        show_progress = True\n    elif not total_length:\n        show_progress = True\n    else:\n        show_progress = False\n\n    show_url = link.show_url\n\n    def resp_read(chunk_size):\n        try:\n            # Special case for urllib3.\n            for chunk in resp.raw.stream(\n                    chunk_size,\n                    # We use decode_content=False here because we don't\n                    # want urllib3 to mess with the raw bytes we get\n                    # from the server. If we decompress inside of\n                    # urllib3 then we cannot verify the checksum\n                    # because the checksum will be of the compressed\n                    # file. This breakage will only occur if the\n                    # server adds a Content-Encoding header, which\n                    # depends on how the server was configured:\n                    # - Some servers will notice that the file isn't a\n                    #   compressible file and will leave the file alone\n                    #   and with an empty Content-Encoding\n                    # - Some servers will notice that the file is\n                    #   already compressed and will leave the file\n                    #   alone and will add a Content-Encoding: gzip\n                    #   header\n                    # - Some servers won't notice anything at all and\n                    #   will take a file that's already been compressed\n                    #   and compress it again and set the\n                    #   Content-Encoding: gzip header\n                    #\n                    # By setting this not to decode automatically we\n                    # hope to eliminate problems with the second case.\n                    decode_content=False):\n                yield chunk\n        except AttributeError:\n            # Standard file-like object.\n            while True:\n                chunk = resp.raw.read(chunk_size)\n                if not chunk:\n                    break\n                yield chunk\n\n    def written_chunks(chunks):\n        for chunk in chunks:\n            content_file.write(chunk)\n            yield chunk\n\n    progress_indicator = _progress_indicator\n\n    if link.netloc == PyPI.netloc:\n        url = show_url\n    else:\n        url = link.url_without_fragment\n\n    if show_progress:  # We don't show progress on cached responses\n        progress_indicator = DownloadProgressProvider(progress_bar,\n                                                      max=total_length)\n        if total_length:\n            logger.info(\"Downloading %s (%s)\", url, format_size(total_length))\n        else:\n            logger.info(\"Downloading %s\", url)\n    elif cached_resp:\n        logger.info(\"Using cached %s\", url)\n    else:\n        logger.info(\"Downloading %s\", url)\n\n    logger.debug('Downloading from URL %s', link)\n\n    downloaded_chunks = written_chunks(\n        progress_indicator(\n            resp_read(CONTENT_CHUNK_SIZE),\n            CONTENT_CHUNK_SIZE\n        )\n    )\n    if hashes:\n        hashes.check_against_chunks(downloaded_chunks)\n    else:\n        consume(downloaded_chunks)\n\n\ndef _copy_file(filename, location, link):\n    copy = True\n    download_location = os.path.join(location, link.filename)\n    if os.path.exists(download_location):\n        response = ask_path_exists(\n            'The file %s exists. (i)gnore, (w)ipe, (b)ackup, (a)abort' %\n            display_path(download_location), ('i', 'w', 'b', 'a'))\n        if response == 'i':\n            copy = False\n        elif response == 'w':\n            logger.warning('Deleting %s', display_path(download_location))\n            os.remove(download_location)\n        elif response == 'b':\n            dest_file = backup_dir(download_location)\n            logger.warning(\n                'Backing up %s to %s',\n                display_path(download_location),\n                display_path(dest_file),\n            )\n            shutil.move(download_location, dest_file)\n        elif response == 'a':\n            sys.exit(-1)\n    if copy:\n        shutil.copy(filename, download_location)\n        logger.info('Saved %s', display_path(download_location))\n\n\ndef unpack_http_url(\n    link,  # type: Link\n    location,  # type: str\n    download_dir=None,  # type: Optional[str]\n    session=None,  # type: Optional[PipSession]\n    hashes=None,  # type: Optional[Hashes]\n    progress_bar=\"on\"  # type: str\n):\n    # type: (...) -> None\n    if session is None:\n        raise TypeError(\n            \"unpack_http_url() missing 1 required keyword argument: 'session'\"\n        )\n\n    with TempDirectory(kind=\"unpack\") as temp_dir:\n        # If a download dir is specified, is the file already downloaded there?\n        already_downloaded_path = None\n        if download_dir:\n            already_downloaded_path = _check_download_dir(link,\n                                                          download_dir,\n                                                          hashes)\n\n        if already_downloaded_path:\n            from_path = already_downloaded_path\n            content_type = mimetypes.guess_type(from_path)[0]\n        else:\n            # let's download to a tmp dir\n            from_path, content_type = _download_http_url(link,\n                                                         session,\n                                                         temp_dir.path,\n                                                         hashes,\n                                                         progress_bar)\n\n        # unpack the archive to the build dir location. even when only\n        # downloading archives, they have to be unpacked to parse dependencies\n        unpack_file(from_path, location, content_type, link)\n\n        # a download dir is specified; let's copy the archive there\n        if download_dir and not already_downloaded_path:\n            _copy_file(from_path, download_dir, link)\n\n        if not already_downloaded_path:\n            os.unlink(from_path)\n\n\ndef unpack_file_url(\n    link,  # type: Link\n    location,  # type: str\n    download_dir=None,  # type: Optional[str]\n    hashes=None  # type: Optional[Hashes]\n):\n    # type: (...) -> None\n    \"\"\"Unpack link into location.\n\n    If download_dir is provided and link points to a file, make a copy\n    of the link file inside download_dir.\n    \"\"\"\n    link_path = url_to_path(link.url_without_fragment)\n\n    # If it's a url to a local directory\n    if is_dir_url(link):\n        if os.path.isdir(location):\n            rmtree(location)\n        shutil.copytree(link_path, location, symlinks=True)\n        if download_dir:\n            logger.info('Link is a directory, ignoring download_dir')\n        return\n\n    # If --require-hashes is off, `hashes` is either empty, the\n    # link's embedded hash, or MissingHashes; it is required to\n    # match. If --require-hashes is on, we are satisfied by any\n    # hash in `hashes` matching: a URL-based or an option-based\n    # one; no internet-sourced hash will be in `hashes`.\n    if hashes:\n        hashes.check_against_path(link_path)\n\n    # If a download dir is specified, is the file already there and valid?\n    already_downloaded_path = None\n    if download_dir:\n        already_downloaded_path = _check_download_dir(link,\n                                                      download_dir,\n                                                      hashes)\n\n    if already_downloaded_path:\n        from_path = already_downloaded_path\n    else:\n        from_path = link_path\n\n    content_type = mimetypes.guess_type(from_path)[0]\n\n    # unpack the archive to the build dir location. even when only downloading\n    # archives, they have to be unpacked to parse dependencies\n    unpack_file(from_path, location, content_type, link)\n\n    # a download dir is specified and not already downloaded\n    if download_dir and not already_downloaded_path:\n        _copy_file(from_path, download_dir, link)\n\n\nclass PipXmlrpcTransport(xmlrpc_client.Transport):\n    \"\"\"Provide a `xmlrpclib.Transport` implementation via a `PipSession`\n    object.\n    \"\"\"\n\n    def __init__(self, index_url, session, use_datetime=False):\n        xmlrpc_client.Transport.__init__(self, use_datetime)\n        index_parts = urllib_parse.urlparse(index_url)\n        self._scheme = index_parts.scheme\n        self._session = session\n\n    def request(self, host, handler, request_body, verbose=False):\n        parts = (self._scheme, host, handler, None, None, None)\n        url = urllib_parse.urlunparse(parts)\n        try:\n            headers = {'Content-Type': 'text/xml'}\n            response = self._session.post(url, data=request_body,\n                                          headers=headers, stream=True)\n            response.raise_for_status()\n            self.verbose = verbose\n            return self.parse_response(response.raw)\n        except requests.HTTPError as exc:\n            logger.critical(\n                \"HTTP error %s while getting %s\",\n                exc.response.status_code, url,\n            )\n            raise\n\n\ndef unpack_url(\n    link,  # type: Optional[Link]\n    location,  # type: Optional[str]\n    download_dir=None,  # type: Optional[str]\n    only_download=False,  # type: bool\n    session=None,  # type: Optional[PipSession]\n    hashes=None,  # type: Optional[Hashes]\n    progress_bar=\"on\"  # type: str\n):\n    # type: (...) -> None\n    \"\"\"Unpack link.\n       If link is a VCS link:\n         if only_download, export into download_dir and ignore location\n          else unpack into location\n       for other types of link:\n         - unpack into location\n         - if download_dir, copy the file into download_dir\n         - if only_download, mark location for deletion\n\n    :param hashes: A Hashes object, one of whose embedded hashes must match,\n        or HashMismatch will be raised. If the Hashes is empty, no matches are\n        required, and unhashable types of requirements (like VCS ones, which\n        would ordinarily raise HashUnsupported) are allowed.\n    \"\"\"\n    # non-editable vcs urls\n    if is_vcs_url(link):\n        unpack_vcs_link(link, location)\n\n    # file urls\n    elif is_file_url(link):\n        unpack_file_url(link, location, download_dir, hashes=hashes)\n\n    # http urls\n    else:\n        if session is None:\n            session = PipSession()\n\n        unpack_http_url(\n            link,\n            location,\n            download_dir,\n            session,\n            hashes=hashes,\n            progress_bar=progress_bar\n        )\n    if only_download:\n        write_delete_marker_file(location)\n\n\ndef _download_http_url(\n    link,  # type: Link\n    session,  # type: PipSession\n    temp_dir,  # type: str\n    hashes,  # type: Hashes\n    progress_bar  # type: str\n):\n    # type: (...) -> Tuple[str, str]\n    \"\"\"Download link url into temp_dir using provided session\"\"\"\n    target_url = link.url.split('#', 1)[0]\n    try:\n        resp = session.get(\n            target_url,\n            # We use Accept-Encoding: identity here because requests\n            # defaults to accepting compressed responses. This breaks in\n            # a variety of ways depending on how the server is configured.\n            # - Some servers will notice that the file isn't a compressible\n            #   file and will leave the file alone and with an empty\n            #   Content-Encoding\n            # - Some servers will notice that the file is already\n            #   compressed and will leave the file alone and will add a\n            #   Content-Encoding: gzip header\n            # - Some servers won't notice anything at all and will take\n            #   a file that's already been compressed and compress it again\n            #   and set the Content-Encoding: gzip header\n            # By setting this to request only the identity encoding We're\n            # hoping to eliminate the third case. Hopefully there does not\n            # exist a server which when given a file will notice it is\n            # already compressed and that you're not asking for a\n            # compressed file and will then decompress it before sending\n            # because if that's the case I don't think it'll ever be\n            # possible to make this work.\n            headers={\"Accept-Encoding\": \"identity\"},\n            stream=True,\n        )\n        resp.raise_for_status()\n    except requests.HTTPError as exc:\n        logger.critical(\n            \"HTTP error %s while getting %s\", exc.response.status_code, link,\n        )\n        raise\n\n    content_type = resp.headers.get('content-type', '')\n    filename = link.filename  # fallback\n    # Have a look at the Content-Disposition header for a better guess\n    content_disposition = resp.headers.get('content-disposition')\n    if content_disposition:\n        type, params = cgi.parse_header(content_disposition)\n        # We use ``or`` here because we don't want to use an \"empty\" value\n        # from the filename param.\n        filename = params.get('filename') or filename\n    ext = splitext(filename)[1]\n    if not ext:\n        ext = mimetypes.guess_extension(content_type)\n        if ext:\n            filename += ext\n    if not ext and link.url != resp.url:\n        ext = os.path.splitext(resp.url)[1]\n        if ext:\n            filename += ext\n    file_path = os.path.join(temp_dir, filename)\n    with open(file_path, 'wb') as content_file:\n        _download_url(resp, link, content_file, hashes, progress_bar)\n    return file_path, content_type\n\n\ndef _check_download_dir(link, download_dir, hashes):\n    # type: (Link, str, Hashes) -> Optional[str]\n    \"\"\" Check download_dir for previously downloaded file with correct hash\n        If a correct file is found return its path else None\n    \"\"\"\n    download_path = os.path.join(download_dir, link.filename)\n    if os.path.exists(download_path):\n        # If already downloaded, does its hash match?\n        logger.info('File was already downloaded %s', download_path)\n        if hashes:\n            try:\n                hashes.check_against_path(download_path)\n            except HashMismatch:\n                logger.warning(\n                    'Previously-downloaded file %s has bad hash. '\n                    'Re-downloading.',\n                    download_path\n                )\n                os.unlink(download_path)\n                return None\n        return download_path\n    return None\n", "patch": "@@ -66,7 +66,8 @@\n            'is_url', 'url_to_path', 'path_to_url',\n            'is_archive_file', 'unpack_vcs_link',\n            'unpack_file_url', 'is_vcs_url', 'is_file_url',\n-           'unpack_http_url', 'unpack_url']\n+           'unpack_http_url', 'unpack_url',\n+           'parse_content_disposition', 'sanitize_content_filename']\n \n \n logger = logging.getLogger(__name__)\n@@ -1050,6 +1051,29 @@ def unpack_url(\n         write_delete_marker_file(location)\n \n \n+def sanitize_content_filename(filename):\n+    # type: (str) -> str\n+    \"\"\"\n+    Sanitize the \"filename\" value from a Content-Disposition header.\n+    \"\"\"\n+    return os.path.basename(filename)\n+\n+\n+def parse_content_disposition(content_disposition, default_filename):\n+    # type: (str, str) -> str\n+    \"\"\"\n+    Parse the \"filename\" value from a Content-Disposition header, and\n+    return the default filename if the result is empty.\n+    \"\"\"\n+    _type, params = cgi.parse_header(content_disposition)\n+    filename = params.get('filename')\n+    if filename:\n+        # We need to sanitize the filename to prevent directory traversal\n+        # in case the filename contains \"..\" path parts.\n+        filename = sanitize_content_filename(filename)\n+    return filename or default_filename\n+\n+\n def _download_http_url(\n     link,  # type: Link\n     session,  # type: PipSession\n@@ -1097,10 +1121,7 @@ def _download_http_url(\n     # Have a look at the Content-Disposition header for a better guess\n     content_disposition = resp.headers.get('content-disposition')\n     if content_disposition:\n-        type, params = cgi.parse_header(content_disposition)\n-        # We use ``or`` here because we don't want to use an \"empty\" value\n-        # from the filename param.\n-        filename = params.get('filename') or filename\n+        filename = parse_content_disposition(content_disposition, filename)\n     ext = splitext(filename)[1]\n     if not ext:\n         ext = mimetypes.guess_extension(content_type)", "file_path": "files/2020_9/385", "file_language": "py", "file_name": "src/pip/_internal/download.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/gzpan123/pip/raw/a4c735b14a62f9cb864533808ac63936704f2ace/tests%2Funit%2Ftest_download.py", "code": "import functools\nimport hashlib\nimport os\nimport sys\nfrom io import BytesIO\nfrom shutil import copy, rmtree\nfrom tempfile import mkdtemp\n\nimport pytest\nfrom mock import Mock, patch\n\nimport pip\nfrom pip._internal.download import (\n    CI_ENVIRONMENT_VARIABLES, MultiDomainBasicAuth, PipSession, SafeFileCache,\n    _download_http_url, parse_content_disposition, sanitize_content_filename,\n    unpack_file_url, unpack_http_url, url_to_path,\n)\nfrom pip._internal.exceptions import HashMismatch\nfrom pip._internal.models.link import Link\nfrom pip._internal.utils.hashes import Hashes\nfrom pip._internal.utils.misc import path_to_url\nfrom tests.lib import create_file\n\n\n@pytest.fixture(scope=\"function\")\ndef cache_tmpdir(tmpdir):\n    cache_dir = tmpdir.join(\"cache\")\n    cache_dir.makedirs()\n    yield cache_dir\n\n\ndef test_unpack_http_url_with_urllib_response_without_content_type(data):\n    \"\"\"\n    It should download and unpack files even if no Content-Type header exists\n    \"\"\"\n    _real_session = PipSession()\n\n    def _fake_session_get(*args, **kwargs):\n        resp = _real_session.get(*args, **kwargs)\n        del resp.headers[\"Content-Type\"]\n        return resp\n\n    session = Mock()\n    session.get = _fake_session_get\n\n    uri = path_to_url(data.packages.join(\"simple-1.0.tar.gz\"))\n    link = Link(uri)\n    temp_dir = mkdtemp()\n    try:\n        unpack_http_url(\n            link,\n            temp_dir,\n            download_dir=None,\n            session=session,\n        )\n        assert set(os.listdir(temp_dir)) == {\n            'PKG-INFO', 'setup.cfg', 'setup.py', 'simple', 'simple.egg-info'\n        }\n    finally:\n        rmtree(temp_dir)\n\n\ndef get_user_agent():\n    return PipSession().headers[\"User-Agent\"]\n\n\ndef test_user_agent():\n    user_agent = get_user_agent()\n\n    assert user_agent.startswith(\"pip/%s\" % pip.__version__)\n\n\n@pytest.mark.parametrize('name, expected_like_ci', [\n    ('BUILD_BUILDID', True),\n    ('BUILD_ID', True),\n    ('CI', True),\n    ('PIP_IS_CI', True),\n    # Test a prefix substring of one of the variable names we use.\n    ('BUILD', False),\n])\ndef test_user_agent__ci(monkeypatch, name, expected_like_ci):\n    # Delete the variable names we use to check for CI to prevent the\n    # detection from always returning True in case the tests are being run\n    # under actual CI.  It is okay to depend on CI_ENVIRONMENT_VARIABLES\n    # here (part of the code under test) because this setup step can only\n    # prevent false test failures.  It can't cause a false test passage.\n    for ci_name in CI_ENVIRONMENT_VARIABLES:\n        monkeypatch.delenv(ci_name, raising=False)\n\n    # Confirm the baseline before setting the environment variable.\n    user_agent = get_user_agent()\n    assert '\"ci\":null' in user_agent\n    assert '\"ci\":true' not in user_agent\n\n    monkeypatch.setenv(name, 'true')\n    user_agent = get_user_agent()\n    assert ('\"ci\":true' in user_agent) == expected_like_ci\n    assert ('\"ci\":null' in user_agent) == (not expected_like_ci)\n\n\ndef test_user_agent_user_data(monkeypatch):\n    monkeypatch.setenv(\"PIP_USER_AGENT_USER_DATA\", \"some_string\")\n    assert \"some_string\" in PipSession().headers[\"User-Agent\"]\n\n\nclass FakeStream(object):\n\n    def __init__(self, contents):\n        self._io = BytesIO(contents)\n\n    def read(self, size, decode_content=None):\n        return self._io.read(size)\n\n    def stream(self, size, decode_content=None):\n        yield self._io.read(size)\n\n    def release_conn(self):\n        pass\n\n\nclass MockResponse(object):\n\n    def __init__(self, contents):\n        self.raw = FakeStream(contents)\n        self.content = contents\n        self.request = None\n        self.status_code = 200\n        self.connection = None\n        self.url = None\n        self.headers = {}\n        self.history = []\n\n    def raise_for_status(self):\n        pass\n\n\nclass MockConnection(object):\n\n    def _send(self, req, **kwargs):\n        raise NotImplementedError(\"_send must be overridden for tests\")\n\n    def send(self, req, **kwargs):\n        resp = self._send(req, **kwargs)\n        for cb in req.hooks.get(\"response\", []):\n            cb(resp)\n        return resp\n\n\nclass MockRequest(object):\n\n    def __init__(self, url):\n        self.url = url\n        self.headers = {}\n        self.hooks = {}\n\n    def register_hook(self, event_name, callback):\n        self.hooks.setdefault(event_name, []).append(callback)\n\n\n@patch('pip._internal.download.unpack_file')\ndef test_unpack_http_url_bad_downloaded_checksum(mock_unpack_file):\n    \"\"\"\n    If already-downloaded file has bad checksum, re-download.\n    \"\"\"\n    base_url = 'http://www.example.com/somepackage.tgz'\n    contents = b'downloaded'\n    download_hash = hashlib.new('sha1', contents)\n    link = Link(base_url + '#sha1=' + download_hash.hexdigest())\n\n    session = Mock()\n    session.get = Mock()\n    response = session.get.return_value = MockResponse(contents)\n    response.headers = {'content-type': 'application/x-tar'}\n    response.url = base_url\n\n    download_dir = mkdtemp()\n    try:\n        downloaded_file = os.path.join(download_dir, 'somepackage.tgz')\n        create_file(downloaded_file, 'some contents')\n\n        unpack_http_url(\n            link,\n            'location',\n            download_dir=download_dir,\n            session=session,\n            hashes=Hashes({'sha1': [download_hash.hexdigest()]})\n        )\n\n        # despite existence of downloaded file with bad hash, downloaded again\n        session.get.assert_called_once_with(\n            'http://www.example.com/somepackage.tgz',\n            headers={\"Accept-Encoding\": \"identity\"},\n            stream=True,\n        )\n        # cached file is replaced with newly downloaded file\n        with open(downloaded_file) as fh:\n            assert fh.read() == 'downloaded'\n\n    finally:\n        rmtree(download_dir)\n\n\n@pytest.mark.parametrize(\"filename, expected\", [\n    ('dir/file', 'file'),\n    ('../file', 'file'),\n    ('../../file', 'file'),\n    ('../', ''),\n    ('../..', '..'),\n    ('/', ''),\n])\ndef test_sanitize_content_filename(filename, expected):\n    \"\"\"\n    Test inputs where the result is the same for Windows and non-Windows.\n    \"\"\"\n    assert sanitize_content_filename(filename) == expected\n\n\n@pytest.mark.parametrize(\"filename, win_expected, non_win_expected\", [\n    ('dir\\\\file', 'file', 'dir\\\\file'),\n    ('..\\\\file', 'file', '..\\\\file'),\n    ('..\\\\..\\\\file', 'file', '..\\\\..\\\\file'),\n    ('..\\\\', '', '..\\\\'),\n    ('..\\\\..', '..', '..\\\\..'),\n    ('\\\\', '', '\\\\'),\n])\ndef test_sanitize_content_filename__platform_dependent(\n    filename,\n    win_expected,\n    non_win_expected\n):\n    \"\"\"\n    Test inputs where the result is different for Windows and non-Windows.\n    \"\"\"\n    if sys.platform == 'win32':\n        expected = win_expected\n    else:\n        expected = non_win_expected\n    assert sanitize_content_filename(filename) == expected\n\n\n@pytest.mark.parametrize(\"content_disposition, default_filename, expected\", [\n    ('attachment;filename=\"../file\"', 'df', 'file'),\n])\ndef test_parse_content_disposition(\n    content_disposition,\n    default_filename,\n    expected\n):\n    actual = parse_content_disposition(content_disposition, default_filename)\n    assert actual == expected\n\n\ndef test_download_http_url__no_directory_traversal(tmpdir):\n    \"\"\"\n    Test that directory traversal doesn't happen on download when the\n    Content-Disposition header contains a filename with a \"..\" path part.\n    \"\"\"\n    mock_url = 'http://www.example.com/whatever.tgz'\n    contents = b'downloaded'\n    link = Link(mock_url)\n\n    session = Mock()\n    resp = MockResponse(contents)\n    resp.url = mock_url\n    resp.headers = {\n        # Set the content-type to a random value to prevent\n        # mimetypes.guess_extension from guessing the extension.\n        'content-type': 'random',\n        'content-disposition': 'attachment;filename=\"../out_dir_file\"'\n    }\n    session.get.return_value = resp\n\n    download_dir = tmpdir.join('download')\n    os.mkdir(download_dir)\n    file_path, content_type = _download_http_url(\n        link,\n        session,\n        download_dir,\n        hashes=None,\n        progress_bar='on',\n    )\n    # The file should be downloaded to download_dir.\n    actual = os.listdir(download_dir)\n    assert actual == ['out_dir_file']\n\n\n@pytest.mark.parametrize(\"url,win_expected,non_win_expected\", [\n    ('file:tmp', 'tmp', 'tmp'),\n    ('file:c:/path/to/file', r'C:\\path\\to\\file', 'c:/path/to/file'),\n    ('file:/path/to/file', r'\\path\\to\\file', '/path/to/file'),\n    ('file://localhost/tmp/file', r'\\tmp\\file', '/tmp/file'),\n    ('file://localhost/c:/tmp/file', r'C:\\tmp\\file', '/c:/tmp/file'),\n    ('file://somehost/tmp/file', r'\\\\somehost\\tmp\\file', None),\n    ('file:///tmp/file', r'\\tmp\\file', '/tmp/file'),\n    ('file:///c:/tmp/file', r'C:\\tmp\\file', '/c:/tmp/file'),\n])\ndef test_url_to_path(url, win_expected, non_win_expected):\n    if sys.platform == 'win32':\n        expected_path = win_expected\n    else:\n        expected_path = non_win_expected\n\n    if expected_path is None:\n        with pytest.raises(ValueError):\n            url_to_path(url)\n    else:\n        assert url_to_path(url) == expected_path\n\n\n@pytest.mark.skipif(\"sys.platform != 'win32'\")\ndef test_url_to_path_path_to_url_symmetry_win():\n    path = r'C:\\tmp\\file'\n    assert url_to_path(path_to_url(path)) == path\n\n    unc_path = r'\\\\unc\\share\\path'\n    assert url_to_path(path_to_url(unc_path)) == unc_path\n\n\nclass Test_unpack_file_url(object):\n\n    def prep(self, tmpdir, data):\n        self.build_dir = tmpdir.join('build')\n        self.download_dir = tmpdir.join('download')\n        os.mkdir(self.build_dir)\n        os.mkdir(self.download_dir)\n        self.dist_file = \"simple-1.0.tar.gz\"\n        self.dist_file2 = \"simple-2.0.tar.gz\"\n        self.dist_path = data.packages.join(self.dist_file)\n        self.dist_path2 = data.packages.join(self.dist_file2)\n        self.dist_url = Link(path_to_url(self.dist_path))\n        self.dist_url2 = Link(path_to_url(self.dist_path2))\n\n    def test_unpack_file_url_no_download(self, tmpdir, data):\n        self.prep(tmpdir, data)\n        unpack_file_url(self.dist_url, self.build_dir)\n        assert os.path.isdir(os.path.join(self.build_dir, 'simple'))\n        assert not os.path.isfile(\n            os.path.join(self.download_dir, self.dist_file))\n\n    def test_unpack_file_url_and_download(self, tmpdir, data):\n        self.prep(tmpdir, data)\n        unpack_file_url(self.dist_url, self.build_dir,\n                        download_dir=self.download_dir)\n        assert os.path.isdir(os.path.join(self.build_dir, 'simple'))\n        assert os.path.isfile(os.path.join(self.download_dir, self.dist_file))\n\n    def test_unpack_file_url_download_already_exists(self, tmpdir,\n                                                     data, monkeypatch):\n        self.prep(tmpdir, data)\n        # add in previous download (copy simple-2.0 as simple-1.0)\n        # so we can tell it didn't get overwritten\n        dest_file = os.path.join(self.download_dir, self.dist_file)\n        copy(self.dist_path2, dest_file)\n        with open(self.dist_path2, 'rb') as f:\n            dist_path2_md5 = hashlib.md5(f.read()).hexdigest()\n\n        unpack_file_url(self.dist_url, self.build_dir,\n                        download_dir=self.download_dir)\n        # our hash should be the same, i.e. not overwritten by simple-1.0 hash\n        with open(dest_file, 'rb') as f:\n            assert dist_path2_md5 == hashlib.md5(f.read()).hexdigest()\n\n    def test_unpack_file_url_bad_hash(self, tmpdir, data,\n                                      monkeypatch):\n        \"\"\"\n        Test when the file url hash fragment is wrong\n        \"\"\"\n        self.prep(tmpdir, data)\n        self.dist_url.url = \"%s#md5=bogus\" % self.dist_url.url\n        with pytest.raises(HashMismatch):\n            unpack_file_url(self.dist_url,\n                            self.build_dir,\n                            hashes=Hashes({'md5': ['bogus']}))\n\n    def test_unpack_file_url_download_bad_hash(self, tmpdir, data,\n                                               monkeypatch):\n        \"\"\"\n        Test when existing download has different hash from the file url\n        fragment\n        \"\"\"\n        self.prep(tmpdir, data)\n\n        # add in previous download (copy simple-2.0 as simple-1.0 so it's wrong\n        # hash)\n        dest_file = os.path.join(self.download_dir, self.dist_file)\n        copy(self.dist_path2, dest_file)\n\n        with open(self.dist_path, 'rb') as f:\n            dist_path_md5 = hashlib.md5(f.read()).hexdigest()\n        with open(dest_file, 'rb') as f:\n            dist_path2_md5 = hashlib.md5(f.read()).hexdigest()\n\n        assert dist_path_md5 != dist_path2_md5\n\n        self.dist_url.url = \"%s#md5=%s\" % (\n            self.dist_url.url,\n            dist_path_md5\n        )\n        unpack_file_url(self.dist_url, self.build_dir,\n                        download_dir=self.download_dir,\n                        hashes=Hashes({'md5': [dist_path_md5]}))\n\n        # confirm hash is for simple1-1.0\n        # the previous bad download has been removed\n        with open(dest_file, 'rb') as f:\n            assert hashlib.md5(f.read()).hexdigest() == dist_path_md5\n\n    def test_unpack_file_url_thats_a_dir(self, tmpdir, data):\n        self.prep(tmpdir, data)\n        dist_path = data.packages.join(\"FSPkg\")\n        dist_url = Link(path_to_url(dist_path))\n        unpack_file_url(dist_url, self.build_dir,\n                        download_dir=self.download_dir)\n        assert os.path.isdir(os.path.join(self.build_dir, 'fspkg'))\n\n\nclass TestSafeFileCache:\n    \"\"\"\n    The no_perms test are useless on Windows since SafeFileCache uses\n    pip._internal.utils.filesystem.check_path_owner which is based on\n    os.geteuid which is absent on Windows.\n    \"\"\"\n\n    def test_cache_roundtrip(self, cache_tmpdir):\n\n        cache = SafeFileCache(cache_tmpdir)\n        assert cache.get(\"test key\") is None\n        cache.set(\"test key\", b\"a test string\")\n        assert cache.get(\"test key\") == b\"a test string\"\n        cache.delete(\"test key\")\n        assert cache.get(\"test key\") is None\n\n    @pytest.mark.skipif(\"sys.platform == 'win32'\")\n    def test_safe_get_no_perms(self, cache_tmpdir, monkeypatch):\n        os.chmod(cache_tmpdir, 000)\n\n        monkeypatch.setattr(os.path, \"exists\", lambda x: True)\n\n        cache = SafeFileCache(cache_tmpdir)\n        cache.get(\"foo\")\n\n    @pytest.mark.skipif(\"sys.platform == 'win32'\")\n    def test_safe_set_no_perms(self, cache_tmpdir):\n        os.chmod(cache_tmpdir, 000)\n\n        cache = SafeFileCache(cache_tmpdir)\n        cache.set(\"foo\", b\"bar\")\n\n    @pytest.mark.skipif(\"sys.platform == 'win32'\")\n    def test_safe_delete_no_perms(self, cache_tmpdir):\n        os.chmod(cache_tmpdir, 000)\n\n        cache = SafeFileCache(cache_tmpdir)\n        cache.delete(\"foo\")\n\n\nclass TestPipSession:\n\n    def test_cache_defaults_off(self):\n        session = PipSession()\n\n        assert not hasattr(session.adapters[\"http://\"], \"cache\")\n        assert not hasattr(session.adapters[\"https://\"], \"cache\")\n\n    def test_cache_is_enabled(self, tmpdir):\n        session = PipSession(cache=tmpdir.join(\"test-cache\"))\n\n        assert hasattr(session.adapters[\"https://\"], \"cache\")\n\n        assert (session.adapters[\"https://\"].cache.directory ==\n                tmpdir.join(\"test-cache\"))\n\n    def test_http_cache_is_not_enabled(self, tmpdir):\n        session = PipSession(cache=tmpdir.join(\"test-cache\"))\n\n        assert not hasattr(session.adapters[\"http://\"], \"cache\")\n\n    def test_insecure_host_cache_is_not_enabled(self, tmpdir):\n        session = PipSession(\n            cache=tmpdir.join(\"test-cache\"),\n            insecure_hosts=[\"example.com\"],\n        )\n\n        assert not hasattr(session.adapters[\"https://example.com/\"], \"cache\")\n\n\ndef test_get_credentials():\n    auth = MultiDomainBasicAuth()\n    get = auth._get_url_and_credentials\n\n    # Check URL parsing\n    assert get(\"http://foo:bar@example.com/path\") \\\n        == ('http://example.com/path', 'foo', 'bar')\n    assert auth.passwords['example.com'] == ('foo', 'bar')\n\n    auth.passwords['example.com'] = ('user', 'pass')\n    assert get(\"http://foo:bar@example.com/path\") \\\n        == ('http://example.com/path', 'user', 'pass')\n\n\ndef test_get_index_url_credentials():\n    auth = MultiDomainBasicAuth(index_urls=[\n        \"http://foo:bar@example.com/path\"\n    ])\n    get = functools.partial(\n        auth._get_new_credentials,\n        allow_netrc=False,\n        allow_keyring=False\n    )\n\n    # Check resolution of indexes\n    assert get(\"http://example.com/path/path2\") == ('foo', 'bar')\n    assert get(\"http://example.com/path3/path2\") == (None, None)\n\n\nclass KeyringModuleV1(object):\n    \"\"\"Represents the supported API of keyring before get_credential\n    was added.\n    \"\"\"\n\n    def __init__(self):\n        self.saved_passwords = []\n\n    def get_password(self, system, username):\n        if system == \"example.com\" and username:\n            return username + \"!netloc\"\n        if system == \"http://example.com/path2\" and username:\n            return username + \"!url\"\n        return None\n\n    def set_password(self, system, username, password):\n        self.saved_passwords.append((system, username, password))\n\n\n@pytest.mark.parametrize('url, expect', (\n    (\"http://example.com/path1\", (None, None)),\n    # path1 URLs will be resolved by netloc\n    (\"http://user@example.com/path1\", (\"user\", \"user!netloc\")),\n    (\"http://user2@example.com/path1\", (\"user2\", \"user2!netloc\")),\n    # path2 URLs will be resolved by index URL\n    (\"http://example.com/path2/path3\", (None, None)),\n    (\"http://foo@example.com/path2/path3\", (\"foo\", \"foo!url\")),\n))\ndef test_keyring_get_password(monkeypatch, url, expect):\n    monkeypatch.setattr('pip._internal.download.keyring', KeyringModuleV1())\n    auth = MultiDomainBasicAuth(index_urls=[\"http://example.com/path2\"])\n\n    actual = auth._get_new_credentials(url, allow_netrc=False,\n                                       allow_keyring=True)\n    assert actual == expect\n\n\ndef test_keyring_get_password_after_prompt(monkeypatch):\n    monkeypatch.setattr('pip._internal.download.keyring', KeyringModuleV1())\n    auth = MultiDomainBasicAuth()\n\n    def ask_input(prompt):\n        assert prompt == \"User for example.com: \"\n        return \"user\"\n\n    monkeypatch.setattr('pip._internal.download.ask_input', ask_input)\n    actual = auth._prompt_for_password(\"example.com\")\n    assert actual == (\"user\", \"user!netloc\", False)\n\n\ndef test_keyring_get_password_username_in_index(monkeypatch):\n    monkeypatch.setattr('pip._internal.download.keyring', KeyringModuleV1())\n    auth = MultiDomainBasicAuth(index_urls=[\"http://user@example.com/path2\"])\n    get = functools.partial(\n        auth._get_new_credentials,\n        allow_netrc=False,\n        allow_keyring=True\n    )\n\n    assert get(\"http://example.com/path2/path3\") == (\"user\", \"user!url\")\n    assert get(\"http://example.com/path4/path1\") == (None, None)\n\n\n@pytest.mark.parametrize(\"response_status, creds, expect_save\", (\n    (403, (\"user\", \"pass\", True), False),\n    (200, (\"user\", \"pass\", True), True,),\n    (200, (\"user\", \"pass\", False), False,),\n))\ndef test_keyring_set_password(monkeypatch, response_status, creds,\n                              expect_save):\n    keyring = KeyringModuleV1()\n    monkeypatch.setattr('pip._internal.download.keyring', keyring)\n    auth = MultiDomainBasicAuth(prompting=True)\n    monkeypatch.setattr(auth, '_get_url_and_credentials',\n                        lambda u: (u, None, None))\n    monkeypatch.setattr(auth, '_prompt_for_password', lambda *a: creds)\n    if creds[2]:\n        # when _prompt_for_password indicates to save, we should save\n        def should_save_password_to_keyring(*a):\n            return True\n    else:\n        # when _prompt_for_password indicates not to save, we should\n        # never call this function\n        def should_save_password_to_keyring(*a):\n            assert False, (\"_should_save_password_to_keyring should not be \" +\n                           \"called\")\n    monkeypatch.setattr(auth, '_should_save_password_to_keyring',\n                        should_save_password_to_keyring)\n\n    req = MockRequest(\"https://example.com\")\n    resp = MockResponse(b\"\")\n    resp.url = req.url\n    connection = MockConnection()\n\n    def _send(sent_req, **kwargs):\n        assert sent_req is req\n        assert \"Authorization\" in sent_req.headers\n        r = MockResponse(b\"\")\n        r.status_code = response_status\n        return r\n\n    connection._send = _send\n\n    resp.request = req\n    resp.status_code = 401\n    resp.connection = connection\n\n    auth.handle_401(resp)\n\n    if expect_save:\n        assert keyring.saved_passwords == [(\"example.com\", creds[0], creds[1])]\n    else:\n        assert keyring.saved_passwords == []\n\n\nclass KeyringModuleV2(object):\n    \"\"\"Represents the current supported API of keyring\"\"\"\n\n    class Credential(object):\n        def __init__(self, username, password):\n            self.username = username\n            self.password = password\n\n    def get_password(self, system, username):\n        assert False, \"get_password should not ever be called\"\n\n    def get_credential(self, system, username):\n        if system == \"http://example.com/path2\":\n            return self.Credential(\"username\", \"url\")\n        if system == \"example.com\":\n            return self.Credential(\"username\", \"netloc\")\n        return None\n\n\n@pytest.mark.parametrize('url, expect', (\n    (\"http://example.com/path1\", (\"username\", \"netloc\")),\n    (\"http://example.com/path2/path3\", (\"username\", \"url\")),\n    (\"http://user2@example.com/path2/path3\", (\"username\", \"url\")),\n))\ndef test_keyring_get_credential(monkeypatch, url, expect):\n    monkeypatch.setattr(pip._internal.download, 'keyring', KeyringModuleV2())\n    auth = MultiDomainBasicAuth(index_urls=[\"http://example.com/path2\"])\n\n    assert auth._get_new_credentials(url, allow_netrc=False,\n                                     allow_keyring=True) \\\n        == expect\n", "code_before": "import functools\nimport hashlib\nimport os\nimport sys\nfrom io import BytesIO\nfrom shutil import copy, rmtree\nfrom tempfile import mkdtemp\n\nimport pytest\nfrom mock import Mock, patch\n\nimport pip\nfrom pip._internal.download import (\n    CI_ENVIRONMENT_VARIABLES, MultiDomainBasicAuth, PipSession, SafeFileCache,\n    unpack_file_url, unpack_http_url, url_to_path,\n)\nfrom pip._internal.exceptions import HashMismatch\nfrom pip._internal.models.link import Link\nfrom pip._internal.utils.hashes import Hashes\nfrom pip._internal.utils.misc import path_to_url\nfrom tests.lib import create_file\n\n\n@pytest.fixture(scope=\"function\")\ndef cache_tmpdir(tmpdir):\n    cache_dir = tmpdir.join(\"cache\")\n    cache_dir.makedirs()\n    yield cache_dir\n\n\ndef test_unpack_http_url_with_urllib_response_without_content_type(data):\n    \"\"\"\n    It should download and unpack files even if no Content-Type header exists\n    \"\"\"\n    _real_session = PipSession()\n\n    def _fake_session_get(*args, **kwargs):\n        resp = _real_session.get(*args, **kwargs)\n        del resp.headers[\"Content-Type\"]\n        return resp\n\n    session = Mock()\n    session.get = _fake_session_get\n\n    uri = path_to_url(data.packages.join(\"simple-1.0.tar.gz\"))\n    link = Link(uri)\n    temp_dir = mkdtemp()\n    try:\n        unpack_http_url(\n            link,\n            temp_dir,\n            download_dir=None,\n            session=session,\n        )\n        assert set(os.listdir(temp_dir)) == {\n            'PKG-INFO', 'setup.cfg', 'setup.py', 'simple', 'simple.egg-info'\n        }\n    finally:\n        rmtree(temp_dir)\n\n\ndef get_user_agent():\n    return PipSession().headers[\"User-Agent\"]\n\n\ndef test_user_agent():\n    user_agent = get_user_agent()\n\n    assert user_agent.startswith(\"pip/%s\" % pip.__version__)\n\n\n@pytest.mark.parametrize('name, expected_like_ci', [\n    ('BUILD_BUILDID', True),\n    ('BUILD_ID', True),\n    ('CI', True),\n    ('PIP_IS_CI', True),\n    # Test a prefix substring of one of the variable names we use.\n    ('BUILD', False),\n])\ndef test_user_agent__ci(monkeypatch, name, expected_like_ci):\n    # Delete the variable names we use to check for CI to prevent the\n    # detection from always returning True in case the tests are being run\n    # under actual CI.  It is okay to depend on CI_ENVIRONMENT_VARIABLES\n    # here (part of the code under test) because this setup step can only\n    # prevent false test failures.  It can't cause a false test passage.\n    for ci_name in CI_ENVIRONMENT_VARIABLES:\n        monkeypatch.delenv(ci_name, raising=False)\n\n    # Confirm the baseline before setting the environment variable.\n    user_agent = get_user_agent()\n    assert '\"ci\":null' in user_agent\n    assert '\"ci\":true' not in user_agent\n\n    monkeypatch.setenv(name, 'true')\n    user_agent = get_user_agent()\n    assert ('\"ci\":true' in user_agent) == expected_like_ci\n    assert ('\"ci\":null' in user_agent) == (not expected_like_ci)\n\n\ndef test_user_agent_user_data(monkeypatch):\n    monkeypatch.setenv(\"PIP_USER_AGENT_USER_DATA\", \"some_string\")\n    assert \"some_string\" in PipSession().headers[\"User-Agent\"]\n\n\nclass FakeStream(object):\n\n    def __init__(self, contents):\n        self._io = BytesIO(contents)\n\n    def read(self, size, decode_content=None):\n        return self._io.read(size)\n\n    def stream(self, size, decode_content=None):\n        yield self._io.read(size)\n\n    def release_conn(self):\n        pass\n\n\nclass MockResponse(object):\n\n    def __init__(self, contents):\n        self.raw = FakeStream(contents)\n        self.content = contents\n        self.request = None\n        self.status_code = 200\n        self.connection = None\n        self.url = None\n        self.headers = {}\n        self.history = []\n\n    def raise_for_status(self):\n        pass\n\n\nclass MockConnection(object):\n\n    def _send(self, req, **kwargs):\n        raise NotImplementedError(\"_send must be overridden for tests\")\n\n    def send(self, req, **kwargs):\n        resp = self._send(req, **kwargs)\n        for cb in req.hooks.get(\"response\", []):\n            cb(resp)\n        return resp\n\n\nclass MockRequest(object):\n\n    def __init__(self, url):\n        self.url = url\n        self.headers = {}\n        self.hooks = {}\n\n    def register_hook(self, event_name, callback):\n        self.hooks.setdefault(event_name, []).append(callback)\n\n\n@patch('pip._internal.download.unpack_file')\ndef test_unpack_http_url_bad_downloaded_checksum(mock_unpack_file):\n    \"\"\"\n    If already-downloaded file has bad checksum, re-download.\n    \"\"\"\n    base_url = 'http://www.example.com/somepackage.tgz'\n    contents = b'downloaded'\n    download_hash = hashlib.new('sha1', contents)\n    link = Link(base_url + '#sha1=' + download_hash.hexdigest())\n\n    session = Mock()\n    session.get = Mock()\n    response = session.get.return_value = MockResponse(contents)\n    response.headers = {'content-type': 'application/x-tar'}\n    response.url = base_url\n\n    download_dir = mkdtemp()\n    try:\n        downloaded_file = os.path.join(download_dir, 'somepackage.tgz')\n        create_file(downloaded_file, 'some contents')\n\n        unpack_http_url(\n            link,\n            'location',\n            download_dir=download_dir,\n            session=session,\n            hashes=Hashes({'sha1': [download_hash.hexdigest()]})\n        )\n\n        # despite existence of downloaded file with bad hash, downloaded again\n        session.get.assert_called_once_with(\n            'http://www.example.com/somepackage.tgz',\n            headers={\"Accept-Encoding\": \"identity\"},\n            stream=True,\n        )\n        # cached file is replaced with newly downloaded file\n        with open(downloaded_file) as fh:\n            assert fh.read() == 'downloaded'\n\n    finally:\n        rmtree(download_dir)\n\n\n@pytest.mark.parametrize(\"url,win_expected,non_win_expected\", [\n    ('file:tmp', 'tmp', 'tmp'),\n    ('file:c:/path/to/file', r'C:\\path\\to\\file', 'c:/path/to/file'),\n    ('file:/path/to/file', r'\\path\\to\\file', '/path/to/file'),\n    ('file://localhost/tmp/file', r'\\tmp\\file', '/tmp/file'),\n    ('file://localhost/c:/tmp/file', r'C:\\tmp\\file', '/c:/tmp/file'),\n    ('file://somehost/tmp/file', r'\\\\somehost\\tmp\\file', None),\n    ('file:///tmp/file', r'\\tmp\\file', '/tmp/file'),\n    ('file:///c:/tmp/file', r'C:\\tmp\\file', '/c:/tmp/file'),\n])\ndef test_url_to_path(url, win_expected, non_win_expected):\n    if sys.platform == 'win32':\n        expected_path = win_expected\n    else:\n        expected_path = non_win_expected\n\n    if expected_path is None:\n        with pytest.raises(ValueError):\n            url_to_path(url)\n    else:\n        assert url_to_path(url) == expected_path\n\n\n@pytest.mark.skipif(\"sys.platform != 'win32'\")\ndef test_url_to_path_path_to_url_symmetry_win():\n    path = r'C:\\tmp\\file'\n    assert url_to_path(path_to_url(path)) == path\n\n    unc_path = r'\\\\unc\\share\\path'\n    assert url_to_path(path_to_url(unc_path)) == unc_path\n\n\nclass Test_unpack_file_url(object):\n\n    def prep(self, tmpdir, data):\n        self.build_dir = tmpdir.join('build')\n        self.download_dir = tmpdir.join('download')\n        os.mkdir(self.build_dir)\n        os.mkdir(self.download_dir)\n        self.dist_file = \"simple-1.0.tar.gz\"\n        self.dist_file2 = \"simple-2.0.tar.gz\"\n        self.dist_path = data.packages.join(self.dist_file)\n        self.dist_path2 = data.packages.join(self.dist_file2)\n        self.dist_url = Link(path_to_url(self.dist_path))\n        self.dist_url2 = Link(path_to_url(self.dist_path2))\n\n    def test_unpack_file_url_no_download(self, tmpdir, data):\n        self.prep(tmpdir, data)\n        unpack_file_url(self.dist_url, self.build_dir)\n        assert os.path.isdir(os.path.join(self.build_dir, 'simple'))\n        assert not os.path.isfile(\n            os.path.join(self.download_dir, self.dist_file))\n\n    def test_unpack_file_url_and_download(self, tmpdir, data):\n        self.prep(tmpdir, data)\n        unpack_file_url(self.dist_url, self.build_dir,\n                        download_dir=self.download_dir)\n        assert os.path.isdir(os.path.join(self.build_dir, 'simple'))\n        assert os.path.isfile(os.path.join(self.download_dir, self.dist_file))\n\n    def test_unpack_file_url_download_already_exists(self, tmpdir,\n                                                     data, monkeypatch):\n        self.prep(tmpdir, data)\n        # add in previous download (copy simple-2.0 as simple-1.0)\n        # so we can tell it didn't get overwritten\n        dest_file = os.path.join(self.download_dir, self.dist_file)\n        copy(self.dist_path2, dest_file)\n        with open(self.dist_path2, 'rb') as f:\n            dist_path2_md5 = hashlib.md5(f.read()).hexdigest()\n\n        unpack_file_url(self.dist_url, self.build_dir,\n                        download_dir=self.download_dir)\n        # our hash should be the same, i.e. not overwritten by simple-1.0 hash\n        with open(dest_file, 'rb') as f:\n            assert dist_path2_md5 == hashlib.md5(f.read()).hexdigest()\n\n    def test_unpack_file_url_bad_hash(self, tmpdir, data,\n                                      monkeypatch):\n        \"\"\"\n        Test when the file url hash fragment is wrong\n        \"\"\"\n        self.prep(tmpdir, data)\n        self.dist_url.url = \"%s#md5=bogus\" % self.dist_url.url\n        with pytest.raises(HashMismatch):\n            unpack_file_url(self.dist_url,\n                            self.build_dir,\n                            hashes=Hashes({'md5': ['bogus']}))\n\n    def test_unpack_file_url_download_bad_hash(self, tmpdir, data,\n                                               monkeypatch):\n        \"\"\"\n        Test when existing download has different hash from the file url\n        fragment\n        \"\"\"\n        self.prep(tmpdir, data)\n\n        # add in previous download (copy simple-2.0 as simple-1.0 so it's wrong\n        # hash)\n        dest_file = os.path.join(self.download_dir, self.dist_file)\n        copy(self.dist_path2, dest_file)\n\n        with open(self.dist_path, 'rb') as f:\n            dist_path_md5 = hashlib.md5(f.read()).hexdigest()\n        with open(dest_file, 'rb') as f:\n            dist_path2_md5 = hashlib.md5(f.read()).hexdigest()\n\n        assert dist_path_md5 != dist_path2_md5\n\n        self.dist_url.url = \"%s#md5=%s\" % (\n            self.dist_url.url,\n            dist_path_md5\n        )\n        unpack_file_url(self.dist_url, self.build_dir,\n                        download_dir=self.download_dir,\n                        hashes=Hashes({'md5': [dist_path_md5]}))\n\n        # confirm hash is for simple1-1.0\n        # the previous bad download has been removed\n        with open(dest_file, 'rb') as f:\n            assert hashlib.md5(f.read()).hexdigest() == dist_path_md5\n\n    def test_unpack_file_url_thats_a_dir(self, tmpdir, data):\n        self.prep(tmpdir, data)\n        dist_path = data.packages.join(\"FSPkg\")\n        dist_url = Link(path_to_url(dist_path))\n        unpack_file_url(dist_url, self.build_dir,\n                        download_dir=self.download_dir)\n        assert os.path.isdir(os.path.join(self.build_dir, 'fspkg'))\n\n\nclass TestSafeFileCache:\n    \"\"\"\n    The no_perms test are useless on Windows since SafeFileCache uses\n    pip._internal.utils.filesystem.check_path_owner which is based on\n    os.geteuid which is absent on Windows.\n    \"\"\"\n\n    def test_cache_roundtrip(self, cache_tmpdir):\n\n        cache = SafeFileCache(cache_tmpdir)\n        assert cache.get(\"test key\") is None\n        cache.set(\"test key\", b\"a test string\")\n        assert cache.get(\"test key\") == b\"a test string\"\n        cache.delete(\"test key\")\n        assert cache.get(\"test key\") is None\n\n    @pytest.mark.skipif(\"sys.platform == 'win32'\")\n    def test_safe_get_no_perms(self, cache_tmpdir, monkeypatch):\n        os.chmod(cache_tmpdir, 000)\n\n        monkeypatch.setattr(os.path, \"exists\", lambda x: True)\n\n        cache = SafeFileCache(cache_tmpdir)\n        cache.get(\"foo\")\n\n    @pytest.mark.skipif(\"sys.platform == 'win32'\")\n    def test_safe_set_no_perms(self, cache_tmpdir):\n        os.chmod(cache_tmpdir, 000)\n\n        cache = SafeFileCache(cache_tmpdir)\n        cache.set(\"foo\", b\"bar\")\n\n    @pytest.mark.skipif(\"sys.platform == 'win32'\")\n    def test_safe_delete_no_perms(self, cache_tmpdir):\n        os.chmod(cache_tmpdir, 000)\n\n        cache = SafeFileCache(cache_tmpdir)\n        cache.delete(\"foo\")\n\n\nclass TestPipSession:\n\n    def test_cache_defaults_off(self):\n        session = PipSession()\n\n        assert not hasattr(session.adapters[\"http://\"], \"cache\")\n        assert not hasattr(session.adapters[\"https://\"], \"cache\")\n\n    def test_cache_is_enabled(self, tmpdir):\n        session = PipSession(cache=tmpdir.join(\"test-cache\"))\n\n        assert hasattr(session.adapters[\"https://\"], \"cache\")\n\n        assert (session.adapters[\"https://\"].cache.directory ==\n                tmpdir.join(\"test-cache\"))\n\n    def test_http_cache_is_not_enabled(self, tmpdir):\n        session = PipSession(cache=tmpdir.join(\"test-cache\"))\n\n        assert not hasattr(session.adapters[\"http://\"], \"cache\")\n\n    def test_insecure_host_cache_is_not_enabled(self, tmpdir):\n        session = PipSession(\n            cache=tmpdir.join(\"test-cache\"),\n            insecure_hosts=[\"example.com\"],\n        )\n\n        assert not hasattr(session.adapters[\"https://example.com/\"], \"cache\")\n\n\ndef test_get_credentials():\n    auth = MultiDomainBasicAuth()\n    get = auth._get_url_and_credentials\n\n    # Check URL parsing\n    assert get(\"http://foo:bar@example.com/path\") \\\n        == ('http://example.com/path', 'foo', 'bar')\n    assert auth.passwords['example.com'] == ('foo', 'bar')\n\n    auth.passwords['example.com'] = ('user', 'pass')\n    assert get(\"http://foo:bar@example.com/path\") \\\n        == ('http://example.com/path', 'user', 'pass')\n\n\ndef test_get_index_url_credentials():\n    auth = MultiDomainBasicAuth(index_urls=[\n        \"http://foo:bar@example.com/path\"\n    ])\n    get = functools.partial(\n        auth._get_new_credentials,\n        allow_netrc=False,\n        allow_keyring=False\n    )\n\n    # Check resolution of indexes\n    assert get(\"http://example.com/path/path2\") == ('foo', 'bar')\n    assert get(\"http://example.com/path3/path2\") == (None, None)\n\n\nclass KeyringModuleV1(object):\n    \"\"\"Represents the supported API of keyring before get_credential\n    was added.\n    \"\"\"\n\n    def __init__(self):\n        self.saved_passwords = []\n\n    def get_password(self, system, username):\n        if system == \"example.com\" and username:\n            return username + \"!netloc\"\n        if system == \"http://example.com/path2\" and username:\n            return username + \"!url\"\n        return None\n\n    def set_password(self, system, username, password):\n        self.saved_passwords.append((system, username, password))\n\n\n@pytest.mark.parametrize('url, expect', (\n    (\"http://example.com/path1\", (None, None)),\n    # path1 URLs will be resolved by netloc\n    (\"http://user@example.com/path1\", (\"user\", \"user!netloc\")),\n    (\"http://user2@example.com/path1\", (\"user2\", \"user2!netloc\")),\n    # path2 URLs will be resolved by index URL\n    (\"http://example.com/path2/path3\", (None, None)),\n    (\"http://foo@example.com/path2/path3\", (\"foo\", \"foo!url\")),\n))\ndef test_keyring_get_password(monkeypatch, url, expect):\n    monkeypatch.setattr('pip._internal.download.keyring', KeyringModuleV1())\n    auth = MultiDomainBasicAuth(index_urls=[\"http://example.com/path2\"])\n\n    actual = auth._get_new_credentials(url, allow_netrc=False,\n                                       allow_keyring=True)\n    assert actual == expect\n\n\ndef test_keyring_get_password_after_prompt(monkeypatch):\n    monkeypatch.setattr('pip._internal.download.keyring', KeyringModuleV1())\n    auth = MultiDomainBasicAuth()\n\n    def ask_input(prompt):\n        assert prompt == \"User for example.com: \"\n        return \"user\"\n\n    monkeypatch.setattr('pip._internal.download.ask_input', ask_input)\n    actual = auth._prompt_for_password(\"example.com\")\n    assert actual == (\"user\", \"user!netloc\", False)\n\n\ndef test_keyring_get_password_username_in_index(monkeypatch):\n    monkeypatch.setattr('pip._internal.download.keyring', KeyringModuleV1())\n    auth = MultiDomainBasicAuth(index_urls=[\"http://user@example.com/path2\"])\n    get = functools.partial(\n        auth._get_new_credentials,\n        allow_netrc=False,\n        allow_keyring=True\n    )\n\n    assert get(\"http://example.com/path2/path3\") == (\"user\", \"user!url\")\n    assert get(\"http://example.com/path4/path1\") == (None, None)\n\n\n@pytest.mark.parametrize(\"response_status, creds, expect_save\", (\n    (403, (\"user\", \"pass\", True), False),\n    (200, (\"user\", \"pass\", True), True,),\n    (200, (\"user\", \"pass\", False), False,),\n))\ndef test_keyring_set_password(monkeypatch, response_status, creds,\n                              expect_save):\n    keyring = KeyringModuleV1()\n    monkeypatch.setattr('pip._internal.download.keyring', keyring)\n    auth = MultiDomainBasicAuth(prompting=True)\n    monkeypatch.setattr(auth, '_get_url_and_credentials',\n                        lambda u: (u, None, None))\n    monkeypatch.setattr(auth, '_prompt_for_password', lambda *a: creds)\n    if creds[2]:\n        # when _prompt_for_password indicates to save, we should save\n        def should_save_password_to_keyring(*a):\n            return True\n    else:\n        # when _prompt_for_password indicates not to save, we should\n        # never call this function\n        def should_save_password_to_keyring(*a):\n            assert False, (\"_should_save_password_to_keyring should not be \" +\n                           \"called\")\n    monkeypatch.setattr(auth, '_should_save_password_to_keyring',\n                        should_save_password_to_keyring)\n\n    req = MockRequest(\"https://example.com\")\n    resp = MockResponse(b\"\")\n    resp.url = req.url\n    connection = MockConnection()\n\n    def _send(sent_req, **kwargs):\n        assert sent_req is req\n        assert \"Authorization\" in sent_req.headers\n        r = MockResponse(b\"\")\n        r.status_code = response_status\n        return r\n\n    connection._send = _send\n\n    resp.request = req\n    resp.status_code = 401\n    resp.connection = connection\n\n    auth.handle_401(resp)\n\n    if expect_save:\n        assert keyring.saved_passwords == [(\"example.com\", creds[0], creds[1])]\n    else:\n        assert keyring.saved_passwords == []\n\n\nclass KeyringModuleV2(object):\n    \"\"\"Represents the current supported API of keyring\"\"\"\n\n    class Credential(object):\n        def __init__(self, username, password):\n            self.username = username\n            self.password = password\n\n    def get_password(self, system, username):\n        assert False, \"get_password should not ever be called\"\n\n    def get_credential(self, system, username):\n        if system == \"http://example.com/path2\":\n            return self.Credential(\"username\", \"url\")\n        if system == \"example.com\":\n            return self.Credential(\"username\", \"netloc\")\n        return None\n\n\n@pytest.mark.parametrize('url, expect', (\n    (\"http://example.com/path1\", (\"username\", \"netloc\")),\n    (\"http://example.com/path2/path3\", (\"username\", \"url\")),\n    (\"http://user2@example.com/path2/path3\", (\"username\", \"url\")),\n))\ndef test_keyring_get_credential(monkeypatch, url, expect):\n    monkeypatch.setattr(pip._internal.download, 'keyring', KeyringModuleV2())\n    auth = MultiDomainBasicAuth(index_urls=[\"http://example.com/path2\"])\n\n    assert auth._get_new_credentials(url, allow_netrc=False,\n                                     allow_keyring=True) \\\n        == expect\n", "patch": "@@ -12,6 +12,7 @@\n import pip\n from pip._internal.download import (\n     CI_ENVIRONMENT_VARIABLES, MultiDomainBasicAuth, PipSession, SafeFileCache,\n+    _download_http_url, parse_content_disposition, sanitize_content_filename,\n     unpack_file_url, unpack_http_url, url_to_path,\n )\n from pip._internal.exceptions import HashMismatch\n@@ -199,6 +200,90 @@ def test_unpack_http_url_bad_downloaded_checksum(mock_unpack_file):\n         rmtree(download_dir)\n \n \n+@pytest.mark.parametrize(\"filename, expected\", [\n+    ('dir/file', 'file'),\n+    ('../file', 'file'),\n+    ('../../file', 'file'),\n+    ('../', ''),\n+    ('../..', '..'),\n+    ('/', ''),\n+])\n+def test_sanitize_content_filename(filename, expected):\n+    \"\"\"\n+    Test inputs where the result is the same for Windows and non-Windows.\n+    \"\"\"\n+    assert sanitize_content_filename(filename) == expected\n+\n+\n+@pytest.mark.parametrize(\"filename, win_expected, non_win_expected\", [\n+    ('dir\\\\file', 'file', 'dir\\\\file'),\n+    ('..\\\\file', 'file', '..\\\\file'),\n+    ('..\\\\..\\\\file', 'file', '..\\\\..\\\\file'),\n+    ('..\\\\', '', '..\\\\'),\n+    ('..\\\\..', '..', '..\\\\..'),\n+    ('\\\\', '', '\\\\'),\n+])\n+def test_sanitize_content_filename__platform_dependent(\n+    filename,\n+    win_expected,\n+    non_win_expected\n+):\n+    \"\"\"\n+    Test inputs where the result is different for Windows and non-Windows.\n+    \"\"\"\n+    if sys.platform == 'win32':\n+        expected = win_expected\n+    else:\n+        expected = non_win_expected\n+    assert sanitize_content_filename(filename) == expected\n+\n+\n+@pytest.mark.parametrize(\"content_disposition, default_filename, expected\", [\n+    ('attachment;filename=\"../file\"', 'df', 'file'),\n+])\n+def test_parse_content_disposition(\n+    content_disposition,\n+    default_filename,\n+    expected\n+):\n+    actual = parse_content_disposition(content_disposition, default_filename)\n+    assert actual == expected\n+\n+\n+def test_download_http_url__no_directory_traversal(tmpdir):\n+    \"\"\"\n+    Test that directory traversal doesn't happen on download when the\n+    Content-Disposition header contains a filename with a \"..\" path part.\n+    \"\"\"\n+    mock_url = 'http://www.example.com/whatever.tgz'\n+    contents = b'downloaded'\n+    link = Link(mock_url)\n+\n+    session = Mock()\n+    resp = MockResponse(contents)\n+    resp.url = mock_url\n+    resp.headers = {\n+        # Set the content-type to a random value to prevent\n+        # mimetypes.guess_extension from guessing the extension.\n+        'content-type': 'random',\n+        'content-disposition': 'attachment;filename=\"../out_dir_file\"'\n+    }\n+    session.get.return_value = resp\n+\n+    download_dir = tmpdir.join('download')\n+    os.mkdir(download_dir)\n+    file_path, content_type = _download_http_url(\n+        link,\n+        session,\n+        download_dir,\n+        hashes=None,\n+        progress_bar='on',\n+    )\n+    # The file should be downloaded to download_dir.\n+    actual = os.listdir(download_dir)\n+    assert actual == ['out_dir_file']\n+\n+\n @pytest.mark.parametrize(\"url,win_expected,non_win_expected\", [\n     ('file:tmp', 'tmp', 'tmp'),\n     ('file:c:/path/to/file', r'C:\\path\\to\\file', 'c:/path/to/file'),", "file_path": "files/2020_9/386", "file_language": "py", "file_name": "tests/unit/test_download.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
