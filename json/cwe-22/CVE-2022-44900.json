{"index": 10113, "cve_id": "CVE-2022-44900", "cwe_id": ["CWE-22"], "cve_language": "Python", "cve_description": "A directory traversal vulnerability in the SevenZipFile.extractall() function of the python library py7zr v0.20.0 and earlier allows attackers to write arbitrary files via extracting a crafted 7z file.", "cvss": "9.1", "publish_date": "December 6, 2022", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "NONE", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "NONE", "commit_id": "1bb43f17515c7f69673a1c88ab9cc72a7bbef406", "commit_message": "Fix sanity check for path traversal attack\n\n- Previous versions do not detect the attack in some case\n   - fixed it by call resolve()\n   - resolve() converts \"/hoge/fuga/../../../tmp/evil.sh\" to be \"/tmp/evil.sh\" then\n     relative_to() can detect path traversal attack.\n- Add path checker in writef() and writestr() methods\n  - When pass arcname as evil path such as \"../../../../tmp/evil.sh\"\n    it raises ValueError\n- Add test case of bad path detection\n- extraction: check symlink and junction is under target folder\n- Fix relative_path_marker removal\n- Don't put windows file namespace to output file path\n\nSigned-off-by: Hiroshi Miura <miurahr@linux.com>", "commit_date": "2022-10-31T22:04:13Z", "project": "miurahr/py7zr", "url": "https://api.github.com/repos/miurahr/py7zr/commits/1bb43f17515c7f69673a1c88ab9cc72a7bbef406", "html_url": "https://github.com/miurahr/py7zr/commit/1bb43f17515c7f69673a1c88ab9cc72a7bbef406", "windows_before": [{"commit_id": "04e3af5ae5092137ee7b015b85efac1c7fd8974e", "commit_date": "Sun Oct 30 22:42:07 2022 +0900", "commit_message": "Bump inflate64@v0.3.1", "files_name": ["pyproject.toml"]}, {"commit_id": "07a8b3298822f04a964a969aedeffc790ee1b185", "commit_date": "Tue Oct 25 08:02:52 2022 +0900", "commit_message": "Merge pull request #479 from miurahr/dependabot/pip/pyppmd-gte-0.18.1-and-lt-1.1.0", "files_name": ["adf8f57c42b26467f7ce2c0916ac5526cbc40c65 - Tue Oct 25 07:57:58 2022 +0900 : Fix mypy/isort check errors", "py7zr/compressor.py", "py7zr/helpers.py", "pyproject.toml"]}, {"commit_id": "6298527ffcc9efb0b6813cc8de11321b628a4df1", "commit_date": "Mon Oct 24 20:33:27 2022 +0000", "commit_message": "Update pyppmd requirement from <0.19.0,>=0.18.1 to >=0.18.1,<1.1.0", "files_name": ["pyproject.toml"]}, {"commit_id": "d7d509d02344adbd338500c5a9bc3698c827fc8f", "commit_date": "Tue Aug 9 07:39:50 2022 +0900", "commit_message": "Drop setup.py", "files_name": ["pyproject.toml", "setup.cfg", "setup.py"]}, {"commit_id": "e84037119fcb6c5ed6b3965d1f248c667477e603", "commit_date": "Wed Aug 3 07:23:47 2022 +0900", "commit_message": "Merge pull request #474 from miurahr/dependabot/pip/pyppmd-gte-0.18.1-and-lt-1.1.0", "files_name": ["aa0a8fbb5303b47e65ec752afb0b5760ae5e9ce8 - Tue Aug 2 20:22:20 2022 +0000 : Update pyppmd requirement from <0.19.0,>=0.18.1 to >=0.18.1,<1.1.0", "setup.cfg"]}, {"commit_id": "3030070c9792fa27d4fcfb2223edfe4986aa7962", "commit_date": "Tue Aug 2 23:19:18 2022 +0900", "commit_message": "README: remove experimental from ppmd", "files_name": ["README.rst"]}, {"commit_id": "d4eca67852b3e384b369aa47739b2376792bc9a0", "commit_date": "Tue Aug 2 21:54:52 2022 +0900", "commit_message": "Release v0.20.0", "files_name": ["Changelog.rst"]}, {"commit_id": "f68c7d76a6358860705675c1664a06dfee4416e0", "commit_date": "Tue Aug 2 21:37:20 2022 +0900", "commit_message": "Merge pull request #472 from miurahr/topic/miurahr/compress-deflate64", "files_name": ["7b3225e21dc1a444e47798b852394f829d38aa62 - Sun Jul 31 23:21:49 2022 +0900 : Support enhanced deflate compression", "README.rst", "py7zr/compressor.py", "py7zr/py7zr.py", "pyproject.toml", "setup.cfg", "tests/test_extra_codecs.py"]}, {"commit_id": "0f6c037c204d746955cc77dbf18cec5c88ab771a", "commit_date": "Tue Aug 2 21:16:42 2022 +0900", "commit_message": "Merge pull request #473 from miurahr/topic/miurahr/bump-setuptools-63", "files_name": ["55fbfd3d88f2a0c9ba5324d31300618b18682256 - Tue Aug 2 20:00:31 2022 +0900 : Update tox config", "pyproject.toml"]}, {"commit_id": "a7cd2938fe86b704a2d954edfa595983a2f54ab0", "commit_date": "Tue Aug 2 19:51:02 2022 +0900", "commit_message": "Bump actions/setup-python@v4", "files_name": [".github/workflows/run-tox-tests.yml"]}, {"commit_id": "0295127848997f911de505a31622ff4640ca42b7", "commit_date": "Tue Aug 2 19:44:17 2022 +0900", "commit_message": "Actions: test on pypy3.7", "files_name": [".github/workflows/run-tox-tests.yml", "pyproject.toml"]}, {"commit_id": "c8cd86fbd4cd6eae8ba18d42d2d47917230a6838", "commit_date": "Tue Aug 2 19:26:29 2022 +0900", "commit_message": "Update readthedocs config", "files_name": [".readthedocs.yml"]}, {"commit_id": "351903bbe199a1d7aed991bdecaaa118e6099b62", "commit_date": "Tue Aug 2 19:17:18 2022 +0900", "commit_message": "Drop support for python 3.6", "files_name": [".github/workflows/run-tox-tests.yml", "README.rst", "pyproject.toml", "setup.cfg"]}, {"commit_id": "4aaf39a4176fa878672266b04717f0fc015eb283", "commit_date": "Tue Aug 2 17:23:53 2022 +0900", "commit_message": "bump setuptools@63 and setuptools_scm@7", "files_name": ["pyproject.toml", "setup.cfg"]}, {"commit_id": "32d2f3ca3f58ec445d53563303fc0d80af749f98", "commit_date": "Tue Aug 2 16:39:40 2022 +0900", "commit_message": "Actions: update setuptools and setuptools_scm before run", "files_name": [".github/workflows/run-tox-tests.yml"]}, {"commit_id": "919444c8cab8bfdaa9b0c31f4c94f15936ef4534", "commit_date": "Fri Jul 22 12:10:53 2022 +0900", "commit_message": "Merge pull request #468 from JaredNeil/jaredneil-read-reset", "files_name": ["02854f7f24bea7a31f0255d4e50d7ea4e709b582 - Wed Jul 6 11:22:08 2022 -0600 : Start each read with empty _dict", "py7zr/py7zr.py", "tests/data/read_reset.7z", "tests/test_basic.py"]}, {"commit_id": "4c77c7e9caf09dcd297a74ca426e65bfc0109f3d", "commit_date": "Sat Jul 2 12:32:41 2022 +0900", "commit_message": "Merge pull request #464 from miurahr/topic/miurahr/test/update-deflate64-case", "files_name": ["42a071d6b71900b7adf8e2d7427dd05a58a41433 - Sat Jul 2 12:30:46 2022 +0900 : Merge branch 'releases/0.19'", "91a8940a6f59fdd1e103b16bb451e708b27ab1e5 - Sat Jul 2 12:15:47 2022 +0900 : Release v0.19.0", "Changelog.rst", "docs/changelog.rst"]}, {"commit_id": "f635b88c366aebfa4537fa6560b8141c96c4eb87", "commit_date": "Sat Jul 2 12:07:56 2022 +0900", "commit_message": "Merge branch 'releases/0.18' into releases/0.19", "files_name": ["122a886ab4b522b8059bcf9e5bd4ebf3adf53bcb - Sat Jul 2 11:59:27 2022 +0900 : Release v0.18.10", "Changelog.rst"]}, {"commit_id": "388457a9224da572bace3a2f2845ee91803fc94a", "commit_date": "Sat Jul 2 11:46:51 2022 +0900", "commit_message": "test: improve check of deflate64 extraction", "files_name": ["MANIFEST.in", "tests/data/src.zip", "tests/test_extra_codecs.py"]}, {"commit_id": "6c6921e8c2f6277c1278425269a84f5d85b089cd", "commit_date": "Sat Jul 2 10:17:22 2022 +0900", "commit_message": "actions: fix publish script", "files_name": [".github/workflows/publish-to-pypi.yml"]}, {"commit_id": "e91d3f039e090c78b1c72c868f48d7339366e48f", "commit_date": "Sat Jul 2 11:02:29 2022 +0900", "commit_message": "Merge pull request #463 from miurahr/topic/miurahr/test/update-deflate64-case", "files_name": ["c19e2a13d5c99cb08251566ede189e29af16af2b - Sat Jul 2 10:53:23 2022 +0900 : actions: fix publish script typo", ".github/workflows/publish-to-pypi.yml"]}, {"commit_id": "7971a9f134ba16ec4c9e33c5f264e0fb801a82cc", "commit_date": "Sat Jul 2 10:50:37 2022 +0900", "commit_message": "Merge pull request #462 from miurahr/topic/miurahr/actions/fix-publish-of-wheel", "files_name": ["2d2760cd80b4303f96b66b1c2cc2d1ff261c35c3 - Sat Jul 2 10:17:22 2022 +0900 : actions: fix publish script", ".github/workflows/publish-to-pypi.yml"]}, {"commit_id": "da73b5a2667f7158a0c354e3d0a0b40ddee3c931", "commit_date": "Sat Jul 2 10:02:24 2022 +0900", "commit_message": "test: Update case file deflate64.7z", "files_name": ["tests/data/deflate64.7z"]}, {"commit_id": "ae0dd8d7667a76dd313a3a19be051866138d3319", "commit_date": "Fri Jul 1 16:37:26 2022 +0900", "commit_message": "Merge pull request #459 from miurahr/topic/miurahr/migrate-inflate64", "files_name": ["99d3e2f4546184c18655a14de136d7e18fc276ff - Wed Jun 29 12:00:06 2022 +0900 : Update README.rst", "README.rst"]}, {"commit_id": "46ccda27993e9e867bd54239d98d3b087a4d1db2", "commit_date": "Wed Jun 29 10:00:29 2022 +0900", "commit_message": "Fix mypy error", "files_name": ["py7zr/compressor.py"]}, {"commit_id": "f1b266a546589d6b380228873be33ad219c90c90", "commit_date": "Wed Jun 29 00:26:01 2022 +0900", "commit_message": "Switch deflate64 decompressor to inflate64", "files_name": ["README.rst", "py7zr/compressor.py", "pyproject.toml", "setup.cfg", "tests/test_extra_codecs.py", "tests/test_info.py"]}, {"commit_id": "3b70fb0ee1b3eb50cfabc5e90166fd190319baf2", "commit_date": "Tue Jun 14 16:56:55 2022 +0900", "commit_message": "Merge pull request #457 from miurahr/topic/miurahr/tests/fix-skipif-zstd-wrongly", "files_name": ["2c0cdde81fe6e3539acaf7538555563411151ec8 - Mon Jun 13 23:47:39 2022 +0900 : tests: enable skipped cases for zstandard", "tests/test_archive.py"]}, {"commit_id": "5dcf12c262ae66a5ae761886c8c5eadf458e48ff", "commit_date": "Tue Jun 14 14:31:39 2022 +0900", "commit_message": "Merge pull request #458 from miurahr/topic/miurahr/actions/checkout-depth-change", "files_name": ["b9d9a92478229bbe411206b5623e5ebb9ca9c3f2 - Tue Jun 14 12:40:35 2022 +0900 : Azure: update setuptools before running tox", "azure-pipelines.yml"]}, {"commit_id": "0d68465c2a4959ab3a06fca1247c560f6751d621", "commit_date": "Tue Jun 14 07:55:14 2022 +0900", "commit_message": "actions: specify checkout fetch depth for all patterns", "files_name": [".github/workflows/run-tox-tests.yml"]}, {"commit_id": "c07a32288e3ab1ee491e45a01fc6df1cc7ec6461", "commit_date": "Sat Jun 4 22:43:43 2022 +0900", "commit_message": "Release v0.18.9", "files_name": ["Changelog.rst"]}, {"commit_id": "b5aba37c9d709297bc8a797506c6db4441c067a4", "commit_date": "Sat Jun 4 22:14:05 2022 +0900", "commit_message": "Merge pull request #453 from miurahr/topic/miurahr/docs/fix-emphasis-syntax-warnings", "files_name": ["52791f62e014defa51afe67de0961153ac153abd - Sat Jun 4 21:25:28 2022 +0900 : docstring: fix errors", "py7zr/py7zr.py"]}, {"commit_id": "1dca96a6a84019c6ff34e72900d699938b400033", "commit_date": "Sat Jun 4 21:25:12 2022 +0900", "commit_message": "docs: fix errors", "files_name": ["docs/api.rst", "docs/archive_format.rst", "docs/contribution.rst"]}, {"commit_id": "a9269a234b26f948ed82e4f2ce0d01c21ed9fee1", "commit_date": "Sat Jun 4 21:25:00 2022 +0900", "commit_message": "docs: update configuration", "files_name": ["docs/conf.py"]}, {"commit_id": "443b09df9db32cebeade21156e79997e87f323b2", "commit_date": "Sat Jun 4 21:03:15 2022 +0900", "commit_message": "Merge pull request #451 from miurahr/topic/miurahr/actions/pr-condition", "files_name": ["9cb33953095ba5620d7d44a38fff9875e9806b30 - Thu Apr 28 09:42:24 2022 +0900 : Actions: update trigger condition", ".github/workflows/codeql-analysis.yml"]}, {"commit_id": "36050a32cfa9fa02d4c79192ef1184cea593c290", "commit_date": "Sat Jun 4 21:01:13 2022 +0900", "commit_message": "Merge pull request #452 from miurahr/topic/miurahr/doc-devel/memory-profile", "files_name": ["efc1dbe9698131b345351120f6ad34321209d7df - Sat Jun 4 20:56:43 2022 +0900 : Release v0.18.8", "Changelog.rst"]}, {"commit_id": "ce50df9ff8080928dffdb2b05e4c713b6305abfc", "commit_date": "Sat Jun 4 20:52:32 2022 +0900", "commit_message": "Merge pull request #450 from miurahr/topic/miurahr/actions/test-with-py311", "files_name": ["b42aa270818303f728340ada7181d75ae8827658 - Sat Jun 4 20:19:58 2022 +0900 : fix tox configuration", "pyproject.toml"]}, {"commit_id": "de2b5ad2f1299c60a038cc2aebb593da110f9c9b", "commit_date": "Sat Jun 4 20:18:27 2022 +0900", "commit_message": "docs: fix URL that become 404", "files_name": ["docs/changelog.rst", "docs/user_guide.rst"]}, {"commit_id": "f175898057b335c0a223e84e0541f259161188dc", "commit_date": "Sat Jun 4 19:28:57 2022 +0900", "commit_message": "Actions: install graphviz", "files_name": [".github/workflows/run-tox-tests.yml", "setup.cfg"]}, {"commit_id": "f0ab19acc73debb72d59584c3c354db08ca52313", "commit_date": "Sat Jun 4 19:07:23 2022 +0900", "commit_message": "tox: add env py311", "files_name": ["pyproject.toml"]}, {"commit_id": "3304729e7f0a0b4c4dbc1a81c9033c2685a4c0b2", "commit_date": "Sat Jun 4 19:04:53 2022 +0900", "commit_message": "docs: bump sphinx@5.0", "files_name": ["docs/conf.py", "pyproject.toml"]}, {"commit_id": "2040a051197f95ab237e9429be8581d60297dd00", "commit_date": "Sat Jun 4 18:03:50 2022 +0900", "commit_message": "Actions: test with python 3.11 beta", "files_name": [".github/workflows/run-tox-tests.yml", "pyproject.toml"]}, {"commit_id": "0c2581cb972f413c9a7fab20c0aaaceabf01aef6", "commit_date": "Sat Jun 4 18:06:08 2022 +0900", "commit_message": "Docs: set toc depth 1 for changelog", "files_name": ["docs/changelog.rst"]}, {"commit_id": "725ebd2c8a7fac7ce7df02908f5cdaf8784f9007", "commit_date": "Sat Jun 4 17:59:49 2022 +0900", "commit_message": "Update changelog", "files_name": ["Changelog.rst"]}, {"commit_id": "732e33641ed1a8849f88619c5c3262699022bf39", "commit_date": "Sat Jun 4 17:57:29 2022 +0900", "commit_message": "Docs: Add changelog at end of documents", "files_name": ["docs/authors.rst", "docs/changelog.rst", "docs/index.rst"]}, {"commit_id": "f60b2ffc23ba706aedb4c906f3af06065e29b99f", "commit_date": "Sat Jun 4 17:52:04 2022 +0900", "commit_message": "Merge pull request #395 from miurahr/topic-empty-append", "files_name": ["886476602f0f9ba8322a95929c10dee682505afe - Sat Jun 4 16:41:05 2022 +0900 : Append mode: fix bugs", "py7zr/archiveinfo.py", "py7zr/py7zr.py"]}, {"commit_id": "73436dbe35ac2fe7b4c64f880d2dafa2d4ef0a82", "commit_date": "Tue Dec 14 23:39:17 2021 +0900", "commit_message": "test: add reproduce case of append empty", "files_name": ["tests/test_archive.py"]}, {"commit_id": "e1f9ab4c658ff8521387600065de6ee46905bac0", "commit_date": "Wed Jun 1 11:04:20 2022 +0900", "commit_message": "Release v0.18.7", "files_name": ["Changelog.rst"]}, {"commit_id": "369cd7f1bbcbc0d8b57dde16b1881c0685a8a9f7", "commit_date": "Wed Jun 1 09:39:46 2022 +0900", "commit_message": "Update authors", "files_name": []}], "windows_after": [{"commit_id": "8366b670a5f526579ec73a80a8d3b98af90d2935", "commit_date": "Tue Nov 1 07:14:04 2022 +0900", "commit_message": "Update Changelog.rst", "files_name": ["Changelog.rst"]}, {"commit_id": "3a4b4dec34c2467d394a09bb1cc5da066b3335ce", "commit_date": "Tue Nov 1 09:18:18 2022 +0900", "commit_message": "Merge pull request #480 from miurahr/topic/miurahr/security/fix-check-logic-zip-slip-traversal-path-attack", "files_name": ["3b83939d76f06f15b211ebc56493ec3d6e1bb167 - Tue Nov 1 09:26:17 2022 +0900 : Release v0.20.1", "Changelog.rst"]}, {"commit_id": "f6220b0356fcae67dc514fb1bdaf6bc2e2263990", "commit_date": "Tue Nov 1 09:52:16 2022 +0900", "commit_message": "Add test against symlink attach", "files_name": ["tests/test_attack.py"]}, {"commit_id": "35b32f507b5f93eae7587e6114b8f723830cb24e", "commit_date": "Tue Nov 1 10:46:58 2022 +0900", "commit_message": "Merge pull request #481 from miurahr/topic/miurahr/security/symlink-attack-test", "files_name": ["1a61ba2073796d1f7ad8bfd9c48d5bd269df6d71 - Wed Nov 2 08:14:24 2022 +0900 : Introduce helpers.canonical_path()", "py7zr/helpers.py", "py7zr/py7zr.py", "tests/test_badfiles.py"]}, {"commit_id": "3c3c5c0b8367f30e05e2e1ba8bf2db6cb7fc89b5", "commit_date": "Wed Nov 2 15:24:37 2022 +0900", "commit_message": "Merge pull request #483 from miurahr/topic/miurahr/security/get-canonical-path", "files_name": ["777f4082dad86c09dce8252d20faddb721cacae2 - Wed Nov 2 16:05:07 2022 +0900 : Release v0.20.2", "Changelog.rst"]}, {"commit_id": "603c5b28fd0ca191263462d00784adc1471da47d", "commit_date": "Thu Nov 3 08:53:57 2022 +0900", "commit_message": "Move Changelog.rst into docs folder", "files_name": ["MANIFEST.in", "docs/Changelog.rst", "docs/index.rst", "docs/previous_changes.rst"]}, {"commit_id": "e08e6fc19817e2cc3b6dae32e078fe8bc2720715", "commit_date": "Thu Nov 3 10:20:11 2022 +0900", "commit_message": "Update metadata: changlog link", "files_name": ["pyproject.toml"]}, {"commit_id": "a6e70b3cdd92c741ccf712d3826737bae4788cdf", "commit_date": "Thu Nov 3 08:57:30 2022 +0900", "commit_message": "Update pyproject.toml", "files_name": ["pyproject.toml", "setup.cfg"]}, {"commit_id": "8b704c72f0ae65f3352917d8a9652fec0b1fbc3e", "commit_date": "Thu Nov 3 10:01:10 2022 +0900", "commit_message": "Remove setup.cfg", "files_name": [".flake8", "MANIFEST.in"]}, {"commit_id": "110a7d6a630198fc407496b81d892bc708c38bba", "commit_date": "Thu Nov 3 10:43:32 2022 +0900", "commit_message": "Merge pull request #485 from miurahr/topic/miurahr/docs/move-changelog-in-docs", "files_name": ["a5b044e38ce34a4a7daedb510eddb0a1db5548e0 - Thu Nov 3 18:30:12 2022 +0100 : fix documentation", "README.rst"]}, {"commit_id": "d133faa3af7f9df81ba083254ab539c54156cb15", "commit_date": "Fri Nov 4 07:35:02 2022 +0900", "commit_message": "Merge pull request #488 from tesslinger/patch-documentation", "files_name": ["63b1f1d6557d5145667d8e644adf9d0e41d095d7 - Thu Nov 3 10:28:17 2022 +0900 : Use hashlib instead of _hashlib", "py7zr/helpers.py"]}, {"commit_id": "519a94e0ac35a64920f88d574edf8ce48f81c404", "commit_date": "Sat Nov 5 11:49:50 2022 +0900", "commit_message": "Actions: fix benchmark script", "files_name": [".github/workflows/run-benchmark.yml"]}, {"commit_id": "cf9520d3d3fe87f2b69fd55572edc051b0d58a81", "commit_date": "Sat Nov 5 11:55:01 2022 +0900", "commit_message": "Merge pull request #491 from miurahr/topic/actions/update-benchmark-scripts", "files_name": ["ed58469b1785c0800d0911dbb9143c0f6804beb7 - Sat Nov 5 12:17:31 2022 +0900 : Actions: update scripts", ".github/workflows/run-benchmark.yml", ".github/workflows/run-tox-tests.yml", "pyproject.toml"]}, {"commit_id": "cb641a5aabec7255d3cc5c2de3d9e95fdccd38df", "commit_date": "Sat Nov 5 12:19:44 2022 +0900", "commit_message": "Merge pull request #487 from miurahr/topic/miurahr/imports/fix-hashlib-imports", "files_name": ["a00d679c15214d766646099f7850b079da65de3f - Sat Nov 5 12:20:54 2022 +0900 : Merge pull request #486 from miurahr/topic/miurahr/setup/drop-duplicated-configurations", "9683d44e66a8ee9aa4b531d9520e417c94dbe91b - Sat Nov 5 12:41:05 2022 +0900 : Merge pull request #492 from miurahr/topic/actions/change-benchmark-target-ubuntu-pypy", "c87f19cebd5e237125cba2f8a0ae2a6cdab03968 - Sat Nov 5 12:48:05 2022 +0900 : Actions: fix test on pypy", ".github/workflows/run-benchmark.yml", ".github/workflows/run-tox-tests.yml", "pyproject.toml"]}, {"commit_id": "0354a372a9ff1465b2fff5e484b2d2073aabc226", "commit_date": "Mon Nov 14 07:58:36 2022 +0900", "commit_message": "Disable enhanced deflate compression on pypy", "files_name": ["py7zr/compressor.py", "pyproject.toml", "tests/test_extra_codecs.py"]}, {"commit_id": "ad3fd02b8056c78615725e2a0d857664d4a9c535", "commit_date": "Mon Nov 14 08:38:58 2022 +0900", "commit_message": "Fix mypy error", "files_name": ["py7zr/cli.py", "tests/test_extra_codecs.py"]}, {"commit_id": "d9a43c5bfd1be24f44753ae9811c46f09b743c98", "commit_date": "Mon Nov 14 09:13:48 2022 +0900", "commit_message": "Disable deflate64 on pypy", "files_name": ["py7zr/compressor.py", "tests/test_extra_codecs.py"]}, {"commit_id": "14f1c5d3c9620fe509e7bc7ac85650487b56ea26", "commit_date": "Mon Nov 14 09:25:15 2022 +0900", "commit_message": "Update test to skip deflate64 on pypy", "files_name": ["tests/test_info.py"]}, {"commit_id": "d98d38356862794712eb015027c920aa4dd21d4a", "commit_date": "Mon Nov 14 09:35:38 2022 +0900", "commit_message": "Update README.rst", "files_name": ["README.rst"]}, {"commit_id": "b088566efbe772643954685276e19b6a28d64712", "commit_date": "Mon Nov 14 11:02:18 2022 +0900", "commit_message": "Merge pull request #494 from miurahr/topic/miurahr/disable-defate64-on-pypy", "files_name": ["4d653bf1f39171fe9a3ade1e5f6a2c910b135966 - Sat Nov 5 11:31:41 2022 +0900 : Drop manual GC call on close()", "py7zr/py7zr.py"]}, {"commit_id": "a336f342130774f87c899fb541c455e840defa0d", "commit_date": "Mon Nov 14 12:38:54 2022 +0900", "commit_message": "Merge pull request #490 from miurahr/topic/miurahr/performance/drop-manual-gc-call-issue-489", "files_name": ["80a2f68e810495e4def3263cf94dd3d621d499aa - Mon Nov 14 12:40:11 2022 +0900 : Update README", "README.rst"]}, {"commit_id": "4815f6ed9becf8316fe5f7efb72f5ed5d06f1f53", "commit_date": "Mon Nov 14 12:40:46 2022 +0900", "commit_message": "Update README", "files_name": ["README.rst"]}, {"commit_id": "7e0c75f1e5f0a35f3d73e928308d2be05463d4ac", "commit_date": "Mon Nov 14 14:21:33 2022 +0900", "commit_message": "Create SECURITY.rst", "files_name": ["docs/SECURITY.rst"]}, {"commit_id": "1f5c7c12599029c3394497ea4ad63acfe0a0da67", "commit_date": "Tue Nov 15 12:30:03 2022 +0900", "commit_message": "Release v0.20.3", "files_name": ["docs/Changelog.rst"]}, {"commit_id": "52d7ef97a5a40a275f2b538cfb04fffe774a1ec4", "commit_date": "Sat Nov 26 20:50:23 2022 +0530", "commit_message": "Update user_guide.rst", "files_name": ["docs/user_guide.rst"]}, {"commit_id": "f71775e4f5367640dbc9cd24f09ffbddaccf1ec3", "commit_date": "Sun Nov 27 15:55:40 2022 +0900", "commit_message": "Merge pull request #499 from sriteja-t/patch-1", "files_name": ["f475c4b416188bdbf3abc49f419bde20443b3e7e - Wed Dec 7 20:59:27 2022 +0900 : README: add security notifiation", "README.rst"]}, {"commit_id": "e334f03b48330b10a60f1a60e0addad8dcb5c442", "commit_date": "Wed Dec 7 21:03:28 2022 +0900", "commit_message": "Update README.rst", "files_name": ["README.rst"]}, {"commit_id": "8d8de3cf3aca7a86d134ae74f230bfc9e5d3bcae", "commit_date": "Wed Dec 7 21:05:52 2022 +0900", "commit_message": "Merge pull request #496 from miurahr/topic/miurahr/docs/security-policy", "files_name": ["a313fbb87a30c49d3eb7107d60a467c462e32934 - Wed Dec 7 21:11:51 2022 +0900 : Update README.rst", "README.rst"]}, {"commit_id": "65614f337dde7652866cefcfebf00fcf65c0dc1e", "commit_date": "Wed Dec 7 21:13:00 2022 +0900", "commit_message": "Merge pull request #501 from miurahr/topic/miurahr/readme/security-notification-cve-2022-44900", "files_name": ["ad765b38db5e5182cfca6e74a4245ecf3bdd6aed - Thu Dec 15 21:48:17 2022 +0900 : Delete stale.yml", ".github/workflows/stale.yml"]}, {"commit_id": "e7c136584102f37d2afe74043d395de6603f0a6c", "commit_date": "Mon Jan 2 12:52:44 2023 +0900", "commit_message": "pyproject.toml: Fix passenv in tox_ini", "files_name": ["pyproject.toml"]}, {"commit_id": "d7e06072191f2b2a2eebeb023b2ea4e92c6800ca", "commit_date": "Tue Jan 3 07:32:08 2023 +0900", "commit_message": "compressor: update variable type hint in current style", "files_name": ["py7zr/compressor.py"]}, {"commit_id": "1a9ad1fb8b8a73d758012ef8b7955c55a08733d0", "commit_date": "Thu Feb 9 21:31:02 2023 +0800", "commit_message": "Fix installation error in Cygwin (#504)", "files_name": ["py7zr/properties.py", "pyproject.toml", "tests/test_misc.py"]}, {"commit_id": "afccd6a50b8326501adcbe5533f8ea8b48436f63", "commit_date": "Fri Feb 10 22:31:05 2023 +0900", "commit_message": "Release v0.20.4", "files_name": ["docs/Changelog.rst"]}, {"commit_id": "b1280c17b7c43ecd92f49a2a83fa533a5e3ed94c", "commit_date": "Sun Feb 12 15:01:40 2023 +0900", "commit_message": "chore: tox check on python 3.9", "files_name": ["pyproject.toml"]}, {"commit_id": "3d4af4620e4b8de90b1d4c44751cb0ed25d3af6b", "commit_date": "Sun Feb 12 15:02:16 2023 +0900", "commit_message": "style: update by black", "files_name": ["py7zr/cli.py", "py7zr/py7zr.py"]}, {"commit_id": "c1ba9ce0953750b46febc86d0745691730f22844", "commit_date": "Sun Feb 12 18:27:32 2023 +0530", "commit_message": "fix typo in the readme my_filter to my_filters", "files_name": ["README.rst"]}, {"commit_id": "cf9f513a435795e0792c2c3633bc1dc4cfd40adc", "commit_date": "Mon Feb 13 00:30:36 2023 +0900", "commit_message": "Merge pull request #510 from chirag127/patch-1", "files_name": ["81efe8011c21db1f1bf52c08df3fb192b46ace06 - Mon Feb 13 10:20:32 2023 +0900 : chore: add types stub for psutil", "py7zr/properties.py", "pyproject.toml"]}, {"commit_id": "2a27d7be844b771c2bda7e83f298d02bf970f171", "commit_date": "Mon Feb 13 02:40:18 2023 +0000", "commit_message": "Update flake8 requirement from <5 to <7", "files_name": ["pyproject.toml"]}, {"commit_id": "08bf815895996c974c064239dcd6741e95080c4d", "commit_date": "Mon Feb 13 12:11:16 2023 +0900", "commit_message": "Merge pull request #498 from miurahr/dependabot/pip/flake8-lt-7", "files_name": ["77ef85e7cbb0b93111838e971e5ca42057c2d2cb - Mon Apr 10 17:28:56 2023 +0200 : enh(helper): fix bug when fname starts from root.", "py7zr/helpers.py"]}, {"commit_id": "8aaeaed1e63e03f8e286cdab3488e9738c26fceb", "commit_date": "Tue Apr 11 11:42:06 2023 +0200", "commit_message": "enh(tst): add test when fname has root reference", "files_name": ["tests/data/root_path_arcname.7z", "tests/test_extract.py"]}, {"commit_id": "ed63b85a0adc057a64c7fc1d8233afeeb36ab826", "commit_date": "Tue Apr 11 12:32:23 2023 +0200", "commit_message": "cln(tests): Fix typing errors in test", "files_name": ["tests/test_extract.py"]}, {"commit_id": "252632cc8c50a878533e56816183eaee054ef0fd", "commit_date": "Wed Apr 12 15:14:47 2023 +0200", "commit_message": "cln(tests): apply black", "files_name": ["tests/test_extract.py"]}, {"commit_id": "f28f5baab128e14328403e860864285ac23ec3a5", "commit_date": "Sat Apr 15 09:37:12 2023 +0900", "commit_message": "Merge pull request #513 from mmngreco/master", "files_name": ["afa99519193b902b37c144d6a0cc13367edbd3c5 - Sat Apr 15 09:44:25 2023 +0900 : Release v0.20.5", "docs/Changelog.rst"]}, {"commit_id": "54b12dac349e5045473581e544841687e9cb4848", "commit_date": "Thu May 18 16:53:16 2023 -0700", "commit_message": "fixes issue #517 - Error appending file: KeyError: 'lastwritetime'", "files_name": ["py7zr/archiveinfo.py"]}, {"commit_id": "89b475f64d404fc58b77b176e2796551d8ef6300", "commit_date": "Fri May 19 13:28:06 2023 +0900", "commit_message": "chore(ci): bump host macOS-12", "files_name": ["azure-pipelines.yml", "pyproject.toml"]}, {"commit_id": "afc9fd66f99d755eeb7660b99f3f6630e9a723ee", "commit_date": "Fri May 19 13:32:01 2023 +0900", "commit_message": "Merge pull request #518 from pmolodo/pr/fix-issue-517-error-appending-lastwritetime", "files_name": ["f59d236a974b25bbfe2be1fe563a1540a579800c - Fri May 19 13:37:29 2023 +0900 : Merge pull request #519 from miurahr/topic/miurahr/actions/bump-macos-12", "1622072b2711679bb5970927bbcacf1222dc0650 - Fri Jun 2 22:26:58 2023 +0900 : fix: use RLIMIT_DATA to get usable memory size", "py7zr/properties.py"]}, {"commit_id": "e4dfeaeac74bfd88016a81c7c3eb002955b47480", "commit_date": "Sat Jun 24 23:04:03 2023 +0900", "commit_message": "Merge pull request #522 from miurahr/topic/miurahr/openbsd/avoid-undefined-rlimit-variable", "files_name": ["3fefd96b96f1b45aca1dd7e2d421dbc120895d9f - Sat Jul 1 11:56:06 2023 +0900 : Update azure-pipelines.yml", "azure-pipelines.yml"]}, {"commit_id": "32beca821cd9c0c3a7b12473cd393fd53a1acb8c", "commit_date": "Mon Jul 3 10:44:37 2023 -0700", "commit_message": "Fixing a string quote in user_guide document", "files_name": ["docs/user_guide.rst"]}, {"commit_id": "65d019eefcc81fb01a410831b88d3ec52210a741", "commit_date": "Tue Jul 4 17:58:56 2023 +0900", "commit_message": "Merge pull request #524 from sugam11/patch-1", "files_name": ["181268834d2f44badc36e74cd661ceab79c8958b - Wed Jul 26 22:38:06 2023 +0900 : Merge pull request #523 from miurahr/topic/miurahr/fix-issue-520", "8849399a4f868631098949d2cc9921d694ccd32b - Fri Aug 4 12:45:40 2023 +0900 : fix: allow specify target path in relative path", "py7zr/helpers.py", "tests/test_extract.py"]}, {"commit_id": "509fceb0cf45ea4ff907c90a798fd1120bad8152", "commit_date": "Fri Aug 4 19:48:09 2023 +0900", "commit_message": "Merge pull request #530 from miurahr/topic/miurahr/extract-parent-directory", "files_name": ["4f11f67c242a3f396cbb38280305218707a028de - Fri Aug 4 08:20:07 2023 +0900 : fix: sanitize path when write"]}], "parents": [{"commit_id_before": "04e3af5ae5092137ee7b015b85efac1c7fd8974e", "url_before": "https://api.github.com/repos/miurahr/py7zr/commits/04e3af5ae5092137ee7b015b85efac1c7fd8974e", "html_url_before": "https://github.com/miurahr/py7zr/commit/04e3af5ae5092137ee7b015b85efac1c7fd8974e"}], "details": [{"raw_url": "https://github.com/miurahr/py7zr/raw/1bb43f17515c7f69673a1c88ab9cc72a7bbef406/MANIFEST.in", "code": "include *.png\ninclude *.rst\ninclude *.svg\ninclude *.toml\ninclude LICENSE\ninclude py7zr/py.typed\nrecursive-include py7zr *.py\nrecursive-include tests *.py\nrecursive-include tests/data *.txt\nrecursive-include tests/data *.7z\nrecursive-include tests/data *.0*\nrecursive-include tests/data *.bin\nrecursive-include tests/data *.c\nrecursive-include tests/data *.zip\nrecursive-include utils *.py\nrecursive-include utils *.txt\nrecursive-include docs *.bat\nrecursive-include docs *.dot\nrecursive-include docs *.png\nrecursive-include docs *.py\nrecursive-include docs *.rst\nrecursive-include docs *.yml\nrecursive-include docs *.odp\nrecursive-include docs *.pdf\nrecursive-include docs *.svg\nrecursive-include docs Makefile\nprune .github\nexclude .gitignore\nexclude .readthedocs.yml\nexclude codecov.yml\nexclude azure-pipelines.yml\n", "code_before": "include *.png\ninclude *.rst\ninclude *.svg\ninclude *.toml\ninclude LICENSE\ninclude .coveragerc\ninclude tox.ini\ninclude py7zr/py.typed\nrecursive-include py7zr *.py\nrecursive-include tests *.py\nrecursive-include tests/data *.txt\nrecursive-include tests/data *.7z\nrecursive-include tests/data *.0*\nrecursive-include tests/data *.bin\nrecursive-include tests/data *.c\nrecursive-include tests/data *.zip\nrecursive-include utils *.py\nrecursive-include utils *.txt\nrecursive-include docs *.bat\nrecursive-include docs *.dot\nrecursive-include docs *.png\nrecursive-include docs *.py\nrecursive-include docs *.rst\nrecursive-include docs *.yml\nrecursive-include docs *.odp\nrecursive-include docs *.pdf\nrecursive-include docs *.svg\nrecursive-include docs *.txt\nrecursive-include docs Makefile\nprune .github\nexclude .gitignore\nexclude .readthedocs.yml\nexclude codecov.yml\nexclude azure-pipelines.yml\n", "patch": "@@ -3,8 +3,6 @@ include *.rst\n include *.svg\n include *.toml\n include LICENSE\n-include .coveragerc\n-include tox.ini\n include py7zr/py.typed\n recursive-include py7zr *.py\n recursive-include tests *.py\n@@ -25,7 +23,6 @@ recursive-include docs *.yml\n recursive-include docs *.odp\n recursive-include docs *.pdf\n recursive-include docs *.svg\n-recursive-include docs *.txt\n recursive-include docs Makefile\n prune .github\n exclude .gitignore", "file_path": "files/2022_12/1695", "file_language": "in", "file_name": "MANIFEST.in", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/miurahr/py7zr/raw/1bb43f17515c7f69673a1c88ab9cc72a7bbef406/py7zr%2Fhelpers.py", "code": "#!/usr/bin/python -u\n#\n# p7zr library\n#\n# Copyright (c) 2019-2021 Hiroshi Miura <miurahr@linux.com>\n# Copyright (c) 2004-2015 by Joachim Bauch, mail@joachim-bauch.de\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n#\n#\nimport ctypes\nimport os\nimport pathlib\nimport platform\nimport sys\nimport time as _time\nimport zlib\nfrom datetime import datetime, timedelta, timezone, tzinfo\nfrom typing import BinaryIO, Optional, Union\n\nimport _hashlib  # type: ignore  # noqa\n\nimport py7zr.win32compat\nfrom py7zr import Bad7zFile\nfrom py7zr.win32compat import is_windows_native_python, is_windows_unc_path\n\n# String used at the beginning of relative paths\nRELATIVE_PATH_MARKER = \"./\"\n\n\ndef calculate_crc32(data: bytes, value: int = 0, blocksize: int = 1024 * 1024) -> int:\n    \"\"\"Calculate CRC32 of strings with arbitrary lengths.\"\"\"\n    if len(data) <= blocksize:\n        value = zlib.crc32(data, value)\n    else:\n        length = len(data)\n        pos = blocksize\n        value = zlib.crc32(data[:pos], value)\n        while pos < length:\n            value = zlib.crc32(data[pos : pos + blocksize], value)\n            pos += blocksize\n    return value & 0xFFFFFFFF\n\n\ndef _calculate_key1(password: bytes, cycles: int, salt: bytes, digest: str) -> bytes:\n    \"\"\"Calculate 7zip AES encryption key. Base implementation.\"\"\"\n    if digest not in (\"sha256\"):\n        raise ValueError(\"Unknown digest method for password protection.\")\n    assert cycles <= 0x3F\n    if cycles == 0x3F:\n        ba = bytearray(salt + password + bytes(32))\n        key: bytes = bytes(ba[:32])\n    else:\n        rounds = 1 << cycles\n        m = _hashlib.new(digest)\n        for round in range(rounds):\n            m.update(salt + password + round.to_bytes(8, byteorder=\"little\", signed=False))\n        key = m.digest()[:32]\n    return key\n\n\ndef _calculate_key2(password: bytes, cycles: int, salt: bytes, digest: str):\n    \"\"\"Calculate 7zip AES encryption key.\n    It utilize ctypes and memoryview buffer and zero-copy technology on Python.\"\"\"\n    if digest not in (\"sha256\"):\n        raise ValueError(\"Unknown digest method for password protection.\")\n    assert cycles <= 0x3F\n    if cycles == 0x3F:\n        key: bytes = bytes(bytearray(salt + password + bytes(32))[:32])\n    else:\n        rounds = 1 << cycles\n        m = _hashlib.new(digest)\n        length = len(salt) + len(password)\n\n        class RoundBuf(ctypes.LittleEndianStructure):\n            _pack_ = 1\n            _fields_ = [\n                (\"saltpassword\", ctypes.c_ubyte * length),\n                (\"round\", ctypes.c_uint64),\n            ]\n\n        buf = RoundBuf()\n        for i, c in enumerate(salt + password):\n            buf.saltpassword[i] = c\n        buf.round = 0\n        mv = memoryview(buf)\n        while buf.round < rounds:\n            m.update(mv)\n            buf.round += 1\n        key = m.digest()[:32]\n    return key\n\n\ndef _calculate_key3(password: bytes, cycles: int, salt: bytes, digest: str) -> bytes:\n    \"\"\"Calculate 7zip AES encryption key.\n    Concat values in order to reduce number of calls of Hash.update().\"\"\"\n    if digest not in (\"sha256\"):\n        raise ValueError(\"Unknown digest method for password protection.\")\n    assert cycles <= 0x3F\n    if cycles == 0x3F:\n        ba = bytearray(salt + password + bytes(32))\n        key: bytes = bytes(ba[:32])\n    else:\n        cat_cycle = 6\n        if cycles > cat_cycle:\n            rounds = 1 << cat_cycle\n            stages = 1 << (cycles - cat_cycle)\n        else:\n            rounds = 1 << cycles\n            stages = 1 << 0\n        m = _hashlib.new(digest)\n        saltpassword = salt + password\n        s = 0  # type: int  # (0..stages) * rounds\n        if platform.python_implementation() == \"PyPy\":\n            for _ in range(stages):\n                m.update(\n                    memoryview(\n                        b\"\".join(\n                            [saltpassword + (s + i).to_bytes(8, byteorder=\"little\", signed=False) for i in range(rounds)]\n                        )\n                    )\n                )\n                s += rounds\n        else:\n            for _ in range(stages):\n                m.update(\n                    b\"\".join([saltpassword + (s + i).to_bytes(8, byteorder=\"little\", signed=False) for i in range(rounds)])\n                )\n                s += rounds\n        key = m.digest()[:32]\n\n    return key\n\n\nif platform.python_implementation() == \"PyPy\" or sys.version_info > (3, 6):\n    calculate_key = _calculate_key3\nelse:\n    calculate_key = _calculate_key2  # it is faster when CPython 3.6.x\n\n\ndef filetime_to_dt(ft):\n    \"\"\"Convert Windows NTFS file time into python datetime object.\"\"\"\n    EPOCH_AS_FILETIME = 116444736000000000\n    us = (ft - EPOCH_AS_FILETIME) // 10\n    return datetime(1970, 1, 1, tzinfo=timezone.utc) + timedelta(microseconds=us)\n\n\nZERO = timedelta(0)\nHOUR = timedelta(hours=1)\nSECOND = timedelta(seconds=1)\n\n# A class capturing the platform's idea of local time.\n# (May result in wrong values on historical times in\n#  timezones where UTC offset and/or the DST rules had\n#  changed in the past.)\n\nSTDOFFSET = timedelta(seconds=-_time.timezone)\nif _time.daylight:\n    DSTOFFSET = timedelta(seconds=-_time.altzone)\nelse:\n    DSTOFFSET = STDOFFSET\n\nDSTDIFF = DSTOFFSET - STDOFFSET\n\n\nclass LocalTimezone(tzinfo):\n    def fromutc(self, dt):\n        assert dt.tzinfo is self\n        stamp = (dt - datetime(1970, 1, 1, tzinfo=self)) // SECOND\n        args = _time.localtime(stamp)[:6]\n        # dst_diff = DSTDIFF // SECOND\n        # Detect fold\n        # fold = args == _time.localtime(stamp - dst_diff)\n        return datetime(*args, microsecond=dt.microsecond, tzinfo=self)\n\n    def utcoffset(self, dt):\n        if self._isdst(dt):\n            return DSTOFFSET\n        else:\n            return STDOFFSET\n\n    def dst(self, dt):\n        if self._isdst(dt):\n            return DSTDIFF\n        else:\n            return ZERO\n\n    def tzname(self, dt):\n        return _time.tzname[self._isdst(dt)]\n\n    def _isdst(self, dt):\n        tt = (\n            dt.year,\n            dt.month,\n            dt.day,\n            dt.hour,\n            dt.minute,\n            dt.second,\n            dt.weekday(),\n            0,\n            0,\n        )\n        stamp = _time.mktime(tt)\n        tt = _time.localtime(stamp)\n        return tt.tm_isdst > 0\n\n\nLocal = LocalTimezone()\nTIMESTAMP_ADJUST = -11644473600\n\n\nclass UTC(tzinfo):\n    \"\"\"UTC\"\"\"\n\n    def utcoffset(self, dt):\n        return ZERO\n\n    def tzname(self, dt):\n        return \"UTC\"\n\n    def dst(self, dt):\n        return ZERO\n\n    def _call__(self):\n        return self\n\n\nclass ArchiveTimestamp(int):\n    \"\"\"Windows FILETIME timestamp.\"\"\"\n\n    def __repr__(self):\n        return \"%s(%d)\" % (type(self).__name__, self)\n\n    def __index__(self):\n        return self.__int__()\n\n    def totimestamp(self) -> float:\n        \"\"\"Convert 7z FILETIME to Python timestamp.\"\"\"\n        # FILETIME is 100-nanosecond intervals since 1601/01/01 (UTC)\n        return (self / 10000000.0) + TIMESTAMP_ADJUST\n\n    def as_datetime(self):\n        \"\"\"Convert FILETIME to Python datetime object.\"\"\"\n        return datetime.fromtimestamp(self.totimestamp(), UTC())\n\n    @staticmethod\n    def from_datetime(val):\n        return ArchiveTimestamp((val - TIMESTAMP_ADJUST) * 10000000.0)\n\n    @staticmethod\n    def from_now():\n        return ArchiveTimestamp((_time.time() - TIMESTAMP_ADJUST) * 10000000.0)\n\n\ndef islink(path):\n    \"\"\"\n    Cross-platform islink implementation.\n    Support Windows NT symbolic links and reparse points.\n    \"\"\"\n    is_symlink = os.path.islink(str(path))\n    if sys.version_info >= (3, 8) or sys.platform != \"win32\" or sys.getwindowsversion()[0] < 6:\n        return is_symlink\n    # special check for directory junctions which py38 does.\n    if is_symlink:\n        if py7zr.win32compat.is_reparse_point(path):\n            is_symlink = False\n    return is_symlink\n\n\ndef readlink(path: Union[str, pathlib.Path], *, dir_fd=None) -> Union[str, pathlib.Path]:\n    \"\"\"\n    Cross-platform compat implementation of os.readlink and Path.readlink().\n    Support Windows NT symbolic links and reparse points.\n    When called with path argument as pathlike(str), return result as a pathlike(str).\n    When called with Path object, return also Path object.\n    When called with path argument as bytes, return result as a bytes.\n    \"\"\"\n    if sys.version_info >= (3, 9):\n        if isinstance(path, pathlib.Path) and dir_fd is None:\n            return path.readlink()\n        else:\n            return os.readlink(path, dir_fd=dir_fd)\n    elif sys.version_info >= (3, 8) or sys.platform != \"win32\":\n        res = os.readlink(path, dir_fd=dir_fd)\n        # Hack to handle a wrong type of results\n        if isinstance(res, bytes):\n            res = os.fsdecode(res)\n        if isinstance(path, pathlib.Path):\n            return pathlib.Path(res)\n        else:\n            return res\n    elif not os.path.exists(str(path)):\n        raise OSError(22, \"Invalid argument\", path)\n    return py7zr.win32compat.readlink(path)\n\n\nclass MemIO:\n    \"\"\"pathlib.Path-like IO class to write memory(io.Bytes)\"\"\"\n\n    def __init__(self, buf: BinaryIO):\n        self._buf = buf\n\n    def write(self, data: bytes) -> int:\n        return self._buf.write(data)\n\n    def read(self, length: Optional[int] = None) -> bytes:\n        if length is not None:\n            return self._buf.read(length)\n        else:\n            return self._buf.read()\n\n    def close(self) -> None:\n        self._buf.seek(0)\n\n    def flush(self) -> None:\n        pass\n\n    def seek(self, position: int) -> None:\n        self._buf.seek(position)\n\n    def open(self, mode=None):\n        return self\n\n    @property\n    def parent(self):\n        return self\n\n    def mkdir(self, parents=None, exist_ok=False):\n        return None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\nclass NullIO:\n    \"\"\"pathlib.Path-like IO class of /dev/null\"\"\"\n\n    def __init__(self):\n        pass\n\n    def write(self, data):\n        return len(data)\n\n    def read(self, length=None):\n        if length is not None:\n            return bytes(length)\n        else:\n            return b\"\"\n\n    def close(self):\n        pass\n\n    def flush(self):\n        pass\n\n    def open(self, mode=None):\n        return self\n\n    @property\n    def parent(self):\n        return self\n\n    def mkdir(self):\n        return None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\nclass BufferOverflow(Exception):\n    pass\n\n\nclass Buffer:\n    def __init__(self, size: int = 16):\n        self._buf = bytearray(size)\n        self._buflen = 0\n        self.view = memoryview(self._buf[0:0])\n\n    def add(self, data: Union[bytes, bytearray, memoryview]):\n        length = len(data)\n        self._buf[self._buflen :] = data\n        self._buflen += length\n        self.view = memoryview(self._buf[0 : self._buflen])\n\n    def reset(self) -> None:\n        self._buflen = 0\n        self.view = memoryview(self._buf[0:0])\n\n    def set(self, data: Union[bytes, bytearray, memoryview]) -> None:\n        length = len(data)\n        self._buf[0:] = data\n        self._buflen = length\n        self.view = memoryview(self._buf[0:length])\n\n    def get(self) -> bytearray:\n        val = self._buf[: self._buflen]\n        self.reset()\n        return val\n\n    def __len__(self) -> int:\n        return self._buflen\n\n    def __bytes__(self):\n        return bytes(self._buf[0 : self._buflen])\n\n\ndef remove_relative_path_marker(path: str) -> str:\n    \"\"\"\n    Removes './' from the beginning of a path-like string\n    \"\"\"\n    processed_path = path\n\n    if path.startswith(RELATIVE_PATH_MARKER):\n        processed_path = path[len(RELATIVE_PATH_MARKER) :]\n\n    return processed_path\n\n\ndef get_sanitized_output_path(fname: str, path: Optional[pathlib.Path]) -> pathlib.Path:\n    \"\"\"\n    check f.filename has invalid directory traversals\n    do following but is_relative_to introduced in py 3.9,\n    so I replaced it with relative_to. when condition is not satisfied, raise ValueError\n    if not pathlib.Path(...).joinpath(remove_relative_path_marker(outname)).is_relative_to(...):\n        raise Bad7zFile\n    \"\"\"\n    if path is None:\n        try:\n            pathlib.Path(os.getcwd()).joinpath(fname).resolve().relative_to(os.getcwd())\n            outfile = pathlib.Path(remove_relative_path_marker(fname))\n        except ValueError:\n            raise Bad7zFile(f\"Specified path is bad: {fname}\")\n    else:\n        try:\n            outfile = path.joinpath(remove_relative_path_marker(fname))\n            outfile.resolve().relative_to(path)\n        except ValueError:\n            raise Bad7zFile(f\"Specified path is bad: {fname}\")\n    return outfile\n\n\ndef check_archive_path(arcname: str) -> bool:\n    path = pathlib.Path(\"/foo/boo/fuga/hoge/a90sufoiasj09/dafj08sajfa/\")  # dummy path\n    return is_target_path_valid(path, path.joinpath(arcname))\n\n\ndef is_target_path_valid(path: pathlib.Path, target: pathlib.Path) -> bool:\n    try:\n        if path.is_absolute():\n            target.resolve().relative_to(path)\n        else:\n            target.resolve().relative_to(pathlib.Path(os.getcwd()).joinpath(path))\n    except ValueError:\n        return False\n    return True\n\n\ndef check_win32_file_namespace(pathname: pathlib.Path) -> pathlib.Path:\n    # When python on Windows and not python on Cygwin,\n    # Add win32 file namespace to exceed Microsoft Windows\n    # path length limitation to 260 bytes\n    # ref.\n    # https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file\n    # In editions of Windows before Windows 10 version 1607,\n    # the maximum length for a path is MAX_PATH, which is defined as\n    # 260 characters. In later versions of Windows, changing a registry key\n    # or select option when python installation is required to remove the limit.\n    if is_windows_native_python() and pathname.is_absolute() and not is_windows_unc_path(pathname):\n        pathname = pathlib.WindowsPath(\"\\\\\\\\?\\\\\" + str(pathname))\n    return pathname\n", "code_before": "#!/usr/bin/python -u\n#\n# p7zr library\n#\n# Copyright (c) 2019-2021 Hiroshi Miura <miurahr@linux.com>\n# Copyright (c) 2004-2015 by Joachim Bauch, mail@joachim-bauch.de\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n#\n#\nimport ctypes\nimport os\nimport pathlib\nimport platform\nimport sys\nimport time as _time\nimport zlib\nfrom datetime import datetime, timedelta, timezone, tzinfo\nfrom typing import BinaryIO, Optional, Union\n\nimport _hashlib  # type: ignore  # noqa\n\nimport py7zr.win32compat\n\n# String used at the beginning of relative paths\nRELATIVE_PATH_MARKER = \"./\"\n\n\ndef calculate_crc32(data: bytes, value: int = 0, blocksize: int = 1024 * 1024) -> int:\n    \"\"\"Calculate CRC32 of strings with arbitrary lengths.\"\"\"\n    if len(data) <= blocksize:\n        value = zlib.crc32(data, value)\n    else:\n        length = len(data)\n        pos = blocksize\n        value = zlib.crc32(data[:pos], value)\n        while pos < length:\n            value = zlib.crc32(data[pos : pos + blocksize], value)\n            pos += blocksize\n    return value & 0xFFFFFFFF\n\n\ndef _calculate_key1(password: bytes, cycles: int, salt: bytes, digest: str) -> bytes:\n    \"\"\"Calculate 7zip AES encryption key. Base implementation.\"\"\"\n    if digest not in (\"sha256\"):\n        raise ValueError(\"Unknown digest method for password protection.\")\n    assert cycles <= 0x3F\n    if cycles == 0x3F:\n        ba = bytearray(salt + password + bytes(32))\n        key: bytes = bytes(ba[:32])\n    else:\n        rounds = 1 << cycles\n        m = _hashlib.new(digest)\n        for round in range(rounds):\n            m.update(salt + password + round.to_bytes(8, byteorder=\"little\", signed=False))\n        key = m.digest()[:32]\n    return key\n\n\ndef _calculate_key2(password: bytes, cycles: int, salt: bytes, digest: str):\n    \"\"\"Calculate 7zip AES encryption key.\n    It utilize ctypes and memoryview buffer and zero-copy technology on Python.\"\"\"\n    if digest not in (\"sha256\"):\n        raise ValueError(\"Unknown digest method for password protection.\")\n    assert cycles <= 0x3F\n    if cycles == 0x3F:\n        key: bytes = bytes(bytearray(salt + password + bytes(32))[:32])\n    else:\n        rounds = 1 << cycles\n        m = _hashlib.new(digest)\n        length = len(salt) + len(password)\n\n        class RoundBuf(ctypes.LittleEndianStructure):\n            _pack_ = 1\n            _fields_ = [\n                (\"saltpassword\", ctypes.c_ubyte * length),\n                (\"round\", ctypes.c_uint64),\n            ]\n\n        buf = RoundBuf()\n        for i, c in enumerate(salt + password):\n            buf.saltpassword[i] = c\n        buf.round = 0\n        mv = memoryview(buf)\n        while buf.round < rounds:\n            m.update(mv)\n            buf.round += 1\n        key = m.digest()[:32]\n    return key\n\n\ndef _calculate_key3(password: bytes, cycles: int, salt: bytes, digest: str) -> bytes:\n    \"\"\"Calculate 7zip AES encryption key.\n    Concat values in order to reduce number of calls of Hash.update().\"\"\"\n    if digest not in (\"sha256\"):\n        raise ValueError(\"Unknown digest method for password protection.\")\n    assert cycles <= 0x3F\n    if cycles == 0x3F:\n        ba = bytearray(salt + password + bytes(32))\n        key: bytes = bytes(ba[:32])\n    else:\n        cat_cycle = 6\n        if cycles > cat_cycle:\n            rounds = 1 << cat_cycle\n            stages = 1 << (cycles - cat_cycle)\n        else:\n            rounds = 1 << cycles\n            stages = 1 << 0\n        m = _hashlib.new(digest)\n        saltpassword = salt + password\n        s = 0  # type: int  # (0..stages) * rounds\n        if platform.python_implementation() == \"PyPy\":\n            for _ in range(stages):\n                m.update(\n                    memoryview(\n                        b\"\".join(\n                            [saltpassword + (s + i).to_bytes(8, byteorder=\"little\", signed=False) for i in range(rounds)]\n                        )\n                    )\n                )\n                s += rounds\n        else:\n            for _ in range(stages):\n                m.update(\n                    b\"\".join([saltpassword + (s + i).to_bytes(8, byteorder=\"little\", signed=False) for i in range(rounds)])\n                )\n                s += rounds\n        key = m.digest()[:32]\n\n    return key\n\n\nif platform.python_implementation() == \"PyPy\" or sys.version_info > (3, 6):\n    calculate_key = _calculate_key3\nelse:\n    calculate_key = _calculate_key2  # it is faster when CPython 3.6.x\n\n\ndef filetime_to_dt(ft):\n    \"\"\"Convert Windows NTFS file time into python datetime object.\"\"\"\n    EPOCH_AS_FILETIME = 116444736000000000\n    us = (ft - EPOCH_AS_FILETIME) // 10\n    return datetime(1970, 1, 1, tzinfo=timezone.utc) + timedelta(microseconds=us)\n\n\nZERO = timedelta(0)\nHOUR = timedelta(hours=1)\nSECOND = timedelta(seconds=1)\n\n# A class capturing the platform's idea of local time.\n# (May result in wrong values on historical times in\n#  timezones where UTC offset and/or the DST rules had\n#  changed in the past.)\n\nSTDOFFSET = timedelta(seconds=-_time.timezone)\nif _time.daylight:\n    DSTOFFSET = timedelta(seconds=-_time.altzone)\nelse:\n    DSTOFFSET = STDOFFSET\n\nDSTDIFF = DSTOFFSET - STDOFFSET\n\n\nclass LocalTimezone(tzinfo):\n    def fromutc(self, dt):\n        assert dt.tzinfo is self\n        stamp = (dt - datetime(1970, 1, 1, tzinfo=self)) // SECOND\n        args = _time.localtime(stamp)[:6]\n        # dst_diff = DSTDIFF // SECOND\n        # Detect fold\n        # fold = args == _time.localtime(stamp - dst_diff)\n        return datetime(*args, microsecond=dt.microsecond, tzinfo=self)\n\n    def utcoffset(self, dt):\n        if self._isdst(dt):\n            return DSTOFFSET\n        else:\n            return STDOFFSET\n\n    def dst(self, dt):\n        if self._isdst(dt):\n            return DSTDIFF\n        else:\n            return ZERO\n\n    def tzname(self, dt):\n        return _time.tzname[self._isdst(dt)]\n\n    def _isdst(self, dt):\n        tt = (\n            dt.year,\n            dt.month,\n            dt.day,\n            dt.hour,\n            dt.minute,\n            dt.second,\n            dt.weekday(),\n            0,\n            0,\n        )\n        stamp = _time.mktime(tt)\n        tt = _time.localtime(stamp)\n        return tt.tm_isdst > 0\n\n\nLocal = LocalTimezone()\nTIMESTAMP_ADJUST = -11644473600\n\n\nclass UTC(tzinfo):\n    \"\"\"UTC\"\"\"\n\n    def utcoffset(self, dt):\n        return ZERO\n\n    def tzname(self, dt):\n        return \"UTC\"\n\n    def dst(self, dt):\n        return ZERO\n\n    def _call__(self):\n        return self\n\n\nclass ArchiveTimestamp(int):\n    \"\"\"Windows FILETIME timestamp.\"\"\"\n\n    def __repr__(self):\n        return \"%s(%d)\" % (type(self).__name__, self)\n\n    def __index__(self):\n        return self.__int__()\n\n    def totimestamp(self) -> float:\n        \"\"\"Convert 7z FILETIME to Python timestamp.\"\"\"\n        # FILETIME is 100-nanosecond intervals since 1601/01/01 (UTC)\n        return (self / 10000000.0) + TIMESTAMP_ADJUST\n\n    def as_datetime(self):\n        \"\"\"Convert FILETIME to Python datetime object.\"\"\"\n        return datetime.fromtimestamp(self.totimestamp(), UTC())\n\n    @staticmethod\n    def from_datetime(val):\n        return ArchiveTimestamp((val - TIMESTAMP_ADJUST) * 10000000.0)\n\n    @staticmethod\n    def from_now():\n        return ArchiveTimestamp((_time.time() - TIMESTAMP_ADJUST) * 10000000.0)\n\n\ndef islink(path):\n    \"\"\"\n    Cross-platform islink implementation.\n    Supports Windows NT symbolic links and reparse points.\n    \"\"\"\n    is_symlink = os.path.islink(str(path))\n    if sys.version_info >= (3, 8) or sys.platform != \"win32\" or sys.getwindowsversion()[0] < 6:\n        return is_symlink\n    # special check for directory junctions which py38 does.\n    if is_symlink:\n        if py7zr.win32compat.is_reparse_point(path):\n            is_symlink = False\n    return is_symlink\n\n\ndef readlink(path: Union[str, pathlib.Path], *, dir_fd=None) -> Union[str, pathlib.Path]:\n    \"\"\"\n    Cross-platform compat implementation of os.readlink and Path.readlink().\n    Supports Windows NT symbolic links and reparse points.\n    When called with path argument as pathlike(str), return result as a pathlike(str).\n    When called with Path object, return also Path object.\n    When called with path argument as bytes, return result as a bytes.\n    \"\"\"\n    if sys.version_info >= (3, 9):\n        if isinstance(path, pathlib.Path) and dir_fd is None:\n            return path.readlink()\n        else:\n            return os.readlink(path, dir_fd=dir_fd)\n    elif sys.version_info >= (3, 8) or sys.platform != \"win32\":\n        res = os.readlink(path, dir_fd=dir_fd)\n        # Hack to handle a wrong type of results\n        if isinstance(res, bytes):\n            res = os.fsdecode(res)\n        if isinstance(path, pathlib.Path):\n            return pathlib.Path(res)\n        else:\n            return res\n    elif not os.path.exists(str(path)):\n        raise OSError(22, \"Invalid argument\", path)\n    return py7zr.win32compat.readlink(path)\n\n\nclass MemIO:\n    \"\"\"pathlib.Path-like IO class to write memory(io.Bytes)\"\"\"\n\n    def __init__(self, buf: BinaryIO):\n        self._buf = buf\n\n    def write(self, data: bytes) -> int:\n        return self._buf.write(data)\n\n    def read(self, length: Optional[int] = None) -> bytes:\n        if length is not None:\n            return self._buf.read(length)\n        else:\n            return self._buf.read()\n\n    def close(self) -> None:\n        self._buf.seek(0)\n\n    def flush(self) -> None:\n        pass\n\n    def seek(self, position: int) -> None:\n        self._buf.seek(position)\n\n    def open(self, mode=None):\n        return self\n\n    @property\n    def parent(self):\n        return self\n\n    def mkdir(self, parents=None, exist_ok=False):\n        return None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\nclass NullIO:\n    \"\"\"pathlib.Path-like IO class of /dev/null\"\"\"\n\n    def __init__(self):\n        pass\n\n    def write(self, data):\n        return len(data)\n\n    def read(self, length=None):\n        if length is not None:\n            return bytes(length)\n        else:\n            return b\"\"\n\n    def close(self):\n        pass\n\n    def flush(self):\n        pass\n\n    def open(self, mode=None):\n        return self\n\n    @property\n    def parent(self):\n        return self\n\n    def mkdir(self):\n        return None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\nclass BufferOverflow(Exception):\n    pass\n\n\nclass Buffer:\n    def __init__(self, size: int = 16):\n        self._buf = bytearray(size)\n        self._buflen = 0\n        self.view = memoryview(self._buf[0:0])\n\n    def add(self, data: Union[bytes, bytearray, memoryview]):\n        length = len(data)\n        self._buf[self._buflen :] = data\n        self._buflen += length\n        self.view = memoryview(self._buf[0 : self._buflen])\n\n    def reset(self) -> None:\n        self._buflen = 0\n        self.view = memoryview(self._buf[0:0])\n\n    def set(self, data: Union[bytes, bytearray, memoryview]) -> None:\n        length = len(data)\n        self._buf[0:] = data\n        self._buflen = length\n        self.view = memoryview(self._buf[0:length])\n\n    def get(self) -> bytearray:\n        val = self._buf[: self._buflen]\n        self.reset()\n        return val\n\n    def __len__(self) -> int:\n        return self._buflen\n\n    def __bytes__(self):\n        return bytes(self._buf[0 : self._buflen])\n\n\ndef remove_relative_path_marker(path: str) -> str:\n    \"\"\"\n    Removes './' from the beginning of a path-like string\n    \"\"\"\n    processed_path = path\n\n    if path.startswith(RELATIVE_PATH_MARKER):\n        processed_path = path[len(RELATIVE_PATH_MARKER) :]\n\n    return processed_path\n", "patch": "@@ -33,6 +33,8 @@\n import _hashlib  # type: ignore  # noqa\n \n import py7zr.win32compat\n+from py7zr import Bad7zFile\n+from py7zr.win32compat import is_windows_native_python, is_windows_unc_path\n \n # String used at the beginning of relative paths\n RELATIVE_PATH_MARKER = \"./\"\n@@ -265,7 +267,7 @@ def from_now():\n def islink(path):\n     \"\"\"\n     Cross-platform islink implementation.\n-    Supports Windows NT symbolic links and reparse points.\n+    Support Windows NT symbolic links and reparse points.\n     \"\"\"\n     is_symlink = os.path.islink(str(path))\n     if sys.version_info >= (3, 8) or sys.platform != \"win32\" or sys.getwindowsversion()[0] < 6:\n@@ -280,7 +282,7 @@ def islink(path):\n def readlink(path: Union[str, pathlib.Path], *, dir_fd=None) -> Union[str, pathlib.Path]:\n     \"\"\"\n     Cross-platform compat implementation of os.readlink and Path.readlink().\n-    Supports Windows NT symbolic links and reparse points.\n+    Support Windows NT symbolic links and reparse points.\n     When called with path argument as pathlike(str), return result as a pathlike(str).\n     When called with Path object, return also Path object.\n     When called with path argument as bytes, return result as a bytes.\n@@ -431,3 +433,57 @@ def remove_relative_path_marker(path: str) -> str:\n         processed_path = path[len(RELATIVE_PATH_MARKER) :]\n \n     return processed_path\n+\n+\n+def get_sanitized_output_path(fname: str, path: Optional[pathlib.Path]) -> pathlib.Path:\n+    \"\"\"\n+    check f.filename has invalid directory traversals\n+    do following but is_relative_to introduced in py 3.9,\n+    so I replaced it with relative_to. when condition is not satisfied, raise ValueError\n+    if not pathlib.Path(...).joinpath(remove_relative_path_marker(outname)).is_relative_to(...):\n+        raise Bad7zFile\n+    \"\"\"\n+    if path is None:\n+        try:\n+            pathlib.Path(os.getcwd()).joinpath(fname).resolve().relative_to(os.getcwd())\n+            outfile = pathlib.Path(remove_relative_path_marker(fname))\n+        except ValueError:\n+            raise Bad7zFile(f\"Specified path is bad: {fname}\")\n+    else:\n+        try:\n+            outfile = path.joinpath(remove_relative_path_marker(fname))\n+            outfile.resolve().relative_to(path)\n+        except ValueError:\n+            raise Bad7zFile(f\"Specified path is bad: {fname}\")\n+    return outfile\n+\n+\n+def check_archive_path(arcname: str) -> bool:\n+    path = pathlib.Path(\"/foo/boo/fuga/hoge/a90sufoiasj09/dafj08sajfa/\")  # dummy path\n+    return is_target_path_valid(path, path.joinpath(arcname))\n+\n+\n+def is_target_path_valid(path: pathlib.Path, target: pathlib.Path) -> bool:\n+    try:\n+        if path.is_absolute():\n+            target.resolve().relative_to(path)\n+        else:\n+            target.resolve().relative_to(pathlib.Path(os.getcwd()).joinpath(path))\n+    except ValueError:\n+        return False\n+    return True\n+\n+\n+def check_win32_file_namespace(pathname: pathlib.Path) -> pathlib.Path:\n+    # When python on Windows and not python on Cygwin,\n+    # Add win32 file namespace to exceed Microsoft Windows\n+    # path length limitation to 260 bytes\n+    # ref.\n+    # https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file\n+    # In editions of Windows before Windows 10 version 1607,\n+    # the maximum length for a path is MAX_PATH, which is defined as\n+    # 260 characters. In later versions of Windows, changing a registry key\n+    # or select option when python installation is required to remove the limit.\n+    if is_windows_native_python() and pathname.is_absolute() and not is_windows_unc_path(pathname):\n+        pathname = pathlib.WindowsPath(\"\\\\\\\\?\\\\\" + str(pathname))\n+    return pathname", "file_path": "files/2022_12/1696", "file_language": "py", "file_name": "py7zr/helpers.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/miurahr/py7zr/raw/1bb43f17515c7f69673a1c88ab9cc72a7bbef406/py7zr%2Fpy7zr.py", "code": "#!/usr/bin/python -u\n#\n# p7zr library\n#\n# Copyright (c) 2019-2021 Hiroshi Miura <miurahr@linux.com>\n# Copyright (c) 2004-2015 by Joachim Bauch, mail@joachim-bauch.de\n# 7-Zip Copyright (C) 1999-2010 Igor Pavlov\n# LZMA SDK Copyright (C) 1999-2010 Igor Pavlov\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n#\n#\n\"\"\"Read 7zip format archives.\"\"\"\nimport collections.abc\nimport contextlib\nimport datetime\nimport errno\nimport functools\nimport gc\nimport io\nimport os\nimport pathlib\nimport queue\nimport stat\nimport sys\nfrom multiprocessing import Process\nfrom threading import Thread\nfrom typing import IO, Any, BinaryIO, Dict, List, Optional, Tuple, Type, Union\n\nimport multivolumefile\n\nfrom py7zr.archiveinfo import Folder, Header, SignatureHeader\nfrom py7zr.callbacks import ExtractCallback\nfrom py7zr.compressor import SupportedMethods, get_methods_names\nfrom py7zr.exceptions import Bad7zFile, CrcError, DecompressionError, InternalError, UnsupportedCompressionMethodError\nfrom py7zr.helpers import (\n    ArchiveTimestamp,\n    MemIO,\n    NullIO,\n    calculate_crc32,\n    check_archive_path,\n    filetime_to_dt,\n    get_sanitized_output_path,\n    is_target_path_valid,\n    readlink,\n)\nfrom py7zr.properties import DEFAULT_FILTERS, FILTER_DEFLATE64, MAGIC_7Z, get_default_blocksize, get_memory_limit\n\nif sys.platform.startswith(\"win\"):\n    import _winapi\n\nFILE_ATTRIBUTE_UNIX_EXTENSION = 0x8000\nFILE_ATTRIBUTE_WINDOWS_MASK = 0x07FFF\n\n\nclass ArchiveFile:\n    \"\"\"Represent each files metadata inside archive file.\n    It holds file properties; filename, permissions, and type whether\n    it is directory, link or normal file.\n\n    Instances of the :class:`ArchiveFile` class are returned by iterating :attr:`files_list` of\n    :class:`SevenZipFile` objects.\n    Each object stores information about a single member of the 7z archive. Most of users use :meth:`extractall()`.\n\n    The class also hold an archive parameter where file is exist in\n    archive file folder(container).\"\"\"\n\n    def __init__(self, id: int, file_info: Dict[str, Any]) -> None:\n        self.id = id\n        self._file_info = file_info\n\n    def file_properties(self) -> Dict[str, Any]:\n        \"\"\"Return file properties as a hash object. Following keys are included: \u2018readonly\u2019, \u2018is_directory\u2019,\n        \u2018posix_mode\u2019, \u2018archivable\u2019, \u2018emptystream\u2019, \u2018filename\u2019, \u2018creationtime\u2019, \u2018lastaccesstime\u2019,\n        \u2018lastwritetime\u2019, \u2018attributes\u2019\n        \"\"\"\n        properties = self._file_info\n        if properties is not None:\n            properties[\"readonly\"] = self.readonly\n            properties[\"posix_mode\"] = self.posix_mode\n            properties[\"archivable\"] = self.archivable\n            properties[\"is_directory\"] = self.is_directory\n        return properties\n\n    def _get_property(self, key: str) -> Any:\n        try:\n            return self._file_info[key]\n        except KeyError:\n            return None\n\n    @property\n    def origin(self) -> pathlib.Path:\n        return self._get_property(\"origin\")\n\n    @property\n    def folder(self) -> Folder:\n        return self._get_property(\"folder\")\n\n    @property\n    def filename(self) -> str:\n        \"\"\"return filename of archive file.\"\"\"\n        return self._get_property(\"filename\")\n\n    @property\n    def emptystream(self) -> bool:\n        \"\"\"True if file is empty(0-byte file), otherwise False\"\"\"\n        return self._get_property(\"emptystream\")\n\n    @property\n    def uncompressed(self) -> List[int]:\n        return self._get_property(\"uncompressed\")\n\n    @property\n    def compressed(self) -> Optional[int]:\n        \"\"\"Compressed size\"\"\"\n        return self._get_property(\"compressed\")\n\n    @property\n    def crc32(self) -> Optional[int]:\n        \"\"\"CRC of archived file(optional)\"\"\"\n        return self._get_property(\"digest\")\n\n    def _test_attribute(self, target_bit: int) -> bool:\n        attributes = self._get_property(\"attributes\")\n        if attributes is None:\n            return False\n        return attributes & target_bit == target_bit\n\n    @property\n    def archivable(self) -> bool:\n        \"\"\"File has a Windows `archive` flag.\"\"\"\n        if hasattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\"):\n            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\"))\n        return False\n\n    @property\n    def is_directory(self) -> bool:\n        \"\"\"True if file is a directory, otherwise False.\"\"\"\n        if hasattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\"):\n            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\"))\n        return False\n\n    @property\n    def readonly(self) -> bool:\n        \"\"\"True if file is readonly, otherwise False.\"\"\"\n        if hasattr(stat, \"FILE_ATTRIBUTE_READONLY\"):\n            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_READONLY\"))\n        return False\n\n    def _get_unix_extension(self) -> Optional[int]:\n        attributes = self._get_property(\"attributes\")\n        if self._test_attribute(FILE_ATTRIBUTE_UNIX_EXTENSION):\n            return attributes >> 16\n        return None\n\n    def data(self) -> Optional[BinaryIO]:\n        return self._get_property(\"data\")\n\n    def has_strdata(self) -> bool:\n        \"\"\"True if file content is set by writestr() method otherwise False.\"\"\"\n        return \"data\" in self._file_info\n\n    @property\n    def is_symlink(self) -> bool:\n        \"\"\"True if file is a symbolic link, otherwise False.\"\"\"\n        e = self._get_unix_extension()\n        if e is not None:\n            return stat.S_ISLNK(e)\n        if hasattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"):\n            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"))\n        return False\n\n    @property\n    def is_junction(self) -> bool:\n        \"\"\"True if file is a junction/reparse point on windows, otherwise False.\"\"\"\n        if hasattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"):\n            return self._test_attribute(\n                getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\") | getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")\n            )\n        return False\n\n    @property\n    def is_socket(self) -> bool:\n        \"\"\"True if file is a socket, otherwise False.\"\"\"\n        e = self._get_unix_extension()\n        if e is not None:\n            return stat.S_ISSOCK(e)\n        return False\n\n    @property\n    def lastwritetime(self) -> Optional[ArchiveTimestamp]:\n        \"\"\"Return last written timestamp of a file.\"\"\"\n        return self._get_property(\"lastwritetime\")\n\n    @property\n    def posix_mode(self) -> Optional[int]:\n        \"\"\"\n        posix mode when a member has a unix extension property, or None\n        :return: Return file stat mode can be set by os.chmod()\n        \"\"\"\n        e = self._get_unix_extension()\n        if e is not None:\n            return stat.S_IMODE(e)\n        return None\n\n    @property\n    def st_fmt(self) -> Optional[int]:\n        \"\"\"\n        :return: Return the portion of the file mode that describes the file type\n        \"\"\"\n        e = self._get_unix_extension()\n        if e is not None:\n            return stat.S_IFMT(e)\n        return None\n\n\nclass ArchiveFileList(collections.abc.Iterable):\n    \"\"\"Iteratable container of ArchiveFile.\"\"\"\n\n    def __init__(self, offset: int = 0):\n        self.files_list: List[dict] = []\n        self.index = 0\n        self.offset = offset\n\n    def append(self, file_info: Dict[str, Any]) -> None:\n        self.files_list.append(file_info)\n\n    def __len__(self) -> int:\n        return len(self.files_list)\n\n    def __iter__(self) -> \"ArchiveFileListIterator\":\n        return ArchiveFileListIterator(self)\n\n    def __getitem__(self, index):\n        if index > len(self.files_list):\n            raise IndexError\n        if index < 0:\n            raise IndexError\n        res = ArchiveFile(index + self.offset, self.files_list[index])\n        return res\n\n\nclass ArchiveFileListIterator(collections.abc.Iterator):\n    def __init__(self, archive_file_list):\n        self._archive_file_list = archive_file_list\n        self._index = 0\n\n    def __next__(self) -> ArchiveFile:\n        if self._index == len(self._archive_file_list):\n            raise StopIteration\n        res = self._archive_file_list[self._index]\n        self._index += 1\n        return res\n\n\n# ------------------\n# Exported Classes\n# ------------------\nclass ArchiveInfo:\n    \"\"\"Hold archive information\"\"\"\n\n    def __init__(\n        self,\n        filename: str,\n        stat: os.stat_result,\n        header_size: int,\n        method_names: List[str],\n        solid: bool,\n        blocks: int,\n        uncompressed: List[int],\n    ):\n        self.stat = stat\n        self.filename = filename\n        self.size = stat.st_size\n        self.header_size = header_size\n        self.method_names = method_names\n        self.solid = solid\n        self.blocks = blocks\n        self.uncompressed = uncompressed\n\n\nclass FileInfo:\n    \"\"\"Hold archived file information.\"\"\"\n\n    def __init__(\n        self,\n        filename,\n        compressed,\n        uncompressed,\n        archivable,\n        is_directory,\n        creationtime,\n        crc32,\n    ):\n        self.filename = filename\n        self.compressed = compressed\n        self.uncompressed = uncompressed\n        self.archivable = archivable\n        self.is_directory = is_directory\n        self.creationtime = creationtime\n        self.crc32 = crc32\n\n\nclass SevenZipFile(contextlib.AbstractContextManager):\n    \"\"\"The SevenZipFile Class provides an interface to 7z archives.\"\"\"\n\n    def __init__(\n        self,\n        file: Union[BinaryIO, str, pathlib.Path],\n        mode: str = \"r\",\n        *,\n        filters: Optional[List[Dict[str, int]]] = None,\n        dereference=False,\n        password: Optional[str] = None,\n        header_encryption: bool = False,\n        blocksize: Optional[int] = None,\n        mp: bool = False,\n    ) -> None:\n        if mode not in (\"r\", \"w\", \"x\", \"a\"):\n            raise ValueError(\"ZipFile requires mode 'r', 'w', 'x', or 'a'\")\n        self.fp: BinaryIO\n        self.mp = mp\n        self.password_protected = password is not None\n        if blocksize:\n            self._block_size = blocksize\n        else:\n            self._block_size = get_default_blocksize()\n        # Check if we were passed a file-like object or not\n        if isinstance(file, str):\n            self._filePassed: bool = False\n            self.filename: str = file\n            if mode == \"r\":\n                self.fp = open(file, \"rb\")\n            elif mode == \"w\":\n                self.fp = open(file, \"w+b\")\n            elif mode == \"x\":\n                self.fp = open(file, \"x+b\")\n            elif mode == \"a\":\n                self.fp = open(file, \"r+b\")\n            else:\n                raise ValueError(\"File open error.\")\n            self.mode = mode\n        elif isinstance(file, pathlib.Path):\n            self._filePassed = False\n            self.filename = str(file)\n            if mode == \"r\":\n                self.fp = file.open(mode=\"rb\")  # noqa   # typeshed issue: 2911\n            elif mode == \"w\":\n                self.fp = file.open(mode=\"w+b\")  # noqa\n            elif mode == \"x\":\n                self.fp = file.open(mode=\"x+b\")  # noqa\n            elif mode == \"a\":\n                self.fp = file.open(mode=\"r+b\")  # noqa\n            else:\n                raise ValueError(\"File open error.\")\n            self.mode = mode\n        elif isinstance(file, multivolumefile.MultiVolume):\n            self._filePassed = True\n            self.fp = file\n            self.filename = None\n            self.mode = mode  # noqa\n        elif isinstance(file, io.IOBase):\n            self._filePassed = True\n            self.fp = file\n            self.filename = getattr(file, \"name\", None)\n            self.mode = mode  # noqa\n        else:\n            raise TypeError(\"invalid file: {}\".format(type(file)))\n        self.encoded_header_mode = True\n        self.header_encryption = header_encryption\n        self._fileRefCnt = 1\n        try:\n            if mode == \"r\":\n                self._real_get_contents(password)\n                self.fp.seek(self.afterheader)  # seek into start of payload and prepare worker to extract\n                self.worker = Worker(self.files, self.afterheader, self.header, self.mp)\n            elif mode in \"w\":\n                self._prepare_write(filters, password)\n            elif mode in \"x\":\n                raise NotImplementedError\n            elif mode == \"a\":\n                self._real_get_contents(password)\n                self._prepare_append(filters, password)\n            else:\n                raise ValueError(\"Mode must be 'r', 'w', 'x', or 'a'\")\n        except Exception as e:\n            self._fpclose()\n            raise e\n        self._dict: Dict[str, IO[Any]] = {}\n        self.dereference = dereference\n        self.reporterd: Optional[Thread] = None\n        self.q: queue.Queue[Any] = queue.Queue()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def _fpclose(self) -> None:\n        assert self._fileRefCnt > 0\n        self._fileRefCnt -= 1\n        if not self._fileRefCnt and not self._filePassed:\n            self.fp.close()\n\n    def _real_get_contents(self, password) -> None:\n        if not self._check_7zfile(self.fp):\n            raise Bad7zFile(\"not a 7z file\")\n        self.sig_header = SignatureHeader.retrieve(self.fp)\n        self.afterheader: int = self.fp.tell()\n        self.fp.seek(self.sig_header.nextheaderofs, os.SEEK_CUR)\n        buffer = io.BytesIO(self.fp.read(self.sig_header.nextheadersize))\n        if self.sig_header.nextheadercrc != calculate_crc32(buffer.getvalue()):\n            raise Bad7zFile(\"invalid header data\")\n        header = Header.retrieve(self.fp, buffer, self.afterheader, password)\n        if header is None:\n            return\n        header._initilized = True\n        self.header = header\n        header.size += 32 + self.sig_header.nextheadersize\n        buffer.close()\n        self.files = ArchiveFileList()\n        if getattr(self.header, \"files_info\", None) is None:\n            return\n        # Initialize references for convenience\n        if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:\n            folders = self.header.main_streams.unpackinfo.folders\n            for folder in folders:\n                folder.password = password\n            packinfo = self.header.main_streams.packinfo\n            packsizes = packinfo.packsizes\n            subinfo = self.header.main_streams.substreamsinfo\n            if subinfo is not None and subinfo.unpacksizes is not None:\n                unpacksizes = subinfo.unpacksizes\n            else:\n                unpacksizes = [x.unpacksizes[-1] for x in folders]\n        else:\n            subinfo = None\n            folders = None\n            packinfo = None\n            packsizes = []\n            unpacksizes = [0]\n\n        pstat = self.ParseStatus()\n        pstat.src_pos = self.afterheader\n        file_in_solid = 0\n\n        for file_id, file_info in enumerate(self.header.files_info.files):\n            if not file_info[\"emptystream\"] and folders is not None:\n                folder = folders[pstat.folder]\n                numinstreams = max([coder.get(\"numinstreams\", 1) for coder in folder.coders])\n                (maxsize, compressed, uncompressed, packsize, solid,) = self._get_fileinfo_sizes(\n                    pstat,\n                    subinfo,\n                    packinfo,\n                    folder,\n                    packsizes,\n                    unpacksizes,\n                    file_in_solid,\n                    numinstreams,\n                )\n                pstat.input += 1\n                folder.solid = solid\n                file_info[\"folder\"] = folder\n                file_info[\"maxsize\"] = maxsize\n                file_info[\"compressed\"] = compressed\n                file_info[\"uncompressed\"] = uncompressed\n                file_info[\"packsizes\"] = packsize\n                if subinfo.digestsdefined[pstat.outstreams]:\n                    file_info[\"digest\"] = subinfo.digests[pstat.outstreams]\n                if folder is None:\n                    pstat.src_pos += file_info[\"compressed\"]\n                else:\n                    if folder.solid:\n                        file_in_solid += 1\n                    pstat.outstreams += 1\n                    if folder.files is None:\n                        folder.files = ArchiveFileList(offset=file_id)\n                    folder.files.append(file_info)\n                    if pstat.input >= subinfo.num_unpackstreams_folders[pstat.folder]:\n                        file_in_solid = 0\n                        pstat.src_pos += sum(packinfo.packsizes[pstat.stream : pstat.stream + numinstreams])\n                        pstat.folder += 1\n                        pstat.stream += numinstreams\n                        pstat.input = 0\n            else:\n                file_info[\"folder\"] = None\n                file_info[\"maxsize\"] = 0\n                file_info[\"compressed\"] = 0\n                file_info[\"uncompressed\"] = 0\n                file_info[\"packsizes\"] = [0]\n\n            if \"filename\" not in file_info:\n                # compressed file is stored without a name, generate one\n                try:\n                    basefilename = self.filename\n                except AttributeError:\n                    # 7z archive file doesn't have a name\n                    file_info[\"filename\"] = \"contents\"\n                else:\n                    if basefilename is not None:\n                        fn, ext = os.path.splitext(os.path.basename(basefilename))\n                        file_info[\"filename\"] = fn\n                    else:\n                        file_info[\"filename\"] = \"contents\"\n            self.files.append(file_info)\n        if not self.password_protected and self.header.main_streams is not None:\n            # Check specified coders have a crypt method or not.\n            self.password_protected = any(\n                [SupportedMethods.needs_password(folder.coders) for folder in self.header.main_streams.unpackinfo.folders]\n            )\n\n    def _extract(\n        self,\n        path: Optional[Any] = None,\n        targets: Optional[List[str]] = None,\n        return_dict: bool = False,\n        callback: Optional[ExtractCallback] = None,\n    ) -> Optional[Dict[str, IO[Any]]]:\n        if callback is None:\n            pass\n        elif isinstance(callback, ExtractCallback):\n            self.reporterd = Thread(target=self.reporter, args=(callback,), daemon=True)\n            self.reporterd.start()\n        else:\n            raise ValueError(\"Callback specified is not an instance of subclass of py7zr.callbacks.ExtractCallback class\")\n        target_files: List[Tuple[pathlib.Path, Dict[str, Any]]] = []\n        target_dirs: List[pathlib.Path] = []\n        if path is not None:\n            if isinstance(path, str):\n                path = pathlib.Path(path)\n            try:\n                if not path.exists():\n                    path.mkdir(parents=True)\n                else:\n                    pass\n            except OSError as e:\n                if e.errno == errno.EEXIST and path.is_dir():\n                    pass\n                else:\n                    raise e\n        fnames: List[str] = []  # check duplicated filename in one archive?\n        self.q.put((\"pre\", None, None))\n        for f in self.files:\n            # When archive has a multiple files which have same name\n            # To guarantee order of archive, multi-thread decompression becomes off.\n            # Currently always overwrite by latter archives.\n            # TODO: provide option to select overwrite or skip.\n            if f.filename not in fnames:\n                outname = f.filename\n            else:\n                i = 0\n                while True:\n                    outname = f.filename + \"_%d\" % i\n                    if outname not in fnames:\n                        break\n                    i += 1\n            fnames.append(outname)\n            if path is None or path.is_absolute():\n                outfilename = get_sanitized_output_path(outname, path)\n            else:\n                outfilename = get_sanitized_output_path(outname, pathlib.Path(os.getcwd()).joinpath(path))\n            if targets is not None and f.filename not in targets:\n                self.worker.register_filelike(f.id, None)\n                continue\n            if return_dict:\n                if f.is_directory or f.is_socket:\n                    # ignore special files and directories\n                    pass\n                else:\n                    fname = outfilename.as_posix()\n                    _buf = io.BytesIO()\n                    self._dict[fname] = _buf\n                    self.worker.register_filelike(f.id, MemIO(_buf))\n            elif f.is_directory:\n                if not outfilename.exists():\n                    target_dirs.append(outfilename)\n                    target_files.append((outfilename, f.file_properties()))\n                else:\n                    pass\n            elif f.is_socket:\n                pass  # TODO: implement me.\n            elif f.is_symlink or f.is_junction:\n                self.worker.register_filelike(f.id, outfilename)\n            else:\n                self.worker.register_filelike(f.id, outfilename)\n                target_files.append((outfilename, f.file_properties()))\n        for target_dir in sorted(target_dirs):\n            try:\n                target_dir.mkdir(parents=True)\n            except FileExistsError:\n                if target_dir.is_dir():\n                    pass\n                elif target_dir.is_file():\n                    raise DecompressionError(\"Directory {} is existed as a normal file.\".format(str(target_dir)))\n                else:\n                    raise DecompressionError(\"Directory {} making fails on unknown condition.\".format(str(target_dir)))\n\n        if callback is not None:\n            self.worker.extract(\n                self.fp,\n                path,\n                parallel=(not self.password_protected and not self._filePassed),\n                q=self.q,\n            )\n        else:\n            self.worker.extract(\n                self.fp,\n                path,\n                parallel=(not self.password_protected and not self._filePassed),\n            )\n\n        self.q.put((\"post\", None, None))\n        # early return when dict specified\n        if return_dict:\n            return self._dict\n        # set file properties\n        for outfilename, properties in target_files:\n            # mtime\n            lastmodified = None\n            try:\n                lastmodified = ArchiveTimestamp(properties[\"lastwritetime\"]).totimestamp()\n            except KeyError:\n                pass\n            if lastmodified is not None:\n                os.utime(str(outfilename), times=(lastmodified, lastmodified))\n            if os.name == \"posix\":\n                st_mode = properties[\"posix_mode\"]\n                if st_mode is not None:\n                    outfilename.chmod(st_mode)\n                    continue\n            # fallback: only set readonly if specified\n            if properties[\"readonly\"] and not properties[\"is_directory\"]:\n                ro_mask = 0o777 ^ (stat.S_IWRITE | stat.S_IWGRP | stat.S_IWOTH)\n                outfilename.chmod(outfilename.stat().st_mode & ro_mask)\n        return None\n\n    def _prepare_append(self, filters, password):\n        if password is not None and filters is None:\n            filters = DEFAULT_FILTERS.ENCRYPTED_ARCHIVE_FILTER\n        elif filters is None:\n            filters = DEFAULT_FILTERS.ARCHIVE_FILTER\n        else:\n            for f in filters:\n                if f[\"id\"] == FILTER_DEFLATE64:\n                    raise UnsupportedCompressionMethodError(filters, \"Compression with deflate64 is not supported.\")\n        self.header.filters = filters\n        self.header.password = password\n        if self.header.main_streams is not None:\n            pos = self.afterheader + self.header.main_streams.packinfo.packpositions[-1]\n        else:\n            pos = self.afterheader\n        self.fp.seek(pos)\n        self.worker = Worker(self.files, pos, self.header, self.mp)\n\n    def _prepare_write(self, filters, password):\n        if password is not None and filters is None:\n            filters = DEFAULT_FILTERS.ENCRYPTED_ARCHIVE_FILTER\n        elif filters is None:\n            filters = DEFAULT_FILTERS.ARCHIVE_FILTER\n        self.files = ArchiveFileList()\n        self.sig_header = SignatureHeader()\n        self.sig_header._write_skelton(self.fp)\n        self.afterheader = self.fp.tell()\n        self.header = Header.build_header(filters, password)\n        self.fp.seek(self.afterheader)\n        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)\n\n    def _write_flush(self):\n        if self.header._initialized:\n            folder = self.header.main_streams.unpackinfo.folders[-1]\n            self.worker.flush_archive(self.fp, folder)\n        self._write_header()\n\n    def _write_header(self):\n        \"\"\"Write header and update signature header.\"\"\"\n        (header_pos, header_len, header_crc) = self.header.write(\n            self.fp,\n            self.afterheader,\n            encoded=self.encoded_header_mode,\n            encrypted=self.header_encryption,\n        )\n        self.sig_header.nextheaderofs = header_pos - self.afterheader\n        self.sig_header.calccrc(header_len, header_crc)\n        self.sig_header.write(self.fp)\n\n    def _writeall(self, path, arcname):\n        try:\n            if path.is_symlink() and not self.dereference:\n                self.write(path, arcname)\n            elif path.is_file():\n                self.write(path, arcname)\n            elif path.is_dir():\n                if not path.samefile(\".\"):\n                    self.write(path, arcname)\n                for nm in sorted(os.listdir(str(path))):\n                    arc = os.path.join(arcname, nm) if arcname is not None else None\n                    self._writeall(path.joinpath(nm), arc)\n            else:\n                return  # pathlib ignores ELOOP and return False for is_*().\n        except OSError as ose:\n            if self.dereference and ose.errno in [errno.ELOOP]:\n                return  # ignore ELOOP here, this resulted to stop looped symlink reference.\n            elif self.dereference and sys.platform == \"win32\" and ose.errno in [errno.ENOENT]:\n                return  # ignore ENOENT which is happened when a case of ELOOP on windows.\n            else:\n                raise\n\n    class ParseStatus:\n        def __init__(self, src_pos=0):\n            self.src_pos = src_pos\n            self.folder = 0  # 7zip folder where target stored\n            self.outstreams = 0  # output stream count\n            self.input = 0  # unpack stream count in each folder\n            self.stream = 0  # target input stream position\n\n    def _get_fileinfo_sizes(\n        self,\n        pstat,\n        subinfo,\n        packinfo,\n        folder,\n        packsizes,\n        unpacksizes,\n        file_in_solid,\n        numinstreams,\n    ):\n        if pstat.input == 0:\n            folder.solid = subinfo.num_unpackstreams_folders[pstat.folder] > 1\n        maxsize = (folder.solid and packinfo.packsizes[pstat.stream]) or None\n        uncompressed = unpacksizes[pstat.outstreams]\n        if file_in_solid > 0:\n            compressed = None\n        elif pstat.stream < len(packsizes):  # file is compressed\n            compressed = packsizes[pstat.stream]\n        else:  # file is not compressed\n            compressed = uncompressed\n        packsize = packsizes[pstat.stream : pstat.stream + numinstreams]\n        return maxsize, compressed, uncompressed, packsize, folder.solid\n\n    def set_encoded_header_mode(self, mode: bool) -> None:\n        if mode:\n            self.encoded_header_mode = True\n        else:\n            self.encoded_header_mode = False\n            self.header_encryption = False\n\n    def set_encrypted_header(self, mode: bool) -> None:\n        if mode:\n            self.encoded_header_mode = True\n            self.header_encryption = True\n        else:\n            self.header_encryption = False\n\n    @staticmethod\n    def _check_7zfile(fp: Union[BinaryIO, io.BufferedReader, io.IOBase]) -> bool:\n        result = MAGIC_7Z == fp.read(len(MAGIC_7Z))[: len(MAGIC_7Z)]\n        fp.seek(-len(MAGIC_7Z), 1)\n        return result\n\n    def _get_method_names(self) -> List[str]:\n        try:\n            return get_methods_names([folder.coders for folder in self.header.main_streams.unpackinfo.folders])\n        except KeyError:\n            raise UnsupportedCompressionMethodError(self.header.main_streams.unpackinfo.folders, \"Unknown method\")\n\n    def _read_digest(self, pos: int, size: int) -> int:\n        self.fp.seek(pos)\n        remaining_size = size\n        digest = 0\n        while remaining_size > 0:\n            block = min(self._block_size, remaining_size)\n            digest = calculate_crc32(self.fp.read(block), digest)\n            remaining_size -= block\n        return digest\n\n    def _is_solid(self):\n        for f in self.header.main_streams.substreamsinfo.num_unpackstreams_folders:\n            if f > 1:\n                return True\n        return False\n\n    def _var_release(self):\n        self._dict = None\n        self.worker.close()\n        del self.worker\n        del self.files\n        del self.header\n        del self.sig_header\n        gc.collect()\n\n    @staticmethod\n    def _make_file_info(target: pathlib.Path, arcname: Optional[str] = None, dereference=False) -> Dict[str, Any]:\n        f: Dict[str, Any] = {}\n        f[\"origin\"] = target\n        if arcname is not None:\n            f[\"filename\"] = pathlib.Path(arcname).as_posix()\n        else:\n            f[\"filename\"] = target.as_posix()\n        if sys.platform == \"win32\":\n            fstat = target.lstat()\n            if target.is_symlink():\n                if dereference:\n                    fstat = target.stat()\n                    if stat.S_ISDIR(fstat.st_mode):\n                        f[\"emptystream\"] = True\n                        f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa\n                    else:\n                        f[\"emptystream\"] = False\n                        f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE  # noqa\n                        f[\"uncompressed\"] = fstat.st_size\n                else:\n                    f[\"emptystream\"] = False\n                    f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa\n                    # TODO: handle junctions\n                    # f['attributes'] |= stat.FILE_ATTRIBUTE_REPARSE_POINT  # noqa\n            elif target.is_dir():\n                f[\"emptystream\"] = True\n                f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa\n            elif target.is_file():\n                f[\"emptystream\"] = False\n                f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE  # noqa\n                f[\"uncompressed\"] = fstat.st_size\n        elif (\n            sys.platform == \"darwin\"\n            or sys.platform.startswith(\"linux\")\n            or sys.platform.startswith(\"freebsd\")\n            or sys.platform.startswith(\"netbsd\")\n            or sys.platform.startswith(\"sunos\")\n            or sys.platform == \"aix\"\n        ):\n            fstat = target.lstat()\n            if target.is_symlink():\n                if dereference:\n                    fstat = target.stat()\n                    if stat.S_ISDIR(fstat.st_mode):\n                        f[\"emptystream\"] = True\n                        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")\n                        f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFDIR << 16)\n                        f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16\n                    else:\n                        f[\"emptystream\"] = False\n                        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")\n                        f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IMODE(fstat.st_mode) << 16)\n                else:\n                    f[\"emptystream\"] = False\n                    f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\") | getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\")\n                    f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFLNK << 16)\n                    f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16\n            elif target.is_dir():\n                f[\"emptystream\"] = True\n                f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")\n                f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFDIR << 16)\n                f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16\n            elif target.is_file():\n                f[\"emptystream\"] = False\n                f[\"uncompressed\"] = fstat.st_size\n                f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")\n                f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IMODE(fstat.st_mode) << 16)\n        else:\n            fstat = target.stat()\n            if target.is_dir():\n                f[\"emptystream\"] = True\n                f[\"attributes\"] = stat.FILE_ATTRIBUTE_DIRECTORY\n            elif target.is_file():\n                f[\"emptystream\"] = False\n                f[\"uncompressed\"] = fstat.st_size\n                f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE\n\n        f[\"creationtime\"] = ArchiveTimestamp.from_datetime(fstat.st_ctime)\n        f[\"lastwritetime\"] = ArchiveTimestamp.from_datetime(fstat.st_mtime)\n        f[\"lastaccesstime\"] = ArchiveTimestamp.from_datetime(fstat.st_atime)\n        return f\n\n    def _make_file_info_from_name(self, bio, size: int, arcname: str) -> Dict[str, Any]:\n        f: Dict[str, Any] = {}\n        f[\"origin\"] = None\n        f[\"data\"] = bio\n        f[\"filename\"] = pathlib.Path(arcname).as_posix()\n        f[\"uncompressed\"] = size\n        f[\"emptystream\"] = size == 0\n        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")\n        f[\"creationtime\"] = ArchiveTimestamp.from_now()\n        f[\"lastwritetime\"] = ArchiveTimestamp.from_now()\n        return f\n\n    # --------------------------------------------------------------------------\n    # The public methods which SevenZipFile provides:\n    def getnames(self) -> List[str]:\n        \"\"\"Return the members of the archive as a list of their names. It has\n        the same order as the list returned by getmembers().\n        \"\"\"\n        return list(map(lambda x: x.filename, self.files))\n\n    def archiveinfo(self) -> ArchiveInfo:\n        total_uncompressed = functools.reduce(lambda x, y: x + y, [f.uncompressed for f in self.files])\n        if isinstance(self.fp, multivolumefile.MultiVolume):\n            fname = self.fp.name\n            fstat = self.fp.stat()\n        else:\n            fname = self.filename\n            assert fname is not None\n            fstat = os.stat(fname)\n        return ArchiveInfo(\n            fname,\n            fstat,\n            self.header.size,\n            self._get_method_names(),\n            self._is_solid(),\n            len(self.header.main_streams.unpackinfo.folders),\n            total_uncompressed,\n        )\n\n    def needs_password(self) -> bool:\n        return self.password_protected\n\n    def list(self) -> List[FileInfo]:\n        \"\"\"Returns contents information\"\"\"\n        alist: List[FileInfo] = []\n        lastmodified: Optional[datetime.datetime] = None\n        for f in self.files:\n            if f.lastwritetime is not None:\n                lastmodified = filetime_to_dt(f.lastwritetime)\n            alist.append(\n                FileInfo(\n                    f.filename,\n                    f.compressed,\n                    f.uncompressed,\n                    f.archivable,\n                    f.is_directory,\n                    lastmodified,\n                    f.crc32,\n                )\n            )\n        return alist\n\n    def readall(self) -> Optional[Dict[str, IO[Any]]]:\n        self._dict = {}\n        return self._extract(path=None, return_dict=True)\n\n    def extractall(self, path: Optional[Any] = None, callback: Optional[ExtractCallback] = None) -> None:\n        \"\"\"Extract all members from the archive to the current working\n        directory and set owner, modification time and permissions on\n        directories afterwards. ``path`` specifies a different directory\n        to extract to.\n        \"\"\"\n        self._extract(path=path, return_dict=False, callback=callback)\n\n    def read(self, targets: Optional[List[str]] = None) -> Optional[Dict[str, IO[Any]]]:\n        self._dict = {}\n        return self._extract(path=None, targets=targets, return_dict=True)\n\n    def extract(self, path: Optional[Any] = None, targets: Optional[List[str]] = None) -> None:\n        self._extract(path, targets, return_dict=False)\n\n    def reporter(self, callback: ExtractCallback):\n        while True:\n            try:\n                item: Optional[Tuple[str, str, str]] = self.q.get(timeout=1)\n            except queue.Empty:\n                pass\n            else:\n                if item is None:\n                    break\n                elif item[0] == \"s\":\n                    callback.report_start(item[1], item[2])\n                elif item[0] == \"e\":\n                    callback.report_end(item[1], item[2])\n                elif item[0] == \"pre\":\n                    callback.report_start_preparation()\n                elif item[0] == \"post\":\n                    callback.report_postprocess()\n                elif item[0] == \"w\":\n                    callback.report_warning(item[1])\n                else:\n                    pass\n                self.q.task_done()\n\n    def writeall(self, path: Union[pathlib.Path, str], arcname: Optional[str] = None):\n        \"\"\"Write files in target path into archive.\"\"\"\n        if isinstance(path, str):\n            path = pathlib.Path(path)\n        if not path.exists():\n            raise ValueError(\"specified path does not exist.\")\n        if path.is_dir() or path.is_file():\n            self._writeall(path, arcname)\n        else:\n            raise ValueError(\"specified path is not a directory or a file\")\n\n    def write(self, file: Union[pathlib.Path, str], arcname: Optional[str] = None):\n        \"\"\"Write single target file into archive.\"\"\"\n        if isinstance(file, str):\n            path = pathlib.Path(file)\n        elif isinstance(file, pathlib.Path):\n            path = file\n        else:\n            raise ValueError(\"Unsupported file type.\")\n        folder = self.header.initialize()\n        file_info = self._make_file_info(path, arcname, self.dereference)\n        self.header.files_info.files.append(file_info)\n        self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])\n        self.files.append(file_info)\n        self.worker.archive(self.fp, self.files, folder, deref=self.dereference)\n\n    def writed(self, targets: Dict[str, IO[Any]]) -> None:\n        for target, input in targets.items():\n            self.writef(input, target)\n\n    def writef(self, bio: IO[Any], arcname: str):\n        if not check_archive_path(arcname):\n            raise ValueError(f\"Specified path is bad: {arcname}\")\n        return self._writef(bio, arcname)\n\n    def _writef(self, bio: IO[Any], arcname: str):\n        if isinstance(bio, io.BytesIO):\n            size = bio.getbuffer().nbytes\n        elif isinstance(bio, io.TextIOBase):\n            # First check whether is it Text?\n            raise ValueError(\"Unsupported file object type: please open file with Binary mode.\")\n        elif isinstance(bio, io.BufferedIOBase):\n            # come here when subtype of io.BufferedIOBase that don't have __sizeof__ (eg. pypy)\n            # alternative for `size = bio.__sizeof__()`\n            current = bio.tell()\n            bio.seek(0, os.SEEK_END)\n            last = bio.tell()\n            bio.seek(current, os.SEEK_SET)\n            size = last - current\n        else:\n            raise ValueError(\"Wrong argument passed for argument bio.\")\n        if size > 0:\n            folder = self.header.initialize()\n            file_info = self._make_file_info_from_name(bio, size, arcname)\n            self.header.files_info.files.append(file_info)\n            self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])\n            self.files.append(file_info)\n            self.worker.archive(self.fp, self.files, folder, deref=False)\n        else:\n            file_info = self._make_file_info_from_name(bio, size, arcname)\n            self.header.files_info.files.append(file_info)\n            self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])\n            self.files.append(file_info)\n\n    def writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):\n        if not check_archive_path(arcname):\n            raise ValueError(f\"Specified path is bad: {arcname}\")\n        return self._writestr(data, arcname)\n\n    def _writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):\n        if not isinstance(arcname, str):\n            raise ValueError(\"Unsupported arcname\")\n        if isinstance(data, str):\n            self._writef(io.BytesIO(data.encode(\"UTF-8\")), arcname)\n        elif isinstance(data, bytes) or isinstance(data, bytearray) or isinstance(data, memoryview):\n            self._writef(io.BytesIO(bytes(data)), arcname)\n        else:\n            raise ValueError(\"Unsupported data type.\")\n\n    def close(self):\n        \"\"\"Flush all the data into archive and close it.\n        When close py7zr start reading target and writing actual archive file.\n        \"\"\"\n        if \"w\" in self.mode:\n            self._write_flush()\n        if \"a\" in self.mode:\n            self._write_flush()\n        if \"r\" in self.mode:\n            if self.reporterd is not None:\n                self.q.put_nowait(None)\n                self.reporterd.join(1)\n                if self.reporterd.is_alive():\n                    raise InternalError(\"Progress report thread terminate error.\")\n                self.reporterd = None\n        self._fpclose()\n        self._var_release()\n\n    def reset(self) -> None:\n        \"\"\"\n        When read mode, it reset file pointer, decompress worker and decompressor\n        \"\"\"\n        if self.mode == \"r\":\n            self.fp.seek(self.afterheader)\n            self.worker = Worker(self.files, self.afterheader, self.header, self.mp)\n            if self.header.main_streams is not None and self.header.main_streams.unpackinfo.numfolders > 0:\n                for i, folder in enumerate(self.header.main_streams.unpackinfo.folders):\n                    folder.decompressor = None\n\n    def test(self) -> Optional[bool]:\n        self.fp.seek(self.afterheader)\n        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)\n        crcs: Optional[List[int]] = self.header.main_streams.packinfo.crcs\n        if crcs is None or len(crcs) == 0:\n            return None\n        packpos = self.afterheader + self.header.main_streams.packinfo.packpos\n        packsizes = self.header.main_streams.packinfo.packsizes\n        digestdefined = self.header.main_streams.packinfo.digestdefined\n        j = 0\n        for i, d in enumerate(digestdefined):\n            if d:\n                if self._read_digest(packpos, packsizes[i]) != crcs[j]:\n                    return False\n                j += 1\n            packpos += packsizes[i]\n        return True\n\n    def testzip(self) -> Optional[str]:\n        self.fp.seek(self.afterheader)\n        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)\n        for f in self.files:\n            self.worker.register_filelike(f.id, None)\n        try:\n            self.worker.extract(\n                self.fp, None, parallel=(not self.password_protected), skip_notarget=False\n            )  # TODO: print progress\n        except CrcError as crce:\n            return crce.args[2]\n        else:\n            return None\n\n\n# --------------------\n# exported functions\n# --------------------\ndef is_7zfile(file: Union[BinaryIO, str, pathlib.Path]) -> bool:\n    \"\"\"Quickly see if a file is a 7Z file by checking the magic number.\n    The file argument may be a filename or file-like object too.\n    \"\"\"\n    result = False\n    try:\n        if (isinstance(file, BinaryIO) or isinstance(file, io.BufferedReader) or isinstance(file, io.IOBase)) and hasattr(\n            file, \"read\"\n        ):\n            result = SevenZipFile._check_7zfile(file)\n        elif isinstance(file, str):\n            with open(file, \"rb\") as fp:\n                result = SevenZipFile._check_7zfile(fp)\n        elif isinstance(file, pathlib.Path) or isinstance(file, pathlib.PosixPath) or isinstance(file, pathlib.WindowsPath):\n            with file.open(mode=\"rb\") as fp:  # noqa\n                result = SevenZipFile._check_7zfile(fp)\n        else:\n            raise TypeError(\"invalid type: file should be str, pathlib.Path or BinaryIO, but {}\".format(type(file)))\n    except OSError:\n        pass\n    return result\n\n\ndef unpack_7zarchive(archive, path, extra=None):\n    \"\"\"\n    Function for registering with shutil.register_unpack_format().\n    \"\"\"\n    arc = SevenZipFile(archive)\n    arc.extractall(path)\n    arc.close()\n\n\ndef pack_7zarchive(base_name, base_dir, owner=None, group=None, dry_run=None, logger=None):\n    \"\"\"\n    Function for registering with shutil.register_archive_format().\n    \"\"\"\n    target_name = \"{}.7z\".format(base_name)\n    with SevenZipFile(target_name, mode=\"w\") as archive:\n        archive.writeall(path=base_dir)\n    return target_name\n\n\nclass Worker:\n    \"\"\"\n    Extract worker class to invoke handler.\n    \"\"\"\n\n    def __init__(self, files, src_start: int, header, mp=False) -> None:\n        self.target_filepath: Dict[int, Union[MemIO, pathlib.Path, None]] = {}\n        self.files = files\n        self.src_start = src_start\n        self.header = header\n        self.current_file_index = len(self.files)\n        self.last_file_index = len(self.files)\n        if mp:\n            self.concurrent: Union[Type[Thread], Type[Process]] = Process\n        else:\n            self.concurrent = Thread\n\n    def extract(self, fp: BinaryIO, path: Optional[pathlib.Path], parallel: bool, skip_notarget=True, q=None) -> None:\n        \"\"\"Extract worker method to handle 7zip folder and decompress each files.\"\"\"\n        if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:\n            src_end = self.src_start + self.header.main_streams.packinfo.packpositions[-1]\n            numfolders = self.header.main_streams.unpackinfo.numfolders\n            if numfolders == 1:\n                self.extract_single(\n                    fp,\n                    self.files,\n                    path,\n                    self.src_start,\n                    src_end,\n                    q,\n                    skip_notarget=skip_notarget,\n                )\n            else:\n                folders = self.header.main_streams.unpackinfo.folders\n                positions = self.header.main_streams.packinfo.packpositions\n                empty_files = [f for f in self.files if f.emptystream]\n                if not parallel:\n                    self.extract_single(fp, empty_files, path, 0, 0, q)\n                    for i in range(numfolders):\n                        if skip_notarget:\n                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):\n                                continue\n                        self.extract_single(\n                            fp,\n                            folders[i].files,\n                            path,\n                            self.src_start + positions[i],\n                            self.src_start + positions[i + 1],\n                            q,\n                            skip_notarget=skip_notarget,\n                        )\n                else:\n                    if getattr(fp, \"name\", None) is None:\n                        raise InternalError(\"Caught unknown variable status error\")\n                    filename: str = getattr(fp, \"name\", \"\")  # do not become \"\" but it is for type check.\n                    self.extract_single(open(filename, \"rb\"), empty_files, path, 0, 0, q)\n                    concurrent_tasks = []\n                    exc_q: queue.Queue = queue.Queue()\n                    for i in range(numfolders):\n                        if skip_notarget:\n                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):\n                                continue\n                        p = self.concurrent(\n                            target=self.extract_single,\n                            args=(\n                                filename,\n                                folders[i].files,\n                                path,\n                                self.src_start + positions[i],\n                                self.src_start + positions[i + 1],\n                                q,\n                                exc_q,\n                                skip_notarget,\n                            ),\n                        )\n                        p.start()\n                        concurrent_tasks.append(p)\n                    for p in concurrent_tasks:\n                        p.join()\n                    if exc_q.empty():\n                        pass\n                    else:\n                        exc_info = exc_q.get()\n                        raise exc_info[1].with_traceback(exc_info[2])\n        else:\n            empty_files = [f for f in self.files if f.emptystream]\n            self.extract_single(fp, empty_files, path, 0, 0, q)\n\n    def extract_single(\n        self,\n        fp: Union[BinaryIO, str],\n        files,\n        path,\n        src_start: int,\n        src_end: int,\n        q: Optional[queue.Queue],\n        exc_q: Optional[queue.Queue] = None,\n        skip_notarget=True,\n    ) -> None:\n        \"\"\"\n        Single thread extractor that takes file lists in single 7zip folder.\n        \"\"\"\n        if files is None:\n            return\n        try:\n            if isinstance(fp, str):\n                fp = open(fp, \"rb\")\n            fp.seek(src_start)\n            self._extract_single(fp, files, path, src_end, q, skip_notarget)\n        except Exception as e:\n            if exc_q is None:\n                raise e\n            else:\n                exc_tuple = sys.exc_info()\n                exc_q.put(exc_tuple)\n\n    def _extract_single(\n        self,\n        fp: BinaryIO,\n        files,\n        path,\n        src_end: int,\n        q: Optional[queue.Queue],\n        skip_notarget=True,\n    ) -> None:\n        \"\"\"\n        Single thread extractor that takes file lists in single 7zip folder.\n        this may raise exception.\n        \"\"\"\n        just_check: List[ArchiveFile] = []\n        for f in files:\n            if q is not None:\n                q.put(\n                    (\n                        \"s\",\n                        str(f.filename),\n                        str(f.compressed) if f.compressed is not None else \"0\",\n                    )\n                )\n            fileish = self.target_filepath.get(f.id, None)\n            if fileish is None:\n                if not f.emptystream:\n                    just_check.append(f)\n            else:\n                # delayed execution of crc check.\n                self._check(fp, just_check, src_end)\n                just_check = []\n                fileish.parent.mkdir(parents=True, exist_ok=True)\n                if not f.emptystream:\n                    if f.is_junction and not isinstance(fileish, MemIO) and sys.platform == \"win32\":\n                        with io.BytesIO() as ofp:\n                            self.decompress(fp, f.folder, ofp, f.uncompressed, f.compressed, src_end)\n                            dst: str = ofp.read().decode(\"utf-8\")\n                            if is_target_path_valid(path, fileish.parent.joinpath(dst)):\n                                # fileish.unlink(missing_ok=True) > py3.7\n                                if fileish.exists():\n                                    fileish.unlink()\n                                if sys.platform == \"win32\":  # hint for mypy\n                                    _winapi.CreateJunction(str(fileish), dst)  # noqa\n                            else:\n                                raise Bad7zFile(\"Junction point out of target directory.\")\n                    elif f.is_symlink and not isinstance(fileish, MemIO):\n                        with io.BytesIO() as omfp:\n                            self.decompress(fp, f.folder, omfp, f.uncompressed, f.compressed, src_end)\n                            omfp.seek(0)\n                            dst = omfp.read().decode(\"utf-8\")\n                            # check sym_target points inside an archive target?\n                            if is_target_path_valid(path, fileish.parent.joinpath(dst)):\n                                sym_target = pathlib.Path(dst)\n                                # fileish.unlink(missing_ok=True) > py3.7\n                                if fileish.exists():\n                                    fileish.unlink()\n                                fileish.symlink_to(sym_target)\n                            else:\n                                raise Bad7zFile(\"Symlink point out of target directory.\")\n                    else:\n                        with fileish.open(mode=\"wb\") as obfp:\n                            crc32 = self.decompress(fp, f.folder, obfp, f.uncompressed, f.compressed, src_end)\n                            obfp.seek(0)\n                            if f.crc32 is not None and crc32 != f.crc32:\n                                raise CrcError(crc32, f.crc32, f.filename)\n                else:\n                    # just create empty file\n                    if not isinstance(fileish, MemIO):\n                        fileish.touch()\n                    else:\n                        with fileish.open() as ofp:\n                            pass\n            if q is not None:\n                q.put((\"e\", str(f.filename), str(f.uncompressed)))\n        if not skip_notarget:\n            # delayed execution of crc check.\n            self._check(fp, just_check, src_end)\n\n    def _check(self, fp, check_target, src_end):\n        \"\"\"\n        delayed execution of crc check.\n        \"\"\"\n        for f in check_target:\n            with NullIO() as ofp:\n                crc32 = self.decompress(fp, f.folder, ofp, f.uncompressed, f.compressed, src_end)\n            if f.crc32 is not None and crc32 != f.crc32:\n                raise CrcError(crc32, f.crc32, f.filename)\n\n    def decompress(\n        self,\n        fp: BinaryIO,\n        folder,\n        fq: IO[Any],\n        size: int,\n        compressed_size: Optional[int],\n        src_end: int,\n    ) -> int:\n        \"\"\"\n        decompressor wrapper called from extract method.\n\n        :parameter fp: archive source file pointer\n        :parameter folder: Folder object that have decompressor object.\n        :parameter fq: output file pathlib.Path\n        :parameter size: uncompressed size of target file.\n        :parameter compressed_size: compressed size of target file.\n        :parameter src_end: end position of the folder\n\n        :returns None\n\n        \"\"\"\n        assert folder is not None\n        out_remaining = size\n        max_block_size = get_memory_limit()\n        crc32 = 0\n        decompressor = folder.get_decompressor(compressed_size)\n        while out_remaining > 0:\n            tmp = decompressor.decompress(fp, min(out_remaining, max_block_size))\n            if len(tmp) > 0:\n                out_remaining -= len(tmp)\n                fq.write(tmp)\n                crc32 = calculate_crc32(tmp, crc32)\n            if out_remaining <= 0:\n                break\n        if fp.tell() >= src_end:\n            if decompressor.crc is not None and not decompressor.check_crc():\n                raise CrcError(decompressor.crc, decompressor.digest, None)\n        return crc32\n\n    def _find_link_target(self, target):\n        \"\"\"\n        Find the target member of a symlink or hardlink member in the archive.\n        \"\"\"\n        targetname: str = target.as_posix()\n        linkname = readlink(targetname)\n        # Check windows full path symlinks\n        if linkname.startswith(\"\\\\\\\\?\\\\\"):\n            linkname = linkname[4:]\n        # normalize as posix style\n        linkname: str = pathlib.Path(linkname).as_posix()\n        member = None\n        for j in range(len(self.files)):\n            if linkname == self.files[j].origin.as_posix():\n                # FIXME: when API user specify arcname, it will break\n                member = os.path.relpath(linkname, os.path.dirname(targetname))\n                break\n        if member is None:\n            member = linkname\n        return member\n\n    def _after_write(self, insize, foutsize, crc):\n        self.header.main_streams.substreamsinfo.digestsdefined.append(True)\n        self.header.main_streams.substreamsinfo.digests.append(crc)\n        if self.header.main_streams.substreamsinfo.unpacksizes is None:\n            self.header.main_streams.substreamsinfo.unpacksizes = [insize]\n        else:\n            self.header.main_streams.substreamsinfo.unpacksizes.append(insize)\n        if self.header.main_streams.substreamsinfo.num_unpackstreams_folders is None:\n            self.header.main_streams.substreamsinfo.num_unpackstreams_folders = [1]\n        else:\n            self.header.main_streams.substreamsinfo.num_unpackstreams_folders[-1] += 1\n        return foutsize, crc\n\n    def write(self, fp: BinaryIO, f, assym, folder):\n        compressor = folder.get_compressor()\n        if assym:\n            link_target: str = self._find_link_target(f.origin)\n            tgt: bytes = link_target.encode(\"utf-8\")\n            fd = io.BytesIO(tgt)\n            insize, foutsize, crc = compressor.compress(fd, fp)\n            fd.close()\n        else:\n            with f.origin.open(mode=\"rb\") as fd:\n                insize, foutsize, crc = compressor.compress(fd, fp)\n        return self._after_write(insize, foutsize, crc)\n\n    def writestr(self, fp: BinaryIO, f, folder):\n        compressor = folder.get_compressor()\n        insize, foutsize, crc = compressor.compress(f.data(), fp)\n        return self._after_write(insize, foutsize, crc)\n\n    def flush_archive(self, fp, folder):\n        compressor = folder.get_compressor()\n        foutsize = compressor.flush(fp)\n        if len(self.files) > 0:\n            if \"maxsize\" in self.header.files_info.files[self.last_file_index]:\n                self.header.files_info.files[self.last_file_index][\"maxsize\"] += foutsize\n            else:\n                self.header.files_info.files[self.last_file_index][\"maxsize\"] = foutsize\n        # Update size data in header\n        self.header.main_streams.packinfo.numstreams += 1\n        if self.header.main_streams.packinfo.enable_digests:\n            self.header.main_streams.packinfo.crcs.append(compressor.digest)\n            self.header.main_streams.packinfo.digestdefined.append(True)\n        self.header.main_streams.packinfo.packsizes.append(compressor.packsize)\n        folder.unpacksizes = compressor.unpacksizes\n\n    def archive(self, fp: BinaryIO, files, folder, deref=False):\n        \"\"\"Run archive task for specified 7zip folder.\"\"\"\n        f = files[self.current_file_index]\n        if f.has_strdata():\n            foutsize, crc = self.writestr(fp, f, folder)\n            self.header.files_info.files[self.current_file_index][\"maxsize\"] = foutsize\n            self.header.files_info.files[self.current_file_index][\"digest\"] = crc\n            self.last_file_index = self.current_file_index\n        elif (f.is_symlink and not deref) or not f.emptystream:\n            foutsize, crc = self.write(fp, f, (f.is_symlink and not deref), folder)\n            self.header.files_info.files[self.current_file_index][\"maxsize\"] = foutsize\n            self.header.files_info.files[self.current_file_index][\"digest\"] = crc\n            self.last_file_index = self.current_file_index\n        self.current_file_index += 1\n\n    def register_filelike(self, id: int, fileish: Union[MemIO, pathlib.Path, None]) -> None:\n        \"\"\"register file-ish to worker.\"\"\"\n        self.target_filepath[id] = fileish\n\n    def close(self):\n        del self.header\n        del self.files\n        del self.concurrent\n", "code_before": "#!/usr/bin/python -u\n#\n# p7zr library\n#\n# Copyright (c) 2019-2021 Hiroshi Miura <miurahr@linux.com>\n# Copyright (c) 2004-2015 by Joachim Bauch, mail@joachim-bauch.de\n# 7-Zip Copyright (C) 1999-2010 Igor Pavlov\n# LZMA SDK Copyright (C) 1999-2010 Igor Pavlov\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n#\n#\n\"\"\"Read 7zip format archives.\"\"\"\nimport collections.abc\nimport contextlib\nimport datetime\nimport errno\nimport functools\nimport gc\nimport io\nimport os\nimport pathlib\nimport queue\nimport stat\nimport sys\nfrom multiprocessing import Process\nfrom threading import Thread\nfrom typing import IO, Any, BinaryIO, Dict, List, Optional, Tuple, Type, Union\n\nimport multivolumefile\n\nfrom py7zr.archiveinfo import Folder, Header, SignatureHeader\nfrom py7zr.callbacks import ExtractCallback\nfrom py7zr.compressor import SupportedMethods, get_methods_names\nfrom py7zr.exceptions import Bad7zFile, CrcError, DecompressionError, InternalError, UnsupportedCompressionMethodError\nfrom py7zr.helpers import (\n    ArchiveTimestamp,\n    MemIO,\n    NullIO,\n    calculate_crc32,\n    filetime_to_dt,\n    readlink,\n    remove_relative_path_marker,\n)\nfrom py7zr.properties import DEFAULT_FILTERS, FILTER_DEFLATE64, MAGIC_7Z, get_default_blocksize, get_memory_limit\nfrom py7zr.win32compat import is_windows_native_python, is_windows_unc_path\n\nif sys.platform.startswith(\"win\"):\n    import _winapi\n\nFILE_ATTRIBUTE_UNIX_EXTENSION = 0x8000\nFILE_ATTRIBUTE_WINDOWS_MASK = 0x07FFF\n\n\nclass ArchiveFile:\n    \"\"\"Represent each files metadata inside archive file.\n    It holds file properties; filename, permissions, and type whether\n    it is directory, link or normal file.\n\n    Instances of the :class:`ArchiveFile` class are returned by iterating :attr:`files_list` of\n    :class:`SevenZipFile` objects.\n    Each object stores information about a single member of the 7z archive. Most of users use :meth:`extractall()`.\n\n    The class also hold an archive parameter where file is exist in\n    archive file folder(container).\"\"\"\n\n    def __init__(self, id: int, file_info: Dict[str, Any]) -> None:\n        self.id = id\n        self._file_info = file_info\n\n    def file_properties(self) -> Dict[str, Any]:\n        \"\"\"Return file properties as a hash object. Following keys are included: \u2018readonly\u2019, \u2018is_directory\u2019,\n        \u2018posix_mode\u2019, \u2018archivable\u2019, \u2018emptystream\u2019, \u2018filename\u2019, \u2018creationtime\u2019, \u2018lastaccesstime\u2019,\n        \u2018lastwritetime\u2019, \u2018attributes\u2019\n        \"\"\"\n        properties = self._file_info\n        if properties is not None:\n            properties[\"readonly\"] = self.readonly\n            properties[\"posix_mode\"] = self.posix_mode\n            properties[\"archivable\"] = self.archivable\n            properties[\"is_directory\"] = self.is_directory\n        return properties\n\n    def _get_property(self, key: str) -> Any:\n        try:\n            return self._file_info[key]\n        except KeyError:\n            return None\n\n    @property\n    def origin(self) -> pathlib.Path:\n        return self._get_property(\"origin\")\n\n    @property\n    def folder(self) -> Folder:\n        return self._get_property(\"folder\")\n\n    @property\n    def filename(self) -> str:\n        \"\"\"return filename of archive file.\"\"\"\n        return self._get_property(\"filename\")\n\n    @property\n    def emptystream(self) -> bool:\n        \"\"\"True if file is empty(0-byte file), otherwise False\"\"\"\n        return self._get_property(\"emptystream\")\n\n    @property\n    def uncompressed(self) -> List[int]:\n        return self._get_property(\"uncompressed\")\n\n    @property\n    def compressed(self) -> Optional[int]:\n        \"\"\"Compressed size\"\"\"\n        return self._get_property(\"compressed\")\n\n    @property\n    def crc32(self) -> Optional[int]:\n        \"\"\"CRC of archived file(optional)\"\"\"\n        return self._get_property(\"digest\")\n\n    def _test_attribute(self, target_bit: int) -> bool:\n        attributes = self._get_property(\"attributes\")\n        if attributes is None:\n            return False\n        return attributes & target_bit == target_bit\n\n    @property\n    def archivable(self) -> bool:\n        \"\"\"File has a Windows `archive` flag.\"\"\"\n        if hasattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\"):\n            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\"))\n        return False\n\n    @property\n    def is_directory(self) -> bool:\n        \"\"\"True if file is a directory, otherwise False.\"\"\"\n        if hasattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\"):\n            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\"))\n        return False\n\n    @property\n    def readonly(self) -> bool:\n        \"\"\"True if file is readonly, otherwise False.\"\"\"\n        if hasattr(stat, \"FILE_ATTRIBUTE_READONLY\"):\n            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_READONLY\"))\n        return False\n\n    def _get_unix_extension(self) -> Optional[int]:\n        attributes = self._get_property(\"attributes\")\n        if self._test_attribute(FILE_ATTRIBUTE_UNIX_EXTENSION):\n            return attributes >> 16\n        return None\n\n    def data(self) -> Optional[BinaryIO]:\n        return self._get_property(\"data\")\n\n    def has_strdata(self) -> bool:\n        \"\"\"True if file content is set by writestr() method otherwise False.\"\"\"\n        return \"data\" in self._file_info\n\n    @property\n    def is_symlink(self) -> bool:\n        \"\"\"True if file is a symbolic link, otherwise False.\"\"\"\n        e = self._get_unix_extension()\n        if e is not None:\n            return stat.S_ISLNK(e)\n        if hasattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"):\n            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"))\n        return False\n\n    @property\n    def is_junction(self) -> bool:\n        \"\"\"True if file is a junction/reparse point on windows, otherwise False.\"\"\"\n        if hasattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"):\n            return self._test_attribute(\n                getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\") | getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")\n            )\n        return False\n\n    @property\n    def is_socket(self) -> bool:\n        \"\"\"True if file is a socket, otherwise False.\"\"\"\n        e = self._get_unix_extension()\n        if e is not None:\n            return stat.S_ISSOCK(e)\n        return False\n\n    @property\n    def lastwritetime(self) -> Optional[ArchiveTimestamp]:\n        \"\"\"Return last written timestamp of a file.\"\"\"\n        return self._get_property(\"lastwritetime\")\n\n    @property\n    def posix_mode(self) -> Optional[int]:\n        \"\"\"\n        posix mode when a member has a unix extension property, or None\n        :return: Return file stat mode can be set by os.chmod()\n        \"\"\"\n        e = self._get_unix_extension()\n        if e is not None:\n            return stat.S_IMODE(e)\n        return None\n\n    @property\n    def st_fmt(self) -> Optional[int]:\n        \"\"\"\n        :return: Return the portion of the file mode that describes the file type\n        \"\"\"\n        e = self._get_unix_extension()\n        if e is not None:\n            return stat.S_IFMT(e)\n        return None\n\n\nclass ArchiveFileList(collections.abc.Iterable):\n    \"\"\"Iteratable container of ArchiveFile.\"\"\"\n\n    def __init__(self, offset: int = 0):\n        self.files_list: List[dict] = []\n        self.index = 0\n        self.offset = offset\n\n    def append(self, file_info: Dict[str, Any]) -> None:\n        self.files_list.append(file_info)\n\n    def __len__(self) -> int:\n        return len(self.files_list)\n\n    def __iter__(self) -> \"ArchiveFileListIterator\":\n        return ArchiveFileListIterator(self)\n\n    def __getitem__(self, index):\n        if index > len(self.files_list):\n            raise IndexError\n        if index < 0:\n            raise IndexError\n        res = ArchiveFile(index + self.offset, self.files_list[index])\n        return res\n\n\nclass ArchiveFileListIterator(collections.abc.Iterator):\n    def __init__(self, archive_file_list):\n        self._archive_file_list = archive_file_list\n        self._index = 0\n\n    def __next__(self) -> ArchiveFile:\n        if self._index == len(self._archive_file_list):\n            raise StopIteration\n        res = self._archive_file_list[self._index]\n        self._index += 1\n        return res\n\n\n# ------------------\n# Exported Classes\n# ------------------\nclass ArchiveInfo:\n    \"\"\"Hold archive information\"\"\"\n\n    def __init__(\n        self,\n        filename: str,\n        stat: os.stat_result,\n        header_size: int,\n        method_names: List[str],\n        solid: bool,\n        blocks: int,\n        uncompressed: List[int],\n    ):\n        self.stat = stat\n        self.filename = filename\n        self.size = stat.st_size\n        self.header_size = header_size\n        self.method_names = method_names\n        self.solid = solid\n        self.blocks = blocks\n        self.uncompressed = uncompressed\n\n\nclass FileInfo:\n    \"\"\"Hold archived file information.\"\"\"\n\n    def __init__(\n        self,\n        filename,\n        compressed,\n        uncompressed,\n        archivable,\n        is_directory,\n        creationtime,\n        crc32,\n    ):\n        self.filename = filename\n        self.compressed = compressed\n        self.uncompressed = uncompressed\n        self.archivable = archivable\n        self.is_directory = is_directory\n        self.creationtime = creationtime\n        self.crc32 = crc32\n\n\nclass SevenZipFile(contextlib.AbstractContextManager):\n    \"\"\"The SevenZipFile Class provides an interface to 7z archives.\"\"\"\n\n    def __init__(\n        self,\n        file: Union[BinaryIO, str, pathlib.Path],\n        mode: str = \"r\",\n        *,\n        filters: Optional[List[Dict[str, int]]] = None,\n        dereference=False,\n        password: Optional[str] = None,\n        header_encryption: bool = False,\n        blocksize: Optional[int] = None,\n        mp: bool = False,\n    ) -> None:\n        if mode not in (\"r\", \"w\", \"x\", \"a\"):\n            raise ValueError(\"ZipFile requires mode 'r', 'w', 'x', or 'a'\")\n        self.fp: BinaryIO\n        self.mp = mp\n        self.password_protected = password is not None\n        if blocksize:\n            self._block_size = blocksize\n        else:\n            self._block_size = get_default_blocksize()\n        # Check if we were passed a file-like object or not\n        if isinstance(file, str):\n            self._filePassed: bool = False\n            self.filename: str = file\n            if mode == \"r\":\n                self.fp = open(file, \"rb\")\n            elif mode == \"w\":\n                self.fp = open(file, \"w+b\")\n            elif mode == \"x\":\n                self.fp = open(file, \"x+b\")\n            elif mode == \"a\":\n                self.fp = open(file, \"r+b\")\n            else:\n                raise ValueError(\"File open error.\")\n            self.mode = mode\n        elif isinstance(file, pathlib.Path):\n            self._filePassed = False\n            self.filename = str(file)\n            if mode == \"r\":\n                self.fp = file.open(mode=\"rb\")  # noqa   # typeshed issue: 2911\n            elif mode == \"w\":\n                self.fp = file.open(mode=\"w+b\")  # noqa\n            elif mode == \"x\":\n                self.fp = file.open(mode=\"x+b\")  # noqa\n            elif mode == \"a\":\n                self.fp = file.open(mode=\"r+b\")  # noqa\n            else:\n                raise ValueError(\"File open error.\")\n            self.mode = mode\n        elif isinstance(file, multivolumefile.MultiVolume):\n            self._filePassed = True\n            self.fp = file\n            self.filename = None\n            self.mode = mode  # noqa\n        elif isinstance(file, io.IOBase):\n            self._filePassed = True\n            self.fp = file\n            self.filename = getattr(file, \"name\", None)\n            self.mode = mode  # noqa\n        else:\n            raise TypeError(\"invalid file: {}\".format(type(file)))\n        self.encoded_header_mode = True\n        self.header_encryption = header_encryption\n        self._fileRefCnt = 1\n        try:\n            if mode == \"r\":\n                self._real_get_contents(password)\n                self.fp.seek(self.afterheader)  # seek into start of payload and prepare worker to extract\n                self.worker = Worker(self.files, self.afterheader, self.header, self.mp)\n            elif mode in \"w\":\n                self._prepare_write(filters, password)\n            elif mode in \"x\":\n                raise NotImplementedError\n            elif mode == \"a\":\n                self._real_get_contents(password)\n                self._prepare_append(filters, password)\n            else:\n                raise ValueError(\"Mode must be 'r', 'w', 'x', or 'a'\")\n        except Exception as e:\n            self._fpclose()\n            raise e\n        self._dict: Dict[str, IO[Any]] = {}\n        self.dereference = dereference\n        self.reporterd: Optional[Thread] = None\n        self.q: queue.Queue[Any] = queue.Queue()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def _fpclose(self) -> None:\n        assert self._fileRefCnt > 0\n        self._fileRefCnt -= 1\n        if not self._fileRefCnt and not self._filePassed:\n            self.fp.close()\n\n    def _real_get_contents(self, password) -> None:\n        if not self._check_7zfile(self.fp):\n            raise Bad7zFile(\"not a 7z file\")\n        self.sig_header = SignatureHeader.retrieve(self.fp)\n        self.afterheader: int = self.fp.tell()\n        self.fp.seek(self.sig_header.nextheaderofs, os.SEEK_CUR)\n        buffer = io.BytesIO(self.fp.read(self.sig_header.nextheadersize))\n        if self.sig_header.nextheadercrc != calculate_crc32(buffer.getvalue()):\n            raise Bad7zFile(\"invalid header data\")\n        header = Header.retrieve(self.fp, buffer, self.afterheader, password)\n        if header is None:\n            return\n        header._initilized = True\n        self.header = header\n        header.size += 32 + self.sig_header.nextheadersize\n        buffer.close()\n        self.files = ArchiveFileList()\n        if getattr(self.header, \"files_info\", None) is None:\n            return\n        # Initialize references for convenience\n        if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:\n            folders = self.header.main_streams.unpackinfo.folders\n            for folder in folders:\n                folder.password = password\n            packinfo = self.header.main_streams.packinfo\n            packsizes = packinfo.packsizes\n            subinfo = self.header.main_streams.substreamsinfo\n            if subinfo is not None and subinfo.unpacksizes is not None:\n                unpacksizes = subinfo.unpacksizes\n            else:\n                unpacksizes = [x.unpacksizes[-1] for x in folders]\n        else:\n            subinfo = None\n            folders = None\n            packinfo = None\n            packsizes = []\n            unpacksizes = [0]\n\n        pstat = self.ParseStatus()\n        pstat.src_pos = self.afterheader\n        file_in_solid = 0\n\n        for file_id, file_info in enumerate(self.header.files_info.files):\n            if not file_info[\"emptystream\"] and folders is not None:\n                folder = folders[pstat.folder]\n                numinstreams = max([coder.get(\"numinstreams\", 1) for coder in folder.coders])\n                (maxsize, compressed, uncompressed, packsize, solid,) = self._get_fileinfo_sizes(\n                    pstat,\n                    subinfo,\n                    packinfo,\n                    folder,\n                    packsizes,\n                    unpacksizes,\n                    file_in_solid,\n                    numinstreams,\n                )\n                pstat.input += 1\n                folder.solid = solid\n                file_info[\"folder\"] = folder\n                file_info[\"maxsize\"] = maxsize\n                file_info[\"compressed\"] = compressed\n                file_info[\"uncompressed\"] = uncompressed\n                file_info[\"packsizes\"] = packsize\n                if subinfo.digestsdefined[pstat.outstreams]:\n                    file_info[\"digest\"] = subinfo.digests[pstat.outstreams]\n                if folder is None:\n                    pstat.src_pos += file_info[\"compressed\"]\n                else:\n                    if folder.solid:\n                        file_in_solid += 1\n                    pstat.outstreams += 1\n                    if folder.files is None:\n                        folder.files = ArchiveFileList(offset=file_id)\n                    folder.files.append(file_info)\n                    if pstat.input >= subinfo.num_unpackstreams_folders[pstat.folder]:\n                        file_in_solid = 0\n                        pstat.src_pos += sum(packinfo.packsizes[pstat.stream : pstat.stream + numinstreams])\n                        pstat.folder += 1\n                        pstat.stream += numinstreams\n                        pstat.input = 0\n            else:\n                file_info[\"folder\"] = None\n                file_info[\"maxsize\"] = 0\n                file_info[\"compressed\"] = 0\n                file_info[\"uncompressed\"] = 0\n                file_info[\"packsizes\"] = [0]\n\n            if \"filename\" not in file_info:\n                # compressed file is stored without a name, generate one\n                try:\n                    basefilename = self.filename\n                except AttributeError:\n                    # 7z archive file doesn't have a name\n                    file_info[\"filename\"] = \"contents\"\n                else:\n                    if basefilename is not None:\n                        fn, ext = os.path.splitext(os.path.basename(basefilename))\n                        file_info[\"filename\"] = fn\n                    else:\n                        file_info[\"filename\"] = \"contents\"\n            self.files.append(file_info)\n        if not self.password_protected and self.header.main_streams is not None:\n            # Check specified coders have a crypt method or not.\n            self.password_protected = any(\n                [SupportedMethods.needs_password(folder.coders) for folder in self.header.main_streams.unpackinfo.folders]\n            )\n\n    def _extract(\n        self,\n        path: Optional[Any] = None,\n        targets: Optional[List[str]] = None,\n        return_dict: bool = False,\n        callback: Optional[ExtractCallback] = None,\n    ) -> Optional[Dict[str, IO[Any]]]:\n        if callback is None:\n            pass\n        elif isinstance(callback, ExtractCallback):\n            self.reporterd = Thread(target=self.reporter, args=(callback,), daemon=True)\n            self.reporterd.start()\n        else:\n            raise ValueError(\"Callback specified is not an instance of subclass of py7zr.callbacks.ExtractCallback class\")\n        target_files: List[Tuple[pathlib.Path, Dict[str, Any]]] = []\n        target_dirs: List[pathlib.Path] = []\n        if path is not None:\n            if isinstance(path, str):\n                path = pathlib.Path(path)\n            try:\n                if not path.exists():\n                    path.mkdir(parents=True)\n                else:\n                    pass\n            except OSError as e:\n                if e.errno == errno.EEXIST and path.is_dir():\n                    pass\n                else:\n                    raise e\n        fnames: List[str] = []  # check duplicated filename in one archive?\n        self.q.put((\"pre\", None, None))\n        for f in self.files:\n            # When archive has a multiple files which have same name\n            # To guarantee order of archive, multi-thread decompression becomes off.\n            # Currently always overwrite by latter archives.\n            # TODO: provide option to select overwrite or skip.\n            if f.filename not in fnames:\n                outname = f.filename\n            else:\n                i = 0\n                while True:\n                    outname = f.filename + \"_%d\" % i\n                    if outname not in fnames:\n                        break\n                    i += 1\n            fnames.append(outname)\n            # check f.filename has invalid directory traversals\n            if path is None:\n                # do following but is_relative_to introduced in py 3.9\n                # so I replaced it with relative_to. when condition is not satisfied, raise ValueError\n                # if not pathlib.Path(...).joinpath(remove_relative_path_marker(outname)).is_relative_to(...):\n                #    raise Bad7zFile\n                try:\n                    pathlib.Path(os.getcwd()).joinpath(remove_relative_path_marker(outname)).relative_to(os.getcwd())\n                except ValueError:\n                    raise Bad7zFile\n                outfilename = pathlib.Path(remove_relative_path_marker(outname))\n            else:\n                outfilename = path.joinpath(remove_relative_path_marker(outname))\n                try:\n                    outfilename.relative_to(path)\n                except ValueError:\n                    raise Bad7zFile\n            # When python on Windows and not python on Cygwin,\n            # Add win32 file namespace to exceed microsoft windows\n            # path length limitation to 260 bytes\n            # ref.\n            # https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file\n            # In editions of Windows before Windows 10 version 1607,\n            # the maximum length for a path is MAX_PATH, which is defined as\n            # 260 characters. In later versions of Windows, changing a registry key\n            # or select option when python installation is required to remove the limit.\n            if is_windows_native_python() and outfilename.is_absolute() and not is_windows_unc_path(outfilename):\n                outfilename = pathlib.WindowsPath(\"\\\\\\\\?\\\\\" + str(outfilename))\n            if targets is not None and f.filename not in targets:\n                self.worker.register_filelike(f.id, None)\n                continue\n            if return_dict:\n                if f.is_directory or f.is_socket:\n                    # ignore special files and directories\n                    pass\n                else:\n                    fname = outfilename.as_posix()\n                    _buf = io.BytesIO()\n                    self._dict[fname] = _buf\n                    self.worker.register_filelike(f.id, MemIO(_buf))\n            elif f.is_directory:\n                if not outfilename.exists():\n                    target_dirs.append(outfilename)\n                    target_files.append((outfilename, f.file_properties()))\n                else:\n                    pass\n            elif f.is_socket:\n                pass  # TODO: implement me.\n            elif f.is_symlink or f.is_junction:\n                self.worker.register_filelike(f.id, outfilename)\n            else:\n                self.worker.register_filelike(f.id, outfilename)\n                target_files.append((outfilename, f.file_properties()))\n        for target_dir in sorted(target_dirs):\n            try:\n                target_dir.mkdir(parents=True)\n            except FileExistsError:\n                if target_dir.is_dir():\n                    pass\n                elif target_dir.is_file():\n                    raise DecompressionError(\"Directory {} is existed as a normal file.\".format(str(target_dir)))\n                else:\n                    raise DecompressionError(\"Directory {} making fails on unknown condition.\".format(str(target_dir)))\n\n        if callback is not None:\n            self.worker.extract(\n                self.fp,\n                parallel=(not self.password_protected and not self._filePassed),\n                q=self.q,\n            )\n        else:\n            self.worker.extract(\n                self.fp,\n                parallel=(not self.password_protected and not self._filePassed),\n            )\n\n        self.q.put((\"post\", None, None))\n        # early return when dict specified\n        if return_dict:\n            return self._dict\n        # set file properties\n        for outfilename, properties in target_files:\n            # mtime\n            lastmodified = None\n            try:\n                lastmodified = ArchiveTimestamp(properties[\"lastwritetime\"]).totimestamp()\n            except KeyError:\n                pass\n            if lastmodified is not None:\n                os.utime(str(outfilename), times=(lastmodified, lastmodified))\n            if os.name == \"posix\":\n                st_mode = properties[\"posix_mode\"]\n                if st_mode is not None:\n                    outfilename.chmod(st_mode)\n                    continue\n            # fallback: only set readonly if specified\n            if properties[\"readonly\"] and not properties[\"is_directory\"]:\n                ro_mask = 0o777 ^ (stat.S_IWRITE | stat.S_IWGRP | stat.S_IWOTH)\n                outfilename.chmod(outfilename.stat().st_mode & ro_mask)\n        return None\n\n    def _prepare_append(self, filters, password):\n        if password is not None and filters is None:\n            filters = DEFAULT_FILTERS.ENCRYPTED_ARCHIVE_FILTER\n        elif filters is None:\n            filters = DEFAULT_FILTERS.ARCHIVE_FILTER\n        else:\n            for f in filters:\n                if f[\"id\"] == FILTER_DEFLATE64:\n                    raise UnsupportedCompressionMethodError(filters, \"Compression with deflate64 is not supported.\")\n        self.header.filters = filters\n        self.header.password = password\n        if self.header.main_streams is not None:\n            pos = self.afterheader + self.header.main_streams.packinfo.packpositions[-1]\n        else:\n            pos = self.afterheader\n        self.fp.seek(pos)\n        self.worker = Worker(self.files, pos, self.header, self.mp)\n\n    def _prepare_write(self, filters, password):\n        if password is not None and filters is None:\n            filters = DEFAULT_FILTERS.ENCRYPTED_ARCHIVE_FILTER\n        elif filters is None:\n            filters = DEFAULT_FILTERS.ARCHIVE_FILTER\n        self.files = ArchiveFileList()\n        self.sig_header = SignatureHeader()\n        self.sig_header._write_skelton(self.fp)\n        self.afterheader = self.fp.tell()\n        self.header = Header.build_header(filters, password)\n        self.fp.seek(self.afterheader)\n        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)\n\n    def _write_flush(self):\n        if self.header._initialized:\n            folder = self.header.main_streams.unpackinfo.folders[-1]\n            self.worker.flush_archive(self.fp, folder)\n        self._write_header()\n\n    def _write_header(self):\n        \"\"\"Write header and update signature header.\"\"\"\n        (header_pos, header_len, header_crc) = self.header.write(\n            self.fp,\n            self.afterheader,\n            encoded=self.encoded_header_mode,\n            encrypted=self.header_encryption,\n        )\n        self.sig_header.nextheaderofs = header_pos - self.afterheader\n        self.sig_header.calccrc(header_len, header_crc)\n        self.sig_header.write(self.fp)\n\n    def _writeall(self, path, arcname):\n        try:\n            if path.is_symlink() and not self.dereference:\n                self.write(path, arcname)\n            elif path.is_file():\n                self.write(path, arcname)\n            elif path.is_dir():\n                if not path.samefile(\".\"):\n                    self.write(path, arcname)\n                for nm in sorted(os.listdir(str(path))):\n                    arc = os.path.join(arcname, nm) if arcname is not None else None\n                    self._writeall(path.joinpath(nm), arc)\n            else:\n                return  # pathlib ignores ELOOP and return False for is_*().\n        except OSError as ose:\n            if self.dereference and ose.errno in [errno.ELOOP]:\n                return  # ignore ELOOP here, this resulted to stop looped symlink reference.\n            elif self.dereference and sys.platform == \"win32\" and ose.errno in [errno.ENOENT]:\n                return  # ignore ENOENT which is happened when a case of ELOOP on windows.\n            else:\n                raise\n\n    class ParseStatus:\n        def __init__(self, src_pos=0):\n            self.src_pos = src_pos\n            self.folder = 0  # 7zip folder where target stored\n            self.outstreams = 0  # output stream count\n            self.input = 0  # unpack stream count in each folder\n            self.stream = 0  # target input stream position\n\n    def _get_fileinfo_sizes(\n        self,\n        pstat,\n        subinfo,\n        packinfo,\n        folder,\n        packsizes,\n        unpacksizes,\n        file_in_solid,\n        numinstreams,\n    ):\n        if pstat.input == 0:\n            folder.solid = subinfo.num_unpackstreams_folders[pstat.folder] > 1\n        maxsize = (folder.solid and packinfo.packsizes[pstat.stream]) or None\n        uncompressed = unpacksizes[pstat.outstreams]\n        if file_in_solid > 0:\n            compressed = None\n        elif pstat.stream < len(packsizes):  # file is compressed\n            compressed = packsizes[pstat.stream]\n        else:  # file is not compressed\n            compressed = uncompressed\n        packsize = packsizes[pstat.stream : pstat.stream + numinstreams]\n        return maxsize, compressed, uncompressed, packsize, folder.solid\n\n    def set_encoded_header_mode(self, mode: bool) -> None:\n        if mode:\n            self.encoded_header_mode = True\n        else:\n            self.encoded_header_mode = False\n            self.header_encryption = False\n\n    def set_encrypted_header(self, mode: bool) -> None:\n        if mode:\n            self.encoded_header_mode = True\n            self.header_encryption = True\n        else:\n            self.header_encryption = False\n\n    @staticmethod\n    def _check_7zfile(fp: Union[BinaryIO, io.BufferedReader, io.IOBase]) -> bool:\n        result = MAGIC_7Z == fp.read(len(MAGIC_7Z))[: len(MAGIC_7Z)]\n        fp.seek(-len(MAGIC_7Z), 1)\n        return result\n\n    def _get_method_names(self) -> List[str]:\n        try:\n            return get_methods_names([folder.coders for folder in self.header.main_streams.unpackinfo.folders])\n        except KeyError:\n            raise UnsupportedCompressionMethodError(self.header.main_streams.unpackinfo.folders, \"Unknown method\")\n\n    def _read_digest(self, pos: int, size: int) -> int:\n        self.fp.seek(pos)\n        remaining_size = size\n        digest = 0\n        while remaining_size > 0:\n            block = min(self._block_size, remaining_size)\n            digest = calculate_crc32(self.fp.read(block), digest)\n            remaining_size -= block\n        return digest\n\n    def _is_solid(self):\n        for f in self.header.main_streams.substreamsinfo.num_unpackstreams_folders:\n            if f > 1:\n                return True\n        return False\n\n    def _var_release(self):\n        self._dict = None\n        self.worker.close()\n        del self.worker\n        del self.files\n        del self.header\n        del self.sig_header\n        gc.collect()\n\n    @staticmethod\n    def _make_file_info(target: pathlib.Path, arcname: Optional[str] = None, dereference=False) -> Dict[str, Any]:\n        f: Dict[str, Any] = {}\n        f[\"origin\"] = target\n        if arcname is not None:\n            f[\"filename\"] = pathlib.Path(arcname).as_posix()\n        else:\n            f[\"filename\"] = target.as_posix()\n        if sys.platform == \"win32\":\n            fstat = target.lstat()\n            if target.is_symlink():\n                if dereference:\n                    fstat = target.stat()\n                    if stat.S_ISDIR(fstat.st_mode):\n                        f[\"emptystream\"] = True\n                        f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa\n                    else:\n                        f[\"emptystream\"] = False\n                        f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE  # noqa\n                        f[\"uncompressed\"] = fstat.st_size\n                else:\n                    f[\"emptystream\"] = False\n                    f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa\n                    # TODO: handle junctions\n                    # f['attributes'] |= stat.FILE_ATTRIBUTE_REPARSE_POINT  # noqa\n            elif target.is_dir():\n                f[\"emptystream\"] = True\n                f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa\n            elif target.is_file():\n                f[\"emptystream\"] = False\n                f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE  # noqa\n                f[\"uncompressed\"] = fstat.st_size\n        elif (\n            sys.platform == \"darwin\"\n            or sys.platform.startswith(\"linux\")\n            or sys.platform.startswith(\"freebsd\")\n            or sys.platform.startswith(\"netbsd\")\n            or sys.platform.startswith(\"sunos\")\n            or sys.platform == \"aix\"\n        ):\n            fstat = target.lstat()\n            if target.is_symlink():\n                if dereference:\n                    fstat = target.stat()\n                    if stat.S_ISDIR(fstat.st_mode):\n                        f[\"emptystream\"] = True\n                        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")\n                        f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFDIR << 16)\n                        f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16\n                    else:\n                        f[\"emptystream\"] = False\n                        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")\n                        f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IMODE(fstat.st_mode) << 16)\n                else:\n                    f[\"emptystream\"] = False\n                    f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\") | getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\")\n                    f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFLNK << 16)\n                    f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16\n            elif target.is_dir():\n                f[\"emptystream\"] = True\n                f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")\n                f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFDIR << 16)\n                f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16\n            elif target.is_file():\n                f[\"emptystream\"] = False\n                f[\"uncompressed\"] = fstat.st_size\n                f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")\n                f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IMODE(fstat.st_mode) << 16)\n        else:\n            fstat = target.stat()\n            if target.is_dir():\n                f[\"emptystream\"] = True\n                f[\"attributes\"] = stat.FILE_ATTRIBUTE_DIRECTORY\n            elif target.is_file():\n                f[\"emptystream\"] = False\n                f[\"uncompressed\"] = fstat.st_size\n                f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE\n\n        f[\"creationtime\"] = ArchiveTimestamp.from_datetime(fstat.st_ctime)\n        f[\"lastwritetime\"] = ArchiveTimestamp.from_datetime(fstat.st_mtime)\n        f[\"lastaccesstime\"] = ArchiveTimestamp.from_datetime(fstat.st_atime)\n        return f\n\n    def _make_file_info_from_name(self, bio, size: int, arcname: str) -> Dict[str, Any]:\n        f: Dict[str, Any] = {}\n        f[\"origin\"] = None\n        f[\"data\"] = bio\n        f[\"filename\"] = pathlib.Path(arcname).as_posix()\n        f[\"uncompressed\"] = size\n        f[\"emptystream\"] = size == 0\n        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")\n        f[\"creationtime\"] = ArchiveTimestamp.from_now()\n        f[\"lastwritetime\"] = ArchiveTimestamp.from_now()\n        return f\n\n    # --------------------------------------------------------------------------\n    # The public methods which SevenZipFile provides:\n    def getnames(self) -> List[str]:\n        \"\"\"Return the members of the archive as a list of their names. It has\n        the same order as the list returned by getmembers().\n        \"\"\"\n        return list(map(lambda x: x.filename, self.files))\n\n    def archiveinfo(self) -> ArchiveInfo:\n        total_uncompressed = functools.reduce(lambda x, y: x + y, [f.uncompressed for f in self.files])\n        if isinstance(self.fp, multivolumefile.MultiVolume):\n            fname = self.fp.name\n            fstat = self.fp.stat()\n        else:\n            fname = self.filename\n            assert fname is not None\n            fstat = os.stat(fname)\n        return ArchiveInfo(\n            fname,\n            fstat,\n            self.header.size,\n            self._get_method_names(),\n            self._is_solid(),\n            len(self.header.main_streams.unpackinfo.folders),\n            total_uncompressed,\n        )\n\n    def needs_password(self) -> bool:\n        return self.password_protected\n\n    def list(self) -> List[FileInfo]:\n        \"\"\"Returns contents information\"\"\"\n        alist: List[FileInfo] = []\n        lastmodified: Optional[datetime.datetime] = None\n        for f in self.files:\n            if f.lastwritetime is not None:\n                lastmodified = filetime_to_dt(f.lastwritetime)\n            alist.append(\n                FileInfo(\n                    f.filename,\n                    f.compressed,\n                    f.uncompressed,\n                    f.archivable,\n                    f.is_directory,\n                    lastmodified,\n                    f.crc32,\n                )\n            )\n        return alist\n\n    def readall(self) -> Optional[Dict[str, IO[Any]]]:\n        self._dict = {}\n        return self._extract(path=None, return_dict=True)\n\n    def extractall(self, path: Optional[Any] = None, callback: Optional[ExtractCallback] = None) -> None:\n        \"\"\"Extract all members from the archive to the current working\n        directory and set owner, modification time and permissions on\n        directories afterwards. ``path`` specifies a different directory\n        to extract to.\n        \"\"\"\n        self._extract(path=path, return_dict=False, callback=callback)\n\n    def read(self, targets: Optional[List[str]] = None) -> Optional[Dict[str, IO[Any]]]:\n        self._dict = {}\n        return self._extract(path=None, targets=targets, return_dict=True)\n\n    def extract(self, path: Optional[Any] = None, targets: Optional[List[str]] = None) -> None:\n        self._extract(path, targets, return_dict=False)\n\n    def reporter(self, callback: ExtractCallback):\n        while True:\n            try:\n                item: Optional[Tuple[str, str, str]] = self.q.get(timeout=1)\n            except queue.Empty:\n                pass\n            else:\n                if item is None:\n                    break\n                elif item[0] == \"s\":\n                    callback.report_start(item[1], item[2])\n                elif item[0] == \"e\":\n                    callback.report_end(item[1], item[2])\n                elif item[0] == \"pre\":\n                    callback.report_start_preparation()\n                elif item[0] == \"post\":\n                    callback.report_postprocess()\n                elif item[0] == \"w\":\n                    callback.report_warning(item[1])\n                else:\n                    pass\n                self.q.task_done()\n\n    def writeall(self, path: Union[pathlib.Path, str], arcname: Optional[str] = None):\n        \"\"\"Write files in target path into archive.\"\"\"\n        if isinstance(path, str):\n            path = pathlib.Path(path)\n        if not path.exists():\n            raise ValueError(\"specified path does not exist.\")\n        if path.is_dir() or path.is_file():\n            self._writeall(path, arcname)\n        else:\n            raise ValueError(\"specified path is not a directory or a file\")\n\n    def write(self, file: Union[pathlib.Path, str], arcname: Optional[str] = None):\n        \"\"\"Write single target file into archive.\"\"\"\n        if isinstance(file, str):\n            path = pathlib.Path(file)\n        elif isinstance(file, pathlib.Path):\n            path = file\n        else:\n            raise ValueError(\"Unsupported file type.\")\n        folder = self.header.initialize()\n        file_info = self._make_file_info(path, arcname, self.dereference)\n        self.header.files_info.files.append(file_info)\n        self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])\n        self.files.append(file_info)\n        self.worker.archive(self.fp, self.files, folder, deref=self.dereference)\n\n    def writed(self, targets: Dict[str, IO[Any]]) -> None:\n        for target, input in targets.items():\n            self.writef(input, target)\n\n    def writef(self, bio: IO[Any], arcname: str):\n        if isinstance(bio, io.BytesIO):\n            size = bio.getbuffer().nbytes\n        elif isinstance(bio, io.TextIOBase):\n            # First check whether is it Text?\n            raise ValueError(\"Unsupported file object type: please open file with Binary mode.\")\n        elif isinstance(bio, io.BufferedIOBase):\n            # come here when subtype of io.BufferedIOBase that don't have __sizeof__ (eg. pypy)\n            # alternative for `size = bio.__sizeof__()`\n            current = bio.tell()\n            bio.seek(0, os.SEEK_END)\n            last = bio.tell()\n            bio.seek(current, os.SEEK_SET)\n            size = last - current\n        else:\n            raise ValueError(\"Wrong argument passed for argument bio.\")\n        if size > 0:\n            folder = self.header.initialize()\n            file_info = self._make_file_info_from_name(bio, size, arcname)\n            self.header.files_info.files.append(file_info)\n            self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])\n            self.files.append(file_info)\n            self.worker.archive(self.fp, self.files, folder, deref=False)\n        else:\n            file_info = self._make_file_info_from_name(bio, size, arcname)\n            self.header.files_info.files.append(file_info)\n            self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])\n            self.files.append(file_info)\n\n    def writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):\n        if not isinstance(arcname, str):\n            raise ValueError(\"Unsupported arcname\")\n        if isinstance(data, str):\n            self.writef(io.BytesIO(data.encode(\"UTF-8\")), arcname)\n        elif isinstance(data, bytes) or isinstance(data, bytearray) or isinstance(data, memoryview):\n            self.writef(io.BytesIO(data), arcname)\n        else:\n            raise ValueError(\"Unsupported data type.\")\n\n    def close(self):\n        \"\"\"Flush all the data into archive and close it.\n        When close py7zr start reading target and writing actual archive file.\n        \"\"\"\n        if \"w\" in self.mode:\n            self._write_flush()\n        if \"a\" in self.mode:\n            self._write_flush()\n        if \"r\" in self.mode:\n            if self.reporterd is not None:\n                self.q.put_nowait(None)\n                self.reporterd.join(1)\n                if self.reporterd.is_alive():\n                    raise InternalError(\"Progress report thread terminate error.\")\n                self.reporterd = None\n        self._fpclose()\n        self._var_release()\n\n    def reset(self) -> None:\n        \"\"\"\n        When read mode, it reset file pointer, decompress worker and decompressor\n        \"\"\"\n        if self.mode == \"r\":\n            self.fp.seek(self.afterheader)\n            self.worker = Worker(self.files, self.afterheader, self.header, self.mp)\n            if self.header.main_streams is not None and self.header.main_streams.unpackinfo.numfolders > 0:\n                for i, folder in enumerate(self.header.main_streams.unpackinfo.folders):\n                    folder.decompressor = None\n\n    def test(self) -> Optional[bool]:\n        self.fp.seek(self.afterheader)\n        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)\n        crcs: Optional[List[int]] = self.header.main_streams.packinfo.crcs\n        if crcs is None or len(crcs) == 0:\n            return None\n        packpos = self.afterheader + self.header.main_streams.packinfo.packpos\n        packsizes = self.header.main_streams.packinfo.packsizes\n        digestdefined = self.header.main_streams.packinfo.digestdefined\n        j = 0\n        for i, d in enumerate(digestdefined):\n            if d:\n                if self._read_digest(packpos, packsizes[i]) != crcs[j]:\n                    return False\n                j += 1\n            packpos += packsizes[i]\n        return True\n\n    def testzip(self) -> Optional[str]:\n        self.fp.seek(self.afterheader)\n        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)\n        for f in self.files:\n            self.worker.register_filelike(f.id, None)\n        try:\n            self.worker.extract(self.fp, parallel=(not self.password_protected), skip_notarget=False)  # TODO: print progress\n        except CrcError as crce:\n            return crce.args[2]\n        else:\n            return None\n\n\n# --------------------\n# exported functions\n# --------------------\ndef is_7zfile(file: Union[BinaryIO, str, pathlib.Path]) -> bool:\n    \"\"\"Quickly see if a file is a 7Z file by checking the magic number.\n    The file argument may be a filename or file-like object too.\n    \"\"\"\n    result = False\n    try:\n        if (isinstance(file, BinaryIO) or isinstance(file, io.BufferedReader) or isinstance(file, io.IOBase)) and hasattr(\n            file, \"read\"\n        ):\n            result = SevenZipFile._check_7zfile(file)\n        elif isinstance(file, str):\n            with open(file, \"rb\") as fp:\n                result = SevenZipFile._check_7zfile(fp)\n        elif isinstance(file, pathlib.Path) or isinstance(file, pathlib.PosixPath) or isinstance(file, pathlib.WindowsPath):\n            with file.open(mode=\"rb\") as fp:  # noqa\n                result = SevenZipFile._check_7zfile(fp)\n        else:\n            raise TypeError(\"invalid type: file should be str, pathlib.Path or BinaryIO, but {}\".format(type(file)))\n    except OSError:\n        pass\n    return result\n\n\ndef unpack_7zarchive(archive, path, extra=None):\n    \"\"\"\n    Function for registering with shutil.register_unpack_format().\n    \"\"\"\n    arc = SevenZipFile(archive)\n    arc.extractall(path)\n    arc.close()\n\n\ndef pack_7zarchive(base_name, base_dir, owner=None, group=None, dry_run=None, logger=None):\n    \"\"\"\n    Function for registering with shutil.register_archive_format().\n    \"\"\"\n    target_name = \"{}.7z\".format(base_name)\n    with SevenZipFile(target_name, mode=\"w\") as archive:\n        archive.writeall(path=base_dir)\n    return target_name\n\n\nclass Worker:\n    \"\"\"\n    Extract worker class to invoke handler.\n    \"\"\"\n\n    def __init__(self, files, src_start: int, header, mp=False) -> None:\n        self.target_filepath: Dict[int, Union[MemIO, pathlib.Path, None]] = {}\n        self.files = files\n        self.src_start = src_start\n        self.header = header\n        self.current_file_index = len(self.files)\n        self.last_file_index = len(self.files)\n        if mp:\n            self.concurrent: Union[Type[Thread], Type[Process]] = Process\n        else:\n            self.concurrent = Thread\n\n    def extract(self, fp: BinaryIO, parallel: bool, skip_notarget=True, q=None) -> None:\n        \"\"\"Extract worker method to handle 7zip folder and decompress each files.\"\"\"\n        if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:\n            src_end = self.src_start + self.header.main_streams.packinfo.packpositions[-1]\n            numfolders = self.header.main_streams.unpackinfo.numfolders\n            if numfolders == 1:\n                self.extract_single(\n                    fp,\n                    self.files,\n                    self.src_start,\n                    src_end,\n                    q,\n                    skip_notarget=skip_notarget,\n                )\n            else:\n                folders = self.header.main_streams.unpackinfo.folders\n                positions = self.header.main_streams.packinfo.packpositions\n                empty_files = [f for f in self.files if f.emptystream]\n                if not parallel:\n                    self.extract_single(fp, empty_files, 0, 0, q)\n                    for i in range(numfolders):\n                        if skip_notarget:\n                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):\n                                continue\n                        self.extract_single(\n                            fp,\n                            folders[i].files,\n                            self.src_start + positions[i],\n                            self.src_start + positions[i + 1],\n                            q,\n                            skip_notarget=skip_notarget,\n                        )\n                else:\n                    if getattr(fp, \"name\", None) is None:\n                        raise InternalError(\"Caught unknown variable status error\")\n                    filename: str = getattr(fp, \"name\", \"\")  # do not become \"\" but it is for type check.\n                    self.extract_single(open(filename, \"rb\"), empty_files, 0, 0, q)\n                    concurrent_tasks = []\n                    exc_q: queue.Queue = queue.Queue()\n                    for i in range(numfolders):\n                        if skip_notarget:\n                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):\n                                continue\n                        p = self.concurrent(\n                            target=self.extract_single,\n                            args=(\n                                filename,\n                                folders[i].files,\n                                self.src_start + positions[i],\n                                self.src_start + positions[i + 1],\n                                q,\n                                exc_q,\n                                skip_notarget,\n                            ),\n                        )\n                        p.start()\n                        concurrent_tasks.append(p)\n                    for p in concurrent_tasks:\n                        p.join()\n                    if exc_q.empty():\n                        pass\n                    else:\n                        exc_info = exc_q.get()\n                        raise exc_info[1].with_traceback(exc_info[2])\n        else:\n            empty_files = [f for f in self.files if f.emptystream]\n            self.extract_single(fp, empty_files, 0, 0, q)\n\n    def extract_single(\n        self,\n        fp: Union[BinaryIO, str],\n        files,\n        src_start: int,\n        src_end: int,\n        q: Optional[queue.Queue],\n        exc_q: Optional[queue.Queue] = None,\n        skip_notarget=True,\n    ) -> None:\n        \"\"\"\n        Single thread extractor that takes file lists in single 7zip folder.\n        \"\"\"\n        if files is None:\n            return\n        try:\n            if isinstance(fp, str):\n                fp = open(fp, \"rb\")\n            fp.seek(src_start)\n            self._extract_single(fp, files, src_end, q, skip_notarget)\n        except Exception as e:\n            if exc_q is None:\n                raise e\n            else:\n                exc_tuple = sys.exc_info()\n                exc_q.put(exc_tuple)\n\n    def _extract_single(\n        self,\n        fp: BinaryIO,\n        files,\n        src_end: int,\n        q: Optional[queue.Queue],\n        skip_notarget=True,\n    ) -> None:\n        \"\"\"\n        Single thread extractor that takes file lists in single 7zip folder.\n        this may raise exception.\n        \"\"\"\n        just_check: List[ArchiveFile] = []\n        for f in files:\n            if q is not None:\n                q.put(\n                    (\n                        \"s\",\n                        str(f.filename),\n                        str(f.compressed) if f.compressed is not None else \"0\",\n                    )\n                )\n            fileish = self.target_filepath.get(f.id, None)\n            if fileish is None:\n                if not f.emptystream:\n                    just_check.append(f)\n            else:\n                # delayed execution of crc check.\n                self._check(fp, just_check, src_end)\n                just_check = []\n                fileish.parent.mkdir(parents=True, exist_ok=True)\n                if not f.emptystream:\n                    if f.is_junction and not isinstance(fileish, MemIO) and sys.platform == \"win32\":\n                        with io.BytesIO() as ofp:\n                            self.decompress(fp, f.folder, ofp, f.uncompressed, f.compressed, src_end)\n                            dst: str = ofp.read().decode(\"utf-8\")\n                            # fileish.unlink(missing_ok=True) > py3.7\n                            if fileish.exists():\n                                fileish.unlink()\n                            if sys.platform == \"win32\":  # hint for mypy\n                                _winapi.CreateJunction(str(fileish), dst)  # noqa\n                    elif f.is_symlink and not isinstance(fileish, MemIO):\n                        with io.BytesIO() as omfp:\n                            self.decompress(fp, f.folder, omfp, f.uncompressed, f.compressed, src_end)\n                            omfp.seek(0)\n                            sym_target = pathlib.Path(omfp.read().decode(\"utf-8\"))\n                            # fileish.unlink(missing_ok=True) > py3.7\n                            if fileish.exists():\n                                fileish.unlink()\n                            fileish.symlink_to(sym_target)\n                    else:\n                        with fileish.open(mode=\"wb\") as obfp:\n                            crc32 = self.decompress(fp, f.folder, obfp, f.uncompressed, f.compressed, src_end)\n                            obfp.seek(0)\n                            if f.crc32 is not None and crc32 != f.crc32:\n                                raise CrcError(crc32, f.crc32, f.filename)\n                else:\n                    # just create empty file\n                    if not isinstance(fileish, MemIO):\n                        fileish.touch()\n                    else:\n                        with fileish.open() as ofp:\n                            pass\n            if q is not None:\n                q.put((\"e\", str(f.filename), str(f.uncompressed)))\n        if not skip_notarget:\n            # delayed execution of crc check.\n            self._check(fp, just_check, src_end)\n\n    def _check(self, fp, check_target, src_end):\n        \"\"\"\n        delayed execution of crc check.\n        \"\"\"\n        for f in check_target:\n            with NullIO() as ofp:\n                crc32 = self.decompress(fp, f.folder, ofp, f.uncompressed, f.compressed, src_end)\n            if f.crc32 is not None and crc32 != f.crc32:\n                raise CrcError(crc32, f.crc32, f.filename)\n\n    def decompress(\n        self,\n        fp: BinaryIO,\n        folder,\n        fq: IO[Any],\n        size: int,\n        compressed_size: Optional[int],\n        src_end: int,\n    ) -> int:\n        \"\"\"\n        decompressor wrapper called from extract method.\n\n        :parameter fp: archive source file pointer\n        :parameter folder: Folder object that have decompressor object.\n        :parameter fq: output file pathlib.Path\n        :parameter size: uncompressed size of target file.\n        :parameter compressed_size: compressed size of target file.\n        :parameter src_end: end position of the folder\n\n        :returns None\n\n        \"\"\"\n        assert folder is not None\n        out_remaining = size\n        max_block_size = get_memory_limit()\n        crc32 = 0\n        decompressor = folder.get_decompressor(compressed_size)\n        while out_remaining > 0:\n            tmp = decompressor.decompress(fp, min(out_remaining, max_block_size))\n            if len(tmp) > 0:\n                out_remaining -= len(tmp)\n                fq.write(tmp)\n                crc32 = calculate_crc32(tmp, crc32)\n            if out_remaining <= 0:\n                break\n        if fp.tell() >= src_end:\n            if decompressor.crc is not None and not decompressor.check_crc():\n                raise CrcError(decompressor.crc, decompressor.digest, None)\n        return crc32\n\n    def _find_link_target(self, target):\n        \"\"\"\n        Find the target member of a symlink or hardlink member in the archive.\n        \"\"\"\n        targetname: str = target.as_posix()\n        linkname = readlink(targetname)\n        # Check windows full path symlinks\n        if linkname.startswith(\"\\\\\\\\?\\\\\"):\n            linkname = linkname[4:]\n        # normalize as posix style\n        linkname: str = pathlib.Path(linkname).as_posix()\n        member = None\n        for j in range(len(self.files)):\n            if linkname == self.files[j].origin.as_posix():\n                # FIXME: when API user specify arcname, it will break\n                member = os.path.relpath(linkname, os.path.dirname(targetname))\n                break\n        if member is None:\n            member = linkname\n        return member\n\n    def _after_write(self, insize, foutsize, crc):\n        self.header.main_streams.substreamsinfo.digestsdefined.append(True)\n        self.header.main_streams.substreamsinfo.digests.append(crc)\n        if self.header.main_streams.substreamsinfo.unpacksizes is None:\n            self.header.main_streams.substreamsinfo.unpacksizes = [insize]\n        else:\n            self.header.main_streams.substreamsinfo.unpacksizes.append(insize)\n        if self.header.main_streams.substreamsinfo.num_unpackstreams_folders is None:\n            self.header.main_streams.substreamsinfo.num_unpackstreams_folders = [1]\n        else:\n            self.header.main_streams.substreamsinfo.num_unpackstreams_folders[-1] += 1\n        return foutsize, crc\n\n    def write(self, fp: BinaryIO, f, assym, folder):\n        compressor = folder.get_compressor()\n        if assym:\n            link_target: str = self._find_link_target(f.origin)\n            tgt: bytes = link_target.encode(\"utf-8\")\n            fd = io.BytesIO(tgt)\n            insize, foutsize, crc = compressor.compress(fd, fp)\n            fd.close()\n        else:\n            with f.origin.open(mode=\"rb\") as fd:\n                insize, foutsize, crc = compressor.compress(fd, fp)\n        return self._after_write(insize, foutsize, crc)\n\n    def writestr(self, fp: BinaryIO, f, folder):\n        compressor = folder.get_compressor()\n        insize, foutsize, crc = compressor.compress(f.data(), fp)\n        return self._after_write(insize, foutsize, crc)\n\n    def flush_archive(self, fp, folder):\n        compressor = folder.get_compressor()\n        foutsize = compressor.flush(fp)\n        if len(self.files) > 0:\n            if \"maxsize\" in self.header.files_info.files[self.last_file_index]:\n                self.header.files_info.files[self.last_file_index][\"maxsize\"] += foutsize\n            else:\n                self.header.files_info.files[self.last_file_index][\"maxsize\"] = foutsize\n        # Update size data in header\n        self.header.main_streams.packinfo.numstreams += 1\n        if self.header.main_streams.packinfo.enable_digests:\n            self.header.main_streams.packinfo.crcs.append(compressor.digest)\n            self.header.main_streams.packinfo.digestdefined.append(True)\n        self.header.main_streams.packinfo.packsizes.append(compressor.packsize)\n        folder.unpacksizes = compressor.unpacksizes\n\n    def archive(self, fp: BinaryIO, files, folder, deref=False):\n        \"\"\"Run archive task for specified 7zip folder.\"\"\"\n        f = files[self.current_file_index]\n        if f.has_strdata():\n            foutsize, crc = self.writestr(fp, f, folder)\n            self.header.files_info.files[self.current_file_index][\"maxsize\"] = foutsize\n            self.header.files_info.files[self.current_file_index][\"digest\"] = crc\n            self.last_file_index = self.current_file_index\n        elif (f.is_symlink and not deref) or not f.emptystream:\n            foutsize, crc = self.write(fp, f, (f.is_symlink and not deref), folder)\n            self.header.files_info.files[self.current_file_index][\"maxsize\"] = foutsize\n            self.header.files_info.files[self.current_file_index][\"digest\"] = crc\n            self.last_file_index = self.current_file_index\n        self.current_file_index += 1\n\n    def register_filelike(self, id: int, fileish: Union[MemIO, pathlib.Path, None]) -> None:\n        \"\"\"register file-ish to worker.\"\"\"\n        self.target_filepath[id] = fileish\n\n    def close(self):\n        del self.header\n        del self.files\n        del self.concurrent\n", "patch": "@@ -50,12 +50,13 @@\n     MemIO,\n     NullIO,\n     calculate_crc32,\n+    check_archive_path,\n     filetime_to_dt,\n+    get_sanitized_output_path,\n+    is_target_path_valid,\n     readlink,\n-    remove_relative_path_marker,\n )\n from py7zr.properties import DEFAULT_FILTERS, FILTER_DEFLATE64, MAGIC_7Z, get_default_blocksize, get_memory_limit\n-from py7zr.win32compat import is_windows_native_python, is_windows_unc_path\n \n if sys.platform.startswith(\"win\"):\n     import _winapi\n@@ -567,34 +568,10 @@ def _extract(\n                         break\n                     i += 1\n             fnames.append(outname)\n-            # check f.filename has invalid directory traversals\n-            if path is None:\n-                # do following but is_relative_to introduced in py 3.9\n-                # so I replaced it with relative_to. when condition is not satisfied, raise ValueError\n-                # if not pathlib.Path(...).joinpath(remove_relative_path_marker(outname)).is_relative_to(...):\n-                #    raise Bad7zFile\n-                try:\n-                    pathlib.Path(os.getcwd()).joinpath(remove_relative_path_marker(outname)).relative_to(os.getcwd())\n-                except ValueError:\n-                    raise Bad7zFile\n-                outfilename = pathlib.Path(remove_relative_path_marker(outname))\n+            if path is None or path.is_absolute():\n+                outfilename = get_sanitized_output_path(outname, path)\n             else:\n-                outfilename = path.joinpath(remove_relative_path_marker(outname))\n-                try:\n-                    outfilename.relative_to(path)\n-                except ValueError:\n-                    raise Bad7zFile\n-            # When python on Windows and not python on Cygwin,\n-            # Add win32 file namespace to exceed microsoft windows\n-            # path length limitation to 260 bytes\n-            # ref.\n-            # https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file\n-            # In editions of Windows before Windows 10 version 1607,\n-            # the maximum length for a path is MAX_PATH, which is defined as\n-            # 260 characters. In later versions of Windows, changing a registry key\n-            # or select option when python installation is required to remove the limit.\n-            if is_windows_native_python() and outfilename.is_absolute() and not is_windows_unc_path(outfilename):\n-                outfilename = pathlib.WindowsPath(\"\\\\\\\\?\\\\\" + str(outfilename))\n+                outfilename = get_sanitized_output_path(outname, pathlib.Path(os.getcwd()).joinpath(path))\n             if targets is not None and f.filename not in targets:\n                 self.worker.register_filelike(f.id, None)\n                 continue\n@@ -634,12 +611,14 @@ def _extract(\n         if callback is not None:\n             self.worker.extract(\n                 self.fp,\n+                path,\n                 parallel=(not self.password_protected and not self._filePassed),\n                 q=self.q,\n             )\n         else:\n             self.worker.extract(\n                 self.fp,\n+                path,\n                 parallel=(not self.password_protected and not self._filePassed),\n             )\n \n@@ -1040,6 +1019,11 @@ def writed(self, targets: Dict[str, IO[Any]]) -> None:\n             self.writef(input, target)\n \n     def writef(self, bio: IO[Any], arcname: str):\n+        if not check_archive_path(arcname):\n+            raise ValueError(f\"Specified path is bad: {arcname}\")\n+        return self._writef(bio, arcname)\n+\n+    def _writef(self, bio: IO[Any], arcname: str):\n         if isinstance(bio, io.BytesIO):\n             size = bio.getbuffer().nbytes\n         elif isinstance(bio, io.TextIOBase):\n@@ -1069,12 +1053,17 @@ def writef(self, bio: IO[Any], arcname: str):\n             self.files.append(file_info)\n \n     def writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):\n+        if not check_archive_path(arcname):\n+            raise ValueError(f\"Specified path is bad: {arcname}\")\n+        return self._writestr(data, arcname)\n+\n+    def _writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):\n         if not isinstance(arcname, str):\n             raise ValueError(\"Unsupported arcname\")\n         if isinstance(data, str):\n-            self.writef(io.BytesIO(data.encode(\"UTF-8\")), arcname)\n+            self._writef(io.BytesIO(data.encode(\"UTF-8\")), arcname)\n         elif isinstance(data, bytes) or isinstance(data, bytearray) or isinstance(data, memoryview):\n-            self.writef(io.BytesIO(data), arcname)\n+            self._writef(io.BytesIO(bytes(data)), arcname)\n         else:\n             raise ValueError(\"Unsupported data type.\")\n \n@@ -1131,7 +1120,9 @@ def testzip(self) -> Optional[str]:\n         for f in self.files:\n             self.worker.register_filelike(f.id, None)\n         try:\n-            self.worker.extract(self.fp, parallel=(not self.password_protected), skip_notarget=False)  # TODO: print progress\n+            self.worker.extract(\n+                self.fp, None, parallel=(not self.password_protected), skip_notarget=False\n+            )  # TODO: print progress\n         except CrcError as crce:\n             return crce.args[2]\n         else:\n@@ -1200,7 +1191,7 @@ def __init__(self, files, src_start: int, header, mp=False) -> None:\n         else:\n             self.concurrent = Thread\n \n-    def extract(self, fp: BinaryIO, parallel: bool, skip_notarget=True, q=None) -> None:\n+    def extract(self, fp: BinaryIO, path: Optional[pathlib.Path], parallel: bool, skip_notarget=True, q=None) -> None:\n         \"\"\"Extract worker method to handle 7zip folder and decompress each files.\"\"\"\n         if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:\n             src_end = self.src_start + self.header.main_streams.packinfo.packpositions[-1]\n@@ -1209,6 +1200,7 @@ def extract(self, fp: BinaryIO, parallel: bool, skip_notarget=True, q=None) -> N\n                 self.extract_single(\n                     fp,\n                     self.files,\n+                    path,\n                     self.src_start,\n                     src_end,\n                     q,\n@@ -1219,14 +1211,15 @@ def extract(self, fp: BinaryIO, parallel: bool, skip_notarget=True, q=None) -> N\n                 positions = self.header.main_streams.packinfo.packpositions\n                 empty_files = [f for f in self.files if f.emptystream]\n                 if not parallel:\n-                    self.extract_single(fp, empty_files, 0, 0, q)\n+                    self.extract_single(fp, empty_files, path, 0, 0, q)\n                     for i in range(numfolders):\n                         if skip_notarget:\n                             if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):\n                                 continue\n                         self.extract_single(\n                             fp,\n                             folders[i].files,\n+                            path,\n                             self.src_start + positions[i],\n                             self.src_start + positions[i + 1],\n                             q,\n@@ -1236,7 +1229,7 @@ def extract(self, fp: BinaryIO, parallel: bool, skip_notarget=True, q=None) -> N\n                     if getattr(fp, \"name\", None) is None:\n                         raise InternalError(\"Caught unknown variable status error\")\n                     filename: str = getattr(fp, \"name\", \"\")  # do not become \"\" but it is for type check.\n-                    self.extract_single(open(filename, \"rb\"), empty_files, 0, 0, q)\n+                    self.extract_single(open(filename, \"rb\"), empty_files, path, 0, 0, q)\n                     concurrent_tasks = []\n                     exc_q: queue.Queue = queue.Queue()\n                     for i in range(numfolders):\n@@ -1248,6 +1241,7 @@ def extract(self, fp: BinaryIO, parallel: bool, skip_notarget=True, q=None) -> N\n                             args=(\n                                 filename,\n                                 folders[i].files,\n+                                path,\n                                 self.src_start + positions[i],\n                                 self.src_start + positions[i + 1],\n                                 q,\n@@ -1266,12 +1260,13 @@ def extract(self, fp: BinaryIO, parallel: bool, skip_notarget=True, q=None) -> N\n                         raise exc_info[1].with_traceback(exc_info[2])\n         else:\n             empty_files = [f for f in self.files if f.emptystream]\n-            self.extract_single(fp, empty_files, 0, 0, q)\n+            self.extract_single(fp, empty_files, path, 0, 0, q)\n \n     def extract_single(\n         self,\n         fp: Union[BinaryIO, str],\n         files,\n+        path,\n         src_start: int,\n         src_end: int,\n         q: Optional[queue.Queue],\n@@ -1287,7 +1282,7 @@ def extract_single(\n             if isinstance(fp, str):\n                 fp = open(fp, \"rb\")\n             fp.seek(src_start)\n-            self._extract_single(fp, files, src_end, q, skip_notarget)\n+            self._extract_single(fp, files, path, src_end, q, skip_notarget)\n         except Exception as e:\n             if exc_q is None:\n                 raise e\n@@ -1299,6 +1294,7 @@ def _extract_single(\n         self,\n         fp: BinaryIO,\n         files,\n+        path,\n         src_end: int,\n         q: Optional[queue.Queue],\n         skip_notarget=True,\n@@ -1331,20 +1327,28 @@ def _extract_single(\n                         with io.BytesIO() as ofp:\n                             self.decompress(fp, f.folder, ofp, f.uncompressed, f.compressed, src_end)\n                             dst: str = ofp.read().decode(\"utf-8\")\n-                            # fileish.unlink(missing_ok=True) > py3.7\n-                            if fileish.exists():\n-                                fileish.unlink()\n-                            if sys.platform == \"win32\":  # hint for mypy\n-                                _winapi.CreateJunction(str(fileish), dst)  # noqa\n+                            if is_target_path_valid(path, fileish.parent.joinpath(dst)):\n+                                # fileish.unlink(missing_ok=True) > py3.7\n+                                if fileish.exists():\n+                                    fileish.unlink()\n+                                if sys.platform == \"win32\":  # hint for mypy\n+                                    _winapi.CreateJunction(str(fileish), dst)  # noqa\n+                            else:\n+                                raise Bad7zFile(\"Junction point out of target directory.\")\n                     elif f.is_symlink and not isinstance(fileish, MemIO):\n                         with io.BytesIO() as omfp:\n                             self.decompress(fp, f.folder, omfp, f.uncompressed, f.compressed, src_end)\n                             omfp.seek(0)\n-                            sym_target = pathlib.Path(omfp.read().decode(\"utf-8\"))\n-                            # fileish.unlink(missing_ok=True) > py3.7\n-                            if fileish.exists():\n-                                fileish.unlink()\n-                            fileish.symlink_to(sym_target)\n+                            dst = omfp.read().decode(\"utf-8\")\n+                            # check sym_target points inside an archive target?\n+                            if is_target_path_valid(path, fileish.parent.joinpath(dst)):\n+                                sym_target = pathlib.Path(dst)\n+                                # fileish.unlink(missing_ok=True) > py3.7\n+                                if fileish.exists():\n+                                    fileish.unlink()\n+                                fileish.symlink_to(sym_target)\n+                            else:\n+                                raise Bad7zFile(\"Symlink point out of target directory.\")\n                     else:\n                         with fileish.open(mode=\"wb\") as obfp:\n                             crc32 = self.decompress(fp, f.folder, obfp, f.uncompressed, f.compressed, src_end)", "file_path": "files/2022_12/1697", "file_language": "py", "file_name": "py7zr/py7zr.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/miurahr/py7zr/raw/1bb43f17515c7f69673a1c88ab9cc72a7bbef406/tests%2Ftest_extract.py", "code": "import asyncio\nimport binascii\nimport ctypes\nimport hashlib\nimport os\nimport pathlib\nimport shutil\nimport subprocess\nimport sys\nfrom datetime import datetime\n\nimport pytest\n\nimport py7zr\nfrom py7zr import unpack_7zarchive\nfrom py7zr.exceptions import CrcError, UnsupportedCompressionMethodError\nfrom py7zr.helpers import UTC\n\nfrom . import aio7zr, decode_all\n\ntestdata_path = pathlib.Path(os.path.dirname(__file__)).joinpath(\"data\")\nos.umask(0o022)\n\n\ndef check_archive(archive, tmp_path, return_dict: bool):\n    assert sorted(archive.getnames()) == [\"test\", \"test/test2.txt\", \"test1.txt\"]\n    expected = []\n    expected.append({\"filename\": \"test\"})\n    expected.append(\n        {\n            \"lastwritetime\": 12786932616,\n            \"as_datetime\": datetime(2006, 3, 15, 21, 43, 36, 0, UTC()),\n            \"filename\": \"test/test2.txt\",\n        }\n    )\n    expected.append(\n        {\n            \"lastwritetime\": 12786932628,\n            \"as_datetime\": datetime(2006, 3, 15, 21, 43, 48, 0, UTC()),\n            \"filename\": \"test1.txt\",\n        }\n    )\n    for i, cf in enumerate(archive.files):\n        assert cf.filename == expected[i][\"filename\"]\n        if not cf.is_directory:\n            assert cf.lastwritetime // 10000000 == expected[i][\"lastwritetime\"]\n            assert cf.lastwritetime.as_datetime().replace(microsecond=0) == expected[i][\"as_datetime\"]\n    if not return_dict:\n        archive.extractall(path=tmp_path)\n        assert tmp_path.joinpath(\"test/test2.txt\").open(\"rb\").read() == bytes(\"This file is located in a folder.\", \"ascii\")\n        assert tmp_path.joinpath(\"test1.txt\").open(\"rb\").read() == bytes(\"This file is located in the root.\", \"ascii\")\n    else:\n        _dict = archive.readall()\n        actual = _dict[\"test/test2.txt\"].read()\n        assert actual == bytes(\"This file is located in a folder.\", \"ascii\")\n        actual = _dict[\"test1.txt\"].read()\n        assert actual == bytes(\"This file is located in the root.\", \"ascii\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_solid(tmp_path):\n    f = \"solid.7z\"\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(f).open(mode=\"rb\"))\n    check_archive(archive, tmp_path, False)\n\n\n@pytest.mark.files\ndef test_solid_mem(tmp_path):\n    f = \"solid.7z\"\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(f).open(mode=\"rb\"))\n    check_archive(archive, tmp_path, True)\n\n\n@pytest.mark.files\ndef test_empty():\n    # decompress empty archive\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"empty.7z\").open(mode=\"rb\"))\n    assert archive.getnames() == []\n\n\n@pytest.mark.files\ndef test_github_14(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"github_14.7z\").open(mode=\"rb\"))\n    assert archive.getnames() == [\"github_14\"]\n    archive.extractall(path=tmp_path)\n    with tmp_path.joinpath(\"github_14\").open(\"rb\") as f:\n        assert f.read() == bytes(\"Hello GitHub issue #14.\\n\", \"ascii\")\n\n\n@pytest.mark.files\ndef test_github_14_mem(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"github_14.7z\").open(mode=\"rb\"))\n    _dict = archive.readall()\n    actual = _dict[\"github_14\"].read()\n    assert actual == bytes(\"Hello GitHub issue #14.\\n\", \"ascii\")\n\n\n@pytest.mark.files\ndef _test_umlaut_archive(filename: str, target: pathlib.Path, return_dict: bool):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(filename).open(mode=\"rb\"))\n    if not return_dict:\n        assert sorted(archive.getnames()) == [\"t\\xe4st.txt\"]\n        archive.extractall(path=target)\n        actual = target.joinpath(\"t\\xe4st.txt\").open().read()\n        assert actual == \"This file contains a german umlaut in the filename.\"\n    else:\n        _dict = archive.readall()\n        actual = _dict[\"t\\xe4st.txt\"].read()\n        assert actual == b\"This file contains a german umlaut in the filename.\"\n    archive.close()\n\n\n@pytest.mark.files\ndef test_non_solid_umlaut(tmp_path):\n    # test loading of a non-solid archive containing files with umlauts\n    _test_umlaut_archive(\"umlaut-non_solid.7z\", tmp_path, False)\n\n\n@pytest.mark.files\ndef test_non_solid_umlaut_mem(tmp_path):\n    # test loading of a non-solid archive containing files with umlauts\n    _test_umlaut_archive(\"umlaut-non_solid.7z\", tmp_path, True)\n\n\n@pytest.mark.files\ndef test_solid_umlaut(tmp_path):\n    # test loading of a solid archive containing files with umlauts\n    _test_umlaut_archive(\"umlaut-solid.7z\", tmp_path, False)\n\n\n@pytest.mark.files\ndef test_solid_umlaut_mem(tmp_path):\n    # test loading of a solid archive containing files with umlauts\n    _test_umlaut_archive(\"umlaut-solid.7z\", tmp_path, True)\n\n\n@pytest.mark.files\ndef test_bugzilla_4(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"bugzilla_4.7z\").open(mode=\"rb\"))\n    expected = [\n        {\n            \"filename\": \"History.txt\",\n            \"mtime\": 1133704668,\n            \"mode\": 33188,\n            \"digest\": \"46b08f0af612371860ab39e3b47666c3bd6fb742c5e8775159310e19ebedae7e\",\n        },\n        {\n            \"filename\": \"License.txt\",\n            \"mtime\": 1105356710,\n            \"mode\": 33188,\n            \"digest\": \"4f49a4448499449f2864777c895f011fb989836a37990ae1ca532126ca75d25e\",\n        },\n        {\n            \"filename\": \"copying.txt\",\n            \"mtime\": 999116366,\n            \"mode\": 33188,\n            \"digest\": \"2c3c3ef532828bcd42bb3127349625a25291ff5ae7e6f8d42e0fe9b5be836a99\",\n        },\n        {\n            \"filename\": \"readme.txt\",\n            \"mtime\": 1133704646,\n            \"mode\": 33188,\n            \"digest\": \"84f2693d9746e919883cf169fc83467be6566d7501b5044693a2480ab36a4899\",\n        },\n    ]\n    decode_all(archive, expected, tmp_path)\n\n\n@pytest.mark.files\n@pytest.mark.skipif(\n    sys.platform.startswith(\"win\") and (ctypes.windll.shell32.IsUserAnAdmin() == 0),\n    reason=\"Administrator rights is required to make symlink on windows\",\n)\ndef test_extract_symlink(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"symlink.7z\").open(mode=\"rb\"))\n    assert sorted(archive.getnames()) == [\n        \"lib\",\n        \"lib/libabc.so\",\n        \"lib/libabc.so.1\",\n        \"lib/libabc.so.1.2\",\n        \"lib/libabc.so.1.2.3\",\n        \"lib64\",\n    ]\n    archive.extractall(path=tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_symlink_mem():\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"symlink.7z\").open(mode=\"rb\")) as archive:\n        _dict = archive.readall()\n\n\n@pytest.mark.files\ndef test_lzma2bcj(tmp_path):\n    \"\"\"Test extract archive compressed with LZMA2 and BCJ methods.\"\"\"\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"lzma2bcj.7z\").open(mode=\"rb\"))\n    assert archive.getnames() == [\n        \"mingw64\",\n        \"mingw64/bin\",\n        \"mingw64/include\",\n        \"mingw64/lib\",\n        \"mingw64/share\",\n        \"mingw64/share/doc\",\n        \"mingw64/share/doc/szip\",\n        \"mingw64/include/SZconfig.h\",\n        \"mingw64/include/ricehdf.h\",\n        \"mingw64/include/szip_adpt.h\",\n        \"mingw64/include/szlib.h\",\n        \"mingw64/lib/libszip.a\",\n        \"mingw64/lib/libszip.dll.a\",\n        \"mingw64/share/doc/szip/COPYING\",\n        \"mingw64/share/doc/szip/HISTORY.txt\",\n        \"mingw64/share/doc/szip/INSTALL\",\n        \"mingw64/share/doc/szip/README\",\n        \"mingw64/share/doc/szip/RELEASE.txt\",\n        \"mingw64/bin/libszip-0.dll\",\n    ]\n    archive.extractall(path=tmp_path)\n    m = hashlib.sha256()\n    m.update(tmp_path.joinpath(\"mingw64/bin/libszip-0.dll\").open(\"rb\").read())\n    assert m.digest() == binascii.unhexlify(\"13926e3f080c9ca557165864ce5722acc4f832bb52a92d8d86c7f6e583708c4d\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_lzma2bcj_mem():\n    \"\"\"Test extract archive compressed with LZMA2 and BCJ methods.\"\"\"\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"lzma2bcj.7z\").open(mode=\"rb\"))\n    assert archive.getnames() == [\n        \"mingw64\",\n        \"mingw64/bin\",\n        \"mingw64/include\",\n        \"mingw64/lib\",\n        \"mingw64/share\",\n        \"mingw64/share/doc\",\n        \"mingw64/share/doc/szip\",\n        \"mingw64/include/SZconfig.h\",\n        \"mingw64/include/ricehdf.h\",\n        \"mingw64/include/szip_adpt.h\",\n        \"mingw64/include/szlib.h\",\n        \"mingw64/lib/libszip.a\",\n        \"mingw64/lib/libszip.dll.a\",\n        \"mingw64/share/doc/szip/COPYING\",\n        \"mingw64/share/doc/szip/HISTORY.txt\",\n        \"mingw64/share/doc/szip/INSTALL\",\n        \"mingw64/share/doc/szip/README\",\n        \"mingw64/share/doc/szip/RELEASE.txt\",\n        \"mingw64/bin/libszip-0.dll\",\n    ]\n    _dict = archive.readall()\n    m = hashlib.sha256()\n    m.update(_dict[\"mingw64/bin/libszip-0.dll\"].read())\n    assert m.digest() == binascii.unhexlify(\"13926e3f080c9ca557165864ce5722acc4f832bb52a92d8d86c7f6e583708c4d\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_lzma2bcj2(tmp_path):\n    \"\"\"Test extract archive compressed with LZMA2 and BCJ2 methods.\"\"\"\n    with pytest.raises(UnsupportedCompressionMethodError):\n        with testdata_path.joinpath(\"lzma2bcj2.7z\").open(mode=\"rb\") as target:\n            archive = py7zr.SevenZipFile(target)\n            archive.extractall(path=tmp_path)\n            archive.close()\n\n\n@pytest.mark.files\ndef test_lzma2bcj2_2(tmp_path):\n    \"\"\"Test extract archive compressed with LZMA2 and BCJ2 in multiple chunks.\"\"\"\n    with pytest.raises(UnsupportedCompressionMethodError):\n        with testdata_path.joinpath(\"lzma2bcj2_2.7z\").open(mode=\"rb\") as target:\n            archive = py7zr.SevenZipFile(target)\n            archive.extractall(path=tmp_path)\n            archive.close()\n\n\n@pytest.mark.files\ndef test_extract_lzma_1(tmp_path):\n    with testdata_path.joinpath(\"lzma_1.7z\").open(mode=\"rb\") as target:\n        with py7zr.SevenZipFile(target) as ar:\n            ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_lzma2_1(tmp_path):\n    with testdata_path.joinpath(\"lzma2_1.7z\").open(mode=\"rb\") as target:\n        with py7zr.SevenZipFile(target) as ar:\n            _dict = ar.readall()\n\n\n@pytest.mark.files\ndef test_zerosize(tmp_path):\n    with testdata_path.joinpath(\"zerosize.7z\").open(mode=\"rb\") as target:\n        archive = py7zr.SevenZipFile(target)\n        archive.extractall(path=tmp_path)\n        archive.close()\n\n\n@pytest.mark.files\ndef test_zerosize_mem():\n    with testdata_path.joinpath(\"zerosize.7z\").open(mode=\"rb\") as target:\n        archive = py7zr.SevenZipFile(target)\n        _dict = archive.readall()\n        archive.close()\n\n\n@pytest.mark.api\ndef test_register_unpack_archive(tmp_path):\n    shutil.register_unpack_format(\"7zip\", [\".7z\"], unpack_7zarchive)\n    shutil.unpack_archive(str(testdata_path.joinpath(\"test_1.7z\")), str(tmp_path))\n    target = tmp_path.joinpath(\"setup.cfg\")\n    expected_mode = 33188\n    expected_mtime = 1552522033\n    if os.name == \"posix\":\n        assert target.stat().st_mode == expected_mode\n    assert target.stat().st_mtime == expected_mtime\n    m = hashlib.sha256()\n    m.update(target.open(\"rb\").read())\n    assert m.digest() == binascii.unhexlify(\"ff77878e070c4ba52732b0c847b5a055a7c454731939c3217db4a7fb4a1e7240\")\n    m = hashlib.sha256()\n    m.update(tmp_path.joinpath(\"setup.py\").open(\"rb\").read())\n    assert m.digest() == binascii.unhexlify(\"b916eed2a4ee4e48c51a2b51d07d450de0be4dbb83d20e67f6fd166ff7921e49\")\n    m = hashlib.sha256()\n    m.update(tmp_path.joinpath(\"scripts/py7zr\").open(\"rb\").read())\n    assert m.digest() == binascii.unhexlify(\"b0385e71d6a07eb692f5fb9798e9d33aaf87be7dfff936fd2473eab2a593d4fd\")\n\n\n@pytest.mark.files\ndef test_skip():\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"test_1.7z\").open(mode=\"rb\"))\n    for i, cf in enumerate(archive.files):\n        assert cf is not None\n        archive.worker.register_filelike(cf.id, None)\n    archive.worker.extract(archive.fp, None, parallel=True)\n    archive.close()\n\n\n@pytest.mark.files\ndef test_github_14_multi(tmp_path):\n    \"\"\"multiple unnamed objects.\"\"\"\n    archive = py7zr.SevenZipFile(str(testdata_path.joinpath(\"github_14_multi.7z\")), \"r\")\n    assert archive.getnames() == [\"github_14_multi\", \"github_14_multi\"]\n    archive.extractall(path=tmp_path)\n    with tmp_path.joinpath(\"github_14_multi\").open(\"rb\") as f:\n        assert f.read() == bytes(\"Hello GitHub issue #14 1/2.\\n\", \"ascii\")\n    with tmp_path.joinpath(\"github_14_multi_0\").open(\"rb\") as f:\n        assert f.read() == bytes(\"Hello GitHub issue #14 2/2.\\n\", \"ascii\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_github_14_multi_mem():\n    \"\"\"multiple unnamed objects.\"\"\"\n    archive = py7zr.SevenZipFile(str(testdata_path.joinpath(\"github_14_multi.7z\")), \"r\")\n    assert archive.getnames() == [\"github_14_multi\", \"github_14_multi\"]\n    _dict = archive.readall()\n    actual_1 = _dict[\"github_14_multi\"].read()\n    assert actual_1 == bytes(\"Hello GitHub issue #14 1/2.\\n\", \"ascii\")\n    actual_2 = _dict[\"github_14_multi_0\"].read()\n    assert actual_2 == bytes(\"Hello GitHub issue #14 2/2.\\n\", \"ascii\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_multiblock(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"mblock_1.7z\").open(mode=\"rb\"))\n    archive.extractall(path=tmp_path)\n    m = hashlib.sha256()\n    m.update(tmp_path.joinpath(\"bin/7zdec.exe\").open(\"rb\").read())\n    assert m.digest() == binascii.unhexlify(\"e14d8201c5c0d1049e717a63898a3b1c7ce4054a24871daebaa717da64dcaff5\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_multiblock_mem():\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"mblock_1.7z\").open(mode=\"rb\"))\n    _dict = archive.readall()\n    m = hashlib.sha256()\n    m.update(_dict[\"bin/7zdec.exe\"].read())\n    assert m.digest() == binascii.unhexlify(\"e14d8201c5c0d1049e717a63898a3b1c7ce4054a24871daebaa717da64dcaff5\")\n    archive.close()\n\n\n@pytest.mark.files\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"Cannot unlink opened file on Windows\")\ndef test_multiblock_unlink(tmp_path):\n    \"\"\"When passing opened file object, even after unlink it should work.\"\"\"\n    shutil.copy(str(testdata_path.joinpath(\"mblock_1.7z\")), str(tmp_path))\n    src = tmp_path.joinpath(\"mblock_1.7z\")\n    archive = py7zr.SevenZipFile(open(str(src), \"rb\"))\n    os.unlink(str(src))\n    archive.extractall(path=tmp_path)\n    archive.close()\n\n\n@pytest.mark.files\ndef test_copy(tmp_path):\n    \"\"\"test loading of copy compressed files.(help wanted)\"\"\"\n    check_archive(\n        py7zr.SevenZipFile(testdata_path.joinpath(\"copy.7z\").open(mode=\"rb\")),\n        tmp_path,\n        False,\n    )\n\n\n@pytest.mark.files\ndef test_copy_2(tmp_path):\n    \"\"\"test loading of copy compressed files part2.\"\"\"\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"copy_2.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(path=tmp_path)\n\n\n@pytest.mark.files\ndef test_close_unlink(tmp_path):\n    shutil.copyfile(str(testdata_path.joinpath(\"test_1.7z\")), str(tmp_path.joinpath(\"test_1.7z\")))\n    archive = py7zr.SevenZipFile(str(tmp_path.joinpath(\"test_1.7z\")))\n    archive.extractall(path=str(tmp_path))\n    archive.close()\n    tmp_path.joinpath(\"test_1.7z\").unlink()\n\n\n@pytest.mark.files\n@pytest.mark.asyncio\n@pytest.mark.skipif(hasattr(sys, \"pypy_version_info\"), reason=\"Not working with pypy3\")\ndef test_asyncio_executor(tmp_path):\n    shutil.copyfile(os.path.join(testdata_path, \"test_1.7z\"), str(tmp_path.joinpath(\"test_1.7z\")))\n    loop = asyncio.get_event_loop()\n    task = asyncio.ensure_future(aio7zr(tmp_path.joinpath(\"test_1.7z\"), path=tmp_path))\n    loop.run_until_complete(task)\n    loop.run_until_complete(asyncio.sleep(3))\n    os.unlink(str(tmp_path.joinpath(\"test_1.7z\")))\n\n\n@pytest.mark.files\ndef test_no_main_streams(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"test_folder.7z\").open(mode=\"rb\"))\n    archive.extractall(path=tmp_path)\n    archive.close()\n\n\n@pytest.mark.files\ndef test_no_main_streams_mem():\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"test_folder.7z\").open(mode=\"rb\"))\n    _dict = archive.readall()\n    archive.close()\n\n\n@pytest.mark.files\n@pytest.mark.skipif(\n    sys.platform.startswith(\"win\") and (ctypes.windll.shell32.IsUserAnAdmin() == 0),\n    reason=\"Administrator rights is required to make symlink on windows\",\n)\ndef test_extract_symlink_with_relative_target_path(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"symlink.7z\").open(mode=\"rb\"))\n    os.chdir(str(tmp_path))\n    os.makedirs(str(tmp_path.joinpath(\"target\")))  # py35 need str() against pathlib.Path\n    archive.extractall(path=\"target\")\n    assert os.readlink(str(tmp_path.joinpath(\"target/lib/libabc.so.1.2\"))) == \"libabc.so.1.2.3\"\n    archive.close()\n\n\n@pytest.mark.files\n@pytest.mark.skipif(\n    sys.platform.startswith(\"win\") and (ctypes.windll.shell32.IsUserAnAdmin() == 0),\n    reason=\"Administrator rights is required to make symlink on windows\",\n)\ndef test_extract_emptystream_mix(tmp_path):\n    archive = py7zr.SevenZipFile(str(testdata_path.joinpath(\"test_6.7z\")), \"r\")\n    archive.extractall(path=tmp_path)\n    archive.close()\n\n\n@pytest.mark.files\ndef test_extract_longpath_file(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"longpath.7z\").open(\"rb\")) as archive:\n        archive.extractall(path=tmp_path)\n\n\n@pytest.mark.files\n@pytest.mark.skipif(\n    sys.platform.startswith(\"win\") and (ctypes.windll.shell32.IsUserAnAdmin() == 0),\n    reason=\"Administrator rights is required to make symlink on windows\",\n)\ndef test_extract_symlink_overwrite(tmp_path):\n    os.chdir(str(tmp_path))\n    os.makedirs(str(tmp_path.joinpath(\"target\")))  # py35 need str() against pathlib.Path\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"symlink.7z\").open(mode=\"rb\")) as archive:\n        archive.extractall(path=\"target\")\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"symlink.7z\").open(mode=\"rb\")) as archive:\n        archive.extractall(path=\"target\")\n    assert os.readlink(str(tmp_path.joinpath(\"target/lib/libabc.so.1.2\"))) == \"libabc.so.1.2.3\"\n\n\n@pytest.mark.files\ndef test_py7zr_extract_corrupted(tmp_path):\n    with pytest.raises(CrcError):\n        archive = py7zr.SevenZipFile(str(testdata_path.joinpath(\"crc_corrupted.7z\")), \"r\")\n        archive.extract(path=tmp_path)\n        archive.close()\n\n\n@pytest.mark.files\ndef test_extract_lzma2delta(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma2delta_1.7z\").open(\"rb\")) as archive:\n        archive.extractall(path=tmp_path)\n\n\n@pytest.mark.skipif(not shutil.which(\"7z\"), reason=\"no 7z command installed\")\ndef test_decompress_small_files(tmp_path):\n    tmp_path.joinpath(\"t\").mkdir()\n    with tmp_path.joinpath(\"t/a\").open(\"w\") as f:\n        f.write(\"1\")\n    with tmp_path.joinpath(\"t/b\").open(\"w\") as f:\n        f.write(\"2\")\n    result = subprocess.run(\n        [\"7z\", \"a\", (tmp_path / \"target.7z\").as_posix(), (tmp_path / \"t\").as_posix()],\n        stdout=subprocess.PIPE,\n    )\n    if result.returncode != 0:\n        print(result.stdout)\n        pytest.fail(\"7z command report error\")\n    #\n    with py7zr.SevenZipFile(tmp_path / \"target.7z\", \"r\") as arc:\n        arc.testzip()\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_x86(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_x86.7z\").open(mode=\"rb\")) as ar:\n        _dict = ar.readall()\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_arm(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_arm.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_armt(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_armt.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_ppc(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_ppc.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_sparc(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_sparc.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_2(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_2.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_hidden_linux_folder(tmp_path):\n    hidden_folder_name = \".hidden_folder\"\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"hidden_linux_folder.7z\").open(mode=\"rb\")) as archive:\n        assert sorted(archive.getnames()) == [\n            hidden_folder_name,\n        ]\n        archive.extractall(path=tmp_path)\n        assert tmp_path.joinpath(hidden_folder_name).exists()\n\n\n@pytest.mark.files\ndef test_extract_hidden_linux_file(tmp_path):\n    hidden_file_name = \".hidden_file.txt\"\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"hidden_linux_file.7z\").open(mode=\"rb\")) as archive:\n        assert sorted(archive.getnames()) == [\n            hidden_file_name,\n        ]\n        archive.extractall(path=tmp_path)\n        assert tmp_path.joinpath(hidden_file_name).exists()\n", "code_before": "import asyncio\nimport binascii\nimport ctypes\nimport hashlib\nimport os\nimport pathlib\nimport shutil\nimport subprocess\nimport sys\nfrom datetime import datetime\n\nimport pytest\n\nimport py7zr\nfrom py7zr import unpack_7zarchive\nfrom py7zr.exceptions import CrcError, UnsupportedCompressionMethodError\nfrom py7zr.helpers import UTC\n\nfrom . import aio7zr, decode_all\n\ntestdata_path = pathlib.Path(os.path.dirname(__file__)).joinpath(\"data\")\nos.umask(0o022)\n\n\ndef check_archive(archive, tmp_path, return_dict: bool):\n    assert sorted(archive.getnames()) == [\"test\", \"test/test2.txt\", \"test1.txt\"]\n    expected = []\n    expected.append({\"filename\": \"test\"})\n    expected.append(\n        {\n            \"lastwritetime\": 12786932616,\n            \"as_datetime\": datetime(2006, 3, 15, 21, 43, 36, 0, UTC()),\n            \"filename\": \"test/test2.txt\",\n        }\n    )\n    expected.append(\n        {\n            \"lastwritetime\": 12786932628,\n            \"as_datetime\": datetime(2006, 3, 15, 21, 43, 48, 0, UTC()),\n            \"filename\": \"test1.txt\",\n        }\n    )\n    for i, cf in enumerate(archive.files):\n        assert cf.filename == expected[i][\"filename\"]\n        if not cf.is_directory:\n            assert cf.lastwritetime // 10000000 == expected[i][\"lastwritetime\"]\n            assert cf.lastwritetime.as_datetime().replace(microsecond=0) == expected[i][\"as_datetime\"]\n    if not return_dict:\n        archive.extractall(path=tmp_path)\n        assert tmp_path.joinpath(\"test/test2.txt\").open(\"rb\").read() == bytes(\"This file is located in a folder.\", \"ascii\")\n        assert tmp_path.joinpath(\"test1.txt\").open(\"rb\").read() == bytes(\"This file is located in the root.\", \"ascii\")\n    else:\n        _dict = archive.readall()\n        actual = _dict[\"test/test2.txt\"].read()\n        assert actual == bytes(\"This file is located in a folder.\", \"ascii\")\n        actual = _dict[\"test1.txt\"].read()\n        assert actual == bytes(\"This file is located in the root.\", \"ascii\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_solid(tmp_path):\n    f = \"solid.7z\"\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(f).open(mode=\"rb\"))\n    check_archive(archive, tmp_path, False)\n\n\n@pytest.mark.files\ndef test_solid_mem(tmp_path):\n    f = \"solid.7z\"\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(f).open(mode=\"rb\"))\n    check_archive(archive, tmp_path, True)\n\n\n@pytest.mark.files\ndef test_empty():\n    # decompress empty archive\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"empty.7z\").open(mode=\"rb\"))\n    assert archive.getnames() == []\n\n\n@pytest.mark.files\ndef test_github_14(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"github_14.7z\").open(mode=\"rb\"))\n    assert archive.getnames() == [\"github_14\"]\n    archive.extractall(path=tmp_path)\n    with tmp_path.joinpath(\"github_14\").open(\"rb\") as f:\n        assert f.read() == bytes(\"Hello GitHub issue #14.\\n\", \"ascii\")\n\n\n@pytest.mark.files\ndef test_github_14_mem(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"github_14.7z\").open(mode=\"rb\"))\n    _dict = archive.readall()\n    actual = _dict[\"github_14\"].read()\n    assert actual == bytes(\"Hello GitHub issue #14.\\n\", \"ascii\")\n\n\n@pytest.mark.files\ndef _test_umlaut_archive(filename: str, target: pathlib.Path, return_dict: bool):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(filename).open(mode=\"rb\"))\n    if not return_dict:\n        assert sorted(archive.getnames()) == [\"t\\xe4st.txt\"]\n        archive.extractall(path=target)\n        actual = target.joinpath(\"t\\xe4st.txt\").open().read()\n        assert actual == \"This file contains a german umlaut in the filename.\"\n    else:\n        _dict = archive.readall()\n        actual = _dict[\"t\\xe4st.txt\"].read()\n        assert actual == b\"This file contains a german umlaut in the filename.\"\n    archive.close()\n\n\n@pytest.mark.files\ndef test_non_solid_umlaut(tmp_path):\n    # test loading of a non-solid archive containing files with umlauts\n    _test_umlaut_archive(\"umlaut-non_solid.7z\", tmp_path, False)\n\n\n@pytest.mark.files\ndef test_non_solid_umlaut_mem(tmp_path):\n    # test loading of a non-solid archive containing files with umlauts\n    _test_umlaut_archive(\"umlaut-non_solid.7z\", tmp_path, True)\n\n\n@pytest.mark.files\ndef test_solid_umlaut(tmp_path):\n    # test loading of a solid archive containing files with umlauts\n    _test_umlaut_archive(\"umlaut-solid.7z\", tmp_path, False)\n\n\n@pytest.mark.files\ndef test_solid_umlaut_mem(tmp_path):\n    # test loading of a solid archive containing files with umlauts\n    _test_umlaut_archive(\"umlaut-solid.7z\", tmp_path, True)\n\n\n@pytest.mark.files\ndef test_bugzilla_4(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"bugzilla_4.7z\").open(mode=\"rb\"))\n    expected = [\n        {\n            \"filename\": \"History.txt\",\n            \"mtime\": 1133704668,\n            \"mode\": 33188,\n            \"digest\": \"46b08f0af612371860ab39e3b47666c3bd6fb742c5e8775159310e19ebedae7e\",\n        },\n        {\n            \"filename\": \"License.txt\",\n            \"mtime\": 1105356710,\n            \"mode\": 33188,\n            \"digest\": \"4f49a4448499449f2864777c895f011fb989836a37990ae1ca532126ca75d25e\",\n        },\n        {\n            \"filename\": \"copying.txt\",\n            \"mtime\": 999116366,\n            \"mode\": 33188,\n            \"digest\": \"2c3c3ef532828bcd42bb3127349625a25291ff5ae7e6f8d42e0fe9b5be836a99\",\n        },\n        {\n            \"filename\": \"readme.txt\",\n            \"mtime\": 1133704646,\n            \"mode\": 33188,\n            \"digest\": \"84f2693d9746e919883cf169fc83467be6566d7501b5044693a2480ab36a4899\",\n        },\n    ]\n    decode_all(archive, expected, tmp_path)\n\n\n@pytest.mark.files\n@pytest.mark.skipif(\n    sys.platform.startswith(\"win\") and (ctypes.windll.shell32.IsUserAnAdmin() == 0),\n    reason=\"Administrator rights is required to make symlink on windows\",\n)\ndef test_extract_symlink(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"symlink.7z\").open(mode=\"rb\"))\n    assert sorted(archive.getnames()) == [\n        \"lib\",\n        \"lib/libabc.so\",\n        \"lib/libabc.so.1\",\n        \"lib/libabc.so.1.2\",\n        \"lib/libabc.so.1.2.3\",\n        \"lib64\",\n    ]\n    archive.extractall(path=tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_symlink_mem():\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"symlink.7z\").open(mode=\"rb\")) as archive:\n        _dict = archive.readall()\n\n\n@pytest.mark.files\ndef test_lzma2bcj(tmp_path):\n    \"\"\"Test extract archive compressed with LZMA2 and BCJ methods.\"\"\"\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"lzma2bcj.7z\").open(mode=\"rb\"))\n    assert archive.getnames() == [\n        \"mingw64\",\n        \"mingw64/bin\",\n        \"mingw64/include\",\n        \"mingw64/lib\",\n        \"mingw64/share\",\n        \"mingw64/share/doc\",\n        \"mingw64/share/doc/szip\",\n        \"mingw64/include/SZconfig.h\",\n        \"mingw64/include/ricehdf.h\",\n        \"mingw64/include/szip_adpt.h\",\n        \"mingw64/include/szlib.h\",\n        \"mingw64/lib/libszip.a\",\n        \"mingw64/lib/libszip.dll.a\",\n        \"mingw64/share/doc/szip/COPYING\",\n        \"mingw64/share/doc/szip/HISTORY.txt\",\n        \"mingw64/share/doc/szip/INSTALL\",\n        \"mingw64/share/doc/szip/README\",\n        \"mingw64/share/doc/szip/RELEASE.txt\",\n        \"mingw64/bin/libszip-0.dll\",\n    ]\n    archive.extractall(path=tmp_path)\n    m = hashlib.sha256()\n    m.update(tmp_path.joinpath(\"mingw64/bin/libszip-0.dll\").open(\"rb\").read())\n    assert m.digest() == binascii.unhexlify(\"13926e3f080c9ca557165864ce5722acc4f832bb52a92d8d86c7f6e583708c4d\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_lzma2bcj_mem():\n    \"\"\"Test extract archive compressed with LZMA2 and BCJ methods.\"\"\"\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"lzma2bcj.7z\").open(mode=\"rb\"))\n    assert archive.getnames() == [\n        \"mingw64\",\n        \"mingw64/bin\",\n        \"mingw64/include\",\n        \"mingw64/lib\",\n        \"mingw64/share\",\n        \"mingw64/share/doc\",\n        \"mingw64/share/doc/szip\",\n        \"mingw64/include/SZconfig.h\",\n        \"mingw64/include/ricehdf.h\",\n        \"mingw64/include/szip_adpt.h\",\n        \"mingw64/include/szlib.h\",\n        \"mingw64/lib/libszip.a\",\n        \"mingw64/lib/libszip.dll.a\",\n        \"mingw64/share/doc/szip/COPYING\",\n        \"mingw64/share/doc/szip/HISTORY.txt\",\n        \"mingw64/share/doc/szip/INSTALL\",\n        \"mingw64/share/doc/szip/README\",\n        \"mingw64/share/doc/szip/RELEASE.txt\",\n        \"mingw64/bin/libszip-0.dll\",\n    ]\n    _dict = archive.readall()\n    m = hashlib.sha256()\n    m.update(_dict[\"mingw64/bin/libszip-0.dll\"].read())\n    assert m.digest() == binascii.unhexlify(\"13926e3f080c9ca557165864ce5722acc4f832bb52a92d8d86c7f6e583708c4d\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_lzma2bcj2(tmp_path):\n    \"\"\"Test extract archive compressed with LZMA2 and BCJ2 methods.\"\"\"\n    with pytest.raises(UnsupportedCompressionMethodError):\n        with testdata_path.joinpath(\"lzma2bcj2.7z\").open(mode=\"rb\") as target:\n            archive = py7zr.SevenZipFile(target)\n            archive.extractall(path=tmp_path)\n            archive.close()\n\n\n@pytest.mark.files\ndef test_lzma2bcj2_2(tmp_path):\n    \"\"\"Test extract archive compressed with LZMA2 and BCJ2 in multiple chunks.\"\"\"\n    with pytest.raises(UnsupportedCompressionMethodError):\n        with testdata_path.joinpath(\"lzma2bcj2_2.7z\").open(mode=\"rb\") as target:\n            archive = py7zr.SevenZipFile(target)\n            archive.extractall(path=tmp_path)\n            archive.close()\n\n\n@pytest.mark.files\ndef test_extract_lzma_1(tmp_path):\n    with testdata_path.joinpath(\"lzma_1.7z\").open(mode=\"rb\") as target:\n        with py7zr.SevenZipFile(target) as ar:\n            ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_lzma2_1(tmp_path):\n    with testdata_path.joinpath(\"lzma2_1.7z\").open(mode=\"rb\") as target:\n        with py7zr.SevenZipFile(target) as ar:\n            _dict = ar.readall()\n\n\n@pytest.mark.files\ndef test_zerosize(tmp_path):\n    with testdata_path.joinpath(\"zerosize.7z\").open(mode=\"rb\") as target:\n        archive = py7zr.SevenZipFile(target)\n        archive.extractall(path=tmp_path)\n        archive.close()\n\n\n@pytest.mark.files\ndef test_zerosize_mem():\n    with testdata_path.joinpath(\"zerosize.7z\").open(mode=\"rb\") as target:\n        archive = py7zr.SevenZipFile(target)\n        _dict = archive.readall()\n        archive.close()\n\n\n@pytest.mark.api\ndef test_register_unpack_archive(tmp_path):\n    shutil.register_unpack_format(\"7zip\", [\".7z\"], unpack_7zarchive)\n    shutil.unpack_archive(str(testdata_path.joinpath(\"test_1.7z\")), str(tmp_path))\n    target = tmp_path.joinpath(\"setup.cfg\")\n    expected_mode = 33188\n    expected_mtime = 1552522033\n    if os.name == \"posix\":\n        assert target.stat().st_mode == expected_mode\n    assert target.stat().st_mtime == expected_mtime\n    m = hashlib.sha256()\n    m.update(target.open(\"rb\").read())\n    assert m.digest() == binascii.unhexlify(\"ff77878e070c4ba52732b0c847b5a055a7c454731939c3217db4a7fb4a1e7240\")\n    m = hashlib.sha256()\n    m.update(tmp_path.joinpath(\"setup.py\").open(\"rb\").read())\n    assert m.digest() == binascii.unhexlify(\"b916eed2a4ee4e48c51a2b51d07d450de0be4dbb83d20e67f6fd166ff7921e49\")\n    m = hashlib.sha256()\n    m.update(tmp_path.joinpath(\"scripts/py7zr\").open(\"rb\").read())\n    assert m.digest() == binascii.unhexlify(\"b0385e71d6a07eb692f5fb9798e9d33aaf87be7dfff936fd2473eab2a593d4fd\")\n\n\n@pytest.mark.files\ndef test_skip():\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"test_1.7z\").open(mode=\"rb\"))\n    for i, cf in enumerate(archive.files):\n        assert cf is not None\n        archive.worker.register_filelike(cf.id, None)\n    archive.worker.extract(archive.fp, parallel=True)\n    archive.close()\n\n\n@pytest.mark.files\ndef test_github_14_multi(tmp_path):\n    \"\"\"multiple unnamed objects.\"\"\"\n    archive = py7zr.SevenZipFile(str(testdata_path.joinpath(\"github_14_multi.7z\")), \"r\")\n    assert archive.getnames() == [\"github_14_multi\", \"github_14_multi\"]\n    archive.extractall(path=tmp_path)\n    with tmp_path.joinpath(\"github_14_multi\").open(\"rb\") as f:\n        assert f.read() == bytes(\"Hello GitHub issue #14 1/2.\\n\", \"ascii\")\n    with tmp_path.joinpath(\"github_14_multi_0\").open(\"rb\") as f:\n        assert f.read() == bytes(\"Hello GitHub issue #14 2/2.\\n\", \"ascii\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_github_14_multi_mem():\n    \"\"\"multiple unnamed objects.\"\"\"\n    archive = py7zr.SevenZipFile(str(testdata_path.joinpath(\"github_14_multi.7z\")), \"r\")\n    assert archive.getnames() == [\"github_14_multi\", \"github_14_multi\"]\n    _dict = archive.readall()\n    actual_1 = _dict[\"github_14_multi\"].read()\n    assert actual_1 == bytes(\"Hello GitHub issue #14 1/2.\\n\", \"ascii\")\n    actual_2 = _dict[\"github_14_multi_0\"].read()\n    assert actual_2 == bytes(\"Hello GitHub issue #14 2/2.\\n\", \"ascii\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_multiblock(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"mblock_1.7z\").open(mode=\"rb\"))\n    archive.extractall(path=tmp_path)\n    m = hashlib.sha256()\n    m.update(tmp_path.joinpath(\"bin/7zdec.exe\").open(\"rb\").read())\n    assert m.digest() == binascii.unhexlify(\"e14d8201c5c0d1049e717a63898a3b1c7ce4054a24871daebaa717da64dcaff5\")\n    archive.close()\n\n\n@pytest.mark.files\ndef test_multiblock_mem():\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"mblock_1.7z\").open(mode=\"rb\"))\n    _dict = archive.readall()\n    m = hashlib.sha256()\n    m.update(_dict[\"bin/7zdec.exe\"].read())\n    assert m.digest() == binascii.unhexlify(\"e14d8201c5c0d1049e717a63898a3b1c7ce4054a24871daebaa717da64dcaff5\")\n    archive.close()\n\n\n@pytest.mark.files\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"Cannot unlink opened file on Windows\")\ndef test_multiblock_unlink(tmp_path):\n    \"\"\"When passing opened file object, even after unlink it should work.\"\"\"\n    shutil.copy(str(testdata_path.joinpath(\"mblock_1.7z\")), str(tmp_path))\n    src = tmp_path.joinpath(\"mblock_1.7z\")\n    archive = py7zr.SevenZipFile(open(str(src), \"rb\"))\n    os.unlink(str(src))\n    archive.extractall(path=tmp_path)\n    archive.close()\n\n\n@pytest.mark.files\ndef test_copy(tmp_path):\n    \"\"\"test loading of copy compressed files.(help wanted)\"\"\"\n    check_archive(\n        py7zr.SevenZipFile(testdata_path.joinpath(\"copy.7z\").open(mode=\"rb\")),\n        tmp_path,\n        False,\n    )\n\n\n@pytest.mark.files\ndef test_copy_2(tmp_path):\n    \"\"\"test loading of copy compressed files part2.\"\"\"\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"copy_2.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(path=tmp_path)\n\n\n@pytest.mark.files\ndef test_close_unlink(tmp_path):\n    shutil.copyfile(str(testdata_path.joinpath(\"test_1.7z\")), str(tmp_path.joinpath(\"test_1.7z\")))\n    archive = py7zr.SevenZipFile(str(tmp_path.joinpath(\"test_1.7z\")))\n    archive.extractall(path=str(tmp_path))\n    archive.close()\n    tmp_path.joinpath(\"test_1.7z\").unlink()\n\n\n@pytest.mark.files\n@pytest.mark.asyncio\n@pytest.mark.skipif(hasattr(sys, \"pypy_version_info\"), reason=\"Not working with pypy3\")\ndef test_asyncio_executor(tmp_path):\n    shutil.copyfile(os.path.join(testdata_path, \"test_1.7z\"), str(tmp_path.joinpath(\"test_1.7z\")))\n    loop = asyncio.get_event_loop()\n    task = asyncio.ensure_future(aio7zr(tmp_path.joinpath(\"test_1.7z\"), path=tmp_path))\n    loop.run_until_complete(task)\n    loop.run_until_complete(asyncio.sleep(3))\n    os.unlink(str(tmp_path.joinpath(\"test_1.7z\")))\n\n\n@pytest.mark.files\ndef test_no_main_streams(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"test_folder.7z\").open(mode=\"rb\"))\n    archive.extractall(path=tmp_path)\n    archive.close()\n\n\n@pytest.mark.files\ndef test_no_main_streams_mem():\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"test_folder.7z\").open(mode=\"rb\"))\n    _dict = archive.readall()\n    archive.close()\n\n\n@pytest.mark.files\n@pytest.mark.skipif(\n    sys.platform.startswith(\"win\") and (ctypes.windll.shell32.IsUserAnAdmin() == 0),\n    reason=\"Administrator rights is required to make symlink on windows\",\n)\ndef test_extract_symlink_with_relative_target_path(tmp_path):\n    archive = py7zr.SevenZipFile(testdata_path.joinpath(\"symlink.7z\").open(mode=\"rb\"))\n    os.chdir(str(tmp_path))\n    os.makedirs(str(tmp_path.joinpath(\"target\")))  # py35 need str() against pathlib.Path\n    archive.extractall(path=\"target\")\n    assert os.readlink(str(tmp_path.joinpath(\"target/lib/libabc.so.1.2\"))) == \"libabc.so.1.2.3\"\n    archive.close()\n\n\n@pytest.mark.files\n@pytest.mark.skipif(\n    sys.platform.startswith(\"win\") and (ctypes.windll.shell32.IsUserAnAdmin() == 0),\n    reason=\"Administrator rights is required to make symlink on windows\",\n)\ndef test_extract_emptystream_mix(tmp_path):\n    archive = py7zr.SevenZipFile(str(testdata_path.joinpath(\"test_6.7z\")), \"r\")\n    archive.extractall(path=tmp_path)\n    archive.close()\n\n\n@pytest.mark.files\ndef test_extract_longpath_file(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"longpath.7z\").open(\"rb\")) as archive:\n        archive.extractall(path=tmp_path)\n\n\n@pytest.mark.files\n@pytest.mark.skipif(\n    sys.platform.startswith(\"win\") and (ctypes.windll.shell32.IsUserAnAdmin() == 0),\n    reason=\"Administrator rights is required to make symlink on windows\",\n)\ndef test_extract_symlink_overwrite(tmp_path):\n    os.chdir(str(tmp_path))\n    os.makedirs(str(tmp_path.joinpath(\"target\")))  # py35 need str() against pathlib.Path\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"symlink.7z\").open(mode=\"rb\")) as archive:\n        archive.extractall(path=\"target\")\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"symlink.7z\").open(mode=\"rb\")) as archive:\n        archive.extractall(path=\"target\")\n    assert os.readlink(str(tmp_path.joinpath(\"target/lib/libabc.so.1.2\"))) == \"libabc.so.1.2.3\"\n\n\n@pytest.mark.files\ndef test_py7zr_extract_corrupted(tmp_path):\n    with pytest.raises(CrcError):\n        archive = py7zr.SevenZipFile(str(testdata_path.joinpath(\"crc_corrupted.7z\")), \"r\")\n        archive.extract(path=tmp_path)\n        archive.close()\n\n\n@pytest.mark.files\ndef test_extract_lzma2delta(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma2delta_1.7z\").open(\"rb\")) as archive:\n        archive.extractall(path=tmp_path)\n\n\n@pytest.mark.skipif(not shutil.which(\"7z\"), reason=\"no 7z command installed\")\ndef test_decompress_small_files(tmp_path):\n    tmp_path.joinpath(\"t\").mkdir()\n    with tmp_path.joinpath(\"t/a\").open(\"w\") as f:\n        f.write(\"1\")\n    with tmp_path.joinpath(\"t/b\").open(\"w\") as f:\n        f.write(\"2\")\n    result = subprocess.run(\n        [\"7z\", \"a\", (tmp_path / \"target.7z\").as_posix(), (tmp_path / \"t\").as_posix()],\n        stdout=subprocess.PIPE,\n    )\n    if result.returncode != 0:\n        print(result.stdout)\n        pytest.fail(\"7z command report error\")\n    #\n    with py7zr.SevenZipFile(tmp_path / \"target.7z\", \"r\") as arc:\n        arc.testzip()\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_x86(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_x86.7z\").open(mode=\"rb\")) as ar:\n        _dict = ar.readall()\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_arm(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_arm.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_armt(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_armt.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_ppc(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_ppc.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_sparc(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_sparc.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_lzma_bcj_2(tmp_path):\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"lzma_bcj_2.7z\").open(mode=\"rb\")) as ar:\n        ar.extractall(tmp_path)\n\n\n@pytest.mark.files\ndef test_extract_hidden_linux_folder(tmp_path):\n    hidden_folder_name = \".hidden_folder\"\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"hidden_linux_folder.7z\").open(mode=\"rb\")) as archive:\n        assert sorted(archive.getnames()) == [\n            hidden_folder_name,\n        ]\n        archive.extractall(path=tmp_path)\n        assert tmp_path.joinpath(hidden_folder_name).exists()\n\n\n@pytest.mark.files\ndef test_extract_hidden_linux_file(tmp_path):\n    hidden_file_name = \".hidden_file.txt\"\n    with py7zr.SevenZipFile(testdata_path.joinpath(\"hidden_linux_file.7z\").open(mode=\"rb\")) as archive:\n        assert sorted(archive.getnames()) == [\n            hidden_file_name,\n        ]\n        archive.extractall(path=tmp_path)\n        assert tmp_path.joinpath(hidden_file_name).exists()\n", "patch": "@@ -332,7 +332,7 @@ def test_skip():\n     for i, cf in enumerate(archive.files):\n         assert cf is not None\n         archive.worker.register_filelike(cf.id, None)\n-    archive.worker.extract(archive.fp, parallel=True)\n+    archive.worker.extract(archive.fp, None, parallel=True)\n     archive.close()\n \n ", "file_path": "files/2022_12/1698", "file_language": "py", "file_name": "tests/test_extract.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/miurahr/py7zr/raw/1bb43f17515c7f69673a1c88ab9cc72a7bbef406/tests%2Ftest_zipslip.py", "code": "import os\n\nimport pytest\n\nfrom py7zr import SevenZipFile\nfrom py7zr.exceptions import Bad7zFile\nfrom py7zr.helpers import check_archive_path, get_sanitized_output_path\nfrom py7zr.properties import FILTER_LZMA2, PRESET_DEFAULT\n\ntestdata_path = os.path.join(os.path.dirname(__file__), \"data\")\n\n\n@pytest.mark.misc\ndef test_check_archive_path():\n    bad_path = \"../../.../../../../../../tmp/evil.sh\"\n    assert not check_archive_path(bad_path)\n\n\n@pytest.mark.misc\ndef test_get_sanitized_output_path_1(tmp_path):\n    bad_path = \"../../.../../../../../../tmp/evil.sh\"\n    with pytest.raises(Bad7zFile):\n        get_sanitized_output_path(bad_path, tmp_path)\n\n\n@pytest.mark.misc\ndef test_get_sanitized_output_path_2(tmp_path):\n    good_path = \"good.sh\"\n    expected = tmp_path.joinpath(good_path)\n    assert expected == get_sanitized_output_path(good_path, tmp_path)\n\n\n@pytest.mark.misc\ndef test_extract_path_traversal_attack(tmp_path):\n    my_filters = [\n        {\"id\": FILTER_LZMA2, \"preset\": PRESET_DEFAULT},\n    ]\n    target = tmp_path.joinpath(\"target.7z\")\n    good_data = b\"#!/bin/sh\\necho good\\n\"\n    good_path = \"good.sh\"\n    bad_data = b\"!#/bin/sh\\necho bad\\n\"\n    bad_path = \"../../.../../../../../../tmp/evil.sh\"\n    with SevenZipFile(target, \"w\", filters=my_filters) as archive:\n        archive.writestr(good_data, good_path)\n        archive._writestr(bad_data, bad_path)  # bypass a path check\n    with pytest.raises(Bad7zFile):\n        with SevenZipFile(target, \"r\") as archive:\n            archive.extractall(path=tmp_path)\n", "code_before": "", "patch": "@@ -0,0 +1,48 @@\n+import os\n+\n+import pytest\n+\n+from py7zr import SevenZipFile\n+from py7zr.exceptions import Bad7zFile\n+from py7zr.helpers import check_archive_path, get_sanitized_output_path\n+from py7zr.properties import FILTER_LZMA2, PRESET_DEFAULT\n+\n+testdata_path = os.path.join(os.path.dirname(__file__), \"data\")\n+\n+\n+@pytest.mark.misc\n+def test_check_archive_path():\n+    bad_path = \"../../.../../../../../../tmp/evil.sh\"\n+    assert not check_archive_path(bad_path)\n+\n+\n+@pytest.mark.misc\n+def test_get_sanitized_output_path_1(tmp_path):\n+    bad_path = \"../../.../../../../../../tmp/evil.sh\"\n+    with pytest.raises(Bad7zFile):\n+        get_sanitized_output_path(bad_path, tmp_path)\n+\n+\n+@pytest.mark.misc\n+def test_get_sanitized_output_path_2(tmp_path):\n+    good_path = \"good.sh\"\n+    expected = tmp_path.joinpath(good_path)\n+    assert expected == get_sanitized_output_path(good_path, tmp_path)\n+\n+\n+@pytest.mark.misc\n+def test_extract_path_traversal_attack(tmp_path):\n+    my_filters = [\n+        {\"id\": FILTER_LZMA2, \"preset\": PRESET_DEFAULT},\n+    ]\n+    target = tmp_path.joinpath(\"target.7z\")\n+    good_data = b\"#!/bin/sh\\necho good\\n\"\n+    good_path = \"good.sh\"\n+    bad_data = b\"!#/bin/sh\\necho bad\\n\"\n+    bad_path = \"../../.../../../../../../tmp/evil.sh\"\n+    with SevenZipFile(target, \"w\", filters=my_filters) as archive:\n+        archive.writestr(good_data, good_path)\n+        archive._writestr(bad_data, bad_path)  # bypass a path check\n+    with pytest.raises(Bad7zFile):\n+        with SevenZipFile(target, \"r\") as archive:\n+            archive.extractall(path=tmp_path)", "file_path": "files/2022_12/1699", "file_language": "py", "file_name": "tests/test_zipslip.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
