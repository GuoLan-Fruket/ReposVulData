{"index": 6592, "cve_id": "CVE-2021-41131", "cwe_id": ["CWE-22"], "cve_language": "Python", "cve_description": "python-tuf is a Python reference implementation of The Update Framework (TUF). In both clients (`tuf/client` and `tuf/ngclient`), there is a path traversal vulnerability that in the worst case can overwrite files ending in `.json` anywhere on the client system on a call to `get_one_valid_targetinfo()`. It occurs because the rolename is used to form the filename, and may contain path traversal characters (ie `../../name.json`). The impact is mitigated by a few facts: It only affects implementations that allow arbitrary rolename selection for delegated targets metadata, The attack requires the ability to A) insert new metadata for the path-traversing role and B) get the role delegated by an existing targets metadata, The written file content is heavily restricted since it needs to be a valid, signed targets file. The file extension is always .json. A fix is available in version 0.19 or newer. There are no workarounds that do not require code changes. Clients can restrict the allowed character set for rolenames, or they can store metadata in files named in a way that is not vulnerable: neither of these approaches is possible without modifying python-tuf.", "cvss": "8.7", "publish_date": "October 19, 2021", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "NONE", "S": "CHANGED", "C": "NONE", "I": "HIGH", "A": "HIGH", "commit_id": "4ad7ae48fda594b640139c3b7eae21ed5155a102", "commit_message": "Merge pull request from GHSA-wjw6-2cqr-j4qr\n\nFix client issue with rolenames as filenames", "commit_date": "2021-10-19T13:21:29Z", "project": "theupdateframework/python-tuf", "url": "https://api.github.com/repos/theupdateframework/python-tuf/commits/4ad7ae48fda594b640139c3b7eae21ed5155a102", "html_url": "https://github.com/theupdateframework/python-tuf/commit/4ad7ae48fda594b640139c3b7eae21ed5155a102", "windows_before": [{"commit_id": "86d4b35a98384ea34012c0c4a2a9131f1343c528", "commit_date": "Thu Oct 14 11:12:05 2021 +0200", "commit_message": "Add missing method args docs in metadata API", "files_name": ["tuf/api/metadata.py"]}, {"commit_id": "677377899e38094e9e1a175a31b6736f00c32d7a", "commit_date": "Fri Oct 1 13:16:35 2021 +0300", "commit_message": "tests: Use spec version from Metadata API", "files_name": ["tests/test_updater_with_simulator.py"]}, {"commit_id": "7da1f1e41bbb793a3cd9ccf4a7cf6788097f21c0", "commit_date": "Fri Oct 1 12:27:31 2021 +0300", "commit_message": "legacy client: Remove dead code", "files_name": ["tests/test_updater.py", "tuf/client/updater.py"]}, {"commit_id": "98e97e31d9083df3325985e48242366f332f669c", "commit_date": "Fri Oct 1 12:24:06 2021 +0300", "commit_message": "legacy client: Do local filename encoding in all places", "files_name": ["tests/test_updater.py", "tuf/client/updater.py"]}, {"commit_id": "f569754f5ee9e1523f0d338882cb716eff767cef", "commit_date": "Wed Sep 29 10:07:45 2021 +0300", "commit_message": "tests: Fix a bug in RepoSimulator signer lookup", "files_name": ["tests/repository_simulator.py"]}, {"commit_id": "b2b2f21f99300d972e4712b015fcd5e496467f7a", "commit_date": "Thu Sep 23 10:04:40 2021 +0300", "commit_message": "tests: Make sure legacy client copes with unusual rolenames", "files_name": ["tests/repository_data/fishy_rolenames/1.a.json", "tests/repository_data/fishy_rolenames/metadata/1...json", "tests/repository_data/fishy_rolenames/metadata/1.root.json", "tests/repository_data/fishy_rolenames/metadata/1.targets.json", "\"tests/repository_data/fishy_rolenames/metadata/1.\\303\\266.json\"", "tests/repository_data/fishy_rolenames/metadata/2.snapshot.json", "tests/repository_data/fishy_rolenames/metadata/timestamp.json", "tests/test_updater.py"]}, {"commit_id": "81e5862be9eebd1c966912aa39188ab0a4a91765", "commit_date": "Thu Sep 23 10:00:22 2021 +0300", "commit_message": "legacy client: Encode rolenames when using as filenames", "files_name": ["tuf/client/updater.py"]}, {"commit_id": "1846e28ca358867cc481207cd1b20e9cc934f46a", "commit_date": "Wed Sep 22 13:04:33 2021 +0300", "commit_message": "tests: Test ngclient with unusual rolenames", "files_name": ["tests/repository_simulator.py", "tests/test_updater_with_simulator.py"]}, {"commit_id": "94ed456b05b3eb42e3ea9f7562645458f799f8bb", "commit_date": "Wed Sep 22 12:58:14 2021 +0300", "commit_message": "ngclient: Encode rolenames when using as filenames", "files_name": ["tuf/ngclient/updater.py"]}, {"commit_id": "a0cb100cd86c00e495debc253bab1eff27a25cd7", "commit_date": "Wed Sep 22 12:52:36 2021 +0300", "commit_message": "ngclient: Do not use urljoin to form metadata URL", "files_name": ["tuf/ngclient/updater.py"]}, {"commit_id": "4d8cbc70109c131b74b4a64f83eacf54f131b5d5", "commit_date": "Wed Oct 13 15:44:07 2021 +0300", "commit_message": "Merge pull request #1605 from MVrachev/snapshot-hashes-length-check", "files_name": ["1351184028ed0ccc83798e3ee87432356e5d89da - Wed Oct 13 10:05:27 2021 +0000 : build(deps): bump idna from 3.2 to 3.3", "requirements-pinned.txt"]}, {"commit_id": "717eef9bb5c255bdb3aec8edd995594c712a6ae5", "commit_date": "Fri Oct 8 16:46:52 2021 +0300", "commit_message": "Repo simulator: make delegates() to all_targets()", "files_name": ["tests/repository_simulator.py"]}, {"commit_id": "f1f76d259fd5c928165b6940ec8519c8cfebd333", "commit_date": "Thu Oct 7 17:29:22 2021 +0300", "commit_message": "Repository_simulator: add a flag to compute hashes", "files_name": ["tests/repository_simulator.py", "tests/test_updater_with_simulator.py"]}, {"commit_id": "a30425c20def3c0a737b7a52b87b8190cd88e6b9", "commit_date": "Tue Oct 5 17:53:52 2021 +0300", "commit_message": "Introduce the idea of trusted/untrusted snapshot", "files_name": ["tests/test_updater_with_simulator.py", "tuf/ngclient/_internal/trusted_metadata_set.py", "tuf/ngclient/updater.py"]}, {"commit_id": "cb94504ba59d23ca5fe3b5c4d1dd20a2823b64cc", "commit_date": "Tue Oct 12 15:43:13 2021 +0300", "commit_message": "Merge pull request #1596 from theupdateframework/dependabot/pip/cryptography-35.0.0", "files_name": ["88245f15dd9dd17379e277693ff3c967fc888525 - Tue Oct 12 12:52:56 2021 +0300 : Merge pull request #1591 from MVrachev/consistent-targets", "5cdc7dc61629f476b065263ed03aa3428e87f709 - Mon Oct 11 11:40:59 2021 +0300 : Merge pull request #1608 from sechkova/sleep-before-round", "a5096a88b19ec2a295fc233bc62a2f956e04e163 - Thu Oct 7 15:13:45 2021 +0300 : ngclient: remove sleep_before_round", "tuf/ngclient/_internal/requests_fetcher.py"]}, {"commit_id": "c3e746a096c35adbbb3f5db0bc46449a81cd44a2", "commit_date": "Mon Oct 4 18:15:57 2021 +0300", "commit_message": "Tests: assert that test_targets use hash prefixes", "files_name": ["tests/test_updater_with_simulator.py"]}, {"commit_id": "0d73220dff06deec3a91c8361200c420a3494478", "commit_date": "Wed Sep 29 17:43:44 2021 +0300", "commit_message": "Use decorator in test_updater_with_simulator", "files_name": ["tests/test_updater_with_simulator.py"]}, {"commit_id": "9989d3c614f29432d727655cf13402ebd7d4045e", "commit_date": "Mon Oct 4 17:17:25 2021 +0300", "commit_message": "Tests: move decorator in utils so it can be reused", "files_name": ["tests/test_metadata_serialization.py", "tests/utils.py"]}, {"commit_id": "76cf36eb86ca8623792ef9d8255bd9ab4a7e4cad", "commit_date": "Fri Sep 24 15:13:10 2021 +0300", "commit_message": "Handle consistent targets same as legacy updater", "files_name": ["tests/test_updater_with_simulator.py", "tuf/ngclient/updater.py"]}, {"commit_id": "d0213318905604a9ffd882dd90f22d4375cb03fe", "commit_date": "Fri Oct 1 15:13:07 2021 +0300", "commit_message": "Merge pull request #1590 from jku/docs-tweaking", "files_name": ["4e835855df0cdae68a208f8c45e0edc125b264d3 - Fri Oct 1 14:42:54 2021 +0300 : ngclient: Fix docs link to specification", "tuf/ngclient/config.py"]}, {"commit_id": "adc65e55327c209fd067ed36ac380acdab6a1b9f", "commit_date": "Thu Sep 30 10:07:41 2021 +0000", "commit_message": "build(deps): bump cryptography from 3.4.8 to 35.0.0", "files_name": ["requirements-pinned.txt"]}, {"commit_id": "bb1ed9f5d8c3fe92dc62cd8473ec73e1ee31bc18", "commit_date": "Thu Sep 30 10:30:28 2021 +0300", "commit_message": "Merge pull request #1589 from theupdateframework/dependabot/pip/urllib3-1.26.7", "files_name": ["bd84dcf2d3778f0729ec88ee7e2de91d3235c91a - Thu Sep 30 10:28:41 2021 +0300 : Merge pull request #1592 from jku/encode-target-filename", "eeebc416389ed1c37fb640980a01096f62499cc3 - Thu Sep 9 18:34:36 2021 +0300 : ngclient: Don't use target path as local path", "tests/test_updater_with_simulator.py", "tuf/ngclient/updater.py"]}, {"commit_id": "82b09679cc3af44e929181b7763170034ae58a7d", "commit_date": "Mon Sep 27 10:22:54 2021 +0300", "commit_message": "Merge pull request #1588 from sechkova/session-timeout", "files_name": ["e6787ae0e291430a6be59b0d38899965af93b80c - Mon Sep 27 10:21:42 2021 +0300 : Merge pull request #1587 from jku/tests-repo-sim-targets-support", "59b0b99ba30346a779de4bfc63a95614df8abbe0 - Fri Sep 24 11:45:48 2021 +0300 : tests: Improve the docs on RepositorySimulator", "tests/repository_simulator.py"]}, {"commit_id": "1fb4204c267c1b84c9232767b871aa7bb17ab6f3", "commit_date": "Fri Sep 10 15:54:56 2021 +0300", "commit_message": "tests: Add target support to RepositorySimulator", "files_name": ["tests/repository_simulator.py", "tests/test_updater_with_simulator.py"]}, {"commit_id": "8ed446c14bf34044b0b73008faf4fe7d5619bdb4", "commit_date": "Thu Sep 23 17:57:19 2021 +0300", "commit_message": "Metadata API: Stop annotating __init__() return value", "files_name": ["tuf/api/metadata.py", "tuf/api/serialization/json.py"]}, {"commit_id": "ed520ee55d77de4e0a87d6ad4e877c2ef0953ff3", "commit_date": "Thu Sep 23 17:19:05 2021 +0300", "commit_message": "Metadata API: Improve serialization docs", "files_name": ["tuf/api/serialization/__init__.py", "tuf/api/serialization/json.py"]}, {"commit_id": "a77c0831e7eff75fee2e334d6b9db5cc4bcf5b8e", "commit_date": "Thu Sep 23 17:06:17 2021 +0300", "commit_message": "docs: rename \"helpers\" to \"supporting classes\"", "files_name": ["docs/api/tuf.api.metadata.helpers.rst", "docs/api/tuf.api.metadata.supporting.rst", "docs/api/tuf.api.rst", "docs/conf.py"]}, {"commit_id": "892aa04cb395b5d6051551d408da46c8bee0e1a4", "commit_date": "Thu Sep 23 15:25:47 2021 +0300", "commit_message": "Metadata API: Rewrite module doc", "files_name": ["tuf/api/metadata.py"]}, {"commit_id": "424cc3282bec0a41fb6f84f8381e89adaefcf341", "commit_date": "Thu Sep 23 14:06:23 2021 +0300", "commit_message": "docs: Include the Serialization interfaces", "files_name": ["docs/api/tuf.api.serialization.rst"]}, {"commit_id": "d5743c2312ff726369582735094c2da91e7e3a3e", "commit_date": "Thu Sep 23 13:48:21 2021 +0300", "commit_message": "metadata API docs: remove duplication", "files_name": ["tuf/api/metadata.py"]}, {"commit_id": "21ce5e2915e94446c9f281f77549b327e3f67b3c", "commit_date": "Thu Sep 23 13:08:49 2021 +0300", "commit_message": "Hide to_dict()/from_dict()", "files_name": ["docs/conf.py"]}, {"commit_id": "217bd9dbc7969789a573afd5ebe9fdef667ca579", "commit_date": "Thu Sep 23 13:03:28 2021 +0300", "commit_message": "Separate API documentation pages", "files_name": ["docs/api/tuf.api.metadata.helpers.rst", "docs/api/tuf.api.metadata.metadata.rst", "docs/api/tuf.api.metadata.root.rst", "docs/api/tuf.api.metadata.rst", "docs/api/tuf.api.metadata.snapshot.rst", "docs/api/tuf.api.metadata.targets.rst", "docs/api/tuf.api.metadata.timestamp.rst", "docs/api/tuf.api.rst", "docs/api/tuf.api.serialization.rst", "docs/api/tuf.ngclient.config.rst", "docs/api/tuf.ngclient.fetcher.rst", "docs/api/tuf.ngclient.updater.rst", "docs/conf.py"]}, {"commit_id": "868afda420309014ff5d9f584ce4dbaba4a6d2f9", "commit_date": "Fri Sep 17 19:02:43 2021 +0300", "commit_message": "docs: Change object attribute doc style", "files_name": ["docs/api/tuf.api.metadata.rst", "docs/api/tuf.ngclient.config.rst", "docs/api/tuf.ngclient.fetcher.rst", "docs/api/tuf.ngclient.updater.rst", "docs/conf.py", "tuf/api/metadata.py", "tuf/ngclient/updater.py"]}, {"commit_id": "9133788f04ca552990efe0fe1eaf9a0df2507985", "commit_date": "Thu Sep 23 10:05:46 2021 +0000", "commit_message": "build(deps): bump urllib3 from 1.26.6 to 1.26.7", "files_name": ["requirements-pinned.txt"]}, {"commit_id": "cc1f95e789e1e5032d0d22e1dea198d5f72d42f6", "commit_date": "Wed Sep 22 11:12:52 2021 +0300", "commit_message": "Merge pull request #1537 from MVrachev/role-name-uniqueness", "files_name": ["7b61ad853885086aba756ccbb467d490247323d5 - Fri Sep 17 18:08:32 2021 +0300 : ngclient: use mock instead of slow_retrieval_server", "tests/test_fetcher_ng.py"]}, {"commit_id": "119693ff4056a51dbfc1af3d2b7bc6f72d2bf00f", "commit_date": "Fri Sep 17 18:07:58 2021 +0300", "commit_message": "ngclient: handle timeout on session.get", "files_name": ["tests/test_fetcher_ng.py", "tuf/ngclient/_internal/requests_fetcher.py"]}, {"commit_id": "f00295f1471d53c5529f99b5914a2ee3dad19d54", "commit_date": "Thu Sep 16 15:17:31 2021 +0300", "commit_message": "API CHANGE: ValueError in add/remove key in Root", "files_name": ["tests/test_api.py", "tuf/api/metadata.py"]}, {"commit_id": "e27070305f73649892887bff62d79954bdc98af7", "commit_date": "Tue Aug 24 17:39:34 2021 +0300", "commit_message": "Metadata API: Add key helpers in Targets", "files_name": ["tests/test_api.py", "tuf/api/metadata.py"]}, {"commit_id": "1a5912aa7c8216e4aa0b458c455fd1fa4208ed7e", "commit_date": "Tue Aug 24 17:50:55 2021 +0300", "commit_message": "Remove some unused imports", "files_name": ["tests/slow_retrieval_server.py", "tests/test_indefinite_freeze_attack.py", "tests/test_metadata_serialization.py", "tests/test_trusted_metadata_set.py", "tests/test_tutorial.py", "tests/test_updater.py"]}, {"commit_id": "510078b5427a44f4139e589ae693d9f49b3b5957", "commit_date": "Tue Aug 24 14:48:21 2021 +0300", "commit_message": "Move tests to test_metadata_serialization", "files_name": ["tests/test_api.py", "tests/test_metadata_serialization.py"]}, {"commit_id": "f8620c1992dd2179c29f09b5da00bb5cb746978a", "commit_date": "Tue Aug 24 14:00:09 2021 +0300", "commit_message": "API CHANGE: enforce role name uniqueness", "files_name": ["tests/test_metadata_serialization.py", "tuf/api/metadata.py", "tuf/ngclient/updater.py"]}, {"commit_id": "afc67d967b115a7ef10c946292a471257ae9c44f", "commit_date": "Tue Sep 21 11:59:44 2021 +0300", "commit_message": "Merge pull request #1586 from theupdateframework/dependabot/pip/charset-normalizer-2.0.6", "files_name": ["67c52987a612fdcb9fc517504113df527cb6865f - Mon Sep 20 14:39:55 2021 +0300 : Merge pull request #1446 from MVrachev/snapshot-property", "bf12e7565f3df91e83468ad5ff16818608984398 - Mon Jun 14 14:13:15 2021 +0300 : Metadata API: change meta type in Timestamp", "tests/repository_simulator.py", "tests/test_api.py", "tests/test_trusted_metadata_set.py", "tuf/api/metadata.py", "tuf/ngclient/_internal/trusted_metadata_set.py", "tuf/ngclient/updater.py"]}, {"commit_id": "ba28b5a560ca76cd0a14582eefb76ef04c683e91", "commit_date": "Mon Sep 20 10:26:34 2021 +0000", "commit_message": "build(deps): bump charset-normalizer from 2.0.5 to 2.0.6", "files_name": ["requirements-pinned.txt"]}], "windows_after": [{"commit_id": "e86a61571c332643831a56d1c2910f2475f08998", "commit_date": "Tue Oct 19 17:21:41 2021 +0300", "commit_message": "Merge pull request #1625 from jku/release-0.19", "files_name": ["d19d40353d06e43881a4110a5451b1540a56fd17 - Wed Oct 20 11:23:33 2021 +0300 : Merge pull request #1616 from theupdateframework/dependabot/pip/idna-3.3", "761349919b87b3e23fd6f6f2a537768146ec0b12 - Wed Oct 20 11:23:45 2021 +0300 : Merge pull request #1621 from theupdateframework/dependabot/pip/cffi-1.15.0", "b642a44ce19cdd987f60f3120e40fc5a546ab0a6 - Wed Oct 20 08:25:45 2021 +0000 : build(deps): bump certifi from 2021.5.30 to 2021.10.8", "requirements-pinned.txt"]}, {"commit_id": "6839e81edcd036d256013a6fab10c15dd41368f2", "commit_date": "Wed Oct 20 11:39:43 2021 +0300", "commit_message": "Merge pull request #1609 from theupdateframework/dependabot/pip/certifi-2021.10.8", "files_name": ["9864e8ef5e0f7cf46fdc8eacf0a73a12d6f5d4bd - Wed Oct 20 08:40:11 2021 +0000 : build(deps): bump charset-normalizer from 2.0.6 to 2.0.7", "requirements-pinned.txt"]}, {"commit_id": "4aef2b2ae60c609ee0f1357eb271490c6428ad60", "commit_date": "Wed Oct 20 12:04:39 2021 +0300", "commit_message": "Merge pull request #1611 from theupdateframework/dependabot/pip/charset-normalizer-2.0.7", "files_name": ["0cf6ba2258adbc02f2dce3eecbede38011a438c5 - Wed Oct 20 15:51:07 2021 +0200 : Merge pull request #1620 from lukpueh/misc-metadata-api-docs", "4c81340610f3264f4f94af99e8cf142dbaf546a4 - Wed Oct 20 18:59:27 2021 +0300 : Replace depricated ssl function and fix CI errors", "tests/simple_https_server.py"]}, {"commit_id": "6ff852ad0fd3c061e4790c17dfc8f16bc69f913c", "commit_date": "Mon Oct 11 19:56:52 2021 +0300", "commit_message": "Add support for python 3.10", "files_name": [".github/workflows/ci.yml", "setup.py", "tox.ini"]}, {"commit_id": "2e94e392754f64a3fc14601cdd0c8c61d5dc088b", "commit_date": "Wed Oct 20 19:55:43 2021 +0300", "commit_message": "Use quotes for python version for github workflows", "files_name": [".github/workflows/ci.yml"]}, {"commit_id": "69eb29fc80cd186b665692079b5aaf00872af558", "commit_date": "Thu Oct 21 14:40:43 2021 +0300", "commit_message": "Merge pull request #1628 from MVrachev/add-python3.10", "files_name": ["9e113d613577aa4f9cabc94bff00f53bb7daa94e - Tue Oct 19 21:06:15 2021 +0100 : docs/RELEASE: use build for building dists", "docs/RELEASE.md"]}, {"commit_id": "2ed51167f62edd01b104640a8f2f23195a677e24", "commit_date": "Tue Oct 19 21:10:55 2021 +0100", "commit_message": "build: add build and release deps to requirements-dev", "files_name": ["requirements-dev.txt"]}, {"commit_id": "2ab518b3f082535c603b567b4e3a5413ebe45de0", "commit_date": "Tue Oct 19 21:12:16 2021 +0100", "commit_message": "build: cleanup setup.cfg", "files_name": ["setup.cfg"]}, {"commit_id": "08decea2d004ff091d99a076409aebb0ff532141", "commit_date": "Tue Oct 19 21:37:34 2021 +0100", "commit_message": "Remove unused .gitmodules directory", "files_name": [".gitmodules"]}, {"commit_id": "4fd35434bd211e99200aa6904fe95d9038174640", "commit_date": "Tue Oct 19 21:37:50 2021 +0100", "commit_message": "build: update MANIFEST.in to match sdist", "files_name": ["MANIFEST.in"]}, {"commit_id": "36242adc748c11e71adff8395c0a1678296bb373", "commit_date": "Tue Oct 19 21:40:07 2021 +0100", "commit_message": "build: recommend using build, not setup.py", "files_name": ["setup.py"]}, {"commit_id": "d3e34acd9887e024c7c1eff593828f52b12d68dd", "commit_date": "Tue Oct 19 21:42:01 2021 +0100", "commit_message": "build: add docs to project_urls", "files_name": ["setup.py"]}, {"commit_id": "38ea974674af754ee73664ce58dec8cffeda3ad0", "commit_date": "Tue Oct 19 22:11:02 2021 +0100", "commit_message": "build: convert to static setuptools metadata", "files_name": ["setup.cfg", "setup.py"]}, {"commit_id": "ac0ea24ca96ccfb2cbf30c40157eec0b0dd20636", "commit_date": "Thu Oct 21 10:53:35 2021 +0100", "commit_message": "Remove references to setup.py", "files_name": ["docs/RELEASE.md", "tuf/__init__.py"]}, {"commit_id": "52ad17a710375291ba598a5087ceda9459977891", "commit_date": "Tue Oct 19 22:11:59 2021 +0100", "commit_message": "build: update check-manifest options", "files_name": ["setup.cfg"]}, {"commit_id": "ada35c9e8fcc36874bff21ed931f023399db5d38", "commit_date": "Tue Oct 19 22:16:01 2021 +0100", "commit_message": "build: add pyproject.toml to list build tools", "files_name": ["pyproject.toml"]}, {"commit_id": "352f6c2c56c835e78b5869fc1c6bf22b39ca2a39", "commit_date": "Tue Oct 19 22:20:21 2021 +0100", "commit_message": "Add vscode project directory to gitignore", "files_name": [".gitignore"]}, {"commit_id": "de1a3af0198d95d933bd3c49cef790fd71823fdb", "commit_date": "Wed Oct 20 10:49:47 2021 +0100", "commit_message": "build: more intentional about what's included in sdist", "files_name": ["MANIFEST.in", "setup.cfg"]}, {"commit_id": "6d18d53ec003967702e8a3100f2cd67a2291fc2e", "commit_date": "Thu Oct 21 20:01:55 2021 +0100", "commit_message": "Merge pull request #1626 from joshuagl/joshuagl/build", "files_name": ["d3d2f57f3aec2f67c8c4ab5da974e8cd02b032a6 - Sat Oct 23 18:39:22 2021 +0300 : Test files: bump expiration date and resign", "tests/repository_data/fishy_rolenames/1.a.json", "tests/repository_data/fishy_rolenames/metadata/1...json", "tests/repository_data/fishy_rolenames/metadata/1.root.json", "tests/repository_data/fishy_rolenames/metadata/1.targets.json", "\"tests/repository_data/fishy_rolenames/metadata/1.\\303\\266.json\"", "tests/repository_data/fishy_rolenames/metadata/2.snapshot.json", "tests/repository_data/fishy_rolenames/metadata/timestamp.json"]}, {"commit_id": "2206fc917e46ff584dcae75e640e1144c8b9f078", "commit_date": "Mon Oct 25 11:02:34 2021 +0300", "commit_message": "Merge pull request #1631 from MVrachev/fix-test-error", "files_name": ["59b5498918a4f5ff5a838d3ed4deb8f3f484f58a - Mon Oct 25 12:45:08 2021 +0300 : Add TOP_LEVEL_ROLE_NAMES constant", "tuf/api/metadata.py"]}, {"commit_id": "9bc55ee568691f5ff4ec76de87d75851236f492d", "commit_date": "Wed Oct 20 15:29:02 2021 +0300", "commit_message": "Metadata API: validate root role names", "files_name": ["tests/repository_simulator.py", "tests/test_metadata_serialization.py", "tuf/api/metadata.py"]}, {"commit_id": "4158272a7adbcc458563e8b57ced29b7859c3077", "commit_date": "Mon Oct 25 13:20:13 2021 +0300", "commit_message": "Use TOP_LEVEL_ROLE_NAMES across TUF", "files_name": ["tests/repository_simulator.py", "tuf/api/metadata.py"]}, {"commit_id": "cc9f3876c49726d5226f8e33c5b0eae9a3691740", "commit_date": "Thu Sep 9 22:14:21 2021 +0300", "commit_message": "tests: Shorten variable names to reasonable length", "files_name": ["tests/test_updater_ng.py"]}, {"commit_id": "9b761b86208918a95b6641a8cad8ec7c267b3e84", "commit_date": "Thu Sep 9 21:21:57 2021 +0300", "commit_message": "ngclient: Simplify caching", "files_name": ["tests/test_updater_ng.py", "tests/test_updater_with_simulator.py", "tuf/ngclient/updater.py"]}, {"commit_id": "d519a413b0a9e7ada2ecd63658a17bfe423373e9", "commit_date": "Mon Sep 13 10:12:48 2021 +0300", "commit_message": "ngclient: Rename get_one_valid_targetinfo()", "files_name": ["tests/test_updater_ng.py", "tests/test_updater_with_simulator.py", "tuf/ngclient/updater.py"]}, {"commit_id": "6aaa1ead5974e91ba868a4fd6f483b82540f4bae", "commit_date": "Wed Oct 27 10:19:00 2021 +0300", "commit_message": "ngclient: Refactor target path generation", "files_name": ["tuf/ngclient/updater.py"]}, {"commit_id": "1d115b57b6a1712e4c622cfa6cd9afb9f5fa721b", "commit_date": "Wed Oct 27 18:39:58 2021 +0300", "commit_message": "Merge pull request #1630 from MVrachev/validate-role", "files_name": ["7b8ff220b1e93f0f6997f168dfffa90a405c3688 - Wed Oct 27 18:40:49 2021 +0300 : Merge pull request #1604 from jku/ngclient-api-polish", "fd40dfc094ca33f00f153fd4ae44bf5f77441d5a - Fri Oct 8 10:30:31 2021 +0300 : tests: Refactor simulator signer handling", "tests/repository_simulator.py", "tests/test_updater_with_simulator.py"]}, {"commit_id": "ad80bd96c6b1c5bad8fa844389b242d8cb678472", "commit_date": "Fri Oct 8 22:52:46 2021 +0300", "commit_message": "tests: Mark RepositorySimulator. create_key() static", "files_name": ["tests/repository_simulator.py"]}, {"commit_id": "e817473e3c8fdde0d1c8017ac29c2dac628b5d02", "commit_date": "Fri Oct 8 13:15:52 2021 +0300", "commit_message": "tests: Add root key rotation tests", "files_name": ["tests/test_updater_key_rotations.py"]}, {"commit_id": "589ed9e0d48aad9acaea912f409e1445d5913416", "commit_date": "Wed Oct 27 19:07:06 2021 +0300", "commit_message": "Merge pull request #1635 from jku/key-rotation-tests", "files_name": ["15e84dfb2ef89f3130f1f36f144ca4bc42d2f577 - Mon Nov 1 09:47:50 2021 +0200 : GH actions: limit GitHub token visibility", ".github/workflows/ci.yml"]}, {"commit_id": "c98b429643eac97471fa6f998c7b29eeece8c05f", "commit_date": "Tue Oct 12 16:14:24 2021 +0300", "commit_message": "Apply black on the tests of the new code", "files_name": ["tests/repository_simulator.py", "tests/test_api.py", "tests/test_fetcher.py", "tests/test_fetcher_ng.py", "tests/test_metadata_serialization.py", "tests/test_trusted_metadata_set.py", "tests/test_updater_key_rotations.py", "tests/test_updater_ng.py", "tests/test_updater_with_simulator.py"]}, {"commit_id": "2e9ef7976248466765796634635152c4765a7041", "commit_date": "Tue Oct 12 16:54:10 2021 +0300", "commit_message": "Apply isort on the tests of the new code", "files_name": ["tests/repository_simulator.py", "tests/test_api.py", "tests/test_fetcher.py", "tests/test_fetcher_ng.py", "tests/test_metadata_serialization.py", "tests/test_trusted_metadata_set.py", "tests/test_updater_key_rotations.py", "tests/test_updater_ng.py", "tests/test_updater_with_simulator.py"]}, {"commit_id": "6fe36a00c47f351ce15f9670182eb2f3b3f40921", "commit_date": "Tue Oct 12 16:46:52 2021 +0300", "commit_message": "Rename & simplify a couple of tests in test_api.py", "files_name": ["tests/test_api.py"]}, {"commit_id": "a7766ac53aeb2581b0f8c5e62e1c2aae7d29d7c7", "commit_date": "Tue Nov 2 19:45:18 2021 +0200", "commit_message": "pylintc for new code: disable format checker", "files_name": ["tuf/api/pylintrc"]}, {"commit_id": "ec74499fc396b3df52ca1e80d1402ea8863d2714", "commit_date": "Wed Sep 15 19:25:44 2021 +0300", "commit_message": "ngclient: Implicitly call refresh()", "files_name": ["tuf/ngclient/updater.py"]}, {"commit_id": "26213ae576fc34567d3093c77d54ec9ac4942797", "commit_date": "Wed Nov 3 14:51:02 2021 +0200", "commit_message": "tests: Add test for implicit refresh()", "files_name": ["tests/test_updater_ng.py"]}, {"commit_id": "ce4a60eb2677ea11aa12058870bbdf8b8926c77c", "commit_date": "Thu Nov 4 09:32:56 2021 +0200", "commit_message": "Merge pull request #1658 from MVrachev/apply-linters", "files_name": ["e073fea819b271da2677e83ba00dc4ec81764027 - Thu Nov 4 11:39:05 2021 +0200 : github: explicitly set workflow permissions", ".github/workflows/ci.yml"]}, {"commit_id": "d70c3b32f2b85a76659965d333d0aafce3fc306f", "commit_date": "Thu Nov 4 10:52:45 2021 +0100", "commit_message": "Merge pull request #1663 from jku/gh-actions-set-permissions", "files_name": ["b137fbcc3fc3084f709a2228e93d1bb4efee6675 - Fri Nov 5 15:07:39 2021 +0200 : plyintrc: remove redundant format section", "tuf/api/pylintrc"]}, {"commit_id": "8ae944ccb256ba0aa13a62f691fd79d4e16c11ad", "commit_date": "Fri Nov 5 16:19:22 2021 +0200", "commit_message": "Merge pull request #1659 from MVrachev/disable-pylint-format", "files_name": ["beb8087bf5ea43a53ede6214798a6df4f232342e - Thu Oct 28 12:16:32 2021 +0300 : Clarify key rotations test cases by using keywords", "tests/test_updater_key_rotations.py"]}, {"commit_id": "954331c8af75e1d192e0be293621bfd043ebd6fd", "commit_date": "Thu Oct 14 15:14:17 2021 +0300", "commit_message": "ngtests: Add top-level-roles update tests", "files_name": ["tests/test_updater_top_level_update.py"]}, {"commit_id": "29da5da4bbe2f1626785bce5320079b6670d7e5e", "commit_date": "Mon Nov 8 19:30:49 2021 +0200", "commit_message": "Metadata API: make root roles Mapping", "files_name": ["tuf/api/metadata.py"]}, {"commit_id": "8a2c7857ac94fffea519ac0bb2d599844a33abde", "commit_date": "Fri Oct 29 14:56:57 2021 +0300", "commit_message": "ngtests: Add addtional asserts for files on disk", "files_name": ["tests/test_updater_top_level_update.py"]}, {"commit_id": "e51642a290b29474d136b24f65b6ae4819f2b32c", "commit_date": "Fri Oct 29 17:10:17 2021 +0300", "commit_message": "ngtests: Fix formatiing and linter issues", "files_name": ["tests/test_updater_top_level_update.py"]}, {"commit_id": "8418d5267fef4030b52ddbd026081951f772e617", "commit_date": "Tue Nov 9 13:59:44 2021 +0200", "commit_message": "ngtests: Add asserts for expected version", "files_name": ["tests/test_updater_top_level_update.py"]}, {"commit_id": "d66c3baf2753523c600ee7f6aca6371e671d0c12", "commit_date": "Tue Nov 9 14:04:54 2021 +0200", "commit_message": "RepoSim: remove metadata version check", "files_name": ["tests/repository_simulator.py", "tests/test_updater_top_level_update.py"]}, {"commit_id": "0088ebd4449e1f6602b26cf76fe1a92befca8765", "commit_date": "Wed Nov 10 10:09:15 2021 +0200", "commit_message": "Merge pull request #1636 from sechkova/ng-tests-metadata-update", "files_name": ["3c80c5bcd0ddc96a25c3a1d228e95265d222b453 - Wed Nov 10 15:27:03 2021 +0200 : Tests: self.assertRaises -> with self.assertRaises", "tests/test_api.py", "tests/test_utils.py"]}, {"commit_id": "8c6e1575195bd7cbd11b179f942f6cdd0c2c7c84", "commit_date": "Wed Nov 10 15:51:16 2021 +0200", "commit_message": "Tests: test_api split test_sign_verify()", "files_name": ["tests/test_api.py"]}], "parents": [{"commit_id_before": "4d8cbc70109c131b74b4a64f83eacf54f131b5d5", "url_before": "https://api.github.com/repos/theupdateframework/python-tuf/commits/4d8cbc70109c131b74b4a64f83eacf54f131b5d5", "html_url_before": "https://github.com/theupdateframework/python-tuf/commit/4d8cbc70109c131b74b4a64f83eacf54f131b5d5"}, {"commit_id_before": "677377899e38094e9e1a175a31b6736f00c32d7a", "url_before": "https://api.github.com/repos/theupdateframework/python-tuf/commits/677377899e38094e9e1a175a31b6736f00c32d7a", "html_url_before": "https://github.com/theupdateframework/python-tuf/commit/677377899e38094e9e1a175a31b6736f00c32d7a"}], "details": [{"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tests%2Frepository_data%2Ffishy_rolenames%2F1.a.json", "code": "{\n \"signatures\": [\n  {\n   \"keyid\": \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\",\n   \"sig\": \"6550a087bd0f01648f57e02a275f20c8e38974271d73739c446f53a028c4118e070b1d37224bc022ab6e0500c8051494f276365868ed6039ec49c7ecd8b9f602\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"targets\",\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"spec_version\": \"1.0.19\",\n  \"targets\": {},\n  \"version\": 1\n }\n}", "code_before": "{\n \"signatures\": [\n  {\n   \"keyid\": \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\",\n   \"sig\": \"6550a087bd0f01648f57e02a275f20c8e38974271d73739c446f53a028c4118e070b1d37224bc022ab6e0500c8051494f276365868ed6039ec49c7ecd8b9f602\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"targets\",\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"spec_version\": \"1.0.19\",\n  \"targets\": {},\n  \"version\": 1\n }\n}", "patch": "@@ -0,0 +1,15 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\",\n+   \"sig\": \"6550a087bd0f01648f57e02a275f20c8e38974271d73739c446f53a028c4118e070b1d37224bc022ab6e0500c8051494f276365868ed6039ec49c7ecd8b9f602\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"targets\",\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"spec_version\": \"1.0.19\",\n+  \"targets\": {},\n+  \"version\": 1\n+ }\n+}\n\\ No newline at end of file", "file_path": "files/2021_10/344", "file_language": "json", "file_name": "tests/repository_data/fishy_rolenames/1.a.json", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tests%2Frepository_data%2Ffishy_rolenames%2Fmetadata%2F1...json", "code": "{\n \"signatures\": [\n  {\n   \"keyid\": \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\",\n   \"sig\": \"c0266de0724c2ab9c14e679b258033fe3aff8ce3c99419479456170975bb43de9e8539caed437cccc8e6c6068252a921f7badc5384149dab18261a7f157ae406\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"targets\",\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"spec_version\": \"1.0.19\",\n  \"targets\": {},\n  \"version\": 1\n }\n}", "code_before": "{\n \"signatures\": [\n  {\n   \"keyid\": \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\",\n   \"sig\": \"c0266de0724c2ab9c14e679b258033fe3aff8ce3c99419479456170975bb43de9e8539caed437cccc8e6c6068252a921f7badc5384149dab18261a7f157ae406\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"targets\",\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"spec_version\": \"1.0.19\",\n  \"targets\": {},\n  \"version\": 1\n }\n}", "patch": "@@ -0,0 +1,15 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\",\n+   \"sig\": \"c0266de0724c2ab9c14e679b258033fe3aff8ce3c99419479456170975bb43de9e8539caed437cccc8e6c6068252a921f7badc5384149dab18261a7f157ae406\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"targets\",\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"spec_version\": \"1.0.19\",\n+  \"targets\": {},\n+  \"version\": 1\n+ }\n+}\n\\ No newline at end of file", "file_path": "files/2021_10/345", "file_language": "json", "file_name": "tests/repository_data/fishy_rolenames/metadata/1...json", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tests%2Frepository_data%2Ffishy_rolenames%2Fmetadata%2F1.root.json", "code": "{\n \"signatures\": [\n  {\n   \"keyid\": \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\",\n   \"sig\": \"1d3b9cebfdab388db500d01cb2cd499f016320029df17bf2f1196d8f83f12d041832dc165f23667e537d8a8aa66c716d19835bd2bcd55d4c18bbbd0c6eaf4b06\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"root\",\n  \"consistent_snapshot\": true,\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"keys\": {\n   \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\": {\n    \"keytype\": \"ed25519\",\n    \"keyval\": {\n     \"public\": \"d98dace51d795525971342b9f7317cea0d743710dca932543fedb92bb083c2c0\"\n    },\n    \"scheme\": \"ed25519\"\n   },\n   \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\": {\n    \"keytype\": \"ed25519\",\n    \"keyval\": {\n     \"public\": \"46d386175220afd55ad9b09b6b18fa96cd69e25bc29c97ed7024a522e7e7938c\"\n    },\n    \"scheme\": \"ed25519\"\n   },\n   \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\": {\n    \"keytype\": \"ed25519\",\n    \"keyval\": {\n     \"public\": \"a7beb72fb686a645f5ffd52e246a55d2914411853c70a5b47d837ed7b4c40734\"\n    },\n    \"scheme\": \"ed25519\"\n   },\n   \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\": {\n    \"keytype\": \"ed25519\",\n    \"keyval\": {\n     \"public\": \"efdf10805063c1b7356f40ede43d2c5c6d2d11d79e350887ce96fe5d1e44901a\"\n    },\n    \"scheme\": \"ed25519\"\n   }\n  },\n  \"roles\": {\n   \"root\": {\n    \"keyids\": [\n     \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\"\n    ],\n    \"threshold\": 1\n   },\n   \"snapshot\": {\n    \"keyids\": [\n     \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\"\n    ],\n    \"threshold\": 1\n   },\n   \"targets\": {\n    \"keyids\": [\n     \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\"\n    ],\n    \"threshold\": 1\n   },\n   \"timestamp\": {\n    \"keyids\": [\n     \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\"\n    ],\n    \"threshold\": 1\n   }\n  },\n  \"spec_version\": \"1.0.19\",\n  \"version\": 1\n }\n}", "code_before": "{\n \"signatures\": [\n  {\n   \"keyid\": \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\",\n   \"sig\": \"1d3b9cebfdab388db500d01cb2cd499f016320029df17bf2f1196d8f83f12d041832dc165f23667e537d8a8aa66c716d19835bd2bcd55d4c18bbbd0c6eaf4b06\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"root\",\n  \"consistent_snapshot\": true,\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"keys\": {\n   \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\": {\n    \"keytype\": \"ed25519\",\n    \"keyval\": {\n     \"public\": \"d98dace51d795525971342b9f7317cea0d743710dca932543fedb92bb083c2c0\"\n    },\n    \"scheme\": \"ed25519\"\n   },\n   \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\": {\n    \"keytype\": \"ed25519\",\n    \"keyval\": {\n     \"public\": \"46d386175220afd55ad9b09b6b18fa96cd69e25bc29c97ed7024a522e7e7938c\"\n    },\n    \"scheme\": \"ed25519\"\n   },\n   \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\": {\n    \"keytype\": \"ed25519\",\n    \"keyval\": {\n     \"public\": \"a7beb72fb686a645f5ffd52e246a55d2914411853c70a5b47d837ed7b4c40734\"\n    },\n    \"scheme\": \"ed25519\"\n   },\n   \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\": {\n    \"keytype\": \"ed25519\",\n    \"keyval\": {\n     \"public\": \"efdf10805063c1b7356f40ede43d2c5c6d2d11d79e350887ce96fe5d1e44901a\"\n    },\n    \"scheme\": \"ed25519\"\n   }\n  },\n  \"roles\": {\n   \"root\": {\n    \"keyids\": [\n     \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\"\n    ],\n    \"threshold\": 1\n   },\n   \"snapshot\": {\n    \"keyids\": [\n     \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\"\n    ],\n    \"threshold\": 1\n   },\n   \"targets\": {\n    \"keyids\": [\n     \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\"\n    ],\n    \"threshold\": 1\n   },\n   \"timestamp\": {\n    \"keyids\": [\n     \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\"\n    ],\n    \"threshold\": 1\n   }\n  },\n  \"spec_version\": \"1.0.19\",\n  \"version\": 1\n }\n}", "patch": "@@ -0,0 +1,71 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\",\n+   \"sig\": \"1d3b9cebfdab388db500d01cb2cd499f016320029df17bf2f1196d8f83f12d041832dc165f23667e537d8a8aa66c716d19835bd2bcd55d4c18bbbd0c6eaf4b06\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"root\",\n+  \"consistent_snapshot\": true,\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"keys\": {\n+   \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\": {\n+    \"keytype\": \"ed25519\",\n+    \"keyval\": {\n+     \"public\": \"d98dace51d795525971342b9f7317cea0d743710dca932543fedb92bb083c2c0\"\n+    },\n+    \"scheme\": \"ed25519\"\n+   },\n+   \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\": {\n+    \"keytype\": \"ed25519\",\n+    \"keyval\": {\n+     \"public\": \"46d386175220afd55ad9b09b6b18fa96cd69e25bc29c97ed7024a522e7e7938c\"\n+    },\n+    \"scheme\": \"ed25519\"\n+   },\n+   \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\": {\n+    \"keytype\": \"ed25519\",\n+    \"keyval\": {\n+     \"public\": \"a7beb72fb686a645f5ffd52e246a55d2914411853c70a5b47d837ed7b4c40734\"\n+    },\n+    \"scheme\": \"ed25519\"\n+   },\n+   \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\": {\n+    \"keytype\": \"ed25519\",\n+    \"keyval\": {\n+     \"public\": \"efdf10805063c1b7356f40ede43d2c5c6d2d11d79e350887ce96fe5d1e44901a\"\n+    },\n+    \"scheme\": \"ed25519\"\n+   }\n+  },\n+  \"roles\": {\n+   \"root\": {\n+    \"keyids\": [\n+     \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\"\n+    ],\n+    \"threshold\": 1\n+   },\n+   \"snapshot\": {\n+    \"keyids\": [\n+     \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\"\n+    ],\n+    \"threshold\": 1\n+   },\n+   \"targets\": {\n+    \"keyids\": [\n+     \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\"\n+    ],\n+    \"threshold\": 1\n+   },\n+   \"timestamp\": {\n+    \"keyids\": [\n+     \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\"\n+    ],\n+    \"threshold\": 1\n+   }\n+  },\n+  \"spec_version\": \"1.0.19\",\n+  \"version\": 1\n+ }\n+}\n\\ No newline at end of file", "file_path": "files/2021_10/346", "file_language": "json", "file_name": "tests/repository_data/fishy_rolenames/metadata/1.root.json", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tests%2Frepository_data%2Ffishy_rolenames%2Fmetadata%2F1.targets.json", "code": "{\n \"signatures\": [\n  {\n   \"keyid\": \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\",\n   \"sig\": \"ffa055ab5108f9d22f309fecd0160b02971d7a454c8d48db4f99cdaf114b329a401b756a11e42630bff6667ad897fb05f501e3299d25fe786d12651cb0db6c06\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"targets\",\n  \"delegations\": {\n   \"keys\": {\n    \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\": {\n     \"keytype\": \"ed25519\",\n     \"keyval\": {\n      \"public\": \"45d4d9ee28ef61506695130fe600d637e5f2de0de72473c280b02b89467d7aab\"\n     },\n     \"scheme\": \"ed25519\"\n    },\n    \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\": {\n     \"keytype\": \"ed25519\",\n     \"keyval\": {\n      \"public\": \"abe021d7594f04467627c2be390c665b311dceb83cceb685edc9b90a6e229d08\"\n     },\n     \"scheme\": \"ed25519\"\n    },\n    \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\": {\n     \"keytype\": \"ed25519\",\n     \"keyval\": {\n      \"public\": \"52b790190bccf730fad4b769e7073c1551938101483ff8612534eb9105426dce\"\n     },\n     \"scheme\": \"ed25519\"\n    }\n   },\n   \"roles\": [\n    {\n     \"keyids\": [\n      \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\"\n     ],\n     \"name\": \"../a\",\n     \"paths\": [\n      \"*\"\n     ],\n     \"terminating\": false,\n     \"threshold\": 1\n    },\n    {\n     \"keyids\": [\n      \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\"\n     ],\n     \"name\": \".\",\n     \"paths\": [\n      \"*\"\n     ],\n     \"terminating\": false,\n     \"threshold\": 1\n    },\n    {\n     \"keyids\": [\n      \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\"\n     ],\n     \"name\": \"\\u00f6\",\n     \"paths\": [\n      \"*\"\n     ],\n     \"terminating\": false,\n     \"threshold\": 1\n    }\n   ]\n  },\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"spec_version\": \"1.0.19\",\n  \"targets\": {},\n  \"version\": 1\n }\n}", "code_before": "{\n \"signatures\": [\n  {\n   \"keyid\": \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\",\n   \"sig\": \"ffa055ab5108f9d22f309fecd0160b02971d7a454c8d48db4f99cdaf114b329a401b756a11e42630bff6667ad897fb05f501e3299d25fe786d12651cb0db6c06\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"targets\",\n  \"delegations\": {\n   \"keys\": {\n    \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\": {\n     \"keytype\": \"ed25519\",\n     \"keyval\": {\n      \"public\": \"45d4d9ee28ef61506695130fe600d637e5f2de0de72473c280b02b89467d7aab\"\n     },\n     \"scheme\": \"ed25519\"\n    },\n    \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\": {\n     \"keytype\": \"ed25519\",\n     \"keyval\": {\n      \"public\": \"abe021d7594f04467627c2be390c665b311dceb83cceb685edc9b90a6e229d08\"\n     },\n     \"scheme\": \"ed25519\"\n    },\n    \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\": {\n     \"keytype\": \"ed25519\",\n     \"keyval\": {\n      \"public\": \"52b790190bccf730fad4b769e7073c1551938101483ff8612534eb9105426dce\"\n     },\n     \"scheme\": \"ed25519\"\n    }\n   },\n   \"roles\": [\n    {\n     \"keyids\": [\n      \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\"\n     ],\n     \"name\": \"../a\",\n     \"paths\": [\n      \"*\"\n     ],\n     \"terminating\": false,\n     \"threshold\": 1\n    },\n    {\n     \"keyids\": [\n      \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\"\n     ],\n     \"name\": \".\",\n     \"paths\": [\n      \"*\"\n     ],\n     \"terminating\": false,\n     \"threshold\": 1\n    },\n    {\n     \"keyids\": [\n      \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\"\n     ],\n     \"name\": \"\\u00f6\",\n     \"paths\": [\n      \"*\"\n     ],\n     \"terminating\": false,\n     \"threshold\": 1\n    }\n   ]\n  },\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"spec_version\": \"1.0.19\",\n  \"targets\": {},\n  \"version\": 1\n }\n}", "patch": "@@ -0,0 +1,75 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\",\n+   \"sig\": \"ffa055ab5108f9d22f309fecd0160b02971d7a454c8d48db4f99cdaf114b329a401b756a11e42630bff6667ad897fb05f501e3299d25fe786d12651cb0db6c06\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"targets\",\n+  \"delegations\": {\n+   \"keys\": {\n+    \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\": {\n+     \"keytype\": \"ed25519\",\n+     \"keyval\": {\n+      \"public\": \"45d4d9ee28ef61506695130fe600d637e5f2de0de72473c280b02b89467d7aab\"\n+     },\n+     \"scheme\": \"ed25519\"\n+    },\n+    \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\": {\n+     \"keytype\": \"ed25519\",\n+     \"keyval\": {\n+      \"public\": \"abe021d7594f04467627c2be390c665b311dceb83cceb685edc9b90a6e229d08\"\n+     },\n+     \"scheme\": \"ed25519\"\n+    },\n+    \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\": {\n+     \"keytype\": \"ed25519\",\n+     \"keyval\": {\n+      \"public\": \"52b790190bccf730fad4b769e7073c1551938101483ff8612534eb9105426dce\"\n+     },\n+     \"scheme\": \"ed25519\"\n+    }\n+   },\n+   \"roles\": [\n+    {\n+     \"keyids\": [\n+      \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\"\n+     ],\n+     \"name\": \"../a\",\n+     \"paths\": [\n+      \"*\"\n+     ],\n+     \"terminating\": false,\n+     \"threshold\": 1\n+    },\n+    {\n+     \"keyids\": [\n+      \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\"\n+     ],\n+     \"name\": \".\",\n+     \"paths\": [\n+      \"*\"\n+     ],\n+     \"terminating\": false,\n+     \"threshold\": 1\n+    },\n+    {\n+     \"keyids\": [\n+      \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\"\n+     ],\n+     \"name\": \"\\u00f6\",\n+     \"paths\": [\n+      \"*\"\n+     ],\n+     \"terminating\": false,\n+     \"threshold\": 1\n+    }\n+   ]\n+  },\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"spec_version\": \"1.0.19\",\n+  \"targets\": {},\n+  \"version\": 1\n+ }\n+}\n\\ No newline at end of file", "file_path": "files/2021_10/347", "file_language": "json", "file_name": "tests/repository_data/fishy_rolenames/metadata/1.targets.json", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tests%2Frepository_data%2Ffishy_rolenames%2Fmetadata%2F1.%C3%B6.json", "code": "{\n \"signatures\": [\n  {\n   \"keyid\": \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\",\n   \"sig\": \"854fdccea623c33bf968c7ef5abea6e5e5f7c390a691ae0ae5ad87a7580fc00910b566d5dbdbfcaa948f2d8fe4348eecd5a12710d05f576aecf83fbec32c580b\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"targets\",\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"spec_version\": \"1.0.19\",\n  \"targets\": {},\n  \"version\": 1\n }\n}", "code_before": "{\n \"signatures\": [\n  {\n   \"keyid\": \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\",\n   \"sig\": \"854fdccea623c33bf968c7ef5abea6e5e5f7c390a691ae0ae5ad87a7580fc00910b566d5dbdbfcaa948f2d8fe4348eecd5a12710d05f576aecf83fbec32c580b\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"targets\",\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"spec_version\": \"1.0.19\",\n  \"targets\": {},\n  \"version\": 1\n }\n}", "patch": "@@ -0,0 +1,15 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\",\n+   \"sig\": \"854fdccea623c33bf968c7ef5abea6e5e5f7c390a691ae0ae5ad87a7580fc00910b566d5dbdbfcaa948f2d8fe4348eecd5a12710d05f576aecf83fbec32c580b\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"targets\",\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"spec_version\": \"1.0.19\",\n+  \"targets\": {},\n+  \"version\": 1\n+ }\n+}\n\\ No newline at end of file", "file_path": "files/2021_10/348", "file_language": "json", "file_name": "tests/repository_data/fishy_rolenames/metadata/1.\u00f6.json", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tests%2Frepository_data%2Ffishy_rolenames%2Fmetadata%2F2.snapshot.json", "code": "{\n \"signatures\": [\n  {\n   \"keyid\": \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\",\n   \"sig\": \"f00f4b0040dc6879e7ad69867ba611d52bd5e9993cbfd27e6d8073449356c716b4277093c67ae70eba90ab0367a070e69be750284e70e1135615832efda54008\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"snapshot\",\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"meta\": {\n   \"../a.json\": {\n    \"version\": 1\n   },\n   \"..json\": {\n    \"version\": 1\n   },\n   \"targets.json\": {\n    \"version\": 1\n   },\n   \"\\u00f6.json\": {\n    \"version\": 1\n   }\n  },\n  \"spec_version\": \"1.0.19\",\n  \"version\": 2\n }\n}", "code_before": "{\n \"signatures\": [\n  {\n   \"keyid\": \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\",\n   \"sig\": \"f00f4b0040dc6879e7ad69867ba611d52bd5e9993cbfd27e6d8073449356c716b4277093c67ae70eba90ab0367a070e69be750284e70e1135615832efda54008\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"snapshot\",\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"meta\": {\n   \"../a.json\": {\n    \"version\": 1\n   },\n   \"..json\": {\n    \"version\": 1\n   },\n   \"targets.json\": {\n    \"version\": 1\n   },\n   \"\\u00f6.json\": {\n    \"version\": 1\n   }\n  },\n  \"spec_version\": \"1.0.19\",\n  \"version\": 2\n }\n}", "patch": "@@ -0,0 +1,28 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\",\n+   \"sig\": \"f00f4b0040dc6879e7ad69867ba611d52bd5e9993cbfd27e6d8073449356c716b4277093c67ae70eba90ab0367a070e69be750284e70e1135615832efda54008\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"snapshot\",\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"meta\": {\n+   \"../a.json\": {\n+    \"version\": 1\n+   },\n+   \"..json\": {\n+    \"version\": 1\n+   },\n+   \"targets.json\": {\n+    \"version\": 1\n+   },\n+   \"\\u00f6.json\": {\n+    \"version\": 1\n+   }\n+  },\n+  \"spec_version\": \"1.0.19\",\n+  \"version\": 2\n+ }\n+}\n\\ No newline at end of file", "file_path": "files/2021_10/349", "file_language": "json", "file_name": "tests/repository_data/fishy_rolenames/metadata/2.snapshot.json", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tests%2Frepository_data%2Ffishy_rolenames%2Fmetadata%2Ftimestamp.json", "code": "{\n \"signatures\": [\n  {\n   \"keyid\": \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\",\n   \"sig\": \"5a0040f56454f2f338acb8a81b4c2e170e0bc61219a7cd823f635dfc9faeefcf30dfe9c792f148a25949cc9594f8ac1bfffe436b737eff140d236eba57fe9e08\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"timestamp\",\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"meta\": {\n   \"snapshot.json\": {\n    \"version\": 2\n   }\n  },\n  \"spec_version\": \"1.0.19\",\n  \"version\": 2\n }\n}", "code_before": "{\n \"signatures\": [\n  {\n   \"keyid\": \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\",\n   \"sig\": \"5a0040f56454f2f338acb8a81b4c2e170e0bc61219a7cd823f635dfc9faeefcf30dfe9c792f148a25949cc9594f8ac1bfffe436b737eff140d236eba57fe9e08\"\n  }\n ],\n \"signed\": {\n  \"_type\": \"timestamp\",\n  \"expires\": \"2021-10-22T11:21:56Z\",\n  \"meta\": {\n   \"snapshot.json\": {\n    \"version\": 2\n   }\n  },\n  \"spec_version\": \"1.0.19\",\n  \"version\": 2\n }\n}", "patch": "@@ -0,0 +1,19 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\",\n+   \"sig\": \"5a0040f56454f2f338acb8a81b4c2e170e0bc61219a7cd823f635dfc9faeefcf30dfe9c792f148a25949cc9594f8ac1bfffe436b737eff140d236eba57fe9e08\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"timestamp\",\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"meta\": {\n+   \"snapshot.json\": {\n+    \"version\": 2\n+   }\n+  },\n+  \"spec_version\": \"1.0.19\",\n+  \"version\": 2\n+ }\n+}\n\\ No newline at end of file", "file_path": "files/2021_10/350", "file_language": "json", "file_name": "tests/repository_data/fishy_rolenames/metadata/timestamp.json", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tests%2Frepository_simulator.py", "code": "#!/usr/bin/env python\n\n# Copyright 2021, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\"Test utility to simulate a repository\n\nRepositorySimulator provides methods to modify repository metadata so that it's\neasy to \"publish\" new repository versions with modified metadata, while serving\nthe versions to client test code.\n\nRepositorySimulator implements FetcherInterface so Updaters in tests can use it\nas a way to \"download\" new metadata from remote: in practice no downloading,\nnetwork connections or even file access happens as RepositorySimulator serves\neverything from memory.\n\nMetadata and targets \"hosted\" by the simulator are made available in URL paths\n\"/metadata/...\" and \"/targets/...\" respectively.\n\nExample::\n\n    # constructor creates repository with top-level metadata\n    sim = RepositorySimulator()\n\n    # metadata can be modified directly: it is immediately available to clients\n    sim.snapshot.version += 1\n\n    # As an exception, new root versions require explicit publishing\n    sim.root.version += 1\n    sim.publish_root()\n\n    # there are helper functions\n    sim.add_target(\"targets\", b\"content\", \"targetpath\")\n    sim.targets.version += 1\n    sim.update_snapshot()\n\n    # Use the simulated repository from an Updater:\n    updater = Updater(\n        dir,\n        \"https://example.com/metadata/\",\n        \"https://example.com/targets/\",\n        sim\n    )\n    updater.refresh()\n\"\"\"\n\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport logging\nimport os\nimport tempfile\nimport securesystemslib.hash as sslib_hash\nfrom securesystemslib.keys import generate_ed25519_key\nfrom securesystemslib.signer import SSlibSigner\nfrom typing import Dict, Iterator, List, Optional, Tuple\nfrom urllib import parse\n\nfrom tuf.api.serialization.json import JSONSerializer\nfrom tuf.exceptions import FetcherHTTPError\nfrom tuf.api.metadata import (\n    DelegatedRole,\n    Delegations,\n    Key,\n    Metadata,\n    MetaFile,\n    Role,\n    Root,\n    SPECIFICATION_VERSION,\n    Snapshot,\n    TargetFile,\n    Targets,\n    Timestamp,\n)\nfrom tuf.ngclient.fetcher import FetcherInterface\n\nlogger = logging.getLogger(__name__)\n\nSPEC_VER = \".\".join(SPECIFICATION_VERSION)\n\n@dataclass\nclass RepositoryTarget:\n    \"\"\"Contains actual target data and the related target metadata\"\"\"\n    data: bytes\n    target_file: TargetFile\n\nclass RepositorySimulator(FetcherInterface):\n    def __init__(self):\n        self.md_root: Metadata[Root] = None\n        self.md_timestamp: Metadata[Timestamp] = None\n        self.md_snapshot: Metadata[Snapshot] = None\n        self.md_targets: Metadata[Targets] = None\n        self.md_delegates: Dict[str, Metadata[Targets]] = {}\n\n        # other metadata is signed on-demand (when fetched) but roots must be\n        # explicitly published with publish_root() which maintains this list\n        self.signed_roots: List[bytes] = []\n\n        # signers are used on-demand at fetch time to sign metadata\n        self.signers: Dict[str, List[SSlibSigner]] = {}\n\n        # target downloads are served from this dict\n        self.target_files: Dict[str, RepositoryTarget] = {}\n\n        # Whether to compute hashes and legth for meta in snapshot/timestamp\n        self.compute_metafile_hashes_length = False\n\n        self.dump_dir = None\n        self.dump_version = 0\n\n        now = datetime.utcnow()\n        self.safe_expiry = now.replace(microsecond=0) + timedelta(days=30)\n\n        self._initialize()\n\n    @property\n    def root(self) -> Root:\n        return self.md_root.signed\n\n    @property\n    def timestamp(self) -> Timestamp:\n        return self.md_timestamp.signed\n\n    @property\n    def snapshot(self) -> Snapshot:\n        return self.md_snapshot.signed\n\n    @property\n    def targets(self) -> Targets:\n        return self.md_targets.signed\n\n    def all_targets(self) -> Iterator[Tuple[str, Targets]]:\n        yield \"targets\", self.md_targets.signed\n        for role, md in self.md_delegates.items():\n            yield role, md.signed\n\n    def create_key(self) -> Tuple[Key, SSlibSigner]:\n        sslib_key = generate_ed25519_key()\n        return Key.from_securesystemslib_key(sslib_key), SSlibSigner(sslib_key)\n\n    def _initialize(self):\n        \"\"\"Setup a minimal valid repository\"\"\"\n\n        targets = Targets(1, SPEC_VER, self.safe_expiry, {}, None)\n        self.md_targets = Metadata(targets, OrderedDict())\n\n        meta = {\"targets.json\": MetaFile(targets.version)}\n        snapshot = Snapshot(1, SPEC_VER, self.safe_expiry, meta)\n        self.md_snapshot = Metadata(snapshot, OrderedDict())\n\n        snapshot_meta = MetaFile(snapshot.version)\n        timestamp = Timestamp(1, SPEC_VER, self.safe_expiry, snapshot_meta)\n        self.md_timestamp = Metadata(timestamp, OrderedDict())\n\n        root = Root(1, SPEC_VER, self.safe_expiry, {}, {}, True)\n        for role in [\"root\", \"timestamp\", \"snapshot\", \"targets\"]:\n            key, signer = self.create_key()\n            root.roles[role] = Role([], 1)\n            root.add_key(role, key)\n            # store the private key\n            if role not in self.signers:\n                self.signers[role] = []\n            self.signers[role].append(signer)\n        self.md_root = Metadata(root, OrderedDict())\n        self.publish_root()\n\n    def publish_root(self):\n        \"\"\"Sign and store a new serialized version of root\"\"\"\n        self.md_root.signatures.clear()\n        for signer in self.signers[\"root\"]:\n            self.md_root.sign(signer)\n\n        self.signed_roots.append(self.md_root.to_bytes(JSONSerializer()))\n        logger.debug(\"Published root v%d\", self.root.version)\n\n    def fetch(self, url: str) -> Iterator[bytes]:\n        if not self.root.consistent_snapshot:\n            raise NotImplementedError(\"non-consistent snapshot not supported\")\n        path = parse.urlparse(url).path\n        if path.startswith(\"/metadata/\") and path.endswith(\".json\"):\n            ver_and_name = path[len(\"/metadata/\") :][: -len(\".json\")]\n            # only consistent_snapshot supported ATM: timestamp is special case\n            if ver_and_name == \"timestamp\":\n                version = None\n                role = \"timestamp\"\n            else:\n                version, _, role = ver_and_name.partition(\".\")\n                version = int(version)\n            yield self._fetch_metadata(role, version)\n        elif path.startswith(\"/targets/\"):\n            # figure out target path and hash prefix\n            target_path = path[len(\"/targets/\") :]\n            dir_parts, sep , prefixed_filename = target_path.rpartition(\"/\")\n            prefix, _, filename = prefixed_filename.partition(\".\")\n            target_path = f\"{dir_parts}{sep}{filename}\"\n\n            yield self._fetch_target(target_path, prefix)\n        else:\n            raise FetcherHTTPError(f\"Unknown path '{path}'\", 404)\n\n    def _fetch_target(self, target_path: str, hash: Optional[str]) -> bytes:\n        \"\"\"Return data for 'target_path', checking 'hash' if it is given.\n\n        If hash is None, then consistent_snapshot is not used\n        \"\"\"\n        repo_target = self.target_files.get(target_path)\n        if repo_target is None:\n            raise FetcherHTTPError(f\"No target {target_path}\", 404)\n        if hash and hash not in repo_target.target_file.hashes.values():\n            raise FetcherHTTPError(f\"hash mismatch for {target_path}\", 404)\n\n        logger.debug(\"fetched target %s\", target_path)\n        return repo_target.data\n\n    def _fetch_metadata(self, role: str, version: Optional[int] = None) -> bytes:\n        \"\"\"Return signed metadata for 'role', using 'version' if it is given.\n\n        If version is None, non-versioned metadata is being requested\n        \"\"\"\n        if role == \"root\":\n            # return a version previously serialized in publish_root()\n            if version is None or version > len(self.signed_roots):\n                raise FetcherHTTPError(f\"Unknown root version {version}\", 404)\n            logger.debug(\"fetched root version %d\", role, version)\n            return self.signed_roots[version - 1]\n        else:\n            # sign and serialize the requested metadata\n            if role == \"timestamp\":\n                md: Metadata = self.md_timestamp\n            elif role == \"snapshot\":\n                md = self.md_snapshot\n            elif role == \"targets\":\n                md = self.md_targets\n            else:\n                md = self.md_delegates[role]\n\n            if md is None:\n                raise FetcherHTTPError(f\"Unknown role {role}\", 404)\n            if version is not None and version != md.signed.version:\n                raise FetcherHTTPError(f\"Unknown {role} version {version}\", 404)\n\n            md.signatures.clear()\n            for signer in self.signers[role]:\n                md.sign(signer, append=True)\n\n            logger.debug(\n                \"fetched %s v%d with %d sigs\",\n                role,\n                md.signed.version,\n                len(self.signers[role]),\n            )\n            return md.to_bytes(JSONSerializer())\n\n    def _compute_hashes_and_length(\n        self, role: str\n    ) -> Tuple[Dict[str, str], int]:\n        data = self._fetch_metadata(role)\n        digest_object = sslib_hash.digest(sslib_hash.DEFAULT_HASH_ALGORITHM)\n        digest_object.update(data)\n        hashes = {sslib_hash.DEFAULT_HASH_ALGORITHM:  digest_object.hexdigest()}\n        return hashes, len(data)\n\n    def update_timestamp(self):\n        self.timestamp.snapshot_meta.version = self.snapshot.version\n\n        if self.compute_metafile_hashes_length:\n            hashes, length = self._compute_hashes_and_length(\"snapshot\")\n            self.timestamp.snapshot_meta.hashes = hashes\n            self.timestamp.snapshot_meta.length = length\n\n        self.timestamp.version += 1\n\n    def update_snapshot(self):\n        for role, delegate in self.all_targets():\n            hashes = None\n            length = None\n            if self.compute_metafile_hashes_length:\n                hashes, length = self._compute_hashes_and_length(role)\n\n            self.snapshot.meta[f\"{role}.json\"] = MetaFile(\n                delegate.version, length, hashes\n            )\n\n        self.snapshot.version += 1\n        self.update_timestamp()\n\n    def add_target(self, role: str, data: bytes, path: str):\n        if role == \"targets\":\n            targets = self.targets\n        else:\n            targets = self.md_delegates[role].signed\n\n        target = TargetFile.from_data(path, data, [\"sha256\"])\n        targets.targets[path] = target\n        self.target_files[path] = RepositoryTarget(data, target)\n\n    def add_delegation(\n        self,\n        delegator_name: str,\n        name: str,\n        targets: Targets,\n        terminating: bool,\n        paths: Optional[List[str]],\n        hash_prefixes: Optional[List[str]],\n    ):\n        if delegator_name == \"targets\":\n            delegator = self.targets\n        else:\n            delegator = self.md_delegates[delegator_name].signed\n\n        # Create delegation\n        role = DelegatedRole(name, [], 1, terminating, paths, hash_prefixes)\n        if delegator.delegations is None:\n            delegator.delegations = Delegations({}, {})\n        # put delegation last by default\n        delegator.delegations.roles[role.name] = role\n\n        # By default add one new key for the role\n        key, signer = self.create_key()\n        delegator.add_key(role.name, key)\n        if role.name not in self.signers:\n            self.signers[role.name] = []\n        self.signers[role.name].append(signer)\n\n        # Add metadata for the role\n        self.md_delegates[role.name] = Metadata(targets, OrderedDict())\n\n    def write(self):\n        \"\"\"Dump current repository metadata to self.dump_dir\n\n        This is a debugging tool: dumping repository state before running\n        Updater refresh may be useful while debugging a test.\n        \"\"\"\n        if self.dump_dir is None:\n            self.dump_dir = tempfile.mkdtemp()\n            print(f\"Repository Simulator dumps in {self.dump_dir}\")\n\n        self.dump_version += 1\n        dir = os.path.join(self.dump_dir, str(self.dump_version))\n        os.makedirs(dir)\n\n        for ver in range(1, len(self.signed_roots) + 1):\n            with open(os.path.join(dir, f\"{ver}.root.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(\"root\", ver))\n\n        for role in [\"timestamp\", \"snapshot\", \"targets\"]:\n            with open(os.path.join(dir, f\"{role}.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(role))\n\n        for role in self.md_delegates.keys():\n            with open(os.path.join(dir, f\"{role}.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(role))\n", "code_before": "#!/usr/bin/env python\n\n# Copyright 2021, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\"Test utility to simulate a repository\n\nRepositorySimulator provides methods to modify repository metadata so that it's\neasy to \"publish\" new repository versions with modified metadata, while serving\nthe versions to client test code.\n\nRepositorySimulator implements FetcherInterface so Updaters in tests can use it\nas a way to \"download\" new metadata from remote: in practice no downloading,\nnetwork connections or even file access happens as RepositorySimulator serves\neverything from memory.\n\nMetadata and targets \"hosted\" by the simulator are made available in URL paths\n\"/metadata/...\" and \"/targets/...\" respectively.\n\nExample::\n\n    # constructor creates repository with top-level metadata\n    sim = RepositorySimulator()\n\n    # metadata can be modified directly: it is immediately available to clients\n    sim.snapshot.version += 1\n\n    # As an exception, new root versions require explicit publishing\n    sim.root.version += 1\n    sim.publish_root()\n\n    # there are helper functions\n    sim.add_target(\"targets\", b\"content\", \"targetpath\")\n    sim.targets.version += 1\n    sim.update_snapshot()\n\n    # Use the simulated repository from an Updater:\n    updater = Updater(\n        dir,\n        \"https://example.com/metadata/\",\n        \"https://example.com/targets/\",\n        sim\n    )\n    updater.refresh()\n\"\"\"\n\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport logging\nimport os\nimport tempfile\nimport securesystemslib.hash as sslib_hash\nfrom securesystemslib.keys import generate_ed25519_key\nfrom securesystemslib.signer import SSlibSigner\nfrom typing import Dict, Iterator, List, Optional, Tuple\nfrom urllib import parse\n\nfrom tuf.api.serialization.json import JSONSerializer\nfrom tuf.exceptions import FetcherHTTPError\nfrom tuf.api.metadata import (\n    DelegatedRole,\n    Delegations,\n    Key,\n    Metadata,\n    MetaFile,\n    Role,\n    Root,\n    SPECIFICATION_VERSION,\n    Snapshot,\n    TargetFile,\n    Targets,\n    Timestamp,\n)\nfrom tuf.ngclient.fetcher import FetcherInterface\n\nlogger = logging.getLogger(__name__)\n\nSPEC_VER = \".\".join(SPECIFICATION_VERSION)\n\n@dataclass\nclass RepositoryTarget:\n    \"\"\"Contains actual target data and the related target metadata\"\"\"\n    data: bytes\n    target_file: TargetFile\n\nclass RepositorySimulator(FetcherInterface):\n    def __init__(self):\n        self.md_root: Metadata[Root] = None\n        self.md_timestamp: Metadata[Timestamp] = None\n        self.md_snapshot: Metadata[Snapshot] = None\n        self.md_targets: Metadata[Targets] = None\n        self.md_delegates: Dict[str, Metadata[Targets]] = {}\n\n        # other metadata is signed on-demand (when fetched) but roots must be\n        # explicitly published with publish_root() which maintains this list\n        self.signed_roots: List[bytes] = []\n\n        # signers are used on-demand at fetch time to sign metadata\n        self.signers: Dict[str, List[SSlibSigner]] = {}\n\n        # target downloads are served from this dict\n        self.target_files: Dict[str, RepositoryTarget] = {}\n\n        # Whether to compute hashes and legth for meta in snapshot/timestamp\n        self.compute_metafile_hashes_length = False\n\n        self.dump_dir = None\n        self.dump_version = 0\n\n        now = datetime.utcnow()\n        self.safe_expiry = now.replace(microsecond=0) + timedelta(days=30)\n\n        self._initialize()\n\n    @property\n    def root(self) -> Root:\n        return self.md_root.signed\n\n    @property\n    def timestamp(self) -> Timestamp:\n        return self.md_timestamp.signed\n\n    @property\n    def snapshot(self) -> Snapshot:\n        return self.md_snapshot.signed\n\n    @property\n    def targets(self) -> Targets:\n        return self.md_targets.signed\n\n    def all_targets(self) -> Iterator[Tuple[str, Targets]]:\n        yield \"targets\", self.md_targets.signed\n        for role, md in self.md_delegates.items():\n            yield role, md.signed\n\n    def create_key(self) -> Tuple[Key, SSlibSigner]:\n        sslib_key = generate_ed25519_key()\n        return Key.from_securesystemslib_key(sslib_key), SSlibSigner(sslib_key)\n\n    def _initialize(self):\n        \"\"\"Setup a minimal valid repository\"\"\"\n\n        targets = Targets(1, SPEC_VER, self.safe_expiry, {}, None)\n        self.md_targets = Metadata(targets, OrderedDict())\n\n        meta = {\"targets.json\": MetaFile(targets.version)}\n        snapshot = Snapshot(1, SPEC_VER, self.safe_expiry, meta)\n        self.md_snapshot = Metadata(snapshot, OrderedDict())\n\n        snapshot_meta = MetaFile(snapshot.version)\n        timestamp = Timestamp(1, SPEC_VER, self.safe_expiry, snapshot_meta)\n        self.md_timestamp = Metadata(timestamp, OrderedDict())\n\n        root = Root(1, SPEC_VER, self.safe_expiry, {}, {}, True)\n        for role in [\"root\", \"timestamp\", \"snapshot\", \"targets\"]:\n            key, signer = self.create_key()\n            root.roles[role] = Role([], 1)\n            root.add_key(role, key)\n            # store the private key\n            if role not in self.signers:\n                self.signers[role] = []\n            self.signers[role].append(signer)\n        self.md_root = Metadata(root, OrderedDict())\n        self.publish_root()\n\n    def publish_root(self):\n        \"\"\"Sign and store a new serialized version of root\"\"\"\n        self.md_root.signatures.clear()\n        for signer in self.signers[\"root\"]:\n            self.md_root.sign(signer)\n\n        self.signed_roots.append(self.md_root.to_bytes(JSONSerializer()))\n        logger.debug(\"Published root v%d\", self.root.version)\n\n    def fetch(self, url: str) -> Iterator[bytes]:\n        if not self.root.consistent_snapshot:\n            raise NotImplementedError(\"non-consistent snapshot not supported\")\n        path = parse.urlparse(url).path\n        if path.startswith(\"/metadata/\") and path.endswith(\".json\"):\n            ver_and_name = path[len(\"/metadata/\") :][: -len(\".json\")]\n            # only consistent_snapshot supported ATM: timestamp is special case\n            if ver_and_name == \"timestamp\":\n                version = None\n                role = \"timestamp\"\n            else:\n                version, _, role = ver_and_name.partition(\".\")\n                version = int(version)\n            yield self._fetch_metadata(role, version)\n        elif path.startswith(\"/targets/\"):\n            # figure out target path and hash prefix\n            target_path = path[len(\"/targets/\") :]\n            dir_parts, sep , prefixed_filename = target_path.rpartition(\"/\")\n            prefix, _, filename = prefixed_filename.partition(\".\")\n            target_path = f\"{dir_parts}{sep}{filename}\"\n\n            yield self._fetch_target(target_path, prefix)\n        else:\n            raise FetcherHTTPError(f\"Unknown path '{path}'\", 404)\n\n    def _fetch_target(self, target_path: str, hash: Optional[str]) -> bytes:\n        \"\"\"Return data for 'target_path', checking 'hash' if it is given.\n\n        If hash is None, then consistent_snapshot is not used\n        \"\"\"\n        repo_target = self.target_files.get(target_path)\n        if repo_target is None:\n            raise FetcherHTTPError(f\"No target {target_path}\", 404)\n        if hash and hash not in repo_target.target_file.hashes.values():\n            raise FetcherHTTPError(f\"hash mismatch for {target_path}\", 404)\n\n        logger.debug(\"fetched target %s\", target_path)\n        return repo_target.data\n\n    def _fetch_metadata(self, role: str, version: Optional[int] = None) -> bytes:\n        \"\"\"Return signed metadata for 'role', using 'version' if it is given.\n\n        If version is None, non-versioned metadata is being requested\n        \"\"\"\n        if role == \"root\":\n            # return a version previously serialized in publish_root()\n            if version is None or version > len(self.signed_roots):\n                raise FetcherHTTPError(f\"Unknown root version {version}\", 404)\n            logger.debug(\"fetched root version %d\", role, version)\n            return self.signed_roots[version - 1]\n        else:\n            # sign and serialize the requested metadata\n            if role == \"timestamp\":\n                md: Metadata = self.md_timestamp\n            elif role == \"snapshot\":\n                md = self.md_snapshot\n            elif role == \"targets\":\n                md = self.md_targets\n            else:\n                md = self.md_delegates[role]\n\n            if md is None:\n                raise FetcherHTTPError(f\"Unknown role {role}\", 404)\n            if version is not None and version != md.signed.version:\n                raise FetcherHTTPError(f\"Unknown {role} version {version}\", 404)\n\n            md.signatures.clear()\n            for signer in self.signers[role]:\n                md.sign(signer, append=True)\n\n            logger.debug(\n                \"fetched %s v%d with %d sigs\",\n                role,\n                md.signed.version,\n                len(self.signers[role]),\n            )\n            return md.to_bytes(JSONSerializer())\n\n    def _compute_hashes_and_length(\n        self, role: str\n    ) -> Tuple[Dict[str, str], int]:\n        data = self._fetch_metadata(role)\n        digest_object = sslib_hash.digest(sslib_hash.DEFAULT_HASH_ALGORITHM)\n        digest_object.update(data)\n        hashes = {sslib_hash.DEFAULT_HASH_ALGORITHM:  digest_object.hexdigest()}\n        return hashes, len(data)\n\n    def update_timestamp(self):\n        self.timestamp.snapshot_meta.version = self.snapshot.version\n\n        if self.compute_metafile_hashes_length:\n            hashes, length = self._compute_hashes_and_length(\"snapshot\")\n            self.timestamp.snapshot_meta.hashes = hashes\n            self.timestamp.snapshot_meta.length = length\n\n        self.timestamp.version += 1\n\n    def update_snapshot(self):\n        for role, delegate in self.all_targets():\n            hashes = None\n            length = None\n            if self.compute_metafile_hashes_length:\n                hashes, length = self._compute_hashes_and_length(role)\n\n            self.snapshot.meta[f\"{role}.json\"] = MetaFile(\n                delegate.version, length, hashes\n            )\n\n        self.snapshot.version += 1\n        self.update_timestamp()\n\n    def add_target(self, role: str, data: bytes, path: str):\n        if role == \"targets\":\n            targets = self.targets\n        else:\n            targets = self.md_delegates[role].signed\n\n        target = TargetFile.from_data(path, data, [\"sha256\"])\n        targets.targets[path] = target\n        self.target_files[path] = RepositoryTarget(data, target)\n\n    def add_delegation(\n        self,\n        delegator_name: str,\n        name: str,\n        targets: Targets,\n        terminating: bool,\n        paths: Optional[List[str]],\n        hash_prefixes: Optional[List[str]],\n    ):\n        if delegator_name == \"targets\":\n            delegator = self.targets\n        else:\n            delegator = self.md_delegates[delegator_name].signed\n\n        # Create delegation\n        role = DelegatedRole(name, [], 1, terminating, paths, hash_prefixes)\n        if delegator.delegations is None:\n            delegator.delegations = Delegations({}, {})\n        # put delegation last by default\n        delegator.delegations.roles[role.name] = role\n\n        # By default add one new key for the role\n        key, signer = self.create_key()\n        delegator.add_key(role.name, key)\n        if role.name not in self.signers:\n            self.signers[role.name] = []\n        self.signers[role.name].append(signer)\n\n        # Add metadata for the role\n        self.md_delegates[role.name] = Metadata(targets, OrderedDict())\n\n    def write(self):\n        \"\"\"Dump current repository metadata to self.dump_dir\n\n        This is a debugging tool: dumping repository state before running\n        Updater refresh may be useful while debugging a test.\n        \"\"\"\n        if self.dump_dir is None:\n            self.dump_dir = tempfile.mkdtemp()\n            print(f\"Repository Simulator dumps in {self.dump_dir}\")\n\n        self.dump_version += 1\n        dir = os.path.join(self.dump_dir, str(self.dump_version))\n        os.makedirs(dir)\n\n        for ver in range(1, len(self.signed_roots) + 1):\n            with open(os.path.join(dir, f\"{ver}.root.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(\"root\", ver))\n\n        for role in [\"timestamp\", \"snapshot\", \"targets\"]:\n            with open(os.path.join(dir, f\"{role}.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(role))\n\n        for role in self.md_delegates.keys():\n            with open(os.path.join(dir, f\"{role}.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(role))\n", "patch": "@@ -59,6 +59,8 @@\n from tuf.api.serialization.json import JSONSerializer\n from tuf.exceptions import FetcherHTTPError\n from tuf.api.metadata import (\n+    DelegatedRole,\n+    Delegations,\n     Key,\n     Metadata,\n     MetaFile,\n@@ -106,6 +108,9 @@ def __init__(self):\n         self.dump_dir = None\n         self.dump_version = 0\n \n+        now = datetime.utcnow()\n+        self.safe_expiry = now.replace(microsecond=0) + timedelta(days=30)\n+\n         self._initialize()\n \n     @property\n@@ -135,20 +140,19 @@ def create_key(self) -> Tuple[Key, SSlibSigner]:\n \n     def _initialize(self):\n         \"\"\"Setup a minimal valid repository\"\"\"\n-        expiry = datetime.utcnow().replace(microsecond=0) + timedelta(days=30)\n \n-        targets = Targets(1, SPEC_VER, expiry, {}, None)\n+        targets = Targets(1, SPEC_VER, self.safe_expiry, {}, None)\n         self.md_targets = Metadata(targets, OrderedDict())\n \n         meta = {\"targets.json\": MetaFile(targets.version)}\n-        snapshot = Snapshot(1, SPEC_VER, expiry, meta)\n+        snapshot = Snapshot(1, SPEC_VER, self.safe_expiry, meta)\n         self.md_snapshot = Metadata(snapshot, OrderedDict())\n \n         snapshot_meta = MetaFile(snapshot.version)\n-        timestamp = Timestamp(1, SPEC_VER, expiry, snapshot_meta)\n+        timestamp = Timestamp(1, SPEC_VER, self.safe_expiry, snapshot_meta)\n         self.md_timestamp = Metadata(timestamp, OrderedDict())\n \n-        root = Root(1, SPEC_VER, expiry, {}, {}, True)\n+        root = Root(1, SPEC_VER, self.safe_expiry, {}, {}, True)\n         for role in [\"root\", \"timestamp\", \"snapshot\", \"targets\"]:\n             key, signer = self.create_key()\n             root.roles[role] = Role([], 1)\n@@ -172,27 +176,27 @@ def publish_root(self):\n     def fetch(self, url: str) -> Iterator[bytes]:\n         if not self.root.consistent_snapshot:\n             raise NotImplementedError(\"non-consistent snapshot not supported\")\n-\n-        spliturl = parse.urlparse(url)\n-        if spliturl.path.startswith(\"/metadata/\"):\n-            parts = spliturl.path[len(\"/metadata/\") :].split(\".\")\n-            if len(parts) == 3:\n-                version: Optional[int] = int(parts[0])\n-                role = parts[1]\n-            else:\n+        path = parse.urlparse(url).path\n+        if path.startswith(\"/metadata/\") and path.endswith(\".json\"):\n+            ver_and_name = path[len(\"/metadata/\") :][: -len(\".json\")]\n+            # only consistent_snapshot supported ATM: timestamp is special case\n+            if ver_and_name == \"timestamp\":\n                 version = None\n-                role = parts[0]\n+                role = \"timestamp\"\n+            else:\n+                version, _, role = ver_and_name.partition(\".\")\n+                version = int(version)\n             yield self._fetch_metadata(role, version)\n-        elif spliturl.path.startswith(\"/targets/\"):\n+        elif path.startswith(\"/targets/\"):\n             # figure out target path and hash prefix\n-            path = spliturl.path[len(\"/targets/\") :]\n-            dir_parts, sep , prefixed_filename = path.rpartition(\"/\")\n+            target_path = path[len(\"/targets/\") :]\n+            dir_parts, sep , prefixed_filename = target_path.rpartition(\"/\")\n             prefix, _, filename = prefixed_filename.partition(\".\")\n             target_path = f\"{dir_parts}{sep}{filename}\"\n \n             yield self._fetch_target(target_path, prefix)\n         else:\n-            raise FetcherHTTPError(f\"Unknown path '{spliturl.path}'\", 404)\n+            raise FetcherHTTPError(f\"Unknown path '{path}'\", 404)\n \n     def _fetch_target(self, target_path: str, hash: Optional[str]) -> bytes:\n         \"\"\"Return data for 'target_path', checking 'hash' if it is given.\n@@ -268,12 +272,14 @@ def update_timestamp(self):\n \n     def update_snapshot(self):\n         for role, delegate in self.all_targets():\n-            self.snapshot.meta[f\"{role}.json\"].version = delegate.version\n-\n+            hashes = None\n+            length = None\n             if self.compute_metafile_hashes_length:\n                 hashes, length = self._compute_hashes_and_length(role)\n-                self.snapshot.meta[f\"{role}.json\"].hashes = hashes\n-                self.snapshot.meta[f\"{role}.json\"].length = length\n+\n+            self.snapshot.meta[f\"{role}.json\"] = MetaFile(\n+                delegate.version, length, hashes\n+            )\n \n         self.snapshot.version += 1\n         self.update_timestamp()\n@@ -288,6 +294,37 @@ def add_target(self, role: str, data: bytes, path: str):\n         targets.targets[path] = target\n         self.target_files[path] = RepositoryTarget(data, target)\n \n+    def add_delegation(\n+        self,\n+        delegator_name: str,\n+        name: str,\n+        targets: Targets,\n+        terminating: bool,\n+        paths: Optional[List[str]],\n+        hash_prefixes: Optional[List[str]],\n+    ):\n+        if delegator_name == \"targets\":\n+            delegator = self.targets\n+        else:\n+            delegator = self.md_delegates[delegator_name].signed\n+\n+        # Create delegation\n+        role = DelegatedRole(name, [], 1, terminating, paths, hash_prefixes)\n+        if delegator.delegations is None:\n+            delegator.delegations = Delegations({}, {})\n+        # put delegation last by default\n+        delegator.delegations.roles[role.name] = role\n+\n+        # By default add one new key for the role\n+        key, signer = self.create_key()\n+        delegator.add_key(role.name, key)\n+        if role.name not in self.signers:\n+            self.signers[role.name] = []\n+        self.signers[role.name].append(signer)\n+\n+        # Add metadata for the role\n+        self.md_delegates[role.name] = Metadata(targets, OrderedDict())\n+\n     def write(self):\n         \"\"\"Dump current repository metadata to self.dump_dir\n ", "file_path": "files/2021_10/351", "file_language": "py", "file_name": "tests/repository_simulator.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class RepositorySimulator(FetcherInterface):\n    def __init__(self):\n        self.md_root: Metadata[Root] = None\n        self.md_timestamp: Metadata[Timestamp] = None\n        self.md_snapshot: Metadata[Snapshot] = None\n        self.md_targets: Metadata[Targets] = None\n        self.md_delegates: Dict[str, Metadata[Targets]] = {}\n\n        # other metadata is signed on-demand (when fetched) but roots must be\n        # explicitly published with publish_root() which maintains this list\n        self.signed_roots: List[bytes] = []\n\n        # signers are used on-demand at fetch time to sign metadata\n        self.signers: Dict[str, List[SSlibSigner]] = {}\n\n        # target downloads are served from this dict\n        self.target_files: Dict[str, RepositoryTarget] = {}\n\n        # Whether to compute hashes and legth for meta in snapshot/timestamp\n        self.compute_metafile_hashes_length = False\n\n        self.dump_dir = None\n        self.dump_version = 0\n\n        now = datetime.utcnow()\n        self.safe_expiry = now.replace(microsecond=0) + timedelta(days=30)\n\n        self._initialize()\n\n    @property\n    def root(self) -> Root:\n        return self.md_root.signed\n\n    @property\n    def timestamp(self) -> Timestamp:\n        return self.md_timestamp.signed\n\n    @property\n    def snapshot(self) -> Snapshot:\n        return self.md_snapshot.signed\n\n    @property\n    def targets(self) -> Targets:\n        return self.md_targets.signed\n\n    def all_targets(self) -> Iterator[Tuple[str, Targets]]:\n        yield \"targets\", self.md_targets.signed\n        for role, md in self.md_delegates.items():\n            yield role, md.signed\n\n    def create_key(self) -> Tuple[Key, SSlibSigner]:\n        sslib_key = generate_ed25519_key()\n        return Key.from_securesystemslib_key(sslib_key), SSlibSigner(sslib_key)\n\n    def _initialize(self):\n        \"\"\"Setup a minimal valid repository\"\"\"\n\n        targets = Targets(1, SPEC_VER, self.safe_expiry, {}, None)\n        self.md_targets = Metadata(targets, OrderedDict())\n\n        meta = {\"targets.json\": MetaFile(targets.version)}\n        snapshot = Snapshot(1, SPEC_VER, self.safe_expiry, meta)\n        self.md_snapshot = Metadata(snapshot, OrderedDict())\n\n        snapshot_meta = MetaFile(snapshot.version)\n        timestamp = Timestamp(1, SPEC_VER, self.safe_expiry, snapshot_meta)\n        self.md_timestamp = Metadata(timestamp, OrderedDict())\n\n        root = Root(1, SPEC_VER, self.safe_expiry, {}, {}, True)\n        for role in [\"root\", \"timestamp\", \"snapshot\", \"targets\"]:\n            key, signer = self.create_key()\n            root.roles[role] = Role([], 1)\n            root.add_key(role, key)\n            # store the private key\n            if role not in self.signers:\n                self.signers[role] = []\n            self.signers[role].append(signer)\n        self.md_root = Metadata(root, OrderedDict())\n        self.publish_root()\n\n    def publish_root(self):\n        \"\"\"Sign and store a new serialized version of root\"\"\"\n        self.md_root.signatures.clear()\n        for signer in self.signers[\"root\"]:\n            self.md_root.sign(signer)\n\n        self.signed_roots.append(self.md_root.to_bytes(JSONSerializer()))\n        logger.debug(\"Published root v%d\", self.root.version)\n\n    def fetch(self, url: str) -> Iterator[bytes]:\n        if not self.root.consistent_snapshot:\n            raise NotImplementedError(\"non-consistent snapshot not supported\")\n        path = parse.urlparse(url).path\n        if path.startswith(\"/metadata/\") and path.endswith(\".json\"):\n            ver_and_name = path[len(\"/metadata/\") :][: -len(\".json\")]\n            # only consistent_snapshot supported ATM: timestamp is special case\n            if ver_and_name == \"timestamp\":\n                version = None\n                role = \"timestamp\"\n            else:\n                version, _, role = ver_and_name.partition(\".\")\n                version = int(version)\n            yield self._fetch_metadata(role, version)\n        elif path.startswith(\"/targets/\"):\n            # figure out target path and hash prefix\n            target_path = path[len(\"/targets/\") :]\n            dir_parts, sep , prefixed_filename = target_path.rpartition(\"/\")\n            prefix, _, filename = prefixed_filename.partition(\".\")\n            target_path = f\"{dir_parts}{sep}{filename}\"\n\n            yield self._fetch_target(target_path, prefix)\n        else:\n            raise FetcherHTTPError(f\"Unknown path '{path}'\", 404)\n\n    def _fetch_target(self, target_path: str, hash: Optional[str]) -> bytes:\n        \"\"\"Return data for 'target_path', checking 'hash' if it is given.\n\n        If hash is None, then consistent_snapshot is not used\n        \"\"\"\n        repo_target = self.target_files.get(target_path)\n        if repo_target is None:\n            raise FetcherHTTPError(f\"No target {target_path}\", 404)\n        if hash and hash not in repo_target.target_file.hashes.values():\n            raise FetcherHTTPError(f\"hash mismatch for {target_path}\", 404)\n\n        logger.debug(\"fetched target %s\", target_path)\n        return repo_target.data\n\n    def _fetch_metadata(self, role: str, version: Optional[int] = None) -> bytes:\n        \"\"\"Return signed metadata for 'role', using 'version' if it is given.\n\n        If version is None, non-versioned metadata is being requested\n        \"\"\"\n        if role == \"root\":\n            # return a version previously serialized in publish_root()\n            if version is None or version > len(self.signed_roots):\n                raise FetcherHTTPError(f\"Unknown root version {version}\", 404)\n            logger.debug(\"fetched root version %d\", role, version)\n            return self.signed_roots[version - 1]\n        else:\n            # sign and serialize the requested metadata\n            if role == \"timestamp\":\n                md: Metadata = self.md_timestamp\n            elif role == \"snapshot\":\n                md = self.md_snapshot\n            elif role == \"targets\":\n                md = self.md_targets\n            else:\n                md = self.md_delegates[role]\n\n            if md is None:\n                raise FetcherHTTPError(f\"Unknown role {role}\", 404)\n            if version is not None and version != md.signed.version:\n                raise FetcherHTTPError(f\"Unknown {role} version {version}\", 404)\n\n            md.signatures.clear()\n            for signer in self.signers[role]:\n                md.sign(signer, append=True)\n\n            logger.debug(\n                \"fetched %s v%d with %d sigs\",\n                role,\n                md.signed.version,\n                len(self.signers[role]),\n            )\n            return md.to_bytes(JSONSerializer())\n\n    def _compute_hashes_and_length(\n        self, role: str\n    ) -> Tuple[Dict[str, str], int]:\n        data = self._fetch_metadata(role)\n        digest_object = sslib_hash.digest(sslib_hash.DEFAULT_HASH_ALGORITHM)\n        digest_object.update(data)\n        hashes = {sslib_hash.DEFAULT_HASH_ALGORITHM:  digest_object.hexdigest()}\n        return hashes, len(data)\n\n    def update_timestamp(self):\n        self.timestamp.snapshot_meta.version = self.snapshot.version\n\n        if self.compute_metafile_hashes_length:\n            hashes, length = self._compute_hashes_and_length(\"snapshot\")\n            self.timestamp.snapshot_meta.hashes = hashes\n            self.timestamp.snapshot_meta.length = length\n\n        self.timestamp.version += 1\n\n    def update_snapshot(self):\n        for role, delegate in self.all_targets():\n            hashes = None\n            length = None\n            if self.compute_metafile_hashes_length:\n                hashes, length = self._compute_hashes_and_length(role)\n\n            self.snapshot.meta[f\"{role}.json\"] = MetaFile(\n                delegate.version, length, hashes\n            )\n\n        self.snapshot.version += 1\n        self.update_timestamp()\n\n    def add_target(self, role: str, data: bytes, path: str):\n        if role == \"targets\":\n            targets = self.targets\n        else:\n            targets = self.md_delegates[role].signed\n\n        target = TargetFile.from_data(path, data, [\"sha256\"])\n        targets.targets[path] = target\n        self.target_files[path] = RepositoryTarget(data, target)\n\n    def add_delegation(\n        self,\n        delegator_name: str,\n        name: str,\n        targets: Targets,\n        terminating: bool,\n        paths: Optional[List[str]],\n        hash_prefixes: Optional[List[str]],\n    ):\n        if delegator_name == \"targets\":\n            delegator = self.targets\n        else:\n            delegator = self.md_delegates[delegator_name].signed\n\n        # Create delegation\n        role = DelegatedRole(name, [], 1, terminating, paths, hash_prefixes)\n        if delegator.delegations is None:\n            delegator.delegations = Delegations({}, {})\n        # put delegation last by default\n        delegator.delegations.roles[role.name] = role\n\n        # By default add one new key for the role\n        key, signer = self.create_key()\n        delegator.add_key(role.name, key)\n        if role.name not in self.signers:\n            self.signers[role.name] = []\n        self.signers[role.name].append(signer)\n\n        # Add metadata for the role\n        self.md_delegates[role.name] = Metadata(targets, OrderedDict())\n\n    def write(self):\n        \"\"\"Dump current repository metadata to self.dump_dir\n\n        This is a debugging tool: dumping repository state before running\n        Updater refresh may be useful while debugging a test.\n        \"\"\"\n        if self.dump_dir is None:\n            self.dump_dir = tempfile.mkdtemp()\n            print(f\"Repository Simulator dumps in {self.dump_dir}\")\n\n        self.dump_version += 1\n        dir = os.path.join(self.dump_dir, str(self.dump_version))\n        os.makedirs(dir)\n\n        for ver in range(1, len(self.signed_roots) + 1):\n            with open(os.path.join(dir, f\"{ver}.root.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(\"root\", ver))\n\n        for role in [\"timestamp\", \"snapshot\", \"targets\"]:\n            with open(os.path.join(dir, f\"{role}.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(role))\n\n        for role in self.md_delegates.keys():\n            with open(os.path.join(dir, f\"{role}.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(role))", "target": 0}], "function_after": [{"function": "class RepositorySimulator(FetcherInterface):\n    def __init__(self):\n        self.md_root: Metadata[Root] = None\n        self.md_timestamp: Metadata[Timestamp] = None\n        self.md_snapshot: Metadata[Snapshot] = None\n        self.md_targets: Metadata[Targets] = None\n        self.md_delegates: Dict[str, Metadata[Targets]] = {}\n\n        # other metadata is signed on-demand (when fetched) but roots must be\n        # explicitly published with publish_root() which maintains this list\n        self.signed_roots: List[bytes] = []\n\n        # signers are used on-demand at fetch time to sign metadata\n        self.signers: Dict[str, List[SSlibSigner]] = {}\n\n        # target downloads are served from this dict\n        self.target_files: Dict[str, RepositoryTarget] = {}\n\n        # Whether to compute hashes and legth for meta in snapshot/timestamp\n        self.compute_metafile_hashes_length = False\n\n        self.dump_dir = None\n        self.dump_version = 0\n\n        now = datetime.utcnow()\n        self.safe_expiry = now.replace(microsecond=0) + timedelta(days=30)\n\n        self._initialize()\n\n    @property\n    def root(self) -> Root:\n        return self.md_root.signed\n\n    @property\n    def timestamp(self) -> Timestamp:\n        return self.md_timestamp.signed\n\n    @property\n    def snapshot(self) -> Snapshot:\n        return self.md_snapshot.signed\n\n    @property\n    def targets(self) -> Targets:\n        return self.md_targets.signed\n\n    def all_targets(self) -> Iterator[Tuple[str, Targets]]:\n        yield \"targets\", self.md_targets.signed\n        for role, md in self.md_delegates.items():\n            yield role, md.signed\n\n    def create_key(self) -> Tuple[Key, SSlibSigner]:\n        sslib_key = generate_ed25519_key()\n        return Key.from_securesystemslib_key(sslib_key), SSlibSigner(sslib_key)\n\n    def _initialize(self):\n        \"\"\"Setup a minimal valid repository\"\"\"\n\n        targets = Targets(1, SPEC_VER, self.safe_expiry, {}, None)\n        self.md_targets = Metadata(targets, OrderedDict())\n\n        meta = {\"targets.json\": MetaFile(targets.version)}\n        snapshot = Snapshot(1, SPEC_VER, self.safe_expiry, meta)\n        self.md_snapshot = Metadata(snapshot, OrderedDict())\n\n        snapshot_meta = MetaFile(snapshot.version)\n        timestamp = Timestamp(1, SPEC_VER, self.safe_expiry, snapshot_meta)\n        self.md_timestamp = Metadata(timestamp, OrderedDict())\n\n        root = Root(1, SPEC_VER, self.safe_expiry, {}, {}, True)\n        for role in [\"root\", \"timestamp\", \"snapshot\", \"targets\"]:\n            key, signer = self.create_key()\n            root.roles[role] = Role([], 1)\n            root.add_key(role, key)\n            # store the private key\n            if role not in self.signers:\n                self.signers[role] = []\n            self.signers[role].append(signer)\n        self.md_root = Metadata(root, OrderedDict())\n        self.publish_root()\n\n    def publish_root(self):\n        \"\"\"Sign and store a new serialized version of root\"\"\"\n        self.md_root.signatures.clear()\n        for signer in self.signers[\"root\"]:\n            self.md_root.sign(signer)\n\n        self.signed_roots.append(self.md_root.to_bytes(JSONSerializer()))\n        logger.debug(\"Published root v%d\", self.root.version)\n\n    def fetch(self, url: str) -> Iterator[bytes]:\n        if not self.root.consistent_snapshot:\n            raise NotImplementedError(\"non-consistent snapshot not supported\")\n        path = parse.urlparse(url).path\n        if path.startswith(\"/metadata/\") and path.endswith(\".json\"):\n            ver_and_name = path[len(\"/metadata/\") :][: -len(\".json\")]\n            # only consistent_snapshot supported ATM: timestamp is special case\n            if ver_and_name == \"timestamp\":\n                version = None\n                role = \"timestamp\"\n            else:\n                version, _, role = ver_and_name.partition(\".\")\n                version = int(version)\n            yield self._fetch_metadata(role, version)\n        elif path.startswith(\"/targets/\"):\n            # figure out target path and hash prefix\n            target_path = path[len(\"/targets/\") :]\n            dir_parts, sep , prefixed_filename = target_path.rpartition(\"/\")\n            prefix, _, filename = prefixed_filename.partition(\".\")\n            target_path = f\"{dir_parts}{sep}{filename}\"\n\n            yield self._fetch_target(target_path, prefix)\n        else:\n            raise FetcherHTTPError(f\"Unknown path '{path}'\", 404)\n\n    def _fetch_target(self, target_path: str, hash: Optional[str]) -> bytes:\n        \"\"\"Return data for 'target_path', checking 'hash' if it is given.\n\n        If hash is None, then consistent_snapshot is not used\n        \"\"\"\n        repo_target = self.target_files.get(target_path)\n        if repo_target is None:\n            raise FetcherHTTPError(f\"No target {target_path}\", 404)\n        if hash and hash not in repo_target.target_file.hashes.values():\n            raise FetcherHTTPError(f\"hash mismatch for {target_path}\", 404)\n\n        logger.debug(\"fetched target %s\", target_path)\n        return repo_target.data\n\n    def _fetch_metadata(self, role: str, version: Optional[int] = None) -> bytes:\n        \"\"\"Return signed metadata for 'role', using 'version' if it is given.\n\n        If version is None, non-versioned metadata is being requested\n        \"\"\"\n        if role == \"root\":\n            # return a version previously serialized in publish_root()\n            if version is None or version > len(self.signed_roots):\n                raise FetcherHTTPError(f\"Unknown root version {version}\", 404)\n            logger.debug(\"fetched root version %d\", role, version)\n            return self.signed_roots[version - 1]\n        else:\n            # sign and serialize the requested metadata\n            if role == \"timestamp\":\n                md: Metadata = self.md_timestamp\n            elif role == \"snapshot\":\n                md = self.md_snapshot\n            elif role == \"targets\":\n                md = self.md_targets\n            else:\n                md = self.md_delegates[role]\n\n            if md is None:\n                raise FetcherHTTPError(f\"Unknown role {role}\", 404)\n            if version is not None and version != md.signed.version:\n                raise FetcherHTTPError(f\"Unknown {role} version {version}\", 404)\n\n            md.signatures.clear()\n            for signer in self.signers[role]:\n                md.sign(signer, append=True)\n\n            logger.debug(\n                \"fetched %s v%d with %d sigs\",\n                role,\n                md.signed.version,\n                len(self.signers[role]),\n            )\n            return md.to_bytes(JSONSerializer())\n\n    def _compute_hashes_and_length(\n        self, role: str\n    ) -> Tuple[Dict[str, str], int]:\n        data = self._fetch_metadata(role)\n        digest_object = sslib_hash.digest(sslib_hash.DEFAULT_HASH_ALGORITHM)\n        digest_object.update(data)\n        hashes = {sslib_hash.DEFAULT_HASH_ALGORITHM:  digest_object.hexdigest()}\n        return hashes, len(data)\n\n    def update_timestamp(self):\n        self.timestamp.snapshot_meta.version = self.snapshot.version\n\n        if self.compute_metafile_hashes_length:\n            hashes, length = self._compute_hashes_and_length(\"snapshot\")\n            self.timestamp.snapshot_meta.hashes = hashes\n            self.timestamp.snapshot_meta.length = length\n\n        self.timestamp.version += 1\n\n    def update_snapshot(self):\n        for role, delegate in self.all_targets():\n            hashes = None\n            length = None\n            if self.compute_metafile_hashes_length:\n                hashes, length = self._compute_hashes_and_length(role)\n\n            self.snapshot.meta[f\"{role}.json\"] = MetaFile(\n                delegate.version, length, hashes\n            )\n\n        self.snapshot.version += 1\n        self.update_timestamp()\n\n    def add_target(self, role: str, data: bytes, path: str):\n        if role == \"targets\":\n            targets = self.targets\n        else:\n            targets = self.md_delegates[role].signed\n\n        target = TargetFile.from_data(path, data, [\"sha256\"])\n        targets.targets[path] = target\n        self.target_files[path] = RepositoryTarget(data, target)\n\n    def add_delegation(\n        self,\n        delegator_name: str,\n        name: str,\n        targets: Targets,\n        terminating: bool,\n        paths: Optional[List[str]],\n        hash_prefixes: Optional[List[str]],\n    ):\n        if delegator_name == \"targets\":\n            delegator = self.targets\n        else:\n            delegator = self.md_delegates[delegator_name].signed\n\n        # Create delegation\n        role = DelegatedRole(name, [], 1, terminating, paths, hash_prefixes)\n        if delegator.delegations is None:\n            delegator.delegations = Delegations({}, {})\n        # put delegation last by default\n        delegator.delegations.roles[role.name] = role\n\n        # By default add one new key for the role\n        key, signer = self.create_key()\n        delegator.add_key(role.name, key)\n        if role.name not in self.signers:\n            self.signers[role.name] = []\n        self.signers[role.name].append(signer)\n\n        # Add metadata for the role\n        self.md_delegates[role.name] = Metadata(targets, OrderedDict())\n\n    def write(self):\n        \"\"\"Dump current repository metadata to self.dump_dir\n\n        This is a debugging tool: dumping repository state before running\n        Updater refresh may be useful while debugging a test.\n        \"\"\"\n        if self.dump_dir is None:\n            self.dump_dir = tempfile.mkdtemp()\n            print(f\"Repository Simulator dumps in {self.dump_dir}\")\n\n        self.dump_version += 1\n        dir = os.path.join(self.dump_dir, str(self.dump_version))\n        os.makedirs(dir)\n\n        for ver in range(1, len(self.signed_roots) + 1):\n            with open(os.path.join(dir, f\"{ver}.root.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(\"root\", ver))\n\n        for role in [\"timestamp\", \"snapshot\", \"targets\"]:\n            with open(os.path.join(dir, f\"{role}.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(role))\n\n        for role in self.md_delegates.keys():\n            with open(os.path.join(dir, f\"{role}.json\"), \"wb\") as f:\n                f.write(self._fetch_metadata(role))", "target": 0}]}, {"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tests%2Ftest_updater.py", "code": "#!/usr/bin/env python\n\n# Copyright 2012 - 2017, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\n<Program Name>\n  test_updater.py\n\n<Author>\n  Konstantin Andrianov.\n\n<Started>\n  October 15, 2012.\n\n  March 11, 2014.\n    Refactored to remove mocked modules and old repository tool dependence, use\n    exact repositories, and add realistic retrieval of files. -vladimir.v.diaz\n\n<Copyright>\n  See LICENSE-MIT OR LICENSE for licensing information.\n\n<Purpose>\n  'test_updater.py' provides a collection of methods that test the public /\n  non-public methods and functions of 'tuf.client.updater.py'.\n\n  The 'unittest_toolbox.py' module was created to provide additional testing\n  tools, such as automatically deleting temporary files created in test cases.\n  For more information, see 'tests/unittest_toolbox.py'.\n\n<Methodology>\n  Test cases here should follow a specific order (i.e., independent methods are\n  tested before dependent methods). More accurately, least dependent methods\n  are tested before most dependent methods.  There is no reason to rewrite or\n  construct other methods that replicate already-tested methods solely for\n  testing purposes.  This is possible because the 'unittest.TestCase' class\n  guarantees the order of unit tests.  The 'test_something_A' method would\n  be tested before 'test_something_B'.  To ensure the expected order of tests,\n  a number is placed after 'test' and before methods name like so:\n  'test_1_check_directory'.  The number is a measure of dependence, where 1 is\n  less dependent than 2.\n\"\"\"\n\nimport os\nimport time\nimport shutil\nimport copy\nimport tempfile\nimport logging\nimport errno\nimport sys\nimport unittest\nimport json\nimport unittest.mock as mock\n\nimport tuf\nimport tuf.exceptions\nimport tuf.log\nimport tuf.formats\nimport tuf.keydb\nimport tuf.roledb\nimport tuf.repository_tool as repo_tool\nimport tuf.repository_lib as repo_lib\nimport tuf.unittest_toolbox as unittest_toolbox\nimport tuf.client.updater as updater\n\nfrom tests import utils\n\nimport securesystemslib\n\nlogger = logging.getLogger(__name__)\nrepo_tool.disable_console_log_messages()\n\n\nclass TestUpdater(unittest_toolbox.Modified_TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    # Create a temporary directory to store the repository, metadata, and target\n    # files.  'temporary_directory' must be deleted in TearDownModule() so that\n    # temporary files are always removed, even when exceptions occur.\n    cls.temporary_directory = tempfile.mkdtemp(dir=os.getcwd())\n\n    # Needed because in some tests simple_server.py cannot be found.\n    # The reason is that the current working directory\n    # has been changed when executing a subprocess.\n    cls.SIMPLE_SERVER_PATH = os.path.join(os.getcwd(), 'simple_server.py')\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served\n    # by the SimpleHTTPServer launched here.  The test cases of 'test_updater.py'\n    # assume the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n    cls.server_process_handler = utils.TestServerProcess(log=logger,\n        server=cls.SIMPLE_SERVER_PATH)\n\n\n\n  @classmethod\n  def tearDownClass(cls):\n    # Cleans the resources and flush the logged lines (if any).\n    cls.server_process_handler.clean()\n\n    # Remove the temporary repository directory, which should contain all the\n    # metadata, targets, and key files generated for the test cases\n    shutil.rmtree(cls.temporary_directory)\n\n\n\n  def setUp(self):\n    # We are inheriting from custom class.\n    unittest_toolbox.Modified_TestCase.setUp(self)\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    self.repository_name = 'test_repository1'\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf.tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n    temporary_repository_root = \\\n      self.make_temp_directory(directory=self.temporary_directory)\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_client = os.path.join(original_repository_files, 'client')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = \\\n      os.path.join(temporary_repository_root, 'repository')\n    self.keystore_directory = \\\n      os.path.join(temporary_repository_root, 'keystore')\n\n    self.client_directory = os.path.join(temporary_repository_root,\n        'client')\n    self.client_metadata = os.path.join(self.client_directory,\n        self.repository_name, 'metadata')\n    self.client_metadata_current = os.path.join(self.client_metadata,\n        'current')\n    self.client_metadata_previous = os.path.join(self.client_metadata,\n        'previous')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n                                           'metadata_path': 'metadata',\n                                           'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n                                              self.repository_mirrors)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    # Logs stdout and stderr from the sever subprocess.\n    self.server_process_handler.flush_log()\n\n    # Remove temporary directory\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n\n  # UNIT TESTS.\n\n  def test_1__init__exceptions(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, 8,\n                      self.repository_mirrors)\n\n    # Invalid 'repository_mirrors' argument.  'tuf.formats.MIRRORDICT_SCHEMA'\n    # expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, updater.Updater, 8)\n\n\n    # 'tuf.client.updater.py' requires that the client's repositories directory\n    # be configured in 'tuf.settings.py'.\n    tuf.settings.repositories_directory = None\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n    # Test: repository does not exist\n    self.assertRaises(tuf.exceptions.MissingLocalRepositoryError, updater.Updater,\n                      'test_non_existing_repository', self.repository_mirrors)\n\n    # Test: empty client repository (i.e., no metadata directory).\n    metadata_backup = self.client_metadata + '.backup'\n    shutil.move(self.client_metadata, metadata_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's metadata directory.\n    shutil.move(metadata_backup, self.client_metadata)\n\n\n    # Test: repository with only a '{repository_directory}/metadata' directory.\n    # (i.e., missing the required 'current' and 'previous' sub-directories).\n    current_backup = self.client_metadata_current + '.backup'\n    previous_backup = self.client_metadata_previous + '.backup'\n\n    shutil.move(self.client_metadata_current, current_backup)\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n\n    # Restore the client's previous directory.  The required 'current' directory\n    # is still missing.\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test: repository with only a '{repository_directory}/metadata/previous'\n    # directory.\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's current directory.\n    shutil.move(current_backup, self.client_metadata_current)\n\n    # Test: repository with a '{repository_directory}/metadata/current'\n    # directory, but the 'previous' directory is missing.\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test:  repository missing the required 'root.json' file.\n    client_root_file = os.path.join(self.client_metadata_current, 'root.json')\n    backup_root_file = client_root_file + '.backup'\n    shutil.move(client_root_file, backup_root_file)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's 'root.json file.\n    shutil.move(backup_root_file, client_root_file)\n\n    # Test: Normal 'tuf.client.updater.Updater' instantiation.\n    updater.Updater('test_repository1', self.repository_mirrors)\n\n\n\n\n\n  def test_1__load_metadata_from_file(self):\n\n    # Setup\n    # Get the 'role1.json' filepath.  Manually load the role metadata, and\n    # compare it against the loaded metadata by '_load_metadata_from_file()'.\n    role1_filepath = \\\n      os.path.join(self.client_metadata_current, 'role1.json')\n    role1_meta = securesystemslib.util.load_json_file(role1_filepath)\n\n    # Load the 'role1.json' file with _load_metadata_from_file, which should\n    # store the loaded metadata in the 'self.repository_updater.metadata'\n    # store.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n\n    # Verify that the correct number of metadata objects has been loaded\n    # (i.e., only the 'root.json' file should have been loaded.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Verify that the content of root metadata is valid.\n    self.assertEqual(self.repository_updater.metadata['current']['role1'],\n                     role1_meta['signed'])\n\n    # Verify that _load_metadata_from_file() doesn't raise an exception for\n    # improperly formatted metadata, and doesn't load the bad file.\n    with open(role1_filepath, 'ab') as file_object:\n      file_object.write(b'bad JSON data')\n\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Test if we fail gracefully if we can't deserialize a meta file\n    self.repository_updater._load_metadata_from_file('current', 'empty_file')\n    self.assertFalse('empty_file' in self.repository_updater.metadata['current'])\n\n    # Test invalid metadata set argument (must be either\n    # 'current' or 'previous'.)\n    self.assertRaises(securesystemslib.exceptions.Error,\n                      self.repository_updater._load_metadata_from_file,\n                      'bad_metadata_set', 'role1')\n\n\n\n\n  def test_1__rebuild_key_and_role_db(self):\n    # Setup\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_threshold = root_metadata['roles']['root']['threshold']\n    number_of_root_keys = len(root_metadata['keys'])\n\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # Ensure we add 2 to the number of root keys (actually, the number of root\n    # keys multiplied by the number of keyid hash algorithms), to include the\n    # delegated targets key (+1 for its sha512 keyid).  The delegated roles of\n    # 'targets.json' are also loaded when the repository object is\n    # instantiated.\n\n    self.assertEqual(number_of_root_keys + 1, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: normal case.\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # _rebuild_key_and_role_db() will only rebuild the keys and roles specified\n    # in the 'root.json' file, unlike __init__().  Instantiating an updater\n    # object calls both _rebuild_key_and_role_db() and _import_delegations().\n    self.assertEqual(number_of_root_keys, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: properly updated roledb and keydb dicts if the Root role changes.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_metadata['roles']['root']['threshold'] = 8\n    root_metadata['keys'].popitem()\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], 8)\n    self.assertEqual(number_of_root_keys - 1, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n\n\n\n  def test_1__update_versioninfo(self):\n    # Tests\n    # Verify that the 'self.versioninfo' dictionary is empty (it starts off\n    # empty and is only populated if _update_versioninfo() is called.\n    versioninfo_dict = self.repository_updater.versioninfo\n    self.assertEqual(len(versioninfo_dict), 0)\n\n    # Load the versioninfo of the top-level Targets role.  This action\n    # populates the 'self.versioninfo' dictionary.\n    self.repository_updater._update_versioninfo('targets.json')\n    self.assertEqual(len(versioninfo_dict), 1)\n    self.assertTrue(tuf.formats.FILEINFODICT_SCHEMA.matches(versioninfo_dict))\n\n    # The Snapshot role stores the version numbers of all the roles available\n    # on the repository.  Load Snapshot to extract Root's version number\n    # and compare it against the one loaded by 'self.repository_updater'.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    # Verify that the manually loaded version number of root.json matches\n    # the one loaded by the updater object.\n    self.assertTrue('targets.json' in versioninfo_dict)\n    self.assertEqual(versioninfo_dict['targets.json'], targets_versioninfo)\n\n    # Verify that 'self.versioninfo' is incremented if another role is updated.\n    self.repository_updater._update_versioninfo('role1.json')\n    self.assertEqual(len(versioninfo_dict), 2)\n\n    # Verify that 'self.versioninfo' is incremented if a non-existent role is\n    # requested, and has its versioninfo entry set to 'None'.\n    self.repository_updater._update_versioninfo('bad_role.json')\n    self.assertEqual(len(versioninfo_dict), 3)\n    self.assertEqual(versioninfo_dict['bad_role.json'], None)\n\n    # Verify that the versioninfo specified in Timestamp is used if the Snapshot\n    # role hasn't been downloaded yet.\n    del self.repository_updater.metadata['current']['snapshot']\n    #self.assertRaises(self.repository_updater._update_versioninfo('snapshot.json'))\n    self.repository_updater._update_versioninfo('snapshot.json')\n    self.assertEqual(versioninfo_dict['snapshot.json']['version'], 1)\n\n\n\n  def test_1__refresh_must_not_count_duplicate_keyids_towards_threshold(self):\n    # Update root threshold on the server repository and sign twice with 1 key\n    repository = repo_tool.load_repository(self.repository_directory)\n    repository.root.threshold = 2\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n\n    storage_backend = securesystemslib.storage.FilesystemBackend()\n    # The client uses the threshold from the previous root file to verify the\n    # new root. Thus we need to make two updates so that the threshold used for\n    # verification becomes 2. I.e. we bump the version, sign twice with the\n    # same key and write to disk '2.root.json' and '3.root.json'.\n    for version in [2, 3]:\n      repository.root.version = version\n      info = tuf.roledb.get_roleinfo(\"root\")\n      metadata = repo_lib.generate_root_metadata(\n          info[\"version\"], info[\"expires\"], False)\n      signed_metadata = repo_lib.sign_metadata(\n          metadata, info[\"keyids\"], \"root.json\", \"default\")\n      signed_metadata[\"signatures\"].append(signed_metadata[\"signatures\"][0])\n      live_root_path = os.path.join(\n          self.repository_directory, \"metadata\", \"root.json\")\n\n      # Bypass server side verification in 'write' or 'writeall', which would\n      # catch the unmet threshold.\n      # We also skip writing to 'metadata.staged' and copying to 'metadata' and\n      # instead write directly to 'metadata'\n      repo_lib.write_metadata_file(signed_metadata, live_root_path,\n          info[\"version\"], True, storage_backend)\n\n\n    # Update from current '1.root.json' to '3.root.json' on client and assert\n    # raise of 'BadSignatureError' (caused by unmet signature threshold).\n    try:\n      self.repository_updater.refresh()\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      mirror_errors = list(e.mirror_errors.values())\n      self.assertTrue(len(mirror_errors) == 1)\n      self.assertTrue(\n          isinstance(mirror_errors[0],\n          securesystemslib.exceptions.BadSignatureError))\n      self.assertEqual(\n          str(mirror_errors[0]),\n          repr(\"root\") + \" metadata has bad signature.\")\n\n    else:\n      self.fail(\n          \"Expected a NoWorkingMirrorError composed of one BadSignatureError\")\n\n\n  def test_2__import_delegations(self):\n    # Setup.\n    # In order to test '_import_delegations' the parent of the delegation\n    # has to be in Repository.metadata['current'], but it has to be inserted\n    # there without using '_load_metadata_from_file()' since it calls\n    # '_import_delegations()'.\n    repository_name = self.repository_updater.repository_name\n    tuf.keydb.clear_keydb(repository_name)\n    tuf.roledb.clear_roledb(repository_name)\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 0)\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 0)\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n\n    # Take into account the number of keyids algorithms supported by default,\n    # which this test condition expects to be two (sha256 and sha512).\n    self.assertEqual(4, len(tuf.keydb._keydb_dict[repository_name]))\n\n    # Test: pass a role without delegations.\n    self.repository_updater._import_delegations('root')\n\n    # Verify that there was no change to the roledb and keydb dictionaries by\n    # checking the number of elements in the dictionaries.\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n    # Take into account the number of keyid hash algorithms, which this\n    # test condition expects to be one\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4)\n\n    # Test: normal case, first level delegation.\n    self.repository_updater._import_delegations('targets')\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 5)\n    # The number of root keys (times the number of key hash algorithms) +\n    # delegation's key (+1 for its sha512 keyid).\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4 + 1)\n\n    # Verify that roledb dictionary was added.\n    self.assertTrue('role1' in tuf.roledb._roledb_dict[repository_name])\n\n    # Verify that keydb dictionary was updated.\n    role1_signable = \\\n      securesystemslib.util.load_json_file(os.path.join(self.client_metadata_current,\n                                           'role1.json'))\n    keyids = []\n    for signature in role1_signable['signatures']:\n      keyids.append(signature['keyid'])\n\n    for keyid in keyids:\n      self.assertTrue(keyid in tuf.keydb._keydb_dict[repository_name])\n\n    # Verify that _import_delegations() ignores invalid keytypes in the 'keys'\n    # field of parent role's 'delegations'.\n    existing_keyid = keyids[0]\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'bad_keytype'\n    self.repository_updater._import_delegations('targets')\n\n    # Restore the keytype of 'existing_keyid'.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'ed25519'\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated keys is malformed.\n    valid_keyval = self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval']\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = valid_keyval\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated roles is malformed.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['roles'][0]['name'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n\n\n  def test_2__versioninfo_has_been_updated(self):\n    # Verify that the method returns 'False' if a versioninfo was not changed.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    self.assertFalse(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n    # Verify that the method returns 'True' if Root's version number changes.\n    targets_versioninfo['version'] = 8\n    self.assertTrue(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n\n\n\n\n  def test_2__move_current_to_previous(self):\n    # Test case will consist of removing a metadata file from client's\n    # '{client_repository}/metadata/previous' directory, executing the method\n    # and then verifying that the 'previous' directory contains the snapshot\n    # file.\n    previous_snapshot_filepath = os.path.join(self.client_metadata_previous,\n                                              'snapshot.json')\n    os.remove(previous_snapshot_filepath)\n    self.assertFalse(os.path.exists(previous_snapshot_filepath))\n\n    # Verify that the current 'snapshot.json' is moved to the previous directory.\n    self.repository_updater._move_current_to_previous('snapshot')\n    self.assertTrue(os.path.exists(previous_snapshot_filepath))\n\n    # assert that non-ascii alphanumeric role name \"../\u00e4\" (that is url encoded\n    # in local filename) works\n    encoded_current = os.path.join(\n      self.client_metadata_current, '..%2F%C3%A4.json'\n    )\n    encoded_previous = os.path.join(\n      self.client_metadata_previous, '..%2F%C3%A4.json'\n    )\n\n    with open(encoded_current, \"w\"):\n      pass\n    self.repository_updater._move_current_to_previous('../\u00e4')\n    self.assertTrue(os.path.exists(encoded_previous))\n\n\n\n\n\n  def test_2__delete_metadata(self):\n    # This test will verify that 'root' metadata is never deleted.  When a role\n    # is deleted verify that the file is not present in the\n    # 'self.repository_updater.metadata' dictionary.\n    self.repository_updater._delete_metadata('root')\n    self.assertTrue('root' in self.repository_updater.metadata['current'])\n\n    self.repository_updater._delete_metadata('timestamp')\n    self.assertFalse('timestamp' in self.repository_updater.metadata['current'])\n\n\n\n\n\n  def test_2__ensure_not_expired(self):\n    # This test condition will verify that nothing is raised when a metadata\n    # file has a future expiration date.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    self.repository_updater._ensure_not_expired(root_metadata, 'root')\n\n    # Metadata with an expiration time in the future should, of course, not\n    # count as expired\n    expires = tuf.formats.unix_timestamp_to_datetime(int(time.time() + 10))\n    expires = expires.isoformat() + 'Z'\n    root_metadata['expires'] = expires\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    self.repository_updater._ensure_not_expired(root_metadata, 'root')\n\n    # Metadata that expires at the exact current time is considered expired\n    expire_time = int(time.time())\n    expires = \\\n      tuf.formats.unix_timestamp_to_datetime(expire_time).isoformat()+'Z'\n    root_metadata['expires'] = expires\n    mock_time = mock.Mock()\n    mock_time.return_value = expire_time\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    with mock.patch('time.time', mock_time):\n      self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                        self.repository_updater._ensure_not_expired,\n                        root_metadata, 'root')\n\n    # Metadata that expires in the past is considered expired\n    expires = tuf.formats.unix_timestamp_to_datetime(int(time.time() - 10))\n    expires = expires.isoformat() + 'Z'\n    root_metadata['expires'] = expires\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater._ensure_not_expired,\n                      root_metadata, 'root')\n\n\n\n\n\n  def test_3__update_metadata(self):\n    # Setup\n    # _update_metadata() downloads, verifies, and installs the specified\n    # metadata role.  Remove knowledge of currently installed metadata and\n    # verify that they are re-installed after calling _update_metadata().\n\n    # This is the default metadata that we would create for the timestamp role,\n    # because it has no signed metadata for itself.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    # This is the upper bound length for Targets metadata.\n    DEFAULT_TARGETS_FILELENGTH = tuf.settings.DEFAULT_TARGETS_REQUIRED_LENGTH\n\n    # Save the versioninfo of 'targets.json,' needed later when re-installing\n    # with _update_metadata().\n    targets_versioninfo = \\\n      self.repository_updater.metadata['current']['snapshot']['meta']\\\n                                      ['targets.json']\n\n    # Remove the currently installed metadata from the store and disk.  Verify\n    # that the metadata dictionary is re-populated after calling\n    # _update_metadata().\n    del self.repository_updater.metadata['current']['timestamp']\n    del self.repository_updater.metadata['current']['targets']\n\n    timestamp_filepath = \\\n      os.path.join(self.client_metadata_current, 'timestamp.json')\n    targets_filepath = os.path.join(self.client_metadata_current, 'targets.json')\n    root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n    os.remove(timestamp_filepath)\n    os.remove(targets_filepath)\n\n    # Test: normal case.\n    # Verify 'timestamp.json' is properly installed.\n    self.assertFalse('timestamp' in self.repository_updater.metadata)\n\n    logger.info('\\nroleinfo: ' + repr(tuf.roledb.get_rolenames(self.repository_name)))\n    self.repository_updater._update_metadata('timestamp',\n                                             DEFAULT_TIMESTAMP_FILELENGTH)\n    self.assertTrue('timestamp' in self.repository_updater.metadata['current'])\n    os.path.exists(timestamp_filepath)\n\n    # Verify 'targets.json' is properly installed.\n    self.assertFalse('targets' in self.repository_updater.metadata['current'])\n    self.repository_updater._update_metadata('targets',\n                                DEFAULT_TARGETS_FILELENGTH,\n                                targets_versioninfo['version'])\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n\n    targets_signable = securesystemslib.util.load_json_file(targets_filepath)\n    loaded_targets_version = targets_signable['signed']['version']\n    self.assertEqual(targets_versioninfo['version'], loaded_targets_version)\n\n    # Test: Invalid / untrusted version numbers.\n    # Invalid version number for 'targets.json'.\n    self.assertRaises(tuf.exceptions.NoWorkingMirrorError,\n        self.repository_updater._update_metadata,\n        'targets', DEFAULT_TARGETS_FILELENGTH, 88)\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH, 88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.  The version number is checked, so the specific error in\n    # this case should be 'tuf.exceptions.BadVersionNumberError'.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH,\n                                               88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n\n\n\n\n  def test_3__get_metadata_file(self):\n\n    '''\n    This test focuses on making sure that the updater rejects unknown or\n    badly-formatted TUF specification version numbers....\n    '''\n\n    # Make note of the correct supported TUF specification version.\n    correct_specification_version = tuf.SPECIFICATION_VERSION\n\n    # Change it long enough to write new metadata.\n    tuf.SPECIFICATION_VERSION = '0.9.0'\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # Change the supported TUF specification version back to what it should be\n    # so that we can parse the metadata and see that the spec version in the\n    # metadata does not match the code's expected spec version.\n    tuf.SPECIFICATION_VERSION = correct_specification_version\n\n    upperbound_filelength = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n    try:\n      self.repository_updater._get_metadata_file('timestamp', 'timestamp.json',\n      upperbound_filelength, 1)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      # Note that this test provides a piece of metadata which would fail to\n      # be accepted -- with a different error -- if the specification version\n      # number were not a problem.\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(\n            mirror_error, tuf.exceptions.UnsupportedSpecificationError)\n\n    else:\n      self.fail(\n          'Expected a failure to verify metadata when the metadata had a '\n          'specification version number that was unexpected.  '\n          'No error was raised.')\n\n\n\n\n\n  def test_3__update_metadata_if_changed(self):\n    # Setup.\n    # The client repository is initially loaded with only four top-level roles.\n    # Verify that the metadata store contains the metadata of only these four\n    # roles before updating the metadata of 'targets.json'.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Test: normal case.  Update 'targets.json'.  The version number should not\n    # change.\n    self.repository_updater._update_metadata_if_changed('targets')\n\n    # Verify the current version of 'targets.json' has not changed.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = 'file3.txt'\n\n    repository.targets.add_target(target3)\n    repository.root.version = repository.root.version + 1\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update 'targets.json' and verify that the client's current 'targets.json'\n    # has been updated.  'timestamp' and 'snapshot' must be manually updated\n    # so that new 'targets' can be recognized.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    self.repository_updater._update_metadata('timestamp', DEFAULT_TIMESTAMP_FILELENGTH)\n    self.repository_updater._update_metadata_if_changed('snapshot', 'timestamp')\n    self.repository_updater._update_metadata_if_changed('targets')\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertTrue(self.repository_updater.metadata['current']['targets'])\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 2)\n\n    # Test for an invalid 'referenced_metadata' argument.\n    self.assertRaises(tuf.exceptions.RepositoryError,\n        self.repository_updater._update_metadata_if_changed, 'snapshot', 'bad_role')\n\n\n\n  def test_3__targets_of_role(self):\n    # Setup.\n    # Extract the list of targets from 'targets.json', to be compared to what\n    # is returned by _targets_of_role('targets').\n    targets_in_metadata = \\\n      self.repository_updater.metadata['current']['targets']['targets']\n\n    # Test: normal case.\n    targetinfos_list = self.repository_updater._targets_of_role('targets')\n\n    # Verify that the list of targets was returned, and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos_list))\n    for targetinfo in targetinfos_list:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in targets_in_metadata.items())\n\n\n\n\n\n  def test_4_refresh(self):\n    # This unit test is based on adding an extra target file to the\n    # server and rebuilding all server-side metadata.  All top-level metadata\n    # should be updated when the client calls refresh().\n\n    # First verify that an expired root metadata is updated.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.repository_updater.refresh()\n\n    # Second, verify that expired root metadata is not updated if\n    # 'unsafely_update_root_if_necessary' is explicitly set to 'False'.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater.refresh,\n                      unsafely_update_root_if_necessary=False)\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = 'file3.txt'\n\n    repository.targets.add_target(target3)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Reference 'self.Repository.metadata['current']['targets']'.  Ensure\n    # 'target3' is not already specified.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    self.assertFalse(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the roles to be modified.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 1)\n\n    # Test: normal case.  'targes.json' should now specify 'target3', and the\n    # following top-level metadata should have also been updated:\n    # 'snapshot.json' and 'timestamp.json'.\n    self.repository_updater.refresh()\n\n    # Verify that the client's metadata was updated.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    self.assertTrue(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the updated roles.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 2)\n\n\n\n\n\n  def test_4__refresh_targets_metadata(self):\n    # Setup.\n    # It is assumed that the client repository has only loaded the top-level\n    # metadata.  Refresh the 'targets.json' metadata, including all delegated\n    # roles (i.e., the client should add the missing 'role1.json' metadata.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n\n    # Test: normal case.\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n    # Verify that client's metadata files were refreshed successfully.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 6)\n\n    # Test for non-existing rolename.\n    self.repository_updater._refresh_targets_metadata('bad_rolename',\n        refresh_all_delegated_roles=False)\n\n    # Test that non-json metadata in Snapshot is ignored.\n    self.repository_updater.metadata['current']['snapshot']['meta']['bad_role.xml'] = {}\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n\n\n  def test_5_all_targets(self):\n   # Setup\n   # As with '_refresh_targets_metadata()',\n\n   # Update top-level metadata before calling one of the \"targets\" methods, as\n   # recommended by 'updater.py'.\n   self.repository_updater.refresh()\n\n   # Test: normal case.\n   with utils.ignore_deprecation_warnings('tuf.client.updater'):\n    all_targets = self.repository_updater.all_targets()\n\n   # Verify format of 'all_targets', it should correspond to\n   # 'TARGETINFOS_SCHEMA'.\n   self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(all_targets))\n\n   # Verify that there is a correct number of records in 'all_targets' list,\n   # and the expected filepaths specified in the metadata.  On the targets\n   # directory of the repository, there should be 3 target files (2 of\n   # which are specified by 'targets.json'.)  The delegated role 'role1'\n   # specifies 1 target file.  The expected total number targets in\n   # 'all_targets' should be 3.\n   self.assertEqual(len(all_targets), 3)\n\n   target_filepaths = []\n   for target in all_targets:\n    target_filepaths.append(target['filepath'])\n\n   self.assertTrue('file1.txt' in target_filepaths)\n   self.assertTrue('file2.txt' in target_filepaths)\n   self.assertTrue('file3.txt' in target_filepaths)\n\n\n\n\n\n  def test_5_targets_of_role(self):\n    # Setup\n    # Remove knowledge of 'targets.json' from the metadata store.\n    self.repository_updater.metadata['current']['targets']\n\n    # Remove the metadata of the delegated roles.\n    #shutil.rmtree(os.path.join(self.client_metadata, 'targets'))\n    os.remove(os.path.join(self.client_metadata_current, 'targets.json'))\n\n    # Extract the target files specified by the delegated role, 'role1.json',\n    # as available on the server-side version of the role.\n    role1_filepath = os.path.join(self.repository_directory, 'metadata',\n                                  'role1.json')\n    role1_signable = securesystemslib.util.load_json_file(role1_filepath)\n    expected_targets = role1_signable['signed']['targets']\n\n\n    # Test: normal case.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      targetinfos = self.repository_updater.targets_of_role('role1')\n\n    # Verify that the expected role files were downloaded and installed.\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets.json'))\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets',\n                   'role1.json'))\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    self.assertTrue('role1' in self.repository_updater.metadata['current'])\n\n    #  Verify that list of targets was returned and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos))\n    for targetinfo in targetinfos:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in expected_targets.items())\n\n    # Test: Invalid arguments.\n    # targets_of_role() expected a string rolename.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater.targets_of_role,\n                        8)\n      self.assertRaises(tuf.exceptions.UnknownRoleError, self.repository_updater.targets_of_role,\n                        'unknown_rolename')\n\n\n\n\n\n  def test_6_get_one_valid_targetinfo(self):\n    # Setup\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Extract the file information of the targets specified in 'targets.json'.\n    self.repository_updater.refresh()\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n\n    target_files = targets_metadata['targets']\n    # Extract random target from 'target_files', which will be compared to what\n    # is returned by get_one_valid_targetinfo().  Restore the popped target\n    # (dict value stored in the metadata store) so that it can be found later.\n    filepath, fileinfo = target_files.popitem()\n    target_files[filepath] = fileinfo\n\n    target_targetinfo = self.repository_updater.get_one_valid_targetinfo(filepath)\n    self.assertTrue(tuf.formats.TARGETINFO_SCHEMA.matches(target_targetinfo))\n    self.assertEqual(target_targetinfo['filepath'], filepath)\n    self.assertEqual(target_targetinfo['fileinfo'], fileinfo)\n\n    # Test: invalid target path.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        self.repository_updater.get_one_valid_targetinfo,\n        self.random_path().lstrip(os.sep).lstrip('/'))\n\n    # Test updater.get_one_valid_targetinfo() backtracking behavior (enabled by\n    # default.)\n    targets_directory = os.path.join(self.repository_directory, 'targets')\n    os.makedirs(os.path.join(targets_directory, 'foo'))\n\n    foo_package = 'foo/foo1.1.tar.gz'\n    foo_pattern = 'foo/foo*.tar.gz'\n\n    foo_fullpath = os.path.join(targets_directory, foo_package)\n    with open(foo_fullpath, 'wb') as file_object:\n      file_object.write(b'new release')\n\n    # Modify delegations on the remote repository to test backtracking behavior.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n    repository.targets('role4').add_target(foo_package)\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # updater.get_one_valid_targetinfo() should find 'foo1.1.tar.gz' by\n    # backtracking to 'role3'.  'role2' allows backtracking.\n    self.repository_updater.refresh()\n    self.repository_updater.get_one_valid_targetinfo('foo/foo1.1.tar.gz')\n\n    # A leading path separator is disallowed.\n    self.assertRaises(tuf.exceptions.FormatError,\n    self.repository_updater.get_one_valid_targetinfo, '/foo/foo1.1.tar.gz')\n\n    # Test when 'role2' does *not* allow backtracking.  If 'foo/foo1.1.tar.gz'\n    # is not provided by the authoritative 'role2',\n    # updater.get_one_valid_targetinfo() should return a\n    # 'tuf.exceptions.UnknownTargetError' exception.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    repository.targets.revoke('role3')\n    repository.targets.revoke('role4')\n\n    # Ensure we delegate in trusted order (i.e., 'role2' has higher priority.)\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern], terminating=True, list_of_targets=[])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Verify that 'tuf.exceptions.UnknownTargetError' is raised by\n    # updater.get_one_valid_targetinfo().\n    self.repository_updater.refresh()\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n                      self.repository_updater.get_one_valid_targetinfo,\n                      'foo/foo1.1.tar.gz')\n\n    # Verify that a 'tuf.exceptions.FormatError' is raised for delegated paths\n    # that contain a leading path separator.\n    self.assertRaises(tuf.exceptions.FormatError,\n        self.repository_updater.get_one_valid_targetinfo,\n        '/foo/foo1.1.tar.gz')\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n\n  def test_6_download_target(self):\n    # Create temporary directory (destination directory of downloaded targets)\n    # that will be passed as an argument to 'download_target()'.\n    destination_directory = self.make_temp_directory()\n    target_filepaths = \\\n      list(self.repository_updater.metadata['current']['targets']['targets'].keys())\n\n    # Test: normal case.\n    # Get the target info, which is an argument to 'download_target()'.\n\n    # 'target_filepaths' is expected to have at least two targets.  The first\n    # target will be used to test against download_target().  The second\n    # will be used to test against download_target() and a repository with\n    # 'consistent_snapshot' set to True.\n    target_filepath1 = target_filepaths.pop()\n    targetinfo = self.repository_updater.get_one_valid_targetinfo(target_filepath1)\n    self.repository_updater.download_target(targetinfo,\n                                            destination_directory)\n\n    download_filepath = \\\n      os.path.join(destination_directory, target_filepath1.lstrip('/'))\n    self.assertTrue(os.path.exists(download_filepath))\n    length, hashes = \\\n      securesystemslib.util.get_file_details(download_filepath,\n        securesystemslib.settings.HASH_ALGORITHMS)\n    download_targetfileinfo = tuf.formats.make_targets_fileinfo(length, hashes)\n\n    # Add any 'custom' data from the repository's target fileinfo to the\n    # 'download_targetfileinfo' object being tested.\n    if 'custom' in targetinfo['fileinfo']:\n      download_targetfileinfo['custom'] = targetinfo['fileinfo']['custom']\n\n    self.assertEqual(targetinfo['fileinfo'], download_targetfileinfo)\n\n    # Test when consistent snapshots is set.  First, create a valid\n    # repository with consistent snapshots set (root.json contains a\n    # \"consistent_snapshot\" entry that the updater uses to correctly fetch\n    # snapshots.  The updater expects the existence of\n    # '<version_number>.filename' files if root.json sets 'consistent_snapshot\n    # = True'.\n\n    # The repository must be rewritten with 'consistent_snapshot' set.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    # Write metadata for all the top-level roles , since consistent snapshot\n    # is now being set to true (i.e., the pre-generated repository isn't set\n    # to support consistent snapshots.  A new version of targets.json is needed\n    # to ensure <digest>.filename target files are written to disk.\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n\n    repository.writeall(consistent_snapshot=True)\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # And ensure the client has the latest top-level metadata.\n    self.repository_updater.refresh()\n\n    target_filepath2 = target_filepaths.pop()\n    targetinfo2 = self.repository_updater.get_one_valid_targetinfo(target_filepath2)\n    self.repository_updater.download_target(targetinfo2,\n                                            destination_directory)\n\n    # Checks if the file has been successfully downloaded\n    download_filepath = os.path.join(destination_directory, target_filepath2)\n    self.assertTrue(os.path.exists(download_filepath))\n\n    # Removes the file so that it can be downloaded again in the next test\n    os.remove(download_filepath)\n\n    # Test downloading with consistent snapshot enabled, but without adding\n    # the hash of the file as a prefix to its name.\n\n    file1_path = targetinfo2['filepath']\n    file1_hashes = securesystemslib.util.get_file_hashes(\n        os.path.join(self.repository_directory, 'targets', file1_path),\n        hash_algorithms=['sha256', 'sha512'])\n\n    # Currently in the repository directory, those three files exists:\n    # \"file1.txt\", \"<sha256_hash>.file1.txt\" and \"<sha512_hash>.file1.txt\"\n    # where both sha256 and sha512 hashes are for file file1.txt.\n    # Remove the files with the hash digest prefix to ensure that\n    # the served target file is not prefixed.\n    os.remove(os.path.join(self.repository_directory, 'targets',\n        file1_hashes['sha256'] + '.' + file1_path))\n    os.remove(os.path.join(self.repository_directory, 'targets',\n        file1_hashes['sha512'] + '.' + file1_path))\n\n\n    self.repository_updater.download_target(targetinfo2,\n                                            destination_directory,\n                                            prefix_filename_with_hash=False)\n\n    # Checks if the file has been successfully downloaded\n    self.assertTrue(os.path.exists(download_filepath))\n\n    # Test for a destination that cannot be written to (apart from a target\n    # file that already exists at the destination) and which raises an\n    # exception.\n    bad_destination_directory = 'bad' * 2000\n\n    try:\n      self.repository_updater.download_target(targetinfo, bad_destination_directory)\n\n    except OSError as e:\n      self.assertTrue(\n          e.errno in [errno.ENAMETOOLONG, errno.ENOENT, errno.EINVAL],\n          \"wrong errno: \" + str(e.errno))\n\n    else:\n      self.fail('No OSError raised')\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      targetinfo, 8)\n\n    # Test:\n    # Attempt a file download of a valid target, however, a download exception\n    # occurs because the target is not within the mirror's confined target\n    # directories.  Adjust mirrors dictionary, so that 'confined_target_dirs'\n    # field contains at least one confined target and excludes needed target\n    # file.\n    mirrors = self.repository_updater.mirrors\n    for mirror_name, mirror_info in mirrors.items():\n      mirrors[mirror_name]['confined_target_dirs'] = [self.random_path()]\n\n    try:\n      self.repository_updater.download_target(targetinfo,\n                                              destination_directory)\n\n    except tuf.exceptions.NoWorkingMirrorError as exception:\n      # Ensure that no mirrors were found due to mismatch in confined target\n      # directories.  get_list_of_mirrors() returns an empty list in this case,\n      # which does not generate specific exception errors.\n      self.assertEqual(len(exception.mirror_errors), 0)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError with zero mirror errors in it.')\n\n\n\n\n\n  def test_7_updated_targets(self):\n    # Verify that the list of targets returned by updated_targets() contains\n    # all the files that need to be updated, these files include modified and\n    # new target files.  Also, confirm that files that need not to be updated\n    # are absent from the list.\n    # Setup\n\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory which will hold client's target files.\n    destination_directory = self.make_temp_directory()\n\n    # Get the list of target files.  It will be used as an argument to the\n    # 'updated_targets()' function.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    # Test for duplicates and targets in the root directory of the repository.\n    additional_target = all_targets[0].copy()\n    all_targets.append(additional_target)\n    additional_target_in_root_directory = additional_target.copy()\n    additional_target_in_root_directory['filepath'] = 'file1.txt'\n    all_targets.append(additional_target_in_root_directory)\n\n    #  At this point client needs to update and download all targets.\n    # Test: normal cases.\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    # Assumed the pre-generated repository specifies two target files in\n    # 'targets.json' and one delegated target file in 'role1.json'.\n    self.assertEqual(len(updated_targets), 3)\n\n    # Test: download one of the targets.\n    download_target = copy.deepcopy(updated_targets).pop()\n    self.repository_updater.download_target(download_target,\n                                            destination_directory)\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 2)\n\n    # Test: download all the targets.\n    for download_target in all_targets:\n      self.repository_updater.download_target(download_target,\n                                              destination_directory)\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 0)\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      all_targets, 8)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Ensure the client has up-to-date metadata.\n    self.repository_updater.refresh()\n\n    # Verify that the new target file is considered updated.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n    self.assertEqual(len(updated_targets), 1)\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n\n  def test_8_remove_obsolete_targets(self):\n    # Setup.\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory that will hold the client's target files.\n    destination_directory = self.make_temp_directory()\n\n    #  Populate 'destination_direction' with all target files.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    self.assertEqual(len(os.listdir(destination_directory)), 0)\n\n    for target in all_targets:\n      self.repository_updater.download_target(target, destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n\n    # Remove two target files from the server's repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update client's metadata.\n    self.repository_updater.refresh()\n\n    # Test: normal case.\n    # Verify number of target files in 'destination_directory' (should be 1\n    # after the update made to the remote repository), and call\n    # 'remove_obsolete_targets()'.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets,\n                                              destination_directory)\n\n    for updated_target in updated_targets:\n      self.repository_updater.download_target(updated_target,\n                                              destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    #  Verify that, if there are no obsolete files, the number of files\n    #  in 'destination_directory' remains the same.\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    # Test coverage for a destination path that causes an exception not due\n    # to an already removed target.\n    bad_destination_directory = 'bad' * 2000\n    self.repository_updater.remove_obsolete_targets(bad_destination_directory)\n\n    # Test coverage for a target that is not specified in current metadata.\n    del self.repository_updater.metadata['current']['targets']['targets']['file2.txt']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    # Test coverage for a role that doesn't exist in the previously trusted set\n    # of metadata.\n    del self.repository_updater.metadata['previous']['targets']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n  def test_9__get_target_hash(self):\n    # Test normal case.\n    # Test target filepaths with ascii and non-ascii characters.\n    expected_target_hashes = {\n      '/file1.txt': 'e3a3d89eb3b70ce3fbce6017d7b8c12d4abd5635427a0e8a238f53157df85b3d',\n      '/Jalape\\xc3\\xb1o': '78bfd5c314680545eb48ecad508aceb861f8d6e680f4fe1b791da45c298cda88'\n    }\n    for filepath, target_hash in expected_target_hashes.items():\n      self.assertTrue(tuf.formats.RELPATH_SCHEMA.matches(filepath))\n      self.assertTrue(securesystemslib.formats.HASH_SCHEMA.matches(target_hash))\n      self.assertEqual(self.repository_updater._get_target_hash(filepath), target_hash)\n\n    # Test for improperly formatted argument.\n    #self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._get_target_hash, 8)\n\n\n\n\n  def test_10__check_file_length(self):\n    # Test for exception if file object is not equal to trusted file length.\n    with tempfile.TemporaryFile() as temp_file_object:\n      temp_file_object.write(b'X')\n      temp_file_object.seek(0)\n      self.assertRaises(tuf.exceptions.DownloadLengthMismatchError,\n                      self.repository_updater._check_file_length,\n                      temp_file_object, 10)\n\n\n\n\n\n  def test_10__targets_of_role(self):\n    # Test for non-existent role.\n    self.assertRaises(tuf.exceptions.UnknownRoleError,\n                      self.repository_updater._targets_of_role,\n                      'non-existent-role')\n\n    # Test for role that hasn't been loaded yet.\n    del self.repository_updater.metadata['current']['targets']\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets',\n                                                        skip_refresh=True)), 0)\n\n    # 'targets.json' tracks two targets.\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets')),\n                     2)\n\n\n\n  def test_10__preorder_depth_first_walk(self):\n\n    # Test that infinite loop is prevented if the target file is not found and\n    # the max number of delegations is reached.\n    valid_max_number_of_delegations = tuf.settings.MAX_NUMBER_OF_DELEGATIONS\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = 0\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('unknown.txt'))\n\n    # Reset the setting for max number of delegations so that subsequent unit\n    # tests reference the expected setting.\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = valid_max_number_of_delegations\n\n    # Attempt to create a circular delegation, where role1 performs a\n    # delegation to the top-level Targets role.  The updater should ignore the\n    # delegation and not raise an exception.\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    targets_metadata = securesystemslib.util.load_json_file(targets_path)\n    targets_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(targets_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(targets_metadata))\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['name'] = 'targets'\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    role2_path = os.path.join(self.client_metadata_current, 'role2.json')\n    role2_metadata = securesystemslib.util.load_json_file(role2_path)\n    role2_metadata['signed']['delegations']['roles'] = role1_metadata['signed']['delegations']['roles']\n    role2_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role2_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role2_metadata))\n\n    logger.debug('attempting circular delegation')\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('/file8.txt'))\n\n\n\n\n\n\n  def test_10__visit_child_role(self):\n    # Call _visit_child_role and test the dict keys: 'paths',\n    # 'path_hash_prefixes', and if both are missing.\n\n    targets_role = self.repository_updater.metadata['current']['targets']\n    targets_role['delegations']['roles'][0]['paths'] = ['/*.txt', '/target.exe']\n    child_role = targets_role['delegations']['roles'][0]\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/*.exe']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/target.exe'), child_role['name'])\n\n    # Test for a valid path hash prefix...\n    child_role['path_hash_prefixes'] = ['8baf']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), child_role['name'])\n\n    # ... and an invalid one, as well.\n    child_role['path_hash_prefixes'] = ['badd']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), None)\n\n    # Test for a forbidden target.\n    del child_role['path_hash_prefixes']\n    self.repository_updater._visit_child_role(child_role, '/forbidden.tgz')\n\n    # Verify that unequal path_hash_prefixes are skipped.\n    child_role['path_hash_prefixes'] = ['bad', 'bad']\n    self.assertEqual(None, self.repository_updater._visit_child_role(child_role,\n        '/unknown.exe'))\n\n    # Test if both 'path' and 'path_hash_prefixes' are missing.\n    del child_role['paths']\n    del child_role['path_hash_prefixes']\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._visit_child_role,\n        child_role, child_role['name'])\n\n\n\n  def test_11__verify_metadata_file(self):\n    # Test for invalid metadata content.\n    with tempfile.TemporaryFile() as metadata_file_object:\n      metadata_file_object.write(b'X')\n      metadata_file_object.seek(0)\n\n      self.assertRaises(tuf.exceptions.InvalidMetadataJSONError,\n          self.repository_updater._verify_metadata_file,\n          metadata_file_object, 'root')\n\n\n  def test_13__targets_of_role(self):\n    # Test case where a list of targets is given.  By default, the 'targets'\n    # parameter is None.\n    targets = [{'filepath': 'file1.txt', 'fileinfo': {'length': 1, 'hashes': {'sha256': 'abc'}}}]\n    self.repository_updater._targets_of_role('targets',\n        targets=targets, skip_refresh=False)\n\n\n\n\nclass TestMultiRepoUpdater(unittest_toolbox.Modified_TestCase):\n\n  def setUp(self):\n    # Modified_Testcase can handle temp dir removal\n    unittest_toolbox.Modified_TestCase.setUp(self)\n    self.temporary_directory = self.make_temp_directory(directory=os.getcwd())\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf/tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n\n    self.temporary_repository_root = tempfile.mkdtemp(dir=self.temporary_directory)\n\n    # Needed because in some tests simple_server.py cannot be found.\n    # The reason is that the current working directory\n    # has been changed when executing a subprocess.\n    self.SIMPLE_SERVER_PATH = os.path.join(os.getcwd(), 'simple_server.py')\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_client = os.path.join(original_repository_files, 'client', 'test_repository1')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_map_file = os.path.join(original_repository_files, 'map.json')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = os.path.join(self.temporary_repository_root,\n        'repository_server1')\n    self.repository_directory2 = os.path.join(self.temporary_repository_root,\n        'repository_server2')\n\n    # Setting 'tuf.settings.repositories_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.temporary_repository_root\n\n    repository_name = 'test_repository1'\n    repository_name2 = 'test_repository2'\n\n    self.client_directory = os.path.join(self.temporary_repository_root,\n        repository_name)\n    self.client_directory2 = os.path.join(self.temporary_repository_root,\n        repository_name2)\n\n    self.keystore_directory = os.path.join(self.temporary_repository_root,\n        'keystore')\n    self.map_file = os.path.join(self.client_directory, 'map.json')\n    self.map_file2 = os.path.join(self.client_directory2, 'map.json')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_repository, self.repository_directory2)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_client, self.client_directory2)\n    shutil.copyfile(original_map_file, self.map_file)\n    shutil.copyfile(original_map_file, self.map_file2)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served by the\n    # SimpleHTTPServer launched here.  The test cases of this unit test assume\n    # the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n\n    # Creates a subprocess running a server.\n    self.server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH, popen_cwd=self.repository_directory)\n\n    logger.debug('Server process started.')\n\n    # Creates a subprocess running a server.\n    self.server_process_handler2 = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH, popen_cwd=self.repository_directory2)\n\n    logger.debug('Server process 2 started.')\n\n    url_prefix = \\\n        'http://' + utils.TEST_HOST_ADDRESS + ':' + \\\n        str(self.server_process_handler.port)\n    url_prefix2 = \\\n        'http://' + utils.TEST_HOST_ADDRESS + ':' + \\\n        str(self.server_process_handler2.port)\n\n    # We have all of the necessary information for two repository mirrors\n    # in map.json, except for url prefixes.\n    # For the url prefixes, we create subprocesses that run a server script.\n    # In server scripts we get a free port from the OS which is sent\n    # back to the parent process.\n    # That's why we dynamically add the ports to the url prefixes\n    # and changing the content of map.json.\n    self.map_file_path = os.path.join(self.client_directory, 'map.json')\n    data = securesystemslib.util.load_json_file(self.map_file_path)\n\n    data['repositories']['test_repository1'] = [url_prefix]\n    data['repositories']['test_repository2'] = [url_prefix2]\n    with open(self.map_file_path, 'w') as f:\n      json.dump(data, f)\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    self.repository_mirrors2 = {'mirror1': {'url_prefix': url_prefix2,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Create the repository instances.  The test cases will use these client\n    # updaters to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(repository_name,\n        self.repository_mirrors)\n    self.repository_updater2 = updater.Updater(repository_name2,\n        self.repository_mirrors2)\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.multi_repo_updater = updater.MultiRepoUpdater(self.map_file)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n\n    # Cleans the resources and flush the logged lines (if any).\n    self.server_process_handler.clean()\n    self.server_process_handler2.clean()\n\n    # updater.Updater() populates the roledb with the name \"test_repository1\"\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    # Remove top-level temporary directory\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n\n\n  # UNIT TESTS.\n  def test__init__(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, 8)\n\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n    # Test for a non-existent map file.\n    self.assertRaises(tuf.exceptions.Error, updater.MultiRepoUpdater,\n        'non-existent.json')\n\n    # Test for a map file that doesn't contain the required fields.\n    root_filepath = os.path.join(\n        self.repository_directory, 'metadata', 'root.json')\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, root_filepath)\n\n    # Test for a valid instantiation.\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n\n\n  def test__target_matches_path_pattern(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n    paths = ['foo*.tgz', 'bar*.tgz', 'file1.txt']\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('bar-1.0.tgz', paths))\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('file1.txt', paths))\n    self.assertFalse(\n        multi_repo_updater._target_matches_path_pattern('baz-1.0.tgz', paths))\n\n\n\n  def test_get_valid_targetinfo(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n    # Verify the multi repo updater refuses to save targetinfo if\n    # required local repositories are missing.\n    repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1')\n    backup_repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1.backup')\n    shutil.move(repo_dir, backup_repo_dir)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the client's repository directory.\n    shutil.move(backup_repo_dir, repo_dir)\n\n    # Verify that the Root file must exist.\n    root_filepath = os.path.join(repo_dir, 'metadata', 'current', 'root.json')\n    backup_root_filepath = os.path.join(root_filepath, root_filepath + '.backup')\n    shutil.move(root_filepath, backup_root_filepath)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the Root file.\n    shutil.move(backup_root_filepath, root_filepath)\n\n    # Test that the first mapping is skipped if it's irrelevant to the target\n    # file.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n\n    # Verify that a targetinfo is not returned for a non-existent target.\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = False\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = True\n\n    # Test for a mapping that sets terminating = True, and that appears before\n    # the final mapping.\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = True\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'bad3.txt')\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = False\n\n    # Test for the case where multiple repos sign for the same target.\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    # Verify that valid targetinfo is matched for two repositories that provide\n    # different custom field.  Make sure to set the 'match_custom_field'\n    # argument to 'False' when calling get_valid_targetinfo().\n    repository = repo_tool.load_repository(self.repository_directory2)\n\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    custom_field = {\"custom\": \"my_custom_data\"}\n    repository.targets.add_target(os.path.basename(target1), custom_field)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Do we get the expected match for the two targetinfo that only differ\n    # by the custom field?\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo(\n        'file1.txt', match_custom_field=False)\n\n    # Verify the case where two repositories provide different targetinfo.\n    # Modify file1.txt so that different length and hashes are reported by the\n    # two repositories.\n    repository = repo_tool.load_repository(self.repository_directory2)\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Ensure the threshold is modified to 2 (assumed to be 1, by default) and\n    # verify that get_valid_targetinfo() raises an UnknownTargetError\n    # despite both repos signing for file1.txt.\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'file1.txt')\n\n\n\n\n\n  def test_get_updater(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n    # Test for a non-existent repository name.\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n\n    # Test get_updater indirectly via the \"private\" _update_from_repository().\n    self.assertRaises(tuf.exceptions.Error, multi_repo_updater._update_from_repository, 'bad_repo_name', 'file3.txt')\n\n    # Test for a repository that doesn't exist.\n    multi_repo_updater.map_file['repositories']['bad_repo_name'] = ['https://bogus:30002']\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n\n\nclass TestUpdaterRolenames(unittest_toolbox.Modified_TestCase):\n  def setUp(self):\n    unittest_toolbox.Modified_TestCase.setUp(self)\n\n    repo_dir = os.path.join(os.getcwd(), 'repository_data', 'fishy_rolenames')\n\n    self.client_dir = self.make_temp_directory()\n    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"))\n    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"previous\"))\n    shutil.copy(\n      os.path.join(repo_dir, 'metadata', '1.root.json'),\n      os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\", \"root.json\")\n    )\n\n    simple_server_path = os.path.join(os.getcwd(), 'simple_server.py')\n    self.server_process_handler = utils.TestServerProcess(log=logger,\n        server=simple_server_path)\n\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + \"/repository_data/fishy_rolenames\"\n\n    tuf.settings.repositories_directory = self.client_dir\n    mirrors = {'mirror1': {\n      'url_prefix': url_prefix,\n      'metadata_path': 'metadata/',\n      'targets_path': ''\n    }}\n    self.updater = updater.Updater(\"fishy_rolenames\", mirrors)\n\n  def tearDown(self):\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n    self.server_process_handler.flush_log()\n    self.server_process_handler.clean()\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n  def test_unusual_rolenames(self):\n    \"\"\"Test rolenames that may be tricky to handle as filenames\n\n    The test data in repository_data/fishy_rolenames has been produced\n    semi-manually using RepositorySimulator: using the RepositorySimulator\n    in these tests directly (like test_updater_with_simulator.py does for\n    ngclient) might make more sense... but would require some integration work\n    \"\"\"\n\n    # Make a target search that fetches the delegated targets\n    self.updater.refresh()\n    with self.assertRaises(tuf.exceptions.UnknownTargetError):\n      self.updater.get_one_valid_targetinfo(\"anything\")\n\n    # Assert that the metadata files are in the client metadata directory\n    metadata_dir = os.path.join(\n      self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"\n    )\n    local_metadata = os.listdir(metadata_dir)\n    for fname in ['%C3%B6.json', '..%2Fa.json', '..json']:\n      self.assertTrue(fname in local_metadata)\n\n\ndef _load_role_keys(keystore_directory):\n\n  # Populating 'self.role_keys' by importing the required public and private\n  # keys of 'tuf/tests/repository_data/'.  The role keys are needed when\n  # modifying the remote repository used by the test cases in this unit test.\n\n  # The pre-generated key files in 'repository_data/keystore' are all encrypted with\n  # a 'password' passphrase.\n  EXPECTED_KEYFILE_PASSWORD = 'password'\n\n  # Store and return the cryptography keys of the top-level roles, including 1\n  # delegated role.\n  role_keys = {}\n\n  root_key_file = os.path.join(keystore_directory, 'root_key')\n  targets_key_file = os.path.join(keystore_directory, 'targets_key')\n  snapshot_key_file = os.path.join(keystore_directory, 'snapshot_key')\n  timestamp_key_file = os.path.join(keystore_directory, 'timestamp_key')\n  delegation_key_file = os.path.join(keystore_directory, 'delegation_key')\n\n  role_keys = {'root': {}, 'targets': {}, 'snapshot': {}, 'timestamp': {},\n               'role1': {}}\n\n  # Import the top-level and delegated role public keys.\n  role_keys['root']['public'] = \\\n    repo_tool.import_rsa_publickey_from_file(root_key_file+'.pub')\n  role_keys['targets']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(targets_key_file+'.pub')\n  role_keys['snapshot']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(snapshot_key_file+'.pub')\n  role_keys['timestamp']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(timestamp_key_file+'.pub')\n  role_keys['role1']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(delegation_key_file+'.pub')\n\n  # Import the private keys of the top-level and delegated roles.\n  role_keys['root']['private'] = \\\n    repo_tool.import_rsa_privatekey_from_file(root_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['targets']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(targets_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['snapshot']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(snapshot_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['timestamp']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(timestamp_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['role1']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(delegation_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n\n  return role_keys\n\n\nif __name__ == '__main__':\n  utils.configure_test_logging(sys.argv)\n  unittest.main()\n", "code_before": "#!/usr/bin/env python\n\n# Copyright 2012 - 2017, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\n<Program Name>\n  test_updater.py\n\n<Author>\n  Konstantin Andrianov.\n\n<Started>\n  October 15, 2012.\n\n  March 11, 2014.\n    Refactored to remove mocked modules and old repository tool dependence, use\n    exact repositories, and add realistic retrieval of files. -vladimir.v.diaz\n\n<Copyright>\n  See LICENSE-MIT OR LICENSE for licensing information.\n\n<Purpose>\n  'test_updater.py' provides a collection of methods that test the public /\n  non-public methods and functions of 'tuf.client.updater.py'.\n\n  The 'unittest_toolbox.py' module was created to provide additional testing\n  tools, such as automatically deleting temporary files created in test cases.\n  For more information, see 'tests/unittest_toolbox.py'.\n\n<Methodology>\n  Test cases here should follow a specific order (i.e., independent methods are\n  tested before dependent methods). More accurately, least dependent methods\n  are tested before most dependent methods.  There is no reason to rewrite or\n  construct other methods that replicate already-tested methods solely for\n  testing purposes.  This is possible because the 'unittest.TestCase' class\n  guarantees the order of unit tests.  The 'test_something_A' method would\n  be tested before 'test_something_B'.  To ensure the expected order of tests,\n  a number is placed after 'test' and before methods name like so:\n  'test_1_check_directory'.  The number is a measure of dependence, where 1 is\n  less dependent than 2.\n\"\"\"\n\nimport os\nimport time\nimport shutil\nimport copy\nimport tempfile\nimport logging\nimport errno\nimport sys\nimport unittest\nimport json\nimport unittest.mock as mock\n\nimport tuf\nimport tuf.exceptions\nimport tuf.log\nimport tuf.formats\nimport tuf.keydb\nimport tuf.roledb\nimport tuf.repository_tool as repo_tool\nimport tuf.repository_lib as repo_lib\nimport tuf.unittest_toolbox as unittest_toolbox\nimport tuf.client.updater as updater\n\nfrom tests import utils\n\nimport securesystemslib\n\nlogger = logging.getLogger(__name__)\nrepo_tool.disable_console_log_messages()\n\n\nclass TestUpdater(unittest_toolbox.Modified_TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    # Create a temporary directory to store the repository, metadata, and target\n    # files.  'temporary_directory' must be deleted in TearDownModule() so that\n    # temporary files are always removed, even when exceptions occur.\n    cls.temporary_directory = tempfile.mkdtemp(dir=os.getcwd())\n\n    # Needed because in some tests simple_server.py cannot be found.\n    # The reason is that the current working directory\n    # has been changed when executing a subprocess.\n    cls.SIMPLE_SERVER_PATH = os.path.join(os.getcwd(), 'simple_server.py')\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served\n    # by the SimpleHTTPServer launched here.  The test cases of 'test_updater.py'\n    # assume the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n    cls.server_process_handler = utils.TestServerProcess(log=logger,\n        server=cls.SIMPLE_SERVER_PATH)\n\n\n\n  @classmethod\n  def tearDownClass(cls):\n    # Cleans the resources and flush the logged lines (if any).\n    cls.server_process_handler.clean()\n\n    # Remove the temporary repository directory, which should contain all the\n    # metadata, targets, and key files generated for the test cases\n    shutil.rmtree(cls.temporary_directory)\n\n\n\n  def setUp(self):\n    # We are inheriting from custom class.\n    unittest_toolbox.Modified_TestCase.setUp(self)\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    self.repository_name = 'test_repository1'\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf.tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n    temporary_repository_root = \\\n      self.make_temp_directory(directory=self.temporary_directory)\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_client = os.path.join(original_repository_files, 'client')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = \\\n      os.path.join(temporary_repository_root, 'repository')\n    self.keystore_directory = \\\n      os.path.join(temporary_repository_root, 'keystore')\n\n    self.client_directory = os.path.join(temporary_repository_root,\n        'client')\n    self.client_metadata = os.path.join(self.client_directory,\n        self.repository_name, 'metadata')\n    self.client_metadata_current = os.path.join(self.client_metadata,\n        'current')\n    self.client_metadata_previous = os.path.join(self.client_metadata,\n        'previous')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n                                           'metadata_path': 'metadata',\n                                           'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n                                              self.repository_mirrors)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    # Logs stdout and stderr from the sever subprocess.\n    self.server_process_handler.flush_log()\n\n    # Remove temporary directory\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n\n  # UNIT TESTS.\n\n  def test_1__init__exceptions(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, 8,\n                      self.repository_mirrors)\n\n    # Invalid 'repository_mirrors' argument.  'tuf.formats.MIRRORDICT_SCHEMA'\n    # expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, updater.Updater, 8)\n\n\n    # 'tuf.client.updater.py' requires that the client's repositories directory\n    # be configured in 'tuf.settings.py'.\n    tuf.settings.repositories_directory = None\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n    # Test: repository does not exist\n    self.assertRaises(tuf.exceptions.MissingLocalRepositoryError, updater.Updater,\n                      'test_non_existing_repository', self.repository_mirrors)\n\n    # Test: empty client repository (i.e., no metadata directory).\n    metadata_backup = self.client_metadata + '.backup'\n    shutil.move(self.client_metadata, metadata_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's metadata directory.\n    shutil.move(metadata_backup, self.client_metadata)\n\n\n    # Test: repository with only a '{repository_directory}/metadata' directory.\n    # (i.e., missing the required 'current' and 'previous' sub-directories).\n    current_backup = self.client_metadata_current + '.backup'\n    previous_backup = self.client_metadata_previous + '.backup'\n\n    shutil.move(self.client_metadata_current, current_backup)\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n\n    # Restore the client's previous directory.  The required 'current' directory\n    # is still missing.\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test: repository with only a '{repository_directory}/metadata/previous'\n    # directory.\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's current directory.\n    shutil.move(current_backup, self.client_metadata_current)\n\n    # Test: repository with a '{repository_directory}/metadata/current'\n    # directory, but the 'previous' directory is missing.\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test:  repository missing the required 'root.json' file.\n    client_root_file = os.path.join(self.client_metadata_current, 'root.json')\n    backup_root_file = client_root_file + '.backup'\n    shutil.move(client_root_file, backup_root_file)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's 'root.json file.\n    shutil.move(backup_root_file, client_root_file)\n\n    # Test: Normal 'tuf.client.updater.Updater' instantiation.\n    updater.Updater('test_repository1', self.repository_mirrors)\n\n\n\n\n\n  def test_1__load_metadata_from_file(self):\n\n    # Setup\n    # Get the 'role1.json' filepath.  Manually load the role metadata, and\n    # compare it against the loaded metadata by '_load_metadata_from_file()'.\n    role1_filepath = \\\n      os.path.join(self.client_metadata_current, 'role1.json')\n    role1_meta = securesystemslib.util.load_json_file(role1_filepath)\n\n    # Load the 'role1.json' file with _load_metadata_from_file, which should\n    # store the loaded metadata in the 'self.repository_updater.metadata'\n    # store.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n\n    # Verify that the correct number of metadata objects has been loaded\n    # (i.e., only the 'root.json' file should have been loaded.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Verify that the content of root metadata is valid.\n    self.assertEqual(self.repository_updater.metadata['current']['role1'],\n                     role1_meta['signed'])\n\n    # Verify that _load_metadata_from_file() doesn't raise an exception for\n    # improperly formatted metadata, and doesn't load the bad file.\n    with open(role1_filepath, 'ab') as file_object:\n      file_object.write(b'bad JSON data')\n\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Test if we fail gracefully if we can't deserialize a meta file\n    self.repository_updater._load_metadata_from_file('current', 'empty_file')\n    self.assertFalse('empty_file' in self.repository_updater.metadata['current'])\n\n    # Test invalid metadata set argument (must be either\n    # 'current' or 'previous'.)\n    self.assertRaises(securesystemslib.exceptions.Error,\n                      self.repository_updater._load_metadata_from_file,\n                      'bad_metadata_set', 'role1')\n\n\n\n\n  def test_1__rebuild_key_and_role_db(self):\n    # Setup\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_threshold = root_metadata['roles']['root']['threshold']\n    number_of_root_keys = len(root_metadata['keys'])\n\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # Ensure we add 2 to the number of root keys (actually, the number of root\n    # keys multiplied by the number of keyid hash algorithms), to include the\n    # delegated targets key (+1 for its sha512 keyid).  The delegated roles of\n    # 'targets.json' are also loaded when the repository object is\n    # instantiated.\n\n    self.assertEqual(number_of_root_keys + 1, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: normal case.\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # _rebuild_key_and_role_db() will only rebuild the keys and roles specified\n    # in the 'root.json' file, unlike __init__().  Instantiating an updater\n    # object calls both _rebuild_key_and_role_db() and _import_delegations().\n    self.assertEqual(number_of_root_keys, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: properly updated roledb and keydb dicts if the Root role changes.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_metadata['roles']['root']['threshold'] = 8\n    root_metadata['keys'].popitem()\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], 8)\n    self.assertEqual(number_of_root_keys - 1, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n\n\n\n  def test_1__update_versioninfo(self):\n    # Tests\n    # Verify that the 'self.versioninfo' dictionary is empty (it starts off\n    # empty and is only populated if _update_versioninfo() is called.\n    versioninfo_dict = self.repository_updater.versioninfo\n    self.assertEqual(len(versioninfo_dict), 0)\n\n    # Load the versioninfo of the top-level Targets role.  This action\n    # populates the 'self.versioninfo' dictionary.\n    self.repository_updater._update_versioninfo('targets.json')\n    self.assertEqual(len(versioninfo_dict), 1)\n    self.assertTrue(tuf.formats.FILEINFODICT_SCHEMA.matches(versioninfo_dict))\n\n    # The Snapshot role stores the version numbers of all the roles available\n    # on the repository.  Load Snapshot to extract Root's version number\n    # and compare it against the one loaded by 'self.repository_updater'.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    # Verify that the manually loaded version number of root.json matches\n    # the one loaded by the updater object.\n    self.assertTrue('targets.json' in versioninfo_dict)\n    self.assertEqual(versioninfo_dict['targets.json'], targets_versioninfo)\n\n    # Verify that 'self.versioninfo' is incremented if another role is updated.\n    self.repository_updater._update_versioninfo('role1.json')\n    self.assertEqual(len(versioninfo_dict), 2)\n\n    # Verify that 'self.versioninfo' is incremented if a non-existent role is\n    # requested, and has its versioninfo entry set to 'None'.\n    self.repository_updater._update_versioninfo('bad_role.json')\n    self.assertEqual(len(versioninfo_dict), 3)\n    self.assertEqual(versioninfo_dict['bad_role.json'], None)\n\n    # Verify that the versioninfo specified in Timestamp is used if the Snapshot\n    # role hasn't been downloaded yet.\n    del self.repository_updater.metadata['current']['snapshot']\n    #self.assertRaises(self.repository_updater._update_versioninfo('snapshot.json'))\n    self.repository_updater._update_versioninfo('snapshot.json')\n    self.assertEqual(versioninfo_dict['snapshot.json']['version'], 1)\n\n\n\n  def test_1__refresh_must_not_count_duplicate_keyids_towards_threshold(self):\n    # Update root threshold on the server repository and sign twice with 1 key\n    repository = repo_tool.load_repository(self.repository_directory)\n    repository.root.threshold = 2\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n\n    storage_backend = securesystemslib.storage.FilesystemBackend()\n    # The client uses the threshold from the previous root file to verify the\n    # new root. Thus we need to make two updates so that the threshold used for\n    # verification becomes 2. I.e. we bump the version, sign twice with the\n    # same key and write to disk '2.root.json' and '3.root.json'.\n    for version in [2, 3]:\n      repository.root.version = version\n      info = tuf.roledb.get_roleinfo(\"root\")\n      metadata = repo_lib.generate_root_metadata(\n          info[\"version\"], info[\"expires\"], False)\n      signed_metadata = repo_lib.sign_metadata(\n          metadata, info[\"keyids\"], \"root.json\", \"default\")\n      signed_metadata[\"signatures\"].append(signed_metadata[\"signatures\"][0])\n      live_root_path = os.path.join(\n          self.repository_directory, \"metadata\", \"root.json\")\n\n      # Bypass server side verification in 'write' or 'writeall', which would\n      # catch the unmet threshold.\n      # We also skip writing to 'metadata.staged' and copying to 'metadata' and\n      # instead write directly to 'metadata'\n      repo_lib.write_metadata_file(signed_metadata, live_root_path,\n          info[\"version\"], True, storage_backend)\n\n\n    # Update from current '1.root.json' to '3.root.json' on client and assert\n    # raise of 'BadSignatureError' (caused by unmet signature threshold).\n    try:\n      self.repository_updater.refresh()\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      mirror_errors = list(e.mirror_errors.values())\n      self.assertTrue(len(mirror_errors) == 1)\n      self.assertTrue(\n          isinstance(mirror_errors[0],\n          securesystemslib.exceptions.BadSignatureError))\n      self.assertEqual(\n          str(mirror_errors[0]),\n          repr(\"root\") + \" metadata has bad signature.\")\n\n    else:\n      self.fail(\n          \"Expected a NoWorkingMirrorError composed of one BadSignatureError\")\n\n\n  def test_2__import_delegations(self):\n    # Setup.\n    # In order to test '_import_delegations' the parent of the delegation\n    # has to be in Repository.metadata['current'], but it has to be inserted\n    # there without using '_load_metadata_from_file()' since it calls\n    # '_import_delegations()'.\n    repository_name = self.repository_updater.repository_name\n    tuf.keydb.clear_keydb(repository_name)\n    tuf.roledb.clear_roledb(repository_name)\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 0)\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 0)\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n\n    # Take into account the number of keyids algorithms supported by default,\n    # which this test condition expects to be two (sha256 and sha512).\n    self.assertEqual(4, len(tuf.keydb._keydb_dict[repository_name]))\n\n    # Test: pass a role without delegations.\n    self.repository_updater._import_delegations('root')\n\n    # Verify that there was no change to the roledb and keydb dictionaries by\n    # checking the number of elements in the dictionaries.\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n    # Take into account the number of keyid hash algorithms, which this\n    # test condition expects to be one\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4)\n\n    # Test: normal case, first level delegation.\n    self.repository_updater._import_delegations('targets')\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 5)\n    # The number of root keys (times the number of key hash algorithms) +\n    # delegation's key (+1 for its sha512 keyid).\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4 + 1)\n\n    # Verify that roledb dictionary was added.\n    self.assertTrue('role1' in tuf.roledb._roledb_dict[repository_name])\n\n    # Verify that keydb dictionary was updated.\n    role1_signable = \\\n      securesystemslib.util.load_json_file(os.path.join(self.client_metadata_current,\n                                           'role1.json'))\n    keyids = []\n    for signature in role1_signable['signatures']:\n      keyids.append(signature['keyid'])\n\n    for keyid in keyids:\n      self.assertTrue(keyid in tuf.keydb._keydb_dict[repository_name])\n\n    # Verify that _import_delegations() ignores invalid keytypes in the 'keys'\n    # field of parent role's 'delegations'.\n    existing_keyid = keyids[0]\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'bad_keytype'\n    self.repository_updater._import_delegations('targets')\n\n    # Restore the keytype of 'existing_keyid'.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'ed25519'\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated keys is malformed.\n    valid_keyval = self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval']\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = valid_keyval\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated roles is malformed.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['roles'][0]['name'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n\n\n  def test_2__versioninfo_has_been_updated(self):\n    # Verify that the method returns 'False' if a versioninfo was not changed.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    self.assertFalse(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n    # Verify that the method returns 'True' if Root's version number changes.\n    targets_versioninfo['version'] = 8\n    self.assertTrue(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n\n\n\n\n  def test_2__move_current_to_previous(self):\n    # Test case will consist of removing a metadata file from client's\n    # '{client_repository}/metadata/previous' directory, executing the method\n    # and then verifying that the 'previous' directory contains the snapshot\n    # file.\n    previous_snapshot_filepath = os.path.join(self.client_metadata_previous,\n                                              'snapshot.json')\n    os.remove(previous_snapshot_filepath)\n    self.assertFalse(os.path.exists(previous_snapshot_filepath))\n\n    # Verify that the current 'snapshot.json' is moved to the previous directory.\n    self.repository_updater._move_current_to_previous('snapshot')\n    self.assertTrue(os.path.exists(previous_snapshot_filepath))\n\n    # assert that non-ascii alphanumeric role name \"../\u00e4\" (that is url encoded\n    # in local filename) works\n    encoded_current = os.path.join(\n      self.client_metadata_current, '..%2F%C3%A4.json'\n    )\n    encoded_previous = os.path.join(\n      self.client_metadata_previous, '..%2F%C3%A4.json'\n    )\n\n    with open(encoded_current, \"w\"):\n      pass\n    self.repository_updater._move_current_to_previous('../\u00e4')\n    self.assertTrue(os.path.exists(encoded_previous))\n\n\n\n\n\n  def test_2__delete_metadata(self):\n    # This test will verify that 'root' metadata is never deleted.  When a role\n    # is deleted verify that the file is not present in the\n    # 'self.repository_updater.metadata' dictionary.\n    self.repository_updater._delete_metadata('root')\n    self.assertTrue('root' in self.repository_updater.metadata['current'])\n\n    self.repository_updater._delete_metadata('timestamp')\n    self.assertFalse('timestamp' in self.repository_updater.metadata['current'])\n\n\n\n\n\n  def test_2__ensure_not_expired(self):\n    # This test condition will verify that nothing is raised when a metadata\n    # file has a future expiration date.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    self.repository_updater._ensure_not_expired(root_metadata, 'root')\n\n    # Metadata with an expiration time in the future should, of course, not\n    # count as expired\n    expires = tuf.formats.unix_timestamp_to_datetime(int(time.time() + 10))\n    expires = expires.isoformat() + 'Z'\n    root_metadata['expires'] = expires\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    self.repository_updater._ensure_not_expired(root_metadata, 'root')\n\n    # Metadata that expires at the exact current time is considered expired\n    expire_time = int(time.time())\n    expires = \\\n      tuf.formats.unix_timestamp_to_datetime(expire_time).isoformat()+'Z'\n    root_metadata['expires'] = expires\n    mock_time = mock.Mock()\n    mock_time.return_value = expire_time\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    with mock.patch('time.time', mock_time):\n      self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                        self.repository_updater._ensure_not_expired,\n                        root_metadata, 'root')\n\n    # Metadata that expires in the past is considered expired\n    expires = tuf.formats.unix_timestamp_to_datetime(int(time.time() - 10))\n    expires = expires.isoformat() + 'Z'\n    root_metadata['expires'] = expires\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater._ensure_not_expired,\n                      root_metadata, 'root')\n\n\n\n\n\n  def test_3__update_metadata(self):\n    # Setup\n    # _update_metadata() downloads, verifies, and installs the specified\n    # metadata role.  Remove knowledge of currently installed metadata and\n    # verify that they are re-installed after calling _update_metadata().\n\n    # This is the default metadata that we would create for the timestamp role,\n    # because it has no signed metadata for itself.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    # This is the upper bound length for Targets metadata.\n    DEFAULT_TARGETS_FILELENGTH = tuf.settings.DEFAULT_TARGETS_REQUIRED_LENGTH\n\n    # Save the versioninfo of 'targets.json,' needed later when re-installing\n    # with _update_metadata().\n    targets_versioninfo = \\\n      self.repository_updater.metadata['current']['snapshot']['meta']\\\n                                      ['targets.json']\n\n    # Remove the currently installed metadata from the store and disk.  Verify\n    # that the metadata dictionary is re-populated after calling\n    # _update_metadata().\n    del self.repository_updater.metadata['current']['timestamp']\n    del self.repository_updater.metadata['current']['targets']\n\n    timestamp_filepath = \\\n      os.path.join(self.client_metadata_current, 'timestamp.json')\n    targets_filepath = os.path.join(self.client_metadata_current, 'targets.json')\n    root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n    os.remove(timestamp_filepath)\n    os.remove(targets_filepath)\n\n    # Test: normal case.\n    # Verify 'timestamp.json' is properly installed.\n    self.assertFalse('timestamp' in self.repository_updater.metadata)\n\n    logger.info('\\nroleinfo: ' + repr(tuf.roledb.get_rolenames(self.repository_name)))\n    self.repository_updater._update_metadata('timestamp',\n                                             DEFAULT_TIMESTAMP_FILELENGTH)\n    self.assertTrue('timestamp' in self.repository_updater.metadata['current'])\n    os.path.exists(timestamp_filepath)\n\n    # Verify 'targets.json' is properly installed.\n    self.assertFalse('targets' in self.repository_updater.metadata['current'])\n    self.repository_updater._update_metadata('targets',\n                                DEFAULT_TARGETS_FILELENGTH,\n                                targets_versioninfo['version'])\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n\n    targets_signable = securesystemslib.util.load_json_file(targets_filepath)\n    loaded_targets_version = targets_signable['signed']['version']\n    self.assertEqual(targets_versioninfo['version'], loaded_targets_version)\n\n    # Test: Invalid / untrusted version numbers.\n    # Invalid version number for 'targets.json'.\n    self.assertRaises(tuf.exceptions.NoWorkingMirrorError,\n        self.repository_updater._update_metadata,\n        'targets', DEFAULT_TARGETS_FILELENGTH, 88)\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH, 88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.  The version number is checked, so the specific error in\n    # this case should be 'tuf.exceptions.BadVersionNumberError'.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH,\n                                               88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n\n\n\n\n  def test_3__get_metadata_file(self):\n\n    '''\n    This test focuses on making sure that the updater rejects unknown or\n    badly-formatted TUF specification version numbers....\n    '''\n\n    # Make note of the correct supported TUF specification version.\n    correct_specification_version = tuf.SPECIFICATION_VERSION\n\n    # Change it long enough to write new metadata.\n    tuf.SPECIFICATION_VERSION = '0.9.0'\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # Change the supported TUF specification version back to what it should be\n    # so that we can parse the metadata and see that the spec version in the\n    # metadata does not match the code's expected spec version.\n    tuf.SPECIFICATION_VERSION = correct_specification_version\n\n    upperbound_filelength = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n    try:\n      self.repository_updater._get_metadata_file('timestamp', 'timestamp.json',\n      upperbound_filelength, 1)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      # Note that this test provides a piece of metadata which would fail to\n      # be accepted -- with a different error -- if the specification version\n      # number were not a problem.\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(\n            mirror_error, tuf.exceptions.UnsupportedSpecificationError)\n\n    else:\n      self.fail(\n          'Expected a failure to verify metadata when the metadata had a '\n          'specification version number that was unexpected.  '\n          'No error was raised.')\n\n\n\n\n\n  def test_3__update_metadata_if_changed(self):\n    # Setup.\n    # The client repository is initially loaded with only four top-level roles.\n    # Verify that the metadata store contains the metadata of only these four\n    # roles before updating the metadata of 'targets.json'.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Test: normal case.  Update 'targets.json'.  The version number should not\n    # change.\n    self.repository_updater._update_metadata_if_changed('targets')\n\n    # Verify the current version of 'targets.json' has not changed.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = 'file3.txt'\n\n    repository.targets.add_target(target3)\n    repository.root.version = repository.root.version + 1\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update 'targets.json' and verify that the client's current 'targets.json'\n    # has been updated.  'timestamp' and 'snapshot' must be manually updated\n    # so that new 'targets' can be recognized.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    self.repository_updater._update_metadata('timestamp', DEFAULT_TIMESTAMP_FILELENGTH)\n    self.repository_updater._update_metadata_if_changed('snapshot', 'timestamp')\n    self.repository_updater._update_metadata_if_changed('targets')\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertTrue(self.repository_updater.metadata['current']['targets'])\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 2)\n\n    # Test for an invalid 'referenced_metadata' argument.\n    self.assertRaises(tuf.exceptions.RepositoryError,\n        self.repository_updater._update_metadata_if_changed, 'snapshot', 'bad_role')\n\n\n\n  def test_3__targets_of_role(self):\n    # Setup.\n    # Extract the list of targets from 'targets.json', to be compared to what\n    # is returned by _targets_of_role('targets').\n    targets_in_metadata = \\\n      self.repository_updater.metadata['current']['targets']['targets']\n\n    # Test: normal case.\n    targetinfos_list = self.repository_updater._targets_of_role('targets')\n\n    # Verify that the list of targets was returned, and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos_list))\n    for targetinfo in targetinfos_list:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in targets_in_metadata.items())\n\n\n\n\n\n  def test_4_refresh(self):\n    # This unit test is based on adding an extra target file to the\n    # server and rebuilding all server-side metadata.  All top-level metadata\n    # should be updated when the client calls refresh().\n\n    # First verify that an expired root metadata is updated.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.repository_updater.refresh()\n\n    # Second, verify that expired root metadata is not updated if\n    # 'unsafely_update_root_if_necessary' is explicitly set to 'False'.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater.refresh,\n                      unsafely_update_root_if_necessary=False)\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = 'file3.txt'\n\n    repository.targets.add_target(target3)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Reference 'self.Repository.metadata['current']['targets']'.  Ensure\n    # 'target3' is not already specified.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    self.assertFalse(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the roles to be modified.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 1)\n\n    # Test: normal case.  'targes.json' should now specify 'target3', and the\n    # following top-level metadata should have also been updated:\n    # 'snapshot.json' and 'timestamp.json'.\n    self.repository_updater.refresh()\n\n    # Verify that the client's metadata was updated.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    self.assertTrue(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the updated roles.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 2)\n\n\n\n\n\n  def test_4__refresh_targets_metadata(self):\n    # Setup.\n    # It is assumed that the client repository has only loaded the top-level\n    # metadata.  Refresh the 'targets.json' metadata, including all delegated\n    # roles (i.e., the client should add the missing 'role1.json' metadata.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n\n    # Test: normal case.\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n    # Verify that client's metadata files were refreshed successfully.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 6)\n\n    # Test for non-existing rolename.\n    self.repository_updater._refresh_targets_metadata('bad_rolename',\n        refresh_all_delegated_roles=False)\n\n    # Test that non-json metadata in Snapshot is ignored.\n    self.repository_updater.metadata['current']['snapshot']['meta']['bad_role.xml'] = {}\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n\n\n  def test_5_all_targets(self):\n   # Setup\n   # As with '_refresh_targets_metadata()',\n\n   # Update top-level metadata before calling one of the \"targets\" methods, as\n   # recommended by 'updater.py'.\n   self.repository_updater.refresh()\n\n   # Test: normal case.\n   with utils.ignore_deprecation_warnings('tuf.client.updater'):\n    all_targets = self.repository_updater.all_targets()\n\n   # Verify format of 'all_targets', it should correspond to\n   # 'TARGETINFOS_SCHEMA'.\n   self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(all_targets))\n\n   # Verify that there is a correct number of records in 'all_targets' list,\n   # and the expected filepaths specified in the metadata.  On the targets\n   # directory of the repository, there should be 3 target files (2 of\n   # which are specified by 'targets.json'.)  The delegated role 'role1'\n   # specifies 1 target file.  The expected total number targets in\n   # 'all_targets' should be 3.\n   self.assertEqual(len(all_targets), 3)\n\n   target_filepaths = []\n   for target in all_targets:\n    target_filepaths.append(target['filepath'])\n\n   self.assertTrue('file1.txt' in target_filepaths)\n   self.assertTrue('file2.txt' in target_filepaths)\n   self.assertTrue('file3.txt' in target_filepaths)\n\n\n\n\n\n  def test_5_targets_of_role(self):\n    # Setup\n    # Remove knowledge of 'targets.json' from the metadata store.\n    self.repository_updater.metadata['current']['targets']\n\n    # Remove the metadata of the delegated roles.\n    #shutil.rmtree(os.path.join(self.client_metadata, 'targets'))\n    os.remove(os.path.join(self.client_metadata_current, 'targets.json'))\n\n    # Extract the target files specified by the delegated role, 'role1.json',\n    # as available on the server-side version of the role.\n    role1_filepath = os.path.join(self.repository_directory, 'metadata',\n                                  'role1.json')\n    role1_signable = securesystemslib.util.load_json_file(role1_filepath)\n    expected_targets = role1_signable['signed']['targets']\n\n\n    # Test: normal case.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      targetinfos = self.repository_updater.targets_of_role('role1')\n\n    # Verify that the expected role files were downloaded and installed.\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets.json'))\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets',\n                   'role1.json'))\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    self.assertTrue('role1' in self.repository_updater.metadata['current'])\n\n    #  Verify that list of targets was returned and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos))\n    for targetinfo in targetinfos:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in expected_targets.items())\n\n    # Test: Invalid arguments.\n    # targets_of_role() expected a string rolename.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater.targets_of_role,\n                        8)\n      self.assertRaises(tuf.exceptions.UnknownRoleError, self.repository_updater.targets_of_role,\n                        'unknown_rolename')\n\n\n\n\n\n  def test_6_get_one_valid_targetinfo(self):\n    # Setup\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Extract the file information of the targets specified in 'targets.json'.\n    self.repository_updater.refresh()\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n\n    target_files = targets_metadata['targets']\n    # Extract random target from 'target_files', which will be compared to what\n    # is returned by get_one_valid_targetinfo().  Restore the popped target\n    # (dict value stored in the metadata store) so that it can be found later.\n    filepath, fileinfo = target_files.popitem()\n    target_files[filepath] = fileinfo\n\n    target_targetinfo = self.repository_updater.get_one_valid_targetinfo(filepath)\n    self.assertTrue(tuf.formats.TARGETINFO_SCHEMA.matches(target_targetinfo))\n    self.assertEqual(target_targetinfo['filepath'], filepath)\n    self.assertEqual(target_targetinfo['fileinfo'], fileinfo)\n\n    # Test: invalid target path.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        self.repository_updater.get_one_valid_targetinfo,\n        self.random_path().lstrip(os.sep).lstrip('/'))\n\n    # Test updater.get_one_valid_targetinfo() backtracking behavior (enabled by\n    # default.)\n    targets_directory = os.path.join(self.repository_directory, 'targets')\n    os.makedirs(os.path.join(targets_directory, 'foo'))\n\n    foo_package = 'foo/foo1.1.tar.gz'\n    foo_pattern = 'foo/foo*.tar.gz'\n\n    foo_fullpath = os.path.join(targets_directory, foo_package)\n    with open(foo_fullpath, 'wb') as file_object:\n      file_object.write(b'new release')\n\n    # Modify delegations on the remote repository to test backtracking behavior.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n    repository.targets('role4').add_target(foo_package)\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # updater.get_one_valid_targetinfo() should find 'foo1.1.tar.gz' by\n    # backtracking to 'role3'.  'role2' allows backtracking.\n    self.repository_updater.refresh()\n    self.repository_updater.get_one_valid_targetinfo('foo/foo1.1.tar.gz')\n\n    # A leading path separator is disallowed.\n    self.assertRaises(tuf.exceptions.FormatError,\n    self.repository_updater.get_one_valid_targetinfo, '/foo/foo1.1.tar.gz')\n\n    # Test when 'role2' does *not* allow backtracking.  If 'foo/foo1.1.tar.gz'\n    # is not provided by the authoritative 'role2',\n    # updater.get_one_valid_targetinfo() should return a\n    # 'tuf.exceptions.UnknownTargetError' exception.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    repository.targets.revoke('role3')\n    repository.targets.revoke('role4')\n\n    # Ensure we delegate in trusted order (i.e., 'role2' has higher priority.)\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern], terminating=True, list_of_targets=[])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Verify that 'tuf.exceptions.UnknownTargetError' is raised by\n    # updater.get_one_valid_targetinfo().\n    self.repository_updater.refresh()\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n                      self.repository_updater.get_one_valid_targetinfo,\n                      'foo/foo1.1.tar.gz')\n\n    # Verify that a 'tuf.exceptions.FormatError' is raised for delegated paths\n    # that contain a leading path separator.\n    self.assertRaises(tuf.exceptions.FormatError,\n        self.repository_updater.get_one_valid_targetinfo,\n        '/foo/foo1.1.tar.gz')\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n\n  def test_6_download_target(self):\n    # Create temporary directory (destination directory of downloaded targets)\n    # that will be passed as an argument to 'download_target()'.\n    destination_directory = self.make_temp_directory()\n    target_filepaths = \\\n      list(self.repository_updater.metadata['current']['targets']['targets'].keys())\n\n    # Test: normal case.\n    # Get the target info, which is an argument to 'download_target()'.\n\n    # 'target_filepaths' is expected to have at least two targets.  The first\n    # target will be used to test against download_target().  The second\n    # will be used to test against download_target() and a repository with\n    # 'consistent_snapshot' set to True.\n    target_filepath1 = target_filepaths.pop()\n    targetinfo = self.repository_updater.get_one_valid_targetinfo(target_filepath1)\n    self.repository_updater.download_target(targetinfo,\n                                            destination_directory)\n\n    download_filepath = \\\n      os.path.join(destination_directory, target_filepath1.lstrip('/'))\n    self.assertTrue(os.path.exists(download_filepath))\n    length, hashes = \\\n      securesystemslib.util.get_file_details(download_filepath,\n        securesystemslib.settings.HASH_ALGORITHMS)\n    download_targetfileinfo = tuf.formats.make_targets_fileinfo(length, hashes)\n\n    # Add any 'custom' data from the repository's target fileinfo to the\n    # 'download_targetfileinfo' object being tested.\n    if 'custom' in targetinfo['fileinfo']:\n      download_targetfileinfo['custom'] = targetinfo['fileinfo']['custom']\n\n    self.assertEqual(targetinfo['fileinfo'], download_targetfileinfo)\n\n    # Test when consistent snapshots is set.  First, create a valid\n    # repository with consistent snapshots set (root.json contains a\n    # \"consistent_snapshot\" entry that the updater uses to correctly fetch\n    # snapshots.  The updater expects the existence of\n    # '<version_number>.filename' files if root.json sets 'consistent_snapshot\n    # = True'.\n\n    # The repository must be rewritten with 'consistent_snapshot' set.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    # Write metadata for all the top-level roles , since consistent snapshot\n    # is now being set to true (i.e., the pre-generated repository isn't set\n    # to support consistent snapshots.  A new version of targets.json is needed\n    # to ensure <digest>.filename target files are written to disk.\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n\n    repository.writeall(consistent_snapshot=True)\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # And ensure the client has the latest top-level metadata.\n    self.repository_updater.refresh()\n\n    target_filepath2 = target_filepaths.pop()\n    targetinfo2 = self.repository_updater.get_one_valid_targetinfo(target_filepath2)\n    self.repository_updater.download_target(targetinfo2,\n                                            destination_directory)\n\n    # Checks if the file has been successfully downloaded\n    download_filepath = os.path.join(destination_directory, target_filepath2)\n    self.assertTrue(os.path.exists(download_filepath))\n\n    # Removes the file so that it can be downloaded again in the next test\n    os.remove(download_filepath)\n\n    # Test downloading with consistent snapshot enabled, but without adding\n    # the hash of the file as a prefix to its name.\n\n    file1_path = targetinfo2['filepath']\n    file1_hashes = securesystemslib.util.get_file_hashes(\n        os.path.join(self.repository_directory, 'targets', file1_path),\n        hash_algorithms=['sha256', 'sha512'])\n\n    # Currently in the repository directory, those three files exists:\n    # \"file1.txt\", \"<sha256_hash>.file1.txt\" and \"<sha512_hash>.file1.txt\"\n    # where both sha256 and sha512 hashes are for file file1.txt.\n    # Remove the files with the hash digest prefix to ensure that\n    # the served target file is not prefixed.\n    os.remove(os.path.join(self.repository_directory, 'targets',\n        file1_hashes['sha256'] + '.' + file1_path))\n    os.remove(os.path.join(self.repository_directory, 'targets',\n        file1_hashes['sha512'] + '.' + file1_path))\n\n\n    self.repository_updater.download_target(targetinfo2,\n                                            destination_directory,\n                                            prefix_filename_with_hash=False)\n\n    # Checks if the file has been successfully downloaded\n    self.assertTrue(os.path.exists(download_filepath))\n\n    # Test for a destination that cannot be written to (apart from a target\n    # file that already exists at the destination) and which raises an\n    # exception.\n    bad_destination_directory = 'bad' * 2000\n\n    try:\n      self.repository_updater.download_target(targetinfo, bad_destination_directory)\n\n    except OSError as e:\n      self.assertTrue(\n          e.errno in [errno.ENAMETOOLONG, errno.ENOENT, errno.EINVAL],\n          \"wrong errno: \" + str(e.errno))\n\n    else:\n      self.fail('No OSError raised')\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      targetinfo, 8)\n\n    # Test:\n    # Attempt a file download of a valid target, however, a download exception\n    # occurs because the target is not within the mirror's confined target\n    # directories.  Adjust mirrors dictionary, so that 'confined_target_dirs'\n    # field contains at least one confined target and excludes needed target\n    # file.\n    mirrors = self.repository_updater.mirrors\n    for mirror_name, mirror_info in mirrors.items():\n      mirrors[mirror_name]['confined_target_dirs'] = [self.random_path()]\n\n    try:\n      self.repository_updater.download_target(targetinfo,\n                                              destination_directory)\n\n    except tuf.exceptions.NoWorkingMirrorError as exception:\n      # Ensure that no mirrors were found due to mismatch in confined target\n      # directories.  get_list_of_mirrors() returns an empty list in this case,\n      # which does not generate specific exception errors.\n      self.assertEqual(len(exception.mirror_errors), 0)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError with zero mirror errors in it.')\n\n\n\n\n\n  def test_7_updated_targets(self):\n    # Verify that the list of targets returned by updated_targets() contains\n    # all the files that need to be updated, these files include modified and\n    # new target files.  Also, confirm that files that need not to be updated\n    # are absent from the list.\n    # Setup\n\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory which will hold client's target files.\n    destination_directory = self.make_temp_directory()\n\n    # Get the list of target files.  It will be used as an argument to the\n    # 'updated_targets()' function.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    # Test for duplicates and targets in the root directory of the repository.\n    additional_target = all_targets[0].copy()\n    all_targets.append(additional_target)\n    additional_target_in_root_directory = additional_target.copy()\n    additional_target_in_root_directory['filepath'] = 'file1.txt'\n    all_targets.append(additional_target_in_root_directory)\n\n    #  At this point client needs to update and download all targets.\n    # Test: normal cases.\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    # Assumed the pre-generated repository specifies two target files in\n    # 'targets.json' and one delegated target file in 'role1.json'.\n    self.assertEqual(len(updated_targets), 3)\n\n    # Test: download one of the targets.\n    download_target = copy.deepcopy(updated_targets).pop()\n    self.repository_updater.download_target(download_target,\n                                            destination_directory)\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 2)\n\n    # Test: download all the targets.\n    for download_target in all_targets:\n      self.repository_updater.download_target(download_target,\n                                              destination_directory)\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 0)\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      all_targets, 8)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Ensure the client has up-to-date metadata.\n    self.repository_updater.refresh()\n\n    # Verify that the new target file is considered updated.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n    self.assertEqual(len(updated_targets), 1)\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n\n  def test_8_remove_obsolete_targets(self):\n    # Setup.\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory that will hold the client's target files.\n    destination_directory = self.make_temp_directory()\n\n    #  Populate 'destination_direction' with all target files.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    self.assertEqual(len(os.listdir(destination_directory)), 0)\n\n    for target in all_targets:\n      self.repository_updater.download_target(target, destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n\n    # Remove two target files from the server's repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update client's metadata.\n    self.repository_updater.refresh()\n\n    # Test: normal case.\n    # Verify number of target files in 'destination_directory' (should be 1\n    # after the update made to the remote repository), and call\n    # 'remove_obsolete_targets()'.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets,\n                                              destination_directory)\n\n    for updated_target in updated_targets:\n      self.repository_updater.download_target(updated_target,\n                                              destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    #  Verify that, if there are no obsolete files, the number of files\n    #  in 'destination_directory' remains the same.\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    # Test coverage for a destination path that causes an exception not due\n    # to an already removed target.\n    bad_destination_directory = 'bad' * 2000\n    self.repository_updater.remove_obsolete_targets(bad_destination_directory)\n\n    # Test coverage for a target that is not specified in current metadata.\n    del self.repository_updater.metadata['current']['targets']['targets']['file2.txt']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    # Test coverage for a role that doesn't exist in the previously trusted set\n    # of metadata.\n    del self.repository_updater.metadata['previous']['targets']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n  def test_9__get_target_hash(self):\n    # Test normal case.\n    # Test target filepaths with ascii and non-ascii characters.\n    expected_target_hashes = {\n      '/file1.txt': 'e3a3d89eb3b70ce3fbce6017d7b8c12d4abd5635427a0e8a238f53157df85b3d',\n      '/Jalape\\xc3\\xb1o': '78bfd5c314680545eb48ecad508aceb861f8d6e680f4fe1b791da45c298cda88'\n    }\n    for filepath, target_hash in expected_target_hashes.items():\n      self.assertTrue(tuf.formats.RELPATH_SCHEMA.matches(filepath))\n      self.assertTrue(securesystemslib.formats.HASH_SCHEMA.matches(target_hash))\n      self.assertEqual(self.repository_updater._get_target_hash(filepath), target_hash)\n\n    # Test for improperly formatted argument.\n    #self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._get_target_hash, 8)\n\n\n\n\n  def test_10__check_file_length(self):\n    # Test for exception if file object is not equal to trusted file length.\n    with tempfile.TemporaryFile() as temp_file_object:\n      temp_file_object.write(b'X')\n      temp_file_object.seek(0)\n      self.assertRaises(tuf.exceptions.DownloadLengthMismatchError,\n                      self.repository_updater._check_file_length,\n                      temp_file_object, 10)\n\n\n\n\n\n  def test_10__targets_of_role(self):\n    # Test for non-existent role.\n    self.assertRaises(tuf.exceptions.UnknownRoleError,\n                      self.repository_updater._targets_of_role,\n                      'non-existent-role')\n\n    # Test for role that hasn't been loaded yet.\n    del self.repository_updater.metadata['current']['targets']\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets',\n                                                        skip_refresh=True)), 0)\n\n    # 'targets.json' tracks two targets.\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets')),\n                     2)\n\n\n\n  def test_10__preorder_depth_first_walk(self):\n\n    # Test that infinite loop is prevented if the target file is not found and\n    # the max number of delegations is reached.\n    valid_max_number_of_delegations = tuf.settings.MAX_NUMBER_OF_DELEGATIONS\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = 0\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('unknown.txt'))\n\n    # Reset the setting for max number of delegations so that subsequent unit\n    # tests reference the expected setting.\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = valid_max_number_of_delegations\n\n    # Attempt to create a circular delegation, where role1 performs a\n    # delegation to the top-level Targets role.  The updater should ignore the\n    # delegation and not raise an exception.\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    targets_metadata = securesystemslib.util.load_json_file(targets_path)\n    targets_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(targets_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(targets_metadata))\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['name'] = 'targets'\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    role2_path = os.path.join(self.client_metadata_current, 'role2.json')\n    role2_metadata = securesystemslib.util.load_json_file(role2_path)\n    role2_metadata['signed']['delegations']['roles'] = role1_metadata['signed']['delegations']['roles']\n    role2_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role2_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role2_metadata))\n\n    logger.debug('attempting circular delegation')\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('/file8.txt'))\n\n\n\n\n\n\n  def test_10__visit_child_role(self):\n    # Call _visit_child_role and test the dict keys: 'paths',\n    # 'path_hash_prefixes', and if both are missing.\n\n    targets_role = self.repository_updater.metadata['current']['targets']\n    targets_role['delegations']['roles'][0]['paths'] = ['/*.txt', '/target.exe']\n    child_role = targets_role['delegations']['roles'][0]\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/*.exe']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/target.exe'), child_role['name'])\n\n    # Test for a valid path hash prefix...\n    child_role['path_hash_prefixes'] = ['8baf']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), child_role['name'])\n\n    # ... and an invalid one, as well.\n    child_role['path_hash_prefixes'] = ['badd']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), None)\n\n    # Test for a forbidden target.\n    del child_role['path_hash_prefixes']\n    self.repository_updater._visit_child_role(child_role, '/forbidden.tgz')\n\n    # Verify that unequal path_hash_prefixes are skipped.\n    child_role['path_hash_prefixes'] = ['bad', 'bad']\n    self.assertEqual(None, self.repository_updater._visit_child_role(child_role,\n        '/unknown.exe'))\n\n    # Test if both 'path' and 'path_hash_prefixes' are missing.\n    del child_role['paths']\n    del child_role['path_hash_prefixes']\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._visit_child_role,\n        child_role, child_role['name'])\n\n\n\n  def test_11__verify_metadata_file(self):\n    # Test for invalid metadata content.\n    with tempfile.TemporaryFile() as metadata_file_object:\n      metadata_file_object.write(b'X')\n      metadata_file_object.seek(0)\n\n      self.assertRaises(tuf.exceptions.InvalidMetadataJSONError,\n          self.repository_updater._verify_metadata_file,\n          metadata_file_object, 'root')\n\n\n  def test_13__targets_of_role(self):\n    # Test case where a list of targets is given.  By default, the 'targets'\n    # parameter is None.\n    targets = [{'filepath': 'file1.txt', 'fileinfo': {'length': 1, 'hashes': {'sha256': 'abc'}}}]\n    self.repository_updater._targets_of_role('targets',\n        targets=targets, skip_refresh=False)\n\n\n\n\nclass TestMultiRepoUpdater(unittest_toolbox.Modified_TestCase):\n\n  def setUp(self):\n    # Modified_Testcase can handle temp dir removal\n    unittest_toolbox.Modified_TestCase.setUp(self)\n    self.temporary_directory = self.make_temp_directory(directory=os.getcwd())\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf/tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n\n    self.temporary_repository_root = tempfile.mkdtemp(dir=self.temporary_directory)\n\n    # Needed because in some tests simple_server.py cannot be found.\n    # The reason is that the current working directory\n    # has been changed when executing a subprocess.\n    self.SIMPLE_SERVER_PATH = os.path.join(os.getcwd(), 'simple_server.py')\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_client = os.path.join(original_repository_files, 'client', 'test_repository1')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_map_file = os.path.join(original_repository_files, 'map.json')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = os.path.join(self.temporary_repository_root,\n        'repository_server1')\n    self.repository_directory2 = os.path.join(self.temporary_repository_root,\n        'repository_server2')\n\n    # Setting 'tuf.settings.repositories_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.temporary_repository_root\n\n    repository_name = 'test_repository1'\n    repository_name2 = 'test_repository2'\n\n    self.client_directory = os.path.join(self.temporary_repository_root,\n        repository_name)\n    self.client_directory2 = os.path.join(self.temporary_repository_root,\n        repository_name2)\n\n    self.keystore_directory = os.path.join(self.temporary_repository_root,\n        'keystore')\n    self.map_file = os.path.join(self.client_directory, 'map.json')\n    self.map_file2 = os.path.join(self.client_directory2, 'map.json')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_repository, self.repository_directory2)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_client, self.client_directory2)\n    shutil.copyfile(original_map_file, self.map_file)\n    shutil.copyfile(original_map_file, self.map_file2)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served by the\n    # SimpleHTTPServer launched here.  The test cases of this unit test assume\n    # the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n\n    # Creates a subprocess running a server.\n    self.server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH, popen_cwd=self.repository_directory)\n\n    logger.debug('Server process started.')\n\n    # Creates a subprocess running a server.\n    self.server_process_handler2 = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH, popen_cwd=self.repository_directory2)\n\n    logger.debug('Server process 2 started.')\n\n    url_prefix = \\\n        'http://' + utils.TEST_HOST_ADDRESS + ':' + \\\n        str(self.server_process_handler.port)\n    url_prefix2 = \\\n        'http://' + utils.TEST_HOST_ADDRESS + ':' + \\\n        str(self.server_process_handler2.port)\n\n    # We have all of the necessary information for two repository mirrors\n    # in map.json, except for url prefixes.\n    # For the url prefixes, we create subprocesses that run a server script.\n    # In server scripts we get a free port from the OS which is sent\n    # back to the parent process.\n    # That's why we dynamically add the ports to the url prefixes\n    # and changing the content of map.json.\n    self.map_file_path = os.path.join(self.client_directory, 'map.json')\n    data = securesystemslib.util.load_json_file(self.map_file_path)\n\n    data['repositories']['test_repository1'] = [url_prefix]\n    data['repositories']['test_repository2'] = [url_prefix2]\n    with open(self.map_file_path, 'w') as f:\n      json.dump(data, f)\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    self.repository_mirrors2 = {'mirror1': {'url_prefix': url_prefix2,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Create the repository instances.  The test cases will use these client\n    # updaters to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(repository_name,\n        self.repository_mirrors)\n    self.repository_updater2 = updater.Updater(repository_name2,\n        self.repository_mirrors2)\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.multi_repo_updater = updater.MultiRepoUpdater(self.map_file)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n\n    # Cleans the resources and flush the logged lines (if any).\n    self.server_process_handler.clean()\n    self.server_process_handler2.clean()\n\n    # updater.Updater() populates the roledb with the name \"test_repository1\"\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    # Remove top-level temporary directory\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n\n\n  # UNIT TESTS.\n  def test__init__(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, 8)\n\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n    # Test for a non-existent map file.\n    self.assertRaises(tuf.exceptions.Error, updater.MultiRepoUpdater,\n        'non-existent.json')\n\n    # Test for a map file that doesn't contain the required fields.\n    root_filepath = os.path.join(\n        self.repository_directory, 'metadata', 'root.json')\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, root_filepath)\n\n    # Test for a valid instantiation.\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n\n\n  def test__target_matches_path_pattern(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n    paths = ['foo*.tgz', 'bar*.tgz', 'file1.txt']\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('bar-1.0.tgz', paths))\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('file1.txt', paths))\n    self.assertFalse(\n        multi_repo_updater._target_matches_path_pattern('baz-1.0.tgz', paths))\n\n\n\n  def test_get_valid_targetinfo(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n    # Verify the multi repo updater refuses to save targetinfo if\n    # required local repositories are missing.\n    repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1')\n    backup_repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1.backup')\n    shutil.move(repo_dir, backup_repo_dir)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the client's repository directory.\n    shutil.move(backup_repo_dir, repo_dir)\n\n    # Verify that the Root file must exist.\n    root_filepath = os.path.join(repo_dir, 'metadata', 'current', 'root.json')\n    backup_root_filepath = os.path.join(root_filepath, root_filepath + '.backup')\n    shutil.move(root_filepath, backup_root_filepath)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the Root file.\n    shutil.move(backup_root_filepath, root_filepath)\n\n    # Test that the first mapping is skipped if it's irrelevant to the target\n    # file.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n\n    # Verify that a targetinfo is not returned for a non-existent target.\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = False\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = True\n\n    # Test for a mapping that sets terminating = True, and that appears before\n    # the final mapping.\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = True\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'bad3.txt')\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = False\n\n    # Test for the case where multiple repos sign for the same target.\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    # Verify that valid targetinfo is matched for two repositories that provide\n    # different custom field.  Make sure to set the 'match_custom_field'\n    # argument to 'False' when calling get_valid_targetinfo().\n    repository = repo_tool.load_repository(self.repository_directory2)\n\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    custom_field = {\"custom\": \"my_custom_data\"}\n    repository.targets.add_target(os.path.basename(target1), custom_field)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Do we get the expected match for the two targetinfo that only differ\n    # by the custom field?\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo(\n        'file1.txt', match_custom_field=False)\n\n    # Verify the case where two repositories provide different targetinfo.\n    # Modify file1.txt so that different length and hashes are reported by the\n    # two repositories.\n    repository = repo_tool.load_repository(self.repository_directory2)\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Ensure the threshold is modified to 2 (assumed to be 1, by default) and\n    # verify that get_valid_targetinfo() raises an UnknownTargetError\n    # despite both repos signing for file1.txt.\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'file1.txt')\n\n\n\n\n\n  def test_get_updater(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n    # Test for a non-existent repository name.\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n\n    # Test get_updater indirectly via the \"private\" _update_from_repository().\n    self.assertRaises(tuf.exceptions.Error, multi_repo_updater._update_from_repository, 'bad_repo_name', 'file3.txt')\n\n    # Test for a repository that doesn't exist.\n    multi_repo_updater.map_file['repositories']['bad_repo_name'] = ['https://bogus:30002']\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n\n\nclass TestUpdaterRolenames(unittest_toolbox.Modified_TestCase):\n  def setUp(self):\n    unittest_toolbox.Modified_TestCase.setUp(self)\n\n    repo_dir = os.path.join(os.getcwd(), 'repository_data', 'fishy_rolenames')\n\n    self.client_dir = self.make_temp_directory()\n    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"))\n    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"previous\"))\n    shutil.copy(\n      os.path.join(repo_dir, 'metadata', '1.root.json'),\n      os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\", \"root.json\")\n    )\n\n    simple_server_path = os.path.join(os.getcwd(), 'simple_server.py')\n    self.server_process_handler = utils.TestServerProcess(log=logger,\n        server=simple_server_path)\n\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + \"/repository_data/fishy_rolenames\"\n\n    tuf.settings.repositories_directory = self.client_dir\n    mirrors = {'mirror1': {\n      'url_prefix': url_prefix,\n      'metadata_path': 'metadata/',\n      'targets_path': ''\n    }}\n    self.updater = updater.Updater(\"fishy_rolenames\", mirrors)\n\n  def tearDown(self):\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n    self.server_process_handler.flush_log()\n    self.server_process_handler.clean()\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n  def test_unusual_rolenames(self):\n    \"\"\"Test rolenames that may be tricky to handle as filenames\n\n    The test data in repository_data/fishy_rolenames has been produced\n    semi-manually using RepositorySimulator: using the RepositorySimulator\n    in these tests directly (like test_updater_with_simulator.py does for\n    ngclient) might make more sense... but would require some integration work\n    \"\"\"\n\n    # Make a target search that fetches the delegated targets\n    self.updater.refresh()\n    with self.assertRaises(tuf.exceptions.UnknownTargetError):\n      self.updater.get_one_valid_targetinfo(\"anything\")\n\n    # Assert that the metadata files are in the client metadata directory\n    metadata_dir = os.path.join(\n      self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"\n    )\n    local_metadata = os.listdir(metadata_dir)\n    for fname in ['%C3%B6.json', '..%2Fa.json', '..json']:\n      self.assertTrue(fname in local_metadata)\n\n\ndef _load_role_keys(keystore_directory):\n\n  # Populating 'self.role_keys' by importing the required public and private\n  # keys of 'tuf/tests/repository_data/'.  The role keys are needed when\n  # modifying the remote repository used by the test cases in this unit test.\n\n  # The pre-generated key files in 'repository_data/keystore' are all encrypted with\n  # a 'password' passphrase.\n  EXPECTED_KEYFILE_PASSWORD = 'password'\n\n  # Store and return the cryptography keys of the top-level roles, including 1\n  # delegated role.\n  role_keys = {}\n\n  root_key_file = os.path.join(keystore_directory, 'root_key')\n  targets_key_file = os.path.join(keystore_directory, 'targets_key')\n  snapshot_key_file = os.path.join(keystore_directory, 'snapshot_key')\n  timestamp_key_file = os.path.join(keystore_directory, 'timestamp_key')\n  delegation_key_file = os.path.join(keystore_directory, 'delegation_key')\n\n  role_keys = {'root': {}, 'targets': {}, 'snapshot': {}, 'timestamp': {},\n               'role1': {}}\n\n  # Import the top-level and delegated role public keys.\n  role_keys['root']['public'] = \\\n    repo_tool.import_rsa_publickey_from_file(root_key_file+'.pub')\n  role_keys['targets']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(targets_key_file+'.pub')\n  role_keys['snapshot']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(snapshot_key_file+'.pub')\n  role_keys['timestamp']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(timestamp_key_file+'.pub')\n  role_keys['role1']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(delegation_key_file+'.pub')\n\n  # Import the private keys of the top-level and delegated roles.\n  role_keys['root']['private'] = \\\n    repo_tool.import_rsa_privatekey_from_file(root_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['targets']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(targets_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['snapshot']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(snapshot_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['timestamp']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(timestamp_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['role1']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(delegation_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n\n  return role_keys\n\n\nif __name__ == '__main__':\n  utils.configure_test_logging(sys.argv)\n  unittest.main()\n", "patch": "@@ -454,74 +454,6 @@ def test_1__refresh_must_not_count_duplicate_keyids_towards_threshold(self):\n           \"Expected a NoWorkingMirrorError composed of one BadSignatureError\")\n \n \n-  def test_1__update_fileinfo(self):\n-      # Tests\n-      # Verify that the 'self.fileinfo' dictionary is empty (its starts off empty\n-      # and is only populated if _update_fileinfo() is called.\n-      fileinfo_dict = self.repository_updater.fileinfo\n-      self.assertEqual(len(fileinfo_dict), 0)\n-\n-      # Load the fileinfo of the top-level root role.  This populates the\n-      # 'self.fileinfo' dictionary.\n-      self.repository_updater._update_fileinfo('root.json')\n-      self.assertEqual(len(fileinfo_dict), 1)\n-      self.assertTrue(tuf.formats.FILEDICT_SCHEMA.matches(fileinfo_dict))\n-      root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n-      length, hashes = securesystemslib.util.get_file_details(root_filepath)\n-      root_fileinfo = tuf.formats.make_targets_fileinfo(length, hashes)\n-      self.assertTrue('root.json' in fileinfo_dict)\n-      self.assertEqual(fileinfo_dict['root.json'], root_fileinfo)\n-\n-      # Verify that 'self.fileinfo' is incremented if another role is updated.\n-      self.repository_updater._update_fileinfo('targets.json')\n-      self.assertEqual(len(fileinfo_dict), 2)\n-\n-      # Verify that 'self.fileinfo' is inremented if a non-existent role is\n-      # requested, and has its fileinfo entry set to 'None'.\n-      self.repository_updater._update_fileinfo('bad_role.json')\n-      self.assertEqual(len(fileinfo_dict), 3)\n-      self.assertEqual(fileinfo_dict['bad_role.json'], None)\n-\n-\n-\n-\n-  def test_2__fileinfo_has_changed(self):\n-      #  Verify that the method returns 'False' if file info was not changed.\n-      root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n-      length, hashes = securesystemslib.util.get_file_details(root_filepath)\n-      root_fileinfo = tuf.formats.make_targets_fileinfo(length, hashes)\n-      self.assertFalse(self.repository_updater._fileinfo_has_changed('root.json',\n-                                                             root_fileinfo))\n-\n-      # Verify that the method returns 'True' if length or hashes were changed.\n-      new_length = 8\n-      new_root_fileinfo = tuf.formats.make_targets_fileinfo(new_length, hashes)\n-      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n-                                                             new_root_fileinfo))\n-      # Hashes were changed.\n-      new_hashes = {'sha256': self.random_string()}\n-      new_root_fileinfo = tuf.formats.make_targets_fileinfo(length, new_hashes)\n-      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n-                                                             new_root_fileinfo))\n-\n-      # Verify that _fileinfo_has_changed() returns True if no fileinfo (or set\n-      # to None) exists for some role.\n-      self.assertTrue(self.repository_updater._fileinfo_has_changed('bad.json',\n-          new_root_fileinfo))\n-\n-      saved_fileinfo = self.repository_updater.fileinfo['root.json']\n-      self.repository_updater.fileinfo['root.json'] = None\n-      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n-          new_root_fileinfo))\n-\n-\n-      self.repository_updater.fileinfo['root.json'] = saved_fileinfo\n-      new_root_fileinfo['hashes']['sha666'] = '666'\n-      self.repository_updater._fileinfo_has_changed('root.json',\n-          new_root_fileinfo)\n-\n-\n-\n   def test_2__import_delegations(self):\n     # Setup.\n     # In order to test '_import_delegations' the parent of the delegation\n@@ -639,6 +571,20 @@ def test_2__move_current_to_previous(self):\n     self.repository_updater._move_current_to_previous('snapshot')\n     self.assertTrue(os.path.exists(previous_snapshot_filepath))\n \n+    # assert that non-ascii alphanumeric role name \"../\u00e4\" (that is url encoded\n+    # in local filename) works\n+    encoded_current = os.path.join(\n+      self.client_metadata_current, '..%2F%C3%A4.json'\n+    )\n+    encoded_previous = os.path.join(\n+      self.client_metadata_previous, '..%2F%C3%A4.json'\n+    )\n+\n+    with open(encoded_current, \"w\"):\n+      pass\n+    self.repository_updater._move_current_to_previous('../\u00e4')\n+    self.assertTrue(os.path.exists(encoded_previous))\n+\n \n \n \n@@ -2073,6 +2019,64 @@ def test_get_updater(self):\n     self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n \n \n+class TestUpdaterRolenames(unittest_toolbox.Modified_TestCase):\n+  def setUp(self):\n+    unittest_toolbox.Modified_TestCase.setUp(self)\n+\n+    repo_dir = os.path.join(os.getcwd(), 'repository_data', 'fishy_rolenames')\n+\n+    self.client_dir = self.make_temp_directory()\n+    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"))\n+    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"previous\"))\n+    shutil.copy(\n+      os.path.join(repo_dir, 'metadata', '1.root.json'),\n+      os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\", \"root.json\")\n+    )\n+\n+    simple_server_path = os.path.join(os.getcwd(), 'simple_server.py')\n+    self.server_process_handler = utils.TestServerProcess(log=logger,\n+        server=simple_server_path)\n+\n+    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n+        + str(self.server_process_handler.port) + \"/repository_data/fishy_rolenames\"\n+\n+    tuf.settings.repositories_directory = self.client_dir\n+    mirrors = {'mirror1': {\n+      'url_prefix': url_prefix,\n+      'metadata_path': 'metadata/',\n+      'targets_path': ''\n+    }}\n+    self.updater = updater.Updater(\"fishy_rolenames\", mirrors)\n+\n+  def tearDown(self):\n+    tuf.roledb.clear_roledb(clear_all=True)\n+    tuf.keydb.clear_keydb(clear_all=True)\n+    self.server_process_handler.flush_log()\n+    self.server_process_handler.clean()\n+    unittest_toolbox.Modified_TestCase.tearDown(self)\n+\n+  def test_unusual_rolenames(self):\n+    \"\"\"Test rolenames that may be tricky to handle as filenames\n+\n+    The test data in repository_data/fishy_rolenames has been produced\n+    semi-manually using RepositorySimulator: using the RepositorySimulator\n+    in these tests directly (like test_updater_with_simulator.py does for\n+    ngclient) might make more sense... but would require some integration work\n+    \"\"\"\n+\n+    # Make a target search that fetches the delegated targets\n+    self.updater.refresh()\n+    with self.assertRaises(tuf.exceptions.UnknownTargetError):\n+      self.updater.get_one_valid_targetinfo(\"anything\")\n+\n+    # Assert that the metadata files are in the client metadata directory\n+    metadata_dir = os.path.join(\n+      self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"\n+    )\n+    local_metadata = os.listdir(metadata_dir)\n+    for fname in ['%C3%B6.json', '..%2Fa.json', '..json']:\n+      self.assertTrue(fname in local_metadata)\n+\n \n def _load_role_keys(keystore_directory):\n ", "file_path": "files/2021_10/352", "file_language": "py", "file_name": "tests/test_updater.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class TestUpdater(unittest_toolbox.Modified_TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    # Create a temporary directory to store the repository, metadata, and target\n    # files.  'temporary_directory' must be deleted in TearDownModule() so that\n    # temporary files are always removed, even when exceptions occur.\n    cls.temporary_directory = tempfile.mkdtemp(dir=os.getcwd())\n\n    # Needed because in some tests simple_server.py cannot be found.\n    # The reason is that the current working directory\n    # has been changed when executing a subprocess.\n    cls.SIMPLE_SERVER_PATH = os.path.join(os.getcwd(), 'simple_server.py')\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served\n    # by the SimpleHTTPServer launched here.  The test cases of 'test_updater.py'\n    # assume the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n    cls.server_process_handler = utils.TestServerProcess(log=logger,\n        server=cls.SIMPLE_SERVER_PATH)\n\n\n\n  @classmethod\n  def tearDownClass(cls):\n    # Cleans the resources and flush the logged lines (if any).\n    cls.server_process_handler.clean()\n\n    # Remove the temporary repository directory, which should contain all the\n    # metadata, targets, and key files generated for the test cases\n    shutil.rmtree(cls.temporary_directory)\n\n\n\n  def setUp(self):\n    # We are inheriting from custom class.\n    unittest_toolbox.Modified_TestCase.setUp(self)\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    self.repository_name = 'test_repository1'\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf.tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n    temporary_repository_root = \\\n      self.make_temp_directory(directory=self.temporary_directory)\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_client = os.path.join(original_repository_files, 'client')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = \\\n      os.path.join(temporary_repository_root, 'repository')\n    self.keystore_directory = \\\n      os.path.join(temporary_repository_root, 'keystore')\n\n    self.client_directory = os.path.join(temporary_repository_root,\n        'client')\n    self.client_metadata = os.path.join(self.client_directory,\n        self.repository_name, 'metadata')\n    self.client_metadata_current = os.path.join(self.client_metadata,\n        'current')\n    self.client_metadata_previous = os.path.join(self.client_metadata,\n        'previous')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n                                           'metadata_path': 'metadata',\n                                           'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n                                              self.repository_mirrors)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    # Logs stdout and stderr from the sever subprocess.\n    self.server_process_handler.flush_log()\n\n    # Remove temporary directory\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n\n  # UNIT TESTS.\n\n  def test_1__init__exceptions(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, 8,\n                      self.repository_mirrors)\n\n    # Invalid 'repository_mirrors' argument.  'tuf.formats.MIRRORDICT_SCHEMA'\n    # expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, updater.Updater, 8)\n\n\n    # 'tuf.client.updater.py' requires that the client's repositories directory\n    # be configured in 'tuf.settings.py'.\n    tuf.settings.repositories_directory = None\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n    # Test: repository does not exist\n    self.assertRaises(tuf.exceptions.MissingLocalRepositoryError, updater.Updater,\n                      'test_non_existing_repository', self.repository_mirrors)\n\n    # Test: empty client repository (i.e., no metadata directory).\n    metadata_backup = self.client_metadata + '.backup'\n    shutil.move(self.client_metadata, metadata_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's metadata directory.\n    shutil.move(metadata_backup, self.client_metadata)\n\n\n    # Test: repository with only a '{repository_directory}/metadata' directory.\n    # (i.e., missing the required 'current' and 'previous' sub-directories).\n    current_backup = self.client_metadata_current + '.backup'\n    previous_backup = self.client_metadata_previous + '.backup'\n\n    shutil.move(self.client_metadata_current, current_backup)\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n\n    # Restore the client's previous directory.  The required 'current' directory\n    # is still missing.\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test: repository with only a '{repository_directory}/metadata/previous'\n    # directory.\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's current directory.\n    shutil.move(current_backup, self.client_metadata_current)\n\n    # Test: repository with a '{repository_directory}/metadata/current'\n    # directory, but the 'previous' directory is missing.\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test:  repository missing the required 'root.json' file.\n    client_root_file = os.path.join(self.client_metadata_current, 'root.json')\n    backup_root_file = client_root_file + '.backup'\n    shutil.move(client_root_file, backup_root_file)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's 'root.json file.\n    shutil.move(backup_root_file, client_root_file)\n\n    # Test: Normal 'tuf.client.updater.Updater' instantiation.\n    updater.Updater('test_repository1', self.repository_mirrors)\n\n\n\n\n\n  def test_1__load_metadata_from_file(self):\n\n    # Setup\n    # Get the 'role1.json' filepath.  Manually load the role metadata, and\n    # compare it against the loaded metadata by '_load_metadata_from_file()'.\n    role1_filepath = \\\n      os.path.join(self.client_metadata_current, 'role1.json')\n    role1_meta = securesystemslib.util.load_json_file(role1_filepath)\n\n    # Load the 'role1.json' file with _load_metadata_from_file, which should\n    # store the loaded metadata in the 'self.repository_updater.metadata'\n    # store.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n\n    # Verify that the correct number of metadata objects has been loaded\n    # (i.e., only the 'root.json' file should have been loaded.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Verify that the content of root metadata is valid.\n    self.assertEqual(self.repository_updater.metadata['current']['role1'],\n                     role1_meta['signed'])\n\n    # Verify that _load_metadata_from_file() doesn't raise an exception for\n    # improperly formatted metadata, and doesn't load the bad file.\n    with open(role1_filepath, 'ab') as file_object:\n      file_object.write(b'bad JSON data')\n\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Test if we fail gracefully if we can't deserialize a meta file\n    self.repository_updater._load_metadata_from_file('current', 'empty_file')\n    self.assertFalse('empty_file' in self.repository_updater.metadata['current'])\n\n    # Test invalid metadata set argument (must be either\n    # 'current' or 'previous'.)\n    self.assertRaises(securesystemslib.exceptions.Error,\n                      self.repository_updater._load_metadata_from_file,\n                      'bad_metadata_set', 'role1')\n\n\n\n\n  def test_1__rebuild_key_and_role_db(self):\n    # Setup\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_threshold = root_metadata['roles']['root']['threshold']\n    number_of_root_keys = len(root_metadata['keys'])\n\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # Ensure we add 2 to the number of root keys (actually, the number of root\n    # keys multiplied by the number of keyid hash algorithms), to include the\n    # delegated targets key (+1 for its sha512 keyid).  The delegated roles of\n    # 'targets.json' are also loaded when the repository object is\n    # instantiated.\n\n    self.assertEqual(number_of_root_keys + 1, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: normal case.\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # _rebuild_key_and_role_db() will only rebuild the keys and roles specified\n    # in the 'root.json' file, unlike __init__().  Instantiating an updater\n    # object calls both _rebuild_key_and_role_db() and _import_delegations().\n    self.assertEqual(number_of_root_keys, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: properly updated roledb and keydb dicts if the Root role changes.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_metadata['roles']['root']['threshold'] = 8\n    root_metadata['keys'].popitem()\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], 8)\n    self.assertEqual(number_of_root_keys - 1, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n\n\n\n  def test_1__update_versioninfo(self):\n    # Tests\n    # Verify that the 'self.versioninfo' dictionary is empty (it starts off\n    # empty and is only populated if _update_versioninfo() is called.\n    versioninfo_dict = self.repository_updater.versioninfo\n    self.assertEqual(len(versioninfo_dict), 0)\n\n    # Load the versioninfo of the top-level Targets role.  This action\n    # populates the 'self.versioninfo' dictionary.\n    self.repository_updater._update_versioninfo('targets.json')\n    self.assertEqual(len(versioninfo_dict), 1)\n    self.assertTrue(tuf.formats.FILEINFODICT_SCHEMA.matches(versioninfo_dict))\n\n    # The Snapshot role stores the version numbers of all the roles available\n    # on the repository.  Load Snapshot to extract Root's version number\n    # and compare it against the one loaded by 'self.repository_updater'.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    # Verify that the manually loaded version number of root.json matches\n    # the one loaded by the updater object.\n    self.assertTrue('targets.json' in versioninfo_dict)\n    self.assertEqual(versioninfo_dict['targets.json'], targets_versioninfo)\n\n    # Verify that 'self.versioninfo' is incremented if another role is updated.\n    self.repository_updater._update_versioninfo('role1.json')\n    self.assertEqual(len(versioninfo_dict), 2)\n\n    # Verify that 'self.versioninfo' is incremented if a non-existent role is\n    # requested, and has its versioninfo entry set to 'None'.\n    self.repository_updater._update_versioninfo('bad_role.json')\n    self.assertEqual(len(versioninfo_dict), 3)\n    self.assertEqual(versioninfo_dict['bad_role.json'], None)\n\n    # Verify that the versioninfo specified in Timestamp is used if the Snapshot\n    # role hasn't been downloaded yet.\n    del self.repository_updater.metadata['current']['snapshot']\n    #self.assertRaises(self.repository_updater._update_versioninfo('snapshot.json'))\n    self.repository_updater._update_versioninfo('snapshot.json')\n    self.assertEqual(versioninfo_dict['snapshot.json']['version'], 1)\n\n\n\n  def test_1__refresh_must_not_count_duplicate_keyids_towards_threshold(self):\n    # Update root threshold on the server repository and sign twice with 1 key\n    repository = repo_tool.load_repository(self.repository_directory)\n    repository.root.threshold = 2\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n\n    storage_backend = securesystemslib.storage.FilesystemBackend()\n    # The client uses the threshold from the previous root file to verify the\n    # new root. Thus we need to make two updates so that the threshold used for\n    # verification becomes 2. I.e. we bump the version, sign twice with the\n    # same key and write to disk '2.root.json' and '3.root.json'.\n    for version in [2, 3]:\n      repository.root.version = version\n      info = tuf.roledb.get_roleinfo(\"root\")\n      metadata = repo_lib.generate_root_metadata(\n          info[\"version\"], info[\"expires\"], False)\n      signed_metadata = repo_lib.sign_metadata(\n          metadata, info[\"keyids\"], \"root.json\", \"default\")\n      signed_metadata[\"signatures\"].append(signed_metadata[\"signatures\"][0])\n      live_root_path = os.path.join(\n          self.repository_directory, \"metadata\", \"root.json\")\n\n      # Bypass server side verification in 'write' or 'writeall', which would\n      # catch the unmet threshold.\n      # We also skip writing to 'metadata.staged' and copying to 'metadata' and\n      # instead write directly to 'metadata'\n      repo_lib.write_metadata_file(signed_metadata, live_root_path,\n          info[\"version\"], True, storage_backend)\n\n\n    # Update from current '1.root.json' to '3.root.json' on client and assert\n    # raise of 'BadSignatureError' (caused by unmet signature threshold).\n    try:\n      self.repository_updater.refresh()\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      mirror_errors = list(e.mirror_errors.values())\n      self.assertTrue(len(mirror_errors) == 1)\n      self.assertTrue(\n          isinstance(mirror_errors[0],\n          securesystemslib.exceptions.BadSignatureError))\n      self.assertEqual(\n          str(mirror_errors[0]),\n          repr(\"root\") + \" metadata has bad signature.\")\n\n    else:\n      self.fail(\n          \"Expected a NoWorkingMirrorError composed of one BadSignatureError\")\n\n\n  def test_2__import_delegations(self):\n    # Setup.\n    # In order to test '_import_delegations' the parent of the delegation\n    # has to be in Repository.metadata['current'], but it has to be inserted\n    # there without using '_load_metadata_from_file()' since it calls\n    # '_import_delegations()'.\n    repository_name = self.repository_updater.repository_name\n    tuf.keydb.clear_keydb(repository_name)\n    tuf.roledb.clear_roledb(repository_name)\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 0)\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 0)\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n\n    # Take into account the number of keyids algorithms supported by default,\n    # which this test condition expects to be two (sha256 and sha512).\n    self.assertEqual(4, len(tuf.keydb._keydb_dict[repository_name]))\n\n    # Test: pass a role without delegations.\n    self.repository_updater._import_delegations('root')\n\n    # Verify that there was no change to the roledb and keydb dictionaries by\n    # checking the number of elements in the dictionaries.\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n    # Take into account the number of keyid hash algorithms, which this\n    # test condition expects to be one\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4)\n\n    # Test: normal case, first level delegation.\n    self.repository_updater._import_delegations('targets')\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 5)\n    # The number of root keys (times the number of key hash algorithms) +\n    # delegation's key (+1 for its sha512 keyid).\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4 + 1)\n\n    # Verify that roledb dictionary was added.\n    self.assertTrue('role1' in tuf.roledb._roledb_dict[repository_name])\n\n    # Verify that keydb dictionary was updated.\n    role1_signable = \\\n      securesystemslib.util.load_json_file(os.path.join(self.client_metadata_current,\n                                           'role1.json'))\n    keyids = []\n    for signature in role1_signable['signatures']:\n      keyids.append(signature['keyid'])\n\n    for keyid in keyids:\n      self.assertTrue(keyid in tuf.keydb._keydb_dict[repository_name])\n\n    # Verify that _import_delegations() ignores invalid keytypes in the 'keys'\n    # field of parent role's 'delegations'.\n    existing_keyid = keyids[0]\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'bad_keytype'\n    self.repository_updater._import_delegations('targets')\n\n    # Restore the keytype of 'existing_keyid'.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'ed25519'\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated keys is malformed.\n    valid_keyval = self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval']\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = valid_keyval\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated roles is malformed.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['roles'][0]['name'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n\n\n  def test_2__versioninfo_has_been_updated(self):\n    # Verify that the method returns 'False' if a versioninfo was not changed.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    self.assertFalse(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n    # Verify that the method returns 'True' if Root's version number changes.\n    targets_versioninfo['version'] = 8\n    self.assertTrue(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n\n\n\n\n  def test_2__move_current_to_previous(self):\n    # Test case will consist of removing a metadata file from client's\n    # '{client_repository}/metadata/previous' directory, executing the method\n    # and then verifying that the 'previous' directory contains the snapshot\n    # file.\n    previous_snapshot_filepath = os.path.join(self.client_metadata_previous,\n                                              'snapshot.json')\n    os.remove(previous_snapshot_filepath)\n    self.assertFalse(os.path.exists(previous_snapshot_filepath))\n\n    # Verify that the current 'snapshot.json' is moved to the previous directory.\n    self.repository_updater._move_current_to_previous('snapshot')\n    self.assertTrue(os.path.exists(previous_snapshot_filepath))\n\n    # assert that non-ascii alphanumeric role name \"../\u00e4\" (that is url encoded\n    # in local filename) works\n    encoded_current = os.path.join(\n      self.client_metadata_current, '..%2F%C3%A4.json'\n    )\n    encoded_previous = os.path.join(\n      self.client_metadata_previous, '..%2F%C3%A4.json'\n    )\n\n    with open(encoded_current, \"w\"):\n      pass\n    self.repository_updater._move_current_to_previous('../\u00e4')\n    self.assertTrue(os.path.exists(encoded_previous))\n\n\n\n\n\n  def test_2__delete_metadata(self):\n    # This test will verify that 'root' metadata is never deleted.  When a role\n    # is deleted verify that the file is not present in the\n    # 'self.repository_updater.metadata' dictionary.\n    self.repository_updater._delete_metadata('root')\n    self.assertTrue('root' in self.repository_updater.metadata['current'])\n\n    self.repository_updater._delete_metadata('timestamp')\n    self.assertFalse('timestamp' in self.repository_updater.metadata['current'])\n\n\n\n\n\n  def test_2__ensure_not_expired(self):\n    # This test condition will verify that nothing is raised when a metadata\n    # file has a future expiration date.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    self.repository_updater._ensure_not_expired(root_metadata, 'root')\n\n    # Metadata with an expiration time in the future should, of course, not\n    # count as expired\n    expires = tuf.formats.unix_timestamp_to_datetime(int(time.time() + 10))\n    expires = expires.isoformat() + 'Z'\n    root_metadata['expires'] = expires\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    self.repository_updater._ensure_not_expired(root_metadata, 'root')\n\n    # Metadata that expires at the exact current time is considered expired\n    expire_time = int(time.time())\n    expires = \\\n      tuf.formats.unix_timestamp_to_datetime(expire_time).isoformat()+'Z'\n    root_metadata['expires'] = expires\n    mock_time = mock.Mock()\n    mock_time.return_value = expire_time\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    with mock.patch('time.time', mock_time):\n      self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                        self.repository_updater._ensure_not_expired,\n                        root_metadata, 'root')\n\n    # Metadata that expires in the past is considered expired\n    expires = tuf.formats.unix_timestamp_to_datetime(int(time.time() - 10))\n    expires = expires.isoformat() + 'Z'\n    root_metadata['expires'] = expires\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater._ensure_not_expired,\n                      root_metadata, 'root')\n\n\n\n\n\n  def test_3__update_metadata(self):\n    # Setup\n    # _update_metadata() downloads, verifies, and installs the specified\n    # metadata role.  Remove knowledge of currently installed metadata and\n    # verify that they are re-installed after calling _update_metadata().\n\n    # This is the default metadata that we would create for the timestamp role,\n    # because it has no signed metadata for itself.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    # This is the upper bound length for Targets metadata.\n    DEFAULT_TARGETS_FILELENGTH = tuf.settings.DEFAULT_TARGETS_REQUIRED_LENGTH\n\n    # Save the versioninfo of 'targets.json,' needed later when re-installing\n    # with _update_metadata().\n    targets_versioninfo = \\\n      self.repository_updater.metadata['current']['snapshot']['meta']\\\n                                      ['targets.json']\n\n    # Remove the currently installed metadata from the store and disk.  Verify\n    # that the metadata dictionary is re-populated after calling\n    # _update_metadata().\n    del self.repository_updater.metadata['current']['timestamp']\n    del self.repository_updater.metadata['current']['targets']\n\n    timestamp_filepath = \\\n      os.path.join(self.client_metadata_current, 'timestamp.json')\n    targets_filepath = os.path.join(self.client_metadata_current, 'targets.json')\n    root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n    os.remove(timestamp_filepath)\n    os.remove(targets_filepath)\n\n    # Test: normal case.\n    # Verify 'timestamp.json' is properly installed.\n    self.assertFalse('timestamp' in self.repository_updater.metadata)\n\n    logger.info('\\nroleinfo: ' + repr(tuf.roledb.get_rolenames(self.repository_name)))\n    self.repository_updater._update_metadata('timestamp',\n                                             DEFAULT_TIMESTAMP_FILELENGTH)\n    self.assertTrue('timestamp' in self.repository_updater.metadata['current'])\n    os.path.exists(timestamp_filepath)\n\n    # Verify 'targets.json' is properly installed.\n    self.assertFalse('targets' in self.repository_updater.metadata['current'])\n    self.repository_updater._update_metadata('targets',\n                                DEFAULT_TARGETS_FILELENGTH,\n                                targets_versioninfo['version'])\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n\n    targets_signable = securesystemslib.util.load_json_file(targets_filepath)\n    loaded_targets_version = targets_signable['signed']['version']\n    self.assertEqual(targets_versioninfo['version'], loaded_targets_version)\n\n    # Test: Invalid / untrusted version numbers.\n    # Invalid version number for 'targets.json'.\n    self.assertRaises(tuf.exceptions.NoWorkingMirrorError,\n        self.repository_updater._update_metadata,\n        'targets', DEFAULT_TARGETS_FILELENGTH, 88)\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH, 88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.  The version number is checked, so the specific error in\n    # this case should be 'tuf.exceptions.BadVersionNumberError'.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH,\n                                               88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n\n\n\n\n  def test_3__get_metadata_file(self):\n\n    '''\n    This test focuses on making sure that the updater rejects unknown or\n    badly-formatted TUF specification version numbers....\n    '''\n\n    # Make note of the correct supported TUF specification version.\n    correct_specification_version = tuf.SPECIFICATION_VERSION\n\n    # Change it long enough to write new metadata.\n    tuf.SPECIFICATION_VERSION = '0.9.0'\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # Change the supported TUF specification version back to what it should be\n    # so that we can parse the metadata and see that the spec version in the\n    # metadata does not match the code's expected spec version.\n    tuf.SPECIFICATION_VERSION = correct_specification_version\n\n    upperbound_filelength = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n    try:\n      self.repository_updater._get_metadata_file('timestamp', 'timestamp.json',\n      upperbound_filelength, 1)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      # Note that this test provides a piece of metadata which would fail to\n      # be accepted -- with a different error -- if the specification version\n      # number were not a problem.\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(\n            mirror_error, tuf.exceptions.UnsupportedSpecificationError)\n\n    else:\n      self.fail(\n          'Expected a failure to verify metadata when the metadata had a '\n          'specification version number that was unexpected.  '\n          'No error was raised.')\n\n\n\n\n\n  def test_3__update_metadata_if_changed(self):\n    # Setup.\n    # The client repository is initially loaded with only four top-level roles.\n    # Verify that the metadata store contains the metadata of only these four\n    # roles before updating the metadata of 'targets.json'.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Test: normal case.  Update 'targets.json'.  The version number should not\n    # change.\n    self.repository_updater._update_metadata_if_changed('targets')\n\n    # Verify the current version of 'targets.json' has not changed.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = 'file3.txt'\n\n    repository.targets.add_target(target3)\n    repository.root.version = repository.root.version + 1\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update 'targets.json' and verify that the client's current 'targets.json'\n    # has been updated.  'timestamp' and 'snapshot' must be manually updated\n    # so that new 'targets' can be recognized.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    self.repository_updater._update_metadata('timestamp', DEFAULT_TIMESTAMP_FILELENGTH)\n    self.repository_updater._update_metadata_if_changed('snapshot', 'timestamp')\n    self.repository_updater._update_metadata_if_changed('targets')\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertTrue(self.repository_updater.metadata['current']['targets'])\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 2)\n\n    # Test for an invalid 'referenced_metadata' argument.\n    self.assertRaises(tuf.exceptions.RepositoryError,\n        self.repository_updater._update_metadata_if_changed, 'snapshot', 'bad_role')\n\n\n\n  def test_3__targets_of_role(self):\n    # Setup.\n    # Extract the list of targets from 'targets.json', to be compared to what\n    # is returned by _targets_of_role('targets').\n    targets_in_metadata = \\\n      self.repository_updater.metadata['current']['targets']['targets']\n\n    # Test: normal case.\n    targetinfos_list = self.repository_updater._targets_of_role('targets')\n\n    # Verify that the list of targets was returned, and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos_list))\n    for targetinfo in targetinfos_list:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in targets_in_metadata.items())\n\n\n\n\n\n  def test_4_refresh(self):\n    # This unit test is based on adding an extra target file to the\n    # server and rebuilding all server-side metadata.  All top-level metadata\n    # should be updated when the client calls refresh().\n\n    # First verify that an expired root metadata is updated.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.repository_updater.refresh()\n\n    # Second, verify that expired root metadata is not updated if\n    # 'unsafely_update_root_if_necessary' is explicitly set to 'False'.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater.refresh,\n                      unsafely_update_root_if_necessary=False)\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = 'file3.txt'\n\n    repository.targets.add_target(target3)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Reference 'self.Repository.metadata['current']['targets']'.  Ensure\n    # 'target3' is not already specified.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    self.assertFalse(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the roles to be modified.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 1)\n\n    # Test: normal case.  'targes.json' should now specify 'target3', and the\n    # following top-level metadata should have also been updated:\n    # 'snapshot.json' and 'timestamp.json'.\n    self.repository_updater.refresh()\n\n    # Verify that the client's metadata was updated.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    self.assertTrue(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the updated roles.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 2)\n\n\n\n\n\n  def test_4__refresh_targets_metadata(self):\n    # Setup.\n    # It is assumed that the client repository has only loaded the top-level\n    # metadata.  Refresh the 'targets.json' metadata, including all delegated\n    # roles (i.e., the client should add the missing 'role1.json' metadata.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n\n    # Test: normal case.\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n    # Verify that client's metadata files were refreshed successfully.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 6)\n\n    # Test for non-existing rolename.\n    self.repository_updater._refresh_targets_metadata('bad_rolename',\n        refresh_all_delegated_roles=False)\n\n    # Test that non-json metadata in Snapshot is ignored.\n    self.repository_updater.metadata['current']['snapshot']['meta']['bad_role.xml'] = {}\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n\n\n  def test_5_all_targets(self):\n   # Setup\n   # As with '_refresh_targets_metadata()',\n\n   # Update top-level metadata before calling one of the \"targets\" methods, as\n   # recommended by 'updater.py'.\n   self.repository_updater.refresh()\n\n   # Test: normal case.\n   with utils.ignore_deprecation_warnings('tuf.client.updater'):\n    all_targets = self.repository_updater.all_targets()\n\n   # Verify format of 'all_targets', it should correspond to\n   # 'TARGETINFOS_SCHEMA'.\n   self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(all_targets))\n\n   # Verify that there is a correct number of records in 'all_targets' list,\n   # and the expected filepaths specified in the metadata.  On the targets\n   # directory of the repository, there should be 3 target files (2 of\n   # which are specified by 'targets.json'.)  The delegated role 'role1'\n   # specifies 1 target file.  The expected total number targets in\n   # 'all_targets' should be 3.\n   self.assertEqual(len(all_targets), 3)\n\n   target_filepaths = []\n   for target in all_targets:\n    target_filepaths.append(target['filepath'])\n\n   self.assertTrue('file1.txt' in target_filepaths)\n   self.assertTrue('file2.txt' in target_filepaths)\n   self.assertTrue('file3.txt' in target_filepaths)\n\n\n\n\n\n  def test_5_targets_of_role(self):\n    # Setup\n    # Remove knowledge of 'targets.json' from the metadata store.\n    self.repository_updater.metadata['current']['targets']\n\n    # Remove the metadata of the delegated roles.\n    #shutil.rmtree(os.path.join(self.client_metadata, 'targets'))\n    os.remove(os.path.join(self.client_metadata_current, 'targets.json'))\n\n    # Extract the target files specified by the delegated role, 'role1.json',\n    # as available on the server-side version of the role.\n    role1_filepath = os.path.join(self.repository_directory, 'metadata',\n                                  'role1.json')\n    role1_signable = securesystemslib.util.load_json_file(role1_filepath)\n    expected_targets = role1_signable['signed']['targets']\n\n\n    # Test: normal case.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      targetinfos = self.repository_updater.targets_of_role('role1')\n\n    # Verify that the expected role files were downloaded and installed.\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets.json'))\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets',\n                   'role1.json'))\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    self.assertTrue('role1' in self.repository_updater.metadata['current'])\n\n    #  Verify that list of targets was returned and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos))\n    for targetinfo in targetinfos:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in expected_targets.items())\n\n    # Test: Invalid arguments.\n    # targets_of_role() expected a string rolename.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater.targets_of_role,\n                        8)\n      self.assertRaises(tuf.exceptions.UnknownRoleError, self.repository_updater.targets_of_role,\n                        'unknown_rolename')\n\n\n\n\n\n  def test_6_get_one_valid_targetinfo(self):\n    # Setup\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Extract the file information of the targets specified in 'targets.json'.\n    self.repository_updater.refresh()\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n\n    target_files = targets_metadata['targets']\n    # Extract random target from 'target_files', which will be compared to what\n    # is returned by get_one_valid_targetinfo().  Restore the popped target\n    # (dict value stored in the metadata store) so that it can be found later.\n    filepath, fileinfo = target_files.popitem()\n    target_files[filepath] = fileinfo\n\n    target_targetinfo = self.repository_updater.get_one_valid_targetinfo(filepath)\n    self.assertTrue(tuf.formats.TARGETINFO_SCHEMA.matches(target_targetinfo))\n    self.assertEqual(target_targetinfo['filepath'], filepath)\n    self.assertEqual(target_targetinfo['fileinfo'], fileinfo)\n\n    # Test: invalid target path.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        self.repository_updater.get_one_valid_targetinfo,\n        self.random_path().lstrip(os.sep).lstrip('/'))\n\n    # Test updater.get_one_valid_targetinfo() backtracking behavior (enabled by\n    # default.)\n    targets_directory = os.path.join(self.repository_directory, 'targets')\n    os.makedirs(os.path.join(targets_directory, 'foo'))\n\n    foo_package = 'foo/foo1.1.tar.gz'\n    foo_pattern = 'foo/foo*.tar.gz'\n\n    foo_fullpath = os.path.join(targets_directory, foo_package)\n    with open(foo_fullpath, 'wb') as file_object:\n      file_object.write(b'new release')\n\n    # Modify delegations on the remote repository to test backtracking behavior.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n    repository.targets('role4').add_target(foo_package)\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # updater.get_one_valid_targetinfo() should find 'foo1.1.tar.gz' by\n    # backtracking to 'role3'.  'role2' allows backtracking.\n    self.repository_updater.refresh()\n    self.repository_updater.get_one_valid_targetinfo('foo/foo1.1.tar.gz')\n\n    # A leading path separator is disallowed.\n    self.assertRaises(tuf.exceptions.FormatError,\n    self.repository_updater.get_one_valid_targetinfo, '/foo/foo1.1.tar.gz')\n\n    # Test when 'role2' does *not* allow backtracking.  If 'foo/foo1.1.tar.gz'\n    # is not provided by the authoritative 'role2',\n    # updater.get_one_valid_targetinfo() should return a\n    # 'tuf.exceptions.UnknownTargetError' exception.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    repository.targets.revoke('role3')\n    repository.targets.revoke('role4')\n\n    # Ensure we delegate in trusted order (i.e., 'role2' has higher priority.)\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern], terminating=True, list_of_targets=[])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Verify that 'tuf.exceptions.UnknownTargetError' is raised by\n    # updater.get_one_valid_targetinfo().\n    self.repository_updater.refresh()\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n                      self.repository_updater.get_one_valid_targetinfo,\n                      'foo/foo1.1.tar.gz')\n\n    # Verify that a 'tuf.exceptions.FormatError' is raised for delegated paths\n    # that contain a leading path separator.\n    self.assertRaises(tuf.exceptions.FormatError,\n        self.repository_updater.get_one_valid_targetinfo,\n        '/foo/foo1.1.tar.gz')\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n\n  def test_6_download_target(self):\n    # Create temporary directory (destination directory of downloaded targets)\n    # that will be passed as an argument to 'download_target()'.\n    destination_directory = self.make_temp_directory()\n    target_filepaths = \\\n      list(self.repository_updater.metadata['current']['targets']['targets'].keys())\n\n    # Test: normal case.\n    # Get the target info, which is an argument to 'download_target()'.\n\n    # 'target_filepaths' is expected to have at least two targets.  The first\n    # target will be used to test against download_target().  The second\n    # will be used to test against download_target() and a repository with\n    # 'consistent_snapshot' set to True.\n    target_filepath1 = target_filepaths.pop()\n    targetinfo = self.repository_updater.get_one_valid_targetinfo(target_filepath1)\n    self.repository_updater.download_target(targetinfo,\n                                            destination_directory)\n\n    download_filepath = \\\n      os.path.join(destination_directory, target_filepath1.lstrip('/'))\n    self.assertTrue(os.path.exists(download_filepath))\n    length, hashes = \\\n      securesystemslib.util.get_file_details(download_filepath,\n        securesystemslib.settings.HASH_ALGORITHMS)\n    download_targetfileinfo = tuf.formats.make_targets_fileinfo(length, hashes)\n\n    # Add any 'custom' data from the repository's target fileinfo to the\n    # 'download_targetfileinfo' object being tested.\n    if 'custom' in targetinfo['fileinfo']:\n      download_targetfileinfo['custom'] = targetinfo['fileinfo']['custom']\n\n    self.assertEqual(targetinfo['fileinfo'], download_targetfileinfo)\n\n    # Test when consistent snapshots is set.  First, create a valid\n    # repository with consistent snapshots set (root.json contains a\n    # \"consistent_snapshot\" entry that the updater uses to correctly fetch\n    # snapshots.  The updater expects the existence of\n    # '<version_number>.filename' files if root.json sets 'consistent_snapshot\n    # = True'.\n\n    # The repository must be rewritten with 'consistent_snapshot' set.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    # Write metadata for all the top-level roles , since consistent snapshot\n    # is now being set to true (i.e., the pre-generated repository isn't set\n    # to support consistent snapshots.  A new version of targets.json is needed\n    # to ensure <digest>.filename target files are written to disk.\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n\n    repository.writeall(consistent_snapshot=True)\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # And ensure the client has the latest top-level metadata.\n    self.repository_updater.refresh()\n\n    target_filepath2 = target_filepaths.pop()\n    targetinfo2 = self.repository_updater.get_one_valid_targetinfo(target_filepath2)\n    self.repository_updater.download_target(targetinfo2,\n                                            destination_directory)\n\n    # Checks if the file has been successfully downloaded\n    download_filepath = os.path.join(destination_directory, target_filepath2)\n    self.assertTrue(os.path.exists(download_filepath))\n\n    # Removes the file so that it can be downloaded again in the next test\n    os.remove(download_filepath)\n\n    # Test downloading with consistent snapshot enabled, but without adding\n    # the hash of the file as a prefix to its name.\n\n    file1_path = targetinfo2['filepath']\n    file1_hashes = securesystemslib.util.get_file_hashes(\n        os.path.join(self.repository_directory, 'targets', file1_path),\n        hash_algorithms=['sha256', 'sha512'])\n\n    # Currently in the repository directory, those three files exists:\n    # \"file1.txt\", \"<sha256_hash>.file1.txt\" and \"<sha512_hash>.file1.txt\"\n    # where both sha256 and sha512 hashes are for file file1.txt.\n    # Remove the files with the hash digest prefix to ensure that\n    # the served target file is not prefixed.\n    os.remove(os.path.join(self.repository_directory, 'targets',\n        file1_hashes['sha256'] + '.' + file1_path))\n    os.remove(os.path.join(self.repository_directory, 'targets',\n        file1_hashes['sha512'] + '.' + file1_path))\n\n\n    self.repository_updater.download_target(targetinfo2,\n                                            destination_directory,\n                                            prefix_filename_with_hash=False)\n\n    # Checks if the file has been successfully downloaded\n    self.assertTrue(os.path.exists(download_filepath))\n\n    # Test for a destination that cannot be written to (apart from a target\n    # file that already exists at the destination) and which raises an\n    # exception.\n    bad_destination_directory = 'bad' * 2000\n\n    try:\n      self.repository_updater.download_target(targetinfo, bad_destination_directory)\n\n    except OSError as e:\n      self.assertTrue(\n          e.errno in [errno.ENAMETOOLONG, errno.ENOENT, errno.EINVAL],\n          \"wrong errno: \" + str(e.errno))\n\n    else:\n      self.fail('No OSError raised')\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      targetinfo, 8)\n\n    # Test:\n    # Attempt a file download of a valid target, however, a download exception\n    # occurs because the target is not within the mirror's confined target\n    # directories.  Adjust mirrors dictionary, so that 'confined_target_dirs'\n    # field contains at least one confined target and excludes needed target\n    # file.\n    mirrors = self.repository_updater.mirrors\n    for mirror_name, mirror_info in mirrors.items():\n      mirrors[mirror_name]['confined_target_dirs'] = [self.random_path()]\n\n    try:\n      self.repository_updater.download_target(targetinfo,\n                                              destination_directory)\n\n    except tuf.exceptions.NoWorkingMirrorError as exception:\n      # Ensure that no mirrors were found due to mismatch in confined target\n      # directories.  get_list_of_mirrors() returns an empty list in this case,\n      # which does not generate specific exception errors.\n      self.assertEqual(len(exception.mirror_errors), 0)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError with zero mirror errors in it.')\n\n\n\n\n\n  def test_7_updated_targets(self):\n    # Verify that the list of targets returned by updated_targets() contains\n    # all the files that need to be updated, these files include modified and\n    # new target files.  Also, confirm that files that need not to be updated\n    # are absent from the list.\n    # Setup\n\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory which will hold client's target files.\n    destination_directory = self.make_temp_directory()\n\n    # Get the list of target files.  It will be used as an argument to the\n    # 'updated_targets()' function.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    # Test for duplicates and targets in the root directory of the repository.\n    additional_target = all_targets[0].copy()\n    all_targets.append(additional_target)\n    additional_target_in_root_directory = additional_target.copy()\n    additional_target_in_root_directory['filepath'] = 'file1.txt'\n    all_targets.append(additional_target_in_root_directory)\n\n    #  At this point client needs to update and download all targets.\n    # Test: normal cases.\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    # Assumed the pre-generated repository specifies two target files in\n    # 'targets.json' and one delegated target file in 'role1.json'.\n    self.assertEqual(len(updated_targets), 3)\n\n    # Test: download one of the targets.\n    download_target = copy.deepcopy(updated_targets).pop()\n    self.repository_updater.download_target(download_target,\n                                            destination_directory)\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 2)\n\n    # Test: download all the targets.\n    for download_target in all_targets:\n      self.repository_updater.download_target(download_target,\n                                              destination_directory)\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 0)\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      all_targets, 8)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Ensure the client has up-to-date metadata.\n    self.repository_updater.refresh()\n\n    # Verify that the new target file is considered updated.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n    self.assertEqual(len(updated_targets), 1)\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n\n  def test_8_remove_obsolete_targets(self):\n    # Setup.\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory that will hold the client's target files.\n    destination_directory = self.make_temp_directory()\n\n    #  Populate 'destination_direction' with all target files.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    self.assertEqual(len(os.listdir(destination_directory)), 0)\n\n    for target in all_targets:\n      self.repository_updater.download_target(target, destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n\n    # Remove two target files from the server's repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update client's metadata.\n    self.repository_updater.refresh()\n\n    # Test: normal case.\n    # Verify number of target files in 'destination_directory' (should be 1\n    # after the update made to the remote repository), and call\n    # 'remove_obsolete_targets()'.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets,\n                                              destination_directory)\n\n    for updated_target in updated_targets:\n      self.repository_updater.download_target(updated_target,\n                                              destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    #  Verify that, if there are no obsolete files, the number of files\n    #  in 'destination_directory' remains the same.\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    # Test coverage for a destination path that causes an exception not due\n    # to an already removed target.\n    bad_destination_directory = 'bad' * 2000\n    self.repository_updater.remove_obsolete_targets(bad_destination_directory)\n\n    # Test coverage for a target that is not specified in current metadata.\n    del self.repository_updater.metadata['current']['targets']['targets']['file2.txt']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    # Test coverage for a role that doesn't exist in the previously trusted set\n    # of metadata.\n    del self.repository_updater.metadata['previous']['targets']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n  def test_9__get_target_hash(self):\n    # Test normal case.\n    # Test target filepaths with ascii and non-ascii characters.\n    expected_target_hashes = {\n      '/file1.txt': 'e3a3d89eb3b70ce3fbce6017d7b8c12d4abd5635427a0e8a238f53157df85b3d',\n      '/Jalape\\xc3\\xb1o': '78bfd5c314680545eb48ecad508aceb861f8d6e680f4fe1b791da45c298cda88'\n    }\n    for filepath, target_hash in expected_target_hashes.items():\n      self.assertTrue(tuf.formats.RELPATH_SCHEMA.matches(filepath))\n      self.assertTrue(securesystemslib.formats.HASH_SCHEMA.matches(target_hash))\n      self.assertEqual(self.repository_updater._get_target_hash(filepath), target_hash)\n\n    # Test for improperly formatted argument.\n    #self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._get_target_hash, 8)\n\n\n\n\n  def test_10__check_file_length(self):\n    # Test for exception if file object is not equal to trusted file length.\n    with tempfile.TemporaryFile() as temp_file_object:\n      temp_file_object.write(b'X')\n      temp_file_object.seek(0)\n      self.assertRaises(tuf.exceptions.DownloadLengthMismatchError,\n                      self.repository_updater._check_file_length,\n                      temp_file_object, 10)\n\n\n\n\n\n  def test_10__targets_of_role(self):\n    # Test for non-existent role.\n    self.assertRaises(tuf.exceptions.UnknownRoleError,\n                      self.repository_updater._targets_of_role,\n                      'non-existent-role')\n\n    # Test for role that hasn't been loaded yet.\n    del self.repository_updater.metadata['current']['targets']\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets',\n                                                        skip_refresh=True)), 0)\n\n    # 'targets.json' tracks two targets.\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets')),\n                     2)\n\n\n\n  def test_10__preorder_depth_first_walk(self):\n\n    # Test that infinite loop is prevented if the target file is not found and\n    # the max number of delegations is reached.\n    valid_max_number_of_delegations = tuf.settings.MAX_NUMBER_OF_DELEGATIONS\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = 0\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('unknown.txt'))\n\n    # Reset the setting for max number of delegations so that subsequent unit\n    # tests reference the expected setting.\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = valid_max_number_of_delegations\n\n    # Attempt to create a circular delegation, where role1 performs a\n    # delegation to the top-level Targets role.  The updater should ignore the\n    # delegation and not raise an exception.\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    targets_metadata = securesystemslib.util.load_json_file(targets_path)\n    targets_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(targets_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(targets_metadata))\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['name'] = 'targets'\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    role2_path = os.path.join(self.client_metadata_current, 'role2.json')\n    role2_metadata = securesystemslib.util.load_json_file(role2_path)\n    role2_metadata['signed']['delegations']['roles'] = role1_metadata['signed']['delegations']['roles']\n    role2_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role2_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role2_metadata))\n\n    logger.debug('attempting circular delegation')\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('/file8.txt'))\n\n\n\n\n\n\n  def test_10__visit_child_role(self):\n    # Call _visit_child_role and test the dict keys: 'paths',\n    # 'path_hash_prefixes', and if both are missing.\n\n    targets_role = self.repository_updater.metadata['current']['targets']\n    targets_role['delegations']['roles'][0]['paths'] = ['/*.txt', '/target.exe']\n    child_role = targets_role['delegations']['roles'][0]\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/*.exe']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/target.exe'), child_role['name'])\n\n    # Test for a valid path hash prefix...\n    child_role['path_hash_prefixes'] = ['8baf']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), child_role['name'])\n\n    # ... and an invalid one, as well.\n    child_role['path_hash_prefixes'] = ['badd']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), None)\n\n    # Test for a forbidden target.\n    del child_role['path_hash_prefixes']\n    self.repository_updater._visit_child_role(child_role, '/forbidden.tgz')\n\n    # Verify that unequal path_hash_prefixes are skipped.\n    child_role['path_hash_prefixes'] = ['bad', 'bad']\n    self.assertEqual(None, self.repository_updater._visit_child_role(child_role,\n        '/unknown.exe'))\n\n    # Test if both 'path' and 'path_hash_prefixes' are missing.\n    del child_role['paths']\n    del child_role['path_hash_prefixes']\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._visit_child_role,\n        child_role, child_role['name'])\n\n\n\n  def test_11__verify_metadata_file(self):\n    # Test for invalid metadata content.\n    with tempfile.TemporaryFile() as metadata_file_object:\n      metadata_file_object.write(b'X')\n      metadata_file_object.seek(0)\n\n      self.assertRaises(tuf.exceptions.InvalidMetadataJSONError,\n          self.repository_updater._verify_metadata_file,\n          metadata_file_object, 'root')\n\n\n  def test_13__targets_of_role(self):\n    # Test case where a list of targets is given.  By default, the 'targets'\n    # parameter is None.\n    targets = [{'filepath': 'file1.txt', 'fileinfo': {'length': 1, 'hashes': {'sha256': 'abc'}}}]\n    self.repository_updater._targets_of_role('targets',\n        targets=targets, skip_refresh=False)", "target": 0}, {"function": "class TestMultiRepoUpdater(unittest_toolbox.Modified_TestCase):\n\n  def setUp(self):\n    # Modified_Testcase can handle temp dir removal\n    unittest_toolbox.Modified_TestCase.setUp(self)\n    self.temporary_directory = self.make_temp_directory(directory=os.getcwd())\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf/tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n\n    self.temporary_repository_root = tempfile.mkdtemp(dir=self.temporary_directory)\n\n    # Needed because in some tests simple_server.py cannot be found.\n    # The reason is that the current working directory\n    # has been changed when executing a subprocess.\n    self.SIMPLE_SERVER_PATH = os.path.join(os.getcwd(), 'simple_server.py')\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_client = os.path.join(original_repository_files, 'client', 'test_repository1')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_map_file = os.path.join(original_repository_files, 'map.json')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = os.path.join(self.temporary_repository_root,\n        'repository_server1')\n    self.repository_directory2 = os.path.join(self.temporary_repository_root,\n        'repository_server2')\n\n    # Setting 'tuf.settings.repositories_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.temporary_repository_root\n\n    repository_name = 'test_repository1'\n    repository_name2 = 'test_repository2'\n\n    self.client_directory = os.path.join(self.temporary_repository_root,\n        repository_name)\n    self.client_directory2 = os.path.join(self.temporary_repository_root,\n        repository_name2)\n\n    self.keystore_directory = os.path.join(self.temporary_repository_root,\n        'keystore')\n    self.map_file = os.path.join(self.client_directory, 'map.json')\n    self.map_file2 = os.path.join(self.client_directory2, 'map.json')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_repository, self.repository_directory2)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_client, self.client_directory2)\n    shutil.copyfile(original_map_file, self.map_file)\n    shutil.copyfile(original_map_file, self.map_file2)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served by the\n    # SimpleHTTPServer launched here.  The test cases of this unit test assume\n    # the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n\n    # Creates a subprocess running a server.\n    self.server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH, popen_cwd=self.repository_directory)\n\n    logger.debug('Server process started.')\n\n    # Creates a subprocess running a server.\n    self.server_process_handler2 = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH, popen_cwd=self.repository_directory2)\n\n    logger.debug('Server process 2 started.')\n\n    url_prefix = \\\n        'http://' + utils.TEST_HOST_ADDRESS + ':' + \\\n        str(self.server_process_handler.port)\n    url_prefix2 = \\\n        'http://' + utils.TEST_HOST_ADDRESS + ':' + \\\n        str(self.server_process_handler2.port)\n\n    # We have all of the necessary information for two repository mirrors\n    # in map.json, except for url prefixes.\n    # For the url prefixes, we create subprocesses that run a server script.\n    # In server scripts we get a free port from the OS which is sent\n    # back to the parent process.\n    # That's why we dynamically add the ports to the url prefixes\n    # and changing the content of map.json.\n    self.map_file_path = os.path.join(self.client_directory, 'map.json')\n    data = securesystemslib.util.load_json_file(self.map_file_path)\n\n    data['repositories']['test_repository1'] = [url_prefix]\n    data['repositories']['test_repository2'] = [url_prefix2]\n    with open(self.map_file_path, 'w') as f:\n      json.dump(data, f)\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    self.repository_mirrors2 = {'mirror1': {'url_prefix': url_prefix2,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Create the repository instances.  The test cases will use these client\n    # updaters to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(repository_name,\n        self.repository_mirrors)\n    self.repository_updater2 = updater.Updater(repository_name2,\n        self.repository_mirrors2)\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.multi_repo_updater = updater.MultiRepoUpdater(self.map_file)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n\n    # Cleans the resources and flush the logged lines (if any).\n    self.server_process_handler.clean()\n    self.server_process_handler2.clean()\n\n    # updater.Updater() populates the roledb with the name \"test_repository1\"\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    # Remove top-level temporary directory\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n\n\n  # UNIT TESTS.\n  def test__init__(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, 8)\n\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n    # Test for a non-existent map file.\n    self.assertRaises(tuf.exceptions.Error, updater.MultiRepoUpdater,\n        'non-existent.json')\n\n    # Test for a map file that doesn't contain the required fields.\n    root_filepath = os.path.join(\n        self.repository_directory, 'metadata', 'root.json')\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, root_filepath)\n\n    # Test for a valid instantiation.\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n\n\n  def test__target_matches_path_pattern(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n    paths = ['foo*.tgz', 'bar*.tgz', 'file1.txt']\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('bar-1.0.tgz', paths))\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('file1.txt', paths))\n    self.assertFalse(\n        multi_repo_updater._target_matches_path_pattern('baz-1.0.tgz', paths))\n\n\n\n  def test_get_valid_targetinfo(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n    # Verify the multi repo updater refuses to save targetinfo if\n    # required local repositories are missing.\n    repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1')\n    backup_repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1.backup')\n    shutil.move(repo_dir, backup_repo_dir)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the client's repository directory.\n    shutil.move(backup_repo_dir, repo_dir)\n\n    # Verify that the Root file must exist.\n    root_filepath = os.path.join(repo_dir, 'metadata', 'current', 'root.json')\n    backup_root_filepath = os.path.join(root_filepath, root_filepath + '.backup')\n    shutil.move(root_filepath, backup_root_filepath)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the Root file.\n    shutil.move(backup_root_filepath, root_filepath)\n\n    # Test that the first mapping is skipped if it's irrelevant to the target\n    # file.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n\n    # Verify that a targetinfo is not returned for a non-existent target.\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = False\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = True\n\n    # Test for a mapping that sets terminating = True, and that appears before\n    # the final mapping.\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = True\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'bad3.txt')\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = False\n\n    # Test for the case where multiple repos sign for the same target.\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    # Verify that valid targetinfo is matched for two repositories that provide\n    # different custom field.  Make sure to set the 'match_custom_field'\n    # argument to 'False' when calling get_valid_targetinfo().\n    repository = repo_tool.load_repository(self.repository_directory2)\n\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    custom_field = {\"custom\": \"my_custom_data\"}\n    repository.targets.add_target(os.path.basename(target1), custom_field)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Do we get the expected match for the two targetinfo that only differ\n    # by the custom field?\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo(\n        'file1.txt', match_custom_field=False)\n\n    # Verify the case where two repositories provide different targetinfo.\n    # Modify file1.txt so that different length and hashes are reported by the\n    # two repositories.\n    repository = repo_tool.load_repository(self.repository_directory2)\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Ensure the threshold is modified to 2 (assumed to be 1, by default) and\n    # verify that get_valid_targetinfo() raises an UnknownTargetError\n    # despite both repos signing for file1.txt.\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'file1.txt')\n\n\n\n\n\n  def test_get_updater(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n    # Test for a non-existent repository name.\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n\n    # Test get_updater indirectly via the \"private\" _update_from_repository().\n    self.assertRaises(tuf.exceptions.Error, multi_repo_updater._update_from_repository, 'bad_repo_name', 'file3.txt')\n\n    # Test for a repository that doesn't exist.\n    multi_repo_updater.map_file['repositories']['bad_repo_name'] = ['https://bogus:30002']\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))", "target": 0}, {"function": "class TestUpdaterRolenames(unittest_toolbox.Modified_TestCase):\n  def setUp(self):\n    unittest_toolbox.Modified_TestCase.setUp(self)\n\n    repo_dir = os.path.join(os.getcwd(), 'repository_data', 'fishy_rolenames')\n\n    self.client_dir = self.make_temp_directory()\n    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"))\n    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"previous\"))\n    shutil.copy(\n      os.path.join(repo_dir, 'metadata', '1.root.json'),\n      os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\", \"root.json\")\n    )\n\n    simple_server_path = os.path.join(os.getcwd(), 'simple_server.py')\n    self.server_process_handler = utils.TestServerProcess(log=logger,\n        server=simple_server_path)\n\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + \"/repository_data/fishy_rolenames\"\n\n    tuf.settings.repositories_directory = self.client_dir\n    mirrors = {'mirror1': {\n      'url_prefix': url_prefix,\n      'metadata_path': 'metadata/',\n      'targets_path': ''\n    }}\n    self.updater = updater.Updater(\"fishy_rolenames\", mirrors)\n\n  def tearDown(self):\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n    self.server_process_handler.flush_log()\n    self.server_process_handler.clean()\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n  def test_unusual_rolenames(self):\n    \"\"\"Test rolenames that may be tricky to handle as filenames\n\n    The test data in repository_data/fishy_rolenames has been produced\n    semi-manually using RepositorySimulator: using the RepositorySimulator\n    in these tests directly (like test_updater_with_simulator.py does for\n    ngclient) might make more sense... but would require some integration work\n    \"\"\"\n\n    # Make a target search that fetches the delegated targets\n    self.updater.refresh()\n    with self.assertRaises(tuf.exceptions.UnknownTargetError):\n      self.updater.get_one_valid_targetinfo(\"anything\")\n\n    # Assert that the metadata files are in the client metadata directory\n    metadata_dir = os.path.join(\n      self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"\n    )\n    local_metadata = os.listdir(metadata_dir)\n    for fname in ['%C3%B6.json', '..%2Fa.json', '..json']:\n      self.assertTrue(fname in local_metadata)", "target": 0}, {"function": "def _load_role_keys(keystore_directory):\n\n  # Populating 'self.role_keys' by importing the required public and private\n  # keys of 'tuf/tests/repository_data/'.  The role keys are needed when\n  # modifying the remote repository used by the test cases in this unit test.\n\n  # The pre-generated key files in 'repository_data/keystore' are all encrypted with\n  # a 'password' passphrase.\n  EXPECTED_KEYFILE_PASSWORD = 'password'\n\n  # Store and return the cryptography keys of the top-level roles, including 1\n  # delegated role.\n  role_keys = {}\n\n  root_key_file = os.path.join(keystore_directory, 'root_key')\n  targets_key_file = os.path.join(keystore_directory, 'targets_key')\n  snapshot_key_file = os.path.join(keystore_directory, 'snapshot_key')\n  timestamp_key_file = os.path.join(keystore_directory, 'timestamp_key')\n  delegation_key_file = os.path.join(keystore_directory, 'delegation_key')\n\n  role_keys = {'root': {}, 'targets': {}, 'snapshot': {}, 'timestamp': {},\n               'role1': {}}\n\n  # Import the top-level and delegated role public keys.\n  role_keys['root']['public'] = \\\n    repo_tool.import_rsa_publickey_from_file(root_key_file+'.pub')\n  role_keys['targets']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(targets_key_file+'.pub')\n  role_keys['snapshot']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(snapshot_key_file+'.pub')\n  role_keys['timestamp']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(timestamp_key_file+'.pub')\n  role_keys['role1']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(delegation_key_file+'.pub')\n\n  # Import the private keys of the top-level and delegated roles.\n  role_keys['root']['private'] = \\\n    repo_tool.import_rsa_privatekey_from_file(root_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['targets']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(targets_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['snapshot']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(snapshot_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['timestamp']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(timestamp_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['role1']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(delegation_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n\n  return role_keys", "target": 0}], "function_after": [{"function": "class TestUpdater(unittest_toolbox.Modified_TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    # Create a temporary directory to store the repository, metadata, and target\n    # files.  'temporary_directory' must be deleted in TearDownModule() so that\n    # temporary files are always removed, even when exceptions occur.\n    cls.temporary_directory = tempfile.mkdtemp(dir=os.getcwd())\n\n    # Needed because in some tests simple_server.py cannot be found.\n    # The reason is that the current working directory\n    # has been changed when executing a subprocess.\n    cls.SIMPLE_SERVER_PATH = os.path.join(os.getcwd(), 'simple_server.py')\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served\n    # by the SimpleHTTPServer launched here.  The test cases of 'test_updater.py'\n    # assume the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n    cls.server_process_handler = utils.TestServerProcess(log=logger,\n        server=cls.SIMPLE_SERVER_PATH)\n\n\n\n  @classmethod\n  def tearDownClass(cls):\n    # Cleans the resources and flush the logged lines (if any).\n    cls.server_process_handler.clean()\n\n    # Remove the temporary repository directory, which should contain all the\n    # metadata, targets, and key files generated for the test cases\n    shutil.rmtree(cls.temporary_directory)\n\n\n\n  def setUp(self):\n    # We are inheriting from custom class.\n    unittest_toolbox.Modified_TestCase.setUp(self)\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    self.repository_name = 'test_repository1'\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf.tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n    temporary_repository_root = \\\n      self.make_temp_directory(directory=self.temporary_directory)\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_client = os.path.join(original_repository_files, 'client')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = \\\n      os.path.join(temporary_repository_root, 'repository')\n    self.keystore_directory = \\\n      os.path.join(temporary_repository_root, 'keystore')\n\n    self.client_directory = os.path.join(temporary_repository_root,\n        'client')\n    self.client_metadata = os.path.join(self.client_directory,\n        self.repository_name, 'metadata')\n    self.client_metadata_current = os.path.join(self.client_metadata,\n        'current')\n    self.client_metadata_previous = os.path.join(self.client_metadata,\n        'previous')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n                                           'metadata_path': 'metadata',\n                                           'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n                                              self.repository_mirrors)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    # Logs stdout and stderr from the sever subprocess.\n    self.server_process_handler.flush_log()\n\n    # Remove temporary directory\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n\n  # UNIT TESTS.\n\n  def test_1__init__exceptions(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, 8,\n                      self.repository_mirrors)\n\n    # Invalid 'repository_mirrors' argument.  'tuf.formats.MIRRORDICT_SCHEMA'\n    # expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, updater.Updater, 8)\n\n\n    # 'tuf.client.updater.py' requires that the client's repositories directory\n    # be configured in 'tuf.settings.py'.\n    tuf.settings.repositories_directory = None\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n    # Test: repository does not exist\n    self.assertRaises(tuf.exceptions.MissingLocalRepositoryError, updater.Updater,\n                      'test_non_existing_repository', self.repository_mirrors)\n\n    # Test: empty client repository (i.e., no metadata directory).\n    metadata_backup = self.client_metadata + '.backup'\n    shutil.move(self.client_metadata, metadata_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's metadata directory.\n    shutil.move(metadata_backup, self.client_metadata)\n\n\n    # Test: repository with only a '{repository_directory}/metadata' directory.\n    # (i.e., missing the required 'current' and 'previous' sub-directories).\n    current_backup = self.client_metadata_current + '.backup'\n    previous_backup = self.client_metadata_previous + '.backup'\n\n    shutil.move(self.client_metadata_current, current_backup)\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n\n    # Restore the client's previous directory.  The required 'current' directory\n    # is still missing.\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test: repository with only a '{repository_directory}/metadata/previous'\n    # directory.\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's current directory.\n    shutil.move(current_backup, self.client_metadata_current)\n\n    # Test: repository with a '{repository_directory}/metadata/current'\n    # directory, but the 'previous' directory is missing.\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test:  repository missing the required 'root.json' file.\n    client_root_file = os.path.join(self.client_metadata_current, 'root.json')\n    backup_root_file = client_root_file + '.backup'\n    shutil.move(client_root_file, backup_root_file)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's 'root.json file.\n    shutil.move(backup_root_file, client_root_file)\n\n    # Test: Normal 'tuf.client.updater.Updater' instantiation.\n    updater.Updater('test_repository1', self.repository_mirrors)\n\n\n\n\n\n  def test_1__load_metadata_from_file(self):\n\n    # Setup\n    # Get the 'role1.json' filepath.  Manually load the role metadata, and\n    # compare it against the loaded metadata by '_load_metadata_from_file()'.\n    role1_filepath = \\\n      os.path.join(self.client_metadata_current, 'role1.json')\n    role1_meta = securesystemslib.util.load_json_file(role1_filepath)\n\n    # Load the 'role1.json' file with _load_metadata_from_file, which should\n    # store the loaded metadata in the 'self.repository_updater.metadata'\n    # store.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n\n    # Verify that the correct number of metadata objects has been loaded\n    # (i.e., only the 'root.json' file should have been loaded.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Verify that the content of root metadata is valid.\n    self.assertEqual(self.repository_updater.metadata['current']['role1'],\n                     role1_meta['signed'])\n\n    # Verify that _load_metadata_from_file() doesn't raise an exception for\n    # improperly formatted metadata, and doesn't load the bad file.\n    with open(role1_filepath, 'ab') as file_object:\n      file_object.write(b'bad JSON data')\n\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Test if we fail gracefully if we can't deserialize a meta file\n    self.repository_updater._load_metadata_from_file('current', 'empty_file')\n    self.assertFalse('empty_file' in self.repository_updater.metadata['current'])\n\n    # Test invalid metadata set argument (must be either\n    # 'current' or 'previous'.)\n    self.assertRaises(securesystemslib.exceptions.Error,\n                      self.repository_updater._load_metadata_from_file,\n                      'bad_metadata_set', 'role1')\n\n\n\n\n  def test_1__rebuild_key_and_role_db(self):\n    # Setup\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_threshold = root_metadata['roles']['root']['threshold']\n    number_of_root_keys = len(root_metadata['keys'])\n\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # Ensure we add 2 to the number of root keys (actually, the number of root\n    # keys multiplied by the number of keyid hash algorithms), to include the\n    # delegated targets key (+1 for its sha512 keyid).  The delegated roles of\n    # 'targets.json' are also loaded when the repository object is\n    # instantiated.\n\n    self.assertEqual(number_of_root_keys + 1, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: normal case.\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # _rebuild_key_and_role_db() will only rebuild the keys and roles specified\n    # in the 'root.json' file, unlike __init__().  Instantiating an updater\n    # object calls both _rebuild_key_and_role_db() and _import_delegations().\n    self.assertEqual(number_of_root_keys, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: properly updated roledb and keydb dicts if the Root role changes.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_metadata['roles']['root']['threshold'] = 8\n    root_metadata['keys'].popitem()\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], 8)\n    self.assertEqual(number_of_root_keys - 1, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n\n\n\n  def test_1__update_versioninfo(self):\n    # Tests\n    # Verify that the 'self.versioninfo' dictionary is empty (it starts off\n    # empty and is only populated if _update_versioninfo() is called.\n    versioninfo_dict = self.repository_updater.versioninfo\n    self.assertEqual(len(versioninfo_dict), 0)\n\n    # Load the versioninfo of the top-level Targets role.  This action\n    # populates the 'self.versioninfo' dictionary.\n    self.repository_updater._update_versioninfo('targets.json')\n    self.assertEqual(len(versioninfo_dict), 1)\n    self.assertTrue(tuf.formats.FILEINFODICT_SCHEMA.matches(versioninfo_dict))\n\n    # The Snapshot role stores the version numbers of all the roles available\n    # on the repository.  Load Snapshot to extract Root's version number\n    # and compare it against the one loaded by 'self.repository_updater'.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    # Verify that the manually loaded version number of root.json matches\n    # the one loaded by the updater object.\n    self.assertTrue('targets.json' in versioninfo_dict)\n    self.assertEqual(versioninfo_dict['targets.json'], targets_versioninfo)\n\n    # Verify that 'self.versioninfo' is incremented if another role is updated.\n    self.repository_updater._update_versioninfo('role1.json')\n    self.assertEqual(len(versioninfo_dict), 2)\n\n    # Verify that 'self.versioninfo' is incremented if a non-existent role is\n    # requested, and has its versioninfo entry set to 'None'.\n    self.repository_updater._update_versioninfo('bad_role.json')\n    self.assertEqual(len(versioninfo_dict), 3)\n    self.assertEqual(versioninfo_dict['bad_role.json'], None)\n\n    # Verify that the versioninfo specified in Timestamp is used if the Snapshot\n    # role hasn't been downloaded yet.\n    del self.repository_updater.metadata['current']['snapshot']\n    #self.assertRaises(self.repository_updater._update_versioninfo('snapshot.json'))\n    self.repository_updater._update_versioninfo('snapshot.json')\n    self.assertEqual(versioninfo_dict['snapshot.json']['version'], 1)\n\n\n\n  def test_1__refresh_must_not_count_duplicate_keyids_towards_threshold(self):\n    # Update root threshold on the server repository and sign twice with 1 key\n    repository = repo_tool.load_repository(self.repository_directory)\n    repository.root.threshold = 2\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n\n    storage_backend = securesystemslib.storage.FilesystemBackend()\n    # The client uses the threshold from the previous root file to verify the\n    # new root. Thus we need to make two updates so that the threshold used for\n    # verification becomes 2. I.e. we bump the version, sign twice with the\n    # same key and write to disk '2.root.json' and '3.root.json'.\n    for version in [2, 3]:\n      repository.root.version = version\n      info = tuf.roledb.get_roleinfo(\"root\")\n      metadata = repo_lib.generate_root_metadata(\n          info[\"version\"], info[\"expires\"], False)\n      signed_metadata = repo_lib.sign_metadata(\n          metadata, info[\"keyids\"], \"root.json\", \"default\")\n      signed_metadata[\"signatures\"].append(signed_metadata[\"signatures\"][0])\n      live_root_path = os.path.join(\n          self.repository_directory, \"metadata\", \"root.json\")\n\n      # Bypass server side verification in 'write' or 'writeall', which would\n      # catch the unmet threshold.\n      # We also skip writing to 'metadata.staged' and copying to 'metadata' and\n      # instead write directly to 'metadata'\n      repo_lib.write_metadata_file(signed_metadata, live_root_path,\n          info[\"version\"], True, storage_backend)\n\n\n    # Update from current '1.root.json' to '3.root.json' on client and assert\n    # raise of 'BadSignatureError' (caused by unmet signature threshold).\n    try:\n      self.repository_updater.refresh()\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      mirror_errors = list(e.mirror_errors.values())\n      self.assertTrue(len(mirror_errors) == 1)\n      self.assertTrue(\n          isinstance(mirror_errors[0],\n          securesystemslib.exceptions.BadSignatureError))\n      self.assertEqual(\n          str(mirror_errors[0]),\n          repr(\"root\") + \" metadata has bad signature.\")\n\n    else:\n      self.fail(\n          \"Expected a NoWorkingMirrorError composed of one BadSignatureError\")\n\n\n  def test_2__import_delegations(self):\n    # Setup.\n    # In order to test '_import_delegations' the parent of the delegation\n    # has to be in Repository.metadata['current'], but it has to be inserted\n    # there without using '_load_metadata_from_file()' since it calls\n    # '_import_delegations()'.\n    repository_name = self.repository_updater.repository_name\n    tuf.keydb.clear_keydb(repository_name)\n    tuf.roledb.clear_roledb(repository_name)\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 0)\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 0)\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n\n    # Take into account the number of keyids algorithms supported by default,\n    # which this test condition expects to be two (sha256 and sha512).\n    self.assertEqual(4, len(tuf.keydb._keydb_dict[repository_name]))\n\n    # Test: pass a role without delegations.\n    self.repository_updater._import_delegations('root')\n\n    # Verify that there was no change to the roledb and keydb dictionaries by\n    # checking the number of elements in the dictionaries.\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n    # Take into account the number of keyid hash algorithms, which this\n    # test condition expects to be one\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4)\n\n    # Test: normal case, first level delegation.\n    self.repository_updater._import_delegations('targets')\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 5)\n    # The number of root keys (times the number of key hash algorithms) +\n    # delegation's key (+1 for its sha512 keyid).\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4 + 1)\n\n    # Verify that roledb dictionary was added.\n    self.assertTrue('role1' in tuf.roledb._roledb_dict[repository_name])\n\n    # Verify that keydb dictionary was updated.\n    role1_signable = \\\n      securesystemslib.util.load_json_file(os.path.join(self.client_metadata_current,\n                                           'role1.json'))\n    keyids = []\n    for signature in role1_signable['signatures']:\n      keyids.append(signature['keyid'])\n\n    for keyid in keyids:\n      self.assertTrue(keyid in tuf.keydb._keydb_dict[repository_name])\n\n    # Verify that _import_delegations() ignores invalid keytypes in the 'keys'\n    # field of parent role's 'delegations'.\n    existing_keyid = keyids[0]\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'bad_keytype'\n    self.repository_updater._import_delegations('targets')\n\n    # Restore the keytype of 'existing_keyid'.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'ed25519'\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated keys is malformed.\n    valid_keyval = self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval']\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = valid_keyval\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated roles is malformed.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['roles'][0]['name'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n\n\n  def test_2__versioninfo_has_been_updated(self):\n    # Verify that the method returns 'False' if a versioninfo was not changed.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    self.assertFalse(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n    # Verify that the method returns 'True' if Root's version number changes.\n    targets_versioninfo['version'] = 8\n    self.assertTrue(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n\n\n\n\n  def test_2__move_current_to_previous(self):\n    # Test case will consist of removing a metadata file from client's\n    # '{client_repository}/metadata/previous' directory, executing the method\n    # and then verifying that the 'previous' directory contains the snapshot\n    # file.\n    previous_snapshot_filepath = os.path.join(self.client_metadata_previous,\n                                              'snapshot.json')\n    os.remove(previous_snapshot_filepath)\n    self.assertFalse(os.path.exists(previous_snapshot_filepath))\n\n    # Verify that the current 'snapshot.json' is moved to the previous directory.\n    self.repository_updater._move_current_to_previous('snapshot')\n    self.assertTrue(os.path.exists(previous_snapshot_filepath))\n\n    # assert that non-ascii alphanumeric role name \"../\u00e4\" (that is url encoded\n    # in local filename) works\n    encoded_current = os.path.join(\n      self.client_metadata_current, '..%2F%C3%A4.json'\n    )\n    encoded_previous = os.path.join(\n      self.client_metadata_previous, '..%2F%C3%A4.json'\n    )\n\n    with open(encoded_current, \"w\"):\n      pass\n    self.repository_updater._move_current_to_previous('../\u00e4')\n    self.assertTrue(os.path.exists(encoded_previous))\n\n\n\n\n\n  def test_2__delete_metadata(self):\n    # This test will verify that 'root' metadata is never deleted.  When a role\n    # is deleted verify that the file is not present in the\n    # 'self.repository_updater.metadata' dictionary.\n    self.repository_updater._delete_metadata('root')\n    self.assertTrue('root' in self.repository_updater.metadata['current'])\n\n    self.repository_updater._delete_metadata('timestamp')\n    self.assertFalse('timestamp' in self.repository_updater.metadata['current'])\n\n\n\n\n\n  def test_2__ensure_not_expired(self):\n    # This test condition will verify that nothing is raised when a metadata\n    # file has a future expiration date.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    self.repository_updater._ensure_not_expired(root_metadata, 'root')\n\n    # Metadata with an expiration time in the future should, of course, not\n    # count as expired\n    expires = tuf.formats.unix_timestamp_to_datetime(int(time.time() + 10))\n    expires = expires.isoformat() + 'Z'\n    root_metadata['expires'] = expires\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    self.repository_updater._ensure_not_expired(root_metadata, 'root')\n\n    # Metadata that expires at the exact current time is considered expired\n    expire_time = int(time.time())\n    expires = \\\n      tuf.formats.unix_timestamp_to_datetime(expire_time).isoformat()+'Z'\n    root_metadata['expires'] = expires\n    mock_time = mock.Mock()\n    mock_time.return_value = expire_time\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    with mock.patch('time.time', mock_time):\n      self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                        self.repository_updater._ensure_not_expired,\n                        root_metadata, 'root')\n\n    # Metadata that expires in the past is considered expired\n    expires = tuf.formats.unix_timestamp_to_datetime(int(time.time() - 10))\n    expires = expires.isoformat() + 'Z'\n    root_metadata['expires'] = expires\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater._ensure_not_expired,\n                      root_metadata, 'root')\n\n\n\n\n\n  def test_3__update_metadata(self):\n    # Setup\n    # _update_metadata() downloads, verifies, and installs the specified\n    # metadata role.  Remove knowledge of currently installed metadata and\n    # verify that they are re-installed after calling _update_metadata().\n\n    # This is the default metadata that we would create for the timestamp role,\n    # because it has no signed metadata for itself.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    # This is the upper bound length for Targets metadata.\n    DEFAULT_TARGETS_FILELENGTH = tuf.settings.DEFAULT_TARGETS_REQUIRED_LENGTH\n\n    # Save the versioninfo of 'targets.json,' needed later when re-installing\n    # with _update_metadata().\n    targets_versioninfo = \\\n      self.repository_updater.metadata['current']['snapshot']['meta']\\\n                                      ['targets.json']\n\n    # Remove the currently installed metadata from the store and disk.  Verify\n    # that the metadata dictionary is re-populated after calling\n    # _update_metadata().\n    del self.repository_updater.metadata['current']['timestamp']\n    del self.repository_updater.metadata['current']['targets']\n\n    timestamp_filepath = \\\n      os.path.join(self.client_metadata_current, 'timestamp.json')\n    targets_filepath = os.path.join(self.client_metadata_current, 'targets.json')\n    root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n    os.remove(timestamp_filepath)\n    os.remove(targets_filepath)\n\n    # Test: normal case.\n    # Verify 'timestamp.json' is properly installed.\n    self.assertFalse('timestamp' in self.repository_updater.metadata)\n\n    logger.info('\\nroleinfo: ' + repr(tuf.roledb.get_rolenames(self.repository_name)))\n    self.repository_updater._update_metadata('timestamp',\n                                             DEFAULT_TIMESTAMP_FILELENGTH)\n    self.assertTrue('timestamp' in self.repository_updater.metadata['current'])\n    os.path.exists(timestamp_filepath)\n\n    # Verify 'targets.json' is properly installed.\n    self.assertFalse('targets' in self.repository_updater.metadata['current'])\n    self.repository_updater._update_metadata('targets',\n                                DEFAULT_TARGETS_FILELENGTH,\n                                targets_versioninfo['version'])\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n\n    targets_signable = securesystemslib.util.load_json_file(targets_filepath)\n    loaded_targets_version = targets_signable['signed']['version']\n    self.assertEqual(targets_versioninfo['version'], loaded_targets_version)\n\n    # Test: Invalid / untrusted version numbers.\n    # Invalid version number for 'targets.json'.\n    self.assertRaises(tuf.exceptions.NoWorkingMirrorError,\n        self.repository_updater._update_metadata,\n        'targets', DEFAULT_TARGETS_FILELENGTH, 88)\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH, 88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.  The version number is checked, so the specific error in\n    # this case should be 'tuf.exceptions.BadVersionNumberError'.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH,\n                                               88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n\n\n\n\n  def test_3__get_metadata_file(self):\n\n    '''\n    This test focuses on making sure that the updater rejects unknown or\n    badly-formatted TUF specification version numbers....\n    '''\n\n    # Make note of the correct supported TUF specification version.\n    correct_specification_version = tuf.SPECIFICATION_VERSION\n\n    # Change it long enough to write new metadata.\n    tuf.SPECIFICATION_VERSION = '0.9.0'\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # Change the supported TUF specification version back to what it should be\n    # so that we can parse the metadata and see that the spec version in the\n    # metadata does not match the code's expected spec version.\n    tuf.SPECIFICATION_VERSION = correct_specification_version\n\n    upperbound_filelength = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n    try:\n      self.repository_updater._get_metadata_file('timestamp', 'timestamp.json',\n      upperbound_filelength, 1)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      # Note that this test provides a piece of metadata which would fail to\n      # be accepted -- with a different error -- if the specification version\n      # number were not a problem.\n      for mirror_error in e.mirror_errors.values():\n        assert isinstance(\n            mirror_error, tuf.exceptions.UnsupportedSpecificationError)\n\n    else:\n      self.fail(\n          'Expected a failure to verify metadata when the metadata had a '\n          'specification version number that was unexpected.  '\n          'No error was raised.')\n\n\n\n\n\n  def test_3__update_metadata_if_changed(self):\n    # Setup.\n    # The client repository is initially loaded with only four top-level roles.\n    # Verify that the metadata store contains the metadata of only these four\n    # roles before updating the metadata of 'targets.json'.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Test: normal case.  Update 'targets.json'.  The version number should not\n    # change.\n    self.repository_updater._update_metadata_if_changed('targets')\n\n    # Verify the current version of 'targets.json' has not changed.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = 'file3.txt'\n\n    repository.targets.add_target(target3)\n    repository.root.version = repository.root.version + 1\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update 'targets.json' and verify that the client's current 'targets.json'\n    # has been updated.  'timestamp' and 'snapshot' must be manually updated\n    # so that new 'targets' can be recognized.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    self.repository_updater._update_metadata('timestamp', DEFAULT_TIMESTAMP_FILELENGTH)\n    self.repository_updater._update_metadata_if_changed('snapshot', 'timestamp')\n    self.repository_updater._update_metadata_if_changed('targets')\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertTrue(self.repository_updater.metadata['current']['targets'])\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 2)\n\n    # Test for an invalid 'referenced_metadata' argument.\n    self.assertRaises(tuf.exceptions.RepositoryError,\n        self.repository_updater._update_metadata_if_changed, 'snapshot', 'bad_role')\n\n\n\n  def test_3__targets_of_role(self):\n    # Setup.\n    # Extract the list of targets from 'targets.json', to be compared to what\n    # is returned by _targets_of_role('targets').\n    targets_in_metadata = \\\n      self.repository_updater.metadata['current']['targets']['targets']\n\n    # Test: normal case.\n    targetinfos_list = self.repository_updater._targets_of_role('targets')\n\n    # Verify that the list of targets was returned, and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos_list))\n    for targetinfo in targetinfos_list:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in targets_in_metadata.items())\n\n\n\n\n\n  def test_4_refresh(self):\n    # This unit test is based on adding an extra target file to the\n    # server and rebuilding all server-side metadata.  All top-level metadata\n    # should be updated when the client calls refresh().\n\n    # First verify that an expired root metadata is updated.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.repository_updater.refresh()\n\n    # Second, verify that expired root metadata is not updated if\n    # 'unsafely_update_root_if_necessary' is explicitly set to 'False'.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater.refresh,\n                      unsafely_update_root_if_necessary=False)\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = 'file3.txt'\n\n    repository.targets.add_target(target3)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Reference 'self.Repository.metadata['current']['targets']'.  Ensure\n    # 'target3' is not already specified.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    self.assertFalse(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the roles to be modified.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 1)\n\n    # Test: normal case.  'targes.json' should now specify 'target3', and the\n    # following top-level metadata should have also been updated:\n    # 'snapshot.json' and 'timestamp.json'.\n    self.repository_updater.refresh()\n\n    # Verify that the client's metadata was updated.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    self.assertTrue(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the updated roles.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 2)\n\n\n\n\n\n  def test_4__refresh_targets_metadata(self):\n    # Setup.\n    # It is assumed that the client repository has only loaded the top-level\n    # metadata.  Refresh the 'targets.json' metadata, including all delegated\n    # roles (i.e., the client should add the missing 'role1.json' metadata.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n\n    # Test: normal case.\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n    # Verify that client's metadata files were refreshed successfully.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 6)\n\n    # Test for non-existing rolename.\n    self.repository_updater._refresh_targets_metadata('bad_rolename',\n        refresh_all_delegated_roles=False)\n\n    # Test that non-json metadata in Snapshot is ignored.\n    self.repository_updater.metadata['current']['snapshot']['meta']['bad_role.xml'] = {}\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n\n\n  def test_5_all_targets(self):\n   # Setup\n   # As with '_refresh_targets_metadata()',\n\n   # Update top-level metadata before calling one of the \"targets\" methods, as\n   # recommended by 'updater.py'.\n   self.repository_updater.refresh()\n\n   # Test: normal case.\n   with utils.ignore_deprecation_warnings('tuf.client.updater'):\n    all_targets = self.repository_updater.all_targets()\n\n   # Verify format of 'all_targets', it should correspond to\n   # 'TARGETINFOS_SCHEMA'.\n   self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(all_targets))\n\n   # Verify that there is a correct number of records in 'all_targets' list,\n   # and the expected filepaths specified in the metadata.  On the targets\n   # directory of the repository, there should be 3 target files (2 of\n   # which are specified by 'targets.json'.)  The delegated role 'role1'\n   # specifies 1 target file.  The expected total number targets in\n   # 'all_targets' should be 3.\n   self.assertEqual(len(all_targets), 3)\n\n   target_filepaths = []\n   for target in all_targets:\n    target_filepaths.append(target['filepath'])\n\n   self.assertTrue('file1.txt' in target_filepaths)\n   self.assertTrue('file2.txt' in target_filepaths)\n   self.assertTrue('file3.txt' in target_filepaths)\n\n\n\n\n\n  def test_5_targets_of_role(self):\n    # Setup\n    # Remove knowledge of 'targets.json' from the metadata store.\n    self.repository_updater.metadata['current']['targets']\n\n    # Remove the metadata of the delegated roles.\n    #shutil.rmtree(os.path.join(self.client_metadata, 'targets'))\n    os.remove(os.path.join(self.client_metadata_current, 'targets.json'))\n\n    # Extract the target files specified by the delegated role, 'role1.json',\n    # as available on the server-side version of the role.\n    role1_filepath = os.path.join(self.repository_directory, 'metadata',\n                                  'role1.json')\n    role1_signable = securesystemslib.util.load_json_file(role1_filepath)\n    expected_targets = role1_signable['signed']['targets']\n\n\n    # Test: normal case.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      targetinfos = self.repository_updater.targets_of_role('role1')\n\n    # Verify that the expected role files were downloaded and installed.\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets.json'))\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets',\n                   'role1.json'))\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    self.assertTrue('role1' in self.repository_updater.metadata['current'])\n\n    #  Verify that list of targets was returned and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos))\n    for targetinfo in targetinfos:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in expected_targets.items())\n\n    # Test: Invalid arguments.\n    # targets_of_role() expected a string rolename.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater.targets_of_role,\n                        8)\n      self.assertRaises(tuf.exceptions.UnknownRoleError, self.repository_updater.targets_of_role,\n                        'unknown_rolename')\n\n\n\n\n\n  def test_6_get_one_valid_targetinfo(self):\n    # Setup\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Extract the file information of the targets specified in 'targets.json'.\n    self.repository_updater.refresh()\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n\n    target_files = targets_metadata['targets']\n    # Extract random target from 'target_files', which will be compared to what\n    # is returned by get_one_valid_targetinfo().  Restore the popped target\n    # (dict value stored in the metadata store) so that it can be found later.\n    filepath, fileinfo = target_files.popitem()\n    target_files[filepath] = fileinfo\n\n    target_targetinfo = self.repository_updater.get_one_valid_targetinfo(filepath)\n    self.assertTrue(tuf.formats.TARGETINFO_SCHEMA.matches(target_targetinfo))\n    self.assertEqual(target_targetinfo['filepath'], filepath)\n    self.assertEqual(target_targetinfo['fileinfo'], fileinfo)\n\n    # Test: invalid target path.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        self.repository_updater.get_one_valid_targetinfo,\n        self.random_path().lstrip(os.sep).lstrip('/'))\n\n    # Test updater.get_one_valid_targetinfo() backtracking behavior (enabled by\n    # default.)\n    targets_directory = os.path.join(self.repository_directory, 'targets')\n    os.makedirs(os.path.join(targets_directory, 'foo'))\n\n    foo_package = 'foo/foo1.1.tar.gz'\n    foo_pattern = 'foo/foo*.tar.gz'\n\n    foo_fullpath = os.path.join(targets_directory, foo_package)\n    with open(foo_fullpath, 'wb') as file_object:\n      file_object.write(b'new release')\n\n    # Modify delegations on the remote repository to test backtracking behavior.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n    repository.targets('role4').add_target(foo_package)\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # updater.get_one_valid_targetinfo() should find 'foo1.1.tar.gz' by\n    # backtracking to 'role3'.  'role2' allows backtracking.\n    self.repository_updater.refresh()\n    self.repository_updater.get_one_valid_targetinfo('foo/foo1.1.tar.gz')\n\n    # A leading path separator is disallowed.\n    self.assertRaises(tuf.exceptions.FormatError,\n    self.repository_updater.get_one_valid_targetinfo, '/foo/foo1.1.tar.gz')\n\n    # Test when 'role2' does *not* allow backtracking.  If 'foo/foo1.1.tar.gz'\n    # is not provided by the authoritative 'role2',\n    # updater.get_one_valid_targetinfo() should return a\n    # 'tuf.exceptions.UnknownTargetError' exception.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    repository.targets.revoke('role3')\n    repository.targets.revoke('role4')\n\n    # Ensure we delegate in trusted order (i.e., 'role2' has higher priority.)\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern], terminating=True, list_of_targets=[])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Verify that 'tuf.exceptions.UnknownTargetError' is raised by\n    # updater.get_one_valid_targetinfo().\n    self.repository_updater.refresh()\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n                      self.repository_updater.get_one_valid_targetinfo,\n                      'foo/foo1.1.tar.gz')\n\n    # Verify that a 'tuf.exceptions.FormatError' is raised for delegated paths\n    # that contain a leading path separator.\n    self.assertRaises(tuf.exceptions.FormatError,\n        self.repository_updater.get_one_valid_targetinfo,\n        '/foo/foo1.1.tar.gz')\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n\n  def test_6_download_target(self):\n    # Create temporary directory (destination directory of downloaded targets)\n    # that will be passed as an argument to 'download_target()'.\n    destination_directory = self.make_temp_directory()\n    target_filepaths = \\\n      list(self.repository_updater.metadata['current']['targets']['targets'].keys())\n\n    # Test: normal case.\n    # Get the target info, which is an argument to 'download_target()'.\n\n    # 'target_filepaths' is expected to have at least two targets.  The first\n    # target will be used to test against download_target().  The second\n    # will be used to test against download_target() and a repository with\n    # 'consistent_snapshot' set to True.\n    target_filepath1 = target_filepaths.pop()\n    targetinfo = self.repository_updater.get_one_valid_targetinfo(target_filepath1)\n    self.repository_updater.download_target(targetinfo,\n                                            destination_directory)\n\n    download_filepath = \\\n      os.path.join(destination_directory, target_filepath1.lstrip('/'))\n    self.assertTrue(os.path.exists(download_filepath))\n    length, hashes = \\\n      securesystemslib.util.get_file_details(download_filepath,\n        securesystemslib.settings.HASH_ALGORITHMS)\n    download_targetfileinfo = tuf.formats.make_targets_fileinfo(length, hashes)\n\n    # Add any 'custom' data from the repository's target fileinfo to the\n    # 'download_targetfileinfo' object being tested.\n    if 'custom' in targetinfo['fileinfo']:\n      download_targetfileinfo['custom'] = targetinfo['fileinfo']['custom']\n\n    self.assertEqual(targetinfo['fileinfo'], download_targetfileinfo)\n\n    # Test when consistent snapshots is set.  First, create a valid\n    # repository with consistent snapshots set (root.json contains a\n    # \"consistent_snapshot\" entry that the updater uses to correctly fetch\n    # snapshots.  The updater expects the existence of\n    # '<version_number>.filename' files if root.json sets 'consistent_snapshot\n    # = True'.\n\n    # The repository must be rewritten with 'consistent_snapshot' set.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    # Write metadata for all the top-level roles , since consistent snapshot\n    # is now being set to true (i.e., the pre-generated repository isn't set\n    # to support consistent snapshots.  A new version of targets.json is needed\n    # to ensure <digest>.filename target files are written to disk.\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n\n    repository.writeall(consistent_snapshot=True)\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # And ensure the client has the latest top-level metadata.\n    self.repository_updater.refresh()\n\n    target_filepath2 = target_filepaths.pop()\n    targetinfo2 = self.repository_updater.get_one_valid_targetinfo(target_filepath2)\n    self.repository_updater.download_target(targetinfo2,\n                                            destination_directory)\n\n    # Checks if the file has been successfully downloaded\n    download_filepath = os.path.join(destination_directory, target_filepath2)\n    self.assertTrue(os.path.exists(download_filepath))\n\n    # Removes the file so that it can be downloaded again in the next test\n    os.remove(download_filepath)\n\n    # Test downloading with consistent snapshot enabled, but without adding\n    # the hash of the file as a prefix to its name.\n\n    file1_path = targetinfo2['filepath']\n    file1_hashes = securesystemslib.util.get_file_hashes(\n        os.path.join(self.repository_directory, 'targets', file1_path),\n        hash_algorithms=['sha256', 'sha512'])\n\n    # Currently in the repository directory, those three files exists:\n    # \"file1.txt\", \"<sha256_hash>.file1.txt\" and \"<sha512_hash>.file1.txt\"\n    # where both sha256 and sha512 hashes are for file file1.txt.\n    # Remove the files with the hash digest prefix to ensure that\n    # the served target file is not prefixed.\n    os.remove(os.path.join(self.repository_directory, 'targets',\n        file1_hashes['sha256'] + '.' + file1_path))\n    os.remove(os.path.join(self.repository_directory, 'targets',\n        file1_hashes['sha512'] + '.' + file1_path))\n\n\n    self.repository_updater.download_target(targetinfo2,\n                                            destination_directory,\n                                            prefix_filename_with_hash=False)\n\n    # Checks if the file has been successfully downloaded\n    self.assertTrue(os.path.exists(download_filepath))\n\n    # Test for a destination that cannot be written to (apart from a target\n    # file that already exists at the destination) and which raises an\n    # exception.\n    bad_destination_directory = 'bad' * 2000\n\n    try:\n      self.repository_updater.download_target(targetinfo, bad_destination_directory)\n\n    except OSError as e:\n      self.assertTrue(\n          e.errno in [errno.ENAMETOOLONG, errno.ENOENT, errno.EINVAL],\n          \"wrong errno: \" + str(e.errno))\n\n    else:\n      self.fail('No OSError raised')\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      targetinfo, 8)\n\n    # Test:\n    # Attempt a file download of a valid target, however, a download exception\n    # occurs because the target is not within the mirror's confined target\n    # directories.  Adjust mirrors dictionary, so that 'confined_target_dirs'\n    # field contains at least one confined target and excludes needed target\n    # file.\n    mirrors = self.repository_updater.mirrors\n    for mirror_name, mirror_info in mirrors.items():\n      mirrors[mirror_name]['confined_target_dirs'] = [self.random_path()]\n\n    try:\n      self.repository_updater.download_target(targetinfo,\n                                              destination_directory)\n\n    except tuf.exceptions.NoWorkingMirrorError as exception:\n      # Ensure that no mirrors were found due to mismatch in confined target\n      # directories.  get_list_of_mirrors() returns an empty list in this case,\n      # which does not generate specific exception errors.\n      self.assertEqual(len(exception.mirror_errors), 0)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError with zero mirror errors in it.')\n\n\n\n\n\n  def test_7_updated_targets(self):\n    # Verify that the list of targets returned by updated_targets() contains\n    # all the files that need to be updated, these files include modified and\n    # new target files.  Also, confirm that files that need not to be updated\n    # are absent from the list.\n    # Setup\n\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory which will hold client's target files.\n    destination_directory = self.make_temp_directory()\n\n    # Get the list of target files.  It will be used as an argument to the\n    # 'updated_targets()' function.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    # Test for duplicates and targets in the root directory of the repository.\n    additional_target = all_targets[0].copy()\n    all_targets.append(additional_target)\n    additional_target_in_root_directory = additional_target.copy()\n    additional_target_in_root_directory['filepath'] = 'file1.txt'\n    all_targets.append(additional_target_in_root_directory)\n\n    #  At this point client needs to update and download all targets.\n    # Test: normal cases.\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    # Assumed the pre-generated repository specifies two target files in\n    # 'targets.json' and one delegated target file in 'role1.json'.\n    self.assertEqual(len(updated_targets), 3)\n\n    # Test: download one of the targets.\n    download_target = copy.deepcopy(updated_targets).pop()\n    self.repository_updater.download_target(download_target,\n                                            destination_directory)\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 2)\n\n    # Test: download all the targets.\n    for download_target in all_targets:\n      self.repository_updater.download_target(download_target,\n                                              destination_directory)\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 0)\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      all_targets, 8)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Ensure the client has up-to-date metadata.\n    self.repository_updater.refresh()\n\n    # Verify that the new target file is considered updated.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n    self.assertEqual(len(updated_targets), 1)\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n\n  def test_8_remove_obsolete_targets(self):\n    # Setup.\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n\n    # Creates a subprocess running a server.\n    server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory that will hold the client's target files.\n    destination_directory = self.make_temp_directory()\n\n    #  Populate 'destination_direction' with all target files.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    self.assertEqual(len(os.listdir(destination_directory)), 0)\n\n    for target in all_targets:\n      self.repository_updater.download_target(target, destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n\n    # Remove two target files from the server's repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update client's metadata.\n    self.repository_updater.refresh()\n\n    # Test: normal case.\n    # Verify number of target files in 'destination_directory' (should be 1\n    # after the update made to the remote repository), and call\n    # 'remove_obsolete_targets()'.\n    with utils.ignore_deprecation_warnings('tuf.client.updater'):\n      all_targets = self.repository_updater.all_targets()\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets,\n                                              destination_directory)\n\n    for updated_target in updated_targets:\n      self.repository_updater.download_target(updated_target,\n                                              destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    #  Verify that, if there are no obsolete files, the number of files\n    #  in 'destination_directory' remains the same.\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    # Test coverage for a destination path that causes an exception not due\n    # to an already removed target.\n    bad_destination_directory = 'bad' * 2000\n    self.repository_updater.remove_obsolete_targets(bad_destination_directory)\n\n    # Test coverage for a target that is not specified in current metadata.\n    del self.repository_updater.metadata['current']['targets']['targets']['file2.txt']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    # Test coverage for a role that doesn't exist in the previously trusted set\n    # of metadata.\n    del self.repository_updater.metadata['previous']['targets']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    # Cleans the resources and flush the logged lines (if any).\n    server_process_handler.clean()\n\n\n\n  def test_9__get_target_hash(self):\n    # Test normal case.\n    # Test target filepaths with ascii and non-ascii characters.\n    expected_target_hashes = {\n      '/file1.txt': 'e3a3d89eb3b70ce3fbce6017d7b8c12d4abd5635427a0e8a238f53157df85b3d',\n      '/Jalape\\xc3\\xb1o': '78bfd5c314680545eb48ecad508aceb861f8d6e680f4fe1b791da45c298cda88'\n    }\n    for filepath, target_hash in expected_target_hashes.items():\n      self.assertTrue(tuf.formats.RELPATH_SCHEMA.matches(filepath))\n      self.assertTrue(securesystemslib.formats.HASH_SCHEMA.matches(target_hash))\n      self.assertEqual(self.repository_updater._get_target_hash(filepath), target_hash)\n\n    # Test for improperly formatted argument.\n    #self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._get_target_hash, 8)\n\n\n\n\n  def test_10__check_file_length(self):\n    # Test for exception if file object is not equal to trusted file length.\n    with tempfile.TemporaryFile() as temp_file_object:\n      temp_file_object.write(b'X')\n      temp_file_object.seek(0)\n      self.assertRaises(tuf.exceptions.DownloadLengthMismatchError,\n                      self.repository_updater._check_file_length,\n                      temp_file_object, 10)\n\n\n\n\n\n  def test_10__targets_of_role(self):\n    # Test for non-existent role.\n    self.assertRaises(tuf.exceptions.UnknownRoleError,\n                      self.repository_updater._targets_of_role,\n                      'non-existent-role')\n\n    # Test for role that hasn't been loaded yet.\n    del self.repository_updater.metadata['current']['targets']\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets',\n                                                        skip_refresh=True)), 0)\n\n    # 'targets.json' tracks two targets.\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets')),\n                     2)\n\n\n\n  def test_10__preorder_depth_first_walk(self):\n\n    # Test that infinite loop is prevented if the target file is not found and\n    # the max number of delegations is reached.\n    valid_max_number_of_delegations = tuf.settings.MAX_NUMBER_OF_DELEGATIONS\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = 0\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('unknown.txt'))\n\n    # Reset the setting for max number of delegations so that subsequent unit\n    # tests reference the expected setting.\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = valid_max_number_of_delegations\n\n    # Attempt to create a circular delegation, where role1 performs a\n    # delegation to the top-level Targets role.  The updater should ignore the\n    # delegation and not raise an exception.\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    targets_metadata = securesystemslib.util.load_json_file(targets_path)\n    targets_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(targets_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(targets_metadata))\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['name'] = 'targets'\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    role2_path = os.path.join(self.client_metadata_current, 'role2.json')\n    role2_metadata = securesystemslib.util.load_json_file(role2_path)\n    role2_metadata['signed']['delegations']['roles'] = role1_metadata['signed']['delegations']['roles']\n    role2_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role2_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role2_metadata))\n\n    logger.debug('attempting circular delegation')\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('/file8.txt'))\n\n\n\n\n\n\n  def test_10__visit_child_role(self):\n    # Call _visit_child_role and test the dict keys: 'paths',\n    # 'path_hash_prefixes', and if both are missing.\n\n    targets_role = self.repository_updater.metadata['current']['targets']\n    targets_role['delegations']['roles'][0]['paths'] = ['/*.txt', '/target.exe']\n    child_role = targets_role['delegations']['roles'][0]\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/*.exe']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/target.exe'), child_role['name'])\n\n    # Test for a valid path hash prefix...\n    child_role['path_hash_prefixes'] = ['8baf']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), child_role['name'])\n\n    # ... and an invalid one, as well.\n    child_role['path_hash_prefixes'] = ['badd']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), None)\n\n    # Test for a forbidden target.\n    del child_role['path_hash_prefixes']\n    self.repository_updater._visit_child_role(child_role, '/forbidden.tgz')\n\n    # Verify that unequal path_hash_prefixes are skipped.\n    child_role['path_hash_prefixes'] = ['bad', 'bad']\n    self.assertEqual(None, self.repository_updater._visit_child_role(child_role,\n        '/unknown.exe'))\n\n    # Test if both 'path' and 'path_hash_prefixes' are missing.\n    del child_role['paths']\n    del child_role['path_hash_prefixes']\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._visit_child_role,\n        child_role, child_role['name'])\n\n\n\n  def test_11__verify_metadata_file(self):\n    # Test for invalid metadata content.\n    with tempfile.TemporaryFile() as metadata_file_object:\n      metadata_file_object.write(b'X')\n      metadata_file_object.seek(0)\n\n      self.assertRaises(tuf.exceptions.InvalidMetadataJSONError,\n          self.repository_updater._verify_metadata_file,\n          metadata_file_object, 'root')\n\n\n  def test_13__targets_of_role(self):\n    # Test case where a list of targets is given.  By default, the 'targets'\n    # parameter is None.\n    targets = [{'filepath': 'file1.txt', 'fileinfo': {'length': 1, 'hashes': {'sha256': 'abc'}}}]\n    self.repository_updater._targets_of_role('targets',\n        targets=targets, skip_refresh=False)", "target": 0}, {"function": "class TestMultiRepoUpdater(unittest_toolbox.Modified_TestCase):\n\n  def setUp(self):\n    # Modified_Testcase can handle temp dir removal\n    unittest_toolbox.Modified_TestCase.setUp(self)\n    self.temporary_directory = self.make_temp_directory(directory=os.getcwd())\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf/tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n\n    self.temporary_repository_root = tempfile.mkdtemp(dir=self.temporary_directory)\n\n    # Needed because in some tests simple_server.py cannot be found.\n    # The reason is that the current working directory\n    # has been changed when executing a subprocess.\n    self.SIMPLE_SERVER_PATH = os.path.join(os.getcwd(), 'simple_server.py')\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_client = os.path.join(original_repository_files, 'client', 'test_repository1')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_map_file = os.path.join(original_repository_files, 'map.json')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = os.path.join(self.temporary_repository_root,\n        'repository_server1')\n    self.repository_directory2 = os.path.join(self.temporary_repository_root,\n        'repository_server2')\n\n    # Setting 'tuf.settings.repositories_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.temporary_repository_root\n\n    repository_name = 'test_repository1'\n    repository_name2 = 'test_repository2'\n\n    self.client_directory = os.path.join(self.temporary_repository_root,\n        repository_name)\n    self.client_directory2 = os.path.join(self.temporary_repository_root,\n        repository_name2)\n\n    self.keystore_directory = os.path.join(self.temporary_repository_root,\n        'keystore')\n    self.map_file = os.path.join(self.client_directory, 'map.json')\n    self.map_file2 = os.path.join(self.client_directory2, 'map.json')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_repository, self.repository_directory2)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_client, self.client_directory2)\n    shutil.copyfile(original_map_file, self.map_file)\n    shutil.copyfile(original_map_file, self.map_file2)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served by the\n    # SimpleHTTPServer launched here.  The test cases of this unit test assume\n    # the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n\n    # Creates a subprocess running a server.\n    self.server_process_handler = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH, popen_cwd=self.repository_directory)\n\n    logger.debug('Server process started.')\n\n    # Creates a subprocess running a server.\n    self.server_process_handler2 = utils.TestServerProcess(log=logger,\n        server=self.SIMPLE_SERVER_PATH, popen_cwd=self.repository_directory2)\n\n    logger.debug('Server process 2 started.')\n\n    url_prefix = \\\n        'http://' + utils.TEST_HOST_ADDRESS + ':' + \\\n        str(self.server_process_handler.port)\n    url_prefix2 = \\\n        'http://' + utils.TEST_HOST_ADDRESS + ':' + \\\n        str(self.server_process_handler2.port)\n\n    # We have all of the necessary information for two repository mirrors\n    # in map.json, except for url prefixes.\n    # For the url prefixes, we create subprocesses that run a server script.\n    # In server scripts we get a free port from the OS which is sent\n    # back to the parent process.\n    # That's why we dynamically add the ports to the url prefixes\n    # and changing the content of map.json.\n    self.map_file_path = os.path.join(self.client_directory, 'map.json')\n    data = securesystemslib.util.load_json_file(self.map_file_path)\n\n    data['repositories']['test_repository1'] = [url_prefix]\n    data['repositories']['test_repository2'] = [url_prefix2]\n    with open(self.map_file_path, 'w') as f:\n      json.dump(data, f)\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    self.repository_mirrors2 = {'mirror1': {'url_prefix': url_prefix2,\n        'metadata_path': 'metadata', 'targets_path': 'targets'}}\n\n    # Create the repository instances.  The test cases will use these client\n    # updaters to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(repository_name,\n        self.repository_mirrors)\n    self.repository_updater2 = updater.Updater(repository_name2,\n        self.repository_mirrors2)\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.multi_repo_updater = updater.MultiRepoUpdater(self.map_file)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n\n    # Cleans the resources and flush the logged lines (if any).\n    self.server_process_handler.clean()\n    self.server_process_handler2.clean()\n\n    # updater.Updater() populates the roledb with the name \"test_repository1\"\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    # Remove top-level temporary directory\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n\n\n  # UNIT TESTS.\n  def test__init__(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, 8)\n\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n    # Test for a non-existent map file.\n    self.assertRaises(tuf.exceptions.Error, updater.MultiRepoUpdater,\n        'non-existent.json')\n\n    # Test for a map file that doesn't contain the required fields.\n    root_filepath = os.path.join(\n        self.repository_directory, 'metadata', 'root.json')\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, root_filepath)\n\n    # Test for a valid instantiation.\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n\n\n  def test__target_matches_path_pattern(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n    paths = ['foo*.tgz', 'bar*.tgz', 'file1.txt']\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('bar-1.0.tgz', paths))\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('file1.txt', paths))\n    self.assertFalse(\n        multi_repo_updater._target_matches_path_pattern('baz-1.0.tgz', paths))\n\n\n\n  def test_get_valid_targetinfo(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n    # Verify the multi repo updater refuses to save targetinfo if\n    # required local repositories are missing.\n    repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1')\n    backup_repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1.backup')\n    shutil.move(repo_dir, backup_repo_dir)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the client's repository directory.\n    shutil.move(backup_repo_dir, repo_dir)\n\n    # Verify that the Root file must exist.\n    root_filepath = os.path.join(repo_dir, 'metadata', 'current', 'root.json')\n    backup_root_filepath = os.path.join(root_filepath, root_filepath + '.backup')\n    shutil.move(root_filepath, backup_root_filepath)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the Root file.\n    shutil.move(backup_root_filepath, root_filepath)\n\n    # Test that the first mapping is skipped if it's irrelevant to the target\n    # file.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n\n    # Verify that a targetinfo is not returned for a non-existent target.\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = False\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = True\n\n    # Test for a mapping that sets terminating = True, and that appears before\n    # the final mapping.\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = True\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'bad3.txt')\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = False\n\n    # Test for the case where multiple repos sign for the same target.\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    # Verify that valid targetinfo is matched for two repositories that provide\n    # different custom field.  Make sure to set the 'match_custom_field'\n    # argument to 'False' when calling get_valid_targetinfo().\n    repository = repo_tool.load_repository(self.repository_directory2)\n\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    custom_field = {\"custom\": \"my_custom_data\"}\n    repository.targets.add_target(os.path.basename(target1), custom_field)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Do we get the expected match for the two targetinfo that only differ\n    # by the custom field?\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo(\n        'file1.txt', match_custom_field=False)\n\n    # Verify the case where two repositories provide different targetinfo.\n    # Modify file1.txt so that different length and hashes are reported by the\n    # two repositories.\n    repository = repo_tool.load_repository(self.repository_directory2)\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.add_target(os.path.basename(target1))\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Ensure the threshold is modified to 2 (assumed to be 1, by default) and\n    # verify that get_valid_targetinfo() raises an UnknownTargetError\n    # despite both repos signing for file1.txt.\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'file1.txt')\n\n\n\n\n\n  def test_get_updater(self):\n    multi_repo_updater = updater.MultiRepoUpdater(self.map_file_path)\n\n    # Test for a non-existent repository name.\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n\n    # Test get_updater indirectly via the \"private\" _update_from_repository().\n    self.assertRaises(tuf.exceptions.Error, multi_repo_updater._update_from_repository, 'bad_repo_name', 'file3.txt')\n\n    # Test for a repository that doesn't exist.\n    multi_repo_updater.map_file['repositories']['bad_repo_name'] = ['https://bogus:30002']\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))", "target": 0}, {"function": "class TestUpdaterRolenames(unittest_toolbox.Modified_TestCase):\n  def setUp(self):\n    unittest_toolbox.Modified_TestCase.setUp(self)\n\n    repo_dir = os.path.join(os.getcwd(), 'repository_data', 'fishy_rolenames')\n\n    self.client_dir = self.make_temp_directory()\n    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"))\n    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"previous\"))\n    shutil.copy(\n      os.path.join(repo_dir, 'metadata', '1.root.json'),\n      os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\", \"root.json\")\n    )\n\n    simple_server_path = os.path.join(os.getcwd(), 'simple_server.py')\n    self.server_process_handler = utils.TestServerProcess(log=logger,\n        server=simple_server_path)\n\n    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n        + str(self.server_process_handler.port) + \"/repository_data/fishy_rolenames\"\n\n    tuf.settings.repositories_directory = self.client_dir\n    mirrors = {'mirror1': {\n      'url_prefix': url_prefix,\n      'metadata_path': 'metadata/',\n      'targets_path': ''\n    }}\n    self.updater = updater.Updater(\"fishy_rolenames\", mirrors)\n\n  def tearDown(self):\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n    self.server_process_handler.flush_log()\n    self.server_process_handler.clean()\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n  def test_unusual_rolenames(self):\n    \"\"\"Test rolenames that may be tricky to handle as filenames\n\n    The test data in repository_data/fishy_rolenames has been produced\n    semi-manually using RepositorySimulator: using the RepositorySimulator\n    in these tests directly (like test_updater_with_simulator.py does for\n    ngclient) might make more sense... but would require some integration work\n    \"\"\"\n\n    # Make a target search that fetches the delegated targets\n    self.updater.refresh()\n    with self.assertRaises(tuf.exceptions.UnknownTargetError):\n      self.updater.get_one_valid_targetinfo(\"anything\")\n\n    # Assert that the metadata files are in the client metadata directory\n    metadata_dir = os.path.join(\n      self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"\n    )\n    local_metadata = os.listdir(metadata_dir)\n    for fname in ['%C3%B6.json', '..%2Fa.json', '..json']:\n      self.assertTrue(fname in local_metadata)", "target": 0}, {"function": "def _load_role_keys(keystore_directory):\n\n  # Populating 'self.role_keys' by importing the required public and private\n  # keys of 'tuf/tests/repository_data/'.  The role keys are needed when\n  # modifying the remote repository used by the test cases in this unit test.\n\n  # The pre-generated key files in 'repository_data/keystore' are all encrypted with\n  # a 'password' passphrase.\n  EXPECTED_KEYFILE_PASSWORD = 'password'\n\n  # Store and return the cryptography keys of the top-level roles, including 1\n  # delegated role.\n  role_keys = {}\n\n  root_key_file = os.path.join(keystore_directory, 'root_key')\n  targets_key_file = os.path.join(keystore_directory, 'targets_key')\n  snapshot_key_file = os.path.join(keystore_directory, 'snapshot_key')\n  timestamp_key_file = os.path.join(keystore_directory, 'timestamp_key')\n  delegation_key_file = os.path.join(keystore_directory, 'delegation_key')\n\n  role_keys = {'root': {}, 'targets': {}, 'snapshot': {}, 'timestamp': {},\n               'role1': {}}\n\n  # Import the top-level and delegated role public keys.\n  role_keys['root']['public'] = \\\n    repo_tool.import_rsa_publickey_from_file(root_key_file+'.pub')\n  role_keys['targets']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(targets_key_file+'.pub')\n  role_keys['snapshot']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(snapshot_key_file+'.pub')\n  role_keys['timestamp']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(timestamp_key_file+'.pub')\n  role_keys['role1']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(delegation_key_file+'.pub')\n\n  # Import the private keys of the top-level and delegated roles.\n  role_keys['root']['private'] = \\\n    repo_tool.import_rsa_privatekey_from_file(root_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['targets']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(targets_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['snapshot']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(snapshot_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['timestamp']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(timestamp_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['role1']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(delegation_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n\n  return role_keys", "target": 0}]}, {"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tests%2Ftest_updater_with_simulator.py", "code": "#!/usr/bin/env python\n\n# Copyright 2021, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"Test ngclient Updater using the repository simulator\n\"\"\"\n\nimport os\nimport sys\nimport tempfile\nfrom tuf.api.metadata import SPECIFICATION_VERSION, Targets\nfrom typing import Optional, Tuple\nfrom tuf.exceptions import UnsignedMetadataError, BadVersionNumberError\nimport unittest\n\nfrom tuf.ngclient import Updater\n\nfrom tests import utils\nfrom tests.repository_simulator import RepositorySimulator\nfrom securesystemslib import hash as sslib_hash\n\n\nclass TestUpdater(unittest.TestCase):\n    # set dump_dir to trigger repository state dumps\n    dump_dir:Optional[str] = None\n\n    def setUp(self):\n        self.temp_dir = tempfile.TemporaryDirectory()\n        self.metadata_dir = os.path.join(self.temp_dir.name, \"metadata\")\n        self.targets_dir = os.path.join(self.temp_dir.name, \"targets\")\n        os.mkdir(self.metadata_dir)\n        os.mkdir(self.targets_dir)\n\n        # Setup the repository, bootstrap client root.json\n        self.sim = RepositorySimulator()\n        with open(os.path.join(self.metadata_dir, \"root.json\"), \"bw\") as f:\n            root = self.sim.download_bytes(\"https://example.com/metadata/1.root.json\", 100000)\n            f.write(root)\n\n        if self.dump_dir is not None:\n            # create test specific dump directory\n            name = self.id().split('.')[-1]\n            self.sim.dump_dir = os.path.join(self.dump_dir, name)\n            os.mkdir(self.sim.dump_dir)\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    def _run_refresh(self) -> Updater:\n        if self.sim.dump_dir is not None:\n            self.sim.write()\n\n        updater = Updater(\n            self.metadata_dir,\n            \"https://example.com/metadata/\",\n            \"https://example.com/targets/\",\n            self.sim\n        )\n        updater.refresh()\n        return updater\n\n    def test_refresh(self):\n        # Update top level metadata\n        self._run_refresh()\n\n        # New root (root needs to be explicitly signed)\n        self.sim.root.version += 1\n        self.sim.publish_root()\n\n        self._run_refresh()\n\n        # New timestamp\n        self.sim.update_timestamp()\n\n        self._run_refresh()\n\n        # New targets, snapshot, timestamp version\n        self.sim.targets.version += 1\n        self.sim.update_snapshot()\n\n        self._run_refresh()\n\n    targets: utils.DataSet = {\n        \"standard case\": (\"targetpath\", b\"content\", \"targetpath\"),\n        \"non-asci case\": (\"\u00e5\u00e4\u00f6\", b\"more content\", \"%C3%A5%C3%A4%C3%B6\"),\n        \"subdirectory case\": (\"a/b/c/targetpath\", b\"dir target content\", \"a%2Fb%2Fc%2Ftargetpath\"),\n    }\n\n    @utils.run_sub_tests_with_dataset(targets)\n    def test_targets(self, test_case_data: Tuple[str, bytes, str]):\n        targetpath, content, encoded_path = test_case_data\n        # target does not exist yet\n        updater = self._run_refresh()\n        self.assertIsNone(updater.get_one_valid_targetinfo(targetpath))\n\n        # Add targets to repository\n        self.sim.targets.version += 1\n        self.sim.add_target(\"targets\", content, targetpath)\n        self.sim.update_snapshot()\n\n        updater = self._run_refresh()\n        # target now exists, is not in cache yet\n        file_info = updater.get_one_valid_targetinfo(targetpath)\n        self.assertIsNotNone(file_info)\n        self.assertEqual(\n            updater.updated_targets([file_info], self.targets_dir),\n            [file_info]\n        )\n\n        # Assert consistent_snapshot is True and downloaded targets have prefix.\n        self.assertTrue(self.sim.root.consistent_snapshot)\n        self.assertTrue(updater.config.prefix_targets_with_hash)\n        # download target, assert it is in cache and content is correct\n        local_path = updater.download_target(file_info, self.targets_dir)\n        self.assertEqual(\n            updater.updated_targets([file_info], self.targets_dir), []\n        )\n        self.assertTrue(local_path.startswith(self.targets_dir))\n        with open(local_path, \"rb\") as f:\n            self.assertEqual(f.read(), content)\n\n        # Assert that the targetpath was URL encoded as expected.\n        encoded_absolute_path = os.path.join(self.targets_dir, encoded_path)\n        self.assertEqual(local_path, encoded_absolute_path)\n\n\n\n    def test_fishy_rolenames(self):\n        roles_to_filenames = {\n            \"../a\": \"..%2Fa.json\",\n            \"\": \".json\",\n            \".\": \"..json\",\n            \"/\": \"%2F.json\",\n            \"\u00f6\": \"%C3%B6.json\"\n        }\n\n        # Add new delegated targets, update the snapshot\n        spec_version = \".\".join(SPECIFICATION_VERSION)\n        targets = Targets(1, spec_version, self.sim.safe_expiry, {}, None)\n        for role in roles_to_filenames.keys():\n            self.sim.add_delegation(\"targets\", role, targets, False, [\"*\"], None)\n        self.sim.update_snapshot()\n\n        updater = self._run_refresh()\n\n        # trigger updater to fetch the delegated metadata, check filenames\n        updater.get_one_valid_targetinfo(\"anything\")\n        local_metadata = os.listdir(self.metadata_dir)\n        for fname in roles_to_filenames.values():\n            self.assertTrue(fname in local_metadata)\n\n    def test_keys_and_signatures(self):\n        \"\"\"Example of the two trickiest test areas: keys and root updates\"\"\"\n\n        # Update top level metadata\n        self._run_refresh()\n\n        # New targets: signed with a new key that is not in roles keys\n        old_signer = self.sim.signers[\"targets\"].pop()\n        key, signer = self.sim.create_key()\n        self.sim.signers[\"targets\"] = [signer]\n        self.sim.targets.version += 1\n        self.sim.update_snapshot()\n\n        with self.assertRaises(UnsignedMetadataError):\n            self._run_refresh()\n\n        # New root: Add the new key as targets role key\n        # (root changes require explicit publishing)\n        self.sim.root.add_key(\"targets\", key)\n        self.sim.root.version += 1\n        self.sim.publish_root()\n\n        self._run_refresh()\n\n        # New root: Raise targets threshold to 2\n        self.sim.root.roles[\"targets\"].threshold = 2\n        self.sim.root.version += 1\n        self.sim.publish_root()\n\n        with self.assertRaises(UnsignedMetadataError):\n            self._run_refresh()\n\n        # New targets: sign with both new and old key\n        self.sim.signers[\"targets\"] = [signer, old_signer]\n        self.sim.targets.version += 1\n        self.sim.update_snapshot()\n\n        self._run_refresh()\n\n    def test_snapshot_rollback_with_local_snapshot_hash_mismatch(self):\n        # Test triggering snapshot rollback check on a newly downloaded snapshot\n        # when the local snapshot is loaded even when there is a hash mismatch\n        # with timestamp.snapshot_meta.\n\n        # By raising this flag on timestamp update the simulator would:\n        # 1) compute the hash of the new modified version of snapshot\n        # 2) assign the hash to timestamp.snapshot_meta\n        # The purpose is to create a hash mismatch between timestamp.meta and\n        # the local snapshot, but to have hash match between timestamp.meta and\n        # the next snapshot version.\n        self.sim.compute_metafile_hashes_length = True\n\n        # Initialize all metadata and assign targets version higher than 1.\n        self.sim.targets.version = 2\n        self.sim.update_snapshot()\n        self._run_refresh()\n\n        # The new targets should have a lower version than the local trusted one.\n        self.sim.targets.version = 1\n        self.sim.update_snapshot()\n\n        # During the snapshot update, the local snapshot will be loaded even if\n        # there is a hash mismatch with timestamp.snapshot_meta, because it will\n        # be considered as trusted.\n        # Should fail as a new version of snapshot will be fetched which lowers\n        # the snapshot.meta[\"targets.json\"] version by 1 and throws an error.\n        with self.assertRaises(BadVersionNumberError):\n            self._run_refresh()\n\n\nif __name__ == \"__main__\":\n    if \"--dump\" in sys.argv:\n        TestUpdater.dump_dir = tempfile.mkdtemp()\n        print(f\"Repository Simulator dumps in {TestUpdater.dump_dir}\")\n        sys.argv.remove(\"--dump\")\n\n    utils.configure_test_logging(sys.argv)\n    unittest.main()\n", "code_before": "#!/usr/bin/env python\n\n# Copyright 2021, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"Test ngclient Updater using the repository simulator\n\"\"\"\n\nimport os\nimport sys\nimport tempfile\nfrom tuf.api.metadata import SPECIFICATION_VERSION, Targets\nfrom typing import Optional, Tuple\nfrom tuf.exceptions import UnsignedMetadataError, BadVersionNumberError\nimport unittest\n\nfrom tuf.ngclient import Updater\n\nfrom tests import utils\nfrom tests.repository_simulator import RepositorySimulator\nfrom securesystemslib import hash as sslib_hash\n\n\nclass TestUpdater(unittest.TestCase):\n    # set dump_dir to trigger repository state dumps\n    dump_dir:Optional[str] = None\n\n    def setUp(self):\n        self.temp_dir = tempfile.TemporaryDirectory()\n        self.metadata_dir = os.path.join(self.temp_dir.name, \"metadata\")\n        self.targets_dir = os.path.join(self.temp_dir.name, \"targets\")\n        os.mkdir(self.metadata_dir)\n        os.mkdir(self.targets_dir)\n\n        # Setup the repository, bootstrap client root.json\n        self.sim = RepositorySimulator()\n        with open(os.path.join(self.metadata_dir, \"root.json\"), \"bw\") as f:\n            root = self.sim.download_bytes(\"https://example.com/metadata/1.root.json\", 100000)\n            f.write(root)\n\n        if self.dump_dir is not None:\n            # create test specific dump directory\n            name = self.id().split('.')[-1]\n            self.sim.dump_dir = os.path.join(self.dump_dir, name)\n            os.mkdir(self.sim.dump_dir)\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    def _run_refresh(self) -> Updater:\n        if self.sim.dump_dir is not None:\n            self.sim.write()\n\n        updater = Updater(\n            self.metadata_dir,\n            \"https://example.com/metadata/\",\n            \"https://example.com/targets/\",\n            self.sim\n        )\n        updater.refresh()\n        return updater\n\n    def test_refresh(self):\n        # Update top level metadata\n        self._run_refresh()\n\n        # New root (root needs to be explicitly signed)\n        self.sim.root.version += 1\n        self.sim.publish_root()\n\n        self._run_refresh()\n\n        # New timestamp\n        self.sim.update_timestamp()\n\n        self._run_refresh()\n\n        # New targets, snapshot, timestamp version\n        self.sim.targets.version += 1\n        self.sim.update_snapshot()\n\n        self._run_refresh()\n\n    targets: utils.DataSet = {\n        \"standard case\": (\"targetpath\", b\"content\", \"targetpath\"),\n        \"non-asci case\": (\"\u00e5\u00e4\u00f6\", b\"more content\", \"%C3%A5%C3%A4%C3%B6\"),\n        \"subdirectory case\": (\"a/b/c/targetpath\", b\"dir target content\", \"a%2Fb%2Fc%2Ftargetpath\"),\n    }\n\n    @utils.run_sub_tests_with_dataset(targets)\n    def test_targets(self, test_case_data: Tuple[str, bytes, str]):\n        targetpath, content, encoded_path = test_case_data\n        # target does not exist yet\n        updater = self._run_refresh()\n        self.assertIsNone(updater.get_one_valid_targetinfo(targetpath))\n\n        # Add targets to repository\n        self.sim.targets.version += 1\n        self.sim.add_target(\"targets\", content, targetpath)\n        self.sim.update_snapshot()\n\n        updater = self._run_refresh()\n        # target now exists, is not in cache yet\n        file_info = updater.get_one_valid_targetinfo(targetpath)\n        self.assertIsNotNone(file_info)\n        self.assertEqual(\n            updater.updated_targets([file_info], self.targets_dir),\n            [file_info]\n        )\n\n        # Assert consistent_snapshot is True and downloaded targets have prefix.\n        self.assertTrue(self.sim.root.consistent_snapshot)\n        self.assertTrue(updater.config.prefix_targets_with_hash)\n        # download target, assert it is in cache and content is correct\n        local_path = updater.download_target(file_info, self.targets_dir)\n        self.assertEqual(\n            updater.updated_targets([file_info], self.targets_dir), []\n        )\n        self.assertTrue(local_path.startswith(self.targets_dir))\n        with open(local_path, \"rb\") as f:\n            self.assertEqual(f.read(), content)\n\n        # Assert that the targetpath was URL encoded as expected.\n        encoded_absolute_path = os.path.join(self.targets_dir, encoded_path)\n        self.assertEqual(local_path, encoded_absolute_path)\n\n\n\n    def test_fishy_rolenames(self):\n        roles_to_filenames = {\n            \"../a\": \"..%2Fa.json\",\n            \"\": \".json\",\n            \".\": \"..json\",\n            \"/\": \"%2F.json\",\n            \"\u00f6\": \"%C3%B6.json\"\n        }\n\n        # Add new delegated targets, update the snapshot\n        spec_version = \".\".join(SPECIFICATION_VERSION)\n        targets = Targets(1, spec_version, self.sim.safe_expiry, {}, None)\n        for role in roles_to_filenames.keys():\n            self.sim.add_delegation(\"targets\", role, targets, False, [\"*\"], None)\n        self.sim.update_snapshot()\n\n        updater = self._run_refresh()\n\n        # trigger updater to fetch the delegated metadata, check filenames\n        updater.get_one_valid_targetinfo(\"anything\")\n        local_metadata = os.listdir(self.metadata_dir)\n        for fname in roles_to_filenames.values():\n            self.assertTrue(fname in local_metadata)\n\n    def test_keys_and_signatures(self):\n        \"\"\"Example of the two trickiest test areas: keys and root updates\"\"\"\n\n        # Update top level metadata\n        self._run_refresh()\n\n        # New targets: signed with a new key that is not in roles keys\n        old_signer = self.sim.signers[\"targets\"].pop()\n        key, signer = self.sim.create_key()\n        self.sim.signers[\"targets\"] = [signer]\n        self.sim.targets.version += 1\n        self.sim.update_snapshot()\n\n        with self.assertRaises(UnsignedMetadataError):\n            self._run_refresh()\n\n        # New root: Add the new key as targets role key\n        # (root changes require explicit publishing)\n        self.sim.root.add_key(\"targets\", key)\n        self.sim.root.version += 1\n        self.sim.publish_root()\n\n        self._run_refresh()\n\n        # New root: Raise targets threshold to 2\n        self.sim.root.roles[\"targets\"].threshold = 2\n        self.sim.root.version += 1\n        self.sim.publish_root()\n\n        with self.assertRaises(UnsignedMetadataError):\n            self._run_refresh()\n\n        # New targets: sign with both new and old key\n        self.sim.signers[\"targets\"] = [signer, old_signer]\n        self.sim.targets.version += 1\n        self.sim.update_snapshot()\n\n        self._run_refresh()\n\n    def test_snapshot_rollback_with_local_snapshot_hash_mismatch(self):\n        # Test triggering snapshot rollback check on a newly downloaded snapshot\n        # when the local snapshot is loaded even when there is a hash mismatch\n        # with timestamp.snapshot_meta.\n\n        # By raising this flag on timestamp update the simulator would:\n        # 1) compute the hash of the new modified version of snapshot\n        # 2) assign the hash to timestamp.snapshot_meta\n        # The purpose is to create a hash mismatch between timestamp.meta and\n        # the local snapshot, but to have hash match between timestamp.meta and\n        # the next snapshot version.\n        self.sim.compute_metafile_hashes_length = True\n\n        # Initialize all metadata and assign targets version higher than 1.\n        self.sim.targets.version = 2\n        self.sim.update_snapshot()\n        self._run_refresh()\n\n        # The new targets should have a lower version than the local trusted one.\n        self.sim.targets.version = 1\n        self.sim.update_snapshot()\n\n        # During the snapshot update, the local snapshot will be loaded even if\n        # there is a hash mismatch with timestamp.snapshot_meta, because it will\n        # be considered as trusted.\n        # Should fail as a new version of snapshot will be fetched which lowers\n        # the snapshot.meta[\"targets.json\"] version by 1 and throws an error.\n        with self.assertRaises(BadVersionNumberError):\n            self._run_refresh()\n\n\nif __name__ == \"__main__\":\n    if \"--dump\" in sys.argv:\n        TestUpdater.dump_dir = tempfile.mkdtemp()\n        print(f\"Repository Simulator dumps in {TestUpdater.dump_dir}\")\n        sys.argv.remove(\"--dump\")\n\n    utils.configure_test_logging(sys.argv)\n    unittest.main()\n", "patch": "@@ -9,6 +9,7 @@\n import os\n import sys\n import tempfile\n+from tuf.api.metadata import SPECIFICATION_VERSION, Targets\n from typing import Optional, Tuple\n from tuf.exceptions import UnsignedMetadataError, BadVersionNumberError\n import unittest\n@@ -125,6 +126,30 @@ def test_targets(self, test_case_data: Tuple[str, bytes, str]):\n \n \n \n+    def test_fishy_rolenames(self):\n+        roles_to_filenames = {\n+            \"../a\": \"..%2Fa.json\",\n+            \"\": \".json\",\n+            \".\": \"..json\",\n+            \"/\": \"%2F.json\",\n+            \"\u00f6\": \"%C3%B6.json\"\n+        }\n+\n+        # Add new delegated targets, update the snapshot\n+        spec_version = \".\".join(SPECIFICATION_VERSION)\n+        targets = Targets(1, spec_version, self.sim.safe_expiry, {}, None)\n+        for role in roles_to_filenames.keys():\n+            self.sim.add_delegation(\"targets\", role, targets, False, [\"*\"], None)\n+        self.sim.update_snapshot()\n+\n+        updater = self._run_refresh()\n+\n+        # trigger updater to fetch the delegated metadata, check filenames\n+        updater.get_one_valid_targetinfo(\"anything\")\n+        local_metadata = os.listdir(self.metadata_dir)\n+        for fname in roles_to_filenames.values():\n+            self.assertTrue(fname in local_metadata)\n+\n     def test_keys_and_signatures(self):\n         \"\"\"Example of the two trickiest test areas: keys and root updates\"\"\"\n ", "file_path": "files/2021_10/353", "file_language": "py", "file_name": "tests/test_updater_with_simulator.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tuf%2Fclient%2Fupdater.py", "code": "#!/usr/bin/env python\n\n# Copyright 2012 - 2017, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\n<Program Name>\n  updater.py\n\n<Author>\n  Geremy Condra\n  Vladimir Diaz <vladimir.v.diaz@gmail.com>\n\n<Started>\n  July 2012.  Based on a previous version of this module. (VLAD)\n\n<Copyright>\n  See LICENSE-MIT OR LICENSE for licensing information.\n\n<Purpose>\n  'updater.py' is intended to be the only TUF module that software update\n  systems need to utilize.  It provides a single class representing an\n  updater that includes methods to download, install, and verify\n  metadata/target files in a secure manner.  Importing 'updater.py' and\n  instantiating its main class is all that is required by the client prior\n  to a TUF update request.  The importation and instantiation steps allow\n  TUF to load all of the required metadata files and set the repository mirror\n  information.\n\n  An overview of the update process:\n\n  1. The software update system instructs TUF to check for updates.\n\n  2. TUF downloads and verifies timestamp.json.\n\n  3. If timestamp.json indicates that snapshot.json has changed, TUF downloads\n     and verifies snapshot.json.\n\n  4. TUF determines which metadata files listed in snapshot.json differ from\n     those described in the last snapshot.json that TUF has seen.  If root.json\n     has changed, the update process starts over using the new root.json.\n\n  5. TUF provides the software update system with a list of available files\n     according to targets.json.\n\n  6. The software update system instructs TUF to download a specific target\n     file.\n\n  7. TUF downloads and verifies the file and then makes the file available to\n     the software update system.\n\n<Example Client>\n\n  # The client first imports the 'updater.py' module, the only module the\n  # client is required to import.  The client will utilize a single class\n  # from this module.\n  from tuf.client.updater import Updater\n\n  # The only other module the client interacts with is 'tuf.settings'.  The\n  # client accesses this module solely to set the repository directory.\n  # This directory will hold the files downloaded from a remote repository.\n  from tuf import settings\n  settings.repositories_directory = 'local-repository'\n\n  # Next, the client creates a dictionary object containing the repository\n  # mirrors.  The client may download content from any one of these mirrors.\n  # In the example below, a single mirror named 'mirror1' is defined.  The\n  # mirror is located at 'http://localhost:8001', and all of the metadata\n  # and targets files can be found in the 'metadata' and 'targets' directory,\n  # respectively.  If the client wishes to only download target files from\n  # specific directories on the mirror, the 'confined_target_dirs' field\n  # should be set.  In this example, the client hasn't set confined_target_dirs,\n  # which is interpreted as no confinement.\n  # In other words, the client can download\n  # targets from any directory or subdirectories.  If the client had chosen\n  # 'targets1/', they would have been confined to the '/targets/targets1/'\n  # directory on the 'http://localhost:8001' mirror.\n  repository_mirrors = {'mirror1': {'url_prefix': 'http://localhost:8001',\n                                    'metadata_path': 'metadata',\n                                    'targets_path': 'targets'}}\n\n  # The updater may now be instantiated.  The Updater class of 'updater.py'\n  # is called with two arguments.  The first argument assigns a name to this\n  # particular updater and the second argument the repository mirrors defined\n  # above.\n  updater = Updater('updater', repository_mirrors)\n\n  # The client next calls the refresh() method to ensure it has the latest\n  # copies of the metadata files.\n  updater.refresh()\n\n  # get_one_valid_targetinfo() updates role metadata when required.  In other\n  # words, if the client doesn't possess the metadata that lists 'LICENSE.txt',\n  # get_one_valid_targetinfo() will try to fetch / update it.\n  target = updater.get_one_valid_targetinfo('LICENSE.txt')\n\n  # Determine if 'target' has changed since the client's last refresh().  A\n  # target is considered updated if it does not exist in\n  # 'destination_directory' (current directory) or the target located there has\n  # changed.\n  destination_directory = '.'\n  updated_target = updater.updated_targets([target], destination_directory)\n\n  for target in updated_target:\n    updater.download_target(target, destination_directory)\n    # Client code here may also reference target information (including\n    # 'custom') by directly accessing the dictionary entries of the target.\n    # The 'custom' entry is additional file information explicitly set by the\n    # remote repository.\n    target_path = target['filepath']\n    target_length = target['fileinfo']['length']\n    target_hashes = target['fileinfo']['hashes']\n    target_custom_data = target['fileinfo']['custom']\n\"\"\"\n\nimport errno\nimport logging\nimport os\nimport shutil\nimport time\nimport fnmatch\nimport copy\nimport warnings\nimport io\nfrom urllib import parse\n\nfrom securesystemslib import exceptions as sslib_exceptions\nfrom securesystemslib import formats as sslib_formats\nfrom securesystemslib import hash as sslib_hash\nfrom securesystemslib import keys as sslib_keys\nfrom securesystemslib import util as sslib_util\n\nimport tuf\nfrom tuf import download\nfrom tuf import exceptions\nfrom tuf import formats\nfrom tuf import keydb\nfrom tuf import log # pylint: disable=unused-import\nfrom tuf import mirrors\nfrom tuf import roledb\nfrom tuf import settings\nfrom tuf import sig\nfrom tuf import requests_fetcher\n\n# The Timestamp role does not have signed metadata about it; otherwise we\n# would need an infinite regress of metadata. Therefore, we use some\n# default, but sane, upper file length for its metadata.\nDEFAULT_TIMESTAMP_UPPERLENGTH = settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n# The Root role may be updated without knowing its version number if\n# top-level metadata cannot be safely downloaded (e.g., keys may have been\n# revoked, thus requiring a new Root file that includes the updated keys)\n# and 'unsafely_update_root_if_necessary' is True.\n# We use some default, but sane, upper file length for its metadata.\nDEFAULT_ROOT_UPPERLENGTH = settings.DEFAULT_ROOT_REQUIRED_LENGTH\n\n# See 'log.py' to learn how logging is handled in TUF.\nlogger = logging.getLogger(__name__)\n\n\nclass MultiRepoUpdater(object):\n  \"\"\"\n  <Purpose>\n    Provide a way for clients to request a target file from multiple\n    repositories.  Which repositories to query is determined by the map\n    file (i.e,. map.json).\n\n    See TAP 4 for more information on the map file and how to request updates\n    from multiple repositories.  TAP 4 describes how users may specify that a\n    particular threshold of repositories be used for some targets, while a\n    different threshold of repositories be used for others.\n\n  <Arguments>\n    map_file:\n      The path of the map file.  The map file is needed to determine which\n      repositories to query given a target file.\n\n  <Exceptions>\n    securesystemslib.exceptions.FormatError, if the map file is improperly\n    formatted.\n\n    tuf.exceptions.Error, if the map file cannot be loaded.\n\n  <Side Effects>\n    None.\n\n  <Returns>\n    None.\n  \"\"\"\n\n  def __init__(self, map_file):\n    # Is 'map_file' a path?  If not, raise\n    # 'securesystemslib.exceptions.FormatError'.  The actual content of the map\n    # file is validated later on in this method.\n    sslib_formats.PATH_SCHEMA.check_match(map_file)\n\n    # A dictionary mapping repositories to TUF updaters.\n    self.repository_names_to_updaters = {}\n\n    try:\n      # The map file dictionary that associates targets with repositories.\n      self.map_file = sslib_util.load_json_file(map_file)\n\n    except (sslib_exceptions.Error) as e:\n      raise exceptions.Error('Cannot load the map file: ' + str(e))\n\n    # Raise securesystemslib.exceptions.FormatError if the map file is\n    # improperly formatted.\n    formats.MAPFILE_SCHEMA.check_match(self.map_file)\n\n    # Save the \"repositories\" entry of the map file, with the following\n    # example format:\n    #\n    #  \"repositories\": {\n    #      \"Django\": [\"https://djangoproject.com/\"],\n    #      \"PyPI\":   [\"https://pypi.python.org/\"]\n    #  }\n    self.repository_names_to_mirrors = self.map_file['repositories']\n\n\n\n  def get_valid_targetinfo(self, target_filename, match_custom_field=True):\n    \"\"\"\n    <Purpose>\n      Get valid targetinfo, if any, for the given 'target_filename'.  The map\n      file controls the targetinfo returned (see TAP 4).  Return a dict of the\n      form {updater1: targetinfo, updater2: targetinfo, ...}, where the dict\n      keys are updater objects, and the dict values the matching targetinfo for\n      'target_filename'.\n\n    <Arguments>\n      target_filename:\n        The relative path of the target file to update.\n\n      match_custom_field:\n        Boolean that indicates whether the optional custom field in targetinfo\n        should match across the targetinfo provided by the threshold of\n        repositories.\n\n    <Exceptions>\n      tuf.exceptions.FormatError, if the argument is improperly formatted.\n\n      tuf.exceptions.Error, if the required local metadata directory or the\n      Root file does not exist.\n\n      tuf.exceptions.UnknownTargetError, if the repositories in the map file do\n      not agree on the target, or none of them have signed for the target.\n\n    <Side Effects>\n      None.\n\n    <Returns>\n      A dict of the form: {updater1: targetinfo, updater2: targetinfo, ...}.\n      The targetinfo (conformant with tuf.formats.TARGETINFO_SCHEMA) is for\n      'target_filename'.\n    \"\"\"\n\n    # Is the argument properly formatted?  If not, raise\n    # 'tuf.exceptions.FormatError'.\n    formats.RELPATH_SCHEMA.check_match(target_filename)\n\n    # TAP 4 requires that the following attributes be present in mappings:\n    # \"paths\", \"repositories\", \"terminating\", and \"threshold\".\n    formats.MAPPING_SCHEMA.check_match(self.map_file['mapping'])\n\n    # Set the top-level directory containing the metadata for each repository.\n    repositories_directory = settings.repositories_directory\n\n    # Verify that the required local directories exist for each repository.\n    self._verify_metadata_directories(repositories_directory)\n\n    # Iterate mappings.\n    # [{\"paths\": [], \"repositories\": [], \"terminating\": Boolean, \"threshold\":\n    # NUM}, ...]\n    for mapping in self.map_file['mapping']:\n\n      logger.debug('Interrogating mappings..' + repr(mapping))\n      if not self._target_matches_path_pattern(\n          target_filename, mapping['paths']):\n        # The mapping is irrelevant to the target file.  Try the next one, if\n        # any.\n        continue\n\n      # The mapping is relevant to the target...\n      else:\n        # Do the repositories in the mapping provide a threshold of matching\n        # targetinfo?\n        valid_targetinfo = self._matching_targetinfo(target_filename,\n            mapping, match_custom_field)\n\n        if valid_targetinfo:\n          return valid_targetinfo\n\n        else:\n          # If we are here, it means either (1) the mapping is irrelevant to\n          # the target, (2) the targets were missing from all repositories in\n          # this mapping, or (3) the targets on all repositories did not match.\n          # Whatever the case may be, are we allowed to continue to the next\n          # mapping?  Let's check the terminating entry!\n          if not mapping['terminating']:\n            logger.debug('The mapping was irrelevant to the target, and'\n                ' \"terminating\" was set to False.  Trying the next mapping...')\n            continue\n\n          else:\n            raise exceptions.UnknownTargetError('The repositories in the'\n                ' mapping do not agree on the target, or none of them have'\n                ' signed for the target, and \"terminating\" was set to True.')\n\n    # If we are here, it means either there were no mappings, or none of the\n    # mappings provided the target.\n    logger.debug('Did not find valid targetinfo for ' + repr(target_filename))\n    raise exceptions.UnknownTargetError('The repositories in the map'\n        ' file do not agree on the target, or none of them have signed'\n        ' for the target.')\n\n\n\n\n\n  def _verify_metadata_directories(self, repositories_directory):\n    # Iterate 'self.repository_names_to_mirrors' and verify that the expected\n    # local files and directories exist.  TAP 4 requires a separate local\n    # directory for each repository.\n    for repository_name in self.repository_names_to_mirrors:\n\n      logger.debug('Interrogating repository: ' + repr(repository_name))\n      # Each repository must cache its metadata in a separate location.\n      repository_directory = os.path.join(repositories_directory,\n          repository_name)\n\n      if not os.path.isdir(repository_directory):\n        raise exceptions.Error('The metadata directory'\n            ' for ' + repr(repository_name) + ' must exist'\n            ' at ' + repr(repository_directory))\n\n      else:\n        logger.debug('Found local directory for ' + repr(repository_name))\n\n      # The latest known root metadata file must also exist on disk.\n      root_file = os.path.join(\n          repository_directory, 'metadata', 'current', 'root.json')\n\n      if not os.path.isfile(root_file):\n        raise exceptions.Error(\n            'The Root file must exist at ' + repr(root_file))\n\n      else:\n        logger.debug('Found local Root file at ' + repr(root_file))\n\n\n\n\n\n  def _matching_targetinfo(\n      self, target_filename, mapping, match_custom_field=True):\n    valid_targetinfo = {}\n\n    # Retrieve the targetinfo from each repository using the underlying\n    # Updater() instance.\n    for repository_name in mapping['repositories']:\n      logger.debug('Retrieving targetinfo for ' + repr(target_filename) +\n          ' from repository...')\n\n      try:\n        targetinfo, updater = self._update_from_repository(\n            repository_name, target_filename)\n\n      except (exceptions.UnknownTargetError, exceptions.Error):\n        continue\n\n      valid_targetinfo[updater] = targetinfo\n\n      matching_targetinfo = {}\n      logger.debug('Verifying that a threshold of targetinfo are equal...')\n\n      # Iterate 'valid_targetinfo', looking for a threshold number of matches\n      # for 'targetinfo'.  The first targetinfo to reach the required threshold\n      # is returned.  For example, suppose the following list of targetinfo and\n      # a threshold of 2:\n      # [A, B, C, B, A, C]\n      # In this case, targetinfo B is returned.\n      for valid_updater, compared_targetinfo in valid_targetinfo.items():\n\n        if not self._targetinfo_match(\n            targetinfo, compared_targetinfo, match_custom_field):\n          continue\n\n        else:\n\n          matching_targetinfo[valid_updater] = targetinfo\n\n          if not len(matching_targetinfo) >= mapping['threshold']:\n            continue\n\n          else:\n            logger.debug('Found a threshold of matching targetinfo!')\n            # We now have a targetinfo (that matches across a threshold of\n            # repositories as instructed by the map file), along with the\n            # updaters that sign for it.\n            logger.debug(\n                'Returning updaters for targetinfo: ' + repr(targetinfo))\n\n            return matching_targetinfo\n\n    return None\n\n\n\n\n\n  def _targetinfo_match(self, targetinfo1, targetinfo2, match_custom_field=True):\n    if match_custom_field:\n      return (targetinfo1 == targetinfo2)\n\n    else:\n      targetinfo1_without_custom = copy.deepcopy(targetinfo1)\n      targetinfo2_without_custom = copy.deepcopy(targetinfo2)\n      targetinfo1_without_custom['fileinfo'].pop('custom', None)\n      targetinfo2_without_custom['fileinfo'].pop('custom', None)\n\n      return (targetinfo1_without_custom == targetinfo2_without_custom)\n\n\n\n\n\n  def _target_matches_path_pattern(self, target_filename, path_patterns):\n    for path_pattern in path_patterns:\n      logger.debug('Interrogating pattern ' + repr(path_pattern) + 'for'\n          ' target: ' + repr(target_filename))\n\n      # Example: \"foo.tgz\" should match with \"/*.tgz\".  Make sure to strip any\n      # leading path separators so that a match is made if a repo maintainer\n      # uses a leading separator with a delegated glob pattern, but a client\n      # doesn't include one when a target file is requested.\n      if fnmatch.fnmatch(target_filename.lstrip(os.sep), path_pattern.lstrip(os.sep)):\n        logger.debug('Found a match for ' + repr(target_filename))\n        return True\n\n      else:\n        logger.debug('Continue searching for relevant paths.')\n        continue\n\n    # If we are here, then none of the paths are relevant to the target.\n    logger.debug('None of the paths are relevant.')\n    return False\n\n\n\n\n\n\n  def get_updater(self, repository_name):\n    \"\"\"\n    <Purpose>\n      Get the updater instance corresponding to 'repository_name'.\n\n    <Arguments>\n      repository_name:\n        The name of the repository as it appears in the map file.  For example,\n        \"Django\" and \"PyPI\" in the \"repositories\" entry of the map file.\n\n        \"repositories\": {\n            \"Django\": [\"https://djangoproject.com/\"],\n            \"PyPI\":   [\"https://pypi.python.org/\"]\n        }\n\n    <Exceptions>\n      tuf.exceptions.FormatError, if any of the arguments are improperly\n      formatted.\n\n    <Side Effects>\n      None.\n\n    <Returns>\n      Returns the Updater() instance for 'repository_name'.  If the instance\n      does not exist, return None.\n    \"\"\"\n\n    # Are the arguments properly formatted?  If not, raise\n    # 'tuf.exceptions.FormatError'.\n    formats.NAME_SCHEMA.check_match(repository_name)\n\n    updater = self.repository_names_to_updaters.get(repository_name)\n\n    if not updater:\n\n      if repository_name not in self.repository_names_to_mirrors:\n        return None\n\n      else:\n        # Create repository mirrors object needed by the\n        # tuf.client.updater.Updater().  Each 'repository_name' can have more\n        # than one mirror.\n        repo_mirrors = {}\n\n        for url in self.repository_names_to_mirrors[repository_name]:\n          repo_mirrors[url] = {\n            'url_prefix': url,\n            'metadata_path': 'metadata',\n            'targets_path': 'targets'}\n\n        try:\n          # NOTE: State (e.g., keys) should NOT be shared across different\n          # updater instances.\n          logger.debug('Adding updater for ' + repr(repository_name))\n          updater = Updater(repository_name, repo_mirrors)\n\n        except Exception:\n          return None\n\n        else:\n          self.repository_names_to_updaters[repository_name] = updater\n\n    else:\n      logger.debug('Found an updater for ' + repr(repository_name))\n\n    # Ensure the updater's metadata is the latest before returning it.\n    updater.refresh()\n    return updater\n\n\n\n\n\n  def _update_from_repository(self, repository_name, target_filename):\n\n    updater = self.get_updater(repository_name)\n\n    if not updater:\n      raise exceptions.Error(\n          'Cannot load updater for ' + repr(repository_name))\n\n    else:\n      # Get one valid target info from the Updater object.\n      # 'tuf.exceptions.UnknownTargetError' raised by get_one_valid_targetinfo\n      # if a valid target cannot be found.\n      return updater.get_one_valid_targetinfo(target_filename), updater\n\n\n\n\n\nclass Updater(object):\n  \"\"\"\n  <Purpose>\n    Provide a class that can download target files securely.  The updater\n    keeps track of currently and previously trusted metadata, target files\n    available to the client, target file attributes such as file size and\n    hashes, key and role information, metadata signatures, and the ability\n    to determine when the download of a file should be permitted.\n\n  <Updater Attributes>\n    self.metadata:\n      Dictionary holding the currently and previously trusted metadata.\n\n      Example: {'current': {'root': ROOT_SCHEMA,\n                            'targets':TARGETS_SCHEMA, ...},\n                'previous': {'root': ROOT_SCHEMA,\n                             'targets':TARGETS_SCHEMA, ...}}\n\n    self.metadata_directory:\n      The directory where trusted metadata is stored.\n\n    self.versioninfo:\n      A cache of version numbers for the roles available on the repository.\n\n      Example: {'targets.json': {'version': 128}, ...}\n\n    self.mirrors:\n      The repository mirrors from which metadata and targets are available.\n      Conformant to 'tuf.formats.MIRRORDICT_SCHEMA'.\n\n    self.repository_name:\n      The name of the updater instance.\n\n  <Updater Methods>\n    refresh():\n      This method downloads, verifies, and loads metadata for the top-level\n      roles in a specific order (i.e., root -> timestamp -> snapshot -> targets)\n      The expiration time for downloaded metadata is also verified.\n\n      The metadata for delegated roles are not refreshed by this method, but by\n      the method that returns targetinfo (i.e., get_one_valid_targetinfo()).\n      The refresh() method should be called by the client before any target\n      requests.\n\n    get_one_valid_targetinfo(file_path):\n      Returns the target information for a specific file identified by its file\n      path.  This target method also downloads the metadata of updated targets.\n\n    updated_targets(targets, destination_directory):\n      After the client has retrieved the target information for those targets\n      they are interested in updating, they would call this method to determine\n      which targets have changed from those saved locally on disk.  All the\n      targets that have changed are returns in a list.  From this list, they\n      can request a download by calling 'download_target()'.\n\n    download_target(target, destination_directory):\n      This method performs the actual download of the specified target.  The\n      file is saved to the 'destination_directory' argument.\n\n    remove_obsolete_targets(destination_directory):\n      Any files located in 'destination_directory' that were previously\n      served by the repository but have since been removed, can be deleted\n      from disk by the client by calling this method.\n\n    Note: The methods listed above are public and intended for the software\n    updater integrating TUF with this module.  All other methods that may begin\n    with a single leading underscore are non-public and only used internally.\n    updater.py is not subclassed in TUF, nor is it designed to be subclassed,\n    so double leading underscores is not used.\n    http://www.python.org/dev/peps/pep-0008/#method-names-and-instance-variables\n  \"\"\"\n\n  def __init__(self, repository_name, repository_mirrors, fetcher=None):\n    \"\"\"\n    <Purpose>\n      Constructor.  Instantiating an updater object causes all the metadata\n      files for the top-level roles to be read from disk, including the key and\n      role information for the delegated targets of 'targets'.  The actual\n      metadata for delegated roles is not loaded in __init__.  The metadata for\n      these delegated roles, including nested delegated roles, are loaded,\n      updated, and saved to the 'self.metadata' store, as needed, by\n      get_one_valid_targetinfo().\n\n      The initial set of metadata files are provided by the software update\n      system utilizing TUF.\n\n      In order to use an updater, the following directories must already\n      exist locally:\n\n            {tuf.settings.repositories_directory}/{repository_name}/metadata/current\n            {tuf.settings.repositories_directory}/{repository_name}/metadata/previous\n\n      and, at a minimum, the root metadata file must exist:\n\n            {tuf.settings.repositories_directory}/{repository_name}/metadata/current/root.json\n\n    <Arguments>\n      repository_name:\n        The name of the repository.\n\n      repository_mirrors:\n        A dictionary holding repository mirror information, conformant to\n        'tuf.formats.MIRRORDICT_SCHEMA'.  This dictionary holds\n        information such as the directory containing the metadata and target\n        files, the server's URL prefix, and the target content directories the\n        client should be confined to.\n\n        repository_mirrors = {'mirror1': {'url_prefix': 'http://localhost:8001',\n                                          'metadata_path': 'metadata',\n                                          'targets_path': 'targets',\n                                          'confined_target_dirs': ['']}}\n\n      fetcher:\n        A concrete 'FetcherInterface' implementation. Performs the network\n        related download operations. If an external implementation is not\n        provided, tuf.fetcher.RequestsFetcher is used.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If the arguments are improperly formatted.\n\n      tuf.exceptions.RepositoryError:\n        If there is an error with the updater's repository files, such\n        as a missing 'root.json' file.\n\n    <Side Effects>\n      Th metadata files (e.g., 'root.json', 'targets.json') for the top- level\n      roles are read from disk and stored in dictionaries.  In addition, the\n      key and roledb modules are populated with 'repository_name' entries.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Do the arguments have the correct format?\n    # These checks ensure the arguments have the appropriate\n    # number of objects and object types and that all dict\n    # keys are properly named.\n    # Raise 'securesystemslib.exceptions.FormatError' if there is a mistmatch.\n    sslib_formats.NAME_SCHEMA.check_match(repository_name)\n    formats.MIRRORDICT_SCHEMA.check_match(repository_mirrors)\n\n    # Save the validated arguments.\n    self.repository_name = repository_name\n    self.mirrors = repository_mirrors\n\n    # Initialize Updater with an externally provided 'fetcher' implementing\n    # the network download. By default tuf.fetcher.RequestsFetcher is used.\n    if fetcher is None:\n      self.fetcher = requests_fetcher.RequestsFetcher()\n    else:\n      self.fetcher = fetcher\n\n    # Store the trusted metadata read from disk.\n    self.metadata = {}\n\n    # Store the currently trusted/verified metadata.\n    self.metadata['current'] = {}\n\n    # Store the previously trusted/verified metadata.\n    self.metadata['previous'] = {}\n\n    # Store the version numbers of roles available on the repository.  The dict\n    # keys are paths, and the dict values versioninfo data. This information\n    # can help determine whether a metadata file has changed and needs to be\n    # re-downloaded.\n    self.versioninfo = {}\n\n    # Store the file information of the root and snapshot roles.  The dict keys\n    # are paths, the dict values fileinfo data. This information can help\n    # determine whether a metadata file has changed and so needs to be\n    # re-downloaded.\n    self.fileinfo = {}\n\n    # Store the location of the client's metadata directory.\n    self.metadata_directory = {}\n\n    # Store the 'consistent_snapshot' of the Root role.  This setting\n    # determines if metadata and target files downloaded from remote\n    # repositories include the digest.\n    self.consistent_snapshot = False\n\n    # Ensure the repository metadata directory has been set.\n    if settings.repositories_directory is None:\n      raise exceptions.RepositoryError('The TUF update client'\n        ' module must specify the directory containing the local repository'\n        ' files.  \"tuf.settings.repositories_directory\" MUST be set.')\n\n    # Set the path for the current set of metadata files.\n    repositories_directory = settings.repositories_directory\n    repository_directory = os.path.join(repositories_directory, self.repository_name)\n\n    # raise MissingLocalRepository if the repo does not exist at all.\n    if not os.path.exists(repository_directory):\n      raise exceptions.MissingLocalRepositoryError('Local repository ' +\n        repr(repository_directory) + ' does not exist.')\n\n    current_path = os.path.join(repository_directory, 'metadata', 'current')\n\n    # Ensure the current path is valid/exists before saving it.\n    if not os.path.exists(current_path):\n      raise exceptions.RepositoryError('Missing'\n        ' ' + repr(current_path) + '.  This path must exist and, at a minimum,'\n        ' contain the Root metadata file.')\n\n    self.metadata_directory['current'] = current_path\n\n    # Set the path for the previous set of metadata files.\n    previous_path = os.path.join(repository_directory, 'metadata', 'previous')\n\n    # Ensure the previous path is valid/exists.\n    if not os.path.exists(previous_path):\n      raise exceptions.RepositoryError('Missing ' + repr(previous_path) + '.'\n        '  This path MUST exist.')\n\n    self.metadata_directory['previous'] = previous_path\n\n    # Load current and previous metadata.\n    for metadata_set in ['current', 'previous']:\n      for metadata_role in roledb.TOP_LEVEL_ROLES:\n        self._load_metadata_from_file(metadata_set, metadata_role)\n\n    # Raise an exception if the repository is missing the required 'root'\n    # metadata.\n    if 'root' not in self.metadata['current']:\n      raise exceptions.RepositoryError('No root of trust!'\n        ' Could not find the \"root.json\" file.')\n\n\n\n\n\n  def __str__(self):\n    \"\"\"\n      The string representation of an Updater object.\n    \"\"\"\n\n    return self.repository_name\n\n\n  @staticmethod\n  def _get_local_filename(rolename: str) -> str:\n    \"\"\"Return safe local filename for roles metadata\n\n    Use URL encoding to prevent issues with path separators and\n    with forbidden characters in Windows filesystems\"\"\"\n    return parse.quote(rolename, '') + '.json'\n\n\n  def _load_metadata_from_file(self, metadata_set, metadata_role):\n    \"\"\"\n    <Purpose>\n      Non-public method that loads current or previous metadata if there is a\n      local file.  If the expected file belonging to 'metadata_role' (e.g.,\n      'root.json') cannot be loaded, raise an exception.  The extracted metadata\n      object loaded from file is saved to the metadata store (i.e.,\n      self.metadata).\n\n    <Arguments>\n      metadata_set:\n        The string 'current' or 'previous', depending on whether one wants to\n        load the currently or previously trusted metadata file.\n\n      metadata_role:\n        The name of the metadata. This is a role name and should\n        not end in '.json'.  Examples: 'root', 'targets', 'unclaimed'.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If the role object loaded for 'metadata_role' is improperly formatted.\n\n      securesystemslib.exceptions.Error:\n        If there was an error importing a delegated role of 'metadata_role'\n        or the 'metadata_set' is not one currently supported.\n\n    <Side Effects>\n      If the metadata is loaded successfully, it is saved to the metadata\n      store.  If 'metadata_role' is 'root', the role and key databases\n      are reloaded.  If 'metadata_role' is a target metadata, all its\n      delegated roles are refreshed.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Ensure we have a valid metadata set.\n    if metadata_set not in ['current', 'previous']:\n      raise sslib_exceptions.Error(\n          'Invalid metadata set: ' + repr(metadata_set))\n\n    # Save and construct the full metadata path.\n    metadata_directory = self.metadata_directory[metadata_set]\n    metadata_filename = self._get_local_filename(metadata_role)\n    metadata_filepath = os.path.join(metadata_directory, metadata_filename)\n\n    # Ensure the metadata path is valid/exists, else ignore the call.\n    if os.path.exists(metadata_filepath):\n      # Load the file.  The loaded object should conform to\n      # 'tuf.formats.SIGNABLE_SCHEMA'.\n      try:\n        metadata_signable = sslib_util.load_json_file(\n            metadata_filepath)\n\n      # Although the metadata file may exist locally, it may not\n      # be a valid json file.  On the next refresh cycle, it will be\n      # updated as required.  If Root if cannot be loaded from disk\n      # successfully, an exception should be raised by the caller.\n      except sslib_exceptions.Error:\n        return\n\n      formats.check_signable_object_format(metadata_signable)\n\n      # Extract the 'signed' role object from 'metadata_signable'.\n      metadata_object = metadata_signable['signed']\n\n      # Save the metadata object to the metadata store.\n      self.metadata[metadata_set][metadata_role] = metadata_object\n\n      # If 'metadata_role' is 'root' or targets metadata, the key and role\n      # databases must be rebuilt.  If 'root', ensure self.consistent_snaptshots\n      # is updated.\n      if metadata_set == 'current':\n        if metadata_role == 'root':\n          self._rebuild_key_and_role_db()\n          self.consistent_snapshot = metadata_object['consistent_snapshot']\n\n        elif metadata_object['_type'] == 'targets':\n          # TODO: Should we also remove the keys of the delegated roles?\n          self._import_delegations(metadata_role)\n\n\n\n\n\n  def _rebuild_key_and_role_db(self):\n    \"\"\"\n    <Purpose>\n      Non-public method that rebuilds the key and role databases from the\n      currently trusted 'root' metadata object extracted from 'root.json'.\n      This private method is called when a new/updated 'root' metadata file is\n      loaded or when updater.refresh() is called.  This method will only store\n      the role information of the top-level roles (i.e., 'root', 'targets',\n      'snapshot', 'timestamp').\n\n    <Arguments>\n      None.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If the 'root' metadata is improperly formatted.\n\n      securesystemslib.exceptions.Error:\n        If there is an error loading a role contained in the 'root'\n        metadata.\n\n    <Side Effects>\n      The key and role databases are reloaded for the top-level roles.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Clobbering this means all delegated metadata files are rendered outdated\n    # and will need to be reloaded.  However, reloading the delegated metadata\n    # files is avoided here because fetching target information with\n    # get_one_valid_targetinfo() always causes a refresh of these files.  The\n    # metadata files for delegated roles are also not loaded when the\n    # repository is first instantiated.  Due to this setup, reloading delegated\n    # roles is not required here.\n    keydb.create_keydb_from_root_metadata(self.metadata['current']['root'],\n        self.repository_name)\n\n    roledb.create_roledb_from_root_metadata(self.metadata['current']['root'],\n        self.repository_name)\n\n\n\n\n\n  def _import_delegations(self, parent_role):\n    \"\"\"\n    <Purpose>\n      Non-public method that imports all the roles delegated by 'parent_role'.\n\n    <Arguments>\n      parent_role:\n        The role whose delegations will be imported.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If a key attribute of a delegated role's signing key is\n        improperly formatted.\n\n      securesystemslib.exceptions.Error:\n        If the signing key of a delegated role cannot not be loaded.\n\n    <Side Effects>\n      The key and role databases are modified to include the newly loaded roles\n      delegated by 'parent_role'.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    current_parent_metadata = self.metadata['current'][parent_role]\n\n    if 'delegations' not in current_parent_metadata:\n      return\n\n    # This could be quite slow with a large number of delegations.\n    keys_info = current_parent_metadata['delegations'].get('keys', {})\n    roles_info = current_parent_metadata['delegations'].get('roles', [])\n\n    logger.debug('Adding roles delegated from ' + repr(parent_role) + '.')\n\n    # Iterate the keys of the delegated roles of 'parent_role' and load them.\n    for keyid, keyinfo in keys_info.items():\n      if keyinfo['keytype'] in ['rsa', 'ed25519', 'ecdsa', 'ecdsa-sha2-nistp256']:\n\n        # We specify the keyid to ensure that it's the correct keyid\n        # for the key.\n        try:\n          key, _ = sslib_keys.format_metadata_to_key(keyinfo, keyid)\n\n          keydb.add_key(key, repository_name=self.repository_name)\n\n        except exceptions.KeyAlreadyExistsError:\n          pass\n\n        except (sslib_exceptions.FormatError, sslib_exceptions.Error):\n          logger.warning('Invalid key: ' + repr(keyid) + '. Aborting role ' +\n              'delegation for parent role \\'' + parent_role + '\\'.')\n          raise\n\n      else:\n        logger.warning('Invalid key type for ' + repr(keyid) + '.')\n        continue\n\n    # Add the roles to the role database.\n    for roleinfo in roles_info:\n      try:\n        # NOTE: roledb.add_role will take care of the case where rolename\n        # is None.\n        rolename = roleinfo.get('name')\n        logger.debug('Adding delegated role: ' + str(rolename) + '.')\n        roledb.add_role(rolename, roleinfo, self.repository_name)\n\n      except exceptions.RoleAlreadyExistsError:\n        logger.warning('Role already exists: ' + rolename)\n\n      except Exception:\n        logger.warning('Failed to add delegated role: ' + repr(rolename) + '.')\n        raise\n\n\n\n\n\n  def refresh(self, unsafely_update_root_if_necessary=True):\n    \"\"\"\n    <Purpose>\n      Update the latest copies of the metadata for the top-level roles. The\n      update request process follows a specific order to ensure the metadata\n      files are securely updated:\n      root (if necessary) -> timestamp -> snapshot -> targets.\n\n      Delegated metadata is not refreshed by this method. After this method is\n      called, the use of get_one_valid_targetinfo() will update delegated\n      metadata, when required.  Calling refresh() ensures that top-level\n      metadata is up-to-date, so that the target methods can refer to the\n      latest available content. Thus, refresh() should always be called by the\n      client before any requests of target file information.\n\n      The expiration time for downloaded metadata is also verified, including\n      local metadata that the repository claims is up to date.\n\n      If the refresh fails for any reason, then unless\n      'unsafely_update_root_if_necessary' is set, refresh will be retried once\n      after first attempting to update the root metadata file. Only after this\n      check will the exceptions listed here potentially be raised.\n\n    <Arguments>\n      unsafely_update_root_if_necessary:\n        Boolean that indicates whether to unsafely update the Root metadata if\n        any of the top-level metadata cannot be downloaded successfully.  The\n        Root role is unsafely updated if its current version number is unknown.\n\n    <Exceptions>\n      tuf.exceptions.NoWorkingMirrorError:\n        If the metadata for any of the top-level roles cannot be updated.\n\n      tuf.exceptions.ExpiredMetadataError:\n        If any of the top-level metadata is expired and no new version was\n        found.\n\n    <Side Effects>\n      Updates the metadata files of the top-level roles with the latest\n      information.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Do the arguments have the correct format?\n    # This check ensures the arguments have the appropriate\n    # number of objects and object types, and that all dict\n    # keys are properly named.\n    # Raise 'securesystemslib.exceptions.FormatError' if the check fail.\n    sslib_formats.BOOLEAN_SCHEMA.check_match(\n        unsafely_update_root_if_necessary)\n\n    # Update the top-level metadata.  The _update_metadata_if_changed() and\n    # _update_metadata() calls below do NOT perform an update if there\n    # is insufficient trusted signatures for the specified metadata.\n    # Raise 'tuf.exceptions.NoWorkingMirrorError' if an update fails.\n    root_metadata = self.metadata['current']['root']\n\n    try:\n      self._ensure_not_expired(root_metadata, 'root')\n\n    except exceptions.ExpiredMetadataError:\n      # Raise 'tuf.exceptions.NoWorkingMirrorError' if a valid (not\n      # expired, properly signed, and valid metadata) 'root.json' cannot be\n      # installed.\n      if unsafely_update_root_if_necessary:\n        logger.info('Expired Root metadata was loaded from disk.'\n          '  Try to update it now.' )\n\n      # The caller explicitly requested not to unsafely fetch an expired Root.\n      else:\n        logger.info('An expired Root metadata was loaded and must be updated.')\n        raise\n\n    # Update the root metadata and verify it by building a chain of trusted root\n    # keys from the current trusted root metadata file\n    self._update_root_metadata(root_metadata)\n\n    # Ensure that the role and key information of the top-level roles is the\n    # latest.  We do this whether or not Root needed to be updated, in order to\n    # ensure that, e.g., the entries in roledb for top-level roles are\n    # populated with expected keyid info so that roles can be validated.  In\n    # certain circumstances, top-level metadata might be missing because it was\n    # marked obsolete and deleted after a failed attempt, and thus we should\n    # refresh them here as a protective measure.  See Issue #736.\n    self._rebuild_key_and_role_db()\n    self.consistent_snapshot = \\\n        self.metadata['current']['root']['consistent_snapshot']\n\n    # Use default but sane information for timestamp metadata, and do not\n    # require strict checks on its required length.\n    self._update_metadata('timestamp', DEFAULT_TIMESTAMP_UPPERLENGTH)\n\n    self._update_metadata_if_changed('snapshot',\n        referenced_metadata='timestamp')\n    self._update_metadata_if_changed('targets')\n\n\n\n  def _update_root_metadata(self, current_root_metadata):\n    \"\"\"\n    <Purpose>\n      The root file must be signed by the current root threshold and keys as\n      well as the previous root threshold and keys. The update process for root\n      files means that each intermediate root file must be downloaded, to build\n      a chain of trusted root keys from keys already trusted by the client:\n\n        1.root -> 2.root -> 3.root\n\n      3.root must be signed by the threshold and keys of 2.root, and 2.root\n      must be signed by the threshold and keys of 1.root.\n\n    <Arguments>\n      current_root_metadata:\n        The currently held version of root.\n\n    <Side Effects>\n      Updates the root metadata files with the latest information.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    def neither_403_nor_404(mirror_error):\n      if isinstance(mirror_error, tuf.exceptions.FetcherHTTPError):\n        if mirror_error.status_code in {403, 404}:\n          return False\n      return True\n\n    # Temporarily set consistent snapshot. Will be updated to whatever is set\n    # in the latest root.json after running through the intermediates with\n    # _update_metadata().\n    self.consistent_snapshot = True\n\n    # Following the spec, try downloading the N+1th root for a certain maximum\n    # number of times.\n    lower_bound = current_root_metadata['version'] + 1\n    upper_bound = lower_bound + settings.MAX_NUMBER_ROOT_ROTATIONS\n\n    # Try downloading the next root.\n    for next_version in range(lower_bound, upper_bound):\n      try:\n        # Thoroughly verify it.\n        self._update_metadata('root', DEFAULT_ROOT_UPPERLENGTH,\n            version=next_version)\n      # When we run into HTTP 403/404 error from ALL mirrors, break out of\n      # loop, because the next root metadata file is most likely missing.\n      except exceptions.NoWorkingMirrorError as exception:\n        for mirror_error in exception.mirror_errors.values():\n          # Otherwise, reraise the error, because it is not a simple HTTP\n          # error.\n          if neither_403_nor_404(mirror_error):\n            logger.info('Misc error for root version ' + str(next_version))\n            raise\n          else:\n            logger.debug('HTTP error for root version ' + str(next_version))\n        # If we are here, then we ran into only 403 / 404 errors, which are\n        # good reasons to suspect that the next root metadata file does not\n        # exist.\n        break\n\n      # Ensure that the role and key information of the top-level roles is the\n      # latest.  We do this whether or not Root needed to be updated, in order\n      # to ensure that, e.g., the entries in roledb for top-level roles are\n      # populated with expected keyid info so that roles can be validated.  In\n      # certain circumstances, top-level metadata might be missing because it\n      # was marked obsolete and deleted after a failed attempt, and thus we\n      # should refresh them here as a protective measure.  See Issue #736.\n      self._rebuild_key_and_role_db()\n\n    # Set our consistent snapshot property to what the latest root has said.\n    self.consistent_snapshot = \\\n        self.metadata['current']['root']['consistent_snapshot']\n\n\n\n  def _check_hashes(self, file_object, trusted_hashes):\n    \"\"\"\n    <Purpose>\n      Non-public method that verifies multiple secure hashes of 'file_object'.\n\n    <Arguments>\n      file_object:\n        A file object.\n\n      trusted_hashes:\n        A dictionary with hash-algorithm names as keys and hashes as dict values.\n        The hashes should be in the hexdigest format.  Should be Conformant to\n        'securesystemslib.formats.HASHDICT_SCHEMA'.\n\n    <Exceptions>\n      securesystemslib.exceptions.BadHashError, if the hashes don't match.\n\n    <Side Effects>\n      Hash digest object is created using the 'securesystemslib.hash' module.\n      Position within file_object is changed.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Verify each hash, raise an exception if any hash fails to verify\n    for algorithm, trusted_hash in trusted_hashes.items():\n      digest_object = sslib_hash.digest_fileobject(file_object,\n          algorithm)\n      computed_hash = digest_object.hexdigest()\n\n      if trusted_hash != computed_hash:\n        raise sslib_exceptions.BadHashError(trusted_hash,\n            computed_hash)\n\n      else:\n        logger.info('Verified ' + algorithm + ' hash: ' + trusted_hash)\n\n\n\n\n\n  def _check_file_length(self, file_object, trusted_file_length):\n    \"\"\"\n    <Purpose>\n      Non-public method that ensures the length of 'file_object' is strictly\n      equal to 'trusted_file_length'.  This is a deliberately redundant\n      implementation designed to complement\n      download._check_downloaded_length().\n\n    <Arguments>\n      file_object:\n        A file object.\n\n      trusted_file_length:\n        A non-negative integer that is the trusted length of the file.\n\n    <Exceptions>\n      tuf.exceptions.DownloadLengthMismatchError, if the lengths do not match.\n\n    <Side Effects>\n      Reads the contents of 'file_object' and logs a message if 'file_object'\n      matches the trusted length.\n      Position within file_object is changed.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    file_object.seek(0, io.SEEK_END)\n    observed_length = file_object.tell()\n\n    # Return and log a message if the length 'file_object' is equal to\n    # 'trusted_file_length', otherwise raise an exception.  A hard check\n    # ensures that a downloaded file strictly matches a known, or trusted,\n    # file length.\n    if observed_length != trusted_file_length:\n      raise exceptions.DownloadLengthMismatchError(trusted_file_length,\n          observed_length)\n\n    else:\n      logger.debug('Observed length (' + str(observed_length) +\\\n          ') == trusted length (' + str(trusted_file_length) + ')')\n\n\n\n\n\n  def _get_target_file(self, target_filepath, file_length, file_hashes,\n      prefix_filename_with_hash):\n    \"\"\"\n    <Purpose>\n      Non-public method that safely (i.e., the file length and hash are\n      strictly equal to the trusted) downloads a target file up to a certain\n      length, and checks its hashes thereafter.\n\n    <Arguments>\n      target_filepath:\n        The target filepath (relative to the repository targets directory)\n        obtained from TUF targets metadata.\n\n      file_length:\n        The expected compressed length of the target file. If the file is not\n        compressed, then it will simply be its uncompressed length.\n\n      file_hashes:\n        The expected hashes of the target file.\n\n      prefix_filename_with_hash:\n        Whether to prefix the targets file names with their hash when using\n        consistent snapshot.\n        This should be set to False when the served target filenames are not\n        prefixed with hashes (in this case the server uses other means\n        to ensure snapshot consistency).\n\n    <Exceptions>\n      tuf.exceptions.NoWorkingMirrorError:\n        The target could not be fetched. This is raised only when all known\n        mirrors failed to provide a valid copy of the desired target file.\n\n    <Side Effects>\n      The target file is downloaded from all known repository mirrors in the\n      worst case. If a valid copy of the target file is found, it is stored in\n      a temporary file and returned.\n\n    <Returns>\n      A file object containing the target.\n    \"\"\"\n\n    if self.consistent_snapshot and prefix_filename_with_hash:\n      # Note: values() does not return a list in Python 3.  Use list()\n      # on values() for Python 2+3 compatibility.\n      target_digest = list(file_hashes.values()).pop()\n      dirname, basename = os.path.split(target_filepath)\n      target_filepath = os.path.join(dirname, target_digest + '.' + basename)\n\n    file_mirrors = mirrors.get_list_of_mirrors('target', target_filepath,\n        self.mirrors)\n\n    # file_mirror (URL): error (Exception)\n    file_mirror_errors = {}\n    file_object = None\n\n    for file_mirror in file_mirrors:\n      try:\n        file_object = download.safe_download(file_mirror,\n            file_length, self.fetcher)\n\n        # Verify 'file_object' against the expected length and hashes.\n        self._check_file_length(file_object, file_length)\n        self._check_hashes(file_object, file_hashes)\n        # If the file verifies, we don't need to try more mirrors\n        return file_object\n\n      except Exception as exception:\n        # Remember the error from this mirror, close tempfile if one was opened\n        logger.debug('Update failed from ' + file_mirror + '.')\n        file_mirror_errors[file_mirror] = exception\n        if file_object is not None:\n          file_object.close()\n          file_object = None\n\n    logger.debug('Failed to update ' + repr(target_filepath) + ' from'\n        ' all mirrors: ' + repr(file_mirror_errors))\n    raise exceptions.NoWorkingMirrorError(file_mirror_errors)\n\n\n\n\n\n  def _verify_root_self_signed(self, signable):\n    \"\"\"\n    Verify the root metadata in signable is signed by a threshold of keys,\n    where the threshold and valid keys are defined by itself\n    \"\"\"\n    threshold = signable['signed']['roles']['root']['threshold']\n    keyids = signable['signed']['roles']['root']['keyids']\n    keys = signable['signed']['keys']\n    signatures = signable['signatures']\n    signed = sslib_formats.encode_canonical(\n        signable['signed']).encode('utf-8')\n    verified_sig_keyids = set()\n\n    for signature in signatures:\n      keyid = signature['keyid']\n\n      # At this point we are verifying that the root metadata is signed by a\n      # threshold of keys listed in the current root role, therefore skip\n      # keys with a keyid that is not listed in the current root role.\n      if keyid not in keyids:\n        continue\n\n      key = keys[keyid]\n      # The ANYKEY_SCHEMA check in verify_signature expects the keydict to\n      # include a keyid\n      key['keyid'] = keyid\n      valid_sig = sslib_keys.verify_signature(key, signature, signed)\n\n      if valid_sig:\n        verified_sig_keyids.add(keyid)\n\n    if len(verified_sig_keyids) >= threshold:\n      return True\n    return False\n\n\n\n\n\n  def _verify_metadata_file(self, metadata_file_object,\n      metadata_role):\n    \"\"\"\n    <Purpose>\n      Non-public method that verifies a metadata file.  An exception is\n      raised if 'metadata_file_object is invalid.  There is no\n      return value.\n\n    <Arguments>\n      metadata_file_object:\n        A file object containing the metadata file.\n\n      metadata_role:\n        The role name of the metadata (e.g., 'root', 'targets',\n        'unclaimed').\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        In case the metadata file is valid JSON, but not valid TUF metadata.\n\n      tuf.exceptions.InvalidMetadataJSONError:\n        In case the metadata file is not valid JSON.\n\n      tuf.exceptions.ReplayedMetadataError:\n        In case the downloaded metadata file is older than the current one.\n\n      tuf.exceptions.RepositoryError:\n        In case the repository is somehow inconsistent; e.g. a parent has not\n        delegated to a child (contrary to expectations).\n\n      tuf.SignatureError:\n        In case the metadata file does not have a valid signature.\n\n    <Side Effects>\n      The content of 'metadata_file_object' is read and loaded, the current\n      position within the file is changed.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    metadata_file_object.seek(0)\n    metadata = metadata_file_object.read().decode('utf-8')\n\n    try:\n      metadata_signable = sslib_util.load_json_string(metadata)\n\n    except Exception as exception:\n      raise exceptions.InvalidMetadataJSONError(exception)\n\n    else:\n      # Ensure the loaded 'metadata_signable' is properly formatted.  Raise\n      # 'securesystemslib.exceptions.FormatError' if not.\n      formats.check_signable_object_format(metadata_signable)\n\n    # Is 'metadata_signable' expired?\n    self._ensure_not_expired(metadata_signable['signed'], metadata_role)\n\n    # We previously verified version numbers in this function, but have since\n    # moved version number verification to the functions that retrieve\n    # metadata.\n\n    # Verify the signature on the downloaded metadata object.\n    valid = sig.verify(metadata_signable, metadata_role,\n        self.repository_name)\n\n    if not valid:\n      raise sslib_exceptions.BadSignatureError(metadata_role)\n\n    # For root metadata, verify the downloaded root metadata object with the\n    # new threshold of new signatures contained within the downloaded root\n    # metadata object\n    # NOTE: we perform the checks on root metadata here because this enables\n    # us to perform the check before the tempfile is persisted. Furthermore,\n    # by checking here we can easily perform the check for each download\n    # mirror. Whereas if we check after _verify_metadata_file we may be\n    # persisting invalid files and we cannot try copies of the file from other\n    # mirrors.\n    if valid and metadata_role == 'root':\n      valid = self._verify_root_self_signed(metadata_signable)\n      if not valid:\n        raise sslib_exceptions.BadSignatureError(metadata_role)\n\n\n\n\n\n  def _get_metadata_file(self, metadata_role, remote_filename,\n    upperbound_filelength, expected_version):\n    \"\"\"\n    <Purpose>\n      Non-public method that tries downloading, up to a certain length, a\n      metadata file from a list of known mirrors. As soon as the first valid\n      copy of the file is found, the downloaded file is returned and the\n      remaining mirrors are skipped.\n\n    <Arguments>\n      metadata_role:\n        The role name of the metadata (e.g., 'root', 'targets', 'unclaimed').\n\n      remote_filename:\n        The relative file path (on the remove repository) of 'metadata_role'.\n\n      upperbound_filelength:\n        The expected length, or upper bound, of the metadata file to be\n        downloaded.\n\n      expected_version:\n        The expected and required version number of the 'metadata_role' file\n        downloaded.  'expected_version' is an integer.\n\n    <Exceptions>\n      tuf.exceptions.NoWorkingMirrorError:\n        The metadata could not be fetched. This is raised only when all known\n        mirrors failed to provide a valid copy of the desired metadata file.\n\n    <Side Effects>\n      The file is downloaded from all known repository mirrors in the worst\n      case. If a valid copy of the file is found, it is stored in a temporary\n      file and returned.\n\n    <Returns>\n      A file object containing the metadata.\n    \"\"\"\n\n    file_mirrors = mirrors.get_list_of_mirrors('meta', remote_filename,\n        self.mirrors)\n\n    # file_mirror (URL): error (Exception)\n    file_mirror_errors = {}\n    file_object = None\n\n    for file_mirror in file_mirrors:\n      try:\n        file_object = download.unsafe_download(file_mirror,\n            upperbound_filelength, self.fetcher)\n        file_object.seek(0)\n\n        # Verify 'file_object' according to the callable function.\n        # 'file_object' is also verified if decompressed above (i.e., the\n        # uncompressed version).\n        metadata_signable = \\\n          sslib_util.load_json_string(file_object.read().decode('utf-8'))\n\n        # Determine if the specification version number is supported.  It is\n        # assumed that \"spec_version\" is in (major.minor.fix) format, (for\n        # example: \"1.4.3\") and that releases with the same major version\n        # number maintain backwards compatibility.  Consequently, if the major\n        # version number of new metadata equals our expected major version\n        # number, the new metadata is safe to parse.\n        try:\n          metadata_spec_version = metadata_signable['signed']['spec_version']\n          metadata_spec_version_split = metadata_spec_version.split('.')\n          metadata_spec_major_version = int(metadata_spec_version_split[0])\n          metadata_spec_minor_version = int(metadata_spec_version_split[1])\n\n          code_spec_version_split = tuf.SPECIFICATION_VERSION.split('.')\n          code_spec_major_version = int(code_spec_version_split[0])\n          code_spec_minor_version = int(code_spec_version_split[1])\n\n          if metadata_spec_major_version != code_spec_major_version:\n            raise exceptions.UnsupportedSpecificationError(\n                'Downloaded metadata that specifies an unsupported '\n                'spec_version.  This code supports major version number: ' +\n                repr(code_spec_major_version) + '; however, the obtained '\n                'metadata lists version number: ' + str(metadata_spec_version))\n\n          #report to user if minor versions do not match, continue with update\n          if metadata_spec_minor_version != code_spec_minor_version:\n            logger.info(\"Downloaded metadata that specifies a different minor \" +\n                \"spec_version. This code has version \" +\n                str(tuf.SPECIFICATION_VERSION) +\n                \" and the metadata lists version number \" +\n                str(metadata_spec_version) +\n                \". The update will continue as the major versions match.\")\n\n        except (ValueError, TypeError) as error:\n          raise sslib_exceptions.FormatError('Improperly'\n              ' formatted spec_version, which must be in major.minor.fix format') from error\n\n        # If the version number is unspecified, ensure that the version number\n        # downloaded is greater than the currently trusted version number for\n        # 'metadata_role'.\n        version_downloaded = metadata_signable['signed']['version']\n\n        if expected_version is not None:\n          # Verify that the downloaded version matches the version expected by\n          # the caller.\n          if version_downloaded != expected_version:\n            raise exceptions.BadVersionNumberError('Downloaded'\n              ' version number: ' + repr(version_downloaded) + '.  Version'\n              ' number MUST be: ' + repr(expected_version))\n\n        # The caller does not know which version to download.  Verify that the\n        # downloaded version is at least greater than the one locally\n        # available.\n        else:\n          # Verify that the version number of the locally stored\n          # 'timestamp.json', if available, is less than what was downloaded.\n          # Otherwise, accept the new timestamp with version number\n          # 'version_downloaded'.\n\n          try:\n            current_version = \\\n              self.metadata['current'][metadata_role]['version']\n\n            if version_downloaded < current_version:\n              raise exceptions.ReplayedMetadataError(metadata_role,\n                  version_downloaded, current_version)\n\n          except KeyError:\n            logger.info(metadata_role + ' not available locally.')\n\n        self._verify_metadata_file(file_object, metadata_role)\n\n      except Exception as exception:\n        # Remember the error from this mirror, and \"reset\" the target file.\n        logger.debug('Update failed from ' + file_mirror + '.')\n        file_mirror_errors[file_mirror] = exception\n        if file_object:\n          file_object.close()\n          file_object = None\n\n      else:\n        break\n\n    if file_object:\n      return file_object\n\n    else:\n      logger.debug('Failed to update ' + repr(remote_filename) + ' from all'\n        ' mirrors: ' + repr(file_mirror_errors))\n      raise exceptions.NoWorkingMirrorError(file_mirror_errors)\n\n\n\n\n\n  def _update_metadata(self, metadata_role, upperbound_filelength, version=None):\n    \"\"\"\n    <Purpose>\n      Non-public method that downloads, verifies, and 'installs' the metadata\n      belonging to 'metadata_role'.  Calling this method implies that the\n      'metadata_role' on the repository is newer than the client's, and thus\n      needs to be re-downloaded.  The current and previous metadata stores are\n      updated if the newly downloaded metadata is successfully downloaded and\n      verified.  This method also assumes that the store of top-level metadata\n      is the latest and exists.\n\n    <Arguments>\n      metadata_role:\n        The name of the metadata. This is a role name and should not end\n        in '.json'.  Examples: 'root', 'targets', 'targets/linux/x86'.\n\n      upperbound_filelength:\n        The expected length, or upper bound, of the metadata file to be\n        downloaded.\n\n      version:\n        The expected and required version number of the 'metadata_role' file\n        downloaded.  'expected_version' is an integer.\n\n    <Exceptions>\n      tuf.exceptions.NoWorkingMirrorError:\n        The metadata cannot be updated. This is not specific to a single\n        failure but rather indicates that all possible ways to update the\n        metadata have been tried and failed.\n\n    <Side Effects>\n      The metadata file belonging to 'metadata_role' is downloaded from a\n      repository mirror.  If the metadata is valid, it is stored in the\n      metadata store.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Attempt a file download from each mirror until the file is downloaded and\n    # verified.  If the signature of the downloaded file is valid, proceed,\n    # otherwise log a warning and try the next mirror.  'metadata_file_object'\n    # is the file-like object returned by 'download.py'.  'metadata_signable'\n    # is the object extracted from 'metadata_file_object'.  Metadata saved to\n    # files are regarded as 'signable' objects, conformant to\n    # 'tuf.formats.SIGNABLE_SCHEMA'.\n    #\n    # Some metadata (presently timestamp) will be downloaded \"unsafely\", in the\n    # sense that we can only estimate its true length and know nothing about\n    # its version.  This is because not all metadata will have other metadata\n    # for it; otherwise we will have an infinite regress of metadata signing\n    # for each other. In this case, we will download the metadata up to the\n    # best length we can get for it, not request a specific version, but\n    # perform the rest of the checks (e.g., signature verification).\n\n    # Construct the metadata filename as expected by the download/mirror\n    # modules. Local filename is quoted to protect against names like\"../file\".\n\n    remote_filename = metadata_role + '.json'\n    local_filename = self._get_local_filename(metadata_role)\n    filename_version = ''\n\n    if self.consistent_snapshot and version:\n      filename_version = version\n      dirname, basename = os.path.split(remote_filename)\n      remote_filename = os.path.join(\n          dirname, str(filename_version) + '.' + basename)\n\n    metadata_file_object = \\\n      self._get_metadata_file(metadata_role, remote_filename,\n        upperbound_filelength, version)\n\n    # The metadata has been verified. Move the metadata file into place.\n    # First, move the 'current' metadata file to the 'previous' directory\n    # if it exists.\n    current_filepath = os.path.join(self.metadata_directory['current'],\n                local_filename)\n    current_filepath = os.path.abspath(current_filepath)\n    sslib_util.ensure_parent_dir(current_filepath)\n\n    previous_filepath = os.path.join(self.metadata_directory['previous'],\n        local_filename)\n    previous_filepath = os.path.abspath(previous_filepath)\n\n    if os.path.exists(current_filepath):\n      # Previous metadata might not exist, say when delegations are added.\n      sslib_util.ensure_parent_dir(previous_filepath)\n      shutil.move(current_filepath, previous_filepath)\n\n    # Next, move the verified updated metadata file to the 'current' directory.\n    metadata_file_object.seek(0)\n    metadata_signable = \\\n      sslib_util.load_json_string(metadata_file_object.read().decode('utf-8'))\n\n    sslib_util.persist_temp_file(metadata_file_object, current_filepath)\n\n    # Extract the metadata object so we can store it to the metadata store.\n    # 'current_metadata_object' set to 'None' if there is not an object\n    # stored for 'metadata_role'.\n    updated_metadata_object = metadata_signable['signed']\n    current_metadata_object = self.metadata['current'].get(metadata_role)\n\n    # Finally, update the metadata and fileinfo stores, and rebuild the\n    # key and role info for the top-level roles if 'metadata_role' is root.\n    # Rebuilding the key and role info is required if the newly-installed\n    # root metadata has revoked keys or updated any top-level role information.\n    logger.debug('Updated ' + repr(current_filepath) + '.')\n    self.metadata['previous'][metadata_role] = current_metadata_object\n    self.metadata['current'][metadata_role] = updated_metadata_object\n    self._update_versioninfo(remote_filename)\n\n\n\n\n\n  def _update_metadata_if_changed(self, metadata_role,\n    referenced_metadata='snapshot'):\n    \"\"\"\n    <Purpose>\n      Non-public method that updates the metadata for 'metadata_role' if it has\n      changed.  All top-level roles other than the 'timestamp' and 'root'\n      roles are updated by this method.  The 'timestamp' role is always\n      downloaded from a mirror without first checking if it has been updated;\n      it is updated in refresh() by calling _update_metadata('timestamp').\n      The 'root' role is always updated first and verified based on the trusted\n      root metadata file the client already has a copy of; it is updated in\n      refresh() by calling _update_root_metadata().\n      This method is also called for delegated role metadata, which are\n      referenced by 'snapshot'.\n\n      If the metadata needs to be updated but an update cannot be obtained,\n      this method will delete the file.\n\n      Due to the way in which metadata files are updated, it is expected that\n      'referenced_metadata' is not out of date and trusted.  The refresh()\n      method updates the top-level roles in 'root -> timestamp -> snapshot ->\n      targets' order.  For delegated metadata, the parent role is\n      updated before the delegated role.  Taking into account that\n      'referenced_metadata' is updated and verified before 'metadata_role',\n      this method determines if 'metadata_role' has changed by checking\n      the 'meta' field of the newly updated 'referenced_metadata'.\n\n    <Arguments>\n      metadata_role:\n        The name of the metadata. This is a role name and should not end\n        in '.json'.  Examples: 'root', 'targets', 'unclaimed'.\n\n      referenced_metadata:\n        This is the metadata that provides the role information for\n        'metadata_role'.  For the top-level roles, the 'snapshot' role\n        is the referenced metadata for the 'root', and 'targets' roles.\n        The 'timestamp' metadata is always downloaded regardless.  In\n        other words, it is updated by calling _update_metadata('timestamp')\n        and not by this method.  The referenced metadata for 'snapshot'\n        is 'timestamp'.  See refresh().\n\n    <Exceptions>\n      tuf.exceptions.ExpiredMetadataError:\n        If local metadata is expired and newer metadata is not available.\n\n      tuf.exceptions.NoWorkingMirrorError:\n        If 'metadata_role' could not be downloaded after determining that it\n        had changed.\n\n      tuf.exceptions.RepositoryError:\n        If the referenced metadata is missing.\n\n    <Side Effects>\n      If it is determined that 'metadata_role' has been updated, the metadata\n      store (i.e., self.metadata) is updated with the new metadata and the\n      affected stores modified (i.e., the previous metadata store is updated).\n      If the metadata is 'targets' or a delegated targets role, the role\n      database is updated with the new information, including its delegated\n      roles.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    metadata_filename = metadata_role + '.json'\n    expected_versioninfo = None\n\n    # Ensure the referenced metadata has been loaded.  The 'root' role may be\n    # updated without having 'snapshot' available.\n    if referenced_metadata not in self.metadata['current']:\n      raise exceptions.RepositoryError('Cannot update'\n        ' ' + repr(metadata_role) + ' because ' + referenced_metadata + ' is'\n        ' missing.')\n\n    # The referenced metadata has been loaded.  Extract the new versioninfo for\n    # 'metadata_role' from it.\n    else:\n      logger.debug(repr(metadata_role) + ' referenced in ' +\n        repr(referenced_metadata)+ '.  ' + repr(metadata_role) +\n        ' may be updated.')\n\n    # Simply return if the metadata for 'metadata_role' has not been updated,\n    # according to the uncompressed metadata provided by the referenced\n    # metadata.  The metadata is considered updated if its version number is\n    # strictly greater than its currently trusted version number.\n    expected_versioninfo = self.metadata['current'][referenced_metadata] \\\n        ['meta'][metadata_filename]\n\n    if not self._versioninfo_has_been_updated(metadata_filename,\n        expected_versioninfo):\n      logger.info(repr(metadata_filename) + ' up-to-date.')\n\n      # Since we have not downloaded a new version of this metadata, we should\n      # check to see if our local version is stale and notify the user if so.\n      # This raises tuf.exceptions.ExpiredMetadataError if the metadata we have\n      # is expired. Resolves issue #322.\n      self._ensure_not_expired(self.metadata['current'][metadata_role],\n          metadata_role)\n\n      # TODO: If metadata role is snapshot, we should verify that snapshot's\n      # hash matches what's listed in timestamp.json per step 3.1 of the\n      # detailed workflows in the specification\n\n      return\n\n    logger.debug('Metadata ' + repr(metadata_filename) + ' has changed.')\n\n    # The file lengths of metadata are unknown, only their version numbers are\n    # known.  Set an upper limit for the length of the downloaded file for each\n    # expected role.  Note: The Timestamp role is not updated via this\n    # function.\n    if metadata_role == 'snapshot':\n      upperbound_filelength = settings.DEFAULT_SNAPSHOT_REQUIRED_LENGTH\n\n    # The metadata is considered Targets (or delegated Targets metadata).\n    else:\n      upperbound_filelength = settings.DEFAULT_TARGETS_REQUIRED_LENGTH\n\n    try:\n      self._update_metadata(metadata_role, upperbound_filelength,\n          expected_versioninfo['version'])\n\n    except Exception:\n      # The current metadata we have is not current but we couldn't get new\n      # metadata. We shouldn't use the old metadata anymore.  This will get rid\n      # of in-memory knowledge of the role and delegated roles, but will leave\n      # delegated metadata files as current files on disk.\n      #\n      # TODO: Should we get rid of the delegated metadata files?  We shouldn't\n      # need to, but we need to check the trust implications of the current\n      # implementation.\n      self._delete_metadata(metadata_role)\n      logger.warning('Metadata for ' + repr(metadata_role) + ' cannot'\n          ' be updated.')\n      raise\n\n    else:\n      # We need to import the delegated roles of 'metadata_role', since its\n      # list of delegations might have changed from what was previously\n      # loaded..\n      # TODO: Should we remove the keys of the delegated roles?\n      self._import_delegations(metadata_role)\n\n\n\n\n\n  def _versioninfo_has_been_updated(self, metadata_filename, new_versioninfo):\n    \"\"\"\n    <Purpose>\n      Non-public method that determines whether the current versioninfo of\n      'metadata_filename' is less than 'new_versioninfo' (i.e., the version\n      number has been incremented).  The 'new_versioninfo' argument should be\n      extracted from the latest copy of the metadata that references\n      'metadata_filename'.  Example: 'root.json' would be referenced by\n      'snapshot.json'.\n\n      'new_versioninfo' should only be 'None' if this is for updating\n      'root.json' without having 'snapshot.json' available.\n\n    <Arguments>\n      metadadata_filename:\n        The metadata filename for the role.  For the 'root' role,\n        'metadata_filename' would be 'root.json'.\n\n      new_versioninfo:\n        A dict object representing the new file information for\n        'metadata_filename'.  'new_versioninfo' may be 'None' when\n        updating 'root' without having 'snapshot' available.  This\n        dict conforms to 'tuf.formats.VERSIONINFO_SCHEMA' and has\n        the form:\n\n        {'version': 288}\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      If there is no versioninfo currently loaded for 'metadata_filename', try\n      to load it.\n\n    <Returns>\n      Boolean.  True if the versioninfo has changed, False otherwise.\n    \"\"\"\n\n    # If there is no versioninfo currently stored for 'metadata_filename',\n    # try to load the file, calculate the versioninfo, and store it.\n    if metadata_filename not in self.versioninfo:\n      self._update_versioninfo(metadata_filename)\n\n    # Return true if there is no versioninfo for 'metadata_filename'.\n    # 'metadata_filename' is not in the 'self.versioninfo' store\n    # and it doesn't exist in the 'current' metadata location.\n    if self.versioninfo[metadata_filename] is None:\n      return True\n\n    current_versioninfo = self.versioninfo[metadata_filename]\n\n    logger.debug('New version for ' + repr(metadata_filename) +\n        ': ' + repr(new_versioninfo['version']) + '.  Old version: ' +\n        repr(current_versioninfo['version']))\n\n    if new_versioninfo['version'] > current_versioninfo['version']:\n      return True\n\n    else:\n      return False\n\n\n\n\n\n  def _update_versioninfo(self, metadata_filename):\n    \"\"\"\n    <Purpose>\n      Non-public method that updates the 'self.versioninfo' entry for the\n      metadata belonging to 'metadata_filename'.  If the current metadata for\n      'metadata_filename' cannot be loaded, set its 'versioninfo' to 'None' to\n      signal that it is not in 'self.versioninfo' AND it also doesn't exist\n      locally.\n\n    <Arguments>\n      metadata_filename:\n        The metadata filename for the role.  For the 'root' role,\n        'metadata_filename' would be 'root.json'.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      The version number of 'metadata_filename' is calculated and stored in its\n      corresponding entry in 'self.versioninfo'.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # In case we delayed loading the metadata and didn't do it in\n    # __init__ (such as with delegated metadata), then get the version\n    # info now.\n\n    # 'metadata_filename' is the key from meta dictionary: build the\n    # corresponding local filepath like _get_local_filename()\n    local_filename = parse.quote(metadata_filename, \"\")\n    current_filepath = os.path.join(self.metadata_directory['current'],\n        local_filename)\n\n    # If the path is invalid, simply return and leave versioninfo unset.\n    if not os.path.exists(current_filepath):\n      self.versioninfo[metadata_filename] = None\n      return\n\n    # Extract the version information from the trusted snapshot role and save\n    # it to the 'self.versioninfo' store.\n    if metadata_filename == 'timestamp.json':\n      trusted_versioninfo = \\\n        self.metadata['current']['timestamp']['version']\n\n    # When updating snapshot.json, the client either (1) has a copy of\n    # snapshot.json, or (2) is in the process of obtaining it by first\n    # downloading timestamp.json.  Note: Clients are allowed to have only\n    # root.json initially, and perform a refresh of top-level metadata to\n    # obtain the remaining roles.\n    elif metadata_filename == 'snapshot.json':\n\n      # Verify the version number of the currently trusted snapshot.json in\n      # snapshot.json itself.  Checking the version number specified in\n      # timestamp.json may be greater than the version specified in the\n      # client's copy of snapshot.json.\n      try:\n        timestamp_version_number = self.metadata['current']['snapshot']['version']\n        trusted_versioninfo = formats.make_versioninfo(\n            timestamp_version_number)\n\n      except KeyError:\n        trusted_versioninfo = \\\n          self.metadata['current']['timestamp']['meta']['snapshot.json']\n\n    else:\n\n      try:\n        # The metadata file names in 'self.metadata' exclude the role\n        # extension.  Strip the '.json' extension when checking if\n        # 'metadata_filename' currently exists.\n        targets_version_number = \\\n          self.metadata['current'][metadata_filename[:-len('.json')]]['version']\n        trusted_versioninfo = \\\n          formats.make_versioninfo(targets_version_number)\n\n      except KeyError:\n        trusted_versioninfo = \\\n          self.metadata['current']['snapshot']['meta'][metadata_filename]\n\n    self.versioninfo[metadata_filename] = trusted_versioninfo\n\n\n\n\n  def _move_current_to_previous(self, metadata_role):\n    \"\"\"\n    <Purpose>\n      Non-public method that moves the current metadata file for 'metadata_role'\n      to the previous directory.\n\n    <Arguments>\n      metadata_role:\n        The name of the metadata. This is a role name and should not end\n        in '.json'.  Examples: 'root', 'targets', 'targets/linux/x86'.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n     The metadata file for 'metadata_role' is removed from 'current'\n     and moved to the 'previous' directory.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Get the 'current' and 'previous' full file paths for 'metadata_role'\n    metadata_filepath = self._get_local_filename(metadata_role)\n    previous_filepath = os.path.join(self.metadata_directory['previous'],\n                                     metadata_filepath)\n    current_filepath = os.path.join(self.metadata_directory['current'],\n                                    metadata_filepath)\n\n    # Remove the previous path if it exists.\n    if os.path.exists(previous_filepath):\n      os.remove(previous_filepath)\n\n    # Move the current path to the previous path.\n    if os.path.exists(current_filepath):\n      sslib_util.ensure_parent_dir(previous_filepath)\n      os.rename(current_filepath, previous_filepath)\n\n\n\n\n\n  def _delete_metadata(self, metadata_role):\n    \"\"\"\n    <Purpose>\n      Non-public method that removes all (current) knowledge of 'metadata_role'.\n      The metadata belonging to 'metadata_role' is removed from the current\n      'self.metadata' store and from the role database. The 'root.json' role\n      file is never removed.\n\n    <Arguments>\n      metadata_role:\n        The name of the metadata. This is a role name and should not end\n        in '.json'.  Examples: 'root', 'targets', 'targets/linux/x86'.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      The role database is modified and the metadata for 'metadata_role'\n      removed from the 'self.metadata' store.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # The root metadata role is never deleted without a replacement.\n    if metadata_role == 'root':\n      return\n\n    # Get rid of the current metadata file.\n    self._move_current_to_previous(metadata_role)\n\n    # Remove knowledge of the role.\n    if metadata_role in self.metadata['current']:\n      del self.metadata['current'][metadata_role]\n    roledb.remove_role(metadata_role, self.repository_name)\n\n\n\n\n\n  def _ensure_not_expired(self, metadata_object, metadata_rolename):\n    \"\"\"\n    <Purpose>\n      Non-public method that raises an exception if the current specified\n      metadata has expired.\n\n    <Arguments>\n      metadata_object:\n        The metadata that should be expired, a 'tuf.formats.ANYROLE_SCHEMA'\n        object.\n\n      metadata_rolename:\n        The name of the metadata. This is a role name and should not end\n        in '.json'.  Examples: 'root', 'targets', 'targets/linux/x86'.\n\n    <Exceptions>\n      tuf.exceptions.ExpiredMetadataError:\n        If 'metadata_rolename' has expired.\n      securesystemslib.exceptions.FormatError:\n        If the expiration cannot be parsed correctly\n    <Side Effects>\n      None.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Extract the expiration time. Convert it to a unix timestamp and compare it\n    # against the current time.time() (also in Unix/POSIX time format, although\n    # with microseconds attached.)\n    expires_datetime = formats.expiry_string_to_datetime(\n        metadata_object['expires'])\n    expires_timestamp = formats.datetime_to_unix_timestamp(expires_datetime)\n\n    current_time = int(time.time())\n    if expires_timestamp <= current_time:\n      message = 'Metadata '+repr(metadata_rolename)+' expired on ' + \\\n        expires_datetime.ctime() + ' (UTC).'\n      raise exceptions.ExpiredMetadataError(message)\n\n\n\n\n\n  def all_targets(self):\n    \"\"\"\n    <Purpose>\n\n      NOTE: This function is deprecated.  Its behavior with regard to which\n      delegating Targets roles are trusted to determine how to validate a\n      delegated Targets role is NOT WELL DEFINED.  Please transition to use of\n      get_one_valid_targetinfo()!\n\n      Get a list of the target information for all the trusted targets on the\n      repository.  This list also includes all the targets of delegated roles.\n      Targets of the list returned are ordered according the trusted order of\n      the delegated roles, where parent roles come before children.  The list\n      conforms to 'tuf.formats.TARGETINFOS_SCHEMA' and has the form:\n\n      [{'filepath': 'a/b/c.txt',\n        'fileinfo': {'length': 13323,\n                     'hashes': {'sha256': dbfac345..}}\n       ...]\n\n    <Arguments>\n      None.\n\n    <Exceptions>\n      tuf.exceptions.RepositoryError:\n        If the metadata for the 'targets' role is missing from\n        the 'snapshot' metadata.\n\n      tuf.exceptions.UnknownRoleError:\n        If one of the roles could not be found in the role database.\n\n    <Side Effects>\n      The metadata for target roles is updated and stored.\n\n    <Returns>\n     A list of targets, conformant to\n     'tuf.formats.TARGETINFOS_SCHEMA'.\n    \"\"\"\n\n    warnings.warn(\n        'Support for all_targets() will be removed in a future release.'\n        '  get_one_valid_targetinfo() should be used instead.',\n        DeprecationWarning)\n\n    # Load the most up-to-date targets of the 'targets' role and all\n    # delegated roles.\n    self._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n    # Fetch the targets for the 'targets' role.\n    all_targets = self._targets_of_role('targets', skip_refresh=True)\n\n    # Fetch the targets of the delegated roles.  get_rolenames returns\n    # all roles available on the repository.\n    delegated_targets = []\n    for role in roledb.get_rolenames(self.repository_name):\n      if role in roledb.TOP_LEVEL_ROLES:\n        continue\n\n      else:\n        delegated_targets.extend(self._targets_of_role(role, skip_refresh=True))\n\n    all_targets.extend(delegated_targets)\n\n    return all_targets\n\n\n\n\n\n  def _refresh_targets_metadata(self, rolename='targets',\n    refresh_all_delegated_roles=False):\n    \"\"\"\n    <Purpose>\n      Non-public method that refreshes the targets metadata of 'rolename'.  If\n      'refresh_all_delegated_roles' is True, include all the delegations that\n      follow 'rolename'.  The metadata for the 'targets' role is updated in\n      refresh() by the _update_metadata_if_changed('targets') call, not here.\n      Delegated roles are not loaded when the repository is first initialized.\n      They are loaded from disk, updated if they have changed, and stored to\n      the 'self.metadata' store by this method.  This method is called by\n      get_one_valid_targetinfo().\n\n    <Arguments>\n      rolename:\n        This is a delegated role name and should not end in '.json'.  Example:\n        'unclaimed'.\n\n      refresh_all_delegated_roles:\n         Boolean indicating if all the delegated roles available in the\n         repository (via snapshot.json) should be refreshed.\n\n    <Exceptions>\n      tuf.exceptions.ExpiredMetadataError:\n        If local metadata is expired and newer metadata is not available.\n\n      tuf.exceptions.RepositoryError:\n        If the metadata file for the 'targets' role is missing from the\n        'snapshot' metadata.\n\n    <Side Effects>\n      The metadata for the delegated roles are loaded and updated if they\n      have changed.  Delegated metadata is removed from the role database if\n      it has expired.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    roles_to_update = []\n\n    if rolename + '.json' in self.metadata['current']['snapshot']['meta']:\n      roles_to_update.append(rolename)\n\n    if refresh_all_delegated_roles:\n\n      for role in self.metadata['current']['snapshot']['meta'].keys():\n        # snapshot.json keeps track of root.json, targets.json, and delegated\n        # roles (e.g., django.json, unclaimed.json).  Remove the 'targets' role\n        # because it gets updated when the targets.json file is updated in\n        # _update_metadata_if_changed('targets') and root.\n        if role.endswith('.json'):\n          role = role[:-len('.json')]\n          if role not in ['root', 'targets', rolename]:\n            roles_to_update.append(role)\n\n        else:\n          continue\n\n    # If there is nothing to refresh, we are done.\n    if not roles_to_update:\n      return\n\n    logger.debug('Roles to update: ' + repr(roles_to_update) + '.')\n\n    # Iterate 'roles_to_update', and load and update its metadata file if it\n    # has changed.\n    for rolename in roles_to_update:\n      self._load_metadata_from_file('previous', rolename)\n      self._load_metadata_from_file('current', rolename)\n\n      self._update_metadata_if_changed(rolename)\n\n\n\n\n\n  def _targets_of_role(self, rolename, targets=None, skip_refresh=False):\n    \"\"\"\n    <Purpose>\n      Non-public method that returns the target information of all the targets\n      of 'rolename'.  The returned information is a list conformant to\n      'tuf.formats.TARGETINFOS_SCHEMA', and has the form:\n\n      [{'filepath': 'a/b/c.txt',\n        'fileinfo': {'length': 13323,\n                     'hashes': {'sha256': dbfac345..}}\n       ...]\n\n    <Arguments>\n      rolename:\n        This is a role name and should not end in '.json'.  Examples: 'targets',\n        'unclaimed'.\n\n      targets:\n        A list of targets containing target information, conformant to\n        'tuf.formats.TARGETINFOS_SCHEMA'.\n\n      skip_refresh:\n        A boolean indicating if the target metadata for 'rolename'\n        should be refreshed.\n\n    <Exceptions>\n      tuf.exceptions.UnknownRoleError:\n        If 'rolename' is not found in the role database.\n\n    <Side Effects>\n      The metadata for 'rolename' is refreshed if 'skip_refresh' is False.\n\n    <Returns>\n      A list of dict objects containing the target information of all the\n      targets of 'rolename'.  Conformant to\n      'tuf.formats.TARGETINFOS_SCHEMA'.\n    \"\"\"\n\n    if targets is None:\n      targets = []\n\n    targets_of_role = list(targets)\n    logger.debug('Getting targets of role: ' + repr(rolename) + '.')\n\n    if not roledb.role_exists(rolename, self.repository_name):\n      raise exceptions.UnknownRoleError(rolename)\n\n    # We do not need to worry about the target paths being trusted because\n    # this is enforced before any new metadata is accepted.\n    if not skip_refresh:\n      self._refresh_targets_metadata(rolename)\n\n    # Do we have metadata for 'rolename'?\n    if rolename not in self.metadata['current']:\n      logger.debug('No metadata for ' + repr(rolename) + '.'\n        '  Unable to determine targets.')\n      return []\n\n    # Get the targets specified by the role itself.\n    for filepath, fileinfo in self.metadata['current'][rolename].get('targets', []).items():\n      new_target = {}\n      new_target['filepath'] = filepath\n      new_target['fileinfo'] = fileinfo\n\n      targets_of_role.append(new_target)\n\n    return targets_of_role\n\n\n\n\n\n  def targets_of_role(self, rolename='targets'):\n    \"\"\"\n    <Purpose>\n\n      NOTE: This function is deprecated.  Use with rolename 'targets' is secure\n      and the behavior well-defined, but use with any delegated targets role is\n      not. Please transition use for delegated targets roles to\n      get_one_valid_targetinfo().  More information is below.\n\n      Return a list of trusted targets directly specified by 'rolename'.\n      The returned information is a list conformant to\n      'tuf.formats.TARGETINFOS_SCHEMA', and has the form:\n\n      [{'filepath': 'a/b/c.txt',\n        'fileinfo': {'length': 13323,\n                     'hashes': {'sha256': dbfac345..}}\n       ...]\n\n      The metadata of 'rolename' is updated if out of date, including the\n      metadata of its parent roles (i.e., the minimum roles needed to set the\n      chain of trust).\n\n    <Arguments>\n      rolename:\n        The name of the role whose list of targets are wanted.\n        The name of the role should start with 'targets'.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If 'rolename' is improperly formatted.\n\n      tuf.exceptions.RepositoryError:\n        If the metadata of 'rolename' cannot be updated.\n\n      tuf.exceptions.UnknownRoleError:\n        If 'rolename' is not found in the role database.\n\n    <Side Effects>\n      The metadata of updated delegated roles are downloaded and stored.\n\n    <Returns>\n      A list of targets, conformant to\n      'tuf.formats.TARGETINFOS_SCHEMA'.\n    \"\"\"\n\n    warnings.warn(\n        'Support for targets_of_role() will be removed in a future release.'\n        '  get_one_valid_targetinfo() should be used instead.',\n        DeprecationWarning)\n\n    # Does 'rolename' have the correct format?\n    # Raise 'securesystemslib.exceptions.FormatError' if there is a mismatch.\n    formats.RELPATH_SCHEMA.check_match(rolename)\n\n    # If we've been given a delegated targets role, we don't know how to\n    # validate it without knowing what the delegating role is -- there could\n    # be several roles that delegate to the given role.  Behavior of this\n    # function for roles other than Targets is not well defined as a result.\n    # This function is deprecated, but:\n    #   - Usage of this function or a future successor makes sense when the\n    #     role of interest is Targets, since we always know exactly how to\n    #     validate Targets (We use root.).\n    #   - Until it's removed (hopefully soon), we'll try to provide what it has\n    #     always provided.  To do this, we fetch and \"validate\" all delegated\n    #     roles listed by snapshot.  For delegated roles only, the order of the\n    #     validation impacts the security of the validation -- the most-\n    #     recently-validated role delegating to a role you are currently\n    #     validating determines the expected keyids and threshold of the role\n    #     you are currently validating.  That is NOT GOOD.  Again, please switch\n    #     to get_one_valid_targetinfo, which is well-defined and secure.\n    if rolename != 'targets':\n      self._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n\n    if not roledb.role_exists(rolename, self.repository_name):\n      raise exceptions.UnknownRoleError(rolename)\n\n    return self._targets_of_role(rolename, skip_refresh=True)\n\n\n\n\n\n  def get_one_valid_targetinfo(self, target_filepath):\n    \"\"\"\n    <Purpose>\n      Return the target information for 'target_filepath', and update its\n      corresponding metadata, if necessary.  'target_filepath' must match\n      exactly as it appears in metadata, and should not contain URL encoding\n      escapes.\n\n    <Arguments>\n      target_filepath:\n        The path to the target file on the repository. This will be relative to\n        the 'targets' (or equivalent) directory on a given mirror.\n\n    <Exceptions>\n      tuf.exceptions.ExpiredMetadataError:\n        If local metadata is expired and newer metadata is not available.\n\n      securesystemslib.exceptions.FormatError:\n        If 'target_filepath' is improperly formatted.\n\n      tuf.exceptions.UnknownTargetError:\n        If 'target_filepath' was not found.\n\n      Any other unforeseen runtime exception.\n\n    <Side Effects>\n      The metadata for updated delegated roles are downloaded and stored.\n\n    <Returns>\n      The target information for 'target_filepath', conformant to\n      'tuf.formats.TARGETINFO_SCHEMA'.\n    \"\"\"\n\n    # Does 'target_filepath' have the correct format?\n    # Raise 'securesystemslib.exceptions.FormatError' if there is a mismatch.\n    formats.RELPATH_SCHEMA.check_match(target_filepath)\n\n    target_filepath = target_filepath.replace('\\\\', '/')\n\n    if target_filepath.startswith('/'):\n      raise exceptions.FormatError('The requested target file cannot'\n          ' contain a leading path separator: ' + repr(target_filepath))\n\n    # Get target by looking at roles in order of priority tags.\n    target = self._preorder_depth_first_walk(target_filepath)\n\n    # Raise an exception if the target information could not be retrieved.\n    if target is None:\n      raise exceptions.UnknownTargetError(repr(target_filepath) + ' not'\n          ' found.')\n\n    # Otherwise, return the found target.\n    else:\n      return target\n\n\n\n\n\n  def _preorder_depth_first_walk(self, target_filepath):\n    \"\"\"\n    <Purpose>\n      Non-public method that interrogates the tree of target delegations in\n      order of appearance (which implicitly order trustworthiness), and returns\n      the matching target found in the most trusted role.\n\n    <Arguments>\n      target_filepath:\n        The path to the target file on the repository. This will be relative to\n        the 'targets' (or equivalent) directory on a given mirror.\n\n    <Exceptions>\n      tuf.exceptions.ExpiredMetadataError:\n        If local metadata is expired and newer metadata is not available.\n\n      securesystemslib.exceptions.FormatError:\n        If 'target_filepath' is improperly formatted.\n\n      tuf.exceptions.RepositoryError:\n        If 'target_filepath' is not found.\n\n    <Side Effects>\n      The metadata for updated delegated roles are downloaded and stored.\n\n    <Returns>\n      The target information for 'target_filepath', conformant to\n      'tuf.formats.TARGETINFO_SCHEMA'.\n    \"\"\"\n\n    target = None\n    current_metadata = self.metadata['current']\n    role_names = ['targets']\n    visited_role_names = set()\n    number_of_delegations = settings.MAX_NUMBER_OF_DELEGATIONS\n\n    # Ensure the client has the most up-to-date version of 'targets.json'.\n    # Raise 'tuf.exceptions.NoWorkingMirrorError' if the changed metadata\n    # cannot be successfully downloaded and 'tuf.exceptions.RepositoryError' if\n    # the referenced metadata is missing.  Target methods such as this one are\n    # called after the top-level metadata have been refreshed (i.e.,\n    # updater.refresh()).\n    self._update_metadata_if_changed('targets')\n\n    # Preorder depth-first traversal of the graph of target delegations.\n    while target is None and number_of_delegations > 0 and len(role_names) > 0:\n\n      # Pop the role name from the top of the stack.\n      role_name = role_names.pop(-1)\n\n      # Skip any visited current role to prevent cycles.\n      if role_name in visited_role_names:\n        logger.debug('Skipping visited current role ' + repr(role_name))\n        continue\n\n      # The metadata for 'role_name' must be downloaded/updated before its\n      # targets, delegations, and child roles can be inspected.\n      # self.metadata['current'][role_name] is currently missing.\n      # _refresh_targets_metadata() does not refresh 'targets.json', it\n      # expects _update_metadata_if_changed() to have already refreshed it,\n      # which this function has checked above.\n      self._refresh_targets_metadata(role_name,\n          refresh_all_delegated_roles=False)\n\n      role_metadata = current_metadata[role_name]\n      targets = role_metadata['targets']\n      delegations = role_metadata.get('delegations', {})\n      child_roles = delegations.get('roles', [])\n      target = self._get_target_from_targets_role(role_name, targets,\n                                                  target_filepath)\n      # After preorder check, add current role to set of visited roles.\n      visited_role_names.add(role_name)\n\n      # And also decrement number of visited roles.\n      number_of_delegations -= 1\n\n      if target is None:\n\n        child_roles_to_visit = []\n        # NOTE: This may be a slow operation if there are many delegated roles.\n        for child_role in child_roles:\n          child_role_name = self._visit_child_role(child_role, target_filepath)\n          if child_role['terminating'] and child_role_name is not None:\n            logger.debug('Adding child role ' + repr(child_role_name))\n            logger.debug('Not backtracking to other roles.')\n            role_names = []\n            child_roles_to_visit.append(child_role_name)\n            break\n\n          elif child_role_name is None:\n            logger.debug('Skipping child role ' + repr(child_role_name))\n\n          else:\n            logger.debug('Adding child role ' + repr(child_role_name))\n            child_roles_to_visit.append(child_role_name)\n\n        # Push 'child_roles_to_visit' in reverse order of appearance onto\n        # 'role_names'.  Roles are popped from the end of the 'role_names'\n        # list.\n        child_roles_to_visit.reverse()\n        role_names.extend(child_roles_to_visit)\n\n      else:\n        logger.debug('Found target in current role ' + repr(role_name))\n\n    if target is None and number_of_delegations == 0 and len(role_names) > 0:\n      logger.debug(repr(len(role_names)) + ' roles left to visit, ' +\n          'but allowed to visit at most ' +\n          repr(settings.MAX_NUMBER_OF_DELEGATIONS) + ' delegations.')\n\n    return target\n\n\n\n\n\n  def _get_target_from_targets_role(self, role_name, targets, target_filepath):\n    \"\"\"\n    <Purpose>\n      Non-public method that determines whether the targets role with the given\n      'role_name' has the target with the name 'target_filepath'.\n\n    <Arguments>\n      role_name:\n        The name of the targets role that we are inspecting.\n\n      targets:\n        The targets of the Targets role with the name 'role_name'.\n\n      target_filepath:\n        The path to the target file on the repository. This will be relative to\n        the 'targets' (or equivalent) directory on a given mirror.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      None.\n\n    <Returns>\n      The target information for 'target_filepath', conformant to\n      'tuf.formats.TARGETINFO_SCHEMA'.\n    \"\"\"\n\n    # Does the current role name have our target?\n    logger.debug('Asking role ' + repr(role_name) + ' about'\n        ' target ' + repr(target_filepath))\n\n    target = targets.get(target_filepath)\n\n    if target:\n      logger.debug('Found target ' + target_filepath + ' in role ' + role_name)\n      return {'filepath': target_filepath, 'fileinfo': target}\n\n    else:\n      logger.debug(\n          'Target file ' + target_filepath + ' not found in role ' + role_name)\n      return None\n\n\n\n\n\n  def _visit_child_role(self, child_role, target_filepath):\n    \"\"\"\n    <Purpose>\n      Non-public method that determines whether the given 'target_filepath'\n      is an allowed path of 'child_role'.\n\n      Ensure that we explore only delegated roles trusted with the target.  The\n      metadata for 'child_role' should have been refreshed prior to this point,\n      however, the paths/targets that 'child_role' signs for have not been\n      verified (as intended).  The paths/targets that 'child_role' is allowed\n      to specify in its metadata depends on the delegating role, and thus is\n      left to the caller to verify.  We verify here that 'target_filepath'\n      is an allowed path according to the delegated 'child_role'.\n\n      TODO: Should the TUF spec restrict the repository to one particular\n      algorithm?  Should we allow the repository to specify in the role\n      dictionary the algorithm used for these generated hashed paths?\n\n    <Arguments>\n      child_role:\n        The delegation targets role object of 'child_role', containing its\n        paths, path_hash_prefixes, keys, and so on.\n\n      target_filepath:\n        The path to the target file on the repository. This will be relative to\n        the 'targets' (or equivalent) directory on a given mirror.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      None.\n\n    <Returns>\n      If 'child_role' has been delegated the target with the name\n      'target_filepath', then we return the role name of 'child_role'.\n\n      Otherwise, we return None.\n    \"\"\"\n\n    child_role_name = child_role['name']\n    child_role_paths = child_role.get('paths')\n    child_role_path_hash_prefixes = child_role.get('path_hash_prefixes')\n\n    if child_role_path_hash_prefixes is not None:\n      target_filepath_hash = self._get_target_hash(target_filepath)\n      for child_role_path_hash_prefix in child_role_path_hash_prefixes:\n        if target_filepath_hash.startswith(child_role_path_hash_prefix):\n          return child_role_name\n\n        else:\n          continue\n\n    elif child_role_paths is not None:\n      # Is 'child_role_name' allowed to sign for 'target_filepath'?\n      for child_role_path in child_role_paths:\n        # A child role path may be an explicit path or glob pattern (Unix\n        # shell-style wildcards).  The child role 'child_role_name' is returned\n        # if 'target_filepath' is equal to or matches 'child_role_path'.\n        # Explicit filepaths are also considered matches.  A repo maintainer\n        # might delegate a glob pattern with a leading path separator, while\n        # the client requests a matching target without a leading path\n        # separator - make sure to strip any leading path separators so that a\n        # match is made.  Example: \"foo.tgz\" should match with \"/*.tgz\".\n        if fnmatch.fnmatch(target_filepath.lstrip(os.sep), child_role_path.lstrip(os.sep)):\n          logger.debug('Child role ' + repr(child_role_name) + ' is allowed to'\n            ' sign for ' + repr(target_filepath))\n\n          return child_role_name\n\n        else:\n          logger.debug(\n              'The given target path ' + repr(target_filepath) + ' does not'\n              ' match the trusted path or glob pattern: ' + repr(child_role_path))\n          continue\n\n    else:\n      # 'role_name' should have been validated when it was downloaded.\n      # The 'paths' or 'path_hash_prefixes' fields should not be missing,\n      # so we raise a format error here in case they are both missing.\n      raise sslib_exceptions.FormatError(repr(child_role_name) + ' '\n          'has neither a \"paths\" nor \"path_hash_prefixes\".  At least'\n          ' one of these attributes must be present.')\n\n    return None\n\n\n\n  def _get_target_hash(self, target_filepath, hash_function='sha256'):\n    \"\"\"\n    <Purpose>\n      Non-public method that computes the hash of 'target_filepath'. This is\n      useful in conjunction with the \"path_hash_prefixes\" attribute in a\n      delegated targets role, which tells us which paths it is implicitly\n      responsible for.\n\n    <Arguments>\n      target_filepath:\n        The path to the target file on the repository. This will be relative to\n        the 'targets' (or equivalent) directory on a given mirror.\n\n      hash_function:\n        The algorithm used by the repository to generate the hashes of the\n        target filepaths.  The repository may optionally organize targets into\n        hashed bins to ease target delegations and role metadata management.\n        The use of consistent hashing allows for a uniform distribution of\n        targets into bins.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      None.\n\n    <Returns>\n      The hash of 'target_filepath'.\n    \"\"\"\n\n    # Calculate the hash of the filepath to determine which bin to find the\n    # target.  The client currently assumes the repository (i.e., repository\n    # tool) uses 'hash_function' to generate hashes and UTF-8.\n    digest_object = sslib_hash.digest(hash_function)\n    encoded_target_filepath = target_filepath.encode('utf-8')\n    digest_object.update(encoded_target_filepath)\n    target_filepath_hash = digest_object.hexdigest()\n\n    return target_filepath_hash\n\n\n\n\n\n  def remove_obsolete_targets(self, destination_directory):\n    \"\"\"\n    <Purpose>\n      Remove any files that are in 'previous' but not 'current'.  This makes it\n      so if you remove a file from a repository, it actually goes away.  The\n      targets for the 'targets' role and all delegated roles are checked.\n\n    <Arguments>\n      destination_directory:\n        The directory containing the target files tracked by TUF.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If 'destination_directory' is improperly formatted.\n\n      tuf.exceptions.RepositoryError:\n        If an error occurred removing any files.\n\n    <Side Effects>\n      Target files are removed from disk.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Does 'destination_directory' have the correct format?\n    # Raise 'securesystemslib.exceptions.FormatError' if there is a mismatch.\n    sslib_formats.PATH_SCHEMA.check_match(destination_directory)\n\n    # Iterate the rolenames and verify whether the 'previous' directory\n    # contains a target no longer found in 'current'.\n    for role in roledb.get_rolenames(self.repository_name):\n      if role.startswith('targets'):\n        if role in self.metadata['previous'] and self.metadata['previous'][role] != None:\n          for target in self.metadata['previous'][role]['targets']:\n            if target not in self.metadata['current'][role]['targets']:\n              # 'target' is only in 'previous', so remove it.\n              logger.warning('Removing obsolete file: ' + repr(target) + '.')\n\n              # Remove the file if it hasn't been removed already.\n              destination = \\\n                os.path.join(destination_directory, target.lstrip(os.sep))\n              try:\n                os.remove(destination)\n\n              except OSError as e:\n                # If 'filename' already removed, just log it.\n                if e.errno == errno.ENOENT:\n                  logger.info('File ' + repr(destination) + ' was already'\n                    ' removed.')\n\n                else:\n                  logger.warning('Failed to remove obsolete target: ' + str(e) )\n\n            else:\n              logger.debug('Skipping: ' + repr(target) + '.  It is still'\n                ' a current target.')\n        else:\n          logger.debug('Skipping: ' + repr(role) + '.  Not in the previous'\n            ' metadata')\n\n\n\n\n\n  def updated_targets(self, targets, destination_directory):\n    \"\"\"\n    <Purpose>\n      Checks files in the provided directory against the provided file metadata.\n\n      Filters the provided target info, returning a subset: only the metadata\n      for targets for which the target file either does not exist in the\n      provided directory, or for which the target file in the provided directory\n      does not match the provided metadata.\n\n      A principle use of this function is to determine which target files need\n      to be downloaded.  If the caller first uses get_one_valid_target_info()\n      calls to obtain up-to-date, valid metadata for targets, the caller can\n      then call updated_targets() to determine if that metadata does not match\n      what exists already on disk (in the provided directory).  The returned\n      values can then be used in download_file() calls to update the files that\n      didn't exist or didn't match.\n\n      The returned information is a list conformant to\n      'tuf.formats.TARGETINFOS_SCHEMA' and has the form:\n\n      [{'filepath': 'a/b/c.txt',\n        'fileinfo': {'length': 13323,\n                     'hashes': {'sha256': dbfac345..}}\n       ...]\n\n    <Arguments>\n      targets:\n        Metadata about the expected state of target files, against which local\n        files will be checked. This should be a list of target info\n        dictionaries; i.e. 'targets' must be conformant to\n        tuf.formats.TARGETINFOS_SCHEMA.\n\n      destination_directory:\n        The directory containing the target files.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If the arguments are improperly formatted.\n\n    <Side Effects>\n      The files in 'targets' are read and their hashes computed.\n\n    <Returns>\n      A list of target info dictionaries. The list conforms to\n      'tuf.formats.TARGETINFOS_SCHEMA'.\n      This is a strict subset of the argument 'targets'.\n    \"\"\"\n\n    # Do the arguments have the correct format?\n    # Raise 'securesystemslib.exceptions.FormatError' if there is a mismatch.\n    formats.TARGETINFOS_SCHEMA.check_match(targets)\n    sslib_formats.PATH_SCHEMA.check_match(destination_directory)\n\n    # Keep track of the target objects and filepaths of updated targets.\n    # Return 'updated_targets' and use 'updated_targetpaths' to avoid\n    # duplicates.\n    updated_targets = []\n    updated_targetpaths = []\n\n    for target in targets:\n      # Prepend 'destination_directory' to the target's relative filepath (as\n      # stored in metadata.)  Verify the hash of 'target_filepath' against\n      # each hash listed for its fileinfo.  Note: join() discards\n      # 'destination_directory' if 'filepath' contains a leading path separator\n      # (i.e., is treated as an absolute path).\n      filepath = target['filepath']\n      if filepath[0] == '/':\n        filepath = filepath[1:]\n      target_filepath = os.path.join(destination_directory, filepath)\n\n      if target_filepath in updated_targetpaths:\n        continue\n\n      # Try one of the algorithm/digest combos for a mismatch.  We break\n      # as soon as we find a mismatch.\n      for algorithm, digest in target['fileinfo']['hashes'].items():\n        digest_object = None\n        try:\n          digest_object = sslib_hash.digest_filename(target_filepath,\n            algorithm=algorithm)\n\n        # This exception would occur if the target does not exist locally.\n        except sslib_exceptions.StorageError:\n          updated_targets.append(target)\n          updated_targetpaths.append(target_filepath)\n          break\n\n        # The file does exist locally, check if its hash differs.\n        if digest_object.hexdigest() != digest:\n          updated_targets.append(target)\n          updated_targetpaths.append(target_filepath)\n          break\n\n    return updated_targets\n\n\n\n\n\n  def download_target(self, target, destination_directory,\n      prefix_filename_with_hash=True):\n    \"\"\"\n    <Purpose>\n      Download 'target' and verify it is trusted.\n\n      This will only store the file at 'destination_directory' if the\n      downloaded file matches the description of the file in the trusted\n      metadata.\n\n    <Arguments>\n      target:\n        The target to be downloaded.  Conformant to\n        'tuf.formats.TARGETINFO_SCHEMA'.\n\n      destination_directory:\n        The directory to save the downloaded target file.\n\n      prefix_filename_with_hash:\n        Whether to prefix the targets file names with their hash when using\n        consistent snapshot.\n        This should be set to False when the served target filenames are not\n        prefixed with hashes (in this case the server uses other means\n        to ensure snapshot consistency).\n        Default is True.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If 'target' is not properly formatted.\n\n      tuf.exceptions.NoWorkingMirrorError:\n        If a target could not be downloaded from any of the mirrors.\n\n        Although expected to be rare, there might be OSError exceptions (except\n        errno.EEXIST) raised when creating the destination directory (if it\n        doesn't exist).\n\n    <Side Effects>\n      A target file is saved to the local system.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Do the arguments have the correct format?\n    # This check ensures the arguments have the appropriate\n    # number of objects and object types, and that all dict\n    # keys are properly named.\n    # Raise 'securesystemslib.exceptions.FormatError' if the check fail.\n    formats.TARGETINFO_SCHEMA.check_match(target)\n    sslib_formats.PATH_SCHEMA.check_match(destination_directory)\n\n    # Extract the target file information.\n    target_filepath = target['filepath']\n    trusted_length = target['fileinfo']['length']\n    trusted_hashes = target['fileinfo']['hashes']\n\n    # Build absolute 'destination' file path.\n    # Note: join() discards 'destination_directory' if 'target_path' contains\n    # a leading path separator (i.e., is treated as an absolute path).\n    destination = os.path.join(destination_directory,\n        target_filepath.lstrip(os.sep))\n    destination = os.path.abspath(destination)\n    target_dirpath = os.path.dirname(destination)\n\n    # When attempting to create the leaf directory of 'target_dirpath', ignore\n    # any exceptions raised if the root directory already exists.  All other\n    # exceptions potentially thrown by os.makedirs() are re-raised.\n    # Note: os.makedirs can raise OSError if the leaf directory already exists\n    # or cannot be created.\n    try:\n      os.makedirs(target_dirpath)\n\n    except OSError as e:\n      if e.errno == errno.EEXIST:\n        pass\n\n      else:\n        raise\n\n    # '_get_target_file()' checks every mirror and returns the first target\n    # that passes verification.\n    target_file_object = self._get_target_file(target_filepath, trusted_length,\n        trusted_hashes, prefix_filename_with_hash)\n\n    sslib_util.persist_temp_file(target_file_object, destination)\n", "code_before": "#!/usr/bin/env python\n\n# Copyright 2012 - 2017, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\n<Program Name>\n  updater.py\n\n<Author>\n  Geremy Condra\n  Vladimir Diaz <vladimir.v.diaz@gmail.com>\n\n<Started>\n  July 2012.  Based on a previous version of this module. (VLAD)\n\n<Copyright>\n  See LICENSE-MIT OR LICENSE for licensing information.\n\n<Purpose>\n  'updater.py' is intended to be the only TUF module that software update\n  systems need to utilize.  It provides a single class representing an\n  updater that includes methods to download, install, and verify\n  metadata/target files in a secure manner.  Importing 'updater.py' and\n  instantiating its main class is all that is required by the client prior\n  to a TUF update request.  The importation and instantiation steps allow\n  TUF to load all of the required metadata files and set the repository mirror\n  information.\n\n  An overview of the update process:\n\n  1. The software update system instructs TUF to check for updates.\n\n  2. TUF downloads and verifies timestamp.json.\n\n  3. If timestamp.json indicates that snapshot.json has changed, TUF downloads\n     and verifies snapshot.json.\n\n  4. TUF determines which metadata files listed in snapshot.json differ from\n     those described in the last snapshot.json that TUF has seen.  If root.json\n     has changed, the update process starts over using the new root.json.\n\n  5. TUF provides the software update system with a list of available files\n     according to targets.json.\n\n  6. The software update system instructs TUF to download a specific target\n     file.\n\n  7. TUF downloads and verifies the file and then makes the file available to\n     the software update system.\n\n<Example Client>\n\n  # The client first imports the 'updater.py' module, the only module the\n  # client is required to import.  The client will utilize a single class\n  # from this module.\n  from tuf.client.updater import Updater\n\n  # The only other module the client interacts with is 'tuf.settings'.  The\n  # client accesses this module solely to set the repository directory.\n  # This directory will hold the files downloaded from a remote repository.\n  from tuf import settings\n  settings.repositories_directory = 'local-repository'\n\n  # Next, the client creates a dictionary object containing the repository\n  # mirrors.  The client may download content from any one of these mirrors.\n  # In the example below, a single mirror named 'mirror1' is defined.  The\n  # mirror is located at 'http://localhost:8001', and all of the metadata\n  # and targets files can be found in the 'metadata' and 'targets' directory,\n  # respectively.  If the client wishes to only download target files from\n  # specific directories on the mirror, the 'confined_target_dirs' field\n  # should be set.  In this example, the client hasn't set confined_target_dirs,\n  # which is interpreted as no confinement.\n  # In other words, the client can download\n  # targets from any directory or subdirectories.  If the client had chosen\n  # 'targets1/', they would have been confined to the '/targets/targets1/'\n  # directory on the 'http://localhost:8001' mirror.\n  repository_mirrors = {'mirror1': {'url_prefix': 'http://localhost:8001',\n                                    'metadata_path': 'metadata',\n                                    'targets_path': 'targets'}}\n\n  # The updater may now be instantiated.  The Updater class of 'updater.py'\n  # is called with two arguments.  The first argument assigns a name to this\n  # particular updater and the second argument the repository mirrors defined\n  # above.\n  updater = Updater('updater', repository_mirrors)\n\n  # The client next calls the refresh() method to ensure it has the latest\n  # copies of the metadata files.\n  updater.refresh()\n\n  # get_one_valid_targetinfo() updates role metadata when required.  In other\n  # words, if the client doesn't possess the metadata that lists 'LICENSE.txt',\n  # get_one_valid_targetinfo() will try to fetch / update it.\n  target = updater.get_one_valid_targetinfo('LICENSE.txt')\n\n  # Determine if 'target' has changed since the client's last refresh().  A\n  # target is considered updated if it does not exist in\n  # 'destination_directory' (current directory) or the target located there has\n  # changed.\n  destination_directory = '.'\n  updated_target = updater.updated_targets([target], destination_directory)\n\n  for target in updated_target:\n    updater.download_target(target, destination_directory)\n    # Client code here may also reference target information (including\n    # 'custom') by directly accessing the dictionary entries of the target.\n    # The 'custom' entry is additional file information explicitly set by the\n    # remote repository.\n    target_path = target['filepath']\n    target_length = target['fileinfo']['length']\n    target_hashes = target['fileinfo']['hashes']\n    target_custom_data = target['fileinfo']['custom']\n\"\"\"\n\nimport errno\nimport logging\nimport os\nimport shutil\nimport time\nimport fnmatch\nimport copy\nimport warnings\nimport io\nfrom urllib import parse\n\nfrom securesystemslib import exceptions as sslib_exceptions\nfrom securesystemslib import formats as sslib_formats\nfrom securesystemslib import hash as sslib_hash\nfrom securesystemslib import keys as sslib_keys\nfrom securesystemslib import util as sslib_util\n\nimport tuf\nfrom tuf import download\nfrom tuf import exceptions\nfrom tuf import formats\nfrom tuf import keydb\nfrom tuf import log # pylint: disable=unused-import\nfrom tuf import mirrors\nfrom tuf import roledb\nfrom tuf import settings\nfrom tuf import sig\nfrom tuf import requests_fetcher\n\n# The Timestamp role does not have signed metadata about it; otherwise we\n# would need an infinite regress of metadata. Therefore, we use some\n# default, but sane, upper file length for its metadata.\nDEFAULT_TIMESTAMP_UPPERLENGTH = settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n# The Root role may be updated without knowing its version number if\n# top-level metadata cannot be safely downloaded (e.g., keys may have been\n# revoked, thus requiring a new Root file that includes the updated keys)\n# and 'unsafely_update_root_if_necessary' is True.\n# We use some default, but sane, upper file length for its metadata.\nDEFAULT_ROOT_UPPERLENGTH = settings.DEFAULT_ROOT_REQUIRED_LENGTH\n\n# See 'log.py' to learn how logging is handled in TUF.\nlogger = logging.getLogger(__name__)\n\n\nclass MultiRepoUpdater(object):\n  \"\"\"\n  <Purpose>\n    Provide a way for clients to request a target file from multiple\n    repositories.  Which repositories to query is determined by the map\n    file (i.e,. map.json).\n\n    See TAP 4 for more information on the map file and how to request updates\n    from multiple repositories.  TAP 4 describes how users may specify that a\n    particular threshold of repositories be used for some targets, while a\n    different threshold of repositories be used for others.\n\n  <Arguments>\n    map_file:\n      The path of the map file.  The map file is needed to determine which\n      repositories to query given a target file.\n\n  <Exceptions>\n    securesystemslib.exceptions.FormatError, if the map file is improperly\n    formatted.\n\n    tuf.exceptions.Error, if the map file cannot be loaded.\n\n  <Side Effects>\n    None.\n\n  <Returns>\n    None.\n  \"\"\"\n\n  def __init__(self, map_file):\n    # Is 'map_file' a path?  If not, raise\n    # 'securesystemslib.exceptions.FormatError'.  The actual content of the map\n    # file is validated later on in this method.\n    sslib_formats.PATH_SCHEMA.check_match(map_file)\n\n    # A dictionary mapping repositories to TUF updaters.\n    self.repository_names_to_updaters = {}\n\n    try:\n      # The map file dictionary that associates targets with repositories.\n      self.map_file = sslib_util.load_json_file(map_file)\n\n    except (sslib_exceptions.Error) as e:\n      raise exceptions.Error('Cannot load the map file: ' + str(e))\n\n    # Raise securesystemslib.exceptions.FormatError if the map file is\n    # improperly formatted.\n    formats.MAPFILE_SCHEMA.check_match(self.map_file)\n\n    # Save the \"repositories\" entry of the map file, with the following\n    # example format:\n    #\n    #  \"repositories\": {\n    #      \"Django\": [\"https://djangoproject.com/\"],\n    #      \"PyPI\":   [\"https://pypi.python.org/\"]\n    #  }\n    self.repository_names_to_mirrors = self.map_file['repositories']\n\n\n\n  def get_valid_targetinfo(self, target_filename, match_custom_field=True):\n    \"\"\"\n    <Purpose>\n      Get valid targetinfo, if any, for the given 'target_filename'.  The map\n      file controls the targetinfo returned (see TAP 4).  Return a dict of the\n      form {updater1: targetinfo, updater2: targetinfo, ...}, where the dict\n      keys are updater objects, and the dict values the matching targetinfo for\n      'target_filename'.\n\n    <Arguments>\n      target_filename:\n        The relative path of the target file to update.\n\n      match_custom_field:\n        Boolean that indicates whether the optional custom field in targetinfo\n        should match across the targetinfo provided by the threshold of\n        repositories.\n\n    <Exceptions>\n      tuf.exceptions.FormatError, if the argument is improperly formatted.\n\n      tuf.exceptions.Error, if the required local metadata directory or the\n      Root file does not exist.\n\n      tuf.exceptions.UnknownTargetError, if the repositories in the map file do\n      not agree on the target, or none of them have signed for the target.\n\n    <Side Effects>\n      None.\n\n    <Returns>\n      A dict of the form: {updater1: targetinfo, updater2: targetinfo, ...}.\n      The targetinfo (conformant with tuf.formats.TARGETINFO_SCHEMA) is for\n      'target_filename'.\n    \"\"\"\n\n    # Is the argument properly formatted?  If not, raise\n    # 'tuf.exceptions.FormatError'.\n    formats.RELPATH_SCHEMA.check_match(target_filename)\n\n    # TAP 4 requires that the following attributes be present in mappings:\n    # \"paths\", \"repositories\", \"terminating\", and \"threshold\".\n    formats.MAPPING_SCHEMA.check_match(self.map_file['mapping'])\n\n    # Set the top-level directory containing the metadata for each repository.\n    repositories_directory = settings.repositories_directory\n\n    # Verify that the required local directories exist for each repository.\n    self._verify_metadata_directories(repositories_directory)\n\n    # Iterate mappings.\n    # [{\"paths\": [], \"repositories\": [], \"terminating\": Boolean, \"threshold\":\n    # NUM}, ...]\n    for mapping in self.map_file['mapping']:\n\n      logger.debug('Interrogating mappings..' + repr(mapping))\n      if not self._target_matches_path_pattern(\n          target_filename, mapping['paths']):\n        # The mapping is irrelevant to the target file.  Try the next one, if\n        # any.\n        continue\n\n      # The mapping is relevant to the target...\n      else:\n        # Do the repositories in the mapping provide a threshold of matching\n        # targetinfo?\n        valid_targetinfo = self._matching_targetinfo(target_filename,\n            mapping, match_custom_field)\n\n        if valid_targetinfo:\n          return valid_targetinfo\n\n        else:\n          # If we are here, it means either (1) the mapping is irrelevant to\n          # the target, (2) the targets were missing from all repositories in\n          # this mapping, or (3) the targets on all repositories did not match.\n          # Whatever the case may be, are we allowed to continue to the next\n          # mapping?  Let's check the terminating entry!\n          if not mapping['terminating']:\n            logger.debug('The mapping was irrelevant to the target, and'\n                ' \"terminating\" was set to False.  Trying the next mapping...')\n            continue\n\n          else:\n            raise exceptions.UnknownTargetError('The repositories in the'\n                ' mapping do not agree on the target, or none of them have'\n                ' signed for the target, and \"terminating\" was set to True.')\n\n    # If we are here, it means either there were no mappings, or none of the\n    # mappings provided the target.\n    logger.debug('Did not find valid targetinfo for ' + repr(target_filename))\n    raise exceptions.UnknownTargetError('The repositories in the map'\n        ' file do not agree on the target, or none of them have signed'\n        ' for the target.')\n\n\n\n\n\n  def _verify_metadata_directories(self, repositories_directory):\n    # Iterate 'self.repository_names_to_mirrors' and verify that the expected\n    # local files and directories exist.  TAP 4 requires a separate local\n    # directory for each repository.\n    for repository_name in self.repository_names_to_mirrors:\n\n      logger.debug('Interrogating repository: ' + repr(repository_name))\n      # Each repository must cache its metadata in a separate location.\n      repository_directory = os.path.join(repositories_directory,\n          repository_name)\n\n      if not os.path.isdir(repository_directory):\n        raise exceptions.Error('The metadata directory'\n            ' for ' + repr(repository_name) + ' must exist'\n            ' at ' + repr(repository_directory))\n\n      else:\n        logger.debug('Found local directory for ' + repr(repository_name))\n\n      # The latest known root metadata file must also exist on disk.\n      root_file = os.path.join(\n          repository_directory, 'metadata', 'current', 'root.json')\n\n      if not os.path.isfile(root_file):\n        raise exceptions.Error(\n            'The Root file must exist at ' + repr(root_file))\n\n      else:\n        logger.debug('Found local Root file at ' + repr(root_file))\n\n\n\n\n\n  def _matching_targetinfo(\n      self, target_filename, mapping, match_custom_field=True):\n    valid_targetinfo = {}\n\n    # Retrieve the targetinfo from each repository using the underlying\n    # Updater() instance.\n    for repository_name in mapping['repositories']:\n      logger.debug('Retrieving targetinfo for ' + repr(target_filename) +\n          ' from repository...')\n\n      try:\n        targetinfo, updater = self._update_from_repository(\n            repository_name, target_filename)\n\n      except (exceptions.UnknownTargetError, exceptions.Error):\n        continue\n\n      valid_targetinfo[updater] = targetinfo\n\n      matching_targetinfo = {}\n      logger.debug('Verifying that a threshold of targetinfo are equal...')\n\n      # Iterate 'valid_targetinfo', looking for a threshold number of matches\n      # for 'targetinfo'.  The first targetinfo to reach the required threshold\n      # is returned.  For example, suppose the following list of targetinfo and\n      # a threshold of 2:\n      # [A, B, C, B, A, C]\n      # In this case, targetinfo B is returned.\n      for valid_updater, compared_targetinfo in valid_targetinfo.items():\n\n        if not self._targetinfo_match(\n            targetinfo, compared_targetinfo, match_custom_field):\n          continue\n\n        else:\n\n          matching_targetinfo[valid_updater] = targetinfo\n\n          if not len(matching_targetinfo) >= mapping['threshold']:\n            continue\n\n          else:\n            logger.debug('Found a threshold of matching targetinfo!')\n            # We now have a targetinfo (that matches across a threshold of\n            # repositories as instructed by the map file), along with the\n            # updaters that sign for it.\n            logger.debug(\n                'Returning updaters for targetinfo: ' + repr(targetinfo))\n\n            return matching_targetinfo\n\n    return None\n\n\n\n\n\n  def _targetinfo_match(self, targetinfo1, targetinfo2, match_custom_field=True):\n    if match_custom_field:\n      return (targetinfo1 == targetinfo2)\n\n    else:\n      targetinfo1_without_custom = copy.deepcopy(targetinfo1)\n      targetinfo2_without_custom = copy.deepcopy(targetinfo2)\n      targetinfo1_without_custom['fileinfo'].pop('custom', None)\n      targetinfo2_without_custom['fileinfo'].pop('custom', None)\n\n      return (targetinfo1_without_custom == targetinfo2_without_custom)\n\n\n\n\n\n  def _target_matches_path_pattern(self, target_filename, path_patterns):\n    for path_pattern in path_patterns:\n      logger.debug('Interrogating pattern ' + repr(path_pattern) + 'for'\n          ' target: ' + repr(target_filename))\n\n      # Example: \"foo.tgz\" should match with \"/*.tgz\".  Make sure to strip any\n      # leading path separators so that a match is made if a repo maintainer\n      # uses a leading separator with a delegated glob pattern, but a client\n      # doesn't include one when a target file is requested.\n      if fnmatch.fnmatch(target_filename.lstrip(os.sep), path_pattern.lstrip(os.sep)):\n        logger.debug('Found a match for ' + repr(target_filename))\n        return True\n\n      else:\n        logger.debug('Continue searching for relevant paths.')\n        continue\n\n    # If we are here, then none of the paths are relevant to the target.\n    logger.debug('None of the paths are relevant.')\n    return False\n\n\n\n\n\n\n  def get_updater(self, repository_name):\n    \"\"\"\n    <Purpose>\n      Get the updater instance corresponding to 'repository_name'.\n\n    <Arguments>\n      repository_name:\n        The name of the repository as it appears in the map file.  For example,\n        \"Django\" and \"PyPI\" in the \"repositories\" entry of the map file.\n\n        \"repositories\": {\n            \"Django\": [\"https://djangoproject.com/\"],\n            \"PyPI\":   [\"https://pypi.python.org/\"]\n        }\n\n    <Exceptions>\n      tuf.exceptions.FormatError, if any of the arguments are improperly\n      formatted.\n\n    <Side Effects>\n      None.\n\n    <Returns>\n      Returns the Updater() instance for 'repository_name'.  If the instance\n      does not exist, return None.\n    \"\"\"\n\n    # Are the arguments properly formatted?  If not, raise\n    # 'tuf.exceptions.FormatError'.\n    formats.NAME_SCHEMA.check_match(repository_name)\n\n    updater = self.repository_names_to_updaters.get(repository_name)\n\n    if not updater:\n\n      if repository_name not in self.repository_names_to_mirrors:\n        return None\n\n      else:\n        # Create repository mirrors object needed by the\n        # tuf.client.updater.Updater().  Each 'repository_name' can have more\n        # than one mirror.\n        repo_mirrors = {}\n\n        for url in self.repository_names_to_mirrors[repository_name]:\n          repo_mirrors[url] = {\n            'url_prefix': url,\n            'metadata_path': 'metadata',\n            'targets_path': 'targets'}\n\n        try:\n          # NOTE: State (e.g., keys) should NOT be shared across different\n          # updater instances.\n          logger.debug('Adding updater for ' + repr(repository_name))\n          updater = Updater(repository_name, repo_mirrors)\n\n        except Exception:\n          return None\n\n        else:\n          self.repository_names_to_updaters[repository_name] = updater\n\n    else:\n      logger.debug('Found an updater for ' + repr(repository_name))\n\n    # Ensure the updater's metadata is the latest before returning it.\n    updater.refresh()\n    return updater\n\n\n\n\n\n  def _update_from_repository(self, repository_name, target_filename):\n\n    updater = self.get_updater(repository_name)\n\n    if not updater:\n      raise exceptions.Error(\n          'Cannot load updater for ' + repr(repository_name))\n\n    else:\n      # Get one valid target info from the Updater object.\n      # 'tuf.exceptions.UnknownTargetError' raised by get_one_valid_targetinfo\n      # if a valid target cannot be found.\n      return updater.get_one_valid_targetinfo(target_filename), updater\n\n\n\n\n\nclass Updater(object):\n  \"\"\"\n  <Purpose>\n    Provide a class that can download target files securely.  The updater\n    keeps track of currently and previously trusted metadata, target files\n    available to the client, target file attributes such as file size and\n    hashes, key and role information, metadata signatures, and the ability\n    to determine when the download of a file should be permitted.\n\n  <Updater Attributes>\n    self.metadata:\n      Dictionary holding the currently and previously trusted metadata.\n\n      Example: {'current': {'root': ROOT_SCHEMA,\n                            'targets':TARGETS_SCHEMA, ...},\n                'previous': {'root': ROOT_SCHEMA,\n                             'targets':TARGETS_SCHEMA, ...}}\n\n    self.metadata_directory:\n      The directory where trusted metadata is stored.\n\n    self.versioninfo:\n      A cache of version numbers for the roles available on the repository.\n\n      Example: {'targets.json': {'version': 128}, ...}\n\n    self.mirrors:\n      The repository mirrors from which metadata and targets are available.\n      Conformant to 'tuf.formats.MIRRORDICT_SCHEMA'.\n\n    self.repository_name:\n      The name of the updater instance.\n\n  <Updater Methods>\n    refresh():\n      This method downloads, verifies, and loads metadata for the top-level\n      roles in a specific order (i.e., root -> timestamp -> snapshot -> targets)\n      The expiration time for downloaded metadata is also verified.\n\n      The metadata for delegated roles are not refreshed by this method, but by\n      the method that returns targetinfo (i.e., get_one_valid_targetinfo()).\n      The refresh() method should be called by the client before any target\n      requests.\n\n    get_one_valid_targetinfo(file_path):\n      Returns the target information for a specific file identified by its file\n      path.  This target method also downloads the metadata of updated targets.\n\n    updated_targets(targets, destination_directory):\n      After the client has retrieved the target information for those targets\n      they are interested in updating, they would call this method to determine\n      which targets have changed from those saved locally on disk.  All the\n      targets that have changed are returns in a list.  From this list, they\n      can request a download by calling 'download_target()'.\n\n    download_target(target, destination_directory):\n      This method performs the actual download of the specified target.  The\n      file is saved to the 'destination_directory' argument.\n\n    remove_obsolete_targets(destination_directory):\n      Any files located in 'destination_directory' that were previously\n      served by the repository but have since been removed, can be deleted\n      from disk by the client by calling this method.\n\n    Note: The methods listed above are public and intended for the software\n    updater integrating TUF with this module.  All other methods that may begin\n    with a single leading underscore are non-public and only used internally.\n    updater.py is not subclassed in TUF, nor is it designed to be subclassed,\n    so double leading underscores is not used.\n    http://www.python.org/dev/peps/pep-0008/#method-names-and-instance-variables\n  \"\"\"\n\n  def __init__(self, repository_name, repository_mirrors, fetcher=None):\n    \"\"\"\n    <Purpose>\n      Constructor.  Instantiating an updater object causes all the metadata\n      files for the top-level roles to be read from disk, including the key and\n      role information for the delegated targets of 'targets'.  The actual\n      metadata for delegated roles is not loaded in __init__.  The metadata for\n      these delegated roles, including nested delegated roles, are loaded,\n      updated, and saved to the 'self.metadata' store, as needed, by\n      get_one_valid_targetinfo().\n\n      The initial set of metadata files are provided by the software update\n      system utilizing TUF.\n\n      In order to use an updater, the following directories must already\n      exist locally:\n\n            {tuf.settings.repositories_directory}/{repository_name}/metadata/current\n            {tuf.settings.repositories_directory}/{repository_name}/metadata/previous\n\n      and, at a minimum, the root metadata file must exist:\n\n            {tuf.settings.repositories_directory}/{repository_name}/metadata/current/root.json\n\n    <Arguments>\n      repository_name:\n        The name of the repository.\n\n      repository_mirrors:\n        A dictionary holding repository mirror information, conformant to\n        'tuf.formats.MIRRORDICT_SCHEMA'.  This dictionary holds\n        information such as the directory containing the metadata and target\n        files, the server's URL prefix, and the target content directories the\n        client should be confined to.\n\n        repository_mirrors = {'mirror1': {'url_prefix': 'http://localhost:8001',\n                                          'metadata_path': 'metadata',\n                                          'targets_path': 'targets',\n                                          'confined_target_dirs': ['']}}\n\n      fetcher:\n        A concrete 'FetcherInterface' implementation. Performs the network\n        related download operations. If an external implementation is not\n        provided, tuf.fetcher.RequestsFetcher is used.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If the arguments are improperly formatted.\n\n      tuf.exceptions.RepositoryError:\n        If there is an error with the updater's repository files, such\n        as a missing 'root.json' file.\n\n    <Side Effects>\n      Th metadata files (e.g., 'root.json', 'targets.json') for the top- level\n      roles are read from disk and stored in dictionaries.  In addition, the\n      key and roledb modules are populated with 'repository_name' entries.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Do the arguments have the correct format?\n    # These checks ensure the arguments have the appropriate\n    # number of objects and object types and that all dict\n    # keys are properly named.\n    # Raise 'securesystemslib.exceptions.FormatError' if there is a mistmatch.\n    sslib_formats.NAME_SCHEMA.check_match(repository_name)\n    formats.MIRRORDICT_SCHEMA.check_match(repository_mirrors)\n\n    # Save the validated arguments.\n    self.repository_name = repository_name\n    self.mirrors = repository_mirrors\n\n    # Initialize Updater with an externally provided 'fetcher' implementing\n    # the network download. By default tuf.fetcher.RequestsFetcher is used.\n    if fetcher is None:\n      self.fetcher = requests_fetcher.RequestsFetcher()\n    else:\n      self.fetcher = fetcher\n\n    # Store the trusted metadata read from disk.\n    self.metadata = {}\n\n    # Store the currently trusted/verified metadata.\n    self.metadata['current'] = {}\n\n    # Store the previously trusted/verified metadata.\n    self.metadata['previous'] = {}\n\n    # Store the version numbers of roles available on the repository.  The dict\n    # keys are paths, and the dict values versioninfo data. This information\n    # can help determine whether a metadata file has changed and needs to be\n    # re-downloaded.\n    self.versioninfo = {}\n\n    # Store the file information of the root and snapshot roles.  The dict keys\n    # are paths, the dict values fileinfo data. This information can help\n    # determine whether a metadata file has changed and so needs to be\n    # re-downloaded.\n    self.fileinfo = {}\n\n    # Store the location of the client's metadata directory.\n    self.metadata_directory = {}\n\n    # Store the 'consistent_snapshot' of the Root role.  This setting\n    # determines if metadata and target files downloaded from remote\n    # repositories include the digest.\n    self.consistent_snapshot = False\n\n    # Ensure the repository metadata directory has been set.\n    if settings.repositories_directory is None:\n      raise exceptions.RepositoryError('The TUF update client'\n        ' module must specify the directory containing the local repository'\n        ' files.  \"tuf.settings.repositories_directory\" MUST be set.')\n\n    # Set the path for the current set of metadata files.\n    repositories_directory = settings.repositories_directory\n    repository_directory = os.path.join(repositories_directory, self.repository_name)\n\n    # raise MissingLocalRepository if the repo does not exist at all.\n    if not os.path.exists(repository_directory):\n      raise exceptions.MissingLocalRepositoryError('Local repository ' +\n        repr(repository_directory) + ' does not exist.')\n\n    current_path = os.path.join(repository_directory, 'metadata', 'current')\n\n    # Ensure the current path is valid/exists before saving it.\n    if not os.path.exists(current_path):\n      raise exceptions.RepositoryError('Missing'\n        ' ' + repr(current_path) + '.  This path must exist and, at a minimum,'\n        ' contain the Root metadata file.')\n\n    self.metadata_directory['current'] = current_path\n\n    # Set the path for the previous set of metadata files.\n    previous_path = os.path.join(repository_directory, 'metadata', 'previous')\n\n    # Ensure the previous path is valid/exists.\n    if not os.path.exists(previous_path):\n      raise exceptions.RepositoryError('Missing ' + repr(previous_path) + '.'\n        '  This path MUST exist.')\n\n    self.metadata_directory['previous'] = previous_path\n\n    # Load current and previous metadata.\n    for metadata_set in ['current', 'previous']:\n      for metadata_role in roledb.TOP_LEVEL_ROLES:\n        self._load_metadata_from_file(metadata_set, metadata_role)\n\n    # Raise an exception if the repository is missing the required 'root'\n    # metadata.\n    if 'root' not in self.metadata['current']:\n      raise exceptions.RepositoryError('No root of trust!'\n        ' Could not find the \"root.json\" file.')\n\n\n\n\n\n  def __str__(self):\n    \"\"\"\n      The string representation of an Updater object.\n    \"\"\"\n\n    return self.repository_name\n\n\n  @staticmethod\n  def _get_local_filename(rolename: str) -> str:\n    \"\"\"Return safe local filename for roles metadata\n\n    Use URL encoding to prevent issues with path separators and\n    with forbidden characters in Windows filesystems\"\"\"\n    return parse.quote(rolename, '') + '.json'\n\n\n  def _load_metadata_from_file(self, metadata_set, metadata_role):\n    \"\"\"\n    <Purpose>\n      Non-public method that loads current or previous metadata if there is a\n      local file.  If the expected file belonging to 'metadata_role' (e.g.,\n      'root.json') cannot be loaded, raise an exception.  The extracted metadata\n      object loaded from file is saved to the metadata store (i.e.,\n      self.metadata).\n\n    <Arguments>\n      metadata_set:\n        The string 'current' or 'previous', depending on whether one wants to\n        load the currently or previously trusted metadata file.\n\n      metadata_role:\n        The name of the metadata. This is a role name and should\n        not end in '.json'.  Examples: 'root', 'targets', 'unclaimed'.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If the role object loaded for 'metadata_role' is improperly formatted.\n\n      securesystemslib.exceptions.Error:\n        If there was an error importing a delegated role of 'metadata_role'\n        or the 'metadata_set' is not one currently supported.\n\n    <Side Effects>\n      If the metadata is loaded successfully, it is saved to the metadata\n      store.  If 'metadata_role' is 'root', the role and key databases\n      are reloaded.  If 'metadata_role' is a target metadata, all its\n      delegated roles are refreshed.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Ensure we have a valid metadata set.\n    if metadata_set not in ['current', 'previous']:\n      raise sslib_exceptions.Error(\n          'Invalid metadata set: ' + repr(metadata_set))\n\n    # Save and construct the full metadata path.\n    metadata_directory = self.metadata_directory[metadata_set]\n    metadata_filename = self._get_local_filename(metadata_role)\n    metadata_filepath = os.path.join(metadata_directory, metadata_filename)\n\n    # Ensure the metadata path is valid/exists, else ignore the call.\n    if os.path.exists(metadata_filepath):\n      # Load the file.  The loaded object should conform to\n      # 'tuf.formats.SIGNABLE_SCHEMA'.\n      try:\n        metadata_signable = sslib_util.load_json_file(\n            metadata_filepath)\n\n      # Although the metadata file may exist locally, it may not\n      # be a valid json file.  On the next refresh cycle, it will be\n      # updated as required.  If Root if cannot be loaded from disk\n      # successfully, an exception should be raised by the caller.\n      except sslib_exceptions.Error:\n        return\n\n      formats.check_signable_object_format(metadata_signable)\n\n      # Extract the 'signed' role object from 'metadata_signable'.\n      metadata_object = metadata_signable['signed']\n\n      # Save the metadata object to the metadata store.\n      self.metadata[metadata_set][metadata_role] = metadata_object\n\n      # If 'metadata_role' is 'root' or targets metadata, the key and role\n      # databases must be rebuilt.  If 'root', ensure self.consistent_snaptshots\n      # is updated.\n      if metadata_set == 'current':\n        if metadata_role == 'root':\n          self._rebuild_key_and_role_db()\n          self.consistent_snapshot = metadata_object['consistent_snapshot']\n\n        elif metadata_object['_type'] == 'targets':\n          # TODO: Should we also remove the keys of the delegated roles?\n          self._import_delegations(metadata_role)\n\n\n\n\n\n  def _rebuild_key_and_role_db(self):\n    \"\"\"\n    <Purpose>\n      Non-public method that rebuilds the key and role databases from the\n      currently trusted 'root' metadata object extracted from 'root.json'.\n      This private method is called when a new/updated 'root' metadata file is\n      loaded or when updater.refresh() is called.  This method will only store\n      the role information of the top-level roles (i.e., 'root', 'targets',\n      'snapshot', 'timestamp').\n\n    <Arguments>\n      None.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If the 'root' metadata is improperly formatted.\n\n      securesystemslib.exceptions.Error:\n        If there is an error loading a role contained in the 'root'\n        metadata.\n\n    <Side Effects>\n      The key and role databases are reloaded for the top-level roles.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Clobbering this means all delegated metadata files are rendered outdated\n    # and will need to be reloaded.  However, reloading the delegated metadata\n    # files is avoided here because fetching target information with\n    # get_one_valid_targetinfo() always causes a refresh of these files.  The\n    # metadata files for delegated roles are also not loaded when the\n    # repository is first instantiated.  Due to this setup, reloading delegated\n    # roles is not required here.\n    keydb.create_keydb_from_root_metadata(self.metadata['current']['root'],\n        self.repository_name)\n\n    roledb.create_roledb_from_root_metadata(self.metadata['current']['root'],\n        self.repository_name)\n\n\n\n\n\n  def _import_delegations(self, parent_role):\n    \"\"\"\n    <Purpose>\n      Non-public method that imports all the roles delegated by 'parent_role'.\n\n    <Arguments>\n      parent_role:\n        The role whose delegations will be imported.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If a key attribute of a delegated role's signing key is\n        improperly formatted.\n\n      securesystemslib.exceptions.Error:\n        If the signing key of a delegated role cannot not be loaded.\n\n    <Side Effects>\n      The key and role databases are modified to include the newly loaded roles\n      delegated by 'parent_role'.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    current_parent_metadata = self.metadata['current'][parent_role]\n\n    if 'delegations' not in current_parent_metadata:\n      return\n\n    # This could be quite slow with a large number of delegations.\n    keys_info = current_parent_metadata['delegations'].get('keys', {})\n    roles_info = current_parent_metadata['delegations'].get('roles', [])\n\n    logger.debug('Adding roles delegated from ' + repr(parent_role) + '.')\n\n    # Iterate the keys of the delegated roles of 'parent_role' and load them.\n    for keyid, keyinfo in keys_info.items():\n      if keyinfo['keytype'] in ['rsa', 'ed25519', 'ecdsa', 'ecdsa-sha2-nistp256']:\n\n        # We specify the keyid to ensure that it's the correct keyid\n        # for the key.\n        try:\n          key, _ = sslib_keys.format_metadata_to_key(keyinfo, keyid)\n\n          keydb.add_key(key, repository_name=self.repository_name)\n\n        except exceptions.KeyAlreadyExistsError:\n          pass\n\n        except (sslib_exceptions.FormatError, sslib_exceptions.Error):\n          logger.warning('Invalid key: ' + repr(keyid) + '. Aborting role ' +\n              'delegation for parent role \\'' + parent_role + '\\'.')\n          raise\n\n      else:\n        logger.warning('Invalid key type for ' + repr(keyid) + '.')\n        continue\n\n    # Add the roles to the role database.\n    for roleinfo in roles_info:\n      try:\n        # NOTE: roledb.add_role will take care of the case where rolename\n        # is None.\n        rolename = roleinfo.get('name')\n        logger.debug('Adding delegated role: ' + str(rolename) + '.')\n        roledb.add_role(rolename, roleinfo, self.repository_name)\n\n      except exceptions.RoleAlreadyExistsError:\n        logger.warning('Role already exists: ' + rolename)\n\n      except Exception:\n        logger.warning('Failed to add delegated role: ' + repr(rolename) + '.')\n        raise\n\n\n\n\n\n  def refresh(self, unsafely_update_root_if_necessary=True):\n    \"\"\"\n    <Purpose>\n      Update the latest copies of the metadata for the top-level roles. The\n      update request process follows a specific order to ensure the metadata\n      files are securely updated:\n      root (if necessary) -> timestamp -> snapshot -> targets.\n\n      Delegated metadata is not refreshed by this method. After this method is\n      called, the use of get_one_valid_targetinfo() will update delegated\n      metadata, when required.  Calling refresh() ensures that top-level\n      metadata is up-to-date, so that the target methods can refer to the\n      latest available content. Thus, refresh() should always be called by the\n      client before any requests of target file information.\n\n      The expiration time for downloaded metadata is also verified, including\n      local metadata that the repository claims is up to date.\n\n      If the refresh fails for any reason, then unless\n      'unsafely_update_root_if_necessary' is set, refresh will be retried once\n      after first attempting to update the root metadata file. Only after this\n      check will the exceptions listed here potentially be raised.\n\n    <Arguments>\n      unsafely_update_root_if_necessary:\n        Boolean that indicates whether to unsafely update the Root metadata if\n        any of the top-level metadata cannot be downloaded successfully.  The\n        Root role is unsafely updated if its current version number is unknown.\n\n    <Exceptions>\n      tuf.exceptions.NoWorkingMirrorError:\n        If the metadata for any of the top-level roles cannot be updated.\n\n      tuf.exceptions.ExpiredMetadataError:\n        If any of the top-level metadata is expired and no new version was\n        found.\n\n    <Side Effects>\n      Updates the metadata files of the top-level roles with the latest\n      information.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Do the arguments have the correct format?\n    # This check ensures the arguments have the appropriate\n    # number of objects and object types, and that all dict\n    # keys are properly named.\n    # Raise 'securesystemslib.exceptions.FormatError' if the check fail.\n    sslib_formats.BOOLEAN_SCHEMA.check_match(\n        unsafely_update_root_if_necessary)\n\n    # Update the top-level metadata.  The _update_metadata_if_changed() and\n    # _update_metadata() calls below do NOT perform an update if there\n    # is insufficient trusted signatures for the specified metadata.\n    # Raise 'tuf.exceptions.NoWorkingMirrorError' if an update fails.\n    root_metadata = self.metadata['current']['root']\n\n    try:\n      self._ensure_not_expired(root_metadata, 'root')\n\n    except exceptions.ExpiredMetadataError:\n      # Raise 'tuf.exceptions.NoWorkingMirrorError' if a valid (not\n      # expired, properly signed, and valid metadata) 'root.json' cannot be\n      # installed.\n      if unsafely_update_root_if_necessary:\n        logger.info('Expired Root metadata was loaded from disk.'\n          '  Try to update it now.' )\n\n      # The caller explicitly requested not to unsafely fetch an expired Root.\n      else:\n        logger.info('An expired Root metadata was loaded and must be updated.')\n        raise\n\n    # Update the root metadata and verify it by building a chain of trusted root\n    # keys from the current trusted root metadata file\n    self._update_root_metadata(root_metadata)\n\n    # Ensure that the role and key information of the top-level roles is the\n    # latest.  We do this whether or not Root needed to be updated, in order to\n    # ensure that, e.g., the entries in roledb for top-level roles are\n    # populated with expected keyid info so that roles can be validated.  In\n    # certain circumstances, top-level metadata might be missing because it was\n    # marked obsolete and deleted after a failed attempt, and thus we should\n    # refresh them here as a protective measure.  See Issue #736.\n    self._rebuild_key_and_role_db()\n    self.consistent_snapshot = \\\n        self.metadata['current']['root']['consistent_snapshot']\n\n    # Use default but sane information for timestamp metadata, and do not\n    # require strict checks on its required length.\n    self._update_metadata('timestamp', DEFAULT_TIMESTAMP_UPPERLENGTH)\n\n    self._update_metadata_if_changed('snapshot',\n        referenced_metadata='timestamp')\n    self._update_metadata_if_changed('targets')\n\n\n\n  def _update_root_metadata(self, current_root_metadata):\n    \"\"\"\n    <Purpose>\n      The root file must be signed by the current root threshold and keys as\n      well as the previous root threshold and keys. The update process for root\n      files means that each intermediate root file must be downloaded, to build\n      a chain of trusted root keys from keys already trusted by the client:\n\n        1.root -> 2.root -> 3.root\n\n      3.root must be signed by the threshold and keys of 2.root, and 2.root\n      must be signed by the threshold and keys of 1.root.\n\n    <Arguments>\n      current_root_metadata:\n        The currently held version of root.\n\n    <Side Effects>\n      Updates the root metadata files with the latest information.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    def neither_403_nor_404(mirror_error):\n      if isinstance(mirror_error, tuf.exceptions.FetcherHTTPError):\n        if mirror_error.status_code in {403, 404}:\n          return False\n      return True\n\n    # Temporarily set consistent snapshot. Will be updated to whatever is set\n    # in the latest root.json after running through the intermediates with\n    # _update_metadata().\n    self.consistent_snapshot = True\n\n    # Following the spec, try downloading the N+1th root for a certain maximum\n    # number of times.\n    lower_bound = current_root_metadata['version'] + 1\n    upper_bound = lower_bound + settings.MAX_NUMBER_ROOT_ROTATIONS\n\n    # Try downloading the next root.\n    for next_version in range(lower_bound, upper_bound):\n      try:\n        # Thoroughly verify it.\n        self._update_metadata('root', DEFAULT_ROOT_UPPERLENGTH,\n            version=next_version)\n      # When we run into HTTP 403/404 error from ALL mirrors, break out of\n      # loop, because the next root metadata file is most likely missing.\n      except exceptions.NoWorkingMirrorError as exception:\n        for mirror_error in exception.mirror_errors.values():\n          # Otherwise, reraise the error, because it is not a simple HTTP\n          # error.\n          if neither_403_nor_404(mirror_error):\n            logger.info('Misc error for root version ' + str(next_version))\n            raise\n          else:\n            logger.debug('HTTP error for root version ' + str(next_version))\n        # If we are here, then we ran into only 403 / 404 errors, which are\n        # good reasons to suspect that the next root metadata file does not\n        # exist.\n        break\n\n      # Ensure that the role and key information of the top-level roles is the\n      # latest.  We do this whether or not Root needed to be updated, in order\n      # to ensure that, e.g., the entries in roledb for top-level roles are\n      # populated with expected keyid info so that roles can be validated.  In\n      # certain circumstances, top-level metadata might be missing because it\n      # was marked obsolete and deleted after a failed attempt, and thus we\n      # should refresh them here as a protective measure.  See Issue #736.\n      self._rebuild_key_and_role_db()\n\n    # Set our consistent snapshot property to what the latest root has said.\n    self.consistent_snapshot = \\\n        self.metadata['current']['root']['consistent_snapshot']\n\n\n\n  def _check_hashes(self, file_object, trusted_hashes):\n    \"\"\"\n    <Purpose>\n      Non-public method that verifies multiple secure hashes of 'file_object'.\n\n    <Arguments>\n      file_object:\n        A file object.\n\n      trusted_hashes:\n        A dictionary with hash-algorithm names as keys and hashes as dict values.\n        The hashes should be in the hexdigest format.  Should be Conformant to\n        'securesystemslib.formats.HASHDICT_SCHEMA'.\n\n    <Exceptions>\n      securesystemslib.exceptions.BadHashError, if the hashes don't match.\n\n    <Side Effects>\n      Hash digest object is created using the 'securesystemslib.hash' module.\n      Position within file_object is changed.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Verify each hash, raise an exception if any hash fails to verify\n    for algorithm, trusted_hash in trusted_hashes.items():\n      digest_object = sslib_hash.digest_fileobject(file_object,\n          algorithm)\n      computed_hash = digest_object.hexdigest()\n\n      if trusted_hash != computed_hash:\n        raise sslib_exceptions.BadHashError(trusted_hash,\n            computed_hash)\n\n      else:\n        logger.info('Verified ' + algorithm + ' hash: ' + trusted_hash)\n\n\n\n\n\n  def _check_file_length(self, file_object, trusted_file_length):\n    \"\"\"\n    <Purpose>\n      Non-public method that ensures the length of 'file_object' is strictly\n      equal to 'trusted_file_length'.  This is a deliberately redundant\n      implementation designed to complement\n      download._check_downloaded_length().\n\n    <Arguments>\n      file_object:\n        A file object.\n\n      trusted_file_length:\n        A non-negative integer that is the trusted length of the file.\n\n    <Exceptions>\n      tuf.exceptions.DownloadLengthMismatchError, if the lengths do not match.\n\n    <Side Effects>\n      Reads the contents of 'file_object' and logs a message if 'file_object'\n      matches the trusted length.\n      Position within file_object is changed.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    file_object.seek(0, io.SEEK_END)\n    observed_length = file_object.tell()\n\n    # Return and log a message if the length 'file_object' is equal to\n    # 'trusted_file_length', otherwise raise an exception.  A hard check\n    # ensures that a downloaded file strictly matches a known, or trusted,\n    # file length.\n    if observed_length != trusted_file_length:\n      raise exceptions.DownloadLengthMismatchError(trusted_file_length,\n          observed_length)\n\n    else:\n      logger.debug('Observed length (' + str(observed_length) +\\\n          ') == trusted length (' + str(trusted_file_length) + ')')\n\n\n\n\n\n  def _get_target_file(self, target_filepath, file_length, file_hashes,\n      prefix_filename_with_hash):\n    \"\"\"\n    <Purpose>\n      Non-public method that safely (i.e., the file length and hash are\n      strictly equal to the trusted) downloads a target file up to a certain\n      length, and checks its hashes thereafter.\n\n    <Arguments>\n      target_filepath:\n        The target filepath (relative to the repository targets directory)\n        obtained from TUF targets metadata.\n\n      file_length:\n        The expected compressed length of the target file. If the file is not\n        compressed, then it will simply be its uncompressed length.\n\n      file_hashes:\n        The expected hashes of the target file.\n\n      prefix_filename_with_hash:\n        Whether to prefix the targets file names with their hash when using\n        consistent snapshot.\n        This should be set to False when the served target filenames are not\n        prefixed with hashes (in this case the server uses other means\n        to ensure snapshot consistency).\n\n    <Exceptions>\n      tuf.exceptions.NoWorkingMirrorError:\n        The target could not be fetched. This is raised only when all known\n        mirrors failed to provide a valid copy of the desired target file.\n\n    <Side Effects>\n      The target file is downloaded from all known repository mirrors in the\n      worst case. If a valid copy of the target file is found, it is stored in\n      a temporary file and returned.\n\n    <Returns>\n      A file object containing the target.\n    \"\"\"\n\n    if self.consistent_snapshot and prefix_filename_with_hash:\n      # Note: values() does not return a list in Python 3.  Use list()\n      # on values() for Python 2+3 compatibility.\n      target_digest = list(file_hashes.values()).pop()\n      dirname, basename = os.path.split(target_filepath)\n      target_filepath = os.path.join(dirname, target_digest + '.' + basename)\n\n    file_mirrors = mirrors.get_list_of_mirrors('target', target_filepath,\n        self.mirrors)\n\n    # file_mirror (URL): error (Exception)\n    file_mirror_errors = {}\n    file_object = None\n\n    for file_mirror in file_mirrors:\n      try:\n        file_object = download.safe_download(file_mirror,\n            file_length, self.fetcher)\n\n        # Verify 'file_object' against the expected length and hashes.\n        self._check_file_length(file_object, file_length)\n        self._check_hashes(file_object, file_hashes)\n        # If the file verifies, we don't need to try more mirrors\n        return file_object\n\n      except Exception as exception:\n        # Remember the error from this mirror, close tempfile if one was opened\n        logger.debug('Update failed from ' + file_mirror + '.')\n        file_mirror_errors[file_mirror] = exception\n        if file_object is not None:\n          file_object.close()\n          file_object = None\n\n    logger.debug('Failed to update ' + repr(target_filepath) + ' from'\n        ' all mirrors: ' + repr(file_mirror_errors))\n    raise exceptions.NoWorkingMirrorError(file_mirror_errors)\n\n\n\n\n\n  def _verify_root_self_signed(self, signable):\n    \"\"\"\n    Verify the root metadata in signable is signed by a threshold of keys,\n    where the threshold and valid keys are defined by itself\n    \"\"\"\n    threshold = signable['signed']['roles']['root']['threshold']\n    keyids = signable['signed']['roles']['root']['keyids']\n    keys = signable['signed']['keys']\n    signatures = signable['signatures']\n    signed = sslib_formats.encode_canonical(\n        signable['signed']).encode('utf-8')\n    verified_sig_keyids = set()\n\n    for signature in signatures:\n      keyid = signature['keyid']\n\n      # At this point we are verifying that the root metadata is signed by a\n      # threshold of keys listed in the current root role, therefore skip\n      # keys with a keyid that is not listed in the current root role.\n      if keyid not in keyids:\n        continue\n\n      key = keys[keyid]\n      # The ANYKEY_SCHEMA check in verify_signature expects the keydict to\n      # include a keyid\n      key['keyid'] = keyid\n      valid_sig = sslib_keys.verify_signature(key, signature, signed)\n\n      if valid_sig:\n        verified_sig_keyids.add(keyid)\n\n    if len(verified_sig_keyids) >= threshold:\n      return True\n    return False\n\n\n\n\n\n  def _verify_metadata_file(self, metadata_file_object,\n      metadata_role):\n    \"\"\"\n    <Purpose>\n      Non-public method that verifies a metadata file.  An exception is\n      raised if 'metadata_file_object is invalid.  There is no\n      return value.\n\n    <Arguments>\n      metadata_file_object:\n        A file object containing the metadata file.\n\n      metadata_role:\n        The role name of the metadata (e.g., 'root', 'targets',\n        'unclaimed').\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        In case the metadata file is valid JSON, but not valid TUF metadata.\n\n      tuf.exceptions.InvalidMetadataJSONError:\n        In case the metadata file is not valid JSON.\n\n      tuf.exceptions.ReplayedMetadataError:\n        In case the downloaded metadata file is older than the current one.\n\n      tuf.exceptions.RepositoryError:\n        In case the repository is somehow inconsistent; e.g. a parent has not\n        delegated to a child (contrary to expectations).\n\n      tuf.SignatureError:\n        In case the metadata file does not have a valid signature.\n\n    <Side Effects>\n      The content of 'metadata_file_object' is read and loaded, the current\n      position within the file is changed.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    metadata_file_object.seek(0)\n    metadata = metadata_file_object.read().decode('utf-8')\n\n    try:\n      metadata_signable = sslib_util.load_json_string(metadata)\n\n    except Exception as exception:\n      raise exceptions.InvalidMetadataJSONError(exception)\n\n    else:\n      # Ensure the loaded 'metadata_signable' is properly formatted.  Raise\n      # 'securesystemslib.exceptions.FormatError' if not.\n      formats.check_signable_object_format(metadata_signable)\n\n    # Is 'metadata_signable' expired?\n    self._ensure_not_expired(metadata_signable['signed'], metadata_role)\n\n    # We previously verified version numbers in this function, but have since\n    # moved version number verification to the functions that retrieve\n    # metadata.\n\n    # Verify the signature on the downloaded metadata object.\n    valid = sig.verify(metadata_signable, metadata_role,\n        self.repository_name)\n\n    if not valid:\n      raise sslib_exceptions.BadSignatureError(metadata_role)\n\n    # For root metadata, verify the downloaded root metadata object with the\n    # new threshold of new signatures contained within the downloaded root\n    # metadata object\n    # NOTE: we perform the checks on root metadata here because this enables\n    # us to perform the check before the tempfile is persisted. Furthermore,\n    # by checking here we can easily perform the check for each download\n    # mirror. Whereas if we check after _verify_metadata_file we may be\n    # persisting invalid files and we cannot try copies of the file from other\n    # mirrors.\n    if valid and metadata_role == 'root':\n      valid = self._verify_root_self_signed(metadata_signable)\n      if not valid:\n        raise sslib_exceptions.BadSignatureError(metadata_role)\n\n\n\n\n\n  def _get_metadata_file(self, metadata_role, remote_filename,\n    upperbound_filelength, expected_version):\n    \"\"\"\n    <Purpose>\n      Non-public method that tries downloading, up to a certain length, a\n      metadata file from a list of known mirrors. As soon as the first valid\n      copy of the file is found, the downloaded file is returned and the\n      remaining mirrors are skipped.\n\n    <Arguments>\n      metadata_role:\n        The role name of the metadata (e.g., 'root', 'targets', 'unclaimed').\n\n      remote_filename:\n        The relative file path (on the remove repository) of 'metadata_role'.\n\n      upperbound_filelength:\n        The expected length, or upper bound, of the metadata file to be\n        downloaded.\n\n      expected_version:\n        The expected and required version number of the 'metadata_role' file\n        downloaded.  'expected_version' is an integer.\n\n    <Exceptions>\n      tuf.exceptions.NoWorkingMirrorError:\n        The metadata could not be fetched. This is raised only when all known\n        mirrors failed to provide a valid copy of the desired metadata file.\n\n    <Side Effects>\n      The file is downloaded from all known repository mirrors in the worst\n      case. If a valid copy of the file is found, it is stored in a temporary\n      file and returned.\n\n    <Returns>\n      A file object containing the metadata.\n    \"\"\"\n\n    file_mirrors = mirrors.get_list_of_mirrors('meta', remote_filename,\n        self.mirrors)\n\n    # file_mirror (URL): error (Exception)\n    file_mirror_errors = {}\n    file_object = None\n\n    for file_mirror in file_mirrors:\n      try:\n        file_object = download.unsafe_download(file_mirror,\n            upperbound_filelength, self.fetcher)\n        file_object.seek(0)\n\n        # Verify 'file_object' according to the callable function.\n        # 'file_object' is also verified if decompressed above (i.e., the\n        # uncompressed version).\n        metadata_signable = \\\n          sslib_util.load_json_string(file_object.read().decode('utf-8'))\n\n        # Determine if the specification version number is supported.  It is\n        # assumed that \"spec_version\" is in (major.minor.fix) format, (for\n        # example: \"1.4.3\") and that releases with the same major version\n        # number maintain backwards compatibility.  Consequently, if the major\n        # version number of new metadata equals our expected major version\n        # number, the new metadata is safe to parse.\n        try:\n          metadata_spec_version = metadata_signable['signed']['spec_version']\n          metadata_spec_version_split = metadata_spec_version.split('.')\n          metadata_spec_major_version = int(metadata_spec_version_split[0])\n          metadata_spec_minor_version = int(metadata_spec_version_split[1])\n\n          code_spec_version_split = tuf.SPECIFICATION_VERSION.split('.')\n          code_spec_major_version = int(code_spec_version_split[0])\n          code_spec_minor_version = int(code_spec_version_split[1])\n\n          if metadata_spec_major_version != code_spec_major_version:\n            raise exceptions.UnsupportedSpecificationError(\n                'Downloaded metadata that specifies an unsupported '\n                'spec_version.  This code supports major version number: ' +\n                repr(code_spec_major_version) + '; however, the obtained '\n                'metadata lists version number: ' + str(metadata_spec_version))\n\n          #report to user if minor versions do not match, continue with update\n          if metadata_spec_minor_version != code_spec_minor_version:\n            logger.info(\"Downloaded metadata that specifies a different minor \" +\n                \"spec_version. This code has version \" +\n                str(tuf.SPECIFICATION_VERSION) +\n                \" and the metadata lists version number \" +\n                str(metadata_spec_version) +\n                \". The update will continue as the major versions match.\")\n\n        except (ValueError, TypeError) as error:\n          raise sslib_exceptions.FormatError('Improperly'\n              ' formatted spec_version, which must be in major.minor.fix format') from error\n\n        # If the version number is unspecified, ensure that the version number\n        # downloaded is greater than the currently trusted version number for\n        # 'metadata_role'.\n        version_downloaded = metadata_signable['signed']['version']\n\n        if expected_version is not None:\n          # Verify that the downloaded version matches the version expected by\n          # the caller.\n          if version_downloaded != expected_version:\n            raise exceptions.BadVersionNumberError('Downloaded'\n              ' version number: ' + repr(version_downloaded) + '.  Version'\n              ' number MUST be: ' + repr(expected_version))\n\n        # The caller does not know which version to download.  Verify that the\n        # downloaded version is at least greater than the one locally\n        # available.\n        else:\n          # Verify that the version number of the locally stored\n          # 'timestamp.json', if available, is less than what was downloaded.\n          # Otherwise, accept the new timestamp with version number\n          # 'version_downloaded'.\n\n          try:\n            current_version = \\\n              self.metadata['current'][metadata_role]['version']\n\n            if version_downloaded < current_version:\n              raise exceptions.ReplayedMetadataError(metadata_role,\n                  version_downloaded, current_version)\n\n          except KeyError:\n            logger.info(metadata_role + ' not available locally.')\n\n        self._verify_metadata_file(file_object, metadata_role)\n\n      except Exception as exception:\n        # Remember the error from this mirror, and \"reset\" the target file.\n        logger.debug('Update failed from ' + file_mirror + '.')\n        file_mirror_errors[file_mirror] = exception\n        if file_object:\n          file_object.close()\n          file_object = None\n\n      else:\n        break\n\n    if file_object:\n      return file_object\n\n    else:\n      logger.debug('Failed to update ' + repr(remote_filename) + ' from all'\n        ' mirrors: ' + repr(file_mirror_errors))\n      raise exceptions.NoWorkingMirrorError(file_mirror_errors)\n\n\n\n\n\n  def _update_metadata(self, metadata_role, upperbound_filelength, version=None):\n    \"\"\"\n    <Purpose>\n      Non-public method that downloads, verifies, and 'installs' the metadata\n      belonging to 'metadata_role'.  Calling this method implies that the\n      'metadata_role' on the repository is newer than the client's, and thus\n      needs to be re-downloaded.  The current and previous metadata stores are\n      updated if the newly downloaded metadata is successfully downloaded and\n      verified.  This method also assumes that the store of top-level metadata\n      is the latest and exists.\n\n    <Arguments>\n      metadata_role:\n        The name of the metadata. This is a role name and should not end\n        in '.json'.  Examples: 'root', 'targets', 'targets/linux/x86'.\n\n      upperbound_filelength:\n        The expected length, or upper bound, of the metadata file to be\n        downloaded.\n\n      version:\n        The expected and required version number of the 'metadata_role' file\n        downloaded.  'expected_version' is an integer.\n\n    <Exceptions>\n      tuf.exceptions.NoWorkingMirrorError:\n        The metadata cannot be updated. This is not specific to a single\n        failure but rather indicates that all possible ways to update the\n        metadata have been tried and failed.\n\n    <Side Effects>\n      The metadata file belonging to 'metadata_role' is downloaded from a\n      repository mirror.  If the metadata is valid, it is stored in the\n      metadata store.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Attempt a file download from each mirror until the file is downloaded and\n    # verified.  If the signature of the downloaded file is valid, proceed,\n    # otherwise log a warning and try the next mirror.  'metadata_file_object'\n    # is the file-like object returned by 'download.py'.  'metadata_signable'\n    # is the object extracted from 'metadata_file_object'.  Metadata saved to\n    # files are regarded as 'signable' objects, conformant to\n    # 'tuf.formats.SIGNABLE_SCHEMA'.\n    #\n    # Some metadata (presently timestamp) will be downloaded \"unsafely\", in the\n    # sense that we can only estimate its true length and know nothing about\n    # its version.  This is because not all metadata will have other metadata\n    # for it; otherwise we will have an infinite regress of metadata signing\n    # for each other. In this case, we will download the metadata up to the\n    # best length we can get for it, not request a specific version, but\n    # perform the rest of the checks (e.g., signature verification).\n\n    # Construct the metadata filename as expected by the download/mirror\n    # modules. Local filename is quoted to protect against names like\"../file\".\n\n    remote_filename = metadata_role + '.json'\n    local_filename = self._get_local_filename(metadata_role)\n    filename_version = ''\n\n    if self.consistent_snapshot and version:\n      filename_version = version\n      dirname, basename = os.path.split(remote_filename)\n      remote_filename = os.path.join(\n          dirname, str(filename_version) + '.' + basename)\n\n    metadata_file_object = \\\n      self._get_metadata_file(metadata_role, remote_filename,\n        upperbound_filelength, version)\n\n    # The metadata has been verified. Move the metadata file into place.\n    # First, move the 'current' metadata file to the 'previous' directory\n    # if it exists.\n    current_filepath = os.path.join(self.metadata_directory['current'],\n                local_filename)\n    current_filepath = os.path.abspath(current_filepath)\n    sslib_util.ensure_parent_dir(current_filepath)\n\n    previous_filepath = os.path.join(self.metadata_directory['previous'],\n        local_filename)\n    previous_filepath = os.path.abspath(previous_filepath)\n\n    if os.path.exists(current_filepath):\n      # Previous metadata might not exist, say when delegations are added.\n      sslib_util.ensure_parent_dir(previous_filepath)\n      shutil.move(current_filepath, previous_filepath)\n\n    # Next, move the verified updated metadata file to the 'current' directory.\n    metadata_file_object.seek(0)\n    metadata_signable = \\\n      sslib_util.load_json_string(metadata_file_object.read().decode('utf-8'))\n\n    sslib_util.persist_temp_file(metadata_file_object, current_filepath)\n\n    # Extract the metadata object so we can store it to the metadata store.\n    # 'current_metadata_object' set to 'None' if there is not an object\n    # stored for 'metadata_role'.\n    updated_metadata_object = metadata_signable['signed']\n    current_metadata_object = self.metadata['current'].get(metadata_role)\n\n    # Finally, update the metadata and fileinfo stores, and rebuild the\n    # key and role info for the top-level roles if 'metadata_role' is root.\n    # Rebuilding the key and role info is required if the newly-installed\n    # root metadata has revoked keys or updated any top-level role information.\n    logger.debug('Updated ' + repr(current_filepath) + '.')\n    self.metadata['previous'][metadata_role] = current_metadata_object\n    self.metadata['current'][metadata_role] = updated_metadata_object\n    self._update_versioninfo(remote_filename)\n\n\n\n\n\n  def _update_metadata_if_changed(self, metadata_role,\n    referenced_metadata='snapshot'):\n    \"\"\"\n    <Purpose>\n      Non-public method that updates the metadata for 'metadata_role' if it has\n      changed.  All top-level roles other than the 'timestamp' and 'root'\n      roles are updated by this method.  The 'timestamp' role is always\n      downloaded from a mirror without first checking if it has been updated;\n      it is updated in refresh() by calling _update_metadata('timestamp').\n      The 'root' role is always updated first and verified based on the trusted\n      root metadata file the client already has a copy of; it is updated in\n      refresh() by calling _update_root_metadata().\n      This method is also called for delegated role metadata, which are\n      referenced by 'snapshot'.\n\n      If the metadata needs to be updated but an update cannot be obtained,\n      this method will delete the file.\n\n      Due to the way in which metadata files are updated, it is expected that\n      'referenced_metadata' is not out of date and trusted.  The refresh()\n      method updates the top-level roles in 'root -> timestamp -> snapshot ->\n      targets' order.  For delegated metadata, the parent role is\n      updated before the delegated role.  Taking into account that\n      'referenced_metadata' is updated and verified before 'metadata_role',\n      this method determines if 'metadata_role' has changed by checking\n      the 'meta' field of the newly updated 'referenced_metadata'.\n\n    <Arguments>\n      metadata_role:\n        The name of the metadata. This is a role name and should not end\n        in '.json'.  Examples: 'root', 'targets', 'unclaimed'.\n\n      referenced_metadata:\n        This is the metadata that provides the role information for\n        'metadata_role'.  For the top-level roles, the 'snapshot' role\n        is the referenced metadata for the 'root', and 'targets' roles.\n        The 'timestamp' metadata is always downloaded regardless.  In\n        other words, it is updated by calling _update_metadata('timestamp')\n        and not by this method.  The referenced metadata for 'snapshot'\n        is 'timestamp'.  See refresh().\n\n    <Exceptions>\n      tuf.exceptions.ExpiredMetadataError:\n        If local metadata is expired and newer metadata is not available.\n\n      tuf.exceptions.NoWorkingMirrorError:\n        If 'metadata_role' could not be downloaded after determining that it\n        had changed.\n\n      tuf.exceptions.RepositoryError:\n        If the referenced metadata is missing.\n\n    <Side Effects>\n      If it is determined that 'metadata_role' has been updated, the metadata\n      store (i.e., self.metadata) is updated with the new metadata and the\n      affected stores modified (i.e., the previous metadata store is updated).\n      If the metadata is 'targets' or a delegated targets role, the role\n      database is updated with the new information, including its delegated\n      roles.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    metadata_filename = metadata_role + '.json'\n    expected_versioninfo = None\n\n    # Ensure the referenced metadata has been loaded.  The 'root' role may be\n    # updated without having 'snapshot' available.\n    if referenced_metadata not in self.metadata['current']:\n      raise exceptions.RepositoryError('Cannot update'\n        ' ' + repr(metadata_role) + ' because ' + referenced_metadata + ' is'\n        ' missing.')\n\n    # The referenced metadata has been loaded.  Extract the new versioninfo for\n    # 'metadata_role' from it.\n    else:\n      logger.debug(repr(metadata_role) + ' referenced in ' +\n        repr(referenced_metadata)+ '.  ' + repr(metadata_role) +\n        ' may be updated.')\n\n    # Simply return if the metadata for 'metadata_role' has not been updated,\n    # according to the uncompressed metadata provided by the referenced\n    # metadata.  The metadata is considered updated if its version number is\n    # strictly greater than its currently trusted version number.\n    expected_versioninfo = self.metadata['current'][referenced_metadata] \\\n        ['meta'][metadata_filename]\n\n    if not self._versioninfo_has_been_updated(metadata_filename,\n        expected_versioninfo):\n      logger.info(repr(metadata_filename) + ' up-to-date.')\n\n      # Since we have not downloaded a new version of this metadata, we should\n      # check to see if our local version is stale and notify the user if so.\n      # This raises tuf.exceptions.ExpiredMetadataError if the metadata we have\n      # is expired. Resolves issue #322.\n      self._ensure_not_expired(self.metadata['current'][metadata_role],\n          metadata_role)\n\n      # TODO: If metadata role is snapshot, we should verify that snapshot's\n      # hash matches what's listed in timestamp.json per step 3.1 of the\n      # detailed workflows in the specification\n\n      return\n\n    logger.debug('Metadata ' + repr(metadata_filename) + ' has changed.')\n\n    # The file lengths of metadata are unknown, only their version numbers are\n    # known.  Set an upper limit for the length of the downloaded file for each\n    # expected role.  Note: The Timestamp role is not updated via this\n    # function.\n    if metadata_role == 'snapshot':\n      upperbound_filelength = settings.DEFAULT_SNAPSHOT_REQUIRED_LENGTH\n\n    # The metadata is considered Targets (or delegated Targets metadata).\n    else:\n      upperbound_filelength = settings.DEFAULT_TARGETS_REQUIRED_LENGTH\n\n    try:\n      self._update_metadata(metadata_role, upperbound_filelength,\n          expected_versioninfo['version'])\n\n    except Exception:\n      # The current metadata we have is not current but we couldn't get new\n      # metadata. We shouldn't use the old metadata anymore.  This will get rid\n      # of in-memory knowledge of the role and delegated roles, but will leave\n      # delegated metadata files as current files on disk.\n      #\n      # TODO: Should we get rid of the delegated metadata files?  We shouldn't\n      # need to, but we need to check the trust implications of the current\n      # implementation.\n      self._delete_metadata(metadata_role)\n      logger.warning('Metadata for ' + repr(metadata_role) + ' cannot'\n          ' be updated.')\n      raise\n\n    else:\n      # We need to import the delegated roles of 'metadata_role', since its\n      # list of delegations might have changed from what was previously\n      # loaded..\n      # TODO: Should we remove the keys of the delegated roles?\n      self._import_delegations(metadata_role)\n\n\n\n\n\n  def _versioninfo_has_been_updated(self, metadata_filename, new_versioninfo):\n    \"\"\"\n    <Purpose>\n      Non-public method that determines whether the current versioninfo of\n      'metadata_filename' is less than 'new_versioninfo' (i.e., the version\n      number has been incremented).  The 'new_versioninfo' argument should be\n      extracted from the latest copy of the metadata that references\n      'metadata_filename'.  Example: 'root.json' would be referenced by\n      'snapshot.json'.\n\n      'new_versioninfo' should only be 'None' if this is for updating\n      'root.json' without having 'snapshot.json' available.\n\n    <Arguments>\n      metadadata_filename:\n        The metadata filename for the role.  For the 'root' role,\n        'metadata_filename' would be 'root.json'.\n\n      new_versioninfo:\n        A dict object representing the new file information for\n        'metadata_filename'.  'new_versioninfo' may be 'None' when\n        updating 'root' without having 'snapshot' available.  This\n        dict conforms to 'tuf.formats.VERSIONINFO_SCHEMA' and has\n        the form:\n\n        {'version': 288}\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      If there is no versioninfo currently loaded for 'metadata_filename', try\n      to load it.\n\n    <Returns>\n      Boolean.  True if the versioninfo has changed, False otherwise.\n    \"\"\"\n\n    # If there is no versioninfo currently stored for 'metadata_filename',\n    # try to load the file, calculate the versioninfo, and store it.\n    if metadata_filename not in self.versioninfo:\n      self._update_versioninfo(metadata_filename)\n\n    # Return true if there is no versioninfo for 'metadata_filename'.\n    # 'metadata_filename' is not in the 'self.versioninfo' store\n    # and it doesn't exist in the 'current' metadata location.\n    if self.versioninfo[metadata_filename] is None:\n      return True\n\n    current_versioninfo = self.versioninfo[metadata_filename]\n\n    logger.debug('New version for ' + repr(metadata_filename) +\n        ': ' + repr(new_versioninfo['version']) + '.  Old version: ' +\n        repr(current_versioninfo['version']))\n\n    if new_versioninfo['version'] > current_versioninfo['version']:\n      return True\n\n    else:\n      return False\n\n\n\n\n\n  def _update_versioninfo(self, metadata_filename):\n    \"\"\"\n    <Purpose>\n      Non-public method that updates the 'self.versioninfo' entry for the\n      metadata belonging to 'metadata_filename'.  If the current metadata for\n      'metadata_filename' cannot be loaded, set its 'versioninfo' to 'None' to\n      signal that it is not in 'self.versioninfo' AND it also doesn't exist\n      locally.\n\n    <Arguments>\n      metadata_filename:\n        The metadata filename for the role.  For the 'root' role,\n        'metadata_filename' would be 'root.json'.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      The version number of 'metadata_filename' is calculated and stored in its\n      corresponding entry in 'self.versioninfo'.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # In case we delayed loading the metadata and didn't do it in\n    # __init__ (such as with delegated metadata), then get the version\n    # info now.\n\n    # 'metadata_filename' is the key from meta dictionary: build the\n    # corresponding local filepath like _get_local_filename()\n    local_filename = parse.quote(metadata_filename, \"\")\n    current_filepath = os.path.join(self.metadata_directory['current'],\n        local_filename)\n\n    # If the path is invalid, simply return and leave versioninfo unset.\n    if not os.path.exists(current_filepath):\n      self.versioninfo[metadata_filename] = None\n      return\n\n    # Extract the version information from the trusted snapshot role and save\n    # it to the 'self.versioninfo' store.\n    if metadata_filename == 'timestamp.json':\n      trusted_versioninfo = \\\n        self.metadata['current']['timestamp']['version']\n\n    # When updating snapshot.json, the client either (1) has a copy of\n    # snapshot.json, or (2) is in the process of obtaining it by first\n    # downloading timestamp.json.  Note: Clients are allowed to have only\n    # root.json initially, and perform a refresh of top-level metadata to\n    # obtain the remaining roles.\n    elif metadata_filename == 'snapshot.json':\n\n      # Verify the version number of the currently trusted snapshot.json in\n      # snapshot.json itself.  Checking the version number specified in\n      # timestamp.json may be greater than the version specified in the\n      # client's copy of snapshot.json.\n      try:\n        timestamp_version_number = self.metadata['current']['snapshot']['version']\n        trusted_versioninfo = formats.make_versioninfo(\n            timestamp_version_number)\n\n      except KeyError:\n        trusted_versioninfo = \\\n          self.metadata['current']['timestamp']['meta']['snapshot.json']\n\n    else:\n\n      try:\n        # The metadata file names in 'self.metadata' exclude the role\n        # extension.  Strip the '.json' extension when checking if\n        # 'metadata_filename' currently exists.\n        targets_version_number = \\\n          self.metadata['current'][metadata_filename[:-len('.json')]]['version']\n        trusted_versioninfo = \\\n          formats.make_versioninfo(targets_version_number)\n\n      except KeyError:\n        trusted_versioninfo = \\\n          self.metadata['current']['snapshot']['meta'][metadata_filename]\n\n    self.versioninfo[metadata_filename] = trusted_versioninfo\n\n\n\n\n  def _move_current_to_previous(self, metadata_role):\n    \"\"\"\n    <Purpose>\n      Non-public method that moves the current metadata file for 'metadata_role'\n      to the previous directory.\n\n    <Arguments>\n      metadata_role:\n        The name of the metadata. This is a role name and should not end\n        in '.json'.  Examples: 'root', 'targets', 'targets/linux/x86'.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n     The metadata file for 'metadata_role' is removed from 'current'\n     and moved to the 'previous' directory.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Get the 'current' and 'previous' full file paths for 'metadata_role'\n    metadata_filepath = self._get_local_filename(metadata_role)\n    previous_filepath = os.path.join(self.metadata_directory['previous'],\n                                     metadata_filepath)\n    current_filepath = os.path.join(self.metadata_directory['current'],\n                                    metadata_filepath)\n\n    # Remove the previous path if it exists.\n    if os.path.exists(previous_filepath):\n      os.remove(previous_filepath)\n\n    # Move the current path to the previous path.\n    if os.path.exists(current_filepath):\n      sslib_util.ensure_parent_dir(previous_filepath)\n      os.rename(current_filepath, previous_filepath)\n\n\n\n\n\n  def _delete_metadata(self, metadata_role):\n    \"\"\"\n    <Purpose>\n      Non-public method that removes all (current) knowledge of 'metadata_role'.\n      The metadata belonging to 'metadata_role' is removed from the current\n      'self.metadata' store and from the role database. The 'root.json' role\n      file is never removed.\n\n    <Arguments>\n      metadata_role:\n        The name of the metadata. This is a role name and should not end\n        in '.json'.  Examples: 'root', 'targets', 'targets/linux/x86'.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      The role database is modified and the metadata for 'metadata_role'\n      removed from the 'self.metadata' store.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # The root metadata role is never deleted without a replacement.\n    if metadata_role == 'root':\n      return\n\n    # Get rid of the current metadata file.\n    self._move_current_to_previous(metadata_role)\n\n    # Remove knowledge of the role.\n    if metadata_role in self.metadata['current']:\n      del self.metadata['current'][metadata_role]\n    roledb.remove_role(metadata_role, self.repository_name)\n\n\n\n\n\n  def _ensure_not_expired(self, metadata_object, metadata_rolename):\n    \"\"\"\n    <Purpose>\n      Non-public method that raises an exception if the current specified\n      metadata has expired.\n\n    <Arguments>\n      metadata_object:\n        The metadata that should be expired, a 'tuf.formats.ANYROLE_SCHEMA'\n        object.\n\n      metadata_rolename:\n        The name of the metadata. This is a role name and should not end\n        in '.json'.  Examples: 'root', 'targets', 'targets/linux/x86'.\n\n    <Exceptions>\n      tuf.exceptions.ExpiredMetadataError:\n        If 'metadata_rolename' has expired.\n      securesystemslib.exceptions.FormatError:\n        If the expiration cannot be parsed correctly\n    <Side Effects>\n      None.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Extract the expiration time. Convert it to a unix timestamp and compare it\n    # against the current time.time() (also in Unix/POSIX time format, although\n    # with microseconds attached.)\n    expires_datetime = formats.expiry_string_to_datetime(\n        metadata_object['expires'])\n    expires_timestamp = formats.datetime_to_unix_timestamp(expires_datetime)\n\n    current_time = int(time.time())\n    if expires_timestamp <= current_time:\n      message = 'Metadata '+repr(metadata_rolename)+' expired on ' + \\\n        expires_datetime.ctime() + ' (UTC).'\n      raise exceptions.ExpiredMetadataError(message)\n\n\n\n\n\n  def all_targets(self):\n    \"\"\"\n    <Purpose>\n\n      NOTE: This function is deprecated.  Its behavior with regard to which\n      delegating Targets roles are trusted to determine how to validate a\n      delegated Targets role is NOT WELL DEFINED.  Please transition to use of\n      get_one_valid_targetinfo()!\n\n      Get a list of the target information for all the trusted targets on the\n      repository.  This list also includes all the targets of delegated roles.\n      Targets of the list returned are ordered according the trusted order of\n      the delegated roles, where parent roles come before children.  The list\n      conforms to 'tuf.formats.TARGETINFOS_SCHEMA' and has the form:\n\n      [{'filepath': 'a/b/c.txt',\n        'fileinfo': {'length': 13323,\n                     'hashes': {'sha256': dbfac345..}}\n       ...]\n\n    <Arguments>\n      None.\n\n    <Exceptions>\n      tuf.exceptions.RepositoryError:\n        If the metadata for the 'targets' role is missing from\n        the 'snapshot' metadata.\n\n      tuf.exceptions.UnknownRoleError:\n        If one of the roles could not be found in the role database.\n\n    <Side Effects>\n      The metadata for target roles is updated and stored.\n\n    <Returns>\n     A list of targets, conformant to\n     'tuf.formats.TARGETINFOS_SCHEMA'.\n    \"\"\"\n\n    warnings.warn(\n        'Support for all_targets() will be removed in a future release.'\n        '  get_one_valid_targetinfo() should be used instead.',\n        DeprecationWarning)\n\n    # Load the most up-to-date targets of the 'targets' role and all\n    # delegated roles.\n    self._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n    # Fetch the targets for the 'targets' role.\n    all_targets = self._targets_of_role('targets', skip_refresh=True)\n\n    # Fetch the targets of the delegated roles.  get_rolenames returns\n    # all roles available on the repository.\n    delegated_targets = []\n    for role in roledb.get_rolenames(self.repository_name):\n      if role in roledb.TOP_LEVEL_ROLES:\n        continue\n\n      else:\n        delegated_targets.extend(self._targets_of_role(role, skip_refresh=True))\n\n    all_targets.extend(delegated_targets)\n\n    return all_targets\n\n\n\n\n\n  def _refresh_targets_metadata(self, rolename='targets',\n    refresh_all_delegated_roles=False):\n    \"\"\"\n    <Purpose>\n      Non-public method that refreshes the targets metadata of 'rolename'.  If\n      'refresh_all_delegated_roles' is True, include all the delegations that\n      follow 'rolename'.  The metadata for the 'targets' role is updated in\n      refresh() by the _update_metadata_if_changed('targets') call, not here.\n      Delegated roles are not loaded when the repository is first initialized.\n      They are loaded from disk, updated if they have changed, and stored to\n      the 'self.metadata' store by this method.  This method is called by\n      get_one_valid_targetinfo().\n\n    <Arguments>\n      rolename:\n        This is a delegated role name and should not end in '.json'.  Example:\n        'unclaimed'.\n\n      refresh_all_delegated_roles:\n         Boolean indicating if all the delegated roles available in the\n         repository (via snapshot.json) should be refreshed.\n\n    <Exceptions>\n      tuf.exceptions.ExpiredMetadataError:\n        If local metadata is expired and newer metadata is not available.\n\n      tuf.exceptions.RepositoryError:\n        If the metadata file for the 'targets' role is missing from the\n        'snapshot' metadata.\n\n    <Side Effects>\n      The metadata for the delegated roles are loaded and updated if they\n      have changed.  Delegated metadata is removed from the role database if\n      it has expired.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    roles_to_update = []\n\n    if rolename + '.json' in self.metadata['current']['snapshot']['meta']:\n      roles_to_update.append(rolename)\n\n    if refresh_all_delegated_roles:\n\n      for role in self.metadata['current']['snapshot']['meta'].keys():\n        # snapshot.json keeps track of root.json, targets.json, and delegated\n        # roles (e.g., django.json, unclaimed.json).  Remove the 'targets' role\n        # because it gets updated when the targets.json file is updated in\n        # _update_metadata_if_changed('targets') and root.\n        if role.endswith('.json'):\n          role = role[:-len('.json')]\n          if role not in ['root', 'targets', rolename]:\n            roles_to_update.append(role)\n\n        else:\n          continue\n\n    # If there is nothing to refresh, we are done.\n    if not roles_to_update:\n      return\n\n    logger.debug('Roles to update: ' + repr(roles_to_update) + '.')\n\n    # Iterate 'roles_to_update', and load and update its metadata file if it\n    # has changed.\n    for rolename in roles_to_update:\n      self._load_metadata_from_file('previous', rolename)\n      self._load_metadata_from_file('current', rolename)\n\n      self._update_metadata_if_changed(rolename)\n\n\n\n\n\n  def _targets_of_role(self, rolename, targets=None, skip_refresh=False):\n    \"\"\"\n    <Purpose>\n      Non-public method that returns the target information of all the targets\n      of 'rolename'.  The returned information is a list conformant to\n      'tuf.formats.TARGETINFOS_SCHEMA', and has the form:\n\n      [{'filepath': 'a/b/c.txt',\n        'fileinfo': {'length': 13323,\n                     'hashes': {'sha256': dbfac345..}}\n       ...]\n\n    <Arguments>\n      rolename:\n        This is a role name and should not end in '.json'.  Examples: 'targets',\n        'unclaimed'.\n\n      targets:\n        A list of targets containing target information, conformant to\n        'tuf.formats.TARGETINFOS_SCHEMA'.\n\n      skip_refresh:\n        A boolean indicating if the target metadata for 'rolename'\n        should be refreshed.\n\n    <Exceptions>\n      tuf.exceptions.UnknownRoleError:\n        If 'rolename' is not found in the role database.\n\n    <Side Effects>\n      The metadata for 'rolename' is refreshed if 'skip_refresh' is False.\n\n    <Returns>\n      A list of dict objects containing the target information of all the\n      targets of 'rolename'.  Conformant to\n      'tuf.formats.TARGETINFOS_SCHEMA'.\n    \"\"\"\n\n    if targets is None:\n      targets = []\n\n    targets_of_role = list(targets)\n    logger.debug('Getting targets of role: ' + repr(rolename) + '.')\n\n    if not roledb.role_exists(rolename, self.repository_name):\n      raise exceptions.UnknownRoleError(rolename)\n\n    # We do not need to worry about the target paths being trusted because\n    # this is enforced before any new metadata is accepted.\n    if not skip_refresh:\n      self._refresh_targets_metadata(rolename)\n\n    # Do we have metadata for 'rolename'?\n    if rolename not in self.metadata['current']:\n      logger.debug('No metadata for ' + repr(rolename) + '.'\n        '  Unable to determine targets.')\n      return []\n\n    # Get the targets specified by the role itself.\n    for filepath, fileinfo in self.metadata['current'][rolename].get('targets', []).items():\n      new_target = {}\n      new_target['filepath'] = filepath\n      new_target['fileinfo'] = fileinfo\n\n      targets_of_role.append(new_target)\n\n    return targets_of_role\n\n\n\n\n\n  def targets_of_role(self, rolename='targets'):\n    \"\"\"\n    <Purpose>\n\n      NOTE: This function is deprecated.  Use with rolename 'targets' is secure\n      and the behavior well-defined, but use with any delegated targets role is\n      not. Please transition use for delegated targets roles to\n      get_one_valid_targetinfo().  More information is below.\n\n      Return a list of trusted targets directly specified by 'rolename'.\n      The returned information is a list conformant to\n      'tuf.formats.TARGETINFOS_SCHEMA', and has the form:\n\n      [{'filepath': 'a/b/c.txt',\n        'fileinfo': {'length': 13323,\n                     'hashes': {'sha256': dbfac345..}}\n       ...]\n\n      The metadata of 'rolename' is updated if out of date, including the\n      metadata of its parent roles (i.e., the minimum roles needed to set the\n      chain of trust).\n\n    <Arguments>\n      rolename:\n        The name of the role whose list of targets are wanted.\n        The name of the role should start with 'targets'.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If 'rolename' is improperly formatted.\n\n      tuf.exceptions.RepositoryError:\n        If the metadata of 'rolename' cannot be updated.\n\n      tuf.exceptions.UnknownRoleError:\n        If 'rolename' is not found in the role database.\n\n    <Side Effects>\n      The metadata of updated delegated roles are downloaded and stored.\n\n    <Returns>\n      A list of targets, conformant to\n      'tuf.formats.TARGETINFOS_SCHEMA'.\n    \"\"\"\n\n    warnings.warn(\n        'Support for targets_of_role() will be removed in a future release.'\n        '  get_one_valid_targetinfo() should be used instead.',\n        DeprecationWarning)\n\n    # Does 'rolename' have the correct format?\n    # Raise 'securesystemslib.exceptions.FormatError' if there is a mismatch.\n    formats.RELPATH_SCHEMA.check_match(rolename)\n\n    # If we've been given a delegated targets role, we don't know how to\n    # validate it without knowing what the delegating role is -- there could\n    # be several roles that delegate to the given role.  Behavior of this\n    # function for roles other than Targets is not well defined as a result.\n    # This function is deprecated, but:\n    #   - Usage of this function or a future successor makes sense when the\n    #     role of interest is Targets, since we always know exactly how to\n    #     validate Targets (We use root.).\n    #   - Until it's removed (hopefully soon), we'll try to provide what it has\n    #     always provided.  To do this, we fetch and \"validate\" all delegated\n    #     roles listed by snapshot.  For delegated roles only, the order of the\n    #     validation impacts the security of the validation -- the most-\n    #     recently-validated role delegating to a role you are currently\n    #     validating determines the expected keyids and threshold of the role\n    #     you are currently validating.  That is NOT GOOD.  Again, please switch\n    #     to get_one_valid_targetinfo, which is well-defined and secure.\n    if rolename != 'targets':\n      self._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n\n    if not roledb.role_exists(rolename, self.repository_name):\n      raise exceptions.UnknownRoleError(rolename)\n\n    return self._targets_of_role(rolename, skip_refresh=True)\n\n\n\n\n\n  def get_one_valid_targetinfo(self, target_filepath):\n    \"\"\"\n    <Purpose>\n      Return the target information for 'target_filepath', and update its\n      corresponding metadata, if necessary.  'target_filepath' must match\n      exactly as it appears in metadata, and should not contain URL encoding\n      escapes.\n\n    <Arguments>\n      target_filepath:\n        The path to the target file on the repository. This will be relative to\n        the 'targets' (or equivalent) directory on a given mirror.\n\n    <Exceptions>\n      tuf.exceptions.ExpiredMetadataError:\n        If local metadata is expired and newer metadata is not available.\n\n      securesystemslib.exceptions.FormatError:\n        If 'target_filepath' is improperly formatted.\n\n      tuf.exceptions.UnknownTargetError:\n        If 'target_filepath' was not found.\n\n      Any other unforeseen runtime exception.\n\n    <Side Effects>\n      The metadata for updated delegated roles are downloaded and stored.\n\n    <Returns>\n      The target information for 'target_filepath', conformant to\n      'tuf.formats.TARGETINFO_SCHEMA'.\n    \"\"\"\n\n    # Does 'target_filepath' have the correct format?\n    # Raise 'securesystemslib.exceptions.FormatError' if there is a mismatch.\n    formats.RELPATH_SCHEMA.check_match(target_filepath)\n\n    target_filepath = target_filepath.replace('\\\\', '/')\n\n    if target_filepath.startswith('/'):\n      raise exceptions.FormatError('The requested target file cannot'\n          ' contain a leading path separator: ' + repr(target_filepath))\n\n    # Get target by looking at roles in order of priority tags.\n    target = self._preorder_depth_first_walk(target_filepath)\n\n    # Raise an exception if the target information could not be retrieved.\n    if target is None:\n      raise exceptions.UnknownTargetError(repr(target_filepath) + ' not'\n          ' found.')\n\n    # Otherwise, return the found target.\n    else:\n      return target\n\n\n\n\n\n  def _preorder_depth_first_walk(self, target_filepath):\n    \"\"\"\n    <Purpose>\n      Non-public method that interrogates the tree of target delegations in\n      order of appearance (which implicitly order trustworthiness), and returns\n      the matching target found in the most trusted role.\n\n    <Arguments>\n      target_filepath:\n        The path to the target file on the repository. This will be relative to\n        the 'targets' (or equivalent) directory on a given mirror.\n\n    <Exceptions>\n      tuf.exceptions.ExpiredMetadataError:\n        If local metadata is expired and newer metadata is not available.\n\n      securesystemslib.exceptions.FormatError:\n        If 'target_filepath' is improperly formatted.\n\n      tuf.exceptions.RepositoryError:\n        If 'target_filepath' is not found.\n\n    <Side Effects>\n      The metadata for updated delegated roles are downloaded and stored.\n\n    <Returns>\n      The target information for 'target_filepath', conformant to\n      'tuf.formats.TARGETINFO_SCHEMA'.\n    \"\"\"\n\n    target = None\n    current_metadata = self.metadata['current']\n    role_names = ['targets']\n    visited_role_names = set()\n    number_of_delegations = settings.MAX_NUMBER_OF_DELEGATIONS\n\n    # Ensure the client has the most up-to-date version of 'targets.json'.\n    # Raise 'tuf.exceptions.NoWorkingMirrorError' if the changed metadata\n    # cannot be successfully downloaded and 'tuf.exceptions.RepositoryError' if\n    # the referenced metadata is missing.  Target methods such as this one are\n    # called after the top-level metadata have been refreshed (i.e.,\n    # updater.refresh()).\n    self._update_metadata_if_changed('targets')\n\n    # Preorder depth-first traversal of the graph of target delegations.\n    while target is None and number_of_delegations > 0 and len(role_names) > 0:\n\n      # Pop the role name from the top of the stack.\n      role_name = role_names.pop(-1)\n\n      # Skip any visited current role to prevent cycles.\n      if role_name in visited_role_names:\n        logger.debug('Skipping visited current role ' + repr(role_name))\n        continue\n\n      # The metadata for 'role_name' must be downloaded/updated before its\n      # targets, delegations, and child roles can be inspected.\n      # self.metadata['current'][role_name] is currently missing.\n      # _refresh_targets_metadata() does not refresh 'targets.json', it\n      # expects _update_metadata_if_changed() to have already refreshed it,\n      # which this function has checked above.\n      self._refresh_targets_metadata(role_name,\n          refresh_all_delegated_roles=False)\n\n      role_metadata = current_metadata[role_name]\n      targets = role_metadata['targets']\n      delegations = role_metadata.get('delegations', {})\n      child_roles = delegations.get('roles', [])\n      target = self._get_target_from_targets_role(role_name, targets,\n                                                  target_filepath)\n      # After preorder check, add current role to set of visited roles.\n      visited_role_names.add(role_name)\n\n      # And also decrement number of visited roles.\n      number_of_delegations -= 1\n\n      if target is None:\n\n        child_roles_to_visit = []\n        # NOTE: This may be a slow operation if there are many delegated roles.\n        for child_role in child_roles:\n          child_role_name = self._visit_child_role(child_role, target_filepath)\n          if child_role['terminating'] and child_role_name is not None:\n            logger.debug('Adding child role ' + repr(child_role_name))\n            logger.debug('Not backtracking to other roles.')\n            role_names = []\n            child_roles_to_visit.append(child_role_name)\n            break\n\n          elif child_role_name is None:\n            logger.debug('Skipping child role ' + repr(child_role_name))\n\n          else:\n            logger.debug('Adding child role ' + repr(child_role_name))\n            child_roles_to_visit.append(child_role_name)\n\n        # Push 'child_roles_to_visit' in reverse order of appearance onto\n        # 'role_names'.  Roles are popped from the end of the 'role_names'\n        # list.\n        child_roles_to_visit.reverse()\n        role_names.extend(child_roles_to_visit)\n\n      else:\n        logger.debug('Found target in current role ' + repr(role_name))\n\n    if target is None and number_of_delegations == 0 and len(role_names) > 0:\n      logger.debug(repr(len(role_names)) + ' roles left to visit, ' +\n          'but allowed to visit at most ' +\n          repr(settings.MAX_NUMBER_OF_DELEGATIONS) + ' delegations.')\n\n    return target\n\n\n\n\n\n  def _get_target_from_targets_role(self, role_name, targets, target_filepath):\n    \"\"\"\n    <Purpose>\n      Non-public method that determines whether the targets role with the given\n      'role_name' has the target with the name 'target_filepath'.\n\n    <Arguments>\n      role_name:\n        The name of the targets role that we are inspecting.\n\n      targets:\n        The targets of the Targets role with the name 'role_name'.\n\n      target_filepath:\n        The path to the target file on the repository. This will be relative to\n        the 'targets' (or equivalent) directory on a given mirror.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      None.\n\n    <Returns>\n      The target information for 'target_filepath', conformant to\n      'tuf.formats.TARGETINFO_SCHEMA'.\n    \"\"\"\n\n    # Does the current role name have our target?\n    logger.debug('Asking role ' + repr(role_name) + ' about'\n        ' target ' + repr(target_filepath))\n\n    target = targets.get(target_filepath)\n\n    if target:\n      logger.debug('Found target ' + target_filepath + ' in role ' + role_name)\n      return {'filepath': target_filepath, 'fileinfo': target}\n\n    else:\n      logger.debug(\n          'Target file ' + target_filepath + ' not found in role ' + role_name)\n      return None\n\n\n\n\n\n  def _visit_child_role(self, child_role, target_filepath):\n    \"\"\"\n    <Purpose>\n      Non-public method that determines whether the given 'target_filepath'\n      is an allowed path of 'child_role'.\n\n      Ensure that we explore only delegated roles trusted with the target.  The\n      metadata for 'child_role' should have been refreshed prior to this point,\n      however, the paths/targets that 'child_role' signs for have not been\n      verified (as intended).  The paths/targets that 'child_role' is allowed\n      to specify in its metadata depends on the delegating role, and thus is\n      left to the caller to verify.  We verify here that 'target_filepath'\n      is an allowed path according to the delegated 'child_role'.\n\n      TODO: Should the TUF spec restrict the repository to one particular\n      algorithm?  Should we allow the repository to specify in the role\n      dictionary the algorithm used for these generated hashed paths?\n\n    <Arguments>\n      child_role:\n        The delegation targets role object of 'child_role', containing its\n        paths, path_hash_prefixes, keys, and so on.\n\n      target_filepath:\n        The path to the target file on the repository. This will be relative to\n        the 'targets' (or equivalent) directory on a given mirror.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      None.\n\n    <Returns>\n      If 'child_role' has been delegated the target with the name\n      'target_filepath', then we return the role name of 'child_role'.\n\n      Otherwise, we return None.\n    \"\"\"\n\n    child_role_name = child_role['name']\n    child_role_paths = child_role.get('paths')\n    child_role_path_hash_prefixes = child_role.get('path_hash_prefixes')\n\n    if child_role_path_hash_prefixes is not None:\n      target_filepath_hash = self._get_target_hash(target_filepath)\n      for child_role_path_hash_prefix in child_role_path_hash_prefixes:\n        if target_filepath_hash.startswith(child_role_path_hash_prefix):\n          return child_role_name\n\n        else:\n          continue\n\n    elif child_role_paths is not None:\n      # Is 'child_role_name' allowed to sign for 'target_filepath'?\n      for child_role_path in child_role_paths:\n        # A child role path may be an explicit path or glob pattern (Unix\n        # shell-style wildcards).  The child role 'child_role_name' is returned\n        # if 'target_filepath' is equal to or matches 'child_role_path'.\n        # Explicit filepaths are also considered matches.  A repo maintainer\n        # might delegate a glob pattern with a leading path separator, while\n        # the client requests a matching target without a leading path\n        # separator - make sure to strip any leading path separators so that a\n        # match is made.  Example: \"foo.tgz\" should match with \"/*.tgz\".\n        if fnmatch.fnmatch(target_filepath.lstrip(os.sep), child_role_path.lstrip(os.sep)):\n          logger.debug('Child role ' + repr(child_role_name) + ' is allowed to'\n            ' sign for ' + repr(target_filepath))\n\n          return child_role_name\n\n        else:\n          logger.debug(\n              'The given target path ' + repr(target_filepath) + ' does not'\n              ' match the trusted path or glob pattern: ' + repr(child_role_path))\n          continue\n\n    else:\n      # 'role_name' should have been validated when it was downloaded.\n      # The 'paths' or 'path_hash_prefixes' fields should not be missing,\n      # so we raise a format error here in case they are both missing.\n      raise sslib_exceptions.FormatError(repr(child_role_name) + ' '\n          'has neither a \"paths\" nor \"path_hash_prefixes\".  At least'\n          ' one of these attributes must be present.')\n\n    return None\n\n\n\n  def _get_target_hash(self, target_filepath, hash_function='sha256'):\n    \"\"\"\n    <Purpose>\n      Non-public method that computes the hash of 'target_filepath'. This is\n      useful in conjunction with the \"path_hash_prefixes\" attribute in a\n      delegated targets role, which tells us which paths it is implicitly\n      responsible for.\n\n    <Arguments>\n      target_filepath:\n        The path to the target file on the repository. This will be relative to\n        the 'targets' (or equivalent) directory on a given mirror.\n\n      hash_function:\n        The algorithm used by the repository to generate the hashes of the\n        target filepaths.  The repository may optionally organize targets into\n        hashed bins to ease target delegations and role metadata management.\n        The use of consistent hashing allows for a uniform distribution of\n        targets into bins.\n\n    <Exceptions>\n      None.\n\n    <Side Effects>\n      None.\n\n    <Returns>\n      The hash of 'target_filepath'.\n    \"\"\"\n\n    # Calculate the hash of the filepath to determine which bin to find the\n    # target.  The client currently assumes the repository (i.e., repository\n    # tool) uses 'hash_function' to generate hashes and UTF-8.\n    digest_object = sslib_hash.digest(hash_function)\n    encoded_target_filepath = target_filepath.encode('utf-8')\n    digest_object.update(encoded_target_filepath)\n    target_filepath_hash = digest_object.hexdigest()\n\n    return target_filepath_hash\n\n\n\n\n\n  def remove_obsolete_targets(self, destination_directory):\n    \"\"\"\n    <Purpose>\n      Remove any files that are in 'previous' but not 'current'.  This makes it\n      so if you remove a file from a repository, it actually goes away.  The\n      targets for the 'targets' role and all delegated roles are checked.\n\n    <Arguments>\n      destination_directory:\n        The directory containing the target files tracked by TUF.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If 'destination_directory' is improperly formatted.\n\n      tuf.exceptions.RepositoryError:\n        If an error occurred removing any files.\n\n    <Side Effects>\n      Target files are removed from disk.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Does 'destination_directory' have the correct format?\n    # Raise 'securesystemslib.exceptions.FormatError' if there is a mismatch.\n    sslib_formats.PATH_SCHEMA.check_match(destination_directory)\n\n    # Iterate the rolenames and verify whether the 'previous' directory\n    # contains a target no longer found in 'current'.\n    for role in roledb.get_rolenames(self.repository_name):\n      if role.startswith('targets'):\n        if role in self.metadata['previous'] and self.metadata['previous'][role] != None:\n          for target in self.metadata['previous'][role]['targets']:\n            if target not in self.metadata['current'][role]['targets']:\n              # 'target' is only in 'previous', so remove it.\n              logger.warning('Removing obsolete file: ' + repr(target) + '.')\n\n              # Remove the file if it hasn't been removed already.\n              destination = \\\n                os.path.join(destination_directory, target.lstrip(os.sep))\n              try:\n                os.remove(destination)\n\n              except OSError as e:\n                # If 'filename' already removed, just log it.\n                if e.errno == errno.ENOENT:\n                  logger.info('File ' + repr(destination) + ' was already'\n                    ' removed.')\n\n                else:\n                  logger.warning('Failed to remove obsolete target: ' + str(e) )\n\n            else:\n              logger.debug('Skipping: ' + repr(target) + '.  It is still'\n                ' a current target.')\n        else:\n          logger.debug('Skipping: ' + repr(role) + '.  Not in the previous'\n            ' metadata')\n\n\n\n\n\n  def updated_targets(self, targets, destination_directory):\n    \"\"\"\n    <Purpose>\n      Checks files in the provided directory against the provided file metadata.\n\n      Filters the provided target info, returning a subset: only the metadata\n      for targets for which the target file either does not exist in the\n      provided directory, or for which the target file in the provided directory\n      does not match the provided metadata.\n\n      A principle use of this function is to determine which target files need\n      to be downloaded.  If the caller first uses get_one_valid_target_info()\n      calls to obtain up-to-date, valid metadata for targets, the caller can\n      then call updated_targets() to determine if that metadata does not match\n      what exists already on disk (in the provided directory).  The returned\n      values can then be used in download_file() calls to update the files that\n      didn't exist or didn't match.\n\n      The returned information is a list conformant to\n      'tuf.formats.TARGETINFOS_SCHEMA' and has the form:\n\n      [{'filepath': 'a/b/c.txt',\n        'fileinfo': {'length': 13323,\n                     'hashes': {'sha256': dbfac345..}}\n       ...]\n\n    <Arguments>\n      targets:\n        Metadata about the expected state of target files, against which local\n        files will be checked. This should be a list of target info\n        dictionaries; i.e. 'targets' must be conformant to\n        tuf.formats.TARGETINFOS_SCHEMA.\n\n      destination_directory:\n        The directory containing the target files.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If the arguments are improperly formatted.\n\n    <Side Effects>\n      The files in 'targets' are read and their hashes computed.\n\n    <Returns>\n      A list of target info dictionaries. The list conforms to\n      'tuf.formats.TARGETINFOS_SCHEMA'.\n      This is a strict subset of the argument 'targets'.\n    \"\"\"\n\n    # Do the arguments have the correct format?\n    # Raise 'securesystemslib.exceptions.FormatError' if there is a mismatch.\n    formats.TARGETINFOS_SCHEMA.check_match(targets)\n    sslib_formats.PATH_SCHEMA.check_match(destination_directory)\n\n    # Keep track of the target objects and filepaths of updated targets.\n    # Return 'updated_targets' and use 'updated_targetpaths' to avoid\n    # duplicates.\n    updated_targets = []\n    updated_targetpaths = []\n\n    for target in targets:\n      # Prepend 'destination_directory' to the target's relative filepath (as\n      # stored in metadata.)  Verify the hash of 'target_filepath' against\n      # each hash listed for its fileinfo.  Note: join() discards\n      # 'destination_directory' if 'filepath' contains a leading path separator\n      # (i.e., is treated as an absolute path).\n      filepath = target['filepath']\n      if filepath[0] == '/':\n        filepath = filepath[1:]\n      target_filepath = os.path.join(destination_directory, filepath)\n\n      if target_filepath in updated_targetpaths:\n        continue\n\n      # Try one of the algorithm/digest combos for a mismatch.  We break\n      # as soon as we find a mismatch.\n      for algorithm, digest in target['fileinfo']['hashes'].items():\n        digest_object = None\n        try:\n          digest_object = sslib_hash.digest_filename(target_filepath,\n            algorithm=algorithm)\n\n        # This exception would occur if the target does not exist locally.\n        except sslib_exceptions.StorageError:\n          updated_targets.append(target)\n          updated_targetpaths.append(target_filepath)\n          break\n\n        # The file does exist locally, check if its hash differs.\n        if digest_object.hexdigest() != digest:\n          updated_targets.append(target)\n          updated_targetpaths.append(target_filepath)\n          break\n\n    return updated_targets\n\n\n\n\n\n  def download_target(self, target, destination_directory,\n      prefix_filename_with_hash=True):\n    \"\"\"\n    <Purpose>\n      Download 'target' and verify it is trusted.\n\n      This will only store the file at 'destination_directory' if the\n      downloaded file matches the description of the file in the trusted\n      metadata.\n\n    <Arguments>\n      target:\n        The target to be downloaded.  Conformant to\n        'tuf.formats.TARGETINFO_SCHEMA'.\n\n      destination_directory:\n        The directory to save the downloaded target file.\n\n      prefix_filename_with_hash:\n        Whether to prefix the targets file names with their hash when using\n        consistent snapshot.\n        This should be set to False when the served target filenames are not\n        prefixed with hashes (in this case the server uses other means\n        to ensure snapshot consistency).\n        Default is True.\n\n    <Exceptions>\n      securesystemslib.exceptions.FormatError:\n        If 'target' is not properly formatted.\n\n      tuf.exceptions.NoWorkingMirrorError:\n        If a target could not be downloaded from any of the mirrors.\n\n        Although expected to be rare, there might be OSError exceptions (except\n        errno.EEXIST) raised when creating the destination directory (if it\n        doesn't exist).\n\n    <Side Effects>\n      A target file is saved to the local system.\n\n    <Returns>\n      None.\n    \"\"\"\n\n    # Do the arguments have the correct format?\n    # This check ensures the arguments have the appropriate\n    # number of objects and object types, and that all dict\n    # keys are properly named.\n    # Raise 'securesystemslib.exceptions.FormatError' if the check fail.\n    formats.TARGETINFO_SCHEMA.check_match(target)\n    sslib_formats.PATH_SCHEMA.check_match(destination_directory)\n\n    # Extract the target file information.\n    target_filepath = target['filepath']\n    trusted_length = target['fileinfo']['length']\n    trusted_hashes = target['fileinfo']['hashes']\n\n    # Build absolute 'destination' file path.\n    # Note: join() discards 'destination_directory' if 'target_path' contains\n    # a leading path separator (i.e., is treated as an absolute path).\n    destination = os.path.join(destination_directory,\n        target_filepath.lstrip(os.sep))\n    destination = os.path.abspath(destination)\n    target_dirpath = os.path.dirname(destination)\n\n    # When attempting to create the leaf directory of 'target_dirpath', ignore\n    # any exceptions raised if the root directory already exists.  All other\n    # exceptions potentially thrown by os.makedirs() are re-raised.\n    # Note: os.makedirs can raise OSError if the leaf directory already exists\n    # or cannot be created.\n    try:\n      os.makedirs(target_dirpath)\n\n    except OSError as e:\n      if e.errno == errno.EEXIST:\n        pass\n\n      else:\n        raise\n\n    # '_get_target_file()' checks every mirror and returns the first target\n    # that passes verification.\n    target_file_object = self._get_target_file(target_filepath, trusted_length,\n        trusted_hashes, prefix_filename_with_hash)\n\n    sslib_util.persist_temp_file(target_file_object, destination)\n", "patch": "@@ -122,6 +122,7 @@\n import copy\n import warnings\n import io\n+from urllib import parse\n \n from securesystemslib import exceptions as sslib_exceptions\n from securesystemslib import formats as sslib_formats\n@@ -781,7 +782,13 @@ def __str__(self):\n     return self.repository_name\n \n \n+  @staticmethod\n+  def _get_local_filename(rolename: str) -> str:\n+    \"\"\"Return safe local filename for roles metadata\n \n+    Use URL encoding to prevent issues with path separators and\n+    with forbidden characters in Windows filesystems\"\"\"\n+    return parse.quote(rolename, '') + '.json'\n \n \n   def _load_metadata_from_file(self, metadata_set, metadata_role):\n@@ -827,7 +834,7 @@ def _load_metadata_from_file(self, metadata_set, metadata_role):\n \n     # Save and construct the full metadata path.\n     metadata_directory = self.metadata_directory[metadata_set]\n-    metadata_filename = metadata_role + '.json'\n+    metadata_filename = self._get_local_filename(metadata_role)\n     metadata_filepath = os.path.join(metadata_directory, metadata_filename)\n \n     # Ensure the metadata path is valid/exists, else ignore the call.\n@@ -1656,10 +1663,6 @@ def _update_metadata(self, metadata_role, upperbound_filelength, version=None):\n       None.\n     \"\"\"\n \n-    # Construct the metadata filename as expected by the download/mirror\n-    # modules.\n-    metadata_filename = metadata_role + '.json'\n-\n     # Attempt a file download from each mirror until the file is downloaded and\n     # verified.  If the signature of the downloaded file is valid, proceed,\n     # otherwise log a warning and try the next mirror.  'metadata_file_object'\n@@ -1676,7 +1679,11 @@ def _update_metadata(self, metadata_role, upperbound_filelength, version=None):\n     # best length we can get for it, not request a specific version, but\n     # perform the rest of the checks (e.g., signature verification).\n \n-    remote_filename = metadata_filename\n+    # Construct the metadata filename as expected by the download/mirror\n+    # modules. Local filename is quoted to protect against names like\"../file\".\n+\n+    remote_filename = metadata_role + '.json'\n+    local_filename = self._get_local_filename(metadata_role)\n     filename_version = ''\n \n     if self.consistent_snapshot and version:\n@@ -1693,12 +1700,12 @@ def _update_metadata(self, metadata_role, upperbound_filelength, version=None):\n     # First, move the 'current' metadata file to the 'previous' directory\n     # if it exists.\n     current_filepath = os.path.join(self.metadata_directory['current'],\n-                metadata_filename)\n+                local_filename)\n     current_filepath = os.path.abspath(current_filepath)\n     sslib_util.ensure_parent_dir(current_filepath)\n \n     previous_filepath = os.path.join(self.metadata_directory['previous'],\n-        metadata_filename)\n+        local_filename)\n     previous_filepath = os.path.abspath(previous_filepath)\n \n     if os.path.exists(current_filepath):\n@@ -1726,7 +1733,7 @@ def _update_metadata(self, metadata_role, upperbound_filelength, version=None):\n     logger.debug('Updated ' + repr(current_filepath) + '.')\n     self.metadata['previous'][metadata_role] = current_metadata_object\n     self.metadata['current'][metadata_role] = updated_metadata_object\n-    self._update_versioninfo(metadata_filename)\n+    self._update_versioninfo(remote_filename)\n \n \n \n@@ -1973,9 +1980,11 @@ def _update_versioninfo(self, metadata_filename):\n     # __init__ (such as with delegated metadata), then get the version\n     # info now.\n \n-    # Save the path to the current metadata file for 'metadata_filename'.\n+    # 'metadata_filename' is the key from meta dictionary: build the\n+    # corresponding local filepath like _get_local_filename()\n+    local_filename = parse.quote(metadata_filename, \"\")\n     current_filepath = os.path.join(self.metadata_directory['current'],\n-        metadata_filename)\n+        local_filename)\n \n     # If the path is invalid, simply return and leave versioninfo unset.\n     if not os.path.exists(current_filepath):\n@@ -2028,130 +2037,6 @@ def _update_versioninfo(self, metadata_filename):\n \n \n \n-\n-  def _fileinfo_has_changed(self, metadata_filename, new_fileinfo):\n-    \"\"\"\n-    <Purpose>\n-      Non-public method that determines whether the current fileinfo of\n-      'metadata_filename' differs from 'new_fileinfo'.  The 'new_fileinfo'\n-      argument should be extracted from the latest copy of the metadata that\n-      references 'metadata_filename'.  Example: 'root.json' would be referenced\n-      by 'snapshot.json'.\n-\n-      'new_fileinfo' should only be 'None' if this is for updating 'root.json'\n-      without having 'snapshot.json' available.\n-\n-    <Arguments>\n-      metadadata_filename:\n-        The metadata filename for the role.  For the 'root' role,\n-        'metadata_filename' would be 'root.json'.\n-\n-      new_fileinfo:\n-        A dict object representing the new file information for\n-        'metadata_filename'.  'new_fileinfo' may be 'None' when\n-        updating 'root' without having 'snapshot' available.  This\n-        dict conforms to 'tuf.formats.TARGETS_FILEINFO_SCHEMA' and has\n-        the form:\n-\n-        {'length': 23423\n-         'hashes': {'sha256': adfbc32343..}}\n-\n-    <Exceptions>\n-      None.\n-\n-    <Side Effects>\n-      If there is no fileinfo currently loaded for 'metada_filename',\n-      try to load it.\n-\n-    <Returns>\n-      Boolean.  True if the fileinfo has changed, false otherwise.\n-    \"\"\"\n-\n-    # If there is no fileinfo currently stored for 'metadata_filename',\n-    # try to load the file, calculate the fileinfo, and store it.\n-    if metadata_filename not in self.fileinfo:\n-      self._update_fileinfo(metadata_filename)\n-\n-    # Return true if there is no fileinfo for 'metadata_filename'.\n-    # 'metadata_filename' is not in the 'self.fileinfo' store\n-    # and it doesn't exist in the 'current' metadata location.\n-    if self.fileinfo[metadata_filename] is None:\n-      return True\n-\n-    current_fileinfo = self.fileinfo[metadata_filename]\n-\n-    if current_fileinfo['length'] != new_fileinfo['length']:\n-      return True\n-\n-    # Now compare hashes. Note that the reason we can't just do a simple\n-    # equality check on the fileinfo dicts is that we want to support the\n-    # case where the hash algorithms listed in the metadata have changed\n-    # without having that result in considering all files as needing to be\n-    # updated, or not all hash algorithms listed can be calculated on the\n-    # specific client.\n-    for algorithm, hash_value in new_fileinfo['hashes'].items():\n-      # We're only looking for a single match. This isn't a security\n-      # check, we just want to prevent unnecessary downloads.\n-      if algorithm in current_fileinfo['hashes']:\n-        if hash_value == current_fileinfo['hashes'][algorithm]:\n-          return False\n-\n-    return True\n-\n-\n-\n-\n-\n-  def _update_fileinfo(self, metadata_filename):\n-    \"\"\"\n-    <Purpose>\n-      Non-public method that updates the 'self.fileinfo' entry for the metadata\n-      belonging to 'metadata_filename'.  If the 'current' metadata for\n-      'metadata_filename' cannot be loaded, set its fileinfo' to 'None' to\n-      signal that it is not in the 'self.fileinfo' AND it also doesn't exist\n-      locally.\n-\n-    <Arguments>\n-      metadata_filename:\n-        The metadata filename for the role.  For the 'root' role,\n-        'metadata_filename' would be 'root.json'.\n-\n-    <Exceptions>\n-      None.\n-\n-    <Side Effects>\n-      The file details of 'metadata_filename' is calculated and\n-      stored in 'self.fileinfo'.\n-\n-    <Returns>\n-      None.\n-    \"\"\"\n-\n-    # In case we delayed loading the metadata and didn't do it in\n-    # __init__ (such as with delegated metadata), then get the file\n-    # info now.\n-\n-    # Save the path to the current metadata file for 'metadata_filename'.\n-    current_filepath = os.path.join(self.metadata_directory['current'],\n-        metadata_filename)\n-\n-    # If the path is invalid, simply return and leave fileinfo unset.\n-    if not os.path.exists(current_filepath):\n-      self.fileinfo[metadata_filename] = None\n-      return\n-\n-    # Extract the file information from the actual file and save it\n-    # to the fileinfo store.\n-    file_length, hashes = sslib_util.get_file_details(current_filepath)\n-    metadata_fileinfo = formats.make_targets_fileinfo(file_length, hashes)\n-    self.fileinfo[metadata_filename] = metadata_fileinfo\n-\n-\n-\n-\n-\n-\n-\n   def _move_current_to_previous(self, metadata_role):\n     \"\"\"\n     <Purpose>\n@@ -2175,7 +2060,7 @@ def _move_current_to_previous(self, metadata_role):\n     \"\"\"\n \n     # Get the 'current' and 'previous' full file paths for 'metadata_role'\n-    metadata_filepath = metadata_role + '.json'\n+    metadata_filepath = self._get_local_filename(metadata_role)\n     previous_filepath = os.path.join(self.metadata_directory['previous'],\n                                      metadata_filepath)\n     current_filepath = os.path.join(self.metadata_directory['current'],", "file_path": "files/2021_10/354", "file_language": "py", "file_name": "tuf/client/updater.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/theupdateframework/python-tuf/raw/4ad7ae48fda594b640139c3b7eae21ed5155a102/tuf%2Fngclient%2Fupdater.py", "code": "# Copyright 2020, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"Client update workflow implementation\n\nThe Updater class provides an implementation of the\n`TUF client workflow\n<https://theupdateframework.github.io/specification/latest/#detailed-client-workflow>`_.\nUpdater provides an API to query available targets and to download them in a\nsecure manner: All downloaded files are verified by signed metadata.\n\nHigh-level description of Updater functionality:\n  * Initializing an :class:`~tuf.ngclient.updater.Updater` loads and validates\n    the trusted local root metadata: This root metadata is used as the source\n    of trust for all other metadata.\n  * Calling :func:`~tuf.ngclient.updater.Updater.refresh()` will update root\n    metadata and load all other top-level metadata as described in the\n    specification, using both locally cached metadata and metadata downloaded\n    from the remote repository.\n  * When metadata is up-to-date, targets can be dowloaded. The repository\n    snapshot is consistent so multiple targets can be downloaded without\n    fear of repository content changing. For each target:\n\n      * :func:`~tuf.ngclient.updater.Updater.get_one_valid_targetinfo()` is\n        used to find information about a specific target. This will load new\n        targets metadata as needed (from local cache or remote repository).\n      * :func:`~tuf.ngclient.updater.Updater.updated_targets()` can be used to\n        check if target files are already locally cached.\n      * :func:`~tuf.ngclient.updater.Updater.download_target()` downloads a\n        target file and ensures it is verified correct by the metadata.\n\nBelow is a simple example of using the Updater to download and verify\n\"file.txt\" from a remote repository. The required environment for this example\nis:\n\n    * A webserver running on http://localhost:8000, serving TUF repository\n      metadata at \"/tuf-repo/\" and targets at \"/targets/\"\n    * Local metadata directory \"~/tufclient/metadata/\" is writable and contains\n      a root metadata version for the remote repository\n    * Download directory \"~/tufclient/downloads/\" is writable\n\nExample::\n\n    from tuf.ngclient import Updater\n\n    # Load trusted local root metadata from client metadata cache. Define the\n    # remote repository metadata URL prefix and target URL prefix.\n    updater = Updater(\n        repository_dir=\"~/tufclient/metadata/\",\n        metadata_base_url=\"http://localhost:8000/tuf-repo/\",\n        target_base_url=\"http://localhost:8000/targets/\",\n    )\n\n    # Update top-level metadata from remote\n    updater.refresh()\n\n    # Securely download a target:\n    # Update target metadata, then download and verify target\n    targetinfo = updater.get_one_valid_targetinfo(\"file.txt\")\n    updater.download_target(targetinfo, \"~/tufclient/downloads/\")\n\"\"\"\n\nimport logging\nimport os\nimport tempfile\nfrom typing import List, Optional, Set, Tuple\nfrom urllib import parse\n\nfrom securesystemslib import util as sslib_util\n\nfrom tuf import exceptions\nfrom tuf.api.metadata import TargetFile, Targets\nfrom tuf.ngclient._internal import requests_fetcher, trusted_metadata_set\nfrom tuf.ngclient.config import UpdaterConfig\nfrom tuf.ngclient.fetcher import FetcherInterface\n\nlogger = logging.getLogger(__name__)\n\n\nclass Updater:\n    \"\"\"Creates a new Updater instance and loads trusted root metadata.\n\n    Args:\n        repository_dir: Local metadata directory. Directory must be\n            writable and it must contain a trusted root.json file.\n        metadata_base_url: Base URL for all remote metadata downloads\n        target_base_url: Optional; Default base URL for all remote target\n            downloads. Can be individually set in download_target()\n        fetcher: Optional; FetcherInterface implementation used to download\n            both metadata and targets. Default is RequestsFetcher\n\n    Raises:\n        OSError: Local root.json cannot be read\n        RepositoryError: Local root.json is invalid\n    \"\"\"\n\n    def __init__(\n        self,\n        repository_dir: str,\n        metadata_base_url: str,\n        target_base_url: Optional[str] = None,\n        fetcher: Optional[FetcherInterface] = None,\n        config: Optional[UpdaterConfig] = None,\n    ):\n        self._dir = repository_dir\n        self._metadata_base_url = _ensure_trailing_slash(metadata_base_url)\n        if target_base_url is None:\n            self._target_base_url = None\n        else:\n            self._target_base_url = _ensure_trailing_slash(target_base_url)\n\n        # Read trusted local root metadata\n        data = self._load_local_metadata(\"root\")\n        self._trusted_set = trusted_metadata_set.TrustedMetadataSet(data)\n        self._fetcher = fetcher or requests_fetcher.RequestsFetcher()\n        self.config = config or UpdaterConfig()\n\n    def refresh(self) -> None:\n        \"\"\"Refreshes top-level metadata.\n\n        Downloads, verifies, and loads metadata for the top-level roles in the\n        specified order (root -> timestamp -> snapshot -> targets) implementing\n        all the checks required in the TUF client workflow.\n\n        The metadata for delegated roles are not refreshed by this method as\n        that happens on demand during get_one_valid_targetinfo().\n\n        The refresh() method should be called by the client before any other\n        method calls.\n\n        Raises:\n            OSError: New metadata could not be written to disk\n            RepositoryError: Metadata failed to verify in some way\n            TODO: download-related errors\n        \"\"\"\n\n        self._load_root()\n        self._load_timestamp()\n        self._load_snapshot()\n        self._load_targets(\"targets\", \"root\")\n\n    def get_one_valid_targetinfo(\n        self, target_path: str\n    ) -> Optional[TargetFile]:\n        \"\"\"Returns TargetFile instance with information for 'target_path'.\n\n        The return value can be used as an argument to\n        :func:`download_target()` and :func:`updated_targets()`.\n\n        :func:`refresh()` must be called before calling\n        `get_one_valid_targetinfo()`. Subsequent calls to\n        `get_one_valid_targetinfo()` will use the same consistent repository\n        state: Changes that happen in the repository between calling\n        :func:`refresh()` and `get_one_valid_targetinfo()` will not be\n        seen by the updater.\n\n        As a side-effect this method downloads all the additional (delegated\n        targets) metadata it needs to return the target information.\n\n        Args:\n            target_path: A target identifier that is a path-relative-URL string\n                (https://url.spec.whatwg.org/#path-relative-url-string).\n                Typically this is also the unix file path of the eventually\n                downloaded file.\n\n        Raises:\n            OSError: New metadata could not be written to disk\n            RepositoryError: Metadata failed to verify in some way\n            TODO: download-related errors\n\n        Returns:\n            A TargetFile instance or None.\n        \"\"\"\n        return self._preorder_depth_first_walk(target_path)\n\n    @staticmethod\n    def updated_targets(\n        targets: List[TargetFile], destination_directory: str\n    ) -> List[TargetFile]:\n        \"\"\"Checks whether local cached target files are up to date\n\n        After retrieving the target information for the targets that should be\n        updated, updated_targets() can be called to determine which targets\n        have changed compared to locally stored versions.\n\n        All the targets that are not up-to-date in destination_directory are\n        returned in a list. The list items can be downloaded with\n        'download_target()'.\n        \"\"\"\n        # Keep track of TargetFiles and local paths. Return 'updated_targets'\n        # and use 'local_paths' to avoid duplicates.\n        updated_targets: List[TargetFile] = []\n        local_paths: List[str] = []\n\n        for target in targets:\n            # URL encode to get local filename like download_target() does\n            filename = parse.quote(target.path, \"\")\n            local_path = os.path.join(destination_directory, filename)\n\n            if local_path in local_paths:\n                continue\n\n            try:\n                with open(local_path, \"rb\") as target_file:\n                    target.verify_length_and_hashes(target_file)\n            # If the file does not exist locally or length and hashes\n            # do not match, append to updated targets.\n            except (OSError, exceptions.LengthOrHashMismatchError):\n                updated_targets.append(target)\n                local_paths.append(local_path)\n\n        return updated_targets\n\n    def download_target(\n        self,\n        targetinfo: TargetFile,\n        destination_directory: str,\n        target_base_url: Optional[str] = None,\n    ) -> str:\n        \"\"\"Downloads the target file specified by 'targetinfo'.\n\n        Args:\n            targetinfo: TargetFile instance received from\n                get_one_valid_targetinfo() or updated_targets().\n            destination_directory: existing local directory to download into.\n                Note that new directories may be created inside\n                destination_directory as required.\n            target_base_url: Optional; Base URL used to form the final target\n                download URL. Default is the value provided in Updater()\n\n        Raises:\n            ValueError: Invalid arguments\n            TODO: download-related errors\n            TODO: file write errors\n\n        Returns:\n            Path to downloaded file\n        \"\"\"\n\n        if target_base_url is None:\n            if self._target_base_url is None:\n                raise ValueError(\n                    \"target_base_url must be set in either \"\n                    \"download_target() or constructor\"\n                )\n\n            target_base_url = self._target_base_url\n        else:\n            target_base_url = _ensure_trailing_slash(target_base_url)\n\n        target_filepath = targetinfo.path\n        consistent_snapshot = self._trusted_set.root.signed.consistent_snapshot\n        if consistent_snapshot and self.config.prefix_targets_with_hash:\n            hashes = list(targetinfo.hashes.values())\n            dirname, sep, basename = target_filepath.rpartition(\"/\")\n            target_filepath = f\"{dirname}{sep}{hashes[0]}.{basename}\"\n        full_url = f\"{target_base_url}{target_filepath}\"\n\n        with self._fetcher.download_file(\n            full_url, targetinfo.length\n        ) as target_file:\n            try:\n                targetinfo.verify_length_and_hashes(target_file)\n            except exceptions.LengthOrHashMismatchError as e:\n                raise exceptions.RepositoryError(\n                    f\"{target_filepath} length or hashes do not match\"\n                ) from e\n\n            # Use a URL encoded targetpath as the local filename\n            filename = parse.quote(targetinfo.path, \"\")\n            local_filepath = os.path.join(destination_directory, filename)\n            sslib_util.persist_temp_file(target_file, local_filepath)\n\n            return local_filepath\n\n    def _download_metadata(\n        self, rolename: str, length: int, version: Optional[int] = None\n    ) -> bytes:\n        \"\"\"Download a metadata file and return it as bytes\"\"\"\n        if version is None:\n            url = f\"{self._metadata_base_url}{rolename}.json\"\n        else:\n            url = f\"{self._metadata_base_url}{version}.{rolename}.json\"\n        return self._fetcher.download_bytes(url, length)\n\n    def _load_local_metadata(self, rolename: str) -> bytes:\n        encoded_name = parse.quote(rolename, \"\")\n        with open(os.path.join(self._dir, f\"{encoded_name}.json\"), \"rb\") as f:\n            return f.read()\n\n    def _persist_metadata(self, rolename: str, data: bytes) -> None:\n        \"\"\"Write metadata to disk atomically to avoid data loss.\"\"\"\n\n        # encode the rolename to avoid issues with e.g. path separators\n        encoded_name = parse.quote(rolename, \"\")\n        filename = os.path.join(self._dir, f\"{encoded_name}.json\")\n        with tempfile.NamedTemporaryFile(\n            dir=self._dir, delete=False\n        ) as temp_file:\n            temp_file.write(data)\n        os.replace(temp_file.name, filename)\n\n    def _load_root(self) -> None:\n        \"\"\"Load remote root metadata.\n\n        Sequentially load and persist on local disk every newer root metadata\n        version available on the remote.\n        \"\"\"\n\n        # Update the root role\n        lower_bound = self._trusted_set.root.signed.version + 1\n        upper_bound = lower_bound + self.config.max_root_rotations\n\n        for next_version in range(lower_bound, upper_bound):\n            try:\n                data = self._download_metadata(\n                    \"root\", self.config.root_max_length, next_version\n                )\n                self._trusted_set.update_root(data)\n                self._persist_metadata(\"root\", data)\n\n            except exceptions.FetcherHTTPError as exception:\n                if exception.status_code not in {403, 404}:\n                    raise\n                # 404/403 means current root is newest available\n                break\n\n    def _load_timestamp(self) -> None:\n        \"\"\"Load local and remote timestamp metadata\"\"\"\n        try:\n            data = self._load_local_metadata(\"timestamp\")\n            self._trusted_set.update_timestamp(data)\n        except (OSError, exceptions.RepositoryError) as e:\n            # Local timestamp does not exist or is invalid\n            logger.debug(\"Local timestamp not valid as final: %s\", e)\n\n        # Load from remote (whether local load succeeded or not)\n        data = self._download_metadata(\n            \"timestamp\", self.config.timestamp_max_length\n        )\n        self._trusted_set.update_timestamp(data)\n        self._persist_metadata(\"timestamp\", data)\n\n    def _load_snapshot(self) -> None:\n        \"\"\"Load local (and if needed remote) snapshot metadata\"\"\"\n        try:\n            data = self._load_local_metadata(\"snapshot\")\n            self._trusted_set.update_snapshot(data, trusted=True)\n            logger.debug(\"Local snapshot is valid: not downloading new one\")\n        except (OSError, exceptions.RepositoryError) as e:\n            # Local snapshot does not exist or is invalid: update from remote\n            logger.debug(\"Local snapshot not valid as final: %s\", e)\n\n            assert self._trusted_set.timestamp is not None  # nosec\n            snapshot_meta = self._trusted_set.timestamp.signed.snapshot_meta\n            length = snapshot_meta.length or self.config.snapshot_max_length\n            version = None\n            if self._trusted_set.root.signed.consistent_snapshot:\n                version = snapshot_meta.version\n\n            data = self._download_metadata(\"snapshot\", length, version)\n            self._trusted_set.update_snapshot(data)\n            self._persist_metadata(\"snapshot\", data)\n\n    def _load_targets(self, role: str, parent_role: str) -> None:\n        \"\"\"Load local (and if needed remote) metadata for 'role'.\"\"\"\n        try:\n            data = self._load_local_metadata(role)\n            self._trusted_set.update_delegated_targets(data, role, parent_role)\n            logger.debug(\"Local %s is valid: not downloading new one\", role)\n        except (OSError, exceptions.RepositoryError) as e:\n            # Local 'role' does not exist or is invalid: update from remote\n            logger.debug(\"Failed to load local %s: %s\", role, e)\n\n            assert self._trusted_set.snapshot is not None  # nosec\n            metainfo = self._trusted_set.snapshot.signed.meta[f\"{role}.json\"]\n            length = metainfo.length or self.config.targets_max_length\n            version = None\n            if self._trusted_set.root.signed.consistent_snapshot:\n                version = metainfo.version\n\n            data = self._download_metadata(role, length, version)\n            self._trusted_set.update_delegated_targets(data, role, parent_role)\n            self._persist_metadata(role, data)\n\n    def _preorder_depth_first_walk(\n        self, target_filepath: str\n    ) -> Optional[TargetFile]:\n        \"\"\"\n        Interrogates the tree of target delegations in order of appearance\n        (which implicitly order trustworthiness), and returns the matching\n        target found in the most trusted role.\n        \"\"\"\n\n        # List of delegations to be interrogated. A (role, parent role) pair\n        # is needed to load and verify the delegated targets metadata.\n        delegations_to_visit = [(\"targets\", \"root\")]\n        visited_role_names: Set[Tuple[str, str]] = set()\n        number_of_delegations = self.config.max_delegations\n\n        # Preorder depth-first traversal of the graph of target delegations.\n        while number_of_delegations > 0 and len(delegations_to_visit) > 0:\n\n            # Pop the role name from the top of the stack.\n            role_name, parent_role = delegations_to_visit.pop(-1)\n\n            # Skip any visited current role to prevent cycles.\n            if (role_name, parent_role) in visited_role_names:\n                logger.debug(\"Skipping visited current role %s\", role_name)\n                continue\n\n            # The metadata for 'role_name' must be downloaded/updated before\n            # its targets, delegations, and child roles can be inspected.\n            self._load_targets(role_name, parent_role)\n\n            role_metadata: Targets = self._trusted_set[role_name].signed\n            target = role_metadata.targets.get(target_filepath)\n\n            if target is not None:\n                logger.debug(\"Found target in current role %s\", role_name)\n                return target\n\n            # After preorder check, add current role to set of visited roles.\n            visited_role_names.add((role_name, parent_role))\n\n            # And also decrement number of visited roles.\n            number_of_delegations -= 1\n\n            if role_metadata.delegations is not None:\n                child_roles_to_visit = []\n                # NOTE: This may be a slow operation if there are many\n                # delegated roles.\n                for child_role in role_metadata.delegations.roles.values():\n                    if child_role.is_delegated_path(target_filepath):\n                        logger.debug(\"Adding child role %s\", child_role.name)\n\n                        child_roles_to_visit.append(\n                            (child_role.name, role_name)\n                        )\n                        if child_role.terminating:\n                            logger.debug(\"Not backtracking to other roles.\")\n                            delegations_to_visit = []\n                            break\n                # Push 'child_roles_to_visit' in reverse order of appearance\n                # onto 'delegations_to_visit'.  Roles are popped from the end of\n                # the list.\n                child_roles_to_visit.reverse()\n                delegations_to_visit.extend(child_roles_to_visit)\n\n        if number_of_delegations == 0 and len(delegations_to_visit) > 0:\n            logger.debug(\n                \"%d roles left to visit, but allowed to \"\n                \"visit at most %d delegations.\",\n                len(delegations_to_visit),\n                self.config.max_delegations,\n            )\n\n        # If this point is reached then target is not found, return None\n        return None\n\n\ndef _ensure_trailing_slash(url: str) -> str:\n    \"\"\"Return url guaranteed to end in a slash\"\"\"\n    return url if url.endswith(\"/\") else f\"{url}/\"\n", "code_before": "# Copyright 2020, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"Client update workflow implementation\n\nThe Updater class provides an implementation of the\n`TUF client workflow\n<https://theupdateframework.github.io/specification/latest/#detailed-client-workflow>`_.\nUpdater provides an API to query available targets and to download them in a\nsecure manner: All downloaded files are verified by signed metadata.\n\nHigh-level description of Updater functionality:\n  * Initializing an :class:`~tuf.ngclient.updater.Updater` loads and validates\n    the trusted local root metadata: This root metadata is used as the source\n    of trust for all other metadata.\n  * Calling :func:`~tuf.ngclient.updater.Updater.refresh()` will update root\n    metadata and load all other top-level metadata as described in the\n    specification, using both locally cached metadata and metadata downloaded\n    from the remote repository.\n  * When metadata is up-to-date, targets can be dowloaded. The repository\n    snapshot is consistent so multiple targets can be downloaded without\n    fear of repository content changing. For each target:\n\n      * :func:`~tuf.ngclient.updater.Updater.get_one_valid_targetinfo()` is\n        used to find information about a specific target. This will load new\n        targets metadata as needed (from local cache or remote repository).\n      * :func:`~tuf.ngclient.updater.Updater.updated_targets()` can be used to\n        check if target files are already locally cached.\n      * :func:`~tuf.ngclient.updater.Updater.download_target()` downloads a\n        target file and ensures it is verified correct by the metadata.\n\nBelow is a simple example of using the Updater to download and verify\n\"file.txt\" from a remote repository. The required environment for this example\nis:\n\n    * A webserver running on http://localhost:8000, serving TUF repository\n      metadata at \"/tuf-repo/\" and targets at \"/targets/\"\n    * Local metadata directory \"~/tufclient/metadata/\" is writable and contains\n      a root metadata version for the remote repository\n    * Download directory \"~/tufclient/downloads/\" is writable\n\nExample::\n\n    from tuf.ngclient import Updater\n\n    # Load trusted local root metadata from client metadata cache. Define the\n    # remote repository metadata URL prefix and target URL prefix.\n    updater = Updater(\n        repository_dir=\"~/tufclient/metadata/\",\n        metadata_base_url=\"http://localhost:8000/tuf-repo/\",\n        target_base_url=\"http://localhost:8000/targets/\",\n    )\n\n    # Update top-level metadata from remote\n    updater.refresh()\n\n    # Securely download a target:\n    # Update target metadata, then download and verify target\n    targetinfo = updater.get_one_valid_targetinfo(\"file.txt\")\n    updater.download_target(targetinfo, \"~/tufclient/downloads/\")\n\"\"\"\n\nimport logging\nimport os\nimport tempfile\nfrom typing import List, Optional, Set, Tuple\nfrom urllib import parse\n\nfrom securesystemslib import util as sslib_util\n\nfrom tuf import exceptions\nfrom tuf.api.metadata import TargetFile, Targets\nfrom tuf.ngclient._internal import requests_fetcher, trusted_metadata_set\nfrom tuf.ngclient.config import UpdaterConfig\nfrom tuf.ngclient.fetcher import FetcherInterface\n\nlogger = logging.getLogger(__name__)\n\n\nclass Updater:\n    \"\"\"Creates a new Updater instance and loads trusted root metadata.\n\n    Args:\n        repository_dir: Local metadata directory. Directory must be\n            writable and it must contain a trusted root.json file.\n        metadata_base_url: Base URL for all remote metadata downloads\n        target_base_url: Optional; Default base URL for all remote target\n            downloads. Can be individually set in download_target()\n        fetcher: Optional; FetcherInterface implementation used to download\n            both metadata and targets. Default is RequestsFetcher\n\n    Raises:\n        OSError: Local root.json cannot be read\n        RepositoryError: Local root.json is invalid\n    \"\"\"\n\n    def __init__(\n        self,\n        repository_dir: str,\n        metadata_base_url: str,\n        target_base_url: Optional[str] = None,\n        fetcher: Optional[FetcherInterface] = None,\n        config: Optional[UpdaterConfig] = None,\n    ):\n        self._dir = repository_dir\n        self._metadata_base_url = _ensure_trailing_slash(metadata_base_url)\n        if target_base_url is None:\n            self._target_base_url = None\n        else:\n            self._target_base_url = _ensure_trailing_slash(target_base_url)\n\n        # Read trusted local root metadata\n        data = self._load_local_metadata(\"root\")\n        self._trusted_set = trusted_metadata_set.TrustedMetadataSet(data)\n        self._fetcher = fetcher or requests_fetcher.RequestsFetcher()\n        self.config = config or UpdaterConfig()\n\n    def refresh(self) -> None:\n        \"\"\"Refreshes top-level metadata.\n\n        Downloads, verifies, and loads metadata for the top-level roles in the\n        specified order (root -> timestamp -> snapshot -> targets) implementing\n        all the checks required in the TUF client workflow.\n\n        The metadata for delegated roles are not refreshed by this method as\n        that happens on demand during get_one_valid_targetinfo().\n\n        The refresh() method should be called by the client before any other\n        method calls.\n\n        Raises:\n            OSError: New metadata could not be written to disk\n            RepositoryError: Metadata failed to verify in some way\n            TODO: download-related errors\n        \"\"\"\n\n        self._load_root()\n        self._load_timestamp()\n        self._load_snapshot()\n        self._load_targets(\"targets\", \"root\")\n\n    def get_one_valid_targetinfo(\n        self, target_path: str\n    ) -> Optional[TargetFile]:\n        \"\"\"Returns TargetFile instance with information for 'target_path'.\n\n        The return value can be used as an argument to\n        :func:`download_target()` and :func:`updated_targets()`.\n\n        :func:`refresh()` must be called before calling\n        `get_one_valid_targetinfo()`. Subsequent calls to\n        `get_one_valid_targetinfo()` will use the same consistent repository\n        state: Changes that happen in the repository between calling\n        :func:`refresh()` and `get_one_valid_targetinfo()` will not be\n        seen by the updater.\n\n        As a side-effect this method downloads all the additional (delegated\n        targets) metadata it needs to return the target information.\n\n        Args:\n            target_path: A target identifier that is a path-relative-URL string\n                (https://url.spec.whatwg.org/#path-relative-url-string).\n                Typically this is also the unix file path of the eventually\n                downloaded file.\n\n        Raises:\n            OSError: New metadata could not be written to disk\n            RepositoryError: Metadata failed to verify in some way\n            TODO: download-related errors\n\n        Returns:\n            A TargetFile instance or None.\n        \"\"\"\n        return self._preorder_depth_first_walk(target_path)\n\n    @staticmethod\n    def updated_targets(\n        targets: List[TargetFile], destination_directory: str\n    ) -> List[TargetFile]:\n        \"\"\"Checks whether local cached target files are up to date\n\n        After retrieving the target information for the targets that should be\n        updated, updated_targets() can be called to determine which targets\n        have changed compared to locally stored versions.\n\n        All the targets that are not up-to-date in destination_directory are\n        returned in a list. The list items can be downloaded with\n        'download_target()'.\n        \"\"\"\n        # Keep track of TargetFiles and local paths. Return 'updated_targets'\n        # and use 'local_paths' to avoid duplicates.\n        updated_targets: List[TargetFile] = []\n        local_paths: List[str] = []\n\n        for target in targets:\n            # URL encode to get local filename like download_target() does\n            filename = parse.quote(target.path, \"\")\n            local_path = os.path.join(destination_directory, filename)\n\n            if local_path in local_paths:\n                continue\n\n            try:\n                with open(local_path, \"rb\") as target_file:\n                    target.verify_length_and_hashes(target_file)\n            # If the file does not exist locally or length and hashes\n            # do not match, append to updated targets.\n            except (OSError, exceptions.LengthOrHashMismatchError):\n                updated_targets.append(target)\n                local_paths.append(local_path)\n\n        return updated_targets\n\n    def download_target(\n        self,\n        targetinfo: TargetFile,\n        destination_directory: str,\n        target_base_url: Optional[str] = None,\n    ) -> str:\n        \"\"\"Downloads the target file specified by 'targetinfo'.\n\n        Args:\n            targetinfo: TargetFile instance received from\n                get_one_valid_targetinfo() or updated_targets().\n            destination_directory: existing local directory to download into.\n                Note that new directories may be created inside\n                destination_directory as required.\n            target_base_url: Optional; Base URL used to form the final target\n                download URL. Default is the value provided in Updater()\n\n        Raises:\n            ValueError: Invalid arguments\n            TODO: download-related errors\n            TODO: file write errors\n\n        Returns:\n            Path to downloaded file\n        \"\"\"\n\n        if target_base_url is None:\n            if self._target_base_url is None:\n                raise ValueError(\n                    \"target_base_url must be set in either \"\n                    \"download_target() or constructor\"\n                )\n\n            target_base_url = self._target_base_url\n        else:\n            target_base_url = _ensure_trailing_slash(target_base_url)\n\n        target_filepath = targetinfo.path\n        consistent_snapshot = self._trusted_set.root.signed.consistent_snapshot\n        if consistent_snapshot and self.config.prefix_targets_with_hash:\n            hashes = list(targetinfo.hashes.values())\n            dirname, sep, basename = target_filepath.rpartition(\"/\")\n            target_filepath = f\"{dirname}{sep}{hashes[0]}.{basename}\"\n        full_url = f\"{target_base_url}{target_filepath}\"\n\n        with self._fetcher.download_file(\n            full_url, targetinfo.length\n        ) as target_file:\n            try:\n                targetinfo.verify_length_and_hashes(target_file)\n            except exceptions.LengthOrHashMismatchError as e:\n                raise exceptions.RepositoryError(\n                    f\"{target_filepath} length or hashes do not match\"\n                ) from e\n\n            # Use a URL encoded targetpath as the local filename\n            filename = parse.quote(targetinfo.path, \"\")\n            local_filepath = os.path.join(destination_directory, filename)\n            sslib_util.persist_temp_file(target_file, local_filepath)\n\n            return local_filepath\n\n    def _download_metadata(\n        self, rolename: str, length: int, version: Optional[int] = None\n    ) -> bytes:\n        \"\"\"Download a metadata file and return it as bytes\"\"\"\n        if version is None:\n            url = f\"{self._metadata_base_url}{rolename}.json\"\n        else:\n            url = f\"{self._metadata_base_url}{version}.{rolename}.json\"\n        return self._fetcher.download_bytes(url, length)\n\n    def _load_local_metadata(self, rolename: str) -> bytes:\n        encoded_name = parse.quote(rolename, \"\")\n        with open(os.path.join(self._dir, f\"{encoded_name}.json\"), \"rb\") as f:\n            return f.read()\n\n    def _persist_metadata(self, rolename: str, data: bytes) -> None:\n        \"\"\"Write metadata to disk atomically to avoid data loss.\"\"\"\n\n        # encode the rolename to avoid issues with e.g. path separators\n        encoded_name = parse.quote(rolename, \"\")\n        filename = os.path.join(self._dir, f\"{encoded_name}.json\")\n        with tempfile.NamedTemporaryFile(\n            dir=self._dir, delete=False\n        ) as temp_file:\n            temp_file.write(data)\n        os.replace(temp_file.name, filename)\n\n    def _load_root(self) -> None:\n        \"\"\"Load remote root metadata.\n\n        Sequentially load and persist on local disk every newer root metadata\n        version available on the remote.\n        \"\"\"\n\n        # Update the root role\n        lower_bound = self._trusted_set.root.signed.version + 1\n        upper_bound = lower_bound + self.config.max_root_rotations\n\n        for next_version in range(lower_bound, upper_bound):\n            try:\n                data = self._download_metadata(\n                    \"root\", self.config.root_max_length, next_version\n                )\n                self._trusted_set.update_root(data)\n                self._persist_metadata(\"root\", data)\n\n            except exceptions.FetcherHTTPError as exception:\n                if exception.status_code not in {403, 404}:\n                    raise\n                # 404/403 means current root is newest available\n                break\n\n    def _load_timestamp(self) -> None:\n        \"\"\"Load local and remote timestamp metadata\"\"\"\n        try:\n            data = self._load_local_metadata(\"timestamp\")\n            self._trusted_set.update_timestamp(data)\n        except (OSError, exceptions.RepositoryError) as e:\n            # Local timestamp does not exist or is invalid\n            logger.debug(\"Local timestamp not valid as final: %s\", e)\n\n        # Load from remote (whether local load succeeded or not)\n        data = self._download_metadata(\n            \"timestamp\", self.config.timestamp_max_length\n        )\n        self._trusted_set.update_timestamp(data)\n        self._persist_metadata(\"timestamp\", data)\n\n    def _load_snapshot(self) -> None:\n        \"\"\"Load local (and if needed remote) snapshot metadata\"\"\"\n        try:\n            data = self._load_local_metadata(\"snapshot\")\n            self._trusted_set.update_snapshot(data, trusted=True)\n            logger.debug(\"Local snapshot is valid: not downloading new one\")\n        except (OSError, exceptions.RepositoryError) as e:\n            # Local snapshot does not exist or is invalid: update from remote\n            logger.debug(\"Local snapshot not valid as final: %s\", e)\n\n            assert self._trusted_set.timestamp is not None  # nosec\n            snapshot_meta = self._trusted_set.timestamp.signed.snapshot_meta\n            length = snapshot_meta.length or self.config.snapshot_max_length\n            version = None\n            if self._trusted_set.root.signed.consistent_snapshot:\n                version = snapshot_meta.version\n\n            data = self._download_metadata(\"snapshot\", length, version)\n            self._trusted_set.update_snapshot(data)\n            self._persist_metadata(\"snapshot\", data)\n\n    def _load_targets(self, role: str, parent_role: str) -> None:\n        \"\"\"Load local (and if needed remote) metadata for 'role'.\"\"\"\n        try:\n            data = self._load_local_metadata(role)\n            self._trusted_set.update_delegated_targets(data, role, parent_role)\n            logger.debug(\"Local %s is valid: not downloading new one\", role)\n        except (OSError, exceptions.RepositoryError) as e:\n            # Local 'role' does not exist or is invalid: update from remote\n            logger.debug(\"Failed to load local %s: %s\", role, e)\n\n            assert self._trusted_set.snapshot is not None  # nosec\n            metainfo = self._trusted_set.snapshot.signed.meta[f\"{role}.json\"]\n            length = metainfo.length or self.config.targets_max_length\n            version = None\n            if self._trusted_set.root.signed.consistent_snapshot:\n                version = metainfo.version\n\n            data = self._download_metadata(role, length, version)\n            self._trusted_set.update_delegated_targets(data, role, parent_role)\n            self._persist_metadata(role, data)\n\n    def _preorder_depth_first_walk(\n        self, target_filepath: str\n    ) -> Optional[TargetFile]:\n        \"\"\"\n        Interrogates the tree of target delegations in order of appearance\n        (which implicitly order trustworthiness), and returns the matching\n        target found in the most trusted role.\n        \"\"\"\n\n        # List of delegations to be interrogated. A (role, parent role) pair\n        # is needed to load and verify the delegated targets metadata.\n        delegations_to_visit = [(\"targets\", \"root\")]\n        visited_role_names: Set[Tuple[str, str]] = set()\n        number_of_delegations = self.config.max_delegations\n\n        # Preorder depth-first traversal of the graph of target delegations.\n        while number_of_delegations > 0 and len(delegations_to_visit) > 0:\n\n            # Pop the role name from the top of the stack.\n            role_name, parent_role = delegations_to_visit.pop(-1)\n\n            # Skip any visited current role to prevent cycles.\n            if (role_name, parent_role) in visited_role_names:\n                logger.debug(\"Skipping visited current role %s\", role_name)\n                continue\n\n            # The metadata for 'role_name' must be downloaded/updated before\n            # its targets, delegations, and child roles can be inspected.\n            self._load_targets(role_name, parent_role)\n\n            role_metadata: Targets = self._trusted_set[role_name].signed\n            target = role_metadata.targets.get(target_filepath)\n\n            if target is not None:\n                logger.debug(\"Found target in current role %s\", role_name)\n                return target\n\n            # After preorder check, add current role to set of visited roles.\n            visited_role_names.add((role_name, parent_role))\n\n            # And also decrement number of visited roles.\n            number_of_delegations -= 1\n\n            if role_metadata.delegations is not None:\n                child_roles_to_visit = []\n                # NOTE: This may be a slow operation if there are many\n                # delegated roles.\n                for child_role in role_metadata.delegations.roles.values():\n                    if child_role.is_delegated_path(target_filepath):\n                        logger.debug(\"Adding child role %s\", child_role.name)\n\n                        child_roles_to_visit.append(\n                            (child_role.name, role_name)\n                        )\n                        if child_role.terminating:\n                            logger.debug(\"Not backtracking to other roles.\")\n                            delegations_to_visit = []\n                            break\n                # Push 'child_roles_to_visit' in reverse order of appearance\n                # onto 'delegations_to_visit'.  Roles are popped from the end of\n                # the list.\n                child_roles_to_visit.reverse()\n                delegations_to_visit.extend(child_roles_to_visit)\n\n        if number_of_delegations == 0 and len(delegations_to_visit) > 0:\n            logger.debug(\n                \"%d roles left to visit, but allowed to \"\n                \"visit at most %d delegations.\",\n                len(delegations_to_visit),\n                self.config.max_delegations,\n            )\n\n        # If this point is reached then target is not found, return None\n        return None\n\n\ndef _ensure_trailing_slash(url: str) -> str:\n    \"\"\"Return url guaranteed to end in a slash\"\"\"\n    return url if url.endswith(\"/\") else f\"{url}/\"\n", "patch": "@@ -278,28 +278,27 @@ def _download_metadata(\n     ) -> bytes:\n         \"\"\"Download a metadata file and return it as bytes\"\"\"\n         if version is None:\n-            filename = f\"{rolename}.json\"\n+            url = f\"{self._metadata_base_url}{rolename}.json\"\n         else:\n-            filename = f\"{version}.{rolename}.json\"\n-        url = parse.urljoin(self._metadata_base_url, filename)\n+            url = f\"{self._metadata_base_url}{version}.{rolename}.json\"\n         return self._fetcher.download_bytes(url, length)\n \n     def _load_local_metadata(self, rolename: str) -> bytes:\n-        with open(os.path.join(self._dir, f\"{rolename}.json\"), \"rb\") as f:\n+        encoded_name = parse.quote(rolename, \"\")\n+        with open(os.path.join(self._dir, f\"{encoded_name}.json\"), \"rb\") as f:\n             return f.read()\n \n     def _persist_metadata(self, rolename: str, data: bytes) -> None:\n-        \"\"\"Acts as an atomic write operation to make sure\n-        that if the process of writing is halted, at least the\n-        original data is left intact.\n-        \"\"\"\n+        \"\"\"Write metadata to disk atomically to avoid data loss.\"\"\"\n \n-        original_filename = os.path.join(self._dir, f\"{rolename}.json\")\n+        # encode the rolename to avoid issues with e.g. path separators\n+        encoded_name = parse.quote(rolename, \"\")\n+        filename = os.path.join(self._dir, f\"{encoded_name}.json\")\n         with tempfile.NamedTemporaryFile(\n             dir=self._dir, delete=False\n         ) as temp_file:\n             temp_file.write(data)\n-        os.replace(temp_file.name, original_filename)\n+        os.replace(temp_file.name, filename)\n \n     def _load_root(self) -> None:\n         \"\"\"Load remote root metadata.", "file_path": "files/2021_10/355", "file_language": "py", "file_name": "tuf/ngclient/updater.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
