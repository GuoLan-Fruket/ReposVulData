{"index": 6703, "cve_id": "CVE-2021-41281", "cwe_id": ["CWE-22"], "cve_language": "Python", "cve_description": "Synapse is a package for Matrix homeservers written in Python 3/Twisted. Prior to version 1.47.1, Synapse instances with the media repository enabled can be tricked into downloading a file from a remote server into an arbitrary directory. No authentication is required for the affected endpoint. The last 2 directories and file name of the path are chosen randomly by Synapse and cannot be controlled by an attacker, which limits the impact. Homeservers with the media repository disabled are unaffected. Homeservers with a federation whitelist are also unaffected, since Synapse will check the remote hostname, including the trailing `../`s, against the whitelist. Server administrators should upgrade to 1.47.1 or later. Server administrators using a reverse proxy could, at the expense of losing media functionality, may block the certain endpoints as a workaround. Alternatively, non-containerized deployments can be adapted to use the hardened systemd config.", "cvss": "7.5", "publish_date": "November 23, 2021", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "NONE", "S": "UNCHANGED", "C": "NONE", "I": "HIGH", "A": "NONE", "commit_id": "91f2bd0907f1d05af67166846988e49644eb650c", "commit_message": "Prevent the media store from writing outside of the configured directory\n\nAlso tighten validation of server names by forbidding invalid characters\nin IPv6 addresses and empty domain labels.", "commit_date": "2021-11-19T13:39:15Z", "project": "matrix-org/synapse", "url": "https://api.github.com/repos/matrix-org/synapse/commits/91f2bd0907f1d05af67166846988e49644eb650c", "html_url": "https://github.com/matrix-org/synapse/commit/91f2bd0907f1d05af67166846988e49644eb650c", "windows_before": [{"commit_id": "4d6d38ac2f015294c5fca5d0e5b70649997d4b08", "commit_date": "Fri Nov 19 07:07:22 2021 -0500", "commit_message": "Remove dead code from acme support. (#11393)", "files_name": ["changelog.d/11393.misc", "synapse/config/tls.py"]}, {"commit_id": "5505da21094733bd168d08d7842f9de8b9bdcf4f", "commit_date": "Fri Nov 19 07:06:16 2021 -0500", "commit_message": "Remove msc2716 from the list of tests for complement. (#11389)", "files_name": ["changelog.d/11389.misc", "scripts-dev/complement.sh"]}, {"commit_id": "eca7cffb73fce77d025a0d7a08badb855b6df133", "commit_date": "Fri Nov 19 06:40:12 2021 -0500", "commit_message": "Keep fallback key marked as used if it's re-uploaded (#11382)", "files_name": ["changelog.d/11382.misc", "synapse/storage/databases/main/end_to_end_keys.py", "tests/handlers/test_e2e_keys.py"]}, {"commit_id": "e2e9bea1ce61e7765fcb1df8b489b0fe14bef42e", "commit_date": "Fri Nov 19 10:56:59 2021 +0000", "commit_message": "Publish a `develop` docker image (#11380)", "files_name": [".github/workflows/docker.yml", "changelog.d/11380.misc"]}, {"commit_id": "a6f7f845702c56dd7d1e7dfabd3f50a71f245cc1", "commit_date": "Fri Nov 19 10:55:09 2021 +0000", "commit_message": "Fix verification of objects signed with old local keys (#11379)", "files_name": ["changelog.d/11379.bugfix", "synapse/crypto/keyring.py", "tests/crypto/test_keyring.py"]}, {"commit_id": "7ffddd819c2e0e7edff141e987372205894084b6", "commit_date": "Thu Nov 18 14:16:08 2021 -0600", "commit_message": "Prevent historical state from being pushed to an application service via `/transactions` (MSC2716) (#11265)", "files_name": ["changelog.d/11265.bugfix", "synapse/appservice/api.py", "synapse/handlers/room_batch.py", "synapse/handlers/room_member.py"]}, {"commit_id": "92b75388f520cfec412bf2ff57bcdbfa22d8c01d", "commit_date": "Thu Nov 18 10:56:32 2021 -0800", "commit_message": "Remove legacy code related to deprecated `trust_identity_server_for_password_resets` config flag (#11333)", "files_name": ["changelog.d/11333.misc", "synapse/config/emailconfig.py", "synapse/config/registration.py", "synapse/handlers/identity.py", "tests/config/test_load.py"]}, {"commit_id": "81b18fe5c060a0532ab64b9575d54b84ddbad278", "commit_date": "Thu Nov 18 18:43:49 2021 +0100", "commit_message": "Add dedicated admin API for blocking a room (#11324)", "files_name": ["changelog.d/11324.feature", "docs/admin_api/rooms.md", "synapse/rest/admin/__init__.py", "synapse/rest/admin/rooms.py", "synapse/storage/databases/main/room.py", "tests/rest/admin/test_room.py"]}, {"commit_id": "5f81c0ce9c1b45e383f0651f819f36f44ccbc9fb", "commit_date": "Thu Nov 18 16:55:33 2021 +0000", "commit_message": "Add/Unerase annotations to Module API (#11341)", "files_name": ["changelog.d/11341.misc", "synapse/module_api/__init__.py"]}, {"commit_id": "433ee159cbc89f5f00a6c5ef7849007a81d58baf", "commit_date": "Thu Nov 18 14:45:38 2021 +0000", "commit_message": "Rename `get_refresh_token_for_user_id` to `create_refresh_token_for_user_id` (#11370)", "files_name": ["changelog.d/11370.misc", "synapse/handlers/auth.py", "synapse/handlers/register.py"]}, {"commit_id": "539e44139911dc95c34784f3df2b3706c00b7db9", "commit_date": "Thu Nov 18 14:40:26 2021 +0000", "commit_message": "Use auto_attribs for RefreshTokenLookupResult (#11386)", "files_name": ["changelog.d/11386.misc", "synapse/storage/databases/main/registration.py"]}, {"commit_id": "4bd54b263ef7e2ac29acdc85e0c6392684c44281", "commit_date": "Thu Nov 18 08:43:09 2021 -0500", "commit_message": "Do not allow MSC3440 threads to fork threads (#11161)", "files_name": ["changelog.d/11161.feature", "synapse/handlers/message.py", "synapse/storage/databases/main/relations.py", "tests/rest/client/test_relations.py"]}, {"commit_id": "e2dabec99649e75a676bbe035988753f7495aef9", "commit_date": "Thu Nov 18 12:24:40 2021 +0000", "commit_message": "Docs: Quote wildcard `federation_certificate_verification_whitelist` (#11381)", "files_name": ["changelog.d/11381.doc", "docs/sample_config.yaml", "synapse/config/tls.py"]}, {"commit_id": "84fac0f814f69645ff1ad564ef8294b31203dc95", "commit_date": "Wed Nov 17 19:07:02 2021 +0000", "commit_message": "Add type annotations to `synapse.metrics` (#10847)", "files_name": ["changelog.d/10847.misc", "mypy.ini", "synapse/app/_base.py", "synapse/groups/attestations.py", "synapse/handlers/typing.py", "synapse/metrics/__init__.py", "synapse/metrics/_exposition.py", "synapse/metrics/background_process_metrics.py", "synapse/metrics/jemalloc.py", "synapse/storage/database.py", "synapse/util/caches/expiringcache.py", "synapse/util/metrics.py"]}, {"commit_id": "d993c3bb1e89f77d91af6302bfb118494c6f6664", "commit_date": "Wed Nov 17 09:30:24 2021 -0600", "commit_message": "Add support for `/_matrix/media/v3` APIs (#11371)", "files_name": ["changelog.d/11371.feature", "docker/configure_workers_and_start.py", "docs/workers.md", "synapse/api/urls.py", "synapse/app/generic_worker.py", "synapse/app/homeserver.py"]}, {"commit_id": "b76337fdf825282b7c75ebc38e9a64f0b00f7cf0", "commit_date": "Wed Nov 17 14:19:56 2021 +0000", "commit_message": "Merge branch 'master' into develop", "files_name": ["077b74929f8f412395d1156e1b97eb16701059fa - Wed Nov 17 14:19:27 2021 +0000 : Merge remote-tracking branch 'origin/release-v1.47'", "0d86f6334ae5dc6be76b5bf2eea9ac5677fb89e8 - Wed Nov 17 14:10:57 2021 +0000 : Rename `get_access_token_for_user_id` method to `create_access_token_for_user_id` (#11369)", "changelog.d/11369.misc", "synapse/handlers/auth.py", "synapse/handlers/register.py", "synapse/rest/admin/users.py", "tests/handlers/test_auth.py", "tests/rest/admin/test_user.py", "tests/rest/client/test_capabilities.py"]}, {"commit_id": "60ecb6b4d4a8b5a8ed5ac918af7855f98e1570ac", "commit_date": "Wed Nov 17 09:04:50 2021 -0500", "commit_message": "Fix running complement.sh script. (#11368)", "files_name": ["changelog.d/11368.misc", "scripts-dev/complement.sh"]}, {"commit_id": "9f9d82aa846332189e818f51d49daf2335780014", "commit_date": "Wed Nov 17 13:10:12 2021 +0000", "commit_message": "1.47.0", "files_name": ["CHANGES.md", "debian/changelog", "synapse/__init__.py"]}, {"commit_id": "319dcb955ea2f8ecab4ad3924d0e2e1222ef89f7", "commit_date": "Tue Nov 16 11:36:46 2021 -0500", "commit_message": "Fix incorrect return value in tests. (#11359)", "files_name": ["changelog.d/11359.misc", "tests/storage/test_rollback_worker.py"]}, {"commit_id": "0caf20883c85f60d6d05d9157a936da1b4b2b78d", "commit_date": "Tue Nov 16 15:46:45 2021 +0000", "commit_message": "Merge tag 'v1.47.0rc3' into develop", "files_name": ["88375beeaab3f3902d6f22fea6a14daa8f2b8a5a - Tue Nov 16 15:40:47 2021 +0000 : Avoid sharing room hierarchy responses between users (#11355)", "changelog.d/11355.bugfix", "synapse/handlers/room_summary.py", "tests/handlers/test_room_summary.py"]}, {"commit_id": "7baa671dc821327cb28eb3eb01ecbe65e5ae4926", "commit_date": "Tue Nov 16 14:42:21 2021 +0000", "commit_message": "fix up changelog language", "files_name": ["CHANGES.md"]}, {"commit_id": "729acd82c86d2b705fee34d0e74ca8215b5b7658", "commit_date": "Tue Nov 16 14:40:54 2021 +0000", "commit_message": "mark the migration file migration as a bug", "files_name": ["CHANGES.md"]}, {"commit_id": "edcdc5fd82ccdf3862d811f95b3c93abad8e8578", "commit_date": "Tue Nov 16 14:34:46 2021 +0000", "commit_message": "1.47.0rc3", "files_name": ["CHANGES.md", "changelog.d/11303.misc", "changelog.d/11346.bugfix", "changelog.d/11353.misc", "debian/changelog", "synapse/__init__.py"]}, {"commit_id": "dfa536490ee2295d664160362c6339d5e39b6b85", "commit_date": "Tue Nov 16 07:47:58 2021 -0600", "commit_message": "Add support for `/_matrix/client/v3` APIs (#11318)", "files_name": ["changelog.d/11318.feature", "synapse/app/homeserver.py", "synapse/rest/client/_base.py", "synapse/rest/client/keys.py"]}, {"commit_id": "7468723697e4d292315ce807b5000062a02b37be", "commit_date": "Tue Nov 16 08:47:36 2021 -0500", "commit_message": "Add most missing type hints to synapse.util (#11328)", "files_name": ["changelog.d/11328.misc", "mypy.ini", "synapse/util/async_helpers.py", "synapse/util/caches/__init__.py", "synapse/util/caches/deferred_cache.py", "synapse/util/caches/descriptors.py", "synapse/util/caches/expiringcache.py", "synapse/util/distributor.py", "synapse/util/gai_resolver.py", "synapse/util/metrics.py"]}, {"commit_id": "6e084b62b88cf3d18646a036b7079c8a899349ab", "commit_date": "Tue Nov 16 13:16:43 2021 +0000", "commit_message": "Rename `remove_deleted_devices_from_device_inbox` to ensure it is always run (#11353)", "files_name": ["changelog.d/11353.misc", "synapse/storage/schema/main/delta/65/06remove_deleted_devices_from_device_inbox.sql"]}, {"commit_id": "3a1462f7e0693f2fb5f3654a8647552ea46dd67a", "commit_date": "Tue Nov 16 12:53:31 2021 +0000", "commit_message": "Properly register all callback hooks for legacy password authentication providers (#11340)", "files_name": ["changelog.d/11340.bugfix", "synapse/handlers/auth.py"]}, {"commit_id": "24b61f379ac1fc740e1b569b85363e2a0411883a", "commit_date": "Tue Nov 16 07:43:53 2021 -0500", "commit_message": "Add ability to un-shadow-ban via the admin API. (#11347)", "files_name": ["changelog.d/11347.feature", "docs/admin_api/user_admin_api.md", "synapse/rest/admin/users.py", "synapse/storage/databases/main/registration.py", "tests/rest/admin/test_user.py"]}, {"commit_id": "0dda1a79687b8375dd5b23763ba1585e5dad030d", "commit_date": "Tue Nov 16 10:41:35 2021 +0000", "commit_message": "Misc typing fixes for tests, part 2 of N (#11330)", "files_name": ["changelog.d/11330.misc", "tests/handlers/test_register.py", "tests/rest/client/utils.py", "tests/server.py", "tests/unittest.py"]}, {"commit_id": "e72135b9d3601b36a32cc8b6cf0c5e0448f534b3", "commit_date": "Tue Nov 16 15:51:01 2021 +0530", "commit_message": "change 'Home Server' to one word 'homeserver' (#11320)", "files_name": ["CHANGES.md", "changelog.d/11320.doc", "docs/ancient_architecture_notes.md", "docs/turn-howto.md"]}, {"commit_id": "9c59e117db6b448a1e930365014b043fa7ef26b6", "commit_date": "Mon Nov 15 17:34:15 2021 +0000", "commit_message": "Run _upgrade_existing_database on workers if at current schema_version (#11346)", "files_name": ["changelog.d/11346.bugfix", "synapse/storage/prepare_database.py", "tests/storage/test_rollback_worker.py"]}, {"commit_id": "e605e4b8f2447f0b6afa9acc104ae1882a732090", "commit_date": "Mon Nov 15 12:59:33 2021 +0000", "commit_message": "Database storage profile passes mypy (#11342)", "files_name": ["changelog.d/11342.misc", "mypy.ini", "synapse/storage/databases/main/profile.py", "tests/storage/test_profile.py"]}, {"commit_id": "5562ce6a534db61777ad81338aa5dd0a9a54032f", "commit_date": "Mon Nov 15 12:59:05 2021 +0000", "commit_message": "Get directory db file to pass mypy (#11339)", "files_name": ["changelog.d/11339.misc", "mypy.ini"]}], "windows_after": [{"commit_id": "97a402302c5b23ce49671ada42e117f34417f42f", "commit_date": "Fri Nov 19 14:08:59 2021 +0000", "commit_message": "1.47.1", "files_name": ["CHANGES.md", "debian/changelog", "synapse/__init__.py"]}, {"commit_id": "8d4dcac7e906d0c6f3cea0c9581d4b6cc7287703", "commit_date": "Fri Nov 19 14:11:05 2021 +0000", "commit_message": "Update 1.47.1 release date in CHANGES.md", "files_name": ["CHANGES.md"]}, {"commit_id": "9c21a68995cb147c5dc40b2d1263623c11498b5f", "commit_date": "Fri Nov 19 14:11:35 2021 +0000", "commit_message": "Refer to 1.47.1 without the v", "files_name": ["CHANGES.md"]}, {"commit_id": "7ae559944af1b27e36a6f4423177f51d7b4b3826", "commit_date": "Fri Nov 19 10:19:32 2021 -0500", "commit_message": "Fix checking whether a room can be published on creation. (#11392)", "files_name": ["changelog.d/11392.bugfix", "synapse/config/room_directory.py", "synapse/handlers/room.py", "tests/handlers/test_directory.py"]}, {"commit_id": "8fa83999d688bb4c1747f2237002422e566e085f", "commit_date": "Fri Nov 19 18:40:13 2021 +0000", "commit_message": "Add CVE number", "files_name": ["CHANGES.md"]}, {"commit_id": "ea20937084903864865f76e22f67d27729f2d6dc", "commit_date": "Fri Nov 19 20:39:46 2021 +0100", "commit_message": "Add an admin API to run background jobs. (#11352)", "files_name": ["changelog.d/11352.feature", "docs/sample_config.yaml", "docs/usage/administration/admin_api/background_updates.md", "docs/user_directory.md", "synapse/config/user_directory.py", "synapse/rest/admin/__init__.py", "synapse/rest/admin/background_updates.py", "synapse/storage/background_updates.py", "tests/rest/admin/test_background_updates.py"]}, {"commit_id": "d9e9771d6b5a92d2eb0b98b7b6437b25c5ec314e", "commit_date": "Fri Nov 19 14:01:55 2021 -0800", "commit_message": "Update README.md", "files_name": ["docs/README.md"]}, {"commit_id": "3d893b8cf2358f947678dfb995b73f426200b099", "commit_date": "Mon Nov 22 12:01:47 2021 -0500", "commit_message": "Store arbitrary relations from events. (#11391)", "files_name": ["changelog.d/11391.feature", "synapse/storage/databases/main/events.py", "synapse/storage/databases/main/events_bg_updates.py", "synapse/storage/schema/main/delta/65/07_arbitrary_relations.sql", "tests/rest/client/test_relations.py", "tests/unittest.py"]}, {"commit_id": "1035663833a76196c3e3ba425fd6500c5420bbe2", "commit_date": "Mon Nov 22 19:01:03 2021 +0100", "commit_message": "Add config for customizing the claim used for JWT logins. (#11361)", "files_name": ["changelog.d/11361.feature", "docs/jwt.md", "docs/sample_config.yaml", "synapse/config/jwt.py", "synapse/rest/client/login.py", "tests/rest/client/test_login.py"]}, {"commit_id": "6a5dd485bd82b269e7e169c0385290d081eae801", "commit_date": "Tue Nov 23 06:43:56 2021 -0500", "commit_message": "Refactor the code to inject bundled relations during serialization. (#11408)", "files_name": ["changelog.d/11408.misc", "synapse/events/utils.py", "synapse/handlers/events.py", "synapse/handlers/message.py", "synapse/rest/admin/rooms.py", "synapse/rest/client/relations.py", "synapse/rest/client/room.py", "synapse/rest/client/sync.py"]}, {"commit_id": "fcb944179192ff51b00e846b06755648c527de7d", "commit_date": "Tue Nov 23 12:39:09 2021 +0000", "commit_message": "Merge tag 'v1.47.1'", "files_name": ["454c3d7694aaec4a225bdb2cedfb9432e7e9a597 - Tue Nov 23 13:06:56 2021 +0000 : Merge branch 'master' into develop", "7cebaf96447a8ff50c4525ba7667f58127876c5e - Tue Nov 23 06:46:40 2021 -0800 : Remove code invalidated by deprecated config flag 'trust_identity_servers_for_password_resets' (#11395)", "changelog.d/11395.removal", "docker/conf/homeserver.yaml", "synapse/storage/databases/main/registration.py", "tests/utils.py"]}, {"commit_id": "55669bd3de7137553085e9f1c16a686ff657108c", "commit_date": "Tue Nov 23 10:21:19 2021 -0500", "commit_message": "Add missing type hints to config base classes (#11377)", "files_name": ["changelog.d/11377.bugfix", "changelog.d/11377.misc", "mypy.ini", "synapse/config/_base.py", "synapse/config/_base.pyi", "synapse/config/cache.py", "synapse/config/key.py", "synapse/config/logger.py", "synapse/config/server.py", "synapse/config/tls.py", "synapse/module_api/__init__.py", "synapse/storage/databases/main/registration.py", "tests/config/test_load.py"]}, {"commit_id": "f25c75d376ff8517e3245e06abdacf813606bd1f", "commit_date": "Tue Nov 23 17:01:34 2021 +0000", "commit_message": "Rename unstable `access_token_lifetime` configuration option to `refreshable_access_token_lifetime` to make it clear it only concerns refreshable access tokens. (#11388)", "files_name": ["changelog.d/11388.misc", "synapse/config/registration.py", "synapse/handlers/register.py", "synapse/rest/client/login.py", "synapse/rest/client/register.py", "tests/rest/client/test_auth.py"]}, {"commit_id": "7f9841bdec349075eb424c943db5569439acb83c", "commit_date": "Wed Nov 24 20:21:44 2021 +0100", "commit_message": "Lower minumum batch size to 1 for background updates (#11422)", "files_name": ["changelog.d/11422.bugfix", "synapse/storage/background_updates.py", "tests/rest/admin/test_background_updates.py", "tests/storage/test_background_update.py"]}, {"commit_id": "0d88c4f9030c50bb9e016d9a28f6db27f7913d0b", "commit_date": "Thu Nov 25 16:14:54 2021 +0100", "commit_message": "Improve performance of `remove_{hidden,deleted}_devices_from_device_inbox` (#11421)", "files_name": ["changelog.d/11421.bugfix", "synapse/storage/databases/main/deviceinbox.py", "synapse/storage/schema/main/delta/65/08_device_inbox_background_updates.sql", "tests/storage/databases/main/test_deviceinbox.py"]}, {"commit_id": "946c102ac9ac8f96c12c98582f84d957dd638d34", "commit_date": "Thu Nov 25 15:57:04 2021 +0000", "commit_message": "1.48.0rc1", "files_name": ["CHANGES.md", "changelog.d/10847.misc", "changelog.d/11161.feature", "changelog.d/11223.feature", "changelog.d/11228.feature", "changelog.d/11230.bugfix", "changelog.d/11236.feature", "changelog.d/11242.misc", "changelog.d/11247.misc", "changelog.d/11265.bugfix", "changelog.d/11278.misc", "changelog.d/11280.misc", "changelog.d/11281.doc", "changelog.d/11282.misc", "changelog.d/11285.misc", "changelog.d/11286.doc", "changelog.d/11287.misc", "changelog.d/11288.bugfix", "changelog.d/11292.misc", "changelog.d/11297.misc", "changelog.d/11298.doc", "changelog.d/11303.misc", "changelog.d/11307.misc", "changelog.d/11310.misc", "changelog.d/11311.misc", "changelog.d/11312.misc", "changelog.d/11313.misc", "changelog.d/11314.misc", "changelog.d/11316.misc", "changelog.d/11318.feature", "changelog.d/11320.doc", "changelog.d/11321.misc", "changelog.d/11322.misc", "changelog.d/11323.misc", "changelog.d/11324.feature", "changelog.d/11327.misc", "changelog.d/11328.misc", "changelog.d/11330.misc", "changelog.d/11332.misc", "changelog.d/11333.misc", "changelog.d/11335.feature", "changelog.d/11339.misc", "changelog.d/11340.bugfix", "changelog.d/11341.misc", "changelog.d/11342.misc", "changelog.d/11347.feature", "changelog.d/11352.feature", "changelog.d/11355.bugfix", "changelog.d/11359.misc", "changelog.d/11361.feature", "changelog.d/11368.misc", "changelog.d/11369.misc", "changelog.d/11370.misc", "changelog.d/11371.feature", "changelog.d/11377.bugfix", "changelog.d/11377.misc", "changelog.d/11379.bugfix", "changelog.d/11380.misc", "changelog.d/11381.doc", "changelog.d/11382.misc", "changelog.d/11386.misc", "changelog.d/11388.misc", "changelog.d/11389.misc", "changelog.d/11391.feature", "changelog.d/11392.bugfix", "changelog.d/11393.misc", "changelog.d/11395.removal", "changelog.d/11408.misc", "changelog.d/11421.bugfix", "changelog.d/11422.bugfix", "debian/changelog", "synapse/__init__.py"]}, {"commit_id": "b757b68454fc82591f46123665b6b7321b033fdc", "commit_date": "Thu Nov 25 16:07:23 2021 +0000", "commit_message": "Fixup changelog", "files_name": ["CHANGES.md"]}, {"commit_id": "7862f821de30b17ed035a3355e552d027429dc6b", "commit_date": "Thu Nov 25 16:14:23 2021 +0000", "commit_message": "Annotate string constants in `synapse.api.constants` with `Final` (#11356)", "files_name": ["changelog.d/11356.misc", "synapse/api/constants.py"]}, {"commit_id": "d4dcc0524fbadf82d977b51c6c6ebeefb1bb40a2", "commit_date": "Thu Nov 25 16:21:00 2021 +0000", "commit_message": "Incorporate review from synapse-dev", "files_name": ["CHANGES.md"]}, {"commit_id": "c54c9df286e68dc2794f5561ca0db76f7358f158", "commit_date": "Thu Nov 25 16:22:54 2021 +0000", "commit_message": "Fix docker hub name", "files_name": ["CHANGES.md"]}, {"commit_id": "c675a18071ea86cb20bdc426a4c4c34bc629b6af", "commit_date": "Fri Nov 26 13:47:24 2021 +0000", "commit_message": "Track ongoing event fetches correctly (again) (#11376)", "files_name": ["changelog.d/11376.bugfix", "synapse/storage/databases/main/events_worker.py"]}], "parents": [{"commit_id_before": "9f9d82aa846332189e818f51d49daf2335780014", "url_before": "https://api.github.com/repos/matrix-org/synapse/commits/9f9d82aa846332189e818f51d49daf2335780014", "html_url_before": "https://github.com/matrix-org/synapse/commit/9f9d82aa846332189e818f51d49daf2335780014"}], "details": [{"raw_url": "https://github.com/matrix-org/synapse/raw/91f2bd0907f1d05af67166846988e49644eb650c/synapse%2Frest%2Fmedia%2Fv1%2F_base.py", "code": "# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2019-2021 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport os\nimport urllib\nfrom types import TracebackType\nfrom typing import Awaitable, Dict, Generator, List, Optional, Tuple, Type\n\nimport attr\n\nfrom twisted.internet.interfaces import IConsumer\nfrom twisted.protocols.basic import FileSender\nfrom twisted.web.server import Request\n\nfrom synapse.api.errors import Codes, SynapseError, cs_error\nfrom synapse.http.server import finish_request, respond_with_json\nfrom synapse.http.site import SynapseRequest\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.util.stringutils import is_ascii, parse_and_validate_server_name\n\nlogger = logging.getLogger(__name__)\n\n# list all text content types that will have the charset default to UTF-8 when\n# none is given\nTEXT_CONTENT_TYPES = [\n    \"text/css\",\n    \"text/csv\",\n    \"text/html\",\n    \"text/calendar\",\n    \"text/plain\",\n    \"text/javascript\",\n    \"application/json\",\n    \"application/ld+json\",\n    \"application/rtf\",\n    \"image/svg+xml\",\n    \"text/xml\",\n]\n\n\ndef parse_media_id(request: Request) -> Tuple[str, str, Optional[str]]:\n    \"\"\"Parses the server name, media ID and optional file name from the request URI\n\n    Also performs some rough validation on the server name.\n\n    Args:\n        request: The `Request`.\n\n    Returns:\n        A tuple containing the parsed server name, media ID and optional file name.\n\n    Raises:\n        SynapseError(404): if parsing or validation fail for any reason\n    \"\"\"\n    try:\n        # The type on postpath seems incorrect in Twisted 21.2.0.\n        postpath: List[bytes] = request.postpath  # type: ignore\n        assert postpath\n\n        # This allows users to append e.g. /test.png to the URL. Useful for\n        # clients that parse the URL to see content type.\n        server_name_bytes, media_id_bytes = postpath[:2]\n        server_name = server_name_bytes.decode(\"utf-8\")\n        media_id = media_id_bytes.decode(\"utf8\")\n\n        # Validate the server name, raising if invalid\n        parse_and_validate_server_name(server_name)\n\n        file_name = None\n        if len(postpath) > 2:\n            try:\n                file_name = urllib.parse.unquote(postpath[-1].decode(\"utf-8\"))\n            except UnicodeDecodeError:\n                pass\n        return server_name, media_id, file_name\n    except Exception:\n        raise SynapseError(\n            404, \"Invalid media id token %r\" % (request.postpath,), Codes.UNKNOWN\n        )\n\n\ndef respond_404(request: SynapseRequest) -> None:\n    respond_with_json(\n        request,\n        404,\n        cs_error(\"Not found %r\" % (request.postpath,), code=Codes.NOT_FOUND),\n        send_cors=True,\n    )\n\n\nasync def respond_with_file(\n    request: SynapseRequest,\n    media_type: str,\n    file_path: str,\n    file_size: Optional[int] = None,\n    upload_name: Optional[str] = None,\n) -> None:\n    logger.debug(\"Responding with %r\", file_path)\n\n    if os.path.isfile(file_path):\n        if file_size is None:\n            stat = os.stat(file_path)\n            file_size = stat.st_size\n\n        add_file_headers(request, media_type, file_size, upload_name)\n\n        with open(file_path, \"rb\") as f:\n            await make_deferred_yieldable(FileSender().beginFileTransfer(f, request))\n\n        finish_request(request)\n    else:\n        respond_404(request)\n\n\ndef add_file_headers(\n    request: Request,\n    media_type: str,\n    file_size: Optional[int],\n    upload_name: Optional[str],\n) -> None:\n    \"\"\"Adds the correct response headers in preparation for responding with the\n    media.\n\n    Args:\n        request\n        media_type: The media/content type.\n        file_size: Size in bytes of the media, if known.\n        upload_name: The name of the requested file, if any.\n    \"\"\"\n\n    def _quote(x: str) -> str:\n        return urllib.parse.quote(x.encode(\"utf-8\"))\n\n    # Default to a UTF-8 charset for text content types.\n    # ex, uses UTF-8 for 'text/css' but not 'text/css; charset=UTF-16'\n    if media_type.lower() in TEXT_CONTENT_TYPES:\n        content_type = media_type + \"; charset=UTF-8\"\n    else:\n        content_type = media_type\n\n    request.setHeader(b\"Content-Type\", content_type.encode(\"UTF-8\"))\n    if upload_name:\n        # RFC6266 section 4.1 [1] defines both `filename` and `filename*`.\n        #\n        # `filename` is defined to be a `value`, which is defined by RFC2616\n        # section 3.6 [2] to be a `token` or a `quoted-string`, where a `token`\n        # is (essentially) a single US-ASCII word, and a `quoted-string` is a\n        # US-ASCII string surrounded by double-quotes, using backslash as an\n        # escape character. Note that %-encoding is *not* permitted.\n        #\n        # `filename*` is defined to be an `ext-value`, which is defined in\n        # RFC5987 section 3.2.1 [3] to be `charset \"'\" [ language ] \"'\" value-chars`,\n        # where `value-chars` is essentially a %-encoded string in the given charset.\n        #\n        # [1]: https://tools.ietf.org/html/rfc6266#section-4.1\n        # [2]: https://tools.ietf.org/html/rfc2616#section-3.6\n        # [3]: https://tools.ietf.org/html/rfc5987#section-3.2.1\n\n        # We avoid the quoted-string version of `filename`, because (a) synapse didn't\n        # correctly interpret those as of 0.99.2 and (b) they are a bit of a pain and we\n        # may as well just do the filename* version.\n        if _can_encode_filename_as_token(upload_name):\n            disposition = \"inline; filename=%s\" % (upload_name,)\n        else:\n            disposition = \"inline; filename*=utf-8''%s\" % (_quote(upload_name),)\n\n        request.setHeader(b\"Content-Disposition\", disposition.encode(\"ascii\"))\n\n    # cache for at least a day.\n    # XXX: we might want to turn this off for data we don't want to\n    # recommend caching as it's sensitive or private - or at least\n    # select private. don't bother setting Expires as all our\n    # clients are smart enough to be happy with Cache-Control\n    request.setHeader(b\"Cache-Control\", b\"public,max-age=86400,s-maxage=86400\")\n    if file_size is not None:\n        request.setHeader(b\"Content-Length\", b\"%d\" % (file_size,))\n\n    # Tell web crawlers to not index, archive, or follow links in media. This\n    # should help to prevent things in the media repo from showing up in web\n    # search results.\n    request.setHeader(b\"X-Robots-Tag\", \"noindex, nofollow, noarchive, noimageindex\")\n\n\n# separators as defined in RFC2616. SP and HT are handled separately.\n# see _can_encode_filename_as_token.\n_FILENAME_SEPARATOR_CHARS = {\n    \"(\",\n    \")\",\n    \"<\",\n    \">\",\n    \"@\",\n    \",\",\n    \";\",\n    \":\",\n    \"\\\\\",\n    '\"',\n    \"/\",\n    \"[\",\n    \"]\",\n    \"?\",\n    \"=\",\n    \"{\",\n    \"}\",\n}\n\n\ndef _can_encode_filename_as_token(x: str) -> bool:\n    for c in x:\n        # from RFC2616:\n        #\n        #        token          = 1*<any CHAR except CTLs or separators>\n        #\n        #        separators     = \"(\" | \")\" | \"<\" | \">\" | \"@\"\n        #                       | \",\" | \";\" | \":\" | \"\\\" | <\">\n        #                       | \"/\" | \"[\" | \"]\" | \"?\" | \"=\"\n        #                       | \"{\" | \"}\" | SP | HT\n        #\n        #        CHAR           = <any US-ASCII character (octets 0 - 127)>\n        #\n        #        CTL            = <any US-ASCII control character\n        #                         (octets 0 - 31) and DEL (127)>\n        #\n        if ord(c) >= 127 or ord(c) <= 32 or c in _FILENAME_SEPARATOR_CHARS:\n            return False\n    return True\n\n\nasync def respond_with_responder(\n    request: SynapseRequest,\n    responder: \"Optional[Responder]\",\n    media_type: str,\n    file_size: Optional[int],\n    upload_name: Optional[str] = None,\n) -> None:\n    \"\"\"Responds to the request with given responder. If responder is None then\n    returns 404.\n\n    Args:\n        request\n        responder\n        media_type: The media/content type.\n        file_size: Size in bytes of the media. If not known it should be None\n        upload_name: The name of the requested file, if any.\n    \"\"\"\n    if request._disconnected:\n        logger.warning(\n            \"Not sending response to request %s, already disconnected.\", request\n        )\n        return\n\n    if not responder:\n        respond_404(request)\n        return\n\n    logger.debug(\"Responding to media request with responder %s\", responder)\n    add_file_headers(request, media_type, file_size, upload_name)\n    try:\n        with responder:\n            await responder.write_to_consumer(request)\n    except Exception as e:\n        # The majority of the time this will be due to the client having gone\n        # away. Unfortunately, Twisted simply throws a generic exception at us\n        # in that case.\n        logger.warning(\"Failed to write to consumer: %s %s\", type(e), e)\n\n        # Unregister the producer, if it has one, so Twisted doesn't complain\n        if request.producer:\n            request.unregisterProducer()\n\n    finish_request(request)\n\n\nclass Responder:\n    \"\"\"Represents a response that can be streamed to the requester.\n\n    Responder is a context manager which *must* be used, so that any resources\n    held can be cleaned up.\n    \"\"\"\n\n    def write_to_consumer(self, consumer: IConsumer) -> Awaitable:\n        \"\"\"Stream response into consumer\n\n        Args:\n            consumer: The consumer to stream into.\n\n        Returns:\n            Resolves once the response has finished being written\n        \"\"\"\n        pass\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        pass\n\n\n@attr.s(slots=True, frozen=True, auto_attribs=True)\nclass ThumbnailInfo:\n    \"\"\"Details about a generated thumbnail.\"\"\"\n\n    width: int\n    height: int\n    method: str\n    # Content type of thumbnail, e.g. image/png\n    type: str\n    # The size of the media file, in bytes.\n    length: Optional[int] = None\n\n\n@attr.s(slots=True, frozen=True, auto_attribs=True)\nclass FileInfo:\n    \"\"\"Details about a requested/uploaded file.\"\"\"\n\n    # The server name where the media originated from, or None if local.\n    server_name: Optional[str]\n    # The local ID of the file. For local files this is the same as the media_id\n    file_id: str\n    # If the file is for the url preview cache\n    url_cache: bool = False\n    # Whether the file is a thumbnail or not.\n    thumbnail: Optional[ThumbnailInfo] = None\n\n    # The below properties exist to maintain compatibility with third-party modules.\n    @property\n    def thumbnail_width(self) -> Optional[int]:\n        if not self.thumbnail:\n            return None\n        return self.thumbnail.width\n\n    @property\n    def thumbnail_height(self) -> Optional[int]:\n        if not self.thumbnail:\n            return None\n        return self.thumbnail.height\n\n    @property\n    def thumbnail_method(self) -> Optional[str]:\n        if not self.thumbnail:\n            return None\n        return self.thumbnail.method\n\n    @property\n    def thumbnail_type(self) -> Optional[str]:\n        if not self.thumbnail:\n            return None\n        return self.thumbnail.type\n\n    @property\n    def thumbnail_length(self) -> Optional[int]:\n        if not self.thumbnail:\n            return None\n        return self.thumbnail.length\n\n\ndef get_filename_from_headers(headers: Dict[bytes, List[bytes]]) -> Optional[str]:\n    \"\"\"\n    Get the filename of the downloaded file by inspecting the\n    Content-Disposition HTTP header.\n\n    Args:\n        headers: The HTTP request headers.\n\n    Returns:\n        The filename, or None.\n    \"\"\"\n    content_disposition = headers.get(b\"Content-Disposition\", [b\"\"])\n\n    # No header, bail out.\n    if not content_disposition[0]:\n        return None\n\n    _, params = _parse_header(content_disposition[0])\n\n    upload_name = None\n\n    # First check if there is a valid UTF-8 filename\n    upload_name_utf8 = params.get(b\"filename*\", None)\n    if upload_name_utf8:\n        if upload_name_utf8.lower().startswith(b\"utf-8''\"):\n            upload_name_utf8 = upload_name_utf8[7:]\n            # We have a filename*= section. This MUST be ASCII, and any UTF-8\n            # bytes are %-quoted.\n            try:\n                # Once it is decoded, we can then unquote the %-encoded\n                # parts strictly into a unicode string.\n                upload_name = urllib.parse.unquote(\n                    upload_name_utf8.decode(\"ascii\"), errors=\"strict\"\n                )\n            except UnicodeDecodeError:\n                # Incorrect UTF-8.\n                pass\n\n    # If there isn't check for an ascii name.\n    if not upload_name:\n        upload_name_ascii = params.get(b\"filename\", None)\n        if upload_name_ascii and is_ascii(upload_name_ascii):\n            upload_name = upload_name_ascii.decode(\"ascii\")\n\n    # This may be None here, indicating we did not find a matching name.\n    return upload_name\n\n\ndef _parse_header(line: bytes) -> Tuple[bytes, Dict[bytes, bytes]]:\n    \"\"\"Parse a Content-type like header.\n\n    Cargo-culted from `cgi`, but works on bytes rather than strings.\n\n    Args:\n        line: header to be parsed\n\n    Returns:\n        The main content-type, followed by the parameter dictionary\n    \"\"\"\n    parts = _parseparam(b\";\" + line)\n    key = next(parts)\n    pdict = {}\n    for p in parts:\n        i = p.find(b\"=\")\n        if i >= 0:\n            name = p[:i].strip().lower()\n            value = p[i + 1 :].strip()\n\n            # strip double-quotes\n            if len(value) >= 2 and value[0:1] == value[-1:] == b'\"':\n                value = value[1:-1]\n                value = value.replace(b\"\\\\\\\\\", b\"\\\\\").replace(b'\\\\\"', b'\"')\n            pdict[name] = value\n\n    return key, pdict\n\n\ndef _parseparam(s: bytes) -> Generator[bytes, None, None]:\n    \"\"\"Generator which splits the input on ;, respecting double-quoted sequences\n\n    Cargo-culted from `cgi`, but works on bytes rather than strings.\n\n    Args:\n        s: header to be parsed\n\n    Returns:\n        The split input\n    \"\"\"\n    while s[:1] == b\";\":\n        s = s[1:]\n\n        # look for the next ;\n        end = s.find(b\";\")\n\n        # if there is an odd number of \" marks between here and the next ;, skip to the\n        # next ; instead\n        while end > 0 and (s.count(b'\"', 0, end) - s.count(b'\\\\\"', 0, end)) % 2:\n            end = s.find(b\";\", end + 1)\n\n        if end < 0:\n            end = len(s)\n        f = s[:end]\n        yield f.strip()\n        s = s[end:]\n", "code_before": "# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2019-2021 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport os\nimport urllib\nfrom types import TracebackType\nfrom typing import Awaitable, Dict, Generator, List, Optional, Tuple, Type\n\nimport attr\n\nfrom twisted.internet.interfaces import IConsumer\nfrom twisted.protocols.basic import FileSender\nfrom twisted.web.server import Request\n\nfrom synapse.api.errors import Codes, SynapseError, cs_error\nfrom synapse.http.server import finish_request, respond_with_json\nfrom synapse.http.site import SynapseRequest\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.util.stringutils import is_ascii\n\nlogger = logging.getLogger(__name__)\n\n# list all text content types that will have the charset default to UTF-8 when\n# none is given\nTEXT_CONTENT_TYPES = [\n    \"text/css\",\n    \"text/csv\",\n    \"text/html\",\n    \"text/calendar\",\n    \"text/plain\",\n    \"text/javascript\",\n    \"application/json\",\n    \"application/ld+json\",\n    \"application/rtf\",\n    \"image/svg+xml\",\n    \"text/xml\",\n]\n\n\ndef parse_media_id(request: Request) -> Tuple[str, str, Optional[str]]:\n    try:\n        # The type on postpath seems incorrect in Twisted 21.2.0.\n        postpath: List[bytes] = request.postpath  # type: ignore\n        assert postpath\n\n        # This allows users to append e.g. /test.png to the URL. Useful for\n        # clients that parse the URL to see content type.\n        server_name_bytes, media_id_bytes = postpath[:2]\n        server_name = server_name_bytes.decode(\"utf-8\")\n        media_id = media_id_bytes.decode(\"utf8\")\n\n        file_name = None\n        if len(postpath) > 2:\n            try:\n                file_name = urllib.parse.unquote(postpath[-1].decode(\"utf-8\"))\n            except UnicodeDecodeError:\n                pass\n        return server_name, media_id, file_name\n    except Exception:\n        raise SynapseError(\n            404, \"Invalid media id token %r\" % (request.postpath,), Codes.UNKNOWN\n        )\n\n\ndef respond_404(request: SynapseRequest) -> None:\n    respond_with_json(\n        request,\n        404,\n        cs_error(\"Not found %r\" % (request.postpath,), code=Codes.NOT_FOUND),\n        send_cors=True,\n    )\n\n\nasync def respond_with_file(\n    request: SynapseRequest,\n    media_type: str,\n    file_path: str,\n    file_size: Optional[int] = None,\n    upload_name: Optional[str] = None,\n) -> None:\n    logger.debug(\"Responding with %r\", file_path)\n\n    if os.path.isfile(file_path):\n        if file_size is None:\n            stat = os.stat(file_path)\n            file_size = stat.st_size\n\n        add_file_headers(request, media_type, file_size, upload_name)\n\n        with open(file_path, \"rb\") as f:\n            await make_deferred_yieldable(FileSender().beginFileTransfer(f, request))\n\n        finish_request(request)\n    else:\n        respond_404(request)\n\n\ndef add_file_headers(\n    request: Request,\n    media_type: str,\n    file_size: Optional[int],\n    upload_name: Optional[str],\n) -> None:\n    \"\"\"Adds the correct response headers in preparation for responding with the\n    media.\n\n    Args:\n        request\n        media_type: The media/content type.\n        file_size: Size in bytes of the media, if known.\n        upload_name: The name of the requested file, if any.\n    \"\"\"\n\n    def _quote(x: str) -> str:\n        return urllib.parse.quote(x.encode(\"utf-8\"))\n\n    # Default to a UTF-8 charset for text content types.\n    # ex, uses UTF-8 for 'text/css' but not 'text/css; charset=UTF-16'\n    if media_type.lower() in TEXT_CONTENT_TYPES:\n        content_type = media_type + \"; charset=UTF-8\"\n    else:\n        content_type = media_type\n\n    request.setHeader(b\"Content-Type\", content_type.encode(\"UTF-8\"))\n    if upload_name:\n        # RFC6266 section 4.1 [1] defines both `filename` and `filename*`.\n        #\n        # `filename` is defined to be a `value`, which is defined by RFC2616\n        # section 3.6 [2] to be a `token` or a `quoted-string`, where a `token`\n        # is (essentially) a single US-ASCII word, and a `quoted-string` is a\n        # US-ASCII string surrounded by double-quotes, using backslash as an\n        # escape character. Note that %-encoding is *not* permitted.\n        #\n        # `filename*` is defined to be an `ext-value`, which is defined in\n        # RFC5987 section 3.2.1 [3] to be `charset \"'\" [ language ] \"'\" value-chars`,\n        # where `value-chars` is essentially a %-encoded string in the given charset.\n        #\n        # [1]: https://tools.ietf.org/html/rfc6266#section-4.1\n        # [2]: https://tools.ietf.org/html/rfc2616#section-3.6\n        # [3]: https://tools.ietf.org/html/rfc5987#section-3.2.1\n\n        # We avoid the quoted-string version of `filename`, because (a) synapse didn't\n        # correctly interpret those as of 0.99.2 and (b) they are a bit of a pain and we\n        # may as well just do the filename* version.\n        if _can_encode_filename_as_token(upload_name):\n            disposition = \"inline; filename=%s\" % (upload_name,)\n        else:\n            disposition = \"inline; filename*=utf-8''%s\" % (_quote(upload_name),)\n\n        request.setHeader(b\"Content-Disposition\", disposition.encode(\"ascii\"))\n\n    # cache for at least a day.\n    # XXX: we might want to turn this off for data we don't want to\n    # recommend caching as it's sensitive or private - or at least\n    # select private. don't bother setting Expires as all our\n    # clients are smart enough to be happy with Cache-Control\n    request.setHeader(b\"Cache-Control\", b\"public,max-age=86400,s-maxage=86400\")\n    if file_size is not None:\n        request.setHeader(b\"Content-Length\", b\"%d\" % (file_size,))\n\n    # Tell web crawlers to not index, archive, or follow links in media. This\n    # should help to prevent things in the media repo from showing up in web\n    # search results.\n    request.setHeader(b\"X-Robots-Tag\", \"noindex, nofollow, noarchive, noimageindex\")\n\n\n# separators as defined in RFC2616. SP and HT are handled separately.\n# see _can_encode_filename_as_token.\n_FILENAME_SEPARATOR_CHARS = {\n    \"(\",\n    \")\",\n    \"<\",\n    \">\",\n    \"@\",\n    \",\",\n    \";\",\n    \":\",\n    \"\\\\\",\n    '\"',\n    \"/\",\n    \"[\",\n    \"]\",\n    \"?\",\n    \"=\",\n    \"{\",\n    \"}\",\n}\n\n\ndef _can_encode_filename_as_token(x: str) -> bool:\n    for c in x:\n        # from RFC2616:\n        #\n        #        token          = 1*<any CHAR except CTLs or separators>\n        #\n        #        separators     = \"(\" | \")\" | \"<\" | \">\" | \"@\"\n        #                       | \",\" | \";\" | \":\" | \"\\\" | <\">\n        #                       | \"/\" | \"[\" | \"]\" | \"?\" | \"=\"\n        #                       | \"{\" | \"}\" | SP | HT\n        #\n        #        CHAR           = <any US-ASCII character (octets 0 - 127)>\n        #\n        #        CTL            = <any US-ASCII control character\n        #                         (octets 0 - 31) and DEL (127)>\n        #\n        if ord(c) >= 127 or ord(c) <= 32 or c in _FILENAME_SEPARATOR_CHARS:\n            return False\n    return True\n\n\nasync def respond_with_responder(\n    request: SynapseRequest,\n    responder: \"Optional[Responder]\",\n    media_type: str,\n    file_size: Optional[int],\n    upload_name: Optional[str] = None,\n) -> None:\n    \"\"\"Responds to the request with given responder. If responder is None then\n    returns 404.\n\n    Args:\n        request\n        responder\n        media_type: The media/content type.\n        file_size: Size in bytes of the media. If not known it should be None\n        upload_name: The name of the requested file, if any.\n    \"\"\"\n    if request._disconnected:\n        logger.warning(\n            \"Not sending response to request %s, already disconnected.\", request\n        )\n        return\n\n    if not responder:\n        respond_404(request)\n        return\n\n    logger.debug(\"Responding to media request with responder %s\", responder)\n    add_file_headers(request, media_type, file_size, upload_name)\n    try:\n        with responder:\n            await responder.write_to_consumer(request)\n    except Exception as e:\n        # The majority of the time this will be due to the client having gone\n        # away. Unfortunately, Twisted simply throws a generic exception at us\n        # in that case.\n        logger.warning(\"Failed to write to consumer: %s %s\", type(e), e)\n\n        # Unregister the producer, if it has one, so Twisted doesn't complain\n        if request.producer:\n            request.unregisterProducer()\n\n    finish_request(request)\n\n\nclass Responder:\n    \"\"\"Represents a response that can be streamed to the requester.\n\n    Responder is a context manager which *must* be used, so that any resources\n    held can be cleaned up.\n    \"\"\"\n\n    def write_to_consumer(self, consumer: IConsumer) -> Awaitable:\n        \"\"\"Stream response into consumer\n\n        Args:\n            consumer: The consumer to stream into.\n\n        Returns:\n            Resolves once the response has finished being written\n        \"\"\"\n        pass\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        pass\n\n\n@attr.s(slots=True, frozen=True, auto_attribs=True)\nclass ThumbnailInfo:\n    \"\"\"Details about a generated thumbnail.\"\"\"\n\n    width: int\n    height: int\n    method: str\n    # Content type of thumbnail, e.g. image/png\n    type: str\n    # The size of the media file, in bytes.\n    length: Optional[int] = None\n\n\n@attr.s(slots=True, frozen=True, auto_attribs=True)\nclass FileInfo:\n    \"\"\"Details about a requested/uploaded file.\"\"\"\n\n    # The server name where the media originated from, or None if local.\n    server_name: Optional[str]\n    # The local ID of the file. For local files this is the same as the media_id\n    file_id: str\n    # If the file is for the url preview cache\n    url_cache: bool = False\n    # Whether the file is a thumbnail or not.\n    thumbnail: Optional[ThumbnailInfo] = None\n\n    # The below properties exist to maintain compatibility with third-party modules.\n    @property\n    def thumbnail_width(self) -> Optional[int]:\n        if not self.thumbnail:\n            return None\n        return self.thumbnail.width\n\n    @property\n    def thumbnail_height(self) -> Optional[int]:\n        if not self.thumbnail:\n            return None\n        return self.thumbnail.height\n\n    @property\n    def thumbnail_method(self) -> Optional[str]:\n        if not self.thumbnail:\n            return None\n        return self.thumbnail.method\n\n    @property\n    def thumbnail_type(self) -> Optional[str]:\n        if not self.thumbnail:\n            return None\n        return self.thumbnail.type\n\n    @property\n    def thumbnail_length(self) -> Optional[int]:\n        if not self.thumbnail:\n            return None\n        return self.thumbnail.length\n\n\ndef get_filename_from_headers(headers: Dict[bytes, List[bytes]]) -> Optional[str]:\n    \"\"\"\n    Get the filename of the downloaded file by inspecting the\n    Content-Disposition HTTP header.\n\n    Args:\n        headers: The HTTP request headers.\n\n    Returns:\n        The filename, or None.\n    \"\"\"\n    content_disposition = headers.get(b\"Content-Disposition\", [b\"\"])\n\n    # No header, bail out.\n    if not content_disposition[0]:\n        return None\n\n    _, params = _parse_header(content_disposition[0])\n\n    upload_name = None\n\n    # First check if there is a valid UTF-8 filename\n    upload_name_utf8 = params.get(b\"filename*\", None)\n    if upload_name_utf8:\n        if upload_name_utf8.lower().startswith(b\"utf-8''\"):\n            upload_name_utf8 = upload_name_utf8[7:]\n            # We have a filename*= section. This MUST be ASCII, and any UTF-8\n            # bytes are %-quoted.\n            try:\n                # Once it is decoded, we can then unquote the %-encoded\n                # parts strictly into a unicode string.\n                upload_name = urllib.parse.unquote(\n                    upload_name_utf8.decode(\"ascii\"), errors=\"strict\"\n                )\n            except UnicodeDecodeError:\n                # Incorrect UTF-8.\n                pass\n\n    # If there isn't check for an ascii name.\n    if not upload_name:\n        upload_name_ascii = params.get(b\"filename\", None)\n        if upload_name_ascii and is_ascii(upload_name_ascii):\n            upload_name = upload_name_ascii.decode(\"ascii\")\n\n    # This may be None here, indicating we did not find a matching name.\n    return upload_name\n\n\ndef _parse_header(line: bytes) -> Tuple[bytes, Dict[bytes, bytes]]:\n    \"\"\"Parse a Content-type like header.\n\n    Cargo-culted from `cgi`, but works on bytes rather than strings.\n\n    Args:\n        line: header to be parsed\n\n    Returns:\n        The main content-type, followed by the parameter dictionary\n    \"\"\"\n    parts = _parseparam(b\";\" + line)\n    key = next(parts)\n    pdict = {}\n    for p in parts:\n        i = p.find(b\"=\")\n        if i >= 0:\n            name = p[:i].strip().lower()\n            value = p[i + 1 :].strip()\n\n            # strip double-quotes\n            if len(value) >= 2 and value[0:1] == value[-1:] == b'\"':\n                value = value[1:-1]\n                value = value.replace(b\"\\\\\\\\\", b\"\\\\\").replace(b'\\\\\"', b'\"')\n            pdict[name] = value\n\n    return key, pdict\n\n\ndef _parseparam(s: bytes) -> Generator[bytes, None, None]:\n    \"\"\"Generator which splits the input on ;, respecting double-quoted sequences\n\n    Cargo-culted from `cgi`, but works on bytes rather than strings.\n\n    Args:\n        s: header to be parsed\n\n    Returns:\n        The split input\n    \"\"\"\n    while s[:1] == b\";\":\n        s = s[1:]\n\n        # look for the next ;\n        end = s.find(b\";\")\n\n        # if there is an odd number of \" marks between here and the next ;, skip to the\n        # next ; instead\n        while end > 0 and (s.count(b'\"', 0, end) - s.count(b'\\\\\"', 0, end)) % 2:\n            end = s.find(b\";\", end + 1)\n\n        if end < 0:\n            end = len(s)\n        f = s[:end]\n        yield f.strip()\n        s = s[end:]\n", "patch": "@@ -29,7 +29,7 @@\n from synapse.http.server import finish_request, respond_with_json\n from synapse.http.site import SynapseRequest\n from synapse.logging.context import make_deferred_yieldable\n-from synapse.util.stringutils import is_ascii\n+from synapse.util.stringutils import is_ascii, parse_and_validate_server_name\n \n logger = logging.getLogger(__name__)\n \n@@ -51,6 +51,19 @@\n \n \n def parse_media_id(request: Request) -> Tuple[str, str, Optional[str]]:\n+    \"\"\"Parses the server name, media ID and optional file name from the request URI\n+\n+    Also performs some rough validation on the server name.\n+\n+    Args:\n+        request: The `Request`.\n+\n+    Returns:\n+        A tuple containing the parsed server name, media ID and optional file name.\n+\n+    Raises:\n+        SynapseError(404): if parsing or validation fail for any reason\n+    \"\"\"\n     try:\n         # The type on postpath seems incorrect in Twisted 21.2.0.\n         postpath: List[bytes] = request.postpath  # type: ignore\n@@ -62,6 +75,9 @@ def parse_media_id(request: Request) -> Tuple[str, str, Optional[str]]:\n         server_name = server_name_bytes.decode(\"utf-8\")\n         media_id = media_id_bytes.decode(\"utf8\")\n \n+        # Validate the server name, raising if invalid\n+        parse_and_validate_server_name(server_name)\n+\n         file_name = None\n         if len(postpath) > 2:\n             try:", "file_path": "files/2021_11/65", "file_language": "py", "file_name": "synapse/rest/media/v1/_base.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/matrix-org/synapse/raw/91f2bd0907f1d05af67166846988e49644eb650c/synapse%2Frest%2Fmedia%2Fv1%2Ffilepath.py", "code": "# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2020-2021 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport functools\nimport os\nimport re\nimport string\nfrom typing import Any, Callable, List, TypeVar, Union, cast\n\nNEW_FORMAT_ID_RE = re.compile(r\"^\\d\\d\\d\\d-\\d\\d-\\d\\d\")\n\n\nF = TypeVar(\"F\", bound=Callable[..., str])\n\n\ndef _wrap_in_base_path(func: F) -> F:\n    \"\"\"Takes a function that returns a relative path and turns it into an\n    absolute path based on the location of the primary media store\n    \"\"\"\n\n    @functools.wraps(func)\n    def _wrapped(self: \"MediaFilePaths\", *args: Any, **kwargs: Any) -> str:\n        path = func(self, *args, **kwargs)\n        return os.path.join(self.base_path, path)\n\n    return cast(F, _wrapped)\n\n\nGetPathMethod = TypeVar(\n    \"GetPathMethod\", bound=Union[Callable[..., str], Callable[..., List[str]]]\n)\n\n\ndef _wrap_with_jail_check(func: GetPathMethod) -> GetPathMethod:\n    \"\"\"Wraps a path-returning method to check that the returned path(s) do not escape\n    the media store directory.\n\n    The check is not expected to ever fail, unless `func` is missing a call to\n    `_validate_path_component`, or `_validate_path_component` is buggy.\n\n    Args:\n        func: The `MediaFilePaths` method to wrap. The method may return either a single\n            path, or a list of paths. Returned paths may be either absolute or relative.\n\n    Returns:\n        The method, wrapped with a check to ensure that the returned path(s) lie within\n        the media store directory. Raises a `ValueError` if the check fails.\n    \"\"\"\n\n    @functools.wraps(func)\n    def _wrapped(\n        self: \"MediaFilePaths\", *args: Any, **kwargs: Any\n    ) -> Union[str, List[str]]:\n        path_or_paths = func(self, *args, **kwargs)\n\n        if isinstance(path_or_paths, list):\n            paths_to_check = path_or_paths\n        else:\n            paths_to_check = [path_or_paths]\n\n        for path in paths_to_check:\n            # path may be an absolute or relative path, depending on the method being\n            # wrapped. When \"appending\" an absolute path, `os.path.join` discards the\n            # previous path, which is desired here.\n            normalized_path = os.path.normpath(os.path.join(self.real_base_path, path))\n            if (\n                os.path.commonpath([normalized_path, self.real_base_path])\n                != self.real_base_path\n            ):\n                raise ValueError(f\"Invalid media store path: {path!r}\")\n\n        return path_or_paths\n\n    return cast(GetPathMethod, _wrapped)\n\n\nALLOWED_CHARACTERS = set(\n    string.ascii_letters\n    + string.digits\n    + \"_-\"\n    + \".[]:\"  # Domain names, IPv6 addresses and ports in server names\n)\nFORBIDDEN_NAMES = {\n    \"\",\n    os.path.curdir,  # \".\" for the current platform\n    os.path.pardir,  # \"..\" for the current platform\n}\n\n\ndef _validate_path_component(name: str) -> str:\n    \"\"\"Checks that the given string can be safely used as a path component\n\n    Args:\n        name: The path component to check.\n\n    Returns:\n        The path component if valid.\n\n    Raises:\n        ValueError: If `name` cannot be safely used as a path component.\n    \"\"\"\n    if not ALLOWED_CHARACTERS.issuperset(name) or name in FORBIDDEN_NAMES:\n        raise ValueError(f\"Invalid path component: {name!r}\")\n\n    return name\n\n\nclass MediaFilePaths:\n    \"\"\"Describes where files are stored on disk.\n\n    Most of the functions have a `*_rel` variant which returns a file path that\n    is relative to the base media store path. This is mainly used when we want\n    to write to the backup media store (when one is configured)\n    \"\"\"\n\n    def __init__(self, primary_base_path: str):\n        self.base_path = primary_base_path\n\n        # The media store directory, with all symlinks resolved.\n        self.real_base_path = os.path.realpath(primary_base_path)\n\n        # Refuse to initialize if paths cannot be validated correctly for the current\n        # platform.\n        assert os.path.sep not in ALLOWED_CHARACTERS\n        assert os.path.altsep not in ALLOWED_CHARACTERS\n        # On Windows, paths have all sorts of weirdness which `_validate_path_component`\n        # does not consider. In any case, the remote media store can't work correctly\n        # for certain homeservers there, since \":\"s aren't allowed in paths.\n        assert os.name == \"posix\"\n\n    @_wrap_with_jail_check\n    def local_media_filepath_rel(self, media_id: str) -> str:\n        return os.path.join(\n            \"local_content\",\n            _validate_path_component(media_id[0:2]),\n            _validate_path_component(media_id[2:4]),\n            _validate_path_component(media_id[4:]),\n        )\n\n    local_media_filepath = _wrap_in_base_path(local_media_filepath_rel)\n\n    @_wrap_with_jail_check\n    def local_media_thumbnail_rel(\n        self, media_id: str, width: int, height: int, content_type: str, method: str\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n        return os.path.join(\n            \"local_thumbnails\",\n            _validate_path_component(media_id[0:2]),\n            _validate_path_component(media_id[2:4]),\n            _validate_path_component(media_id[4:]),\n            _validate_path_component(file_name),\n        )\n\n    local_media_thumbnail = _wrap_in_base_path(local_media_thumbnail_rel)\n\n    @_wrap_with_jail_check\n    def local_media_thumbnail_dir(self, media_id: str) -> str:\n        \"\"\"\n        Retrieve the local store path of thumbnails of a given media_id\n\n        Args:\n            media_id: The media ID to query.\n        Returns:\n            Path of local_thumbnails from media_id\n        \"\"\"\n        return os.path.join(\n            self.base_path,\n            \"local_thumbnails\",\n            _validate_path_component(media_id[0:2]),\n            _validate_path_component(media_id[2:4]),\n            _validate_path_component(media_id[4:]),\n        )\n\n    @_wrap_with_jail_check\n    def remote_media_filepath_rel(self, server_name: str, file_id: str) -> str:\n        return os.path.join(\n            \"remote_content\",\n            _validate_path_component(server_name),\n            _validate_path_component(file_id[0:2]),\n            _validate_path_component(file_id[2:4]),\n            _validate_path_component(file_id[4:]),\n        )\n\n    remote_media_filepath = _wrap_in_base_path(remote_media_filepath_rel)\n\n    @_wrap_with_jail_check\n    def remote_media_thumbnail_rel(\n        self,\n        server_name: str,\n        file_id: str,\n        width: int,\n        height: int,\n        content_type: str,\n        method: str,\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n        return os.path.join(\n            \"remote_thumbnail\",\n            _validate_path_component(server_name),\n            _validate_path_component(file_id[0:2]),\n            _validate_path_component(file_id[2:4]),\n            _validate_path_component(file_id[4:]),\n            _validate_path_component(file_name),\n        )\n\n    remote_media_thumbnail = _wrap_in_base_path(remote_media_thumbnail_rel)\n\n    # Legacy path that was used to store thumbnails previously.\n    # Should be removed after some time, when most of the thumbnails are stored\n    # using the new path.\n    @_wrap_with_jail_check\n    def remote_media_thumbnail_rel_legacy(\n        self, server_name: str, file_id: str, width: int, height: int, content_type: str\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s\" % (width, height, top_level_type, sub_type)\n        return os.path.join(\n            \"remote_thumbnail\",\n            _validate_path_component(server_name),\n            _validate_path_component(file_id[0:2]),\n            _validate_path_component(file_id[2:4]),\n            _validate_path_component(file_id[4:]),\n            _validate_path_component(file_name),\n        )\n\n    def remote_media_thumbnail_dir(self, server_name: str, file_id: str) -> str:\n        return os.path.join(\n            self.base_path,\n            \"remote_thumbnail\",\n            _validate_path_component(server_name),\n            _validate_path_component(file_id[0:2]),\n            _validate_path_component(file_id[2:4]),\n            _validate_path_component(file_id[4:]),\n        )\n\n    @_wrap_with_jail_check\n    def url_cache_filepath_rel(self, media_id: str) -> str:\n        if NEW_FORMAT_ID_RE.match(media_id):\n            # Media id is of the form <DATE><RANDOM_STRING>\n            # E.g.: 2017-09-28-fsdRDt24DS234dsf\n            return os.path.join(\n                \"url_cache\",\n                _validate_path_component(media_id[:10]),\n                _validate_path_component(media_id[11:]),\n            )\n        else:\n            return os.path.join(\n                \"url_cache\",\n                _validate_path_component(media_id[0:2]),\n                _validate_path_component(media_id[2:4]),\n                _validate_path_component(media_id[4:]),\n            )\n\n    url_cache_filepath = _wrap_in_base_path(url_cache_filepath_rel)\n\n    @_wrap_with_jail_check\n    def url_cache_filepath_dirs_to_delete(self, media_id: str) -> List[str]:\n        \"The dirs to try and remove if we delete the media_id file\"\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return [\n                os.path.join(\n                    self.base_path, \"url_cache\", _validate_path_component(media_id[:10])\n                )\n            ]\n        else:\n            return [\n                os.path.join(\n                    self.base_path,\n                    \"url_cache\",\n                    _validate_path_component(media_id[0:2]),\n                    _validate_path_component(media_id[2:4]),\n                ),\n                os.path.join(\n                    self.base_path, \"url_cache\", _validate_path_component(media_id[0:2])\n                ),\n            ]\n\n    @_wrap_with_jail_check\n    def url_cache_thumbnail_rel(\n        self, media_id: str, width: int, height: int, content_type: str, method: str\n    ) -> str:\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                _validate_path_component(media_id[:10]),\n                _validate_path_component(media_id[11:]),\n                _validate_path_component(file_name),\n            )\n        else:\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                _validate_path_component(media_id[0:2]),\n                _validate_path_component(media_id[2:4]),\n                _validate_path_component(media_id[4:]),\n                _validate_path_component(file_name),\n            )\n\n    url_cache_thumbnail = _wrap_in_base_path(url_cache_thumbnail_rel)\n\n    @_wrap_with_jail_check\n    def url_cache_thumbnail_directory_rel(self, media_id: str) -> str:\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                _validate_path_component(media_id[:10]),\n                _validate_path_component(media_id[11:]),\n            )\n        else:\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                _validate_path_component(media_id[0:2]),\n                _validate_path_component(media_id[2:4]),\n                _validate_path_component(media_id[4:]),\n            )\n\n    url_cache_thumbnail_directory = _wrap_in_base_path(\n        url_cache_thumbnail_directory_rel\n    )\n\n    @_wrap_with_jail_check\n    def url_cache_thumbnail_dirs_to_delete(self, media_id: str) -> List[str]:\n        \"The dirs to try and remove if we delete the media_id thumbnails\"\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return [\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    _validate_path_component(media_id[:10]),\n                    _validate_path_component(media_id[11:]),\n                ),\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    _validate_path_component(media_id[:10]),\n                ),\n            ]\n        else:\n            return [\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    _validate_path_component(media_id[0:2]),\n                    _validate_path_component(media_id[2:4]),\n                    _validate_path_component(media_id[4:]),\n                ),\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    _validate_path_component(media_id[0:2]),\n                    _validate_path_component(media_id[2:4]),\n                ),\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    _validate_path_component(media_id[0:2]),\n                ),\n            ]\n", "code_before": "# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2020-2021 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport functools\nimport os\nimport re\nfrom typing import Any, Callable, List, TypeVar, cast\n\nNEW_FORMAT_ID_RE = re.compile(r\"^\\d\\d\\d\\d-\\d\\d-\\d\\d\")\n\n\nF = TypeVar(\"F\", bound=Callable[..., str])\n\n\ndef _wrap_in_base_path(func: F) -> F:\n    \"\"\"Takes a function that returns a relative path and turns it into an\n    absolute path based on the location of the primary media store\n    \"\"\"\n\n    @functools.wraps(func)\n    def _wrapped(self: \"MediaFilePaths\", *args: Any, **kwargs: Any) -> str:\n        path = func(self, *args, **kwargs)\n        return os.path.join(self.base_path, path)\n\n    return cast(F, _wrapped)\n\n\nclass MediaFilePaths:\n    \"\"\"Describes where files are stored on disk.\n\n    Most of the functions have a `*_rel` variant which returns a file path that\n    is relative to the base media store path. This is mainly used when we want\n    to write to the backup media store (when one is configured)\n    \"\"\"\n\n    def __init__(self, primary_base_path: str):\n        self.base_path = primary_base_path\n\n    def local_media_filepath_rel(self, media_id: str) -> str:\n        return os.path.join(\"local_content\", media_id[0:2], media_id[2:4], media_id[4:])\n\n    local_media_filepath = _wrap_in_base_path(local_media_filepath_rel)\n\n    def local_media_thumbnail_rel(\n        self, media_id: str, width: int, height: int, content_type: str, method: str\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n        return os.path.join(\n            \"local_thumbnails\", media_id[0:2], media_id[2:4], media_id[4:], file_name\n        )\n\n    local_media_thumbnail = _wrap_in_base_path(local_media_thumbnail_rel)\n\n    def local_media_thumbnail_dir(self, media_id: str) -> str:\n        \"\"\"\n        Retrieve the local store path of thumbnails of a given media_id\n\n        Args:\n            media_id: The media ID to query.\n        Returns:\n            Path of local_thumbnails from media_id\n        \"\"\"\n        return os.path.join(\n            self.base_path,\n            \"local_thumbnails\",\n            media_id[0:2],\n            media_id[2:4],\n            media_id[4:],\n        )\n\n    def remote_media_filepath_rel(self, server_name: str, file_id: str) -> str:\n        return os.path.join(\n            \"remote_content\", server_name, file_id[0:2], file_id[2:4], file_id[4:]\n        )\n\n    remote_media_filepath = _wrap_in_base_path(remote_media_filepath_rel)\n\n    def remote_media_thumbnail_rel(\n        self,\n        server_name: str,\n        file_id: str,\n        width: int,\n        height: int,\n        content_type: str,\n        method: str,\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n        return os.path.join(\n            \"remote_thumbnail\",\n            server_name,\n            file_id[0:2],\n            file_id[2:4],\n            file_id[4:],\n            file_name,\n        )\n\n    remote_media_thumbnail = _wrap_in_base_path(remote_media_thumbnail_rel)\n\n    # Legacy path that was used to store thumbnails previously.\n    # Should be removed after some time, when most of the thumbnails are stored\n    # using the new path.\n    def remote_media_thumbnail_rel_legacy(\n        self, server_name: str, file_id: str, width: int, height: int, content_type: str\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s\" % (width, height, top_level_type, sub_type)\n        return os.path.join(\n            \"remote_thumbnail\",\n            server_name,\n            file_id[0:2],\n            file_id[2:4],\n            file_id[4:],\n            file_name,\n        )\n\n    def remote_media_thumbnail_dir(self, server_name: str, file_id: str) -> str:\n        return os.path.join(\n            self.base_path,\n            \"remote_thumbnail\",\n            server_name,\n            file_id[0:2],\n            file_id[2:4],\n            file_id[4:],\n        )\n\n    def url_cache_filepath_rel(self, media_id: str) -> str:\n        if NEW_FORMAT_ID_RE.match(media_id):\n            # Media id is of the form <DATE><RANDOM_STRING>\n            # E.g.: 2017-09-28-fsdRDt24DS234dsf\n            return os.path.join(\"url_cache\", media_id[:10], media_id[11:])\n        else:\n            return os.path.join(\"url_cache\", media_id[0:2], media_id[2:4], media_id[4:])\n\n    url_cache_filepath = _wrap_in_base_path(url_cache_filepath_rel)\n\n    def url_cache_filepath_dirs_to_delete(self, media_id: str) -> List[str]:\n        \"The dirs to try and remove if we delete the media_id file\"\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return [os.path.join(self.base_path, \"url_cache\", media_id[:10])]\n        else:\n            return [\n                os.path.join(self.base_path, \"url_cache\", media_id[0:2], media_id[2:4]),\n                os.path.join(self.base_path, \"url_cache\", media_id[0:2]),\n            ]\n\n    def url_cache_thumbnail_rel(\n        self, media_id: str, width: int, height: int, content_type: str, method: str\n    ) -> str:\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return os.path.join(\n                \"url_cache_thumbnails\", media_id[:10], media_id[11:], file_name\n            )\n        else:\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                media_id[0:2],\n                media_id[2:4],\n                media_id[4:],\n                file_name,\n            )\n\n    url_cache_thumbnail = _wrap_in_base_path(url_cache_thumbnail_rel)\n\n    def url_cache_thumbnail_directory_rel(self, media_id: str) -> str:\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return os.path.join(\"url_cache_thumbnails\", media_id[:10], media_id[11:])\n        else:\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                media_id[0:2],\n                media_id[2:4],\n                media_id[4:],\n            )\n\n    url_cache_thumbnail_directory = _wrap_in_base_path(\n        url_cache_thumbnail_directory_rel\n    )\n\n    def url_cache_thumbnail_dirs_to_delete(self, media_id: str) -> List[str]:\n        \"The dirs to try and remove if we delete the media_id thumbnails\"\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return [\n                os.path.join(\n                    self.base_path, \"url_cache_thumbnails\", media_id[:10], media_id[11:]\n                ),\n                os.path.join(self.base_path, \"url_cache_thumbnails\", media_id[:10]),\n            ]\n        else:\n            return [\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    media_id[0:2],\n                    media_id[2:4],\n                    media_id[4:],\n                ),\n                os.path.join(\n                    self.base_path, \"url_cache_thumbnails\", media_id[0:2], media_id[2:4]\n                ),\n                os.path.join(self.base_path, \"url_cache_thumbnails\", media_id[0:2]),\n            ]\n", "patch": "@@ -16,7 +16,8 @@\n import functools\n import os\n import re\n-from typing import Any, Callable, List, TypeVar, cast\n+import string\n+from typing import Any, Callable, List, TypeVar, Union, cast\n \n NEW_FORMAT_ID_RE = re.compile(r\"^\\d\\d\\d\\d-\\d\\d-\\d\\d\")\n \n@@ -37,6 +38,85 @@ def _wrapped(self: \"MediaFilePaths\", *args: Any, **kwargs: Any) -> str:\n     return cast(F, _wrapped)\n \n \n+GetPathMethod = TypeVar(\n+    \"GetPathMethod\", bound=Union[Callable[..., str], Callable[..., List[str]]]\n+)\n+\n+\n+def _wrap_with_jail_check(func: GetPathMethod) -> GetPathMethod:\n+    \"\"\"Wraps a path-returning method to check that the returned path(s) do not escape\n+    the media store directory.\n+\n+    The check is not expected to ever fail, unless `func` is missing a call to\n+    `_validate_path_component`, or `_validate_path_component` is buggy.\n+\n+    Args:\n+        func: The `MediaFilePaths` method to wrap. The method may return either a single\n+            path, or a list of paths. Returned paths may be either absolute or relative.\n+\n+    Returns:\n+        The method, wrapped with a check to ensure that the returned path(s) lie within\n+        the media store directory. Raises a `ValueError` if the check fails.\n+    \"\"\"\n+\n+    @functools.wraps(func)\n+    def _wrapped(\n+        self: \"MediaFilePaths\", *args: Any, **kwargs: Any\n+    ) -> Union[str, List[str]]:\n+        path_or_paths = func(self, *args, **kwargs)\n+\n+        if isinstance(path_or_paths, list):\n+            paths_to_check = path_or_paths\n+        else:\n+            paths_to_check = [path_or_paths]\n+\n+        for path in paths_to_check:\n+            # path may be an absolute or relative path, depending on the method being\n+            # wrapped. When \"appending\" an absolute path, `os.path.join` discards the\n+            # previous path, which is desired here.\n+            normalized_path = os.path.normpath(os.path.join(self.real_base_path, path))\n+            if (\n+                os.path.commonpath([normalized_path, self.real_base_path])\n+                != self.real_base_path\n+            ):\n+                raise ValueError(f\"Invalid media store path: {path!r}\")\n+\n+        return path_or_paths\n+\n+    return cast(GetPathMethod, _wrapped)\n+\n+\n+ALLOWED_CHARACTERS = set(\n+    string.ascii_letters\n+    + string.digits\n+    + \"_-\"\n+    + \".[]:\"  # Domain names, IPv6 addresses and ports in server names\n+)\n+FORBIDDEN_NAMES = {\n+    \"\",\n+    os.path.curdir,  # \".\" for the current platform\n+    os.path.pardir,  # \"..\" for the current platform\n+}\n+\n+\n+def _validate_path_component(name: str) -> str:\n+    \"\"\"Checks that the given string can be safely used as a path component\n+\n+    Args:\n+        name: The path component to check.\n+\n+    Returns:\n+        The path component if valid.\n+\n+    Raises:\n+        ValueError: If `name` cannot be safely used as a path component.\n+    \"\"\"\n+    if not ALLOWED_CHARACTERS.issuperset(name) or name in FORBIDDEN_NAMES:\n+        raise ValueError(f\"Invalid path component: {name!r}\")\n+\n+    return name\n+\n+\n class MediaFilePaths:\n     \"\"\"Describes where files are stored on disk.\n \n@@ -48,22 +128,46 @@ class MediaFilePaths:\n     def __init__(self, primary_base_path: str):\n         self.base_path = primary_base_path\n \n+        # The media store directory, with all symlinks resolved.\n+        self.real_base_path = os.path.realpath(primary_base_path)\n+\n+        # Refuse to initialize if paths cannot be validated correctly for the current\n+        # platform.\n+        assert os.path.sep not in ALLOWED_CHARACTERS\n+        assert os.path.altsep not in ALLOWED_CHARACTERS\n+        # On Windows, paths have all sorts of weirdness which `_validate_path_component`\n+        # does not consider. In any case, the remote media store can't work correctly\n+        # for certain homeservers there, since \":\"s aren't allowed in paths.\n+        assert os.name == \"posix\"\n+\n+    @_wrap_with_jail_check\n     def local_media_filepath_rel(self, media_id: str) -> str:\n-        return os.path.join(\"local_content\", media_id[0:2], media_id[2:4], media_id[4:])\n+        return os.path.join(\n+            \"local_content\",\n+            _validate_path_component(media_id[0:2]),\n+            _validate_path_component(media_id[2:4]),\n+            _validate_path_component(media_id[4:]),\n+        )\n \n     local_media_filepath = _wrap_in_base_path(local_media_filepath_rel)\n \n+    @_wrap_with_jail_check\n     def local_media_thumbnail_rel(\n         self, media_id: str, width: int, height: int, content_type: str, method: str\n     ) -> str:\n         top_level_type, sub_type = content_type.split(\"/\")\n         file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n         return os.path.join(\n-            \"local_thumbnails\", media_id[0:2], media_id[2:4], media_id[4:], file_name\n+            \"local_thumbnails\",\n+            _validate_path_component(media_id[0:2]),\n+            _validate_path_component(media_id[2:4]),\n+            _validate_path_component(media_id[4:]),\n+            _validate_path_component(file_name),\n         )\n \n     local_media_thumbnail = _wrap_in_base_path(local_media_thumbnail_rel)\n \n+    @_wrap_with_jail_check\n     def local_media_thumbnail_dir(self, media_id: str) -> str:\n         \"\"\"\n         Retrieve the local store path of thumbnails of a given media_id\n@@ -76,18 +180,24 @@ def local_media_thumbnail_dir(self, media_id: str) -> str:\n         return os.path.join(\n             self.base_path,\n             \"local_thumbnails\",\n-            media_id[0:2],\n-            media_id[2:4],\n-            media_id[4:],\n+            _validate_path_component(media_id[0:2]),\n+            _validate_path_component(media_id[2:4]),\n+            _validate_path_component(media_id[4:]),\n         )\n \n+    @_wrap_with_jail_check\n     def remote_media_filepath_rel(self, server_name: str, file_id: str) -> str:\n         return os.path.join(\n-            \"remote_content\", server_name, file_id[0:2], file_id[2:4], file_id[4:]\n+            \"remote_content\",\n+            _validate_path_component(server_name),\n+            _validate_path_component(file_id[0:2]),\n+            _validate_path_component(file_id[2:4]),\n+            _validate_path_component(file_id[4:]),\n         )\n \n     remote_media_filepath = _wrap_in_base_path(remote_media_filepath_rel)\n \n+    @_wrap_with_jail_check\n     def remote_media_thumbnail_rel(\n         self,\n         server_name: str,\n@@ -101,62 +211,86 @@ def remote_media_thumbnail_rel(\n         file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n         return os.path.join(\n             \"remote_thumbnail\",\n-            server_name,\n-            file_id[0:2],\n-            file_id[2:4],\n-            file_id[4:],\n-            file_name,\n+            _validate_path_component(server_name),\n+            _validate_path_component(file_id[0:2]),\n+            _validate_path_component(file_id[2:4]),\n+            _validate_path_component(file_id[4:]),\n+            _validate_path_component(file_name),\n         )\n \n     remote_media_thumbnail = _wrap_in_base_path(remote_media_thumbnail_rel)\n \n     # Legacy path that was used to store thumbnails previously.\n     # Should be removed after some time, when most of the thumbnails are stored\n     # using the new path.\n+    @_wrap_with_jail_check\n     def remote_media_thumbnail_rel_legacy(\n         self, server_name: str, file_id: str, width: int, height: int, content_type: str\n     ) -> str:\n         top_level_type, sub_type = content_type.split(\"/\")\n         file_name = \"%i-%i-%s-%s\" % (width, height, top_level_type, sub_type)\n         return os.path.join(\n             \"remote_thumbnail\",\n-            server_name,\n-            file_id[0:2],\n-            file_id[2:4],\n-            file_id[4:],\n-            file_name,\n+            _validate_path_component(server_name),\n+            _validate_path_component(file_id[0:2]),\n+            _validate_path_component(file_id[2:4]),\n+            _validate_path_component(file_id[4:]),\n+            _validate_path_component(file_name),\n         )\n \n     def remote_media_thumbnail_dir(self, server_name: str, file_id: str) -> str:\n         return os.path.join(\n             self.base_path,\n             \"remote_thumbnail\",\n-            server_name,\n-            file_id[0:2],\n-            file_id[2:4],\n-            file_id[4:],\n+            _validate_path_component(server_name),\n+            _validate_path_component(file_id[0:2]),\n+            _validate_path_component(file_id[2:4]),\n+            _validate_path_component(file_id[4:]),\n         )\n \n+    @_wrap_with_jail_check\n     def url_cache_filepath_rel(self, media_id: str) -> str:\n         if NEW_FORMAT_ID_RE.match(media_id):\n             # Media id is of the form <DATE><RANDOM_STRING>\n             # E.g.: 2017-09-28-fsdRDt24DS234dsf\n-            return os.path.join(\"url_cache\", media_id[:10], media_id[11:])\n+            return os.path.join(\n+                \"url_cache\",\n+                _validate_path_component(media_id[:10]),\n+                _validate_path_component(media_id[11:]),\n+            )\n         else:\n-            return os.path.join(\"url_cache\", media_id[0:2], media_id[2:4], media_id[4:])\n+            return os.path.join(\n+                \"url_cache\",\n+                _validate_path_component(media_id[0:2]),\n+                _validate_path_component(media_id[2:4]),\n+                _validate_path_component(media_id[4:]),\n+            )\n \n     url_cache_filepath = _wrap_in_base_path(url_cache_filepath_rel)\n \n+    @_wrap_with_jail_check\n     def url_cache_filepath_dirs_to_delete(self, media_id: str) -> List[str]:\n         \"The dirs to try and remove if we delete the media_id file\"\n         if NEW_FORMAT_ID_RE.match(media_id):\n-            return [os.path.join(self.base_path, \"url_cache\", media_id[:10])]\n+            return [\n+                os.path.join(\n+                    self.base_path, \"url_cache\", _validate_path_component(media_id[:10])\n+                )\n+            ]\n         else:\n             return [\n-                os.path.join(self.base_path, \"url_cache\", media_id[0:2], media_id[2:4]),\n-                os.path.join(self.base_path, \"url_cache\", media_id[0:2]),\n+                os.path.join(\n+                    self.base_path,\n+                    \"url_cache\",\n+                    _validate_path_component(media_id[0:2]),\n+                    _validate_path_component(media_id[2:4]),\n+                ),\n+                os.path.join(\n+                    self.base_path, \"url_cache\", _validate_path_component(media_id[0:2])\n+                ),\n             ]\n \n+    @_wrap_with_jail_check\n     def url_cache_thumbnail_rel(\n         self, media_id: str, width: int, height: int, content_type: str, method: str\n     ) -> str:\n@@ -168,59 +302,82 @@ def url_cache_thumbnail_rel(\n \n         if NEW_FORMAT_ID_RE.match(media_id):\n             return os.path.join(\n-                \"url_cache_thumbnails\", media_id[:10], media_id[11:], file_name\n+                \"url_cache_thumbnails\",\n+                _validate_path_component(media_id[:10]),\n+                _validate_path_component(media_id[11:]),\n+                _validate_path_component(file_name),\n             )\n         else:\n             return os.path.join(\n                 \"url_cache_thumbnails\",\n-                media_id[0:2],\n-                media_id[2:4],\n-                media_id[4:],\n-                file_name,\n+                _validate_path_component(media_id[0:2]),\n+                _validate_path_component(media_id[2:4]),\n+                _validate_path_component(media_id[4:]),\n+                _validate_path_component(file_name),\n             )\n \n     url_cache_thumbnail = _wrap_in_base_path(url_cache_thumbnail_rel)\n \n+    @_wrap_with_jail_check\n     def url_cache_thumbnail_directory_rel(self, media_id: str) -> str:\n         # Media id is of the form <DATE><RANDOM_STRING>\n         # E.g.: 2017-09-28-fsdRDt24DS234dsf\n \n         if NEW_FORMAT_ID_RE.match(media_id):\n-            return os.path.join(\"url_cache_thumbnails\", media_id[:10], media_id[11:])\n+            return os.path.join(\n+                \"url_cache_thumbnails\",\n+                _validate_path_component(media_id[:10]),\n+                _validate_path_component(media_id[11:]),\n+            )\n         else:\n             return os.path.join(\n                 \"url_cache_thumbnails\",\n-                media_id[0:2],\n-                media_id[2:4],\n-                media_id[4:],\n+                _validate_path_component(media_id[0:2]),\n+                _validate_path_component(media_id[2:4]),\n+                _validate_path_component(media_id[4:]),\n             )\n \n     url_cache_thumbnail_directory = _wrap_in_base_path(\n         url_cache_thumbnail_directory_rel\n     )\n \n+    @_wrap_with_jail_check\n     def url_cache_thumbnail_dirs_to_delete(self, media_id: str) -> List[str]:\n         \"The dirs to try and remove if we delete the media_id thumbnails\"\n         # Media id is of the form <DATE><RANDOM_STRING>\n         # E.g.: 2017-09-28-fsdRDt24DS234dsf\n         if NEW_FORMAT_ID_RE.match(media_id):\n             return [\n                 os.path.join(\n-                    self.base_path, \"url_cache_thumbnails\", media_id[:10], media_id[11:]\n+                    self.base_path,\n+                    \"url_cache_thumbnails\",\n+                    _validate_path_component(media_id[:10]),\n+                    _validate_path_component(media_id[11:]),\n+                ),\n+                os.path.join(\n+                    self.base_path,\n+                    \"url_cache_thumbnails\",\n+                    _validate_path_component(media_id[:10]),\n                 ),\n-                os.path.join(self.base_path, \"url_cache_thumbnails\", media_id[:10]),\n             ]\n         else:\n             return [\n                 os.path.join(\n                     self.base_path,\n                     \"url_cache_thumbnails\",\n-                    media_id[0:2],\n-                    media_id[2:4],\n-                    media_id[4:],\n+                    _validate_path_component(media_id[0:2]),\n+                    _validate_path_component(media_id[2:4]),\n+                    _validate_path_component(media_id[4:]),\n                 ),\n                 os.path.join(\n-                    self.base_path, \"url_cache_thumbnails\", media_id[0:2], media_id[2:4]\n+                    self.base_path,\n+                    \"url_cache_thumbnails\",\n+                    _validate_path_component(media_id[0:2]),\n+                    _validate_path_component(media_id[2:4]),\n+                ),\n+                os.path.join(\n+                    self.base_path,\n+                    \"url_cache_thumbnails\",\n+                    _validate_path_component(media_id[0:2]),\n                 ),\n-                os.path.join(self.base_path, \"url_cache_thumbnails\", media_id[0:2]),\n             ]", "file_path": "files/2021_11/66", "file_language": "py", "file_name": "synapse/rest/media/v1/filepath.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 1, "static": {"rats": [true, ["/data/rdhu/other/Static/tmp/2021_11_66.py:21: High: compile\nNEW_FORMAT_ID_RE = re.compile(r\"^\\d\\d\\d\\d-\\d\\d-\\d\\d\")\nArgument 1 to this function call should be checked to ensure that it does not\ncome from an untrusted source without first verifying that it contains nothing\ndangerous."]], "semgrep": [false, []]}, "target": 1, "function_before": [{"function": "def _wrap_in_base_path(func: F) -> F:\n    \"\"\"Takes a function that returns a relative path and turns it into an\n    absolute path based on the location of the primary media store\n    \"\"\"\n\n    @functools.wraps(func)\n    def _wrapped(self: \"MediaFilePaths\", *args: Any, **kwargs: Any) -> str:\n        path = func(self, *args, **kwargs)\n        return os.path.join(self.base_path, path)\n\n    return cast(F, _wrapped)", "target": 1, "line": "@@  -37,6 +38,85  @@ def _wrapped(self: \"MediaFilePaths\", *args: Any, **kwargs: Any) -> str:\n     return cast(F, _wrapped)\n \n \n+GetPathMethod = TypeVar(\n+    \"GetPathMethod\", bound=Union[Callable[..., str], Callable[..., List[str]]]\n+)\n+\n+\n+def _wrap_with_jail_check(func: GetPathMethod) -> GetPathMethod:\n+    \"\"\"Wraps a path-returning method to check that the returned path(s) do not escape\n+    the media store directory.\n+\n+    The check is not expected to ever fail, unless `func` is missing a call to\n+    `_validate_path_component`, or `_validate_path_component` is buggy.\n+\n+    Args:\n+        func: The `MediaFilePaths` method to wrap. The method may return either a single\n+            path, or a list of paths. Returned paths may be either absolute or relative.\n+\n+    Returns:\n+        The method, wrapped with a check to ensure that the returned path(s) lie within\n+        the media store directory. Raises a `ValueError` if the check fails.\n+    \"\"\"\n+\n+    @functools.wraps(func)\n+    def _wrapped(\n+        self: \"MediaFilePaths\", *args: Any, **kwargs: Any\n+    ) -> Union[str, List[str]]:\n+        path_or_paths = func(self, *args, **kwargs)\n+\n+        if isinstance(path_or_paths, list):\n+            paths_to_check = path_or_paths\n+        else:\n+            paths_to_check = [path_or_paths]\n+\n+        for path in paths_to_check:\n+            # path may be an absolute or relative path, depending on the method being\n+            # wrapped. When \"appending\" an absolute path, `os.path.join` discards the\n+            # previous path, which is desired here.\n+            normalized_path = os.path.normpath(os.path.join(self.real_base_path, path))\n+            if (\n+                os.path.commonpath([normalized_path, self.real_base_path])\n+                != self.real_base_path\n+            ):\n+                raise ValueError(f\"Invalid media store path: {path!r}\")\n+\n+        return path_or_paths\n+\n+    return cast(GetPathMethod, _wrapped)\n+\n+\n+ALLOWED_CHARACTERS = set(\n+    string.ascii_letters\n+    + string.digits\n+    + \"_-\"\n+    + \".[]:\"  # Domain names, IPv6 addresses and ports in server names\n+)\n+FORBIDDEN_NAMES = {\n+    \"\",\n+    os.path.curdir,  # \".\" for the current platform\n+    os.path.pardir,  # \"..\" for the current platform\n+}\n+\n+\n+def _validate_path_component(name: str) -> str:\n+    \"\"\"Checks that the given string can be safely used as a path component\n+\n+    Args:\n+        name: The path component to check.\n+\n+    Returns:\n+        The path component if valid.\n+\n+    Raises:\n+        ValueError: If `name` cannot be safely used as a path component.\n+    \"\"\"\n+    if not ALLOWED_CHARACTERS.issuperset(name) or name in FORBIDDEN_NAMES:\n+        raise ValueError(f\"Invalid path component: {name!r}\")\n+\n+    return name\n+\n+\n class MediaFilePaths:\n     \"\"\"Describes where files are stored on disk.\n \n"}, {"function": "class MediaFilePaths:\n    \"\"\"Describes where files are stored on disk.\n\n    Most of the functions have a `*_rel` variant which returns a file path that\n    is relative to the base media store path. This is mainly used when we want\n    to write to the backup media store (when one is configured)\n    \"\"\"\n\n    def __init__(self, primary_base_path: str):\n        self.base_path = primary_base_path\n\n    def local_media_filepath_rel(self, media_id: str) -> str:\n        return os.path.join(\"local_content\", media_id[0:2], media_id[2:4], media_id[4:])\n\n    local_media_filepath = _wrap_in_base_path(local_media_filepath_rel)\n\n    def local_media_thumbnail_rel(\n        self, media_id: str, width: int, height: int, content_type: str, method: str\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n        return os.path.join(\n            \"local_thumbnails\", media_id[0:2], media_id[2:4], media_id[4:], file_name\n        )\n\n    local_media_thumbnail = _wrap_in_base_path(local_media_thumbnail_rel)\n\n    def local_media_thumbnail_dir(self, media_id: str) -> str:\n        \"\"\"\n        Retrieve the local store path of thumbnails of a given media_id\n\n        Args:\n            media_id: The media ID to query.\n        Returns:\n            Path of local_thumbnails from media_id\n        \"\"\"\n        return os.path.join(\n            self.base_path,\n            \"local_thumbnails\",\n            media_id[0:2],\n            media_id[2:4],\n            media_id[4:],\n        )\n\n    def remote_media_filepath_rel(self, server_name: str, file_id: str) -> str:\n        return os.path.join(\n            \"remote_content\", server_name, file_id[0:2], file_id[2:4], file_id[4:]\n        )\n\n    remote_media_filepath = _wrap_in_base_path(remote_media_filepath_rel)\n\n    def remote_media_thumbnail_rel(\n        self,\n        server_name: str,\n        file_id: str,\n        width: int,\n        height: int,\n        content_type: str,\n        method: str,\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n        return os.path.join(\n            \"remote_thumbnail\",\n            server_name,\n            file_id[0:2],\n            file_id[2:4],\n            file_id[4:],\n            file_name,\n        )\n\n    remote_media_thumbnail = _wrap_in_base_path(remote_media_thumbnail_rel)\n\n    # Legacy path that was used to store thumbnails previously.\n    # Should be removed after some time, when most of the thumbnails are stored\n    # using the new path.\n    def remote_media_thumbnail_rel_legacy(\n        self, server_name: str, file_id: str, width: int, height: int, content_type: str\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s\" % (width, height, top_level_type, sub_type)\n        return os.path.join(\n            \"remote_thumbnail\",\n            server_name,\n            file_id[0:2],\n            file_id[2:4],\n            file_id[4:],\n            file_name,\n        )\n\n    def remote_media_thumbnail_dir(self, server_name: str, file_id: str) -> str:\n        return os.path.join(\n            self.base_path,\n            \"remote_thumbnail\",\n            server_name,\n            file_id[0:2],\n            file_id[2:4],\n            file_id[4:],\n        )\n\n    def url_cache_filepath_rel(self, media_id: str) -> str:\n        if NEW_FORMAT_ID_RE.match(media_id):\n            # Media id is of the form <DATE><RANDOM_STRING>\n            # E.g.: 2017-09-28-fsdRDt24DS234dsf\n            return os.path.join(\"url_cache\", media_id[:10], media_id[11:])\n        else:\n            return os.path.join(\"url_cache\", media_id[0:2], media_id[2:4], media_id[4:])\n\n    url_cache_filepath = _wrap_in_base_path(url_cache_filepath_rel)\n\n    def url_cache_filepath_dirs_to_delete(self, media_id: str) -> List[str]:\n        \"The dirs to try and remove if we delete the media_id file\"\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return [os.path.join(self.base_path, \"url_cache\", media_id[:10])]\n        else:\n            return [\n                os.path.join(self.base_path, \"url_cache\", media_id[0:2], media_id[2:4]),\n                os.path.join(self.base_path, \"url_cache\", media_id[0:2]),\n            ]\n\n    def url_cache_thumbnail_rel(\n        self, media_id: str, width: int, height: int, content_type: str, method: str\n    ) -> str:\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return os.path.join(\n                \"url_cache_thumbnails\", media_id[:10], media_id[11:], file_name\n            )\n        else:\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                media_id[0:2],\n                media_id[2:4],\n                media_id[4:],\n                file_name,\n            )\n\n    url_cache_thumbnail = _wrap_in_base_path(url_cache_thumbnail_rel)\n\n    def url_cache_thumbnail_directory_rel(self, media_id: str) -> str:\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return os.path.join(\"url_cache_thumbnails\", media_id[:10], media_id[11:])\n        else:\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                media_id[0:2],\n                media_id[2:4],\n                media_id[4:],\n            )\n\n    url_cache_thumbnail_directory = _wrap_in_base_path(\n        url_cache_thumbnail_directory_rel\n    )\n\n    def url_cache_thumbnail_dirs_to_delete(self, media_id: str) -> List[str]:\n        \"The dirs to try and remove if we delete the media_id thumbnails\"\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return [\n                os.path.join(\n                    self.base_path, \"url_cache_thumbnails\", media_id[:10], media_id[11:]\n                ),\n                os.path.join(self.base_path, \"url_cache_thumbnails\", media_id[:10]),\n            ]\n        else:\n            return [\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    media_id[0:2],\n                    media_id[2:4],\n                    media_id[4:],\n                ),\n                os.path.join(\n                    self.base_path, \"url_cache_thumbnails\", media_id[0:2], media_id[2:4]\n                ),\n                os.path.join(self.base_path, \"url_cache_thumbnails\", media_id[0:2]),\n            ]", "target": 1, "line": "@@  -37,6 +38,85  @@ def _wrapped(self: \"MediaFilePaths\", *args: Any, **kwargs: Any) -> str:\n     return cast(F, _wrapped)\n \n \n+GetPathMethod = TypeVar(\n+    \"GetPathMethod\", bound=Union[Callable[..., str], Callable[..., List[str]]]\n+)\n+\n+\n+def _wrap_with_jail_check(func: GetPathMethod) -> GetPathMethod:\n+    \"\"\"Wraps a path-returning method to check that the returned path(s) do not escape\n+    the media store directory.\n+\n+    The check is not expected to ever fail, unless `func` is missing a call to\n+    `_validate_path_component`, or `_validate_path_component` is buggy.\n+\n+    Args:\n+        func: The `MediaFilePaths` method to wrap. The method may return either a single\n+            path, or a list of paths. Returned paths may be either absolute or relative.\n+\n+    Returns:\n+        The method, wrapped with a check to ensure that the returned path(s) lie within\n+        the media store directory. Raises a `ValueError` if the check fails.\n+    \"\"\"\n+\n+    @functools.wraps(func)\n+    def _wrapped(\n+        self: \"MediaFilePaths\", *args: Any, **kwargs: Any\n+    ) -> Union[str, List[str]]:\n+        path_or_paths = func(self, *args, **kwargs)\n+\n+        if isinstance(path_or_paths, list):\n+            paths_to_check = path_or_paths\n+        else:\n+            paths_to_check = [path_or_paths]\n+\n+        for path in paths_to_check:\n+            # path may be an absolute or relative path, depending on the method being\n+            # wrapped. When \"appending\" an absolute path, `os.path.join` discards the\n+            # previous path, which is desired here.\n+            normalized_path = os.path.normpath(os.path.join(self.real_base_path, path))\n+            if (\n+                os.path.commonpath([normalized_path, self.real_base_path])\n+                != self.real_base_path\n+            ):\n+                raise ValueError(f\"Invalid media store path: {path!r}\")\n+\n+        return path_or_paths\n+\n+    return cast(GetPathMethod, _wrapped)\n+\n+\n+ALLOWED_CHARACTERS = set(\n+    string.ascii_letters\n+    + string.digits\n+    + \"_-\"\n+    + \".[]:\"  # Domain names, IPv6 addresses and ports in server names\n+)\n+FORBIDDEN_NAMES = {\n+    \"\",\n+    os.path.curdir,  # \".\" for the current platform\n+    os.path.pardir,  # \"..\" for the current platform\n+}\n+\n+\n+def _validate_path_component(name: str) -> str:\n+    \"\"\"Checks that the given string can be safely used as a path component\n+\n+    Args:\n+        name: The path component to check.\n+\n+    Returns:\n+        The path component if valid.\n+\n+    Raises:\n+        ValueError: If `name` cannot be safely used as a path component.\n+    \"\"\"\n+    if not ALLOWED_CHARACTERS.issuperset(name) or name in FORBIDDEN_NAMES:\n+        raise ValueError(f\"Invalid path component: {name!r}\")\n+\n+    return name\n+\n+\n class MediaFilePaths:\n     \"\"\"Describes where files are stored on disk.\n \n@@  -48,22 +128,46  @@ class MediaFilePaths:\n     def __init__(self, primary_base_path: str):\n         self.base_path = primary_base_path\n \n+        # The media store directory, with all symlinks resolved.\n+        self.real_base_path = os.path.realpath(primary_base_path)\n+\n+        # Refuse to initialize if paths cannot be validated correctly for the current\n+        # platform.\n+        assert os.path.sep not in ALLOWED_CHARACTERS\n+        assert os.path.altsep not in ALLOWED_CHARACTERS\n+        # On Windows, paths have all sorts of weirdness which `_validate_path_component`\n+        # does not consider. In any case, the remote media store can't work correctly\n+        # for certain homeservers there, since \":\"s aren't allowed in paths.\n+        assert os.name == \"posix\"\n+\n+    @_wrap_with_jail_check\n     def local_media_filepath_rel(self, media_id: str) -> str:\n-        return os.path.join(\"local_content\", media_id[0:2], media_id[2:4], media_id[4:])\n+        return os.path.join(\n+            \"local_content\",\n+            _validate_path_component(media_id[0:2]),\n+            _validate_path_component(media_id[2:4]),\n+            _validate_path_component(media_id[4:]),\n+        )\n \n     local_media_filepath = _wrap_in_base_path(local_media_filepath_rel)\n \n+    @_wrap_with_jail_check\n     def local_media_thumbnail_rel(\n         self, media_id: str, width: int, height: int, content_type: str, method: str\n     ) -> str:\n         top_level_type, sub_type = content_type.split(\"/\")\n         file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n         return os.path.join(\n-            \"local_thumbnails\", media_id[0:2], media_id[2:4], media_id[4:], file_name\n+            \"local_thumbnails\",\n+            _validate_path_component(media_id[0:2]),\n+            _validate_path_component(media_id[2:4]),\n+            _validate_path_component(media_id[4:]),\n+            _validate_path_component(file_name),\n         )\n \n     local_media_thumbnail = _wrap_in_base_path(local_media_thumbnail_rel)\n \n+    @_wrap_with_jail_check\n     def local_media_thumbnail_dir(self, media_id: str) -> str:\n         \"\"\"\n         Retrieve the local store path of thumbnails of a given media_id\n@@  -76,18 +180,24  @@ def local_media_thumbnail_dir(self, media_id: str) -> str:\n         return os.path.join(\n             self.base_path,\n             \"local_thumbnails\",\n-            media_id[0:2],\n-            media_id[2:4],\n-            media_id[4:],\n+            _validate_path_component(media_id[0:2]),\n+            _validate_path_component(media_id[2:4]),\n+            _validate_path_component(media_id[4:]),\n         )\n \n+    @_wrap_with_jail_check\n     def remote_media_filepath_rel(self, server_name: str, file_id: str) -> str:\n         return os.path.join(\n-            \"remote_content\", server_name, file_id[0:2], file_id[2:4], file_id[4:]\n+            \"remote_content\",\n+            _validate_path_component(server_name),\n+            _validate_path_component(file_id[0:2]),\n+            _validate_path_component(file_id[2:4]),\n+            _validate_path_component(file_id[4:]),\n         )\n \n     remote_media_filepath = _wrap_in_base_path(remote_media_filepath_rel)\n \n+    @_wrap_with_jail_check\n     def remote_media_thumbnail_rel(\n         self,\n         server_name: str,\n@@  -101,62 +211,86  @@ def remote_media_thumbnail_rel(\n         file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n         return os.path.join(\n             \"remote_thumbnail\",\n-            server_name,\n-            file_id[0:2],\n-            file_id[2:4],\n-            file_id[4:],\n-            file_name,\n+            _validate_path_component(server_name),\n+            _validate_path_component(file_id[0:2]),\n+            _validate_path_component(file_id[2:4]),\n+            _validate_path_component(file_id[4:]),\n+            _validate_path_component(file_name),\n         )\n \n     remote_media_thumbnail = _wrap_in_base_path(remote_media_thumbnail_rel)\n \n     # Legacy path that was used to store thumbnails previously.\n     # Should be removed after some time, when most of the thumbnails are stored\n     # using the new path.\n+    @_wrap_with_jail_check\n     def remote_media_thumbnail_rel_legacy(\n         self, server_name: str, file_id: str, width: int, height: int, content_type: str\n     ) -> str:\n         top_level_type, sub_type = content_type.split(\"/\")\n         file_name = \"%i-%i-%s-%s\" % (width, height, top_level_type, sub_type)\n         return os.path.join(\n             \"remote_thumbnail\",\n-            server_name,\n-            file_id[0:2],\n-            file_id[2:4],\n-            file_id[4:],\n-            file_name,\n+            _validate_path_component(server_name),\n+            _validate_path_component(file_id[0:2]),\n+            _validate_path_component(file_id[2:4]),\n+            _validate_path_component(file_id[4:]),\n+            _validate_path_component(file_name),\n         )\n \n     def remote_media_thumbnail_dir(self, server_name: str, file_id: str) -> str:\n         return os.path.join(\n             self.base_path,\n             \"remote_thumbnail\",\n-            server_name,\n-            file_id[0:2],\n-            file_id[2:4],\n-            file_id[4:],\n+            _validate_path_component(server_name),\n+            _validate_path_component(file_id[0:2]),\n+            _validate_path_component(file_id[2:4]),\n+            _validate_path_component(file_id[4:]),\n         )\n \n+    @_wrap_with_jail_check\n     def url_cache_filepath_rel(self, media_id: str) -> str:\n         if NEW_FORMAT_ID_RE.match(media_id):\n             # Media id is of the form <DATE><RANDOM_STRING>\n             # E.g.: 2017-09-28-fsdRDt24DS234dsf\n-            return os.path.join(\"url_cache\", media_id[:10], media_id[11:])\n+            return os.path.join(\n+                \"url_cache\",\n+                _validate_path_component(media_id[:10]),\n+                _validate_path_component(media_id[11:]),\n+            )\n         else:\n-            return os.path.join(\"url_cache\", media_id[0:2], media_id[2:4], media_id[4:])\n+            return os.path.join(\n+                \"url_cache\",\n+                _validate_path_component(media_id[0:2]),\n+                _validate_path_component(media_id[2:4]),\n+                _validate_path_component(media_id[4:]),\n+            )\n \n     url_cache_filepath = _wrap_in_base_path(url_cache_filepath_rel)\n \n+    @_wrap_with_jail_check\n     def url_cache_filepath_dirs_to_delete(self, media_id: str) -> List[str]:\n         \"The dirs to try and remove if we delete the media_id file\"\n         if NEW_FORMAT_ID_RE.match(media_id):\n-            return [os.path.join(self.base_path, \"url_cache\", media_id[:10])]\n+            return [\n+                os.path.join(\n+                    self.base_path, \"url_cache\", _validate_path_component(media_id[:10])\n+                )\n+            ]\n         else:\n             return [\n-                os.path.join(self.base_path, \"url_cache\", media_id[0:2], media_id[2:4]),\n-                os.path.join(self.base_path, \"url_cache\", media_id[0:2]),\n+                os.path.join(\n+                    self.base_path,\n+                    \"url_cache\",\n+                    _validate_path_component(media_id[0:2]),\n+                    _validate_path_component(media_id[2:4]),\n+                ),\n+                os.path.join(\n+                    self.base_path, \"url_cache\", _validate_path_component(media_id[0:2])\n+                ),\n             ]\n \n+    @_wrap_with_jail_check\n     def url_cache_thumbnail_rel(\n         self, media_id: str, width: int, height: int, content_type: str, method: str\n     ) -> str:\n@@  -168,59 +302,82  @@ def url_cache_thumbnail_rel(\n \n         if NEW_FORMAT_ID_RE.match(media_id):\n             return os.path.join(\n-                \"url_cache_thumbnails\", media_id[:10], media_id[11:], file_name\n+                \"url_cache_thumbnails\",\n+                _validate_path_component(media_id[:10]),\n+                _validate_path_component(media_id[11:]),\n+                _validate_path_component(file_name),\n             )\n         else:\n             return os.path.join(\n                 \"url_cache_thumbnails\",\n-                media_id[0:2],\n-                media_id[2:4],\n-                media_id[4:],\n-                file_name,\n+                _validate_path_component(media_id[0:2]),\n+                _validate_path_component(media_id[2:4]),\n+                _validate_path_component(media_id[4:]),\n+                _validate_path_component(file_name),\n             )\n \n     url_cache_thumbnail = _wrap_in_base_path(url_cache_thumbnail_rel)\n \n+    @_wrap_with_jail_check\n     def url_cache_thumbnail_directory_rel(self, media_id: str) -> str:\n         # Media id is of the form <DATE><RANDOM_STRING>\n         # E.g.: 2017-09-28-fsdRDt24DS234dsf\n \n         if NEW_FORMAT_ID_RE.match(media_id):\n-            return os.path.join(\"url_cache_thumbnails\", media_id[:10], media_id[11:])\n+            return os.path.join(\n+                \"url_cache_thumbnails\",\n+                _validate_path_component(media_id[:10]),\n+                _validate_path_component(media_id[11:]),\n+            )\n         else:\n             return os.path.join(\n                 \"url_cache_thumbnails\",\n-                media_id[0:2],\n-                media_id[2:4],\n-                media_id[4:],\n+                _validate_path_component(media_id[0:2]),\n+                _validate_path_component(media_id[2:4]),\n+                _validate_path_component(media_id[4:]),\n             )\n \n     url_cache_thumbnail_directory = _wrap_in_base_path(\n         url_cache_thumbnail_directory_rel\n     )\n \n+    @_wrap_with_jail_check\n     def url_cache_thumbnail_dirs_to_delete(self, media_id: str) -> List[str]:\n         \"The dirs to try and remove if we delete the media_id thumbnails\"\n         # Media id is of the form <DATE><RANDOM_STRING>\n         # E.g.: 2017-09-28-fsdRDt24DS234dsf\n         if NEW_FORMAT_ID_RE.match(media_id):\n             return [\n                 os.path.join(\n-                    self.base_path, \"url_cache_thumbnails\", media_id[:10], media_id[11:]\n+                    self.base_path,\n+                    \"url_cache_thumbnails\",\n+                    _validate_path_component(media_id[:10]),\n+                    _validate_path_component(media_id[11:]),\n+                ),\n+                os.path.join(\n+                    self.base_path,\n+                    \"url_cache_thumbnails\",\n+                    _validate_path_component(media_id[:10]),\n                 ),\n-                os.path.join(self.base_path, \"url_cache_thumbnails\", media_id[:10]),\n             ]\n         else:\n             return [\n                 os.path.join(\n                     self.base_path,\n                     \"url_cache_thumbnails\",\n-                    media_id[0:2],\n-                    media_id[2:4],\n-                    media_id[4:],\n+                    _validate_path_component(media_id[0:2]),\n+                    _validate_path_component(media_id[2:4]),\n+                    _validate_path_component(media_id[4:]),\n                 ),\n                 os.path.join(\n-                    self.base_path, \"url_cache_thumbnails\", media_id[0:2], media_id[2:4]\n+                    self.base_path,\n+                    \"url_cache_thumbnails\",\n+                    _validate_path_component(media_id[0:2]),\n+                    _validate_path_component(media_id[2:4]),\n+                ),\n+                os.path.join(\n+                    self.base_path,\n+                    \"url_cache_thumbnails\",\n+                    _validate_path_component(media_id[0:2]),\n                 ),\n-                os.path.join(self.base_path, \"url_cache_thumbnails\", media_id[0:2]),\n             ]"}], "function_after": [{"function": "def _wrap_in_base_path(func: F) -> F:\n    \"\"\"Takes a function that returns a relative path and turns it into an\n    absolute path based on the location of the primary media store\n    \"\"\"\n\n    @functools.wraps(func)\n    def _wrapped(self: \"MediaFilePaths\", *args: Any, **kwargs: Any) -> str:\n        path = func(self, *args, **kwargs)\n        return os.path.join(self.base_path, path)\n\n    return cast(F, _wrapped)", "target": 0}, {"function": "def _wrap_with_jail_check(func: GetPathMethod) -> GetPathMethod:\n    \"\"\"Wraps a path-returning method to check that the returned path(s) do not escape\n    the media store directory.\n\n    The check is not expected to ever fail, unless `func` is missing a call to\n    `_validate_path_component`, or `_validate_path_component` is buggy.\n\n    Args:\n        func: The `MediaFilePaths` method to wrap. The method may return either a single\n            path, or a list of paths. Returned paths may be either absolute or relative.\n\n    Returns:\n        The method, wrapped with a check to ensure that the returned path(s) lie within\n        the media store directory. Raises a `ValueError` if the check fails.\n    \"\"\"\n\n    @functools.wraps(func)\n    def _wrapped(\n        self: \"MediaFilePaths\", *args: Any, **kwargs: Any\n    ) -> Union[str, List[str]]:\n        path_or_paths = func(self, *args, **kwargs)\n\n        if isinstance(path_or_paths, list):\n            paths_to_check = path_or_paths\n        else:\n            paths_to_check = [path_or_paths]\n\n        for path in paths_to_check:\n            # path may be an absolute or relative path, depending on the method being\n            # wrapped. When \"appending\" an absolute path, `os.path.join` discards the\n            # previous path, which is desired here.\n            normalized_path = os.path.normpath(os.path.join(self.real_base_path, path))\n            if (\n                os.path.commonpath([normalized_path, self.real_base_path])\n                != self.real_base_path\n            ):\n                raise ValueError(f\"Invalid media store path: {path!r}\")\n\n        return path_or_paths\n\n    return cast(GetPathMethod, _wrapped)", "target": 0}, {"function": "def _validate_path_component(name: str) -> str:\n    \"\"\"Checks that the given string can be safely used as a path component\n\n    Args:\n        name: The path component to check.\n\n    Returns:\n        The path component if valid.\n\n    Raises:\n        ValueError: If `name` cannot be safely used as a path component.\n    \"\"\"\n    if not ALLOWED_CHARACTERS.issuperset(name) or name in FORBIDDEN_NAMES:\n        raise ValueError(f\"Invalid path component: {name!r}\")\n\n    return name", "target": 0}, {"function": "class MediaFilePaths:\n    \"\"\"Describes where files are stored on disk.\n\n    Most of the functions have a `*_rel` variant which returns a file path that\n    is relative to the base media store path. This is mainly used when we want\n    to write to the backup media store (when one is configured)\n    \"\"\"\n\n    def __init__(self, primary_base_path: str):\n        self.base_path = primary_base_path\n\n        # The media store directory, with all symlinks resolved.\n        self.real_base_path = os.path.realpath(primary_base_path)\n\n        # Refuse to initialize if paths cannot be validated correctly for the current\n        # platform.\n        assert os.path.sep not in ALLOWED_CHARACTERS\n        assert os.path.altsep not in ALLOWED_CHARACTERS\n        # On Windows, paths have all sorts of weirdness which `_validate_path_component`\n        # does not consider. In any case, the remote media store can't work correctly\n        # for certain homeservers there, since \":\"s aren't allowed in paths.\n        assert os.name == \"posix\"\n\n    @_wrap_with_jail_check\n    def local_media_filepath_rel(self, media_id: str) -> str:\n        return os.path.join(\n            \"local_content\",\n            _validate_path_component(media_id[0:2]),\n            _validate_path_component(media_id[2:4]),\n            _validate_path_component(media_id[4:]),\n        )\n\n    local_media_filepath = _wrap_in_base_path(local_media_filepath_rel)\n\n    @_wrap_with_jail_check\n    def local_media_thumbnail_rel(\n        self, media_id: str, width: int, height: int, content_type: str, method: str\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n        return os.path.join(\n            \"local_thumbnails\",\n            _validate_path_component(media_id[0:2]),\n            _validate_path_component(media_id[2:4]),\n            _validate_path_component(media_id[4:]),\n            _validate_path_component(file_name),\n        )\n\n    local_media_thumbnail = _wrap_in_base_path(local_media_thumbnail_rel)\n\n    @_wrap_with_jail_check\n    def local_media_thumbnail_dir(self, media_id: str) -> str:\n        \"\"\"\n        Retrieve the local store path of thumbnails of a given media_id\n\n        Args:\n            media_id: The media ID to query.\n        Returns:\n            Path of local_thumbnails from media_id\n        \"\"\"\n        return os.path.join(\n            self.base_path,\n            \"local_thumbnails\",\n            _validate_path_component(media_id[0:2]),\n            _validate_path_component(media_id[2:4]),\n            _validate_path_component(media_id[4:]),\n        )\n\n    @_wrap_with_jail_check\n    def remote_media_filepath_rel(self, server_name: str, file_id: str) -> str:\n        return os.path.join(\n            \"remote_content\",\n            _validate_path_component(server_name),\n            _validate_path_component(file_id[0:2]),\n            _validate_path_component(file_id[2:4]),\n            _validate_path_component(file_id[4:]),\n        )\n\n    remote_media_filepath = _wrap_in_base_path(remote_media_filepath_rel)\n\n    @_wrap_with_jail_check\n    def remote_media_thumbnail_rel(\n        self,\n        server_name: str,\n        file_id: str,\n        width: int,\n        height: int,\n        content_type: str,\n        method: str,\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n        return os.path.join(\n            \"remote_thumbnail\",\n            _validate_path_component(server_name),\n            _validate_path_component(file_id[0:2]),\n            _validate_path_component(file_id[2:4]),\n            _validate_path_component(file_id[4:]),\n            _validate_path_component(file_name),\n        )\n\n    remote_media_thumbnail = _wrap_in_base_path(remote_media_thumbnail_rel)\n\n    # Legacy path that was used to store thumbnails previously.\n    # Should be removed after some time, when most of the thumbnails are stored\n    # using the new path.\n    @_wrap_with_jail_check\n    def remote_media_thumbnail_rel_legacy(\n        self, server_name: str, file_id: str, width: int, height: int, content_type: str\n    ) -> str:\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s\" % (width, height, top_level_type, sub_type)\n        return os.path.join(\n            \"remote_thumbnail\",\n            _validate_path_component(server_name),\n            _validate_path_component(file_id[0:2]),\n            _validate_path_component(file_id[2:4]),\n            _validate_path_component(file_id[4:]),\n            _validate_path_component(file_name),\n        )\n\n    def remote_media_thumbnail_dir(self, server_name: str, file_id: str) -> str:\n        return os.path.join(\n            self.base_path,\n            \"remote_thumbnail\",\n            _validate_path_component(server_name),\n            _validate_path_component(file_id[0:2]),\n            _validate_path_component(file_id[2:4]),\n            _validate_path_component(file_id[4:]),\n        )\n\n    @_wrap_with_jail_check\n    def url_cache_filepath_rel(self, media_id: str) -> str:\n        if NEW_FORMAT_ID_RE.match(media_id):\n            # Media id is of the form <DATE><RANDOM_STRING>\n            # E.g.: 2017-09-28-fsdRDt24DS234dsf\n            return os.path.join(\n                \"url_cache\",\n                _validate_path_component(media_id[:10]),\n                _validate_path_component(media_id[11:]),\n            )\n        else:\n            return os.path.join(\n                \"url_cache\",\n                _validate_path_component(media_id[0:2]),\n                _validate_path_component(media_id[2:4]),\n                _validate_path_component(media_id[4:]),\n            )\n\n    url_cache_filepath = _wrap_in_base_path(url_cache_filepath_rel)\n\n    @_wrap_with_jail_check\n    def url_cache_filepath_dirs_to_delete(self, media_id: str) -> List[str]:\n        \"The dirs to try and remove if we delete the media_id file\"\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return [\n                os.path.join(\n                    self.base_path, \"url_cache\", _validate_path_component(media_id[:10])\n                )\n            ]\n        else:\n            return [\n                os.path.join(\n                    self.base_path,\n                    \"url_cache\",\n                    _validate_path_component(media_id[0:2]),\n                    _validate_path_component(media_id[2:4]),\n                ),\n                os.path.join(\n                    self.base_path, \"url_cache\", _validate_path_component(media_id[0:2])\n                ),\n            ]\n\n    @_wrap_with_jail_check\n    def url_cache_thumbnail_rel(\n        self, media_id: str, width: int, height: int, content_type: str, method: str\n    ) -> str:\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n\n        top_level_type, sub_type = content_type.split(\"/\")\n        file_name = \"%i-%i-%s-%s-%s\" % (width, height, top_level_type, sub_type, method)\n\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                _validate_path_component(media_id[:10]),\n                _validate_path_component(media_id[11:]),\n                _validate_path_component(file_name),\n            )\n        else:\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                _validate_path_component(media_id[0:2]),\n                _validate_path_component(media_id[2:4]),\n                _validate_path_component(media_id[4:]),\n                _validate_path_component(file_name),\n            )\n\n    url_cache_thumbnail = _wrap_in_base_path(url_cache_thumbnail_rel)\n\n    @_wrap_with_jail_check\n    def url_cache_thumbnail_directory_rel(self, media_id: str) -> str:\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                _validate_path_component(media_id[:10]),\n                _validate_path_component(media_id[11:]),\n            )\n        else:\n            return os.path.join(\n                \"url_cache_thumbnails\",\n                _validate_path_component(media_id[0:2]),\n                _validate_path_component(media_id[2:4]),\n                _validate_path_component(media_id[4:]),\n            )\n\n    url_cache_thumbnail_directory = _wrap_in_base_path(\n        url_cache_thumbnail_directory_rel\n    )\n\n    @_wrap_with_jail_check\n    def url_cache_thumbnail_dirs_to_delete(self, media_id: str) -> List[str]:\n        \"The dirs to try and remove if we delete the media_id thumbnails\"\n        # Media id is of the form <DATE><RANDOM_STRING>\n        # E.g.: 2017-09-28-fsdRDt24DS234dsf\n        if NEW_FORMAT_ID_RE.match(media_id):\n            return [\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    _validate_path_component(media_id[:10]),\n                    _validate_path_component(media_id[11:]),\n                ),\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    _validate_path_component(media_id[:10]),\n                ),\n            ]\n        else:\n            return [\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    _validate_path_component(media_id[0:2]),\n                    _validate_path_component(media_id[2:4]),\n                    _validate_path_component(media_id[4:]),\n                ),\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    _validate_path_component(media_id[0:2]),\n                    _validate_path_component(media_id[2:4]),\n                ),\n                os.path.join(\n                    self.base_path,\n                    \"url_cache_thumbnails\",\n                    _validate_path_component(media_id[0:2]),\n                ),\n            ]", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/91f2bd0907f1d05af67166846988e49644eb650c/synapse%2Futil%2Fstringutils.py", "code": "# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport itertools\nimport re\nimport secrets\nimport string\nfrom collections.abc import Iterable\nfrom typing import Optional, Tuple\n\nfrom netaddr import valid_ipv6\n\nfrom synapse.api.errors import Codes, SynapseError\n\n_string_with_symbols = string.digits + string.ascii_letters + \".,;:^&*-_+=#~@\"\n\n# https://matrix.org/docs/spec/client_server/r0.6.0#post-matrix-client-r0-register-email-requesttoken\nCLIENT_SECRET_REGEX = re.compile(r\"^[0-9a-zA-Z\\.=_\\-]+$\")\n\n# https://matrix.org/docs/spec/client_server/r0.6.1#matrix-content-mxc-uris,\n# together with https://github.com/matrix-org/matrix-doc/issues/2177 which basically\n# says \"there is no grammar for media ids\"\n#\n# The server_name part of this is purposely lax: use parse_and_validate_mxc for\n# additional validation.\n#\nMXC_REGEX = re.compile(\"^mxc://([^/]+)/([^/#?]+)$\")\n\n\ndef random_string(length: int) -> str:\n    \"\"\"Generate a cryptographically secure string of random letters.\n\n    Drawn from the characters: `a-z` and `A-Z`\n    \"\"\"\n    return \"\".join(secrets.choice(string.ascii_letters) for _ in range(length))\n\n\ndef random_string_with_symbols(length: int) -> str:\n    \"\"\"Generate a cryptographically secure string of random letters/numbers/symbols.\n\n    Drawn from the characters: `a-z`, `A-Z`, `0-9`, and `.,;:^&*-_+=#~@`\n    \"\"\"\n    return \"\".join(secrets.choice(_string_with_symbols) for _ in range(length))\n\n\ndef is_ascii(s: bytes) -> bool:\n    try:\n        s.decode(\"ascii\").encode(\"ascii\")\n    except UnicodeError:\n        return False\n    return True\n\n\ndef assert_valid_client_secret(client_secret: str) -> None:\n    \"\"\"Validate that a given string matches the client_secret defined by the spec\"\"\"\n    if (\n        len(client_secret) <= 0\n        or len(client_secret) > 255\n        or CLIENT_SECRET_REGEX.match(client_secret) is None\n    ):\n        raise SynapseError(\n            400, \"Invalid client_secret parameter\", errcode=Codes.INVALID_PARAM\n        )\n\n\ndef parse_server_name(server_name: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Split a server name into host/port parts.\n\n    Args:\n        server_name: server name to parse\n\n    Returns:\n        host/port parts.\n\n    Raises:\n        ValueError if the server name could not be parsed.\n    \"\"\"\n    try:\n        if server_name[-1] == \"]\":\n            # ipv6 literal, hopefully\n            return server_name, None\n\n        domain_port = server_name.rsplit(\":\", 1)\n        domain = domain_port[0]\n        port = int(domain_port[1]) if domain_port[1:] else None\n        return domain, port\n    except Exception:\n        raise ValueError(\"Invalid server name '%s'\" % server_name)\n\n\n# An approximation of the domain name syntax in RFC 1035, section 2.3.1.\n# NB: \"\\Z\" is not equivalent to \"$\".\n#     The latter will match the position before a \"\\n\" at the end of a string.\nVALID_HOST_REGEX = re.compile(\"\\\\A[0-9a-zA-Z-]+(?:\\\\.[0-9a-zA-Z-]+)*\\\\Z\")\n\n\ndef parse_and_validate_server_name(server_name: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Split a server name into host/port parts and do some basic validation.\n\n    Args:\n        server_name: server name to parse\n\n    Returns:\n        host/port parts.\n\n    Raises:\n        ValueError if the server name could not be parsed.\n    \"\"\"\n    host, port = parse_server_name(server_name)\n\n    # these tests don't need to be bulletproof as we'll find out soon enough\n    # if somebody is giving us invalid data. What we *do* need is to be sure\n    # that nobody is sneaking IP literals in that look like hostnames, etc.\n\n    # look for ipv6 literals\n    if host[0] == \"[\":\n        if host[-1] != \"]\":\n            raise ValueError(\"Mismatched [...] in server name '%s'\" % (server_name,))\n\n        # valid_ipv6 raises when given an empty string\n        ipv6_address = host[1:-1]\n        if not ipv6_address or not valid_ipv6(ipv6_address):\n            raise ValueError(\n                \"Server name '%s' is not a valid IPv6 address\" % (server_name,)\n            )\n    elif not VALID_HOST_REGEX.match(host):\n        raise ValueError(\"Server name '%s' has an invalid format\" % (server_name,))\n\n    return host, port\n\n\ndef valid_id_server_location(id_server: str) -> bool:\n    \"\"\"Check whether an identity server location, such as the one passed as the\n    `id_server` parameter to `/_matrix/client/r0/account/3pid/bind`, is valid.\n\n    A valid identity server location consists of a valid hostname and optional\n    port number, optionally followed by any number of `/` delimited path\n    components, without any fragment or query string parts.\n\n    Args:\n        id_server: identity server location string to validate\n\n    Returns:\n        True if valid, False otherwise.\n    \"\"\"\n\n    components = id_server.split(\"/\", 1)\n\n    host = components[0]\n\n    try:\n        parse_and_validate_server_name(host)\n    except ValueError:\n        return False\n\n    if len(components) < 2:\n        # no path\n        return True\n\n    path = components[1]\n    return \"#\" not in path and \"?\" not in path\n\n\ndef parse_and_validate_mxc_uri(mxc: str) -> Tuple[str, Optional[int], str]:\n    \"\"\"Parse the given string as an MXC URI\n\n    Checks that the \"server name\" part is a valid server name\n\n    Args:\n        mxc: the (alleged) MXC URI to be checked\n    Returns:\n        hostname, port, media id\n    Raises:\n        ValueError if the URI cannot be parsed\n    \"\"\"\n    m = MXC_REGEX.match(mxc)\n    if not m:\n        raise ValueError(\"mxc URI %r did not match expected format\" % (mxc,))\n    server_name = m.group(1)\n    media_id = m.group(2)\n    host, port = parse_and_validate_server_name(server_name)\n    return host, port, media_id\n\n\ndef shortstr(iterable: Iterable, maxitems: int = 5) -> str:\n    \"\"\"If iterable has maxitems or fewer, return the stringification of a list\n    containing those items.\n\n    Otherwise, return the stringification of a a list with the first maxitems items,\n    followed by \"...\".\n\n    Args:\n        iterable: iterable to truncate\n        maxitems: number of items to return before truncating\n    \"\"\"\n\n    items = list(itertools.islice(iterable, maxitems + 1))\n    if len(items) <= maxitems:\n        return str(items)\n    return \"[\" + \", \".join(repr(r) for r in items[:maxitems]) + \", ...]\"\n\n\ndef strtobool(val: str) -> bool:\n    \"\"\"Convert a string representation of truth to True or False\n\n    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n    'val' is anything else.\n\n    This is lifted from distutils.util.strtobool, with the exception that it actually\n    returns a bool, rather than an int.\n    \"\"\"\n    val = val.lower()\n    if val in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\n        return True\n    elif val in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\n        return False\n    else:\n        raise ValueError(\"invalid truth value %r\" % (val,))\n\n\n_BASE62 = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n\n\ndef base62_encode(num: int, minwidth: int = 1) -> str:\n    \"\"\"Encode a number using base62\n\n    Args:\n        num: number to be encoded\n        minwidth: width to pad to, if the number is small\n    \"\"\"\n    res = \"\"\n    while num:\n        num, rem = divmod(num, 62)\n        res = _BASE62[rem] + res\n\n    # pad to minimum width\n    pad = \"0\" * (minwidth - len(res))\n    return pad + res\n", "code_before": "# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport itertools\nimport re\nimport secrets\nimport string\nfrom collections.abc import Iterable\nfrom typing import Optional, Tuple\n\nfrom synapse.api.errors import Codes, SynapseError\n\n_string_with_symbols = string.digits + string.ascii_letters + \".,;:^&*-_+=#~@\"\n\n# https://matrix.org/docs/spec/client_server/r0.6.0#post-matrix-client-r0-register-email-requesttoken\nCLIENT_SECRET_REGEX = re.compile(r\"^[0-9a-zA-Z\\.=_\\-]+$\")\n\n# https://matrix.org/docs/spec/client_server/r0.6.1#matrix-content-mxc-uris,\n# together with https://github.com/matrix-org/matrix-doc/issues/2177 which basically\n# says \"there is no grammar for media ids\"\n#\n# The server_name part of this is purposely lax: use parse_and_validate_mxc for\n# additional validation.\n#\nMXC_REGEX = re.compile(\"^mxc://([^/]+)/([^/#?]+)$\")\n\n\ndef random_string(length: int) -> str:\n    \"\"\"Generate a cryptographically secure string of random letters.\n\n    Drawn from the characters: `a-z` and `A-Z`\n    \"\"\"\n    return \"\".join(secrets.choice(string.ascii_letters) for _ in range(length))\n\n\ndef random_string_with_symbols(length: int) -> str:\n    \"\"\"Generate a cryptographically secure string of random letters/numbers/symbols.\n\n    Drawn from the characters: `a-z`, `A-Z`, `0-9`, and `.,;:^&*-_+=#~@`\n    \"\"\"\n    return \"\".join(secrets.choice(_string_with_symbols) for _ in range(length))\n\n\ndef is_ascii(s: bytes) -> bool:\n    try:\n        s.decode(\"ascii\").encode(\"ascii\")\n    except UnicodeError:\n        return False\n    return True\n\n\ndef assert_valid_client_secret(client_secret: str) -> None:\n    \"\"\"Validate that a given string matches the client_secret defined by the spec\"\"\"\n    if (\n        len(client_secret) <= 0\n        or len(client_secret) > 255\n        or CLIENT_SECRET_REGEX.match(client_secret) is None\n    ):\n        raise SynapseError(\n            400, \"Invalid client_secret parameter\", errcode=Codes.INVALID_PARAM\n        )\n\n\ndef parse_server_name(server_name: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Split a server name into host/port parts.\n\n    Args:\n        server_name: server name to parse\n\n    Returns:\n        host/port parts.\n\n    Raises:\n        ValueError if the server name could not be parsed.\n    \"\"\"\n    try:\n        if server_name[-1] == \"]\":\n            # ipv6 literal, hopefully\n            return server_name, None\n\n        domain_port = server_name.rsplit(\":\", 1)\n        domain = domain_port[0]\n        port = int(domain_port[1]) if domain_port[1:] else None\n        return domain, port\n    except Exception:\n        raise ValueError(\"Invalid server name '%s'\" % server_name)\n\n\nVALID_HOST_REGEX = re.compile(\"\\\\A[0-9a-zA-Z.-]+\\\\Z\")\n\n\ndef parse_and_validate_server_name(server_name: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Split a server name into host/port parts and do some basic validation.\n\n    Args:\n        server_name: server name to parse\n\n    Returns:\n        host/port parts.\n\n    Raises:\n        ValueError if the server name could not be parsed.\n    \"\"\"\n    host, port = parse_server_name(server_name)\n\n    # these tests don't need to be bulletproof as we'll find out soon enough\n    # if somebody is giving us invalid data. What we *do* need is to be sure\n    # that nobody is sneaking IP literals in that look like hostnames, etc.\n\n    # look for ipv6 literals\n    if host[0] == \"[\":\n        if host[-1] != \"]\":\n            raise ValueError(\"Mismatched [...] in server name '%s'\" % (server_name,))\n        return host, port\n\n    # otherwise it should only be alphanumerics.\n    if not VALID_HOST_REGEX.match(host):\n        raise ValueError(\n            \"Server name '%s' contains invalid characters\" % (server_name,)\n        )\n\n    return host, port\n\n\ndef valid_id_server_location(id_server: str) -> bool:\n    \"\"\"Check whether an identity server location, such as the one passed as the\n    `id_server` parameter to `/_matrix/client/r0/account/3pid/bind`, is valid.\n\n    A valid identity server location consists of a valid hostname and optional\n    port number, optionally followed by any number of `/` delimited path\n    components, without any fragment or query string parts.\n\n    Args:\n        id_server: identity server location string to validate\n\n    Returns:\n        True if valid, False otherwise.\n    \"\"\"\n\n    components = id_server.split(\"/\", 1)\n\n    host = components[0]\n\n    try:\n        parse_and_validate_server_name(host)\n    except ValueError:\n        return False\n\n    if len(components) < 2:\n        # no path\n        return True\n\n    path = components[1]\n    return \"#\" not in path and \"?\" not in path\n\n\ndef parse_and_validate_mxc_uri(mxc: str) -> Tuple[str, Optional[int], str]:\n    \"\"\"Parse the given string as an MXC URI\n\n    Checks that the \"server name\" part is a valid server name\n\n    Args:\n        mxc: the (alleged) MXC URI to be checked\n    Returns:\n        hostname, port, media id\n    Raises:\n        ValueError if the URI cannot be parsed\n    \"\"\"\n    m = MXC_REGEX.match(mxc)\n    if not m:\n        raise ValueError(\"mxc URI %r did not match expected format\" % (mxc,))\n    server_name = m.group(1)\n    media_id = m.group(2)\n    host, port = parse_and_validate_server_name(server_name)\n    return host, port, media_id\n\n\ndef shortstr(iterable: Iterable, maxitems: int = 5) -> str:\n    \"\"\"If iterable has maxitems or fewer, return the stringification of a list\n    containing those items.\n\n    Otherwise, return the stringification of a a list with the first maxitems items,\n    followed by \"...\".\n\n    Args:\n        iterable: iterable to truncate\n        maxitems: number of items to return before truncating\n    \"\"\"\n\n    items = list(itertools.islice(iterable, maxitems + 1))\n    if len(items) <= maxitems:\n        return str(items)\n    return \"[\" + \", \".join(repr(r) for r in items[:maxitems]) + \", ...]\"\n\n\ndef strtobool(val: str) -> bool:\n    \"\"\"Convert a string representation of truth to True or False\n\n    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n    'val' is anything else.\n\n    This is lifted from distutils.util.strtobool, with the exception that it actually\n    returns a bool, rather than an int.\n    \"\"\"\n    val = val.lower()\n    if val in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\n        return True\n    elif val in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\n        return False\n    else:\n        raise ValueError(\"invalid truth value %r\" % (val,))\n\n\n_BASE62 = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n\n\ndef base62_encode(num: int, minwidth: int = 1) -> str:\n    \"\"\"Encode a number using base62\n\n    Args:\n        num: number to be encoded\n        minwidth: width to pad to, if the number is small\n    \"\"\"\n    res = \"\"\n    while num:\n        num, rem = divmod(num, 62)\n        res = _BASE62[rem] + res\n\n    # pad to minimum width\n    pad = \"0\" * (minwidth - len(res))\n    return pad + res\n", "patch": "@@ -19,6 +19,8 @@\n from collections.abc import Iterable\n from typing import Optional, Tuple\n \n+from netaddr import valid_ipv6\n+\n from synapse.api.errors import Codes, SynapseError\n \n _string_with_symbols = string.digits + string.ascii_letters + \".,;:^&*-_+=#~@\"\n@@ -97,7 +99,10 @@ def parse_server_name(server_name: str) -> Tuple[str, Optional[int]]:\n         raise ValueError(\"Invalid server name '%s'\" % server_name)\n \n \n-VALID_HOST_REGEX = re.compile(\"\\\\A[0-9a-zA-Z.-]+\\\\Z\")\n+# An approximation of the domain name syntax in RFC 1035, section 2.3.1.\n+# NB: \"\\Z\" is not equivalent to \"$\".\n+#     The latter will match the position before a \"\\n\" at the end of a string.\n+VALID_HOST_REGEX = re.compile(\"\\\\A[0-9a-zA-Z-]+(?:\\\\.[0-9a-zA-Z-]+)*\\\\Z\")\n \n \n def parse_and_validate_server_name(server_name: str) -> Tuple[str, Optional[int]]:\n@@ -122,13 +127,15 @@ def parse_and_validate_server_name(server_name: str) -> Tuple[str, Optional[int]\n     if host[0] == \"[\":\n         if host[-1] != \"]\":\n             raise ValueError(\"Mismatched [...] in server name '%s'\" % (server_name,))\n-        return host, port\n \n-    # otherwise it should only be alphanumerics.\n-    if not VALID_HOST_REGEX.match(host):\n-        raise ValueError(\n-            \"Server name '%s' contains invalid characters\" % (server_name,)\n-        )\n+        # valid_ipv6 raises when given an empty string\n+        ipv6_address = host[1:-1]\n+        if not ipv6_address or not valid_ipv6(ipv6_address):\n+            raise ValueError(\n+                \"Server name '%s' is not a valid IPv6 address\" % (server_name,)\n+            )\n+    elif not VALID_HOST_REGEX.match(host):\n+        raise ValueError(\"Server name '%s' has an invalid format\" % (server_name,))\n \n     return host, port\n ", "file_path": "files/2021_11/67", "file_language": "py", "file_name": "synapse/util/stringutils.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/matrix-org/synapse/raw/91f2bd0907f1d05af67166846988e49644eb650c/tests%2Fhttp%2Ftest_endpoint.py", "code": "# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom synapse.util.stringutils import parse_and_validate_server_name, parse_server_name\n\nfrom tests import unittest\n\n\nclass ServerNameTestCase(unittest.TestCase):\n    def test_parse_server_name(self):\n        test_data = {\n            \"localhost\": (\"localhost\", None),\n            \"my-example.com:1234\": (\"my-example.com\", 1234),\n            \"1.2.3.4\": (\"1.2.3.4\", None),\n            \"[0abc:1def::1234]\": (\"[0abc:1def::1234]\", None),\n            \"1.2.3.4:1\": (\"1.2.3.4\", 1),\n            \"[0abc:1def::1234]:8080\": (\"[0abc:1def::1234]\", 8080),\n        }\n\n        for i, o in test_data.items():\n            self.assertEqual(parse_server_name(i), o)\n\n    def test_validate_bad_server_names(self):\n        test_data = [\n            \"\",  # empty\n            \"localhost:http\",  # non-numeric port\n            \"1234]\",  # smells like ipv6 literal but isn't\n            \"[1234\",\n            \"[1.2.3.4]\",\n            \"underscore_.com\",\n            \"percent%65.com\",\n            \"newline.com\\n\",\n            \".empty-label.com\",\n            \"1234:5678:80\",  # too many colons\n        ]\n        for i in test_data:\n            try:\n                parse_and_validate_server_name(i)\n                self.fail(\n                    \"Expected parse_and_validate_server_name('%s') to throw\" % (i,)\n                )\n            except ValueError:\n                pass\n", "code_before": "# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom synapse.util.stringutils import parse_and_validate_server_name, parse_server_name\n\nfrom tests import unittest\n\n\nclass ServerNameTestCase(unittest.TestCase):\n    def test_parse_server_name(self):\n        test_data = {\n            \"localhost\": (\"localhost\", None),\n            \"my-example.com:1234\": (\"my-example.com\", 1234),\n            \"1.2.3.4\": (\"1.2.3.4\", None),\n            \"[0abc:1def::1234]\": (\"[0abc:1def::1234]\", None),\n            \"1.2.3.4:1\": (\"1.2.3.4\", 1),\n            \"[0abc:1def::1234]:8080\": (\"[0abc:1def::1234]\", 8080),\n        }\n\n        for i, o in test_data.items():\n            self.assertEqual(parse_server_name(i), o)\n\n    def test_validate_bad_server_names(self):\n        test_data = [\n            \"\",  # empty\n            \"localhost:http\",  # non-numeric port\n            \"1234]\",  # smells like ipv6 literal but isn't\n            \"[1234\",\n            \"underscore_.com\",\n            \"percent%65.com\",\n            \"1234:5678:80\",  # too many colons\n        ]\n        for i in test_data:\n            try:\n                parse_and_validate_server_name(i)\n                self.fail(\n                    \"Expected parse_and_validate_server_name('%s') to throw\" % (i,)\n                )\n            except ValueError:\n                pass\n", "patch": "@@ -36,8 +36,11 @@ def test_validate_bad_server_names(self):\n             \"localhost:http\",  # non-numeric port\n             \"1234]\",  # smells like ipv6 literal but isn't\n             \"[1234\",\n+            \"[1.2.3.4]\",\n             \"underscore_.com\",\n             \"percent%65.com\",\n+            \"newline.com\\n\",\n+            \".empty-label.com\",\n             \"1234:5678:80\",  # too many colons\n         ]\n         for i in test_data:", "file_path": "files/2021_11/68", "file_language": "py", "file_name": "tests/http/test_endpoint.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class ServerNameTestCase(unittest.TestCase):\n    def test_parse_server_name(self):\n        test_data = {\n            \"localhost\": (\"localhost\", None),\n            \"my-example.com:1234\": (\"my-example.com\", 1234),\n            \"1.2.3.4\": (\"1.2.3.4\", None),\n            \"[0abc:1def::1234]\": (\"[0abc:1def::1234]\", None),\n            \"1.2.3.4:1\": (\"1.2.3.4\", 1),\n            \"[0abc:1def::1234]:8080\": (\"[0abc:1def::1234]\", 8080),\n        }\n\n        for i, o in test_data.items():\n            self.assertEqual(parse_server_name(i), o)\n\n    def test_validate_bad_server_names(self):\n        test_data = [\n            \"\",  # empty\n            \"localhost:http\",  # non-numeric port\n            \"1234]\",  # smells like ipv6 literal but isn't\n            \"[1234\",\n            \"underscore_.com\",\n            \"percent%65.com\",\n            \"1234:5678:80\",  # too many colons\n        ]\n        for i in test_data:\n            try:\n                parse_and_validate_server_name(i)\n                self.fail(\n                    \"Expected parse_and_validate_server_name('%s') to throw\" % (i,)\n                )\n            except ValueError:\n                pass", "target": 0}], "function_after": [{"function": "class ServerNameTestCase(unittest.TestCase):\n    def test_parse_server_name(self):\n        test_data = {\n            \"localhost\": (\"localhost\", None),\n            \"my-example.com:1234\": (\"my-example.com\", 1234),\n            \"1.2.3.4\": (\"1.2.3.4\", None),\n            \"[0abc:1def::1234]\": (\"[0abc:1def::1234]\", None),\n            \"1.2.3.4:1\": (\"1.2.3.4\", 1),\n            \"[0abc:1def::1234]:8080\": (\"[0abc:1def::1234]\", 8080),\n        }\n\n        for i, o in test_data.items():\n            self.assertEqual(parse_server_name(i), o)\n\n    def test_validate_bad_server_names(self):\n        test_data = [\n            \"\",  # empty\n            \"localhost:http\",  # non-numeric port\n            \"1234]\",  # smells like ipv6 literal but isn't\n            \"[1234\",\n            \"[1.2.3.4]\",\n            \"underscore_.com\",\n            \"percent%65.com\",\n            \"newline.com\\n\",\n            \".empty-label.com\",\n            \"1234:5678:80\",  # too many colons\n        ]\n        for i in test_data:\n            try:\n                parse_and_validate_server_name(i)\n                self.fail(\n                    \"Expected parse_and_validate_server_name('%s') to throw\" % (i,)\n                )\n            except ValueError:\n                pass", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/91f2bd0907f1d05af67166846988e49644eb650c/tests%2Frest%2Fmedia%2Fv1%2Ftest_filepath.py", "code": "# Copyright 2021 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport inspect\nfrom typing import Iterable\n\nfrom synapse.rest.media.v1.filepath import MediaFilePaths\n\nfrom tests import unittest\n\n\nclass MediaFilePathsTestCase(unittest.TestCase):\n    def setUp(self):\n        super().setUp()\n\n        self.filepaths = MediaFilePaths(\"/media_store\")\n\n    def test_local_media_filepath(self):\n        \"\"\"Test local media paths\"\"\"\n        self.assertEqual(\n            self.filepaths.local_media_filepath_rel(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"local_content/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n        self.assertEqual(\n            self.filepaths.local_media_filepath(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"/media_store/local_content/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_local_media_thumbnail(self):\n        \"\"\"Test local media thumbnail paths\"\"\"\n        self.assertEqual(\n            self.filepaths.local_media_thumbnail_rel(\n                \"GerZNDnDZVjsOtardLuwfIBg\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"local_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n        self.assertEqual(\n            self.filepaths.local_media_thumbnail(\n                \"GerZNDnDZVjsOtardLuwfIBg\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"/media_store/local_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n\n    def test_local_media_thumbnail_dir(self):\n        \"\"\"Test local media thumbnail directory paths\"\"\"\n        self.assertEqual(\n            self.filepaths.local_media_thumbnail_dir(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"/media_store/local_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_remote_media_filepath(self):\n        \"\"\"Test remote media paths\"\"\"\n        self.assertEqual(\n            self.filepaths.remote_media_filepath_rel(\n                \"example.com\", \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            \"remote_content/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n        self.assertEqual(\n            self.filepaths.remote_media_filepath(\n                \"example.com\", \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            \"/media_store/remote_content/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_remote_media_thumbnail(self):\n        \"\"\"Test remote media thumbnail paths\"\"\"\n        self.assertEqual(\n            self.filepaths.remote_media_thumbnail_rel(\n                \"example.com\",\n                \"GerZNDnDZVjsOtardLuwfIBg\",\n                800,\n                600,\n                \"image/jpeg\",\n                \"scale\",\n            ),\n            \"remote_thumbnail/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n        self.assertEqual(\n            self.filepaths.remote_media_thumbnail(\n                \"example.com\",\n                \"GerZNDnDZVjsOtardLuwfIBg\",\n                800,\n                600,\n                \"image/jpeg\",\n                \"scale\",\n            ),\n            \"/media_store/remote_thumbnail/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n\n    def test_remote_media_thumbnail_legacy(self):\n        \"\"\"Test old-style remote media thumbnail paths\"\"\"\n        self.assertEqual(\n            self.filepaths.remote_media_thumbnail_rel_legacy(\n                \"example.com\", \"GerZNDnDZVjsOtardLuwfIBg\", 800, 600, \"image/jpeg\"\n            ),\n            \"remote_thumbnail/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg\",\n        )\n\n    def test_remote_media_thumbnail_dir(self):\n        \"\"\"Test remote media thumbnail directory paths\"\"\"\n        self.assertEqual(\n            self.filepaths.remote_media_thumbnail_dir(\n                \"example.com\", \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            \"/media_store/remote_thumbnail/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_url_cache_filepath(self):\n        \"\"\"Test URL cache paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_filepath_rel(\"2020-01-02_GerZNDnDZVjsOtar\"),\n            \"url_cache/2020-01-02/GerZNDnDZVjsOtar\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_filepath(\"2020-01-02_GerZNDnDZVjsOtar\"),\n            \"/media_store/url_cache/2020-01-02/GerZNDnDZVjsOtar\",\n        )\n\n    def test_url_cache_filepath_legacy(self):\n        \"\"\"Test old-style URL cache paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_filepath_rel(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"url_cache/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_filepath(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"/media_store/url_cache/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_url_cache_filepath_dirs_to_delete(self):\n        \"\"\"Test URL cache cleanup paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_filepath_dirs_to_delete(\n                \"2020-01-02_GerZNDnDZVjsOtar\"\n            ),\n            [\"/media_store/url_cache/2020-01-02\"],\n        )\n\n    def test_url_cache_filepath_dirs_to_delete_legacy(self):\n        \"\"\"Test old-style URL cache cleanup paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_filepath_dirs_to_delete(\n                \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            [\n                \"/media_store/url_cache/Ge/rZ\",\n                \"/media_store/url_cache/Ge\",\n            ],\n        )\n\n    def test_url_cache_thumbnail(self):\n        \"\"\"Test URL cache thumbnail paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_rel(\n                \"2020-01-02_GerZNDnDZVjsOtar\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"url_cache_thumbnails/2020-01-02/GerZNDnDZVjsOtar/800-600-image-jpeg-scale\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail(\n                \"2020-01-02_GerZNDnDZVjsOtar\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"/media_store/url_cache_thumbnails/2020-01-02/GerZNDnDZVjsOtar/800-600-image-jpeg-scale\",\n        )\n\n    def test_url_cache_thumbnail_legacy(self):\n        \"\"\"Test old-style URL cache thumbnail paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_rel(\n                \"GerZNDnDZVjsOtardLuwfIBg\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"url_cache_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail(\n                \"GerZNDnDZVjsOtardLuwfIBg\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"/media_store/url_cache_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n\n    def test_url_cache_thumbnail_directory(self):\n        \"\"\"Test URL cache thumbnail directory paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_directory_rel(\n                \"2020-01-02_GerZNDnDZVjsOtar\"\n            ),\n            \"url_cache_thumbnails/2020-01-02/GerZNDnDZVjsOtar\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_directory(\"2020-01-02_GerZNDnDZVjsOtar\"),\n            \"/media_store/url_cache_thumbnails/2020-01-02/GerZNDnDZVjsOtar\",\n        )\n\n    def test_url_cache_thumbnail_directory_legacy(self):\n        \"\"\"Test old-style URL cache thumbnail directory paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_directory_rel(\n                \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            \"url_cache_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_directory(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"/media_store/url_cache_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_url_cache_thumbnail_dirs_to_delete(self):\n        \"\"\"Test URL cache thumbnail cleanup paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_dirs_to_delete(\n                \"2020-01-02_GerZNDnDZVjsOtar\"\n            ),\n            [\n                \"/media_store/url_cache_thumbnails/2020-01-02/GerZNDnDZVjsOtar\",\n                \"/media_store/url_cache_thumbnails/2020-01-02\",\n            ],\n        )\n\n    def test_url_cache_thumbnail_dirs_to_delete_legacy(self):\n        \"\"\"Test old-style URL cache thumbnail cleanup paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_dirs_to_delete(\n                \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            [\n                \"/media_store/url_cache_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n                \"/media_store/url_cache_thumbnails/Ge/rZ\",\n                \"/media_store/url_cache_thumbnails/Ge\",\n            ],\n        )\n\n    def test_server_name_validation(self):\n        \"\"\"Test validation of server names\"\"\"\n        self._test_path_validation(\n            [\n                \"remote_media_filepath_rel\",\n                \"remote_media_filepath\",\n                \"remote_media_thumbnail_rel\",\n                \"remote_media_thumbnail\",\n                \"remote_media_thumbnail_rel_legacy\",\n                \"remote_media_thumbnail_dir\",\n            ],\n            parameter=\"server_name\",\n            valid_values=[\n                \"matrix.org\",\n                \"matrix.org:8448\",\n                \"matrix-federation.matrix.org\",\n                \"matrix-federation.matrix.org:8448\",\n                \"10.1.12.123\",\n                \"10.1.12.123:8448\",\n                \"[fd00:abcd::ffff]\",\n                \"[fd00:abcd::ffff]:8448\",\n            ],\n            invalid_values=[\n                \"/matrix.org\",\n                \"matrix.org/..\",\n                \"matrix.org\\x00\",\n                \"\",\n                \".\",\n                \"..\",\n                \"/\",\n            ],\n        )\n\n    def test_file_id_validation(self):\n        \"\"\"Test validation of local, remote and legacy URL cache file / media IDs\"\"\"\n        # File / media IDs get split into three parts to form paths, consisting of the\n        # first two characters, next two characters and rest of the ID.\n        valid_file_ids = [\n            \"GerZNDnDZVjsOtardLuwfIBg\",\n            # Unexpected, but produces an acceptable path:\n            \"GerZN\",  # \"N\" becomes the last directory\n        ]\n        invalid_file_ids = [\n            \"/erZNDnDZVjsOtardLuwfIBg\",\n            \"Ge/ZNDnDZVjsOtardLuwfIBg\",\n            \"GerZ/DnDZVjsOtardLuwfIBg\",\n            \"GerZ/..\",\n            \"G\\x00rZNDnDZVjsOtardLuwfIBg\",\n            \"Ger\\x00NDnDZVjsOtardLuwfIBg\",\n            \"GerZNDnDZVjsOtardLuwfIBg\\x00\",\n            \"\",\n            \"Ge\",\n            \"GerZ\",\n            \"GerZ.\",\n            \"..rZNDnDZVjsOtardLuwfIBg\",\n            \"Ge..NDnDZVjsOtardLuwfIBg\",\n            \"GerZ..\",\n            \"GerZ/\",\n        ]\n\n        self._test_path_validation(\n            [\n                \"local_media_filepath_rel\",\n                \"local_media_filepath\",\n                \"local_media_thumbnail_rel\",\n                \"local_media_thumbnail\",\n                \"local_media_thumbnail_dir\",\n                # Legacy URL cache media IDs\n                \"url_cache_filepath_rel\",\n                \"url_cache_filepath\",\n                # `url_cache_filepath_dirs_to_delete` is tested below.\n                \"url_cache_thumbnail_rel\",\n                \"url_cache_thumbnail\",\n                \"url_cache_thumbnail_directory_rel\",\n                \"url_cache_thumbnail_directory\",\n                \"url_cache_thumbnail_dirs_to_delete\",\n            ],\n            parameter=\"media_id\",\n            valid_values=valid_file_ids,\n            invalid_values=invalid_file_ids,\n        )\n\n        # `url_cache_filepath_dirs_to_delete` ignores what would be the last path\n        # component, so only the first 4 characters matter.\n        self._test_path_validation(\n            [\n                \"url_cache_filepath_dirs_to_delete\",\n            ],\n            parameter=\"media_id\",\n            valid_values=valid_file_ids,\n            invalid_values=[\n                \"/erZNDnDZVjsOtardLuwfIBg\",\n                \"Ge/ZNDnDZVjsOtardLuwfIBg\",\n                \"G\\x00rZNDnDZVjsOtardLuwfIBg\",\n                \"Ger\\x00NDnDZVjsOtardLuwfIBg\",\n                \"\",\n                \"Ge\",\n                \"..rZNDnDZVjsOtardLuwfIBg\",\n                \"Ge..NDnDZVjsOtardLuwfIBg\",\n            ],\n        )\n\n        self._test_path_validation(\n            [\n                \"remote_media_filepath_rel\",\n                \"remote_media_filepath\",\n                \"remote_media_thumbnail_rel\",\n                \"remote_media_thumbnail\",\n                \"remote_media_thumbnail_rel_legacy\",\n                \"remote_media_thumbnail_dir\",\n            ],\n            parameter=\"file_id\",\n            valid_values=valid_file_ids,\n            invalid_values=invalid_file_ids,\n        )\n\n    def test_url_cache_media_id_validation(self):\n        \"\"\"Test validation of URL cache media IDs\"\"\"\n        self._test_path_validation(\n            [\n                \"url_cache_filepath_rel\",\n                \"url_cache_filepath\",\n                # `url_cache_filepath_dirs_to_delete` only cares about the date prefix\n                \"url_cache_thumbnail_rel\",\n                \"url_cache_thumbnail\",\n                \"url_cache_thumbnail_directory_rel\",\n                \"url_cache_thumbnail_directory\",\n                \"url_cache_thumbnail_dirs_to_delete\",\n            ],\n            parameter=\"media_id\",\n            valid_values=[\n                \"2020-01-02_GerZNDnDZVjsOtar\",\n                \"2020-01-02_G\",  # Unexpected, but produces an acceptable path\n            ],\n            invalid_values=[\n                \"2020-01-02\",\n                \"2020-01-02-\",\n                \"2020-01-02-.\",\n                \"2020-01-02-..\",\n                \"2020-01-02-/\",\n                \"2020-01-02-/GerZNDnDZVjsOtar\",\n                \"2020-01-02-GerZNDnDZVjsOtar/..\",\n                \"2020-01-02-GerZNDnDZVjsOtar\\x00\",\n            ],\n        )\n\n    def test_content_type_validation(self):\n        \"\"\"Test validation of thumbnail content types\"\"\"\n        self._test_path_validation(\n            [\n                \"local_media_thumbnail_rel\",\n                \"local_media_thumbnail\",\n                \"remote_media_thumbnail_rel\",\n                \"remote_media_thumbnail\",\n                \"remote_media_thumbnail_rel_legacy\",\n                \"url_cache_thumbnail_rel\",\n                \"url_cache_thumbnail\",\n            ],\n            parameter=\"content_type\",\n            valid_values=[\n                \"image/jpeg\",\n            ],\n            invalid_values=[\n                \"\",  # ValueError: not enough values to unpack\n                \"image/jpeg/abc\",  # ValueError: too many values to unpack\n                \"image/jpeg\\x00\",\n            ],\n        )\n\n    def test_thumbnail_method_validation(self):\n        \"\"\"Test validation of thumbnail methods\"\"\"\n        self._test_path_validation(\n            [\n                \"local_media_thumbnail_rel\",\n                \"local_media_thumbnail\",\n                \"remote_media_thumbnail_rel\",\n                \"remote_media_thumbnail\",\n                \"url_cache_thumbnail_rel\",\n                \"url_cache_thumbnail\",\n            ],\n            parameter=\"method\",\n            valid_values=[\n                \"crop\",\n                \"scale\",\n            ],\n            invalid_values=[\n                \"/scale\",\n                \"scale/..\",\n                \"scale\\x00\",\n                \"/\",\n            ],\n        )\n\n    def _test_path_validation(\n        self,\n        methods: Iterable[str],\n        parameter: str,\n        valid_values: Iterable[str],\n        invalid_values: Iterable[str],\n    ):\n        \"\"\"Test that the specified methods validate the named parameter as expected\n\n        Args:\n            methods: The names of `MediaFilePaths` methods to test\n            parameter: The name of the parameter to test\n            valid_values: A list of parameter values that are expected to be accepted\n            invalid_values: A list of parameter values that are expected to be rejected\n\n        Raises:\n            AssertionError: If a value was accepted when it should have failed\n                validation.\n            ValueError: If a value failed validation when it should have been accepted.\n        \"\"\"\n        for method in methods:\n            get_path = getattr(self.filepaths, method)\n\n            parameters = inspect.signature(get_path).parameters\n            kwargs = {\n                \"server_name\": \"matrix.org\",\n                \"media_id\": \"GerZNDnDZVjsOtardLuwfIBg\",\n                \"file_id\": \"GerZNDnDZVjsOtardLuwfIBg\",\n                \"width\": 800,\n                \"height\": 600,\n                \"content_type\": \"image/jpeg\",\n                \"method\": \"scale\",\n            }\n\n            if get_path.__name__.startswith(\"url_\"):\n                kwargs[\"media_id\"] = \"2020-01-02_GerZNDnDZVjsOtar\"\n\n            kwargs = {k: v for k, v in kwargs.items() if k in parameters}\n            kwargs.pop(parameter)\n\n            for value in valid_values:\n                kwargs[parameter] = value\n                get_path(**kwargs)\n                # No exception should be raised\n\n            for value in invalid_values:\n                with self.assertRaises(ValueError):\n                    kwargs[parameter] = value\n                    path_or_list = get_path(**kwargs)\n                    self.fail(\n                        f\"{value!r} unexpectedly passed validation: \"\n                        f\"{method} returned {path_or_list!r}\"\n                    )\n", "code_before": "# Copyright 2021 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom synapse.rest.media.v1.filepath import MediaFilePaths\n\nfrom tests import unittest\n\n\nclass MediaFilePathsTestCase(unittest.TestCase):\n    def setUp(self):\n        super().setUp()\n\n        self.filepaths = MediaFilePaths(\"/media_store\")\n\n    def test_local_media_filepath(self):\n        \"\"\"Test local media paths\"\"\"\n        self.assertEqual(\n            self.filepaths.local_media_filepath_rel(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"local_content/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n        self.assertEqual(\n            self.filepaths.local_media_filepath(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"/media_store/local_content/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_local_media_thumbnail(self):\n        \"\"\"Test local media thumbnail paths\"\"\"\n        self.assertEqual(\n            self.filepaths.local_media_thumbnail_rel(\n                \"GerZNDnDZVjsOtardLuwfIBg\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"local_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n        self.assertEqual(\n            self.filepaths.local_media_thumbnail(\n                \"GerZNDnDZVjsOtardLuwfIBg\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"/media_store/local_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n\n    def test_local_media_thumbnail_dir(self):\n        \"\"\"Test local media thumbnail directory paths\"\"\"\n        self.assertEqual(\n            self.filepaths.local_media_thumbnail_dir(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"/media_store/local_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_remote_media_filepath(self):\n        \"\"\"Test remote media paths\"\"\"\n        self.assertEqual(\n            self.filepaths.remote_media_filepath_rel(\n                \"example.com\", \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            \"remote_content/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n        self.assertEqual(\n            self.filepaths.remote_media_filepath(\n                \"example.com\", \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            \"/media_store/remote_content/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_remote_media_thumbnail(self):\n        \"\"\"Test remote media thumbnail paths\"\"\"\n        self.assertEqual(\n            self.filepaths.remote_media_thumbnail_rel(\n                \"example.com\",\n                \"GerZNDnDZVjsOtardLuwfIBg\",\n                800,\n                600,\n                \"image/jpeg\",\n                \"scale\",\n            ),\n            \"remote_thumbnail/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n        self.assertEqual(\n            self.filepaths.remote_media_thumbnail(\n                \"example.com\",\n                \"GerZNDnDZVjsOtardLuwfIBg\",\n                800,\n                600,\n                \"image/jpeg\",\n                \"scale\",\n            ),\n            \"/media_store/remote_thumbnail/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n\n    def test_remote_media_thumbnail_legacy(self):\n        \"\"\"Test old-style remote media thumbnail paths\"\"\"\n        self.assertEqual(\n            self.filepaths.remote_media_thumbnail_rel_legacy(\n                \"example.com\", \"GerZNDnDZVjsOtardLuwfIBg\", 800, 600, \"image/jpeg\"\n            ),\n            \"remote_thumbnail/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg\",\n        )\n\n    def test_remote_media_thumbnail_dir(self):\n        \"\"\"Test remote media thumbnail directory paths\"\"\"\n        self.assertEqual(\n            self.filepaths.remote_media_thumbnail_dir(\n                \"example.com\", \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            \"/media_store/remote_thumbnail/example.com/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_url_cache_filepath(self):\n        \"\"\"Test URL cache paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_filepath_rel(\"2020-01-02_GerZNDnDZVjsOtar\"),\n            \"url_cache/2020-01-02/GerZNDnDZVjsOtar\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_filepath(\"2020-01-02_GerZNDnDZVjsOtar\"),\n            \"/media_store/url_cache/2020-01-02/GerZNDnDZVjsOtar\",\n        )\n\n    def test_url_cache_filepath_legacy(self):\n        \"\"\"Test old-style URL cache paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_filepath_rel(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"url_cache/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_filepath(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"/media_store/url_cache/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_url_cache_filepath_dirs_to_delete(self):\n        \"\"\"Test URL cache cleanup paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_filepath_dirs_to_delete(\n                \"2020-01-02_GerZNDnDZVjsOtar\"\n            ),\n            [\"/media_store/url_cache/2020-01-02\"],\n        )\n\n    def test_url_cache_filepath_dirs_to_delete_legacy(self):\n        \"\"\"Test old-style URL cache cleanup paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_filepath_dirs_to_delete(\n                \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            [\n                \"/media_store/url_cache/Ge/rZ\",\n                \"/media_store/url_cache/Ge\",\n            ],\n        )\n\n    def test_url_cache_thumbnail(self):\n        \"\"\"Test URL cache thumbnail paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_rel(\n                \"2020-01-02_GerZNDnDZVjsOtar\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"url_cache_thumbnails/2020-01-02/GerZNDnDZVjsOtar/800-600-image-jpeg-scale\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail(\n                \"2020-01-02_GerZNDnDZVjsOtar\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"/media_store/url_cache_thumbnails/2020-01-02/GerZNDnDZVjsOtar/800-600-image-jpeg-scale\",\n        )\n\n    def test_url_cache_thumbnail_legacy(self):\n        \"\"\"Test old-style URL cache thumbnail paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_rel(\n                \"GerZNDnDZVjsOtardLuwfIBg\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"url_cache_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail(\n                \"GerZNDnDZVjsOtardLuwfIBg\", 800, 600, \"image/jpeg\", \"scale\"\n            ),\n            \"/media_store/url_cache_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg/800-600-image-jpeg-scale\",\n        )\n\n    def test_url_cache_thumbnail_directory(self):\n        \"\"\"Test URL cache thumbnail directory paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_directory_rel(\n                \"2020-01-02_GerZNDnDZVjsOtar\"\n            ),\n            \"url_cache_thumbnails/2020-01-02/GerZNDnDZVjsOtar\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_directory(\"2020-01-02_GerZNDnDZVjsOtar\"),\n            \"/media_store/url_cache_thumbnails/2020-01-02/GerZNDnDZVjsOtar\",\n        )\n\n    def test_url_cache_thumbnail_directory_legacy(self):\n        \"\"\"Test old-style URL cache thumbnail directory paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_directory_rel(\n                \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            \"url_cache_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_directory(\"GerZNDnDZVjsOtardLuwfIBg\"),\n            \"/media_store/url_cache_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n        )\n\n    def test_url_cache_thumbnail_dirs_to_delete(self):\n        \"\"\"Test URL cache thumbnail cleanup paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_dirs_to_delete(\n                \"2020-01-02_GerZNDnDZVjsOtar\"\n            ),\n            [\n                \"/media_store/url_cache_thumbnails/2020-01-02/GerZNDnDZVjsOtar\",\n                \"/media_store/url_cache_thumbnails/2020-01-02\",\n            ],\n        )\n\n    def test_url_cache_thumbnail_dirs_to_delete_legacy(self):\n        \"\"\"Test old-style URL cache thumbnail cleanup paths\"\"\"\n        self.assertEqual(\n            self.filepaths.url_cache_thumbnail_dirs_to_delete(\n                \"GerZNDnDZVjsOtardLuwfIBg\"\n            ),\n            [\n                \"/media_store/url_cache_thumbnails/Ge/rZ/NDnDZVjsOtardLuwfIBg\",\n                \"/media_store/url_cache_thumbnails/Ge/rZ\",\n                \"/media_store/url_cache_thumbnails/Ge\",\n            ],\n        )\n", "patch": "@@ -11,6 +11,9 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import inspect\n+from typing import Iterable\n+\n from synapse.rest.media.v1.filepath import MediaFilePaths\n \n from tests import unittest\n@@ -236,3 +239,250 @@ def test_url_cache_thumbnail_dirs_to_delete_legacy(self):\n                 \"/media_store/url_cache_thumbnails/Ge\",\n             ],\n         )\n+\n+    def test_server_name_validation(self):\n+        \"\"\"Test validation of server names\"\"\"\n+        self._test_path_validation(\n+            [\n+                \"remote_media_filepath_rel\",\n+                \"remote_media_filepath\",\n+                \"remote_media_thumbnail_rel\",\n+                \"remote_media_thumbnail\",\n+                \"remote_media_thumbnail_rel_legacy\",\n+                \"remote_media_thumbnail_dir\",\n+            ],\n+            parameter=\"server_name\",\n+            valid_values=[\n+                \"matrix.org\",\n+                \"matrix.org:8448\",\n+                \"matrix-federation.matrix.org\",\n+                \"matrix-federation.matrix.org:8448\",\n+                \"10.1.12.123\",\n+                \"10.1.12.123:8448\",\n+                \"[fd00:abcd::ffff]\",\n+                \"[fd00:abcd::ffff]:8448\",\n+            ],\n+            invalid_values=[\n+                \"/matrix.org\",\n+                \"matrix.org/..\",\n+                \"matrix.org\\x00\",\n+                \"\",\n+                \".\",\n+                \"..\",\n+                \"/\",\n+            ],\n+        )\n+\n+    def test_file_id_validation(self):\n+        \"\"\"Test validation of local, remote and legacy URL cache file / media IDs\"\"\"\n+        # File / media IDs get split into three parts to form paths, consisting of the\n+        # first two characters, next two characters and rest of the ID.\n+        valid_file_ids = [\n+            \"GerZNDnDZVjsOtardLuwfIBg\",\n+            # Unexpected, but produces an acceptable path:\n+            \"GerZN\",  # \"N\" becomes the last directory\n+        ]\n+        invalid_file_ids = [\n+            \"/erZNDnDZVjsOtardLuwfIBg\",\n+            \"Ge/ZNDnDZVjsOtardLuwfIBg\",\n+            \"GerZ/DnDZVjsOtardLuwfIBg\",\n+            \"GerZ/..\",\n+            \"G\\x00rZNDnDZVjsOtardLuwfIBg\",\n+            \"Ger\\x00NDnDZVjsOtardLuwfIBg\",\n+            \"GerZNDnDZVjsOtardLuwfIBg\\x00\",\n+            \"\",\n+            \"Ge\",\n+            \"GerZ\",\n+            \"GerZ.\",\n+            \"..rZNDnDZVjsOtardLuwfIBg\",\n+            \"Ge..NDnDZVjsOtardLuwfIBg\",\n+            \"GerZ..\",\n+            \"GerZ/\",\n+        ]\n+\n+        self._test_path_validation(\n+            [\n+                \"local_media_filepath_rel\",\n+                \"local_media_filepath\",\n+                \"local_media_thumbnail_rel\",\n+                \"local_media_thumbnail\",\n+                \"local_media_thumbnail_dir\",\n+                # Legacy URL cache media IDs\n+                \"url_cache_filepath_rel\",\n+                \"url_cache_filepath\",\n+                # `url_cache_filepath_dirs_to_delete` is tested below.\n+                \"url_cache_thumbnail_rel\",\n+                \"url_cache_thumbnail\",\n+                \"url_cache_thumbnail_directory_rel\",\n+                \"url_cache_thumbnail_directory\",\n+                \"url_cache_thumbnail_dirs_to_delete\",\n+            ],\n+            parameter=\"media_id\",\n+            valid_values=valid_file_ids,\n+            invalid_values=invalid_file_ids,\n+        )\n+\n+        # `url_cache_filepath_dirs_to_delete` ignores what would be the last path\n+        # component, so only the first 4 characters matter.\n+        self._test_path_validation(\n+            [\n+                \"url_cache_filepath_dirs_to_delete\",\n+            ],\n+            parameter=\"media_id\",\n+            valid_values=valid_file_ids,\n+            invalid_values=[\n+                \"/erZNDnDZVjsOtardLuwfIBg\",\n+                \"Ge/ZNDnDZVjsOtardLuwfIBg\",\n+                \"G\\x00rZNDnDZVjsOtardLuwfIBg\",\n+                \"Ger\\x00NDnDZVjsOtardLuwfIBg\",\n+                \"\",\n+                \"Ge\",\n+                \"..rZNDnDZVjsOtardLuwfIBg\",\n+                \"Ge..NDnDZVjsOtardLuwfIBg\",\n+            ],\n+        )\n+\n+        self._test_path_validation(\n+            [\n+                \"remote_media_filepath_rel\",\n+                \"remote_media_filepath\",\n+                \"remote_media_thumbnail_rel\",\n+                \"remote_media_thumbnail\",\n+                \"remote_media_thumbnail_rel_legacy\",\n+                \"remote_media_thumbnail_dir\",\n+            ],\n+            parameter=\"file_id\",\n+            valid_values=valid_file_ids,\n+            invalid_values=invalid_file_ids,\n+        )\n+\n+    def test_url_cache_media_id_validation(self):\n+        \"\"\"Test validation of URL cache media IDs\"\"\"\n+        self._test_path_validation(\n+            [\n+                \"url_cache_filepath_rel\",\n+                \"url_cache_filepath\",\n+                # `url_cache_filepath_dirs_to_delete` only cares about the date prefix\n+                \"url_cache_thumbnail_rel\",\n+                \"url_cache_thumbnail\",\n+                \"url_cache_thumbnail_directory_rel\",\n+                \"url_cache_thumbnail_directory\",\n+                \"url_cache_thumbnail_dirs_to_delete\",\n+            ],\n+            parameter=\"media_id\",\n+            valid_values=[\n+                \"2020-01-02_GerZNDnDZVjsOtar\",\n+                \"2020-01-02_G\",  # Unexpected, but produces an acceptable path\n+            ],\n+            invalid_values=[\n+                \"2020-01-02\",\n+                \"2020-01-02-\",\n+                \"2020-01-02-.\",\n+                \"2020-01-02-..\",\n+                \"2020-01-02-/\",\n+                \"2020-01-02-/GerZNDnDZVjsOtar\",\n+                \"2020-01-02-GerZNDnDZVjsOtar/..\",\n+                \"2020-01-02-GerZNDnDZVjsOtar\\x00\",\n+            ],\n+        )\n+\n+    def test_content_type_validation(self):\n+        \"\"\"Test validation of thumbnail content types\"\"\"\n+        self._test_path_validation(\n+            [\n+                \"local_media_thumbnail_rel\",\n+                \"local_media_thumbnail\",\n+                \"remote_media_thumbnail_rel\",\n+                \"remote_media_thumbnail\",\n+                \"remote_media_thumbnail_rel_legacy\",\n+                \"url_cache_thumbnail_rel\",\n+                \"url_cache_thumbnail\",\n+            ],\n+            parameter=\"content_type\",\n+            valid_values=[\n+                \"image/jpeg\",\n+            ],\n+            invalid_values=[\n+                \"\",  # ValueError: not enough values to unpack\n+                \"image/jpeg/abc\",  # ValueError: too many values to unpack\n+                \"image/jpeg\\x00\",\n+            ],\n+        )\n+\n+    def test_thumbnail_method_validation(self):\n+        \"\"\"Test validation of thumbnail methods\"\"\"\n+        self._test_path_validation(\n+            [\n+                \"local_media_thumbnail_rel\",\n+                \"local_media_thumbnail\",\n+                \"remote_media_thumbnail_rel\",\n+                \"remote_media_thumbnail\",\n+                \"url_cache_thumbnail_rel\",\n+                \"url_cache_thumbnail\",\n+            ],\n+            parameter=\"method\",\n+            valid_values=[\n+                \"crop\",\n+                \"scale\",\n+            ],\n+            invalid_values=[\n+                \"/scale\",\n+                \"scale/..\",\n+                \"scale\\x00\",\n+                \"/\",\n+            ],\n+        )\n+\n+    def _test_path_validation(\n+        self,\n+        methods: Iterable[str],\n+        parameter: str,\n+        valid_values: Iterable[str],\n+        invalid_values: Iterable[str],\n+    ):\n+        \"\"\"Test that the specified methods validate the named parameter as expected\n+\n+        Args:\n+            methods: The names of `MediaFilePaths` methods to test\n+            parameter: The name of the parameter to test\n+            valid_values: A list of parameter values that are expected to be accepted\n+            invalid_values: A list of parameter values that are expected to be rejected\n+\n+        Raises:\n+            AssertionError: If a value was accepted when it should have failed\n+                validation.\n+            ValueError: If a value failed validation when it should have been accepted.\n+        \"\"\"\n+        for method in methods:\n+            get_path = getattr(self.filepaths, method)\n+\n+            parameters = inspect.signature(get_path).parameters\n+            kwargs = {\n+                \"server_name\": \"matrix.org\",\n+                \"media_id\": \"GerZNDnDZVjsOtardLuwfIBg\",\n+                \"file_id\": \"GerZNDnDZVjsOtardLuwfIBg\",\n+                \"width\": 800,\n+                \"height\": 600,\n+                \"content_type\": \"image/jpeg\",\n+                \"method\": \"scale\",\n+            }\n+\n+            if get_path.__name__.startswith(\"url_\"):\n+                kwargs[\"media_id\"] = \"2020-01-02_GerZNDnDZVjsOtar\"\n+\n+            kwargs = {k: v for k, v in kwargs.items() if k in parameters}\n+            kwargs.pop(parameter)\n+\n+            for value in valid_values:\n+                kwargs[parameter] = value\n+                get_path(**kwargs)\n+                # No exception should be raised\n+\n+            for value in invalid_values:\n+                with self.assertRaises(ValueError):\n+                    kwargs[parameter] = value\n+                    path_or_list = get_path(**kwargs)\n+                    self.fail(\n+                        f\"{value!r} unexpectedly passed validation: \"\n+                        f\"{method} returned {path_or_list!r}\"\n+                    )", "file_path": "files/2021_11/69", "file_language": "py", "file_name": "tests/rest/media/v1/test_filepath.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
