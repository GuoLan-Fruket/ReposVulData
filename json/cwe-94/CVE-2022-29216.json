{"index": 8145, "cve_id": "CVE-2022-29216", "cwe_id": ["CWE-94"], "cve_language": "Python", "cve_description": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, TensorFlow's `saved_model_cli` tool is vulnerable to a code injection. This can be used to open a reverse shell. This code path was maintained for compatibility reasons as the maintainers had several test cases where numpy expressions were used as arguments. However, given that the tool is always run manually, the impact of this is still not severe. The maintainers have now removed the `safe=False` argument, so all parsing is done without calling `eval`. The patch is available in versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4.", "cvss": "7.8", "publish_date": "May 20, 2022", "AV": "LOCAL", "AC": "LOCAL", "PR": "LOW", "UI": "NONE", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "c5da7af048611aa29e9382371f0aed5018516cac", "commit_message": "Always do safe parsing\n\nPiperOrigin-RevId: 433016287", "commit_date": "2022-03-07T21:51:01Z", "project": "tensorflow/tensorflow", "url": "https://api.github.com/repos/tensorflow/tensorflow/commits/c5da7af048611aa29e9382371f0aed5018516cac", "html_url": "https://github.com/tensorflow/tensorflow/commit/c5da7af048611aa29e9382371f0aed5018516cac", "windows_before": [{"commit_id": "0f239549382ac16ff7642138172631c3f52e3fff", "commit_date": "Mon Mar 7 12:43:26 2022 -0800", "commit_message": "[mhlo] Verifier for mhlo.reduce_precision", "files_name": ["tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/IR/hlo_ops.td", "tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/hlo_ops.cc", "tensorflow/compiler/mlir/hlo/tests/Dialect/mhlo/ops.mlir"]}, {"commit_id": "21264a552155922ec9db03e2b025a6fbea676560", "commit_date": "Mon Mar 7 11:59:45 2022 -0800", "commit_message": "[KernelGen][JIT] Register malloc and free symbols to fix Windows", "files_name": ["tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc"]}, {"commit_id": "b6569d75657f052a9138f3bbecdebd31aef05ccf", "commit_date": "Mon Mar 7 11:58:06 2022 -0800", "commit_message": "Rollback of PR #53769", "files_name": ["tensorflow/compiler/mlir/tools/kernel_gen/kernel_creator.cc", "tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel.cc", "tensorflow/core/kernels/mlir_generated/BUILD", "tensorflow/core/kernels/mlir_generated/build_defs.bzl"]}, {"commit_id": "1f1b5c935c3761d12052734fa7acb8644ec95ef7", "commit_date": "Mon Mar 7 11:55:50 2022 -0800", "commit_message": "Fix Reference Type Supertype", "files_name": ["tensorflow/core/function/trace_type/default_types.py", "tensorflow/core/function/trace_type/default_types_test.py"]}, {"commit_id": "43934884f1dde70c18f60ae4991c25a5a492cbf5", "commit_date": "Mon Mar 7 11:53:26 2022 -0800", "commit_message": "Use kernel utils helper to get number of dimensions and size", "files_name": ["tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc"]}, {"commit_id": "ed779a333c8a75560c9abf13319a5bd4927b04e6", "commit_date": "Mon Mar 7 11:48:09 2022 -0800", "commit_message": "Update comments on literal.h.", "files_name": ["tensorflow/compiler/xla/literal.h"]}, {"commit_id": "4dde6171d12b9a32b4d714ddb3853162a4ea8713", "commit_date": "Mon Mar 7 11:46:48 2022 -0800", "commit_message": "Introduce TensorArray most_specific_common_supertype", "files_name": ["tensorflow/python/ops/tensor_array_ops.py"]}, {"commit_id": "26c2b5a46941907433d367fc95613cdd08f38de7", "commit_date": "Mon Mar 7 11:43:59 2022 -0800", "commit_message": "[XLA:GPU] Unify vectorization logic for single instructions and for fusions", "files_name": ["tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc", "tensorflow/compiler/xla/service/gpu/tests/element_wise_row_vectorization.hlo"]}, {"commit_id": "fdbf63bffe9a95033a5e4a4d0511dbde3a0b7cdf", "commit_date": "Mon Mar 7 11:32:15 2022 -0800", "commit_message": "Internal change", "files_name": ["tensorflow/core/kernels/data/experimental/assert_prev_dataset_op.cc"]}, {"commit_id": "37bd8c85f4786adfb75bd9c587a719d690a95580", "commit_date": "Mon Mar 7 11:44:00 2022 -0800", "commit_message": "Refactoring of ConvertUnary", "files_name": ["tensorflow/compiler/tf2tensorrt/BUILD", "tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc", "tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h", "tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc", "tensorflow/compiler/tf2tensorrt/convert/ops/unary_ops.cc"]}, {"commit_id": "71151ec555926c92ac1ff7738f88d8502f15f9aa", "commit_date": "Mon Mar 7 11:39:28 2022 -0800", "commit_message": "Merge pull request #54810 from Intel-tensorflow:gyshi/fiex_remapper_bug", "files_name": ["396615784235e47c75d92a9d8d838052599263bb - Mon Mar 7 11:15:56 2022 -0800 : Factor out function for getting xla::ExecutableBuildOptions.", "tensorflow/compiler/jit/xla_compilation_cache.cc"]}, {"commit_id": "d192cb9c8f0ee3a0a82f8a469791690114e4a66f", "commit_date": "Mon Mar 7 10:57:15 2022 -0800", "commit_message": "[XNNPACK] Support SPLIT operator with 2 outputs", "files_name": ["tensorflow/lite/delegates/xnnpack/BUILD", "tensorflow/lite/delegates/xnnpack/README.md", "tensorflow/lite/delegates/xnnpack/signed_quantized_split_test.cc", "tensorflow/lite/delegates/xnnpack/split_test.cc", "tensorflow/lite/delegates/xnnpack/split_tester.cc", "tensorflow/lite/delegates/xnnpack/split_tester.h", "tensorflow/lite/delegates/xnnpack/unsigned_quantized_split_test.cc", "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc", "tensorflow/lite/tools/cmake/modules/xnnpack.cmake", "tensorflow/workspace2.bzl"]}, {"commit_id": "c88c8b78860d3b97f2651148b617efa67f6a3a77", "commit_date": "Mon Mar 7 10:54:39 2022 -0800", "commit_message": "Add flatbuffer conversions for while operator.", "files_name": ["tensorflow/lite/core/api/flatbuffer_conversions.cc", "tensorflow/lite/core/api/flatbuffer_conversions.h"]}, {"commit_id": "f29248f14fa88463ef21465f48849f2cd9e45eaa", "commit_date": "Tue Nov 16 15:00:49 2021 +0100", "commit_message": "Extend TF-TRT C++ converter API to convert models which have variables (non-frozen)", "files_name": ["tensorflow/c/BUILD", "tensorflow/compiler/tf2tensorrt/BUILD", "tensorflow/compiler/tf2tensorrt/trt_convert_api.cc", "tensorflow/compiler/tf2tensorrt/trt_convert_api.h", "tensorflow/compiler/tf2tensorrt/trt_convert_api_test.cc"]}, {"commit_id": "11ec173bc20ad214ea4775702b319189088828c7", "commit_date": "Mon Mar 7 10:50:13 2022 -0800", "commit_message": "Ran clang format check", "files_name": ["tensorflow/stream_executor/cuda/redzone_allocator_test.cc"]}, {"commit_id": "518c1eabf392c39087ba7d9bdb9ffc42c69c8c46", "commit_date": "Mon Mar 7 10:26:38 2022 -0800", "commit_message": "[XLA] Simplify `ShapeIndex` and `ShapeIndexView`.", "files_name": ["tensorflow/compiler/xla/literal.cc", "tensorflow/compiler/xla/service/dynamic_dimension_inference.cc", "tensorflow/compiler/xla/service/memory_space_propagation.cc", "tensorflow/compiler/xla/service/pattern_matcher.h", "tensorflow/compiler/xla/service/shaped_buffer_test.cc", "tensorflow/compiler/xla/service/spmd/spmd_partitioner.cc", "tensorflow/compiler/xla/shape_util.cc", "tensorflow/compiler/xla/shape_util.h", "tensorflow/compiler/xla/shape_util_test.cc"]}, {"commit_id": "6643f0796d1f9cde90eaa95593bb7f27f315262b", "commit_date": "Mon Mar 7 10:08:56 2022 -0800", "commit_message": "Rollback of PR #54432", "files_name": ["tensorflow/python/kernel_tests/array_ops/array_ops_test.py", "tensorflow/python/ops/array_ops.py"]}, {"commit_id": "b18d2ddb8ddb2fc485dd3c06bb9868b234da119c", "commit_date": "Mon Mar 7 10:19:54 2022 -0800", "commit_message": "Merge pull request #54972 from yongtang:54855-tf.image.rgb_to_hsv", "files_name": ["741c650f5205337c4321ed7f8fade784b989031f - Mon Mar 7 10:10:46 2022 -0800 : Remove kDefaultMemoryLimit", "tensorflow/stream_executor/cuda/redzone_allocator_test.cc", "tensorflow/stream_executor/gpu/redzone_allocator.h"]}, {"commit_id": "c1a369e066d94418ee4f6d8aeaf7fbe086441fc0", "commit_date": "Mon Mar 7 10:07:21 2022 -0800", "commit_message": "Merge pull request #54490 from MalcolmSlaney:patch-1", "files_name": ["857b918978842012090d22ffeca20e7f96e2263c - Mon Mar 7 10:04:03 2022 -0800 : Merge pull request #54432 from yongtang:54412-tf.boolean_mask-bool", "0f9631036ace6497fdea6da9a066358c8739c752 - Mon Mar 7 09:50:36 2022 -0800 : Merge pull request #54820 from formigone:patch-1", "2886d6df8db626cbdaa59ba6cbb460a0ffc6a45a - Mon Feb 28 12:56:03 2022 -0800 : Added XLA env var for redzone space limit", "tensorflow/compiler/xla/debug_options_flags.cc", "tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc", "tensorflow/compiler/xla/xla.proto"]}, {"commit_id": "0bbffc626ba431bebf692273a3d18ffc7fbb50b5", "commit_date": "Mon Mar 7 09:24:12 2022 -0800", "commit_message": "[tf:tfrt] Disable lowering of shape_cast in transpose codegen", "files_name": ["tensorflow/compiler/mlir/tfrt/jit/transforms/tf_jitrt_lower_vector_transpose.cc"]}, {"commit_id": "a4b20ca8e43cb82926ec2a3e25bb66f13b99deef", "commit_date": "Mon Mar 7 16:29:51 2022 +0000", "commit_message": "buildifier fixes", "files_name": ["tensorflow/compiler/xla/service/gpu/BUILD", "tensorflow/stream_executor/rocm/BUILD"]}, {"commit_id": "80d2c8dfe24003804bb1921632bc0974be1569af", "commit_date": "Mon Mar 7 07:27:06 2022 -0800", "commit_message": "Integrate LLVM at llvm/llvm-project@17a68065c378", "files_name": ["tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/chlo_ops.cc", "tensorflow/core/ir/ops.cc", "third_party/llvm/workspace.bzl"]}, {"commit_id": "c04bb27f8ecc00ebafca701bc830e576f707d469", "commit_date": "Mon Mar 7 06:09:09 2022 -0800", "commit_message": "TestExecutionEnvironment extended to provide info about storages that support in/uint types.", "files_name": ["tensorflow/lite/delegates/gpu/cl/kernels/cl_test.cc", "tensorflow/lite/delegates/gpu/cl/kernels/cl_test.h", "tensorflow/lite/delegates/gpu/common/task/testing_util.h", "tensorflow/lite/delegates/gpu/metal/kernels/test_util.cc", "tensorflow/lite/delegates/gpu/metal/kernels/test_util.h"]}, {"commit_id": "39bbcf736f1b30b494f4dc11a6e43bb0c55a5689", "commit_date": "Thu Sep 9 19:48:36 2021 +1000", "commit_message": "Enable GPU tests in SparseSliceOpTest", "files_name": ["tensorflow/python/kernel_tests/sparse_ops/sparse_slice_op_test.py"]}, {"commit_id": "e679ef5114b50204c4cced8b75fe5d85cda94474", "commit_date": "Thu Sep 9 20:08:56 2021 +1000", "commit_message": "Add GPU implementation of SparseSliceGrad", "files_name": ["tensorflow/core/kernels/BUILD", "tensorflow/core/kernels/sparse_slice_grad_op.cc", "tensorflow/core/kernels/sparse_slice_grad_op_gpu.cu.cc", "tensorflow/core/util/gpu_kernel_helper.h"]}, {"commit_id": "a71ad3288b06db03d250624e2082b7e3b00fbb5d", "commit_date": "Mon Mar 7 02:34:13 2022 -0800", "commit_message": "Added new syntax constructions to abstract types. Support of different types in single high level kernel representation.", "files_name": ["tensorflow/lite/delegates/gpu/common/task/gpu_operation.cc", "tensorflow/lite/delegates/gpu/common/task/tensor_desc.cc", "tensorflow/lite/delegates/gpu/common/task/tensor_desc.h", "tensorflow/lite/delegates/gpu/common/tasks/add_test_util.cc"]}, {"commit_id": "42da2663f3de4c2fbb4758ff03f0547b85d83561", "commit_date": "Mon Mar 7 02:40:42 2022 -0800", "commit_message": "Merge pull request #54926 from ROCmSoftwarePlatform:rocm_bef_thunk_blasgemm_refactor", "files_name": ["e5c40c4ae9de00d6e5f73a8dc65bfa98660028e0 - Mon Mar 7 02:23:42 2022 -0800 : Introduced optional type argument for tensor Write selector. Allow to use tensors with different types for the same kernel source file.", "tensorflow/lite/delegates/gpu/common/task/tensor_desc.cc", "tensorflow/lite/delegates/gpu/common/task/tensor_desc.h", "tensorflow/lite/delegates/gpu/common/tasks/pooling.cc", "tensorflow/lite/delegates/gpu/common/tasks/pooling_test_util.cc"]}, {"commit_id": "7097d0b6d37378ae6c65a94ccfda108eaf253976", "commit_date": "Mon Mar 7 01:01:35 2022 -0800", "commit_message": "compat: Update forward compatibility horizon to 2022-03-07", "files_name": ["tensorflow/python/compat/compat.py"]}, {"commit_id": "5725474d6870e1778771d899a386657696818257", "commit_date": "Mon Mar 7 01:01:35 2022 -0800", "commit_message": "Update GraphDef version to 1063.", "files_name": ["tensorflow/core/public/version.h"]}, {"commit_id": "0f2b2712b8044bba8d56ff61768c1f376252b1ba", "commit_date": "Sun Mar 6 22:24:40 2022 -0800", "commit_message": "PR #54427: Implementation of the converter for Tile operation", "files_name": ["tensorflow/compiler/tf2tensorrt/BUILD", "tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc", "tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc", "tensorflow/compiler/tf2tensorrt/convert/ops/tile.cc", "tensorflow/compiler/tf2tensorrt/convert/utils.h"]}, {"commit_id": "d3c75249fc306306b276c87efaf0787bd6804baa", "commit_date": "Sun Mar 6 01:01:26 2022 -0800", "commit_message": "Update GraphDef version to 1062.", "files_name": ["tensorflow/core/public/version.h"]}, {"commit_id": "d139359e274fa0aff3b0ca02def84f411e475992", "commit_date": "Sun Mar 6 01:01:25 2022 -0800", "commit_message": "compat: Update forward compatibility horizon to 2022-03-06", "files_name": ["tensorflow/python/compat/compat.py"]}, {"commit_id": "0bf8ea886b864ad408aaded0c715e8f6343f2e1f", "commit_date": "Sun Mar 6 03:05:30 2022 +0000", "commit_message": "[create-pull-request] automated change", "files_name": ["tensorflow/tools/toolchains/remote_config/configs.bzl"]}, {"commit_id": "32cc6df68dcc26cc2958f8bf557c77ac623baf9c", "commit_date": "Sat Mar 5 15:51:45 2022 -0800", "commit_message": "Fix build failure for absl::StrContains()", "files_name": ["tensorflow/compiler/mlir/lite/quantization/tools/tflite_op_coverage_spec_getters_gen.cc"]}, {"commit_id": "3a84bebfbb72ba837102305e203d741c365ba40f", "commit_date": "Sat Mar 5 08:35:07 2022 -0600", "commit_message": "Remove unused import", "files_name": ["tensorflow/core/protobuf/worker.proto"]}, {"commit_id": "2f2ef96085f8d938b0f5ade8bb42650c7b6bcace", "commit_date": "Fri Feb 25 16:12:04 2022 +0000", "commit_message": "Enable ROCm support for the batched potrm functions for the cholesky op in XLA.", "files_name": ["tensorflow/compiler/xla/service/gpu/BUILD", "tensorflow/compiler/xla/service/gpu/cholesky_thunk.cc", "tensorflow/compiler/xla/service/gpu/cusolver_context.cc", "tensorflow/compiler/xla/service/gpu/cusolver_context.h", "tensorflow/stream_executor/platform/default/dso_loader.h", "tensorflow/stream_executor/rocm/hipsolver_wrapper.h"]}, {"commit_id": "2615c7ba3eb7fa5c4f5efe9dd9c8affd1358ef2a", "commit_date": "Sat Mar 5 01:01:43 2022 -0800", "commit_message": "Update GraphDef version to 1061.", "files_name": ["tensorflow/core/public/version.h"]}, {"commit_id": "bab20e0cc9e9875466f6b31414757419a27d9295", "commit_date": "Sat Mar 5 01:01:26 2022 -0800", "commit_message": "compat: Update forward compatibility horizon to 2022-03-05", "files_name": ["tensorflow/python/compat/compat.py"]}, {"commit_id": "a50c290ae0cee8466a25e3a214b2a0095b4862da", "commit_date": "Sat Mar 5 00:15:07 2022 -0800", "commit_message": "Visibility changes for the internal builds", "files_name": ["tensorflow/compiler/mlir/lite/quantization/BUILD"]}, {"commit_id": "1d68534ba75ac574f4f4ad39c479b4387bef2ea9", "commit_date": "Fri Mar 4 23:30:46 2022 -0800", "commit_message": "Update TFRT dependency to use revision http://github.com/tensorflow/runtime/commit/191cf88fd619ad114e385eda17fe8ef6dc1ebac1.", "files_name": ["third_party/tf_runtime/workspace.bzl"]}, {"commit_id": "591bdffa3f49501c0bcf08674c4eb7565705ce3c", "commit_date": "Fri Mar 4 21:22:46 2022 -0800", "commit_message": "[XLA] Speed up WhileLoopSimplifier for large shapes.", "files_name": ["tensorflow/compiler/xla/service/while_loop_simplifier.cc"]}, {"commit_id": "0ed3a6bf355bb8199ac9f85b11231558376df00d", "commit_date": "Fri Mar 4 19:57:41 2022 -0800", "commit_message": "[tf.numpy] Fixes linspace's rounding to follow numpy 1.20 .", "files_name": ["tensorflow/python/ops/numpy_ops/np_math_ops.py"]}, {"commit_id": "c511242c43603ecad77a8d7f17aa743a95d7ff04", "commit_date": "Fri Mar 4 18:42:06 2022 -0800", "commit_message": "Update TFRT dependency to use revision http://github.com/tensorflow/runtime/commit/4a8983f7aae9b76a70b6fe4f0fd0a10cecc192fe.", "files_name": ["third_party/tf_runtime/workspace.bzl"]}], "windows_after": [{"commit_id": "13351318c52c8448b582ab4922a0cd6f9c2be8e4", "commit_date": "Mon Mar 7 12:55:21 2022 -0800", "commit_message": "[tf:tfrt] Improve vectorization strategy of transpose ops", "files_name": ["tensorflow/compiler/mlir/tfrt/jit/transforms/tf_jitrt_tile_transpose.cc"]}, {"commit_id": "acfd3697da2cbbfcd92cc76b72372c8194e97350", "commit_date": "Mon Mar 7 13:07:53 2022 -0800", "commit_message": "Add a new API for DeviceAssigment to return a map from physical device to logical device ID.", "files_name": ["tensorflow/compiler/xla/service/BUILD", "tensorflow/compiler/xla/service/computation_placer.cc", "tensorflow/compiler/xla/service/computation_placer.h"]}, {"commit_id": "c63e869e5eb9f89627551e0360dd48f1df2babe0", "commit_date": "Mon Mar 7 13:14:52 2022 -0800", "commit_message": "Removes extra call to native.genrule() because underlying bugs requiring the call are now resolved.  This cleans up the code and reduces Forge impact a tiny bit.", "files_name": ["tensorflow/compiler/aot/tfcompile.bzl"]}, {"commit_id": "6a64642fd0db15ae724a3ac0c451cbd77413c706", "commit_date": "Mon Mar 7 13:46:49 2022 -0800", "commit_message": "Allow the `root` argument of `Checkpoint` to be a weakref.", "files_name": ["tensorflow/python/training/tracking/util.py", "tensorflow/python/training/tracking/util_test.py"]}, {"commit_id": "94d85db0cf3a68dc6394ffb1d98d3740c53054ed", "commit_date": "Mon Mar 7 14:02:46 2022 -0800", "commit_message": "[ST] Add a pattern to fold tensor.cast inside gml_st.loop.", "files_name": ["tensorflow/compiler/mlir/hlo/lib/Dialect/gml_st/IR/gml_st_ops.cc", "tensorflow/compiler/mlir/hlo/tests/Dialect/gml_st/canonicalize.mlir"]}, {"commit_id": "c92c17d1a6aa0d34b8b65d940d027627d28b66b1", "commit_date": "Mon Mar 7 14:53:39 2022 -0800", "commit_message": "Use defines instead of copts under cc_binary to fix Windows build linking issue with Bazel 5", "files_name": ["third_party/llvm_openmp/openmp.bzl"]}, {"commit_id": "218ded9c6d83128f23183bec9c7ed96281315eaa", "commit_date": "Mon Mar 7 14:31:50 2022 -0800", "commit_message": "Add HLO module name to gpu_compiler VLOGs.", "files_name": ["tensorflow/compiler/xla/service/gpu/gpu_compiler.cc"]}, {"commit_id": "33d08f02f757eb47f0e88f819e6f4a6521517f30", "commit_date": "Mon Mar 7 14:36:59 2022 -0800", "commit_message": "Add Gradient Op for Einsum", "files_name": ["tensorflow/cc/BUILD", "tensorflow/cc/gradients/grad_helper.cc", "tensorflow/cc/gradients/grad_helper.h", "tensorflow/cc/gradients/linalg_grad.cc", "tensorflow/cc/gradients/linalg_grad_test.cc", "tensorflow/cc/gradients/math_grad.cc"]}, {"commit_id": "ba2241d3579d5ef7b8499f8b9c04340f7cf45d2d", "commit_date": "Mon Mar 7 14:44:34 2022 -0800", "commit_message": "Use most_specific_common_supertype in nest", "files_name": ["tensorflow/python/framework/tensor_spec.py", "tensorflow/python/framework/type_spec.py", "tensorflow/python/util/nest.py", "tensorflow/python/util/util.cc"]}, {"commit_id": "39907ef00fab0375a2dd5c932b58a86dc8cc2b07", "commit_date": "Mon Mar 7 15:07:02 2022 -0800", "commit_message": "[XLA] Use priority to represent multiple layout assignment passes in layout_assignment.cc without saving the intermediate result inside HLO and then clearing them.", "files_name": ["tensorflow/compiler/xla/service/layout_assignment.cc", "tensorflow/compiler/xla/service/layout_assignment.h", "tensorflow/compiler/xla/service/layout_assignment_test.cc", "tensorflow/compiler/xla/shape_layout.cc"]}, {"commit_id": "a989426ee1346693cc015792f11d715f6944f2b8", "commit_date": "Mon Mar 7 15:09:14 2022 -0800", "commit_message": "Improve to cover scale value greater than one", "files_name": ["tensorflow/lite/kernels/comparisons.cc", "tensorflow/lite/kernels/comparisons_test.cc"]}, {"commit_id": "f3963e82f21c9d094503568699877655f9dac508", "commit_date": "Mon Mar 7 15:11:40 2022 -0800", "commit_message": "Use the new libstdcxx abi in devtoolset-9", "files_name": ["tensorflow/tools/ci_build/devtoolset/build_devtoolset.sh"]}, {"commit_id": "c434d86b2b0d9e9d8260a7b3123285f6812cd8c6", "commit_date": "Mon Mar 7 15:15:47 2022 -0800", "commit_message": "TensorDescriptor Read/Write improved to support all conversions.", "files_name": ["tensorflow/lite/delegates/gpu/common/task/tensor_desc.cc"]}, {"commit_id": "1465c8acc0b04ab1d00d5c5254cf1a9804c6a920", "commit_date": "Mon Mar 7 15:28:00 2022 -0800", "commit_message": "Turns off pfor warning about legacy RNGs.", "files_name": ["tensorflow/python/ops/parallel_for/pfor.py"]}, {"commit_id": "2d86cd070a2c05d022fa315d240448f2191afe0a", "commit_date": "Mon Mar 7 16:00:42 2022 -0800", "commit_message": "fix broken microcontroller link.", "files_name": ["tensorflow/lite/g3doc/microcontrollers/library.md"]}, {"commit_id": "5fd79e41b7ab2c60e15ec60bfdc00bdba2149b4b", "commit_date": "Mon Mar 7 15:41:43 2022 -0800", "commit_message": "Set the incarnation within the service instead of the rpc handler.", "files_name": ["tensorflow/core/distributed_runtime/coordination/coordination_service.cc", "tensorflow/core/distributed_runtime/coordination/coordination_service.h", "tensorflow/core/distributed_runtime/coordination/coordination_service_rpc_handler.cc", "tensorflow/core/distributed_runtime/coordination/coordination_service_rpc_handler.h"]}, {"commit_id": "f4761a9fba97179ffcc819c3c652c3b71361a161", "commit_date": "Mon Mar 7 15:54:55 2022 -0800", "commit_message": "Turn on OptimizationBarrierExpander passes for GPU and CPU.", "files_name": ["tensorflow/compiler/xla/service/cpu/BUILD", "tensorflow/compiler/xla/service/cpu/cpu_compiler.cc", "tensorflow/compiler/xla/service/gpu/BUILD", "tensorflow/compiler/xla/service/gpu/gpu_compiler.cc"]}, {"commit_id": "8f234978f4c598630a9e1a1978d18ba59092a7f4", "commit_date": "Mon Mar 7 16:24:31 2022 -0800", "commit_message": "lite: Use dynamic tensors for large tensors", "files_name": ["tensorflow/lite/core/subgraph.cc", "tensorflow/lite/core/subgraph.h", "tensorflow/lite/interpreter.cc", "tensorflow/lite/interpreter.h", "tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc"]}, {"commit_id": "959e22b3ad17e3a9075bc2670144f787dd656899", "commit_date": "Mon Mar 7 16:29:06 2022 -0800", "commit_message": "Refactor device propagation to use a barrier internally.", "files_name": ["tensorflow/core/distributed_runtime/coordination/coordination_service.cc"]}, {"commit_id": "ffc8fd107224102a70c1641b3899adf42848491f", "commit_date": "Mon Mar 7 16:57:04 2022 -0800", "commit_message": "[tf][tfg] Control-Flow Sink", "files_name": ["tensorflow/core/ir/ops.td", "tensorflow/core/transforms/BUILD", "tensorflow/core/transforms/cf_sink/BUILD", "tensorflow/core/transforms/cf_sink/cf_sink.cc", "tensorflow/core/transforms/cf_sink/cf_sink.h", "tensorflow/core/transforms/cf_sink/tests/sink_basic.mlir", "tensorflow/core/transforms/cf_sink/tests/sink_region_op.mlir", "tensorflow/core/transforms/pass_registration.h", "tensorflow/core/transforms/passes.td", "tensorflow/core/transforms/tfg-transforms-opt.cc"]}, {"commit_id": "3cd776b9720dff39749801455bd284af72603fab", "commit_date": "Mon Mar 7 17:22:08 2022 -0800", "commit_message": "[mhlo] Verifier for mhlo.ScatterOP.", "files_name": ["tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/IR/hlo_ops.td", "tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/hlo_ops.cc", "tensorflow/compiler/mlir/hlo/tests/Dialect/mhlo/scatter_op_verifier.mlir"]}, {"commit_id": "63df3f97643adf42a37bc70fef4da4d82c5c229b", "commit_date": "Mon Mar 7 17:37:14 2022 -0800", "commit_message": "Allow the converter to optionally preserve `TF::AssertOp`.", "files_name": ["tensorflow/compiler/mlir/lite/common/tfl_pass_config.h", "tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc", "tensorflow/compiler/mlir/lite/python/saved_model_to_tfl_flatbuffer.cc", "tensorflow/compiler/mlir/lite/tests/legalize-tf-assert.mlir", "tensorflow/compiler/mlir/lite/tf_tfl_passes.cc", "tensorflow/compiler/mlir/lite/tf_tfl_translate.cc", "tensorflow/compiler/mlir/lite/tf_tfl_translate_cl.cc", "tensorflow/compiler/mlir/lite/tf_tfl_translate_cl.h", "tensorflow/compiler/mlir/lite/transforms/legalize_tf.cc", "tensorflow/compiler/mlir/lite/transforms/passes.h", "tensorflow/lite/python/convert.py", "tensorflow/lite/python/lite.py", "tensorflow/lite/python/lite_v2_test.py", "tensorflow/lite/toco/toco_flags.proto"]}, {"commit_id": "778e16dd47c638c29372437b424479ca99da24d1", "commit_date": "Tue Mar 8 01:52:44 2022 +0000", "commit_message": "resizing line at a better place", "files_name": ["tensorflow/core/tpu/tpu_initializer_helper.cc"]}, {"commit_id": "936e7a1659b5a0c6ddeb1d524926f62b4c5dc5f3", "commit_date": "Mon Mar 7 19:18:43 2022 -0800", "commit_message": "Enhance shape inference for ReduceDatasetOp.", "files_name": ["tensorflow/compiler/mlir/tensorflow/tests/shape_inference.mlir", "tensorflow/compiler/mlir/tensorflow/transforms/shape_inference.cc"]}, {"commit_id": "b2ed59f2f8d6cacaae63027f59552050d516fcd3", "commit_date": "Mon Mar 7 19:40:56 2022 -0800", "commit_message": "Update PyYAML dependency version to 6.0 for Python 3.10 compatibility", "files_name": ["tensorflow/tools/ci_build/release/requirements_ubuntu.txt"]}, {"commit_id": "9ea025c4c5030a165a326572ca79c725b42edcdb", "commit_date": "Mon Mar 7 20:19:07 2022 -0800", "commit_message": "Update container hash for manylinux2014 RBE container.", "files_name": ["tensorflow/tools/toolchains/remote_config/containers.bzl"]}, {"commit_id": "f4dcf5b273aa377d0bcbde7ba812b4681ca2204f", "commit_date": "Mon Mar 7 21:06:11 2022 -0800", "commit_message": "Remove the sample add quantization recipe", "files_name": ["tensorflow/compiler/mlir/quantization/tensorflow/passes/lift_quantizable_spots_as_functions.td", "tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library.mlir", "tensorflow/compiler/mlir/quantization/tensorflow/python/integration_test/quantize_model_test.py", "tensorflow/compiler/mlir/quantization/tensorflow/tests/fake_quant_e2e_flow.mlir", "tensorflow/compiler/mlir/quantization/tensorflow/tests/insert_quantized_functions.mlir", "tensorflow/compiler/mlir/quantization/tensorflow/tests/lift_quantizable_spots_as_functions.mlir", "tensorflow/compiler/mlir/quantization/tensorflow/tests/quantize.mlir", "tensorflow/compiler/mlir/quantization/tensorflow/tests/quantize_composite_functions.mlir"]}, {"commit_id": "c8bf18410b579b87a2ed1f2c2b31b0032b835633", "commit_date": "Mon Mar 7 22:32:48 2022 -0800", "commit_message": "Add optimization method enum to select quantization algorithm", "files_name": ["tensorflow/compiler/mlir/quantization/tensorflow/python/integration_test/concurrency_test.py", "tensorflow/compiler/mlir/quantization/tensorflow/python/integration_test/quantize_model_test.py", "tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.py"]}, {"commit_id": "6e7e5836af7bd09bed2c271938c4373a891e0d0a", "commit_date": "Mon Mar 7 23:14:18 2022 -0800", "commit_message": "Preserve context info when entering merge_call.", "files_name": ["tensorflow/python/distribute/mirrored_run.py", "tensorflow/python/distribute/strategy_common_test.py"]}, {"commit_id": "a48a705ce5d573f52a75380d0a1fcba9d8a6bb1a", "commit_date": "Tue Mar 8 00:06:51 2022 -0800", "commit_message": "Update TFRT dependency to use revision http://github.com/tensorflow/runtime/commit/d865211433e28890946cf960cea38f1e55f7b78e.", "files_name": ["third_party/tf_runtime/workspace.bzl"]}, {"commit_id": "1be79da288e8812bea784769c9df71ab2de246e4", "commit_date": "Tue Mar 8 00:42:47 2022 -0800", "commit_message": "Update TFRT dependency to use revision http://github.com/tensorflow/runtime/commit/54249f2c617e877b120310850e6184a2430b2e2a.", "files_name": ["third_party/tf_runtime/workspace.bzl"]}, {"commit_id": "ded717ac61a8aead60f25e61e27abd88e061c035", "commit_date": "Tue Mar 8 01:01:24 2022 -0800", "commit_message": "Update GraphDef version to 1064.", "files_name": ["tensorflow/core/public/version.h"]}, {"commit_id": "2efd47de550fa1eceb12d36a87449c4cbdf2f861", "commit_date": "Tue Mar 8 01:04:56 2022 -0800", "commit_message": "compat: Update forward compatibility horizon to 2022-03-08", "files_name": ["tensorflow/python/compat/compat.py"]}, {"commit_id": "c56e15a9db1e87e12af84aea6d0fdca8daf25d52", "commit_date": "Tue Mar 8 01:16:31 2022 -0800", "commit_message": "Integrate LLVM at llvm/llvm-project@e1069c1288d1", "files_name": ["tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/hlo_legalize_to_lhlo.cc", "tensorflow/compiler/mlir/hlo/tools/mlir-hlo-opt/mlir-hlo-opt.cpp", "tensorflow/compiler/mlir/lite/experimental/tac/execution_metadata_exporter_test.cc", "tensorflow/compiler/mlir/lite/experimental/tac/tac_translate.cc", "tensorflow/compiler/mlir/lite/experimental/tac/utils/utils.cc", "tensorflow/compiler/mlir/lite/experimental/tac/utils/utils.h", "tensorflow/compiler/mlir/lite/flatbuffer_export.cc", "tensorflow/compiler/mlir/lite/flatbuffer_import.cc", "tensorflow/compiler/mlir/lite/flatbuffer_translate.cc", "tensorflow/compiler/mlir/lite/metrics/error_collector_inst_test.cc", "tensorflow/compiler/mlir/lite/mlir_tflite_runner.cc", "tensorflow/compiler/mlir/lite/python/flatbuffer_to_mlir.cc", "tensorflow/compiler/mlir/lite/tf_tfl_translate.cc", "tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc", "tensorflow/compiler/mlir/python/mlir.cc", "tensorflow/compiler/mlir/python/mlir_wrapper/mlir_wrapper.cc", "tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_quantized_functions.cc", "tensorflow/compiler/mlir/quantization/tensorflow/passes/tf_quant_opt.cc", "tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.cc", "tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc", "tensorflow/compiler/mlir/tensorflow/ir/tf_ops_a_m.cc", "tensorflow/compiler/mlir/tensorflow/ir/tf_ops_n_z.cc", "tensorflow/compiler/mlir/tensorflow/ir/tf_remaining_ops.cc", "tensorflow/compiler/mlir/tensorflow/transforms/materialize_mlir_passthrough_op.cc", "tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate.cc", "tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate_registration.cc", "tensorflow/compiler/mlir/tensorflow/translate/translate_tf_dialect_op.cc", "tensorflow/compiler/mlir/tensorflow/utils/serialize_mlir_module_utils.cc", "tensorflow/compiler/mlir/tensorflow/utils/tf_xla_mlir_translate.cc", "tensorflow/compiler/mlir/tf_mlir_opt_main.cc", "tensorflow/compiler/mlir/tf_mlir_translate_main.cc", "tensorflow/compiler/mlir/tfjs/tfjs_opt.cc", "tensorflow/compiler/mlir/tfjs/translate/json_translate.cc", "tensorflow/compiler/mlir/tfjs/translate/tf_to_tfjs_json.cc", "tensorflow/compiler/mlir/tfr/integration/tfr_decompose_ctx.cc"]}], "parents": [{"commit_id_before": "0f239549382ac16ff7642138172631c3f52e3fff", "url_before": "https://api.github.com/repos/tensorflow/tensorflow/commits/0f239549382ac16ff7642138172631c3f52e3fff", "html_url_before": "https://github.com/tensorflow/tensorflow/commit/0f239549382ac16ff7642138172631c3f52e3fff"}], "details": [{"raw_url": "https://github.com/tensorflow/tensorflow/raw/c5da7af048611aa29e9382371f0aed5018516cac/tensorflow%2Fpython%2Ftools%2Fsaved_model_cli.py", "code": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Command-line interface to inspect and execute a graph in a SavedModel.\n\nFor detailed usages and examples, please refer to:\nhttps://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel\n\n\"\"\"\n\nimport argparse\nimport ast\nimport os\nimport re\nimport sys\n\nfrom absl import app  # pylint: disable=unused-import\nimport numpy as np\nimport six\n\nfrom tensorflow.core.example import example_pb2\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.debug.wrappers import local_cli_wrapper\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.eager import function as defun\nfrom tensorflow.python.framework import meta_graph as meta_graph_lib\nfrom tensorflow.python.framework import ops as ops_lib\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.saved_model import load\nfrom tensorflow.python.saved_model import loader\nfrom tensorflow.python.saved_model import save\nfrom tensorflow.python.saved_model import signature_constants\nfrom tensorflow.python.tools import saved_model_aot_compile\nfrom tensorflow.python.tools import saved_model_utils\nfrom tensorflow.python.tpu import tpu\nfrom tensorflow.python.util.compat import collections_abc\n\n\n_XLA_DEBUG_OPTIONS_URL = (\n    'https://github.com/tensorflow/tensorflow/blob/master/'\n    'tensorflow/compiler/xla/debug_options_flags.cc')\n\n\n# Set of ops to denylist.\n_OP_DENYLIST = set(['WriteFile', 'ReadFile', 'PrintV2'])\n\n\ndef _show_tag_sets(saved_model_dir):\n  \"\"\"Prints the tag-sets stored in SavedModel directory.\n\n  Prints all the tag-sets for MetaGraphs stored in SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  tag_sets = saved_model_utils.get_saved_model_tag_sets(saved_model_dir)\n  print('The given SavedModel contains the following tag-sets:')\n  for tag_set in sorted(tag_sets):\n    print('%r' % ', '.join(sorted(tag_set)))\n\n\ndef _show_signature_def_map_keys(saved_model_dir, tag_set):\n  \"\"\"Prints the keys for each SignatureDef in the SignatureDef map.\n\n  Prints the list of SignatureDef keys from the SignatureDef map specified by\n  the given tag-set and SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n    tag_set: Group of tag(s) of the MetaGraphDef to get SignatureDef map from,\n        in string format, separated by ','. For tag-set contains multiple tags,\n        all tags must be passed in.\n  \"\"\"\n  signature_def_map = get_signature_def_map(saved_model_dir, tag_set)\n  print('The given SavedModel MetaGraphDef contains SignatureDefs with the '\n        'following keys:')\n  for signature_def_key in sorted(signature_def_map.keys()):\n    print('SignatureDef key: \\\"%s\\\"' % signature_def_key)\n\n\ndef _get_inputs_tensor_info_from_meta_graph_def(meta_graph_def,\n                                                signature_def_key):\n  \"\"\"Gets TensorInfo for all inputs of the SignatureDef.\n\n  Returns a dictionary that maps each input key to its TensorInfo for the given\n  signature_def_key in the meta_graph_def\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer with the SignatureDef map to\n        look up SignatureDef key.\n    signature_def_key: A SignatureDef key string.\n\n  Returns:\n    A dictionary that maps input tensor keys to TensorInfos.\n\n  Raises:\n    ValueError if `signature_def_key` is not found in the MetaGraphDef.\n  \"\"\"\n  if signature_def_key not in meta_graph_def.signature_def:\n    raise ValueError(\n        f'Could not find signature \"{signature_def_key}\". Please choose from: '\n        f'{\", \".join(meta_graph_def.signature_def.keys())}')\n  return meta_graph_def.signature_def[signature_def_key].inputs\n\n\ndef _get_outputs_tensor_info_from_meta_graph_def(meta_graph_def,\n                                                 signature_def_key):\n  \"\"\"Gets TensorInfos for all outputs of the SignatureDef.\n\n  Returns a dictionary that maps each output key to its TensorInfo for the given\n  signature_def_key in the meta_graph_def.\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer with the SignatureDefmap to\n    look up signature_def_key.\n    signature_def_key: A SignatureDef key string.\n\n  Returns:\n    A dictionary that maps output tensor keys to TensorInfos.\n  \"\"\"\n  return meta_graph_def.signature_def[signature_def_key].outputs\n\n\ndef _show_inputs_outputs(saved_model_dir, tag_set, signature_def_key, indent=0):\n  \"\"\"Prints input and output TensorInfos.\n\n  Prints the details of input and output TensorInfos for the SignatureDef mapped\n  by the given signature_def_key.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n    tag_set: Group of tag(s) of the MetaGraphDef, in string format, separated by\n        ','. For tag-set contains multiple tags, all tags must be passed in.\n    signature_def_key: A SignatureDef key string.\n    indent: How far (in increments of 2 spaces) to indent each line of output.\n  \"\"\"\n  meta_graph_def = saved_model_utils.get_meta_graph_def(saved_model_dir,\n                                                        tag_set)\n  inputs_tensor_info = _get_inputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n  outputs_tensor_info = _get_outputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n\n  indent_str = '  ' * indent\n  def in_print(s):\n    print(indent_str + s)\n\n  in_print('The given SavedModel SignatureDef contains the following input(s):')\n  for input_key, input_tensor in sorted(inputs_tensor_info.items()):\n    in_print('  inputs[\\'%s\\'] tensor_info:' % input_key)\n    _print_tensor_info(input_tensor, indent+1)\n\n  in_print('The given SavedModel SignatureDef contains the following '\n           'output(s):')\n  for output_key, output_tensor in sorted(outputs_tensor_info.items()):\n    in_print('  outputs[\\'%s\\'] tensor_info:' % output_key)\n    _print_tensor_info(output_tensor, indent+1)\n\n  in_print('Method name is: %s' %\n           meta_graph_def.signature_def[signature_def_key].method_name)\n\n\ndef _show_defined_functions(saved_model_dir):\n  \"\"\"Prints the callable concrete and polymorphic functions of the Saved Model.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  meta_graphs = saved_model_utils.read_saved_model(saved_model_dir).meta_graphs\n  has_object_graph_def = False\n\n  for meta_graph_def in meta_graphs:\n    has_object_graph_def |= meta_graph_def.HasField('object_graph_def')\n  if not has_object_graph_def:\n    return\n  with ops_lib.Graph().as_default():\n    trackable_object = load.load(saved_model_dir)\n\n  print('\\nConcrete Functions:', end='')\n  children = list(\n      save._AugmentedGraphView(trackable_object)  # pylint: disable=protected-access\n      .list_children(trackable_object))\n  children = sorted(children, key=lambda x: x.name)\n  for name, child in children:\n    concrete_functions = []\n    if isinstance(child, defun.ConcreteFunction):\n      concrete_functions.append(child)\n    elif isinstance(child, def_function.Function):\n      concrete_functions.extend(\n          child._list_all_concrete_functions_for_serialization())  # pylint: disable=protected-access\n    else:\n      continue\n    print('\\n  Function Name: \\'%s\\'' % name)\n    concrete_functions = sorted(concrete_functions, key=lambda x: x.name)\n    for index, concrete_function in enumerate(concrete_functions, 1):\n      args, kwargs = None, None\n      if concrete_function.structured_input_signature:\n        args, kwargs = concrete_function.structured_input_signature\n      elif concrete_function._arg_keywords:  # pylint: disable=protected-access\n        # For pure ConcreteFunctions we might have nothing better than\n        # _arg_keywords.\n        args = concrete_function._arg_keywords  # pylint: disable=protected-access\n      if args:\n        print('    Option #%d' % index)\n        print('      Callable with:')\n        _print_args(args, indent=4)\n      if kwargs:\n        _print_args(kwargs, 'Named Argument', indent=4)\n\n\ndef _print_args(arguments, argument_type='Argument', indent=0):\n  \"\"\"Formats and prints the argument of the concrete functions defined in the model.\n\n  Args:\n    arguments: Arguments to format print.\n    argument_type: Type of arguments.\n    indent: How far (in increments of 2 spaces) to indent each line of\n     output.\n  \"\"\"\n  indent_str = '  ' * indent\n\n  def _maybe_add_quotes(value):\n    is_quotes = '\\'' * isinstance(value, str)\n    return is_quotes + str(value) + is_quotes\n\n  def in_print(s, end='\\n'):\n    print(indent_str + s, end=end)\n\n  for index, element in enumerate(arguments, 1):\n    if indent == 4:\n      in_print('%s #%d' % (argument_type, index))\n    if isinstance(element, six.string_types):\n      in_print('  %s' % element)\n    elif isinstance(element, tensor_spec.TensorSpec):\n      print((indent + 1) * '  ' + '%s: %s' % (element.name, repr(element)))\n    elif (isinstance(element, collections_abc.Iterable) and\n          not isinstance(element, dict)):\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: [', end='')\n      for value in element:\n        print('%s' % _maybe_add_quotes(value), end=', ')\n      print('\\b\\b]')\n    elif isinstance(element, dict):\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: {', end='')\n      for (key, value) in element.items():\n        print('\\'%s\\': %s' % (str(key), _maybe_add_quotes(value)), end=', ')\n      print('\\b\\b}')\n    else:\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: %s' % str(element))\n\n\ndef _print_tensor_info(tensor_info, indent=0):\n  \"\"\"Prints details of the given tensor_info.\n\n  Args:\n    tensor_info: TensorInfo object to be printed.\n    indent: How far (in increments of 2 spaces) to indent each line output\n  \"\"\"\n  indent_str = '  ' * indent\n  def in_print(s):\n    print(indent_str + s)\n\n  in_print('    dtype: ' +\n           {value: key\n            for (key, value) in types_pb2.DataType.items()}[tensor_info.dtype])\n  # Display shape as tuple.\n  if tensor_info.tensor_shape.unknown_rank:\n    shape = 'unknown_rank'\n  else:\n    dims = [str(dim.size) for dim in tensor_info.tensor_shape.dim]\n    shape = ', '.join(dims)\n    shape = '(' + shape + ')'\n  in_print('    shape: ' + shape)\n  in_print('    name: ' + tensor_info.name)\n\n\ndef _show_all(saved_model_dir):\n  \"\"\"Prints tag-set, SignatureDef and Inputs/Outputs information in SavedModel.\n\n  Prints all tag-set, SignatureDef and Inputs/Outputs information stored in\n  SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  tag_sets = saved_model_utils.get_saved_model_tag_sets(saved_model_dir)\n  for tag_set in sorted(tag_sets):\n    print(\"\\nMetaGraphDef with tag-set: '%s' \"\n          \"contains the following SignatureDefs:\" % ', '.join(tag_set))\n\n    tag_set = ','.join(tag_set)\n    signature_def_map = get_signature_def_map(saved_model_dir, tag_set)\n    for signature_def_key in sorted(signature_def_map.keys()):\n      print('\\nsignature_def[\\'' + signature_def_key + '\\']:')\n      _show_inputs_outputs(saved_model_dir, tag_set, signature_def_key,\n                           indent=1)\n  _show_defined_functions(saved_model_dir)\n\n\ndef get_meta_graph_def(saved_model_dir, tag_set):\n  \"\"\"DEPRECATED: Use saved_model_utils.get_meta_graph_def instead.\n\n  Gets MetaGraphDef from SavedModel. Returns the MetaGraphDef for the given\n  tag-set and SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect or execute.\n    tag_set: Group of tag(s) of the MetaGraphDef to load, in string format,\n        separated by ','. For tag-set contains multiple tags, all tags must be\n        passed in.\n\n  Raises:\n    RuntimeError: An error when the given tag-set does not exist in the\n        SavedModel.\n\n  Returns:\n    A MetaGraphDef corresponding to the tag-set.\n  \"\"\"\n  return saved_model_utils.get_meta_graph_def(saved_model_dir, tag_set)\n\n\ndef get_signature_def_map(saved_model_dir, tag_set):\n  \"\"\"Gets SignatureDef map from a MetaGraphDef in a SavedModel.\n\n  Returns the SignatureDef map for the given tag-set in the SavedModel\n  directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect or execute.\n    tag_set: Group of tag(s) of the MetaGraphDef with the SignatureDef map, in\n        string format, separated by ','. For tag-set contains multiple tags, all\n        tags must be passed in.\n\n  Returns:\n    A SignatureDef map that maps from string keys to SignatureDefs.\n  \"\"\"\n  meta_graph = saved_model_utils.get_meta_graph_def(saved_model_dir, tag_set)\n  return meta_graph.signature_def\n\n\ndef scan_meta_graph_def(meta_graph_def):\n  \"\"\"Scans meta_graph_def and reports if there are ops on denylist.\n\n  Print ops if they are on black list, or print success if no denylisted ops\n  found.\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer.\n  \"\"\"\n  all_ops_set = set(\n      meta_graph_lib.ops_used_by_graph_def(meta_graph_def.graph_def))\n  denylisted_ops = _OP_DENYLIST & all_ops_set\n  if denylisted_ops:\n    # TODO(yifeif): print more warnings\n    print(\n        'MetaGraph with tag set %s contains the following denylisted ops:' %\n        meta_graph_def.meta_info_def.tags, denylisted_ops)\n  else:\n    print('MetaGraph with tag set %s does not contain denylisted ops.' %\n          meta_graph_def.meta_info_def.tags)\n\n\ndef run_saved_model_with_feed_dict(saved_model_dir,\n                                   tag_set,\n                                   signature_def_key,\n                                   input_tensor_key_feed_dict,\n                                   outdir,\n                                   overwrite_flag,\n                                   worker=None,\n                                   init_tpu=False,\n                                   use_tfrt=False,\n                                   tf_debug=False):\n  \"\"\"Runs SavedModel and fetch all outputs.\n\n  Runs the input dictionary through the MetaGraphDef within a SavedModel\n  specified by the given tag_set and SignatureDef. Also save the outputs to file\n  if outdir is not None.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to execute.\n    tag_set: Group of tag(s) of the MetaGraphDef with the SignatureDef map, in\n        string format, separated by ','. For tag-set contains multiple tags, all\n        tags must be passed in.\n    signature_def_key: A SignatureDef key string.\n    input_tensor_key_feed_dict: A dictionary maps input keys to numpy ndarrays.\n    outdir: A directory to save the outputs to. If the directory doesn't exist,\n        it will be created.\n    overwrite_flag: A boolean flag to allow overwrite output file if file with\n        the same name exists.\n    worker: If provided, the session will be run on the worker.  Valid worker\n        specification is a bns or gRPC path.\n    init_tpu: If true, the TPU system will be initialized after the session\n        is created.\n    use_tfrt: If true, TFRT session will be used.\n    tf_debug: A boolean flag to use TensorFlow Debugger (TFDBG) to observe the\n        intermediate Tensor values and runtime GraphDefs while running the\n        SavedModel.\n\n  Raises:\n    ValueError: When any of the input tensor keys is not valid.\n    RuntimeError: An error when output file already exists and overwrite is not\n    enabled.\n  \"\"\"\n  # Get a list of output tensor names.\n  meta_graph_def = saved_model_utils.get_meta_graph_def(saved_model_dir,\n                                                        tag_set)\n\n  # Re-create feed_dict based on input tensor name instead of key as session.run\n  # uses tensor name.\n  inputs_tensor_info = _get_inputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n\n  # Check if input tensor keys are valid.\n  for input_key_name in input_tensor_key_feed_dict.keys():\n    if input_key_name not in inputs_tensor_info:\n      raise ValueError(\n          '\"%s\" is not a valid input key. Please choose from %s, or use '\n          '--show option.' %\n          (input_key_name, '\"' + '\", \"'.join(inputs_tensor_info.keys()) + '\"'))\n\n  inputs_feed_dict = {\n      inputs_tensor_info[key].name: tensor\n      for key, tensor in input_tensor_key_feed_dict.items()\n  }\n  # Get outputs\n  outputs_tensor_info = _get_outputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n  # Sort to preserve order because we need to go from value to key later.\n  output_tensor_keys_sorted = sorted(outputs_tensor_info.keys())\n  output_tensor_names_sorted = [\n      outputs_tensor_info[tensor_key].name\n      for tensor_key in output_tensor_keys_sorted\n  ]\n\n  config = None\n  if use_tfrt:\n    logging.info('Using TFRT session.')\n    config = config_pb2.ConfigProto(\n        experimental=config_pb2.ConfigProto.Experimental(use_tfrt=True))\n  with session.Session(worker, graph=ops_lib.Graph(), config=config) as sess:\n    if init_tpu:\n      print('Initializing TPU System ...')\n      # This is needed for freshly started worker, or if the job\n      # restarts after a preemption.\n      sess.run(tpu.initialize_system())\n\n    loader.load(sess, tag_set.split(','), saved_model_dir)\n\n    if tf_debug:\n      sess = local_cli_wrapper.LocalCLIDebugWrapperSession(sess)\n\n    outputs = sess.run(output_tensor_names_sorted, feed_dict=inputs_feed_dict)\n\n    for i, output in enumerate(outputs):\n      output_tensor_key = output_tensor_keys_sorted[i]\n      print('Result for output key %s:\\n%s' % (output_tensor_key, output))\n\n      # Only save if outdir is specified.\n      if outdir:\n        # Create directory if outdir does not exist\n        if not os.path.isdir(outdir):\n          os.makedirs(outdir)\n        output_full_path = os.path.join(outdir, output_tensor_key + '.npy')\n\n        # If overwrite not enabled and file already exist, error out\n        if not overwrite_flag and os.path.exists(output_full_path):\n          raise RuntimeError(\n              'Output file %s already exists. Add \\\"--overwrite\\\" to overwrite'\n              ' the existing output files.' % output_full_path)\n\n        np.save(output_full_path, output)\n        print('Output %s is saved to %s' % (output_tensor_key,\n                                            output_full_path))\n\n\ndef preprocess_inputs_arg_string(inputs_str):\n  \"\"\"Parses input arg into dictionary that maps input to file/variable tuple.\n\n  Parses input string in the format of, for example,\n  \"input1=filename1[variable_name1],input2=filename2\" into a\n  dictionary looks like\n  {'input_key1': (filename1, variable_name1),\n   'input_key2': (file2, None)}\n  , which maps input keys to a tuple of file name and variable name(None if\n  empty).\n\n  Args:\n    inputs_str: A string that specified where to load inputs. Inputs are\n    separated by semicolons.\n        * For each input key:\n            '<input_key>=<filename>' or\n            '<input_key>=<filename>[<variable_name>]'\n        * The optional 'variable_name' key will be set to None if not specified.\n\n  Returns:\n    A dictionary that maps input keys to a tuple of file name and variable name.\n\n  Raises:\n    RuntimeError: An error when the given input string is in a bad format.\n  \"\"\"\n  input_dict = {}\n  inputs_raw = inputs_str.split(';')\n  for input_raw in filter(bool, inputs_raw):  # skip empty strings\n    # Format of input=filename[variable_name]'\n    match = re.match(r'([^=]+)=([^\\[\\]]+)\\[([^\\[\\]]+)\\]$', input_raw)\n\n    if match:\n      input_dict[match.group(1)] = match.group(2), match.group(3)\n    else:\n      # Format of input=filename'\n      match = re.match(r'([^=]+)=([^\\[\\]]+)$', input_raw)\n      if match:\n        input_dict[match.group(1)] = match.group(2), None\n      else:\n        raise RuntimeError(\n            '--inputs \"%s\" format is incorrect. Please follow'\n            '\"<input_key>=<filename>\", or'\n            '\"<input_key>=<filename>[<variable_name>]\"' % input_raw)\n\n  return input_dict\n\n\ndef preprocess_input_exprs_arg_string(input_exprs_str, safe=True):\n  \"\"\"Parses input arg into dictionary that maps input key to python expression.\n\n  Parses input string in the format of 'input_key=<python expression>' into a\n  dictionary that maps each input_key to its python expression.\n\n  Args:\n    input_exprs_str: A string that specifies python expression for input keys.\n      Each input is separated by semicolon. For each input key:\n        'input_key=<python expression>'\n    safe: Whether to evaluate the python expression as literals or allow\n      arbitrary calls (e.g. numpy usage).\n\n  Returns:\n    A dictionary that maps input keys to their values.\n\n  Raises:\n    RuntimeError: An error when the given input string is in a bad format.\n  \"\"\"\n  input_dict = {}\n\n  for input_raw in filter(bool, input_exprs_str.split(';')):\n    if '=' not in input_exprs_str:\n      raise RuntimeError('--input_exprs \"%s\" format is incorrect. Please follow'\n                         '\"<input_key>=<python expression>\"' % input_exprs_str)\n    input_key, expr = input_raw.split('=', 1)\n    if safe:\n      try:\n        input_dict[input_key] = ast.literal_eval(expr)\n      except:\n        raise RuntimeError(\n            f'Expression \"{expr}\" is not a valid python literal.')\n    else:\n      # ast.literal_eval does not work with numpy expressions\n      input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\n  return input_dict\n\n\ndef preprocess_input_examples_arg_string(input_examples_str):\n  \"\"\"Parses input into dict that maps input keys to lists of tf.Example.\n\n  Parses input string in the format of 'input_key1=[{feature_name:\n  feature_list}];input_key2=[{feature_name:feature_list}];' into a dictionary\n  that maps each input_key to its list of serialized tf.Example.\n\n  Args:\n    input_examples_str: A string that specifies a list of dictionaries of\n    feature_names and their feature_lists for each input.\n    Each input is separated by semicolon. For each input key:\n      'input=[{feature_name1: feature_list1, feature_name2:feature_list2}]'\n      items in feature_list can be the type of float, int, long or str.\n\n  Returns:\n    A dictionary that maps input keys to lists of serialized tf.Example.\n\n  Raises:\n    ValueError: An error when the given tf.Example is not a list.\n  \"\"\"\n  input_dict = preprocess_input_exprs_arg_string(input_examples_str)\n  for input_key, example_list in input_dict.items():\n    if not isinstance(example_list, list):\n      raise ValueError(\n          'tf.Example input must be a list of dictionaries, but \"%s\" is %s' %\n          (example_list, type(example_list)))\n    input_dict[input_key] = [\n        _create_example_string(example) for example in example_list\n    ]\n  return input_dict\n\n\ndef _create_example_string(example_dict):\n  \"\"\"Create a serialized tf.example from feature dictionary.\"\"\"\n  example = example_pb2.Example()\n  for feature_name, feature_list in example_dict.items():\n    if not isinstance(feature_list, list):\n      raise ValueError('feature value must be a list, but %s: \"%s\" is %s' %\n                       (feature_name, feature_list, type(feature_list)))\n    if isinstance(feature_list[0], float):\n      example.features.feature[feature_name].float_list.value.extend(\n          feature_list)\n    elif isinstance(feature_list[0], str):\n      example.features.feature[feature_name].bytes_list.value.extend(\n          [f.encode('utf8') for f in feature_list])\n    elif isinstance(feature_list[0], bytes):\n      example.features.feature[feature_name].bytes_list.value.extend(\n          feature_list)\n    elif isinstance(feature_list[0], six.integer_types):\n      example.features.feature[feature_name].int64_list.value.extend(\n          feature_list)\n    else:\n      raise ValueError(\n          'Type %s for value %s is not supported for tf.train.Feature.' %\n          (type(feature_list[0]), feature_list[0]))\n  return example.SerializeToString()\n\n\ndef load_inputs_from_input_arg_string(inputs_str, input_exprs_str,\n                                      input_examples_str):\n  \"\"\"Parses input arg strings and create inputs feed_dict.\n\n  Parses '--inputs' string for inputs to be loaded from file, and parses\n  '--input_exprs' string for inputs to be evaluated from python expression.\n  '--input_examples' string for inputs to be created from tf.example feature\n  dictionary list.\n\n  Args:\n    inputs_str: A string that specified where to load inputs. Each input is\n        separated by semicolon.\n        * For each input key:\n            '<input_key>=<filename>' or\n            '<input_key>=<filename>[<variable_name>]'\n        * The optional 'variable_name' key will be set to None if not specified.\n        * File specified by 'filename' will be loaded using numpy.load. Inputs\n            can be loaded from only .npy, .npz or pickle files.\n        * The \"[variable_name]\" key is optional depending on the input file type\n            as descripted in more details below.\n        When loading from a npy file, which always contains a numpy ndarray, the\n        content will be directly assigned to the specified input tensor. If a\n        variable_name is specified, it will be ignored and a warning will be\n        issued.\n        When loading from a npz zip file, user can specify which variable within\n        the zip file to load for the input tensor inside the square brackets. If\n        nothing is specified, this function will check that only one file is\n        included in the zip and load it for the specified input tensor.\n        When loading from a pickle file, if no variable_name is specified in the\n        square brackets, whatever that is inside the pickle file will be passed\n        to the specified input tensor, else SavedModel CLI will assume a\n        dictionary is stored in the pickle file and the value corresponding to\n        the variable_name will be used.\n    input_exprs_str: A string that specifies python expressions for inputs.\n        * In the format of: '<input_key>=<python expression>'.\n        * numpy module is available as np.\n    input_examples_str: A string that specifies tf.Example with dictionary.\n        * In the format of: '<input_key>=<[{feature:value list}]>'\n\n  Returns:\n    A dictionary that maps input tensor keys to numpy ndarrays.\n\n  Raises:\n    RuntimeError: An error when a key is specified, but the input file contains\n        multiple numpy ndarrays, none of which matches the given key.\n    RuntimeError: An error when no key is specified, but the input file contains\n        more than one numpy ndarrays.\n  \"\"\"\n  tensor_key_feed_dict = {}\n\n  inputs = preprocess_inputs_arg_string(inputs_str)\n  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str)\n  input_examples = preprocess_input_examples_arg_string(input_examples_str)\n\n  for input_tensor_key, (filename, variable_name) in inputs.items():\n    data = np.load(file_io.FileIO(filename, mode='rb'), allow_pickle=True)  # pylint: disable=unexpected-keyword-arg\n\n    # When a variable_name key is specified for the input file\n    if variable_name:\n      # if file contains a single ndarray, ignore the input name\n      if isinstance(data, np.ndarray):\n        logging.warn(\n            'Input file %s contains a single ndarray. Name key \\\"%s\\\" ignored.'\n            % (filename, variable_name))\n        tensor_key_feed_dict[input_tensor_key] = data\n      else:\n        if variable_name in data:\n          tensor_key_feed_dict[input_tensor_key] = data[variable_name]\n        else:\n          raise RuntimeError(\n              'Input file %s does not contain variable with name \\\"%s\\\".' %\n              (filename, variable_name))\n    # When no key is specified for the input file.\n    else:\n      # Check if npz file only contains a single numpy ndarray.\n      if isinstance(data, np.lib.npyio.NpzFile):\n        variable_name_list = data.files\n        if len(variable_name_list) != 1:\n          raise RuntimeError(\n              'Input file %s contains more than one ndarrays. Please specify '\n              'the name of ndarray to use.' % filename)\n        tensor_key_feed_dict[input_tensor_key] = data[variable_name_list[0]]\n      else:\n        tensor_key_feed_dict[input_tensor_key] = data\n\n  # When input is a python expression:\n  for input_tensor_key, py_expr_evaluated in input_exprs.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified with both --inputs and --input_exprs'\n          ' options. Value in --input_exprs will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = py_expr_evaluated\n\n  # When input is a tf.Example:\n  for input_tensor_key, example in input_examples.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified in multiple options. Value in '\n          '--input_examples will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = example\n  return tensor_key_feed_dict\n\n\ndef show(args):\n  \"\"\"Function triggered by show command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  # If all tag is specified, display all information.\n  if args.all:\n    _show_all(args.dir)\n  else:\n    # If no tag is specified, display all tag_set, if no signature_def key is\n    # specified, display all SignatureDef keys, else show input output tensor\n    # information corresponding to the given SignatureDef key\n    if args.tag_set is None:\n      _show_tag_sets(args.dir)\n    else:\n      if args.signature_def is None:\n        _show_signature_def_map_keys(args.dir, args.tag_set)\n      else:\n        _show_inputs_outputs(args.dir, args.tag_set, args.signature_def)\n\n\ndef run(args):\n  \"\"\"Function triggered by run command.\n\n  Args:\n    args: A namespace parsed from command line.\n\n  Raises:\n    AttributeError: An error when neither --inputs nor --input_exprs is passed\n    to run command.\n  \"\"\"\n  if not args.inputs and not args.input_exprs and not args.input_examples:\n    raise AttributeError(\n        'At least one of --inputs, --input_exprs or --input_examples must be '\n        'required')\n  tensor_key_feed_dict = load_inputs_from_input_arg_string(\n      args.inputs, args.input_exprs, args.input_examples)\n  run_saved_model_with_feed_dict(\n      args.dir,\n      args.tag_set,\n      args.signature_def,\n      tensor_key_feed_dict,\n      args.outdir,\n      args.overwrite,\n      worker=args.worker,\n      init_tpu=args.init_tpu,\n      use_tfrt=args.use_tfrt,\n      tf_debug=args.tf_debug)\n\n\ndef scan(args):\n  \"\"\"Function triggered by scan command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  if args.tag_set:\n    scan_meta_graph_def(\n        saved_model_utils.get_meta_graph_def(args.dir, args.tag_set))\n  else:\n    saved_model = saved_model_utils.read_saved_model(args.dir)\n    for meta_graph_def in saved_model.meta_graphs:\n      scan_meta_graph_def(meta_graph_def)\n\n\ndef convert_with_tensorrt(args):\n  \"\"\"Function triggered by 'convert tensorrt' command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  # Import here instead of at top, because this will crash if TensorRT is\n  # not installed\n  from tensorflow.python.compiler.tensorrt import trt_convert as trt  # pylint: disable=g-import-not-at-top\n\n  if not args.convert_tf1_model:\n    params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n        max_workspace_size_bytes=args.max_workspace_size_bytes,\n        precision_mode=args.precision_mode,\n        minimum_segment_size=args.minimum_segment_size)\n    converter = trt.TrtGraphConverterV2(\n        input_saved_model_dir=args.dir,\n        input_saved_model_tags=args.tag_set.split(','),\n        **params._asdict())\n    try:\n      converter.convert()\n    except Exception as e:\n      raise RuntimeError(\n          '{}. Try passing \"--convert_tf1_model=True\".'.format(e))\n    converter.save(output_saved_model_dir=args.output_dir)\n  else:\n    trt.create_inference_graph(\n        None,\n        None,\n        max_batch_size=1,\n        max_workspace_size_bytes=args.max_workspace_size_bytes,\n        precision_mode=args.precision_mode,\n        minimum_segment_size=args.minimum_segment_size,\n        is_dynamic_op=True,\n        input_saved_model_dir=args.dir,\n        input_saved_model_tags=args.tag_set.split(','),\n        output_saved_model_dir=args.output_dir)\n\n\ndef freeze_model(args):\n  \"\"\"Function triggered by freeze_model command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  checkpoint_path = (\n      args.checkpoint_path\n      or os.path.join(args.dir, 'variables/variables'))\n  if not args.variables_to_feed:\n    variables_to_feed = []\n  elif args.variables_to_feed.lower() == 'all':\n    variables_to_feed = None  # We will identify them after.\n  else:\n    variables_to_feed = args.variables_to_feed.split(',')\n\n  saved_model_aot_compile.freeze_model(\n      checkpoint_path=checkpoint_path,\n      meta_graph_def=saved_model_utils.get_meta_graph_def(\n          args.dir, args.tag_set),\n      signature_def_key=args.signature_def_key,\n      variables_to_feed=variables_to_feed,\n      output_prefix=args.output_prefix)\n\n\ndef aot_compile_cpu(args):\n  \"\"\"Function triggered by aot_compile_cpu command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  checkpoint_path = (\n      args.checkpoint_path\n      or os.path.join(args.dir, 'variables/variables'))\n  if not args.variables_to_feed:\n    variables_to_feed = []\n  elif args.variables_to_feed.lower() == 'all':\n    variables_to_feed = None  # We will identify them after.\n  else:\n    variables_to_feed = args.variables_to_feed.split(',')\n\n  saved_model_aot_compile.aot_compile_cpu_meta_graph_def(\n      checkpoint_path=checkpoint_path,\n      meta_graph_def=saved_model_utils.get_meta_graph_def(\n          args.dir, args.tag_set),\n      signature_def_key=args.signature_def_key,\n      variables_to_feed=variables_to_feed,\n      output_prefix=args.output_prefix,\n      target_triple=args.target_triple,\n      target_cpu=args.target_cpu,\n      cpp_class=args.cpp_class,\n      multithreading=args.multithreading.lower() not in ('f', 'false', '0'))\n\n\ndef add_show_subparser(subparsers):\n  \"\"\"Add parser for `show`.\"\"\"\n  show_msg = (\n      'Usage examples:\\n'\n      'To show all tag-sets in a SavedModel:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model\\n\\n'\n      'To show all available SignatureDef keys in a '\n      'MetaGraphDef specified by its tag-set:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve\\n\\n'\n      'For a MetaGraphDef with multiple tags in the tag-set, all tags must be '\n      'passed in, separated by \\';\\':\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve,gpu\\n\\n'\n      'To show all inputs and outputs TensorInfo for a specific'\n      ' SignatureDef specified by the SignatureDef key in a'\n      ' MetaGraph.\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve'\n      ' --signature_def serving_default\\n\\n'\n      'To show all available information in the SavedModel:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --all')\n  parser_show = subparsers.add_parser(\n      'show',\n      description=show_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_show.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to inspect')\n  parser_show.add_argument(\n      '--all',\n      action='store_true',\n      help='if set, will output all information in given SavedModel')\n  parser_show.add_argument(\n      '--tag_set',\n      type=str,\n      default=None,\n      help='tag-set of graph in SavedModel to show, separated by \\',\\'')\n  parser_show.add_argument(\n      '--signature_def',\n      type=str,\n      default=None,\n      metavar='SIGNATURE_DEF_KEY',\n      help='key of SignatureDef to display input(s) and output(s) for')\n  parser_show.set_defaults(func=show)\n\n\ndef add_run_subparser(subparsers):\n  \"\"\"Add parser for `run`.\"\"\"\n  run_msg = ('Usage example:\\n'\n             'To run input tensors from files through a MetaGraphDef and save'\n             ' the output tensors to files:\\n'\n             '$saved_model_cli show --dir /tmp/saved_model --tag_set serve \\\\\\n'\n             '   --signature_def serving_default \\\\\\n'\n             '   --inputs input1_key=/tmp/124.npz[x],input2_key=/tmp/123.npy '\n             '\\\\\\n'\n             '   --input_exprs \\'input3_key=np.ones(2)\\' \\\\\\n'\n             '   --input_examples '\n             '\\'input4_key=[{\"id\":[26],\"weights\":[0.5, 0.5]}]\\' \\\\\\n'\n             '   --outdir=/out\\n\\n'\n             'For more information about input file format, please see:\\n'\n             'https://www.tensorflow.org/guide/saved_model_cli\\n')\n  parser_run = subparsers.add_parser(\n      'run', description=run_msg, formatter_class=argparse.RawTextHelpFormatter)\n  parser_run.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to execute')\n  parser_run.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to load, separated by \\',\\'')\n  parser_run.add_argument(\n      '--signature_def',\n      type=str,\n      required=True,\n      metavar='SIGNATURE_DEF_KEY',\n      help='key of SignatureDef to run')\n  msg = ('Loading inputs from files, in the format of \\'<input_key>=<filename>,'\n         ' or \\'<input_key>=<filename>[<variable_name>]\\', separated by \\';\\'.'\n         ' The file format can only be from .npy, .npz or pickle.')\n  parser_run.add_argument('--inputs', type=str, default='', help=msg)\n  msg = ('Specifying inputs by python expressions, in the format of'\n         ' \"<input_key>=\\'<python expression>\\'\", separated by \\';\\'. '\n         'numpy module is available as \\'np\\'. Please note that the expression '\n         'will be evaluated as-is, and is susceptible to code injection. '\n         'When this is set, the value will override duplicate input keys from '\n         '--inputs option.')\n  parser_run.add_argument('--input_exprs', type=str, default='', help=msg)\n  msg = (\n      'Specifying tf.Example inputs as list of dictionaries. For example: '\n      '<input_key>=[{feature0:value_list,feature1:value_list}]. Use \";\" to '\n      'separate input keys. Will override duplicate input keys from --inputs '\n      'and --input_exprs option.')\n  parser_run.add_argument('--input_examples', type=str, default='', help=msg)\n  parser_run.add_argument(\n      '--outdir',\n      type=str,\n      default=None,\n      help='if specified, output tensor(s) will be saved to given directory')\n  parser_run.add_argument(\n      '--overwrite',\n      action='store_true',\n      help='if set, output file will be overwritten if it already exists.')\n  parser_run.add_argument(\n      '--tf_debug',\n      action='store_true',\n      help='if set, will use TensorFlow Debugger (tfdbg) to watch the '\n           'intermediate Tensors and runtime GraphDefs while running the '\n           'SavedModel.')\n  parser_run.add_argument(\n      '--worker',\n      type=str,\n      default=None,\n      help='if specified, a Session will be run on the worker. '\n           'Valid worker specification is a bns or gRPC path.')\n  parser_run.add_argument(\n      '--init_tpu',\n      action='store_true',\n      default=None,\n      help='if specified, tpu.initialize_system will be called on the Session. '\n           'This option should be only used if the worker is a TPU job.')\n  parser_run.add_argument(\n      '--use_tfrt',\n      action='store_true',\n      default=None,\n      help='if specified, TFRT session will be used, instead of TF1 session.')\n  parser_run.set_defaults(func=run)\n\n\ndef add_scan_subparser(subparsers):\n  \"\"\"Add parser for `scan`.\"\"\"\n  scan_msg = ('Usage example:\\n'\n              'To scan for denylisted ops in SavedModel:\\n'\n              '$saved_model_cli scan --dir /tmp/saved_model\\n'\n              'To scan a specific MetaGraph, pass in --tag_set\\n')\n  parser_scan = subparsers.add_parser(\n      'scan',\n      description=scan_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_scan.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to execute')\n  parser_scan.add_argument(\n      '--tag_set',\n      type=str,\n      help='tag-set of graph in SavedModel to scan, separated by \\',\\'')\n  parser_scan.set_defaults(func=scan)\n\n\ndef add_convert_subparser(subparsers):\n  \"\"\"Add parser for `convert`.\"\"\"\n  convert_msg = ('Usage example:\\n'\n                 'To convert the SavedModel to one that have TensorRT ops:\\n'\n                 '$saved_model_cli convert \\\\\\n'\n                 '   --dir /tmp/saved_model \\\\\\n'\n                 '   --tag_set serve \\\\\\n'\n                 '   --output_dir /tmp/saved_model_trt \\\\\\n'\n                 '   tensorrt \\n')\n  parser_convert = subparsers.add_parser(\n      'convert',\n      description=convert_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_convert.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to convert')\n  parser_convert.add_argument(\n      '--output_dir',\n      type=str,\n      required=True,\n      help='output directory for the converted SavedModel')\n  parser_convert.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to convert, separated by \\',\\'')\n  convert_subparsers = parser_convert.add_subparsers(\n      title='conversion methods',\n      description='valid conversion methods',\n      help='the conversion to run with the SavedModel')\n  parser_convert_with_tensorrt = convert_subparsers.add_parser(\n      'tensorrt',\n      description='Convert the SavedModel with Tensorflow-TensorRT integration',\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_convert_with_tensorrt.add_argument(\n      '--max_workspace_size_bytes',\n      type=int,\n      default=2 << 20,\n      help=('the maximum GPU temporary memory which the TRT engine can use at '\n            'execution time'))\n  parser_convert_with_tensorrt.add_argument(\n      '--precision_mode',\n      type=str,\n      default='FP32',\n      help='one of FP32, FP16 and INT8')\n  parser_convert_with_tensorrt.add_argument(\n      '--minimum_segment_size',\n      type=int,\n      default=3,\n      help=('the minimum number of nodes required for a subgraph to be replaced'\n            'in a TensorRT node'))\n  parser_convert_with_tensorrt.add_argument(\n      '--convert_tf1_model',\n      type=bool,\n      default=False,\n      help='support TRT conversion for TF1 models')\n  parser_convert_with_tensorrt.set_defaults(func=convert_with_tensorrt)\n\n\ndef _parse_common_freeze_and_aot(parser_compile):\n  \"\"\"Parse arguments shared by freeze model and aot_compile.\"\"\"\n  parser_compile.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to convert')\n  parser_compile.add_argument(\n      '--output_prefix',\n      type=str,\n      required=True,\n      help=('output directory + filename prefix for the resulting header(s) '\n            'and object file(s)'))\n  parser_compile.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to convert, separated by \\',\\'')\n  parser_compile.add_argument(\n      '--signature_def_key',\n      type=str,\n      default=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY,\n      help=('signature_def key to use.  '\n            'default: DEFAULT_SERVING_SIGNATURE_DEF_KEY'))\n  parser_compile.add_argument(\n      '--checkpoint_path',\n      type=str,\n      default=None,\n      help='Custom checkpoint to use (default: use the SavedModel variables)')\n  parser_compile.add_argument(\n      '--variables_to_feed',\n      type=str,\n      default='',\n      help=('The names of variables that will be fed into the network.  '\n            'Options are: empty (default; all variables are frozen, none may '\n            'be fed), \\'all\\' (all variables may be fed), or a '\n            'comma-delimited list of names of variables that may be fed.  In '\n            'the last case, the non-fed variables will be frozen in the graph.'\n            '**NOTE** Any variables passed to `variables_to_feed` *must be set '\n            'by the user*.  These variables will NOT be frozen and their '\n            'values will be uninitialized in the compiled object '\n            '(this applies to all input arguments from the signature as '\n            'well).'))\n\n\ndef add_freeze_model_subparser(subparsers):\n  \"\"\"Add parser for `freeze_model`.\"\"\"\n  compile_msg = '\\n'.join(\n      ['Usage example:',\n       'To freeze a SavedModel in preparation for tfcompile:',\n       '$saved_model_cli freeze_model \\\\',\n       '   --dir /tmp/saved_model \\\\',\n       '   --tag_set serve \\\\',\n       '   --output_prefix /tmp/saved_model_xla_aot',\n      ])\n\n  parser_compile = subparsers.add_parser(\n      'freeze_model',\n      description=compile_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  _parse_common_freeze_and_aot(parser_compile)\n  parser_compile.set_defaults(func=freeze_model)\n\n\ndef add_aot_compile_cpu_subparser(subparsers):\n  \"\"\"Add parser for `aot_compile_cpu`.\"\"\"\n  compile_msg = '\\n'.join(\n      ['Usage example:',\n       'To compile a SavedModel signature via (CPU) XLA AOT:',\n       '$saved_model_cli aot_compile_cpu \\\\',\n       '   --dir /tmp/saved_model \\\\',\n       '   --tag_set serve \\\\',\n       '   --output_dir /tmp/saved_model_xla_aot',\n       '', '',\n       'Note: Additional XLA compilation options are available by setting the ',\n       'XLA_FLAGS environment variable.  See the XLA debug options flags for ',\n       'all the options: ',\n       '  {}'.format(_XLA_DEBUG_OPTIONS_URL),\n       '',\n       'For example, to disable XLA fast math when compiling:',\n       '',\n       'XLA_FLAGS=\"--xla_cpu_enable_fast_math=false\" $saved_model_cli '\n       'aot_compile_cpu ...',\n       '',\n       'Some possibly useful flags:',\n       '  --xla_cpu_enable_fast_math=false',\n       '  --xla_force_host_platform_device_count=<num threads>',\n       '    (useful in conjunction with disabling multi threading)'\n      ])\n\n  parser_compile = subparsers.add_parser(\n      'aot_compile_cpu',\n      description=compile_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  _parse_common_freeze_and_aot(parser_compile)\n  parser_compile.add_argument(\n      '--target_triple',\n      type=str,\n      default='x86_64-pc-linux',\n      help=('Target triple for LLVM during AOT compilation.  Examples: '\n            'x86_64-none-darwin, x86_64-apple-ios, arm64-none-ios, '\n            'armv7-none-android.  More examples are available in tfcompile.bzl '\n            'in the tensorflow codebase.'))\n  parser_compile.add_argument(\n      '--target_cpu',\n      type=str,\n      default='',\n      help=('Target cpu name for LLVM during AOT compilation.  Examples: '\n            'x86_64, skylake, haswell, westmere, <empty> (unknown).  For '\n            'a complete list of options, run (for x86 targets): '\n            '`llc -march=x86 -mcpu=help`'))\n  parser_compile.add_argument(\n      '--cpp_class',\n      type=str,\n      required=True,\n      help=('The name of the generated C++ class, wrapping the generated '\n            'function.  The syntax of this flag is '\n            '[[<optional_namespace>::],...]<class_name>.  This mirrors the '\n            'C++ syntax for referring to a class, where multiple namespaces '\n            'may precede the class name, separated by double-colons.  '\n            'The class will be generated in the given namespace(s), or if no '\n            'namespaces are given, within the global namespace.'))\n  parser_compile.add_argument(\n      '--multithreading',\n      type=str,\n      default='False',\n      help=('Enable multithreading in the compiled computation.  '\n            'Note that if using this option, the resulting object files '\n            'may have external dependencies on multithreading libraries '\n            'like nsync.'))\n\n  parser_compile.set_defaults(func=aot_compile_cpu)\n\n\ndef create_parser():\n  \"\"\"Creates a parser that parse the command line arguments.\n\n  Returns:\n    A namespace parsed from command line arguments.\n  \"\"\"\n  parser = argparse.ArgumentParser(\n      description='saved_model_cli: Command-line interface for SavedModel')\n  parser.add_argument('-v', '--version', action='version', version='0.1.0')\n\n  subparsers = parser.add_subparsers(\n      title='commands', description='valid commands', help='additional help')\n\n  # show command\n  add_show_subparser(subparsers)\n\n  # run command\n  add_run_subparser(subparsers)\n\n  # scan command\n  add_scan_subparser(subparsers)\n\n  # tensorrt convert command\n  add_convert_subparser(subparsers)\n\n  # aot_compile_cpu command\n  add_aot_compile_cpu_subparser(subparsers)\n\n  # freeze_model command\n  add_freeze_model_subparser(subparsers)\n  return parser\n\n\ndef main():\n  logging.set_verbosity(logging.INFO)\n  parser = create_parser()\n  args = parser.parse_args()\n  if not hasattr(args, 'func'):\n    parser.error('too few arguments')\n  args.func(args)\n\n\nif __name__ == '__main__':\n  sys.exit(main())\n", "code_before": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Command-line interface to inspect and execute a graph in a SavedModel.\n\nFor detailed usages and examples, please refer to:\nhttps://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel\n\n\"\"\"\n\nimport argparse\nimport ast\nimport os\nimport re\nimport sys\n\nfrom absl import app  # pylint: disable=unused-import\nimport numpy as np\nimport six\n\nfrom tensorflow.core.example import example_pb2\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.debug.wrappers import local_cli_wrapper\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.eager import function as defun\nfrom tensorflow.python.framework import meta_graph as meta_graph_lib\nfrom tensorflow.python.framework import ops as ops_lib\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.saved_model import load\nfrom tensorflow.python.saved_model import loader\nfrom tensorflow.python.saved_model import save\nfrom tensorflow.python.saved_model import signature_constants\nfrom tensorflow.python.tools import saved_model_aot_compile\nfrom tensorflow.python.tools import saved_model_utils\nfrom tensorflow.python.tpu import tpu\nfrom tensorflow.python.util.compat import collections_abc\n\n\n_XLA_DEBUG_OPTIONS_URL = (\n    'https://github.com/tensorflow/tensorflow/blob/master/'\n    'tensorflow/compiler/xla/debug_options_flags.cc')\n\n\n# Set of ops to denylist.\n_OP_DENYLIST = set(['WriteFile', 'ReadFile', 'PrintV2'])\n\n\ndef _show_tag_sets(saved_model_dir):\n  \"\"\"Prints the tag-sets stored in SavedModel directory.\n\n  Prints all the tag-sets for MetaGraphs stored in SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  tag_sets = saved_model_utils.get_saved_model_tag_sets(saved_model_dir)\n  print('The given SavedModel contains the following tag-sets:')\n  for tag_set in sorted(tag_sets):\n    print('%r' % ', '.join(sorted(tag_set)))\n\n\ndef _show_signature_def_map_keys(saved_model_dir, tag_set):\n  \"\"\"Prints the keys for each SignatureDef in the SignatureDef map.\n\n  Prints the list of SignatureDef keys from the SignatureDef map specified by\n  the given tag-set and SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n    tag_set: Group of tag(s) of the MetaGraphDef to get SignatureDef map from,\n        in string format, separated by ','. For tag-set contains multiple tags,\n        all tags must be passed in.\n  \"\"\"\n  signature_def_map = get_signature_def_map(saved_model_dir, tag_set)\n  print('The given SavedModel MetaGraphDef contains SignatureDefs with the '\n        'following keys:')\n  for signature_def_key in sorted(signature_def_map.keys()):\n    print('SignatureDef key: \\\"%s\\\"' % signature_def_key)\n\n\ndef _get_inputs_tensor_info_from_meta_graph_def(meta_graph_def,\n                                                signature_def_key):\n  \"\"\"Gets TensorInfo for all inputs of the SignatureDef.\n\n  Returns a dictionary that maps each input key to its TensorInfo for the given\n  signature_def_key in the meta_graph_def\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer with the SignatureDef map to\n        look up SignatureDef key.\n    signature_def_key: A SignatureDef key string.\n\n  Returns:\n    A dictionary that maps input tensor keys to TensorInfos.\n\n  Raises:\n    ValueError if `signature_def_key` is not found in the MetaGraphDef.\n  \"\"\"\n  if signature_def_key not in meta_graph_def.signature_def:\n    raise ValueError(\n        f'Could not find signature \"{signature_def_key}\". Please choose from: '\n        f'{\", \".join(meta_graph_def.signature_def.keys())}')\n  return meta_graph_def.signature_def[signature_def_key].inputs\n\n\ndef _get_outputs_tensor_info_from_meta_graph_def(meta_graph_def,\n                                                 signature_def_key):\n  \"\"\"Gets TensorInfos for all outputs of the SignatureDef.\n\n  Returns a dictionary that maps each output key to its TensorInfo for the given\n  signature_def_key in the meta_graph_def.\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer with the SignatureDefmap to\n    look up signature_def_key.\n    signature_def_key: A SignatureDef key string.\n\n  Returns:\n    A dictionary that maps output tensor keys to TensorInfos.\n  \"\"\"\n  return meta_graph_def.signature_def[signature_def_key].outputs\n\n\ndef _show_inputs_outputs(saved_model_dir, tag_set, signature_def_key, indent=0):\n  \"\"\"Prints input and output TensorInfos.\n\n  Prints the details of input and output TensorInfos for the SignatureDef mapped\n  by the given signature_def_key.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n    tag_set: Group of tag(s) of the MetaGraphDef, in string format, separated by\n        ','. For tag-set contains multiple tags, all tags must be passed in.\n    signature_def_key: A SignatureDef key string.\n    indent: How far (in increments of 2 spaces) to indent each line of output.\n  \"\"\"\n  meta_graph_def = saved_model_utils.get_meta_graph_def(saved_model_dir,\n                                                        tag_set)\n  inputs_tensor_info = _get_inputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n  outputs_tensor_info = _get_outputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n\n  indent_str = '  ' * indent\n  def in_print(s):\n    print(indent_str + s)\n\n  in_print('The given SavedModel SignatureDef contains the following input(s):')\n  for input_key, input_tensor in sorted(inputs_tensor_info.items()):\n    in_print('  inputs[\\'%s\\'] tensor_info:' % input_key)\n    _print_tensor_info(input_tensor, indent+1)\n\n  in_print('The given SavedModel SignatureDef contains the following '\n           'output(s):')\n  for output_key, output_tensor in sorted(outputs_tensor_info.items()):\n    in_print('  outputs[\\'%s\\'] tensor_info:' % output_key)\n    _print_tensor_info(output_tensor, indent+1)\n\n  in_print('Method name is: %s' %\n           meta_graph_def.signature_def[signature_def_key].method_name)\n\n\ndef _show_defined_functions(saved_model_dir):\n  \"\"\"Prints the callable concrete and polymorphic functions of the Saved Model.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  meta_graphs = saved_model_utils.read_saved_model(saved_model_dir).meta_graphs\n  has_object_graph_def = False\n\n  for meta_graph_def in meta_graphs:\n    has_object_graph_def |= meta_graph_def.HasField('object_graph_def')\n  if not has_object_graph_def:\n    return\n  with ops_lib.Graph().as_default():\n    trackable_object = load.load(saved_model_dir)\n\n  print('\\nConcrete Functions:', end='')\n  children = list(\n      save._AugmentedGraphView(trackable_object)  # pylint: disable=protected-access\n      .list_children(trackable_object))\n  children = sorted(children, key=lambda x: x.name)\n  for name, child in children:\n    concrete_functions = []\n    if isinstance(child, defun.ConcreteFunction):\n      concrete_functions.append(child)\n    elif isinstance(child, def_function.Function):\n      concrete_functions.extend(\n          child._list_all_concrete_functions_for_serialization())  # pylint: disable=protected-access\n    else:\n      continue\n    print('\\n  Function Name: \\'%s\\'' % name)\n    concrete_functions = sorted(concrete_functions, key=lambda x: x.name)\n    for index, concrete_function in enumerate(concrete_functions, 1):\n      args, kwargs = None, None\n      if concrete_function.structured_input_signature:\n        args, kwargs = concrete_function.structured_input_signature\n      elif concrete_function._arg_keywords:  # pylint: disable=protected-access\n        # For pure ConcreteFunctions we might have nothing better than\n        # _arg_keywords.\n        args = concrete_function._arg_keywords  # pylint: disable=protected-access\n      if args:\n        print('    Option #%d' % index)\n        print('      Callable with:')\n        _print_args(args, indent=4)\n      if kwargs:\n        _print_args(kwargs, 'Named Argument', indent=4)\n\n\ndef _print_args(arguments, argument_type='Argument', indent=0):\n  \"\"\"Formats and prints the argument of the concrete functions defined in the model.\n\n  Args:\n    arguments: Arguments to format print.\n    argument_type: Type of arguments.\n    indent: How far (in increments of 2 spaces) to indent each line of\n     output.\n  \"\"\"\n  indent_str = '  ' * indent\n\n  def _maybe_add_quotes(value):\n    is_quotes = '\\'' * isinstance(value, str)\n    return is_quotes + str(value) + is_quotes\n\n  def in_print(s, end='\\n'):\n    print(indent_str + s, end=end)\n\n  for index, element in enumerate(arguments, 1):\n    if indent == 4:\n      in_print('%s #%d' % (argument_type, index))\n    if isinstance(element, six.string_types):\n      in_print('  %s' % element)\n    elif isinstance(element, tensor_spec.TensorSpec):\n      print((indent + 1) * '  ' + '%s: %s' % (element.name, repr(element)))\n    elif (isinstance(element, collections_abc.Iterable) and\n          not isinstance(element, dict)):\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: [', end='')\n      for value in element:\n        print('%s' % _maybe_add_quotes(value), end=', ')\n      print('\\b\\b]')\n    elif isinstance(element, dict):\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: {', end='')\n      for (key, value) in element.items():\n        print('\\'%s\\': %s' % (str(key), _maybe_add_quotes(value)), end=', ')\n      print('\\b\\b}')\n    else:\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: %s' % str(element))\n\n\ndef _print_tensor_info(tensor_info, indent=0):\n  \"\"\"Prints details of the given tensor_info.\n\n  Args:\n    tensor_info: TensorInfo object to be printed.\n    indent: How far (in increments of 2 spaces) to indent each line output\n  \"\"\"\n  indent_str = '  ' * indent\n  def in_print(s):\n    print(indent_str + s)\n\n  in_print('    dtype: ' +\n           {value: key\n            for (key, value) in types_pb2.DataType.items()}[tensor_info.dtype])\n  # Display shape as tuple.\n  if tensor_info.tensor_shape.unknown_rank:\n    shape = 'unknown_rank'\n  else:\n    dims = [str(dim.size) for dim in tensor_info.tensor_shape.dim]\n    shape = ', '.join(dims)\n    shape = '(' + shape + ')'\n  in_print('    shape: ' + shape)\n  in_print('    name: ' + tensor_info.name)\n\n\ndef _show_all(saved_model_dir):\n  \"\"\"Prints tag-set, SignatureDef and Inputs/Outputs information in SavedModel.\n\n  Prints all tag-set, SignatureDef and Inputs/Outputs information stored in\n  SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  tag_sets = saved_model_utils.get_saved_model_tag_sets(saved_model_dir)\n  for tag_set in sorted(tag_sets):\n    print(\"\\nMetaGraphDef with tag-set: '%s' \"\n          \"contains the following SignatureDefs:\" % ', '.join(tag_set))\n\n    tag_set = ','.join(tag_set)\n    signature_def_map = get_signature_def_map(saved_model_dir, tag_set)\n    for signature_def_key in sorted(signature_def_map.keys()):\n      print('\\nsignature_def[\\'' + signature_def_key + '\\']:')\n      _show_inputs_outputs(saved_model_dir, tag_set, signature_def_key,\n                           indent=1)\n  _show_defined_functions(saved_model_dir)\n\n\ndef get_meta_graph_def(saved_model_dir, tag_set):\n  \"\"\"DEPRECATED: Use saved_model_utils.get_meta_graph_def instead.\n\n  Gets MetaGraphDef from SavedModel. Returns the MetaGraphDef for the given\n  tag-set and SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect or execute.\n    tag_set: Group of tag(s) of the MetaGraphDef to load, in string format,\n        separated by ','. For tag-set contains multiple tags, all tags must be\n        passed in.\n\n  Raises:\n    RuntimeError: An error when the given tag-set does not exist in the\n        SavedModel.\n\n  Returns:\n    A MetaGraphDef corresponding to the tag-set.\n  \"\"\"\n  return saved_model_utils.get_meta_graph_def(saved_model_dir, tag_set)\n\n\ndef get_signature_def_map(saved_model_dir, tag_set):\n  \"\"\"Gets SignatureDef map from a MetaGraphDef in a SavedModel.\n\n  Returns the SignatureDef map for the given tag-set in the SavedModel\n  directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect or execute.\n    tag_set: Group of tag(s) of the MetaGraphDef with the SignatureDef map, in\n        string format, separated by ','. For tag-set contains multiple tags, all\n        tags must be passed in.\n\n  Returns:\n    A SignatureDef map that maps from string keys to SignatureDefs.\n  \"\"\"\n  meta_graph = saved_model_utils.get_meta_graph_def(saved_model_dir, tag_set)\n  return meta_graph.signature_def\n\n\ndef scan_meta_graph_def(meta_graph_def):\n  \"\"\"Scans meta_graph_def and reports if there are ops on denylist.\n\n  Print ops if they are on black list, or print success if no denylisted ops\n  found.\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer.\n  \"\"\"\n  all_ops_set = set(\n      meta_graph_lib.ops_used_by_graph_def(meta_graph_def.graph_def))\n  denylisted_ops = _OP_DENYLIST & all_ops_set\n  if denylisted_ops:\n    # TODO(yifeif): print more warnings\n    print(\n        'MetaGraph with tag set %s contains the following denylisted ops:' %\n        meta_graph_def.meta_info_def.tags, denylisted_ops)\n  else:\n    print('MetaGraph with tag set %s does not contain denylisted ops.' %\n          meta_graph_def.meta_info_def.tags)\n\n\ndef run_saved_model_with_feed_dict(saved_model_dir,\n                                   tag_set,\n                                   signature_def_key,\n                                   input_tensor_key_feed_dict,\n                                   outdir,\n                                   overwrite_flag,\n                                   worker=None,\n                                   init_tpu=False,\n                                   use_tfrt=False,\n                                   tf_debug=False):\n  \"\"\"Runs SavedModel and fetch all outputs.\n\n  Runs the input dictionary through the MetaGraphDef within a SavedModel\n  specified by the given tag_set and SignatureDef. Also save the outputs to file\n  if outdir is not None.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to execute.\n    tag_set: Group of tag(s) of the MetaGraphDef with the SignatureDef map, in\n        string format, separated by ','. For tag-set contains multiple tags, all\n        tags must be passed in.\n    signature_def_key: A SignatureDef key string.\n    input_tensor_key_feed_dict: A dictionary maps input keys to numpy ndarrays.\n    outdir: A directory to save the outputs to. If the directory doesn't exist,\n        it will be created.\n    overwrite_flag: A boolean flag to allow overwrite output file if file with\n        the same name exists.\n    worker: If provided, the session will be run on the worker.  Valid worker\n        specification is a bns or gRPC path.\n    init_tpu: If true, the TPU system will be initialized after the session\n        is created.\n    use_tfrt: If true, TFRT session will be used.\n    tf_debug: A boolean flag to use TensorFlow Debugger (TFDBG) to observe the\n        intermediate Tensor values and runtime GraphDefs while running the\n        SavedModel.\n\n  Raises:\n    ValueError: When any of the input tensor keys is not valid.\n    RuntimeError: An error when output file already exists and overwrite is not\n    enabled.\n  \"\"\"\n  # Get a list of output tensor names.\n  meta_graph_def = saved_model_utils.get_meta_graph_def(saved_model_dir,\n                                                        tag_set)\n\n  # Re-create feed_dict based on input tensor name instead of key as session.run\n  # uses tensor name.\n  inputs_tensor_info = _get_inputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n\n  # Check if input tensor keys are valid.\n  for input_key_name in input_tensor_key_feed_dict.keys():\n    if input_key_name not in inputs_tensor_info:\n      raise ValueError(\n          '\"%s\" is not a valid input key. Please choose from %s, or use '\n          '--show option.' %\n          (input_key_name, '\"' + '\", \"'.join(inputs_tensor_info.keys()) + '\"'))\n\n  inputs_feed_dict = {\n      inputs_tensor_info[key].name: tensor\n      for key, tensor in input_tensor_key_feed_dict.items()\n  }\n  # Get outputs\n  outputs_tensor_info = _get_outputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n  # Sort to preserve order because we need to go from value to key later.\n  output_tensor_keys_sorted = sorted(outputs_tensor_info.keys())\n  output_tensor_names_sorted = [\n      outputs_tensor_info[tensor_key].name\n      for tensor_key in output_tensor_keys_sorted\n  ]\n\n  config = None\n  if use_tfrt:\n    logging.info('Using TFRT session.')\n    config = config_pb2.ConfigProto(\n        experimental=config_pb2.ConfigProto.Experimental(use_tfrt=True))\n  with session.Session(worker, graph=ops_lib.Graph(), config=config) as sess:\n    if init_tpu:\n      print('Initializing TPU System ...')\n      # This is needed for freshly started worker, or if the job\n      # restarts after a preemption.\n      sess.run(tpu.initialize_system())\n\n    loader.load(sess, tag_set.split(','), saved_model_dir)\n\n    if tf_debug:\n      sess = local_cli_wrapper.LocalCLIDebugWrapperSession(sess)\n\n    outputs = sess.run(output_tensor_names_sorted, feed_dict=inputs_feed_dict)\n\n    for i, output in enumerate(outputs):\n      output_tensor_key = output_tensor_keys_sorted[i]\n      print('Result for output key %s:\\n%s' % (output_tensor_key, output))\n\n      # Only save if outdir is specified.\n      if outdir:\n        # Create directory if outdir does not exist\n        if not os.path.isdir(outdir):\n          os.makedirs(outdir)\n        output_full_path = os.path.join(outdir, output_tensor_key + '.npy')\n\n        # If overwrite not enabled and file already exist, error out\n        if not overwrite_flag and os.path.exists(output_full_path):\n          raise RuntimeError(\n              'Output file %s already exists. Add \\\"--overwrite\\\" to overwrite'\n              ' the existing output files.' % output_full_path)\n\n        np.save(output_full_path, output)\n        print('Output %s is saved to %s' % (output_tensor_key,\n                                            output_full_path))\n\n\ndef preprocess_inputs_arg_string(inputs_str):\n  \"\"\"Parses input arg into dictionary that maps input to file/variable tuple.\n\n  Parses input string in the format of, for example,\n  \"input1=filename1[variable_name1],input2=filename2\" into a\n  dictionary looks like\n  {'input_key1': (filename1, variable_name1),\n   'input_key2': (file2, None)}\n  , which maps input keys to a tuple of file name and variable name(None if\n  empty).\n\n  Args:\n    inputs_str: A string that specified where to load inputs. Inputs are\n    separated by semicolons.\n        * For each input key:\n            '<input_key>=<filename>' or\n            '<input_key>=<filename>[<variable_name>]'\n        * The optional 'variable_name' key will be set to None if not specified.\n\n  Returns:\n    A dictionary that maps input keys to a tuple of file name and variable name.\n\n  Raises:\n    RuntimeError: An error when the given input string is in a bad format.\n  \"\"\"\n  input_dict = {}\n  inputs_raw = inputs_str.split(';')\n  for input_raw in filter(bool, inputs_raw):  # skip empty strings\n    # Format of input=filename[variable_name]'\n    match = re.match(r'([^=]+)=([^\\[\\]]+)\\[([^\\[\\]]+)\\]$', input_raw)\n\n    if match:\n      input_dict[match.group(1)] = match.group(2), match.group(3)\n    else:\n      # Format of input=filename'\n      match = re.match(r'([^=]+)=([^\\[\\]]+)$', input_raw)\n      if match:\n        input_dict[match.group(1)] = match.group(2), None\n      else:\n        raise RuntimeError(\n            '--inputs \"%s\" format is incorrect. Please follow'\n            '\"<input_key>=<filename>\", or'\n            '\"<input_key>=<filename>[<variable_name>]\"' % input_raw)\n\n  return input_dict\n\n\ndef preprocess_input_exprs_arg_string(input_exprs_str, safe=True):\n  \"\"\"Parses input arg into dictionary that maps input key to python expression.\n\n  Parses input string in the format of 'input_key=<python expression>' into a\n  dictionary that maps each input_key to its python expression.\n\n  Args:\n    input_exprs_str: A string that specifies python expression for input keys.\n      Each input is separated by semicolon. For each input key:\n        'input_key=<python expression>'\n    safe: Whether to evaluate the python expression as literals or allow\n      arbitrary calls (e.g. numpy usage).\n\n  Returns:\n    A dictionary that maps input keys to their values.\n\n  Raises:\n    RuntimeError: An error when the given input string is in a bad format.\n  \"\"\"\n  input_dict = {}\n\n  for input_raw in filter(bool, input_exprs_str.split(';')):\n    if '=' not in input_exprs_str:\n      raise RuntimeError('--input_exprs \"%s\" format is incorrect. Please follow'\n                         '\"<input_key>=<python expression>\"' % input_exprs_str)\n    input_key, expr = input_raw.split('=', 1)\n    if safe:\n      try:\n        input_dict[input_key] = ast.literal_eval(expr)\n      except:\n        raise RuntimeError(\n            f'Expression \"{expr}\" is not a valid python literal.')\n    else:\n      # ast.literal_eval does not work with numpy expressions\n      input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\n  return input_dict\n\n\ndef preprocess_input_examples_arg_string(input_examples_str):\n  \"\"\"Parses input into dict that maps input keys to lists of tf.Example.\n\n  Parses input string in the format of 'input_key1=[{feature_name:\n  feature_list}];input_key2=[{feature_name:feature_list}];' into a dictionary\n  that maps each input_key to its list of serialized tf.Example.\n\n  Args:\n    input_examples_str: A string that specifies a list of dictionaries of\n    feature_names and their feature_lists for each input.\n    Each input is separated by semicolon. For each input key:\n      'input=[{feature_name1: feature_list1, feature_name2:feature_list2}]'\n      items in feature_list can be the type of float, int, long or str.\n\n  Returns:\n    A dictionary that maps input keys to lists of serialized tf.Example.\n\n  Raises:\n    ValueError: An error when the given tf.Example is not a list.\n  \"\"\"\n  input_dict = preprocess_input_exprs_arg_string(input_examples_str)\n  for input_key, example_list in input_dict.items():\n    if not isinstance(example_list, list):\n      raise ValueError(\n          'tf.Example input must be a list of dictionaries, but \"%s\" is %s' %\n          (example_list, type(example_list)))\n    input_dict[input_key] = [\n        _create_example_string(example) for example in example_list\n    ]\n  return input_dict\n\n\ndef _create_example_string(example_dict):\n  \"\"\"Create a serialized tf.example from feature dictionary.\"\"\"\n  example = example_pb2.Example()\n  for feature_name, feature_list in example_dict.items():\n    if not isinstance(feature_list, list):\n      raise ValueError('feature value must be a list, but %s: \"%s\" is %s' %\n                       (feature_name, feature_list, type(feature_list)))\n    if isinstance(feature_list[0], float):\n      example.features.feature[feature_name].float_list.value.extend(\n          feature_list)\n    elif isinstance(feature_list[0], str):\n      example.features.feature[feature_name].bytes_list.value.extend(\n          [f.encode('utf8') for f in feature_list])\n    elif isinstance(feature_list[0], bytes):\n      example.features.feature[feature_name].bytes_list.value.extend(\n          feature_list)\n    elif isinstance(feature_list[0], six.integer_types):\n      example.features.feature[feature_name].int64_list.value.extend(\n          feature_list)\n    else:\n      raise ValueError(\n          'Type %s for value %s is not supported for tf.train.Feature.' %\n          (type(feature_list[0]), feature_list[0]))\n  return example.SerializeToString()\n\n\ndef load_inputs_from_input_arg_string(inputs_str, input_exprs_str,\n                                      input_examples_str):\n  \"\"\"Parses input arg strings and create inputs feed_dict.\n\n  Parses '--inputs' string for inputs to be loaded from file, and parses\n  '--input_exprs' string for inputs to be evaluated from python expression.\n  '--input_examples' string for inputs to be created from tf.example feature\n  dictionary list.\n\n  Args:\n    inputs_str: A string that specified where to load inputs. Each input is\n        separated by semicolon.\n        * For each input key:\n            '<input_key>=<filename>' or\n            '<input_key>=<filename>[<variable_name>]'\n        * The optional 'variable_name' key will be set to None if not specified.\n        * File specified by 'filename' will be loaded using numpy.load. Inputs\n            can be loaded from only .npy, .npz or pickle files.\n        * The \"[variable_name]\" key is optional depending on the input file type\n            as descripted in more details below.\n        When loading from a npy file, which always contains a numpy ndarray, the\n        content will be directly assigned to the specified input tensor. If a\n        variable_name is specified, it will be ignored and a warning will be\n        issued.\n        When loading from a npz zip file, user can specify which variable within\n        the zip file to load for the input tensor inside the square brackets. If\n        nothing is specified, this function will check that only one file is\n        included in the zip and load it for the specified input tensor.\n        When loading from a pickle file, if no variable_name is specified in the\n        square brackets, whatever that is inside the pickle file will be passed\n        to the specified input tensor, else SavedModel CLI will assume a\n        dictionary is stored in the pickle file and the value corresponding to\n        the variable_name will be used.\n    input_exprs_str: A string that specifies python expressions for inputs.\n        * In the format of: '<input_key>=<python expression>'.\n        * numpy module is available as np.\n    input_examples_str: A string that specifies tf.Example with dictionary.\n        * In the format of: '<input_key>=<[{feature:value list}]>'\n\n  Returns:\n    A dictionary that maps input tensor keys to numpy ndarrays.\n\n  Raises:\n    RuntimeError: An error when a key is specified, but the input file contains\n        multiple numpy ndarrays, none of which matches the given key.\n    RuntimeError: An error when no key is specified, but the input file contains\n        more than one numpy ndarrays.\n  \"\"\"\n  tensor_key_feed_dict = {}\n\n  inputs = preprocess_inputs_arg_string(inputs_str)\n  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str, safe=False)\n  input_examples = preprocess_input_examples_arg_string(input_examples_str)\n\n  for input_tensor_key, (filename, variable_name) in inputs.items():\n    data = np.load(file_io.FileIO(filename, mode='rb'), allow_pickle=True)  # pylint: disable=unexpected-keyword-arg\n\n    # When a variable_name key is specified for the input file\n    if variable_name:\n      # if file contains a single ndarray, ignore the input name\n      if isinstance(data, np.ndarray):\n        logging.warn(\n            'Input file %s contains a single ndarray. Name key \\\"%s\\\" ignored.'\n            % (filename, variable_name))\n        tensor_key_feed_dict[input_tensor_key] = data\n      else:\n        if variable_name in data:\n          tensor_key_feed_dict[input_tensor_key] = data[variable_name]\n        else:\n          raise RuntimeError(\n              'Input file %s does not contain variable with name \\\"%s\\\".' %\n              (filename, variable_name))\n    # When no key is specified for the input file.\n    else:\n      # Check if npz file only contains a single numpy ndarray.\n      if isinstance(data, np.lib.npyio.NpzFile):\n        variable_name_list = data.files\n        if len(variable_name_list) != 1:\n          raise RuntimeError(\n              'Input file %s contains more than one ndarrays. Please specify '\n              'the name of ndarray to use.' % filename)\n        tensor_key_feed_dict[input_tensor_key] = data[variable_name_list[0]]\n      else:\n        tensor_key_feed_dict[input_tensor_key] = data\n\n  # When input is a python expression:\n  for input_tensor_key, py_expr_evaluated in input_exprs.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified with both --inputs and --input_exprs'\n          ' options. Value in --input_exprs will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = py_expr_evaluated\n\n  # When input is a tf.Example:\n  for input_tensor_key, example in input_examples.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified in multiple options. Value in '\n          '--input_examples will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = example\n  return tensor_key_feed_dict\n\n\ndef show(args):\n  \"\"\"Function triggered by show command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  # If all tag is specified, display all information.\n  if args.all:\n    _show_all(args.dir)\n  else:\n    # If no tag is specified, display all tag_set, if no signature_def key is\n    # specified, display all SignatureDef keys, else show input output tensor\n    # information corresponding to the given SignatureDef key\n    if args.tag_set is None:\n      _show_tag_sets(args.dir)\n    else:\n      if args.signature_def is None:\n        _show_signature_def_map_keys(args.dir, args.tag_set)\n      else:\n        _show_inputs_outputs(args.dir, args.tag_set, args.signature_def)\n\n\ndef run(args):\n  \"\"\"Function triggered by run command.\n\n  Args:\n    args: A namespace parsed from command line.\n\n  Raises:\n    AttributeError: An error when neither --inputs nor --input_exprs is passed\n    to run command.\n  \"\"\"\n  if not args.inputs and not args.input_exprs and not args.input_examples:\n    raise AttributeError(\n        'At least one of --inputs, --input_exprs or --input_examples must be '\n        'required')\n  tensor_key_feed_dict = load_inputs_from_input_arg_string(\n      args.inputs, args.input_exprs, args.input_examples)\n  run_saved_model_with_feed_dict(\n      args.dir,\n      args.tag_set,\n      args.signature_def,\n      tensor_key_feed_dict,\n      args.outdir,\n      args.overwrite,\n      worker=args.worker,\n      init_tpu=args.init_tpu,\n      use_tfrt=args.use_tfrt,\n      tf_debug=args.tf_debug)\n\n\ndef scan(args):\n  \"\"\"Function triggered by scan command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  if args.tag_set:\n    scan_meta_graph_def(\n        saved_model_utils.get_meta_graph_def(args.dir, args.tag_set))\n  else:\n    saved_model = saved_model_utils.read_saved_model(args.dir)\n    for meta_graph_def in saved_model.meta_graphs:\n      scan_meta_graph_def(meta_graph_def)\n\n\ndef convert_with_tensorrt(args):\n  \"\"\"Function triggered by 'convert tensorrt' command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  # Import here instead of at top, because this will crash if TensorRT is\n  # not installed\n  from tensorflow.python.compiler.tensorrt import trt_convert as trt  # pylint: disable=g-import-not-at-top\n\n  if not args.convert_tf1_model:\n    params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n        max_workspace_size_bytes=args.max_workspace_size_bytes,\n        precision_mode=args.precision_mode,\n        minimum_segment_size=args.minimum_segment_size)\n    converter = trt.TrtGraphConverterV2(\n        input_saved_model_dir=args.dir,\n        input_saved_model_tags=args.tag_set.split(','),\n        **params._asdict())\n    try:\n      converter.convert()\n    except Exception as e:\n      raise RuntimeError(\n          '{}. Try passing \"--convert_tf1_model=True\".'.format(e))\n    converter.save(output_saved_model_dir=args.output_dir)\n  else:\n    trt.create_inference_graph(\n        None,\n        None,\n        max_batch_size=1,\n        max_workspace_size_bytes=args.max_workspace_size_bytes,\n        precision_mode=args.precision_mode,\n        minimum_segment_size=args.minimum_segment_size,\n        is_dynamic_op=True,\n        input_saved_model_dir=args.dir,\n        input_saved_model_tags=args.tag_set.split(','),\n        output_saved_model_dir=args.output_dir)\n\n\ndef freeze_model(args):\n  \"\"\"Function triggered by freeze_model command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  checkpoint_path = (\n      args.checkpoint_path\n      or os.path.join(args.dir, 'variables/variables'))\n  if not args.variables_to_feed:\n    variables_to_feed = []\n  elif args.variables_to_feed.lower() == 'all':\n    variables_to_feed = None  # We will identify them after.\n  else:\n    variables_to_feed = args.variables_to_feed.split(',')\n\n  saved_model_aot_compile.freeze_model(\n      checkpoint_path=checkpoint_path,\n      meta_graph_def=saved_model_utils.get_meta_graph_def(\n          args.dir, args.tag_set),\n      signature_def_key=args.signature_def_key,\n      variables_to_feed=variables_to_feed,\n      output_prefix=args.output_prefix)\n\n\ndef aot_compile_cpu(args):\n  \"\"\"Function triggered by aot_compile_cpu command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  checkpoint_path = (\n      args.checkpoint_path\n      or os.path.join(args.dir, 'variables/variables'))\n  if not args.variables_to_feed:\n    variables_to_feed = []\n  elif args.variables_to_feed.lower() == 'all':\n    variables_to_feed = None  # We will identify them after.\n  else:\n    variables_to_feed = args.variables_to_feed.split(',')\n\n  saved_model_aot_compile.aot_compile_cpu_meta_graph_def(\n      checkpoint_path=checkpoint_path,\n      meta_graph_def=saved_model_utils.get_meta_graph_def(\n          args.dir, args.tag_set),\n      signature_def_key=args.signature_def_key,\n      variables_to_feed=variables_to_feed,\n      output_prefix=args.output_prefix,\n      target_triple=args.target_triple,\n      target_cpu=args.target_cpu,\n      cpp_class=args.cpp_class,\n      multithreading=args.multithreading.lower() not in ('f', 'false', '0'))\n\n\ndef add_show_subparser(subparsers):\n  \"\"\"Add parser for `show`.\"\"\"\n  show_msg = (\n      'Usage examples:\\n'\n      'To show all tag-sets in a SavedModel:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model\\n\\n'\n      'To show all available SignatureDef keys in a '\n      'MetaGraphDef specified by its tag-set:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve\\n\\n'\n      'For a MetaGraphDef with multiple tags in the tag-set, all tags must be '\n      'passed in, separated by \\';\\':\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve,gpu\\n\\n'\n      'To show all inputs and outputs TensorInfo for a specific'\n      ' SignatureDef specified by the SignatureDef key in a'\n      ' MetaGraph.\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve'\n      ' --signature_def serving_default\\n\\n'\n      'To show all available information in the SavedModel:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --all')\n  parser_show = subparsers.add_parser(\n      'show',\n      description=show_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_show.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to inspect')\n  parser_show.add_argument(\n      '--all',\n      action='store_true',\n      help='if set, will output all information in given SavedModel')\n  parser_show.add_argument(\n      '--tag_set',\n      type=str,\n      default=None,\n      help='tag-set of graph in SavedModel to show, separated by \\',\\'')\n  parser_show.add_argument(\n      '--signature_def',\n      type=str,\n      default=None,\n      metavar='SIGNATURE_DEF_KEY',\n      help='key of SignatureDef to display input(s) and output(s) for')\n  parser_show.set_defaults(func=show)\n\n\ndef add_run_subparser(subparsers):\n  \"\"\"Add parser for `run`.\"\"\"\n  run_msg = ('Usage example:\\n'\n             'To run input tensors from files through a MetaGraphDef and save'\n             ' the output tensors to files:\\n'\n             '$saved_model_cli show --dir /tmp/saved_model --tag_set serve \\\\\\n'\n             '   --signature_def serving_default \\\\\\n'\n             '   --inputs input1_key=/tmp/124.npz[x],input2_key=/tmp/123.npy '\n             '\\\\\\n'\n             '   --input_exprs \\'input3_key=np.ones(2)\\' \\\\\\n'\n             '   --input_examples '\n             '\\'input4_key=[{\"id\":[26],\"weights\":[0.5, 0.5]}]\\' \\\\\\n'\n             '   --outdir=/out\\n\\n'\n             'For more information about input file format, please see:\\n'\n             'https://www.tensorflow.org/guide/saved_model_cli\\n')\n  parser_run = subparsers.add_parser(\n      'run', description=run_msg, formatter_class=argparse.RawTextHelpFormatter)\n  parser_run.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to execute')\n  parser_run.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to load, separated by \\',\\'')\n  parser_run.add_argument(\n      '--signature_def',\n      type=str,\n      required=True,\n      metavar='SIGNATURE_DEF_KEY',\n      help='key of SignatureDef to run')\n  msg = ('Loading inputs from files, in the format of \\'<input_key>=<filename>,'\n         ' or \\'<input_key>=<filename>[<variable_name>]\\', separated by \\';\\'.'\n         ' The file format can only be from .npy, .npz or pickle.')\n  parser_run.add_argument('--inputs', type=str, default='', help=msg)\n  msg = ('Specifying inputs by python expressions, in the format of'\n         ' \"<input_key>=\\'<python expression>\\'\", separated by \\';\\'. '\n         'numpy module is available as \\'np\\'. Please note that the expression '\n         'will be evaluated as-is, and is susceptible to code injection. '\n         'When this is set, the value will override duplicate input keys from '\n         '--inputs option.')\n  parser_run.add_argument('--input_exprs', type=str, default='', help=msg)\n  msg = (\n      'Specifying tf.Example inputs as list of dictionaries. For example: '\n      '<input_key>=[{feature0:value_list,feature1:value_list}]. Use \";\" to '\n      'separate input keys. Will override duplicate input keys from --inputs '\n      'and --input_exprs option.')\n  parser_run.add_argument('--input_examples', type=str, default='', help=msg)\n  parser_run.add_argument(\n      '--outdir',\n      type=str,\n      default=None,\n      help='if specified, output tensor(s) will be saved to given directory')\n  parser_run.add_argument(\n      '--overwrite',\n      action='store_true',\n      help='if set, output file will be overwritten if it already exists.')\n  parser_run.add_argument(\n      '--tf_debug',\n      action='store_true',\n      help='if set, will use TensorFlow Debugger (tfdbg) to watch the '\n           'intermediate Tensors and runtime GraphDefs while running the '\n           'SavedModel.')\n  parser_run.add_argument(\n      '--worker',\n      type=str,\n      default=None,\n      help='if specified, a Session will be run on the worker. '\n           'Valid worker specification is a bns or gRPC path.')\n  parser_run.add_argument(\n      '--init_tpu',\n      action='store_true',\n      default=None,\n      help='if specified, tpu.initialize_system will be called on the Session. '\n           'This option should be only used if the worker is a TPU job.')\n  parser_run.add_argument(\n      '--use_tfrt',\n      action='store_true',\n      default=None,\n      help='if specified, TFRT session will be used, instead of TF1 session.')\n  parser_run.set_defaults(func=run)\n\n\ndef add_scan_subparser(subparsers):\n  \"\"\"Add parser for `scan`.\"\"\"\n  scan_msg = ('Usage example:\\n'\n              'To scan for denylisted ops in SavedModel:\\n'\n              '$saved_model_cli scan --dir /tmp/saved_model\\n'\n              'To scan a specific MetaGraph, pass in --tag_set\\n')\n  parser_scan = subparsers.add_parser(\n      'scan',\n      description=scan_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_scan.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to execute')\n  parser_scan.add_argument(\n      '--tag_set',\n      type=str,\n      help='tag-set of graph in SavedModel to scan, separated by \\',\\'')\n  parser_scan.set_defaults(func=scan)\n\n\ndef add_convert_subparser(subparsers):\n  \"\"\"Add parser for `convert`.\"\"\"\n  convert_msg = ('Usage example:\\n'\n                 'To convert the SavedModel to one that have TensorRT ops:\\n'\n                 '$saved_model_cli convert \\\\\\n'\n                 '   --dir /tmp/saved_model \\\\\\n'\n                 '   --tag_set serve \\\\\\n'\n                 '   --output_dir /tmp/saved_model_trt \\\\\\n'\n                 '   tensorrt \\n')\n  parser_convert = subparsers.add_parser(\n      'convert',\n      description=convert_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_convert.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to convert')\n  parser_convert.add_argument(\n      '--output_dir',\n      type=str,\n      required=True,\n      help='output directory for the converted SavedModel')\n  parser_convert.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to convert, separated by \\',\\'')\n  convert_subparsers = parser_convert.add_subparsers(\n      title='conversion methods',\n      description='valid conversion methods',\n      help='the conversion to run with the SavedModel')\n  parser_convert_with_tensorrt = convert_subparsers.add_parser(\n      'tensorrt',\n      description='Convert the SavedModel with Tensorflow-TensorRT integration',\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_convert_with_tensorrt.add_argument(\n      '--max_workspace_size_bytes',\n      type=int,\n      default=2 << 20,\n      help=('the maximum GPU temporary memory which the TRT engine can use at '\n            'execution time'))\n  parser_convert_with_tensorrt.add_argument(\n      '--precision_mode',\n      type=str,\n      default='FP32',\n      help='one of FP32, FP16 and INT8')\n  parser_convert_with_tensorrt.add_argument(\n      '--minimum_segment_size',\n      type=int,\n      default=3,\n      help=('the minimum number of nodes required for a subgraph to be replaced'\n            'in a TensorRT node'))\n  parser_convert_with_tensorrt.add_argument(\n      '--convert_tf1_model',\n      type=bool,\n      default=False,\n      help='support TRT conversion for TF1 models')\n  parser_convert_with_tensorrt.set_defaults(func=convert_with_tensorrt)\n\n\ndef _parse_common_freeze_and_aot(parser_compile):\n  \"\"\"Parse arguments shared by freeze model and aot_compile.\"\"\"\n  parser_compile.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to convert')\n  parser_compile.add_argument(\n      '--output_prefix',\n      type=str,\n      required=True,\n      help=('output directory + filename prefix for the resulting header(s) '\n            'and object file(s)'))\n  parser_compile.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to convert, separated by \\',\\'')\n  parser_compile.add_argument(\n      '--signature_def_key',\n      type=str,\n      default=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY,\n      help=('signature_def key to use.  '\n            'default: DEFAULT_SERVING_SIGNATURE_DEF_KEY'))\n  parser_compile.add_argument(\n      '--checkpoint_path',\n      type=str,\n      default=None,\n      help='Custom checkpoint to use (default: use the SavedModel variables)')\n  parser_compile.add_argument(\n      '--variables_to_feed',\n      type=str,\n      default='',\n      help=('The names of variables that will be fed into the network.  '\n            'Options are: empty (default; all variables are frozen, none may '\n            'be fed), \\'all\\' (all variables may be fed), or a '\n            'comma-delimited list of names of variables that may be fed.  In '\n            'the last case, the non-fed variables will be frozen in the graph.'\n            '**NOTE** Any variables passed to `variables_to_feed` *must be set '\n            'by the user*.  These variables will NOT be frozen and their '\n            'values will be uninitialized in the compiled object '\n            '(this applies to all input arguments from the signature as '\n            'well).'))\n\n\ndef add_freeze_model_subparser(subparsers):\n  \"\"\"Add parser for `freeze_model`.\"\"\"\n  compile_msg = '\\n'.join(\n      ['Usage example:',\n       'To freeze a SavedModel in preparation for tfcompile:',\n       '$saved_model_cli freeze_model \\\\',\n       '   --dir /tmp/saved_model \\\\',\n       '   --tag_set serve \\\\',\n       '   --output_prefix /tmp/saved_model_xla_aot',\n      ])\n\n  parser_compile = subparsers.add_parser(\n      'freeze_model',\n      description=compile_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  _parse_common_freeze_and_aot(parser_compile)\n  parser_compile.set_defaults(func=freeze_model)\n\n\ndef add_aot_compile_cpu_subparser(subparsers):\n  \"\"\"Add parser for `aot_compile_cpu`.\"\"\"\n  compile_msg = '\\n'.join(\n      ['Usage example:',\n       'To compile a SavedModel signature via (CPU) XLA AOT:',\n       '$saved_model_cli aot_compile_cpu \\\\',\n       '   --dir /tmp/saved_model \\\\',\n       '   --tag_set serve \\\\',\n       '   --output_dir /tmp/saved_model_xla_aot',\n       '', '',\n       'Note: Additional XLA compilation options are available by setting the ',\n       'XLA_FLAGS environment variable.  See the XLA debug options flags for ',\n       'all the options: ',\n       '  {}'.format(_XLA_DEBUG_OPTIONS_URL),\n       '',\n       'For example, to disable XLA fast math when compiling:',\n       '',\n       'XLA_FLAGS=\"--xla_cpu_enable_fast_math=false\" $saved_model_cli '\n       'aot_compile_cpu ...',\n       '',\n       'Some possibly useful flags:',\n       '  --xla_cpu_enable_fast_math=false',\n       '  --xla_force_host_platform_device_count=<num threads>',\n       '    (useful in conjunction with disabling multi threading)'\n      ])\n\n  parser_compile = subparsers.add_parser(\n      'aot_compile_cpu',\n      description=compile_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  _parse_common_freeze_and_aot(parser_compile)\n  parser_compile.add_argument(\n      '--target_triple',\n      type=str,\n      default='x86_64-pc-linux',\n      help=('Target triple for LLVM during AOT compilation.  Examples: '\n            'x86_64-none-darwin, x86_64-apple-ios, arm64-none-ios, '\n            'armv7-none-android.  More examples are available in tfcompile.bzl '\n            'in the tensorflow codebase.'))\n  parser_compile.add_argument(\n      '--target_cpu',\n      type=str,\n      default='',\n      help=('Target cpu name for LLVM during AOT compilation.  Examples: '\n            'x86_64, skylake, haswell, westmere, <empty> (unknown).  For '\n            'a complete list of options, run (for x86 targets): '\n            '`llc -march=x86 -mcpu=help`'))\n  parser_compile.add_argument(\n      '--cpp_class',\n      type=str,\n      required=True,\n      help=('The name of the generated C++ class, wrapping the generated '\n            'function.  The syntax of this flag is '\n            '[[<optional_namespace>::],...]<class_name>.  This mirrors the '\n            'C++ syntax for referring to a class, where multiple namespaces '\n            'may precede the class name, separated by double-colons.  '\n            'The class will be generated in the given namespace(s), or if no '\n            'namespaces are given, within the global namespace.'))\n  parser_compile.add_argument(\n      '--multithreading',\n      type=str,\n      default='False',\n      help=('Enable multithreading in the compiled computation.  '\n            'Note that if using this option, the resulting object files '\n            'may have external dependencies on multithreading libraries '\n            'like nsync.'))\n\n  parser_compile.set_defaults(func=aot_compile_cpu)\n\n\ndef create_parser():\n  \"\"\"Creates a parser that parse the command line arguments.\n\n  Returns:\n    A namespace parsed from command line arguments.\n  \"\"\"\n  parser = argparse.ArgumentParser(\n      description='saved_model_cli: Command-line interface for SavedModel')\n  parser.add_argument('-v', '--version', action='version', version='0.1.0')\n\n  subparsers = parser.add_subparsers(\n      title='commands', description='valid commands', help='additional help')\n\n  # show command\n  add_show_subparser(subparsers)\n\n  # run command\n  add_run_subparser(subparsers)\n\n  # scan command\n  add_scan_subparser(subparsers)\n\n  # tensorrt convert command\n  add_convert_subparser(subparsers)\n\n  # aot_compile_cpu command\n  add_aot_compile_cpu_subparser(subparsers)\n\n  # freeze_model command\n  add_freeze_model_subparser(subparsers)\n  return parser\n\n\ndef main():\n  logging.set_verbosity(logging.INFO)\n  parser = create_parser()\n  args = parser.parse_args()\n  if not hasattr(args, 'func'):\n    parser.error('too few arguments')\n  args.func(args)\n\n\nif __name__ == '__main__':\n  sys.exit(main())\n", "patch": "@@ -684,7 +684,7 @@ def load_inputs_from_input_arg_string(inputs_str, input_exprs_str,\n   tensor_key_feed_dict = {}\n \n   inputs = preprocess_inputs_arg_string(inputs_str)\n-  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str, safe=False)\n+  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str)\n   input_examples = preprocess_input_examples_arg_string(input_examples_str)\n \n   for input_tensor_key, (filename, variable_name) in inputs.items():", "file_path": "files/2022_5/222", "file_language": "py", "file_name": "tensorflow/python/tools/saved_model_cli.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "def _show_tag_sets(saved_model_dir):\n  \"\"\"Prints the tag-sets stored in SavedModel directory.\n\n  Prints all the tag-sets for MetaGraphs stored in SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  tag_sets = saved_model_utils.get_saved_model_tag_sets(saved_model_dir)\n  print('The given SavedModel contains the following tag-sets:')\n  for tag_set in sorted(tag_sets):\n    print('%r' % ', '.join(sorted(tag_set)))", "target": 0}, {"function": "def _show_signature_def_map_keys(saved_model_dir, tag_set):\n  \"\"\"Prints the keys for each SignatureDef in the SignatureDef map.\n\n  Prints the list of SignatureDef keys from the SignatureDef map specified by\n  the given tag-set and SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n    tag_set: Group of tag(s) of the MetaGraphDef to get SignatureDef map from,\n        in string format, separated by ','. For tag-set contains multiple tags,\n        all tags must be passed in.\n  \"\"\"\n  signature_def_map = get_signature_def_map(saved_model_dir, tag_set)\n  print('The given SavedModel MetaGraphDef contains SignatureDefs with the '\n        'following keys:')\n  for signature_def_key in sorted(signature_def_map.keys()):\n    print('SignatureDef key: \\\"%s\\\"' % signature_def_key)", "target": 0}, {"function": "def _get_inputs_tensor_info_from_meta_graph_def(meta_graph_def,\n                                                signature_def_key):\n  \"\"\"Gets TensorInfo for all inputs of the SignatureDef.\n\n  Returns a dictionary that maps each input key to its TensorInfo for the given\n  signature_def_key in the meta_graph_def\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer with the SignatureDef map to\n        look up SignatureDef key.\n    signature_def_key: A SignatureDef key string.\n\n  Returns:\n    A dictionary that maps input tensor keys to TensorInfos.\n\n  Raises:\n    ValueError if `signature_def_key` is not found in the MetaGraphDef.\n  \"\"\"\n  if signature_def_key not in meta_graph_def.signature_def:\n    raise ValueError(\n        f'Could not find signature \"{signature_def_key}\". Please choose from: '\n        f'{\", \".join(meta_graph_def.signature_def.keys())}')\n  return meta_graph_def.signature_def[signature_def_key].inputs", "target": 0}, {"function": "def _get_outputs_tensor_info_from_meta_graph_def(meta_graph_def,\n                                                 signature_def_key):\n  \"\"\"Gets TensorInfos for all outputs of the SignatureDef.\n\n  Returns a dictionary that maps each output key to its TensorInfo for the given\n  signature_def_key in the meta_graph_def.\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer with the SignatureDefmap to\n    look up signature_def_key.\n    signature_def_key: A SignatureDef key string.\n\n  Returns:\n    A dictionary that maps output tensor keys to TensorInfos.\n  \"\"\"\n  return meta_graph_def.signature_def[signature_def_key].outputs", "target": 0}, {"function": "def _show_inputs_outputs(saved_model_dir, tag_set, signature_def_key, indent=0):\n  \"\"\"Prints input and output TensorInfos.\n\n  Prints the details of input and output TensorInfos for the SignatureDef mapped\n  by the given signature_def_key.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n    tag_set: Group of tag(s) of the MetaGraphDef, in string format, separated by\n        ','. For tag-set contains multiple tags, all tags must be passed in.\n    signature_def_key: A SignatureDef key string.\n    indent: How far (in increments of 2 spaces) to indent each line of output.\n  \"\"\"\n  meta_graph_def = saved_model_utils.get_meta_graph_def(saved_model_dir,\n                                                        tag_set)\n  inputs_tensor_info = _get_inputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n  outputs_tensor_info = _get_outputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n\n  indent_str = '  ' * indent\n  def in_print(s):\n    print(indent_str + s)\n\n  in_print('The given SavedModel SignatureDef contains the following input(s):')\n  for input_key, input_tensor in sorted(inputs_tensor_info.items()):\n    in_print('  inputs[\\'%s\\'] tensor_info:' % input_key)\n    _print_tensor_info(input_tensor, indent+1)\n\n  in_print('The given SavedModel SignatureDef contains the following '\n           'output(s):')\n  for output_key, output_tensor in sorted(outputs_tensor_info.items()):\n    in_print('  outputs[\\'%s\\'] tensor_info:' % output_key)\n    _print_tensor_info(output_tensor, indent+1)\n\n  in_print('Method name is: %s' %\n           meta_graph_def.signature_def[signature_def_key].method_name)", "target": 0}, {"function": "def _show_defined_functions(saved_model_dir):\n  \"\"\"Prints the callable concrete and polymorphic functions of the Saved Model.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  meta_graphs = saved_model_utils.read_saved_model(saved_model_dir).meta_graphs\n  has_object_graph_def = False\n\n  for meta_graph_def in meta_graphs:\n    has_object_graph_def |= meta_graph_def.HasField('object_graph_def')\n  if not has_object_graph_def:\n    return\n  with ops_lib.Graph().as_default():\n    trackable_object = load.load(saved_model_dir)\n\n  print('\\nConcrete Functions:', end='')\n  children = list(\n      save._AugmentedGraphView(trackable_object)  # pylint: disable=protected-access\n      .list_children(trackable_object))\n  children = sorted(children, key=lambda x: x.name)\n  for name, child in children:\n    concrete_functions = []\n    if isinstance(child, defun.ConcreteFunction):\n      concrete_functions.append(child)\n    elif isinstance(child, def_function.Function):\n      concrete_functions.extend(\n          child._list_all_concrete_functions_for_serialization())  # pylint: disable=protected-access\n    else:\n      continue\n    print('\\n  Function Name: \\'%s\\'' % name)\n    concrete_functions = sorted(concrete_functions, key=lambda x: x.name)\n    for index, concrete_function in enumerate(concrete_functions, 1):\n      args, kwargs = None, None\n      if concrete_function.structured_input_signature:\n        args, kwargs = concrete_function.structured_input_signature\n      elif concrete_function._arg_keywords:  # pylint: disable=protected-access\n        # For pure ConcreteFunctions we might have nothing better than\n        # _arg_keywords.\n        args = concrete_function._arg_keywords  # pylint: disable=protected-access\n      if args:\n        print('    Option #%d' % index)\n        print('      Callable with:')\n        _print_args(args, indent=4)\n      if kwargs:\n        _print_args(kwargs, 'Named Argument', indent=4)", "target": 0}, {"function": "def _print_args(arguments, argument_type='Argument', indent=0):\n  \"\"\"Formats and prints the argument of the concrete functions defined in the model.\n\n  Args:\n    arguments: Arguments to format print.\n    argument_type: Type of arguments.\n    indent: How far (in increments of 2 spaces) to indent each line of\n     output.\n  \"\"\"\n  indent_str = '  ' * indent\n\n  def _maybe_add_quotes(value):\n    is_quotes = '\\'' * isinstance(value, str)\n    return is_quotes + str(value) + is_quotes\n\n  def in_print(s, end='\\n'):\n    print(indent_str + s, end=end)\n\n  for index, element in enumerate(arguments, 1):\n    if indent == 4:\n      in_print('%s #%d' % (argument_type, index))\n    if isinstance(element, six.string_types):\n      in_print('  %s' % element)\n    elif isinstance(element, tensor_spec.TensorSpec):\n      print((indent + 1) * '  ' + '%s: %s' % (element.name, repr(element)))\n    elif (isinstance(element, collections_abc.Iterable) and\n          not isinstance(element, dict)):\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: [', end='')\n      for value in element:\n        print('%s' % _maybe_add_quotes(value), end=', ')\n      print('\\b\\b]')\n    elif isinstance(element, dict):\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: {', end='')\n      for (key, value) in element.items():\n        print('\\'%s\\': %s' % (str(key), _maybe_add_quotes(value)), end=', ')\n      print('\\b\\b}')\n    else:\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: %s' % str(element))", "target": 0}, {"function": "def _print_tensor_info(tensor_info, indent=0):\n  \"\"\"Prints details of the given tensor_info.\n\n  Args:\n    tensor_info: TensorInfo object to be printed.\n    indent: How far (in increments of 2 spaces) to indent each line output\n  \"\"\"\n  indent_str = '  ' * indent\n  def in_print(s):\n    print(indent_str + s)\n\n  in_print('    dtype: ' +\n           {value: key\n            for (key, value) in types_pb2.DataType.items()}[tensor_info.dtype])\n  # Display shape as tuple.\n  if tensor_info.tensor_shape.unknown_rank:\n    shape = 'unknown_rank'\n  else:\n    dims = [str(dim.size) for dim in tensor_info.tensor_shape.dim]\n    shape = ', '.join(dims)\n    shape = '(' + shape + ')'\n  in_print('    shape: ' + shape)\n  in_print('    name: ' + tensor_info.name)", "target": 0}, {"function": "def _show_all(saved_model_dir):\n  \"\"\"Prints tag-set, SignatureDef and Inputs/Outputs information in SavedModel.\n\n  Prints all tag-set, SignatureDef and Inputs/Outputs information stored in\n  SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  tag_sets = saved_model_utils.get_saved_model_tag_sets(saved_model_dir)\n  for tag_set in sorted(tag_sets):\n    print(\"\\nMetaGraphDef with tag-set: '%s' \"\n          \"contains the following SignatureDefs:\" % ', '.join(tag_set))\n\n    tag_set = ','.join(tag_set)\n    signature_def_map = get_signature_def_map(saved_model_dir, tag_set)\n    for signature_def_key in sorted(signature_def_map.keys()):\n      print('\\nsignature_def[\\'' + signature_def_key + '\\']:')\n      _show_inputs_outputs(saved_model_dir, tag_set, signature_def_key,\n                           indent=1)\n  _show_defined_functions(saved_model_dir)", "target": 0}, {"function": "def get_meta_graph_def(saved_model_dir, tag_set):\n  \"\"\"DEPRECATED: Use saved_model_utils.get_meta_graph_def instead.\n\n  Gets MetaGraphDef from SavedModel. Returns the MetaGraphDef for the given\n  tag-set and SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect or execute.\n    tag_set: Group of tag(s) of the MetaGraphDef to load, in string format,\n        separated by ','. For tag-set contains multiple tags, all tags must be\n        passed in.\n\n  Raises:\n    RuntimeError: An error when the given tag-set does not exist in the\n        SavedModel.\n\n  Returns:\n    A MetaGraphDef corresponding to the tag-set.\n  \"\"\"\n  return saved_model_utils.get_meta_graph_def(saved_model_dir, tag_set)", "target": 0}, {"function": "def get_signature_def_map(saved_model_dir, tag_set):\n  \"\"\"Gets SignatureDef map from a MetaGraphDef in a SavedModel.\n\n  Returns the SignatureDef map for the given tag-set in the SavedModel\n  directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect or execute.\n    tag_set: Group of tag(s) of the MetaGraphDef with the SignatureDef map, in\n        string format, separated by ','. For tag-set contains multiple tags, all\n        tags must be passed in.\n\n  Returns:\n    A SignatureDef map that maps from string keys to SignatureDefs.\n  \"\"\"\n  meta_graph = saved_model_utils.get_meta_graph_def(saved_model_dir, tag_set)\n  return meta_graph.signature_def", "target": 0}, {"function": "def scan_meta_graph_def(meta_graph_def):\n  \"\"\"Scans meta_graph_def and reports if there are ops on denylist.\n\n  Print ops if they are on black list, or print success if no denylisted ops\n  found.\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer.\n  \"\"\"\n  all_ops_set = set(\n      meta_graph_lib.ops_used_by_graph_def(meta_graph_def.graph_def))\n  denylisted_ops = _OP_DENYLIST & all_ops_set\n  if denylisted_ops:\n    # TODO(yifeif): print more warnings\n    print(\n        'MetaGraph with tag set %s contains the following denylisted ops:' %\n        meta_graph_def.meta_info_def.tags, denylisted_ops)\n  else:\n    print('MetaGraph with tag set %s does not contain denylisted ops.' %\n          meta_graph_def.meta_info_def.tags)", "target": 0}, {"function": "def run_saved_model_with_feed_dict(saved_model_dir,\n                                   tag_set,\n                                   signature_def_key,\n                                   input_tensor_key_feed_dict,\n                                   outdir,\n                                   overwrite_flag,\n                                   worker=None,\n                                   init_tpu=False,\n                                   use_tfrt=False,\n                                   tf_debug=False):\n  \"\"\"Runs SavedModel and fetch all outputs.\n\n  Runs the input dictionary through the MetaGraphDef within a SavedModel\n  specified by the given tag_set and SignatureDef. Also save the outputs to file\n  if outdir is not None.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to execute.\n    tag_set: Group of tag(s) of the MetaGraphDef with the SignatureDef map, in\n        string format, separated by ','. For tag-set contains multiple tags, all\n        tags must be passed in.\n    signature_def_key: A SignatureDef key string.\n    input_tensor_key_feed_dict: A dictionary maps input keys to numpy ndarrays.\n    outdir: A directory to save the outputs to. If the directory doesn't exist,\n        it will be created.\n    overwrite_flag: A boolean flag to allow overwrite output file if file with\n        the same name exists.\n    worker: If provided, the session will be run on the worker.  Valid worker\n        specification is a bns or gRPC path.\n    init_tpu: If true, the TPU system will be initialized after the session\n        is created.\n    use_tfrt: If true, TFRT session will be used.\n    tf_debug: A boolean flag to use TensorFlow Debugger (TFDBG) to observe the\n        intermediate Tensor values and runtime GraphDefs while running the\n        SavedModel.\n\n  Raises:\n    ValueError: When any of the input tensor keys is not valid.\n    RuntimeError: An error when output file already exists and overwrite is not\n    enabled.\n  \"\"\"\n  # Get a list of output tensor names.\n  meta_graph_def = saved_model_utils.get_meta_graph_def(saved_model_dir,\n                                                        tag_set)\n\n  # Re-create feed_dict based on input tensor name instead of key as session.run\n  # uses tensor name.\n  inputs_tensor_info = _get_inputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n\n  # Check if input tensor keys are valid.\n  for input_key_name in input_tensor_key_feed_dict.keys():\n    if input_key_name not in inputs_tensor_info:\n      raise ValueError(\n          '\"%s\" is not a valid input key. Please choose from %s, or use '\n          '--show option.' %\n          (input_key_name, '\"' + '\", \"'.join(inputs_tensor_info.keys()) + '\"'))\n\n  inputs_feed_dict = {\n      inputs_tensor_info[key].name: tensor\n      for key, tensor in input_tensor_key_feed_dict.items()\n  }\n  # Get outputs\n  outputs_tensor_info = _get_outputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n  # Sort to preserve order because we need to go from value to key later.\n  output_tensor_keys_sorted = sorted(outputs_tensor_info.keys())\n  output_tensor_names_sorted = [\n      outputs_tensor_info[tensor_key].name\n      for tensor_key in output_tensor_keys_sorted\n  ]\n\n  config = None\n  if use_tfrt:\n    logging.info('Using TFRT session.')\n    config = config_pb2.ConfigProto(\n        experimental=config_pb2.ConfigProto.Experimental(use_tfrt=True))\n  with session.Session(worker, graph=ops_lib.Graph(), config=config) as sess:\n    if init_tpu:\n      print('Initializing TPU System ...')\n      # This is needed for freshly started worker, or if the job\n      # restarts after a preemption.\n      sess.run(tpu.initialize_system())\n\n    loader.load(sess, tag_set.split(','), saved_model_dir)\n\n    if tf_debug:\n      sess = local_cli_wrapper.LocalCLIDebugWrapperSession(sess)\n\n    outputs = sess.run(output_tensor_names_sorted, feed_dict=inputs_feed_dict)\n\n    for i, output in enumerate(outputs):\n      output_tensor_key = output_tensor_keys_sorted[i]\n      print('Result for output key %s:\\n%s' % (output_tensor_key, output))\n\n      # Only save if outdir is specified.\n      if outdir:\n        # Create directory if outdir does not exist\n        if not os.path.isdir(outdir):\n          os.makedirs(outdir)\n        output_full_path = os.path.join(outdir, output_tensor_key + '.npy')\n\n        # If overwrite not enabled and file already exist, error out\n        if not overwrite_flag and os.path.exists(output_full_path):\n          raise RuntimeError(\n              'Output file %s already exists. Add \\\"--overwrite\\\" to overwrite'\n              ' the existing output files.' % output_full_path)\n\n        np.save(output_full_path, output)\n        print('Output %s is saved to %s' % (output_tensor_key,\n                                            output_full_path))", "target": 0}, {"function": "def preprocess_inputs_arg_string(inputs_str):\n  \"\"\"Parses input arg into dictionary that maps input to file/variable tuple.\n\n  Parses input string in the format of, for example,\n  \"input1=filename1[variable_name1],input2=filename2\" into a\n  dictionary looks like\n  {'input_key1': (filename1, variable_name1),\n   'input_key2': (file2, None)}\n  , which maps input keys to a tuple of file name and variable name(None if\n  empty).\n\n  Args:\n    inputs_str: A string that specified where to load inputs. Inputs are\n    separated by semicolons.\n        * For each input key:\n            '<input_key>=<filename>' or\n            '<input_key>=<filename>[<variable_name>]'\n        * The optional 'variable_name' key will be set to None if not specified.\n\n  Returns:\n    A dictionary that maps input keys to a tuple of file name and variable name.\n\n  Raises:\n    RuntimeError: An error when the given input string is in a bad format.\n  \"\"\"\n  input_dict = {}\n  inputs_raw = inputs_str.split(';')\n  for input_raw in filter(bool, inputs_raw):  # skip empty strings\n    # Format of input=filename[variable_name]'\n    match = re.match(r'([^=]+)=([^\\[\\]]+)\\[([^\\[\\]]+)\\]$', input_raw)\n\n    if match:\n      input_dict[match.group(1)] = match.group(2), match.group(3)\n    else:\n      # Format of input=filename'\n      match = re.match(r'([^=]+)=([^\\[\\]]+)$', input_raw)\n      if match:\n        input_dict[match.group(1)] = match.group(2), None\n      else:\n        raise RuntimeError(\n            '--inputs \"%s\" format is incorrect. Please follow'\n            '\"<input_key>=<filename>\", or'\n            '\"<input_key>=<filename>[<variable_name>]\"' % input_raw)\n\n  return input_dict", "target": 0}, {"function": "def preprocess_input_exprs_arg_string(input_exprs_str, safe=True):\n  \"\"\"Parses input arg into dictionary that maps input key to python expression.\n\n  Parses input string in the format of 'input_key=<python expression>' into a\n  dictionary that maps each input_key to its python expression.\n\n  Args:\n    input_exprs_str: A string that specifies python expression for input keys.\n      Each input is separated by semicolon. For each input key:\n        'input_key=<python expression>'\n    safe: Whether to evaluate the python expression as literals or allow\n      arbitrary calls (e.g. numpy usage).\n\n  Returns:\n    A dictionary that maps input keys to their values.\n\n  Raises:\n    RuntimeError: An error when the given input string is in a bad format.\n  \"\"\"\n  input_dict = {}\n\n  for input_raw in filter(bool, input_exprs_str.split(';')):\n    if '=' not in input_exprs_str:\n      raise RuntimeError('--input_exprs \"%s\" format is incorrect. Please follow'\n                         '\"<input_key>=<python expression>\"' % input_exprs_str)\n    input_key, expr = input_raw.split('=', 1)\n    if safe:\n      try:\n        input_dict[input_key] = ast.literal_eval(expr)\n      except:\n        raise RuntimeError(\n            f'Expression \"{expr}\" is not a valid python literal.')\n    else:\n      # ast.literal_eval does not work with numpy expressions\n      input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\n  return input_dict", "target": 0}, {"function": "def preprocess_input_examples_arg_string(input_examples_str):\n  \"\"\"Parses input into dict that maps input keys to lists of tf.Example.\n\n  Parses input string in the format of 'input_key1=[{feature_name:\n  feature_list}];input_key2=[{feature_name:feature_list}];' into a dictionary\n  that maps each input_key to its list of serialized tf.Example.\n\n  Args:\n    input_examples_str: A string that specifies a list of dictionaries of\n    feature_names and their feature_lists for each input.\n    Each input is separated by semicolon. For each input key:\n      'input=[{feature_name1: feature_list1, feature_name2:feature_list2}]'\n      items in feature_list can be the type of float, int, long or str.\n\n  Returns:\n    A dictionary that maps input keys to lists of serialized tf.Example.\n\n  Raises:\n    ValueError: An error when the given tf.Example is not a list.\n  \"\"\"\n  input_dict = preprocess_input_exprs_arg_string(input_examples_str)\n  for input_key, example_list in input_dict.items():\n    if not isinstance(example_list, list):\n      raise ValueError(\n          'tf.Example input must be a list of dictionaries, but \"%s\" is %s' %\n          (example_list, type(example_list)))\n    input_dict[input_key] = [\n        _create_example_string(example) for example in example_list\n    ]\n  return input_dict", "target": 0}, {"function": "def _create_example_string(example_dict):\n  \"\"\"Create a serialized tf.example from feature dictionary.\"\"\"\n  example = example_pb2.Example()\n  for feature_name, feature_list in example_dict.items():\n    if not isinstance(feature_list, list):\n      raise ValueError('feature value must be a list, but %s: \"%s\" is %s' %\n                       (feature_name, feature_list, type(feature_list)))\n    if isinstance(feature_list[0], float):\n      example.features.feature[feature_name].float_list.value.extend(\n          feature_list)\n    elif isinstance(feature_list[0], str):\n      example.features.feature[feature_name].bytes_list.value.extend(\n          [f.encode('utf8') for f in feature_list])\n    elif isinstance(feature_list[0], bytes):\n      example.features.feature[feature_name].bytes_list.value.extend(\n          feature_list)\n    elif isinstance(feature_list[0], six.integer_types):\n      example.features.feature[feature_name].int64_list.value.extend(\n          feature_list)\n    else:\n      raise ValueError(\n          'Type %s for value %s is not supported for tf.train.Feature.' %\n          (type(feature_list[0]), feature_list[0]))\n  return example.SerializeToString()", "target": 0}, {"function": "def load_inputs_from_input_arg_string(inputs_str, input_exprs_str,\n                                      input_examples_str):\n  \"\"\"Parses input arg strings and create inputs feed_dict.\n\n  Parses '--inputs' string for inputs to be loaded from file, and parses\n  '--input_exprs' string for inputs to be evaluated from python expression.\n  '--input_examples' string for inputs to be created from tf.example feature\n  dictionary list.\n\n  Args:\n    inputs_str: A string that specified where to load inputs. Each input is\n        separated by semicolon.\n        * For each input key:\n            '<input_key>=<filename>' or\n            '<input_key>=<filename>[<variable_name>]'\n        * The optional 'variable_name' key will be set to None if not specified.\n        * File specified by 'filename' will be loaded using numpy.load. Inputs\n            can be loaded from only .npy, .npz or pickle files.\n        * The \"[variable_name]\" key is optional depending on the input file type\n            as descripted in more details below.\n        When loading from a npy file, which always contains a numpy ndarray, the\n        content will be directly assigned to the specified input tensor. If a\n        variable_name is specified, it will be ignored and a warning will be\n        issued.\n        When loading from a npz zip file, user can specify which variable within\n        the zip file to load for the input tensor inside the square brackets. If\n        nothing is specified, this function will check that only one file is\n        included in the zip and load it for the specified input tensor.\n        When loading from a pickle file, if no variable_name is specified in the\n        square brackets, whatever that is inside the pickle file will be passed\n        to the specified input tensor, else SavedModel CLI will assume a\n        dictionary is stored in the pickle file and the value corresponding to\n        the variable_name will be used.\n    input_exprs_str: A string that specifies python expressions for inputs.\n        * In the format of: '<input_key>=<python expression>'.\n        * numpy module is available as np.\n    input_examples_str: A string that specifies tf.Example with dictionary.\n        * In the format of: '<input_key>=<[{feature:value list}]>'\n\n  Returns:\n    A dictionary that maps input tensor keys to numpy ndarrays.\n\n  Raises:\n    RuntimeError: An error when a key is specified, but the input file contains\n        multiple numpy ndarrays, none of which matches the given key.\n    RuntimeError: An error when no key is specified, but the input file contains\n        more than one numpy ndarrays.\n  \"\"\"\n  tensor_key_feed_dict = {}\n\n  inputs = preprocess_inputs_arg_string(inputs_str)\n  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str, safe=False)\n  input_examples = preprocess_input_examples_arg_string(input_examples_str)\n\n  for input_tensor_key, (filename, variable_name) in inputs.items():\n    data = np.load(file_io.FileIO(filename, mode='rb'), allow_pickle=True)  # pylint: disable=unexpected-keyword-arg\n\n    # When a variable_name key is specified for the input file\n    if variable_name:\n      # if file contains a single ndarray, ignore the input name\n      if isinstance(data, np.ndarray):\n        logging.warn(\n            'Input file %s contains a single ndarray. Name key \\\"%s\\\" ignored.'\n            % (filename, variable_name))\n        tensor_key_feed_dict[input_tensor_key] = data\n      else:\n        if variable_name in data:\n          tensor_key_feed_dict[input_tensor_key] = data[variable_name]\n        else:\n          raise RuntimeError(\n              'Input file %s does not contain variable with name \\\"%s\\\".' %\n              (filename, variable_name))\n    # When no key is specified for the input file.\n    else:\n      # Check if npz file only contains a single numpy ndarray.\n      if isinstance(data, np.lib.npyio.NpzFile):\n        variable_name_list = data.files\n        if len(variable_name_list) != 1:\n          raise RuntimeError(\n              'Input file %s contains more than one ndarrays. Please specify '\n              'the name of ndarray to use.' % filename)\n        tensor_key_feed_dict[input_tensor_key] = data[variable_name_list[0]]\n      else:\n        tensor_key_feed_dict[input_tensor_key] = data\n\n  # When input is a python expression:\n  for input_tensor_key, py_expr_evaluated in input_exprs.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified with both --inputs and --input_exprs'\n          ' options. Value in --input_exprs will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = py_expr_evaluated\n\n  # When input is a tf.Example:\n  for input_tensor_key, example in input_examples.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified in multiple options. Value in '\n          '--input_examples will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = example\n  return tensor_key_feed_dict", "target": 0}, {"function": "def show(args):\n  \"\"\"Function triggered by show command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  # If all tag is specified, display all information.\n  if args.all:\n    _show_all(args.dir)\n  else:\n    # If no tag is specified, display all tag_set, if no signature_def key is\n    # specified, display all SignatureDef keys, else show input output tensor\n    # information corresponding to the given SignatureDef key\n    if args.tag_set is None:\n      _show_tag_sets(args.dir)\n    else:\n      if args.signature_def is None:\n        _show_signature_def_map_keys(args.dir, args.tag_set)\n      else:\n        _show_inputs_outputs(args.dir, args.tag_set, args.signature_def)", "target": 0}, {"function": "def run(args):\n  \"\"\"Function triggered by run command.\n\n  Args:\n    args: A namespace parsed from command line.\n\n  Raises:\n    AttributeError: An error when neither --inputs nor --input_exprs is passed\n    to run command.\n  \"\"\"\n  if not args.inputs and not args.input_exprs and not args.input_examples:\n    raise AttributeError(\n        'At least one of --inputs, --input_exprs or --input_examples must be '\n        'required')\n  tensor_key_feed_dict = load_inputs_from_input_arg_string(\n      args.inputs, args.input_exprs, args.input_examples)\n  run_saved_model_with_feed_dict(\n      args.dir,\n      args.tag_set,\n      args.signature_def,\n      tensor_key_feed_dict,\n      args.outdir,\n      args.overwrite,\n      worker=args.worker,\n      init_tpu=args.init_tpu,\n      use_tfrt=args.use_tfrt,\n      tf_debug=args.tf_debug)", "target": 0}, {"function": "def scan(args):\n  \"\"\"Function triggered by scan command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  if args.tag_set:\n    scan_meta_graph_def(\n        saved_model_utils.get_meta_graph_def(args.dir, args.tag_set))\n  else:\n    saved_model = saved_model_utils.read_saved_model(args.dir)\n    for meta_graph_def in saved_model.meta_graphs:\n      scan_meta_graph_def(meta_graph_def)", "target": 0}, {"function": "def convert_with_tensorrt(args):\n  \"\"\"Function triggered by 'convert tensorrt' command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  # Import here instead of at top, because this will crash if TensorRT is\n  # not installed\n  from tensorflow.python.compiler.tensorrt import trt_convert as trt  # pylint: disable=g-import-not-at-top\n\n  if not args.convert_tf1_model:\n    params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n        max_workspace_size_bytes=args.max_workspace_size_bytes,\n        precision_mode=args.precision_mode,\n        minimum_segment_size=args.minimum_segment_size)\n    converter = trt.TrtGraphConverterV2(\n        input_saved_model_dir=args.dir,\n        input_saved_model_tags=args.tag_set.split(','),\n        **params._asdict())\n    try:\n      converter.convert()\n    except Exception as e:\n      raise RuntimeError(\n          '{}. Try passing \"--convert_tf1_model=True\".'.format(e))\n    converter.save(output_saved_model_dir=args.output_dir)\n  else:\n    trt.create_inference_graph(\n        None,\n        None,\n        max_batch_size=1,\n        max_workspace_size_bytes=args.max_workspace_size_bytes,\n        precision_mode=args.precision_mode,\n        minimum_segment_size=args.minimum_segment_size,\n        is_dynamic_op=True,\n        input_saved_model_dir=args.dir,\n        input_saved_model_tags=args.tag_set.split(','),\n        output_saved_model_dir=args.output_dir)", "target": 0}, {"function": "def freeze_model(args):\n  \"\"\"Function triggered by freeze_model command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  checkpoint_path = (\n      args.checkpoint_path\n      or os.path.join(args.dir, 'variables/variables'))\n  if not args.variables_to_feed:\n    variables_to_feed = []\n  elif args.variables_to_feed.lower() == 'all':\n    variables_to_feed = None  # We will identify them after.\n  else:\n    variables_to_feed = args.variables_to_feed.split(',')\n\n  saved_model_aot_compile.freeze_model(\n      checkpoint_path=checkpoint_path,\n      meta_graph_def=saved_model_utils.get_meta_graph_def(\n          args.dir, args.tag_set),\n      signature_def_key=args.signature_def_key,\n      variables_to_feed=variables_to_feed,\n      output_prefix=args.output_prefix)", "target": 0}, {"function": "def aot_compile_cpu(args):\n  \"\"\"Function triggered by aot_compile_cpu command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  checkpoint_path = (\n      args.checkpoint_path\n      or os.path.join(args.dir, 'variables/variables'))\n  if not args.variables_to_feed:\n    variables_to_feed = []\n  elif args.variables_to_feed.lower() == 'all':\n    variables_to_feed = None  # We will identify them after.\n  else:\n    variables_to_feed = args.variables_to_feed.split(',')\n\n  saved_model_aot_compile.aot_compile_cpu_meta_graph_def(\n      checkpoint_path=checkpoint_path,\n      meta_graph_def=saved_model_utils.get_meta_graph_def(\n          args.dir, args.tag_set),\n      signature_def_key=args.signature_def_key,\n      variables_to_feed=variables_to_feed,\n      output_prefix=args.output_prefix,\n      target_triple=args.target_triple,\n      target_cpu=args.target_cpu,\n      cpp_class=args.cpp_class,\n      multithreading=args.multithreading.lower() not in ('f', 'false', '0'))", "target": 0}, {"function": "def add_show_subparser(subparsers):\n  \"\"\"Add parser for `show`.\"\"\"\n  show_msg = (\n      'Usage examples:\\n'\n      'To show all tag-sets in a SavedModel:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model\\n\\n'\n      'To show all available SignatureDef keys in a '\n      'MetaGraphDef specified by its tag-set:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve\\n\\n'\n      'For a MetaGraphDef with multiple tags in the tag-set, all tags must be '\n      'passed in, separated by \\';\\':\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve,gpu\\n\\n'\n      'To show all inputs and outputs TensorInfo for a specific'\n      ' SignatureDef specified by the SignatureDef key in a'\n      ' MetaGraph.\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve'\n      ' --signature_def serving_default\\n\\n'\n      'To show all available information in the SavedModel:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --all')\n  parser_show = subparsers.add_parser(\n      'show',\n      description=show_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_show.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to inspect')\n  parser_show.add_argument(\n      '--all',\n      action='store_true',\n      help='if set, will output all information in given SavedModel')\n  parser_show.add_argument(\n      '--tag_set',\n      type=str,\n      default=None,\n      help='tag-set of graph in SavedModel to show, separated by \\',\\'')\n  parser_show.add_argument(\n      '--signature_def',\n      type=str,\n      default=None,\n      metavar='SIGNATURE_DEF_KEY',\n      help='key of SignatureDef to display input(s) and output(s) for')\n  parser_show.set_defaults(func=show)", "target": 0}, {"function": "def add_run_subparser(subparsers):\n  \"\"\"Add parser for `run`.\"\"\"\n  run_msg = ('Usage example:\\n'\n             'To run input tensors from files through a MetaGraphDef and save'\n             ' the output tensors to files:\\n'\n             '$saved_model_cli show --dir /tmp/saved_model --tag_set serve \\\\\\n'\n             '   --signature_def serving_default \\\\\\n'\n             '   --inputs input1_key=/tmp/124.npz[x],input2_key=/tmp/123.npy '\n             '\\\\\\n'\n             '   --input_exprs \\'input3_key=np.ones(2)\\' \\\\\\n'\n             '   --input_examples '\n             '\\'input4_key=[{\"id\":[26],\"weights\":[0.5, 0.5]}]\\' \\\\\\n'\n             '   --outdir=/out\\n\\n'\n             'For more information about input file format, please see:\\n'\n             'https://www.tensorflow.org/guide/saved_model_cli\\n')\n  parser_run = subparsers.add_parser(\n      'run', description=run_msg, formatter_class=argparse.RawTextHelpFormatter)\n  parser_run.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to execute')\n  parser_run.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to load, separated by \\',\\'')\n  parser_run.add_argument(\n      '--signature_def',\n      type=str,\n      required=True,\n      metavar='SIGNATURE_DEF_KEY',\n      help='key of SignatureDef to run')\n  msg = ('Loading inputs from files, in the format of \\'<input_key>=<filename>,'\n         ' or \\'<input_key>=<filename>[<variable_name>]\\', separated by \\';\\'.'\n         ' The file format can only be from .npy, .npz or pickle.')\n  parser_run.add_argument('--inputs', type=str, default='', help=msg)\n  msg = ('Specifying inputs by python expressions, in the format of'\n         ' \"<input_key>=\\'<python expression>\\'\", separated by \\';\\'. '\n         'numpy module is available as \\'np\\'. Please note that the expression '\n         'will be evaluated as-is, and is susceptible to code injection. '\n         'When this is set, the value will override duplicate input keys from '\n         '--inputs option.')\n  parser_run.add_argument('--input_exprs', type=str, default='', help=msg)\n  msg = (\n      'Specifying tf.Example inputs as list of dictionaries. For example: '\n      '<input_key>=[{feature0:value_list,feature1:value_list}]. Use \";\" to '\n      'separate input keys. Will override duplicate input keys from --inputs '\n      'and --input_exprs option.')\n  parser_run.add_argument('--input_examples', type=str, default='', help=msg)\n  parser_run.add_argument(\n      '--outdir',\n      type=str,\n      default=None,\n      help='if specified, output tensor(s) will be saved to given directory')\n  parser_run.add_argument(\n      '--overwrite',\n      action='store_true',\n      help='if set, output file will be overwritten if it already exists.')\n  parser_run.add_argument(\n      '--tf_debug',\n      action='store_true',\n      help='if set, will use TensorFlow Debugger (tfdbg) to watch the '\n           'intermediate Tensors and runtime GraphDefs while running the '\n           'SavedModel.')\n  parser_run.add_argument(\n      '--worker',\n      type=str,\n      default=None,\n      help='if specified, a Session will be run on the worker. '\n           'Valid worker specification is a bns or gRPC path.')\n  parser_run.add_argument(\n      '--init_tpu',\n      action='store_true',\n      default=None,\n      help='if specified, tpu.initialize_system will be called on the Session. '\n           'This option should be only used if the worker is a TPU job.')\n  parser_run.add_argument(\n      '--use_tfrt',\n      action='store_true',\n      default=None,\n      help='if specified, TFRT session will be used, instead of TF1 session.')\n  parser_run.set_defaults(func=run)", "target": 0}, {"function": "def add_scan_subparser(subparsers):\n  \"\"\"Add parser for `scan`.\"\"\"\n  scan_msg = ('Usage example:\\n'\n              'To scan for denylisted ops in SavedModel:\\n'\n              '$saved_model_cli scan --dir /tmp/saved_model\\n'\n              'To scan a specific MetaGraph, pass in --tag_set\\n')\n  parser_scan = subparsers.add_parser(\n      'scan',\n      description=scan_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_scan.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to execute')\n  parser_scan.add_argument(\n      '--tag_set',\n      type=str,\n      help='tag-set of graph in SavedModel to scan, separated by \\',\\'')\n  parser_scan.set_defaults(func=scan)", "target": 0}, {"function": "def add_convert_subparser(subparsers):\n  \"\"\"Add parser for `convert`.\"\"\"\n  convert_msg = ('Usage example:\\n'\n                 'To convert the SavedModel to one that have TensorRT ops:\\n'\n                 '$saved_model_cli convert \\\\\\n'\n                 '   --dir /tmp/saved_model \\\\\\n'\n                 '   --tag_set serve \\\\\\n'\n                 '   --output_dir /tmp/saved_model_trt \\\\\\n'\n                 '   tensorrt \\n')\n  parser_convert = subparsers.add_parser(\n      'convert',\n      description=convert_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_convert.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to convert')\n  parser_convert.add_argument(\n      '--output_dir',\n      type=str,\n      required=True,\n      help='output directory for the converted SavedModel')\n  parser_convert.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to convert, separated by \\',\\'')\n  convert_subparsers = parser_convert.add_subparsers(\n      title='conversion methods',\n      description='valid conversion methods',\n      help='the conversion to run with the SavedModel')\n  parser_convert_with_tensorrt = convert_subparsers.add_parser(\n      'tensorrt',\n      description='Convert the SavedModel with Tensorflow-TensorRT integration',\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_convert_with_tensorrt.add_argument(\n      '--max_workspace_size_bytes',\n      type=int,\n      default=2 << 20,\n      help=('the maximum GPU temporary memory which the TRT engine can use at '\n            'execution time'))\n  parser_convert_with_tensorrt.add_argument(\n      '--precision_mode',\n      type=str,\n      default='FP32',\n      help='one of FP32, FP16 and INT8')\n  parser_convert_with_tensorrt.add_argument(\n      '--minimum_segment_size',\n      type=int,\n      default=3,\n      help=('the minimum number of nodes required for a subgraph to be replaced'\n            'in a TensorRT node'))\n  parser_convert_with_tensorrt.add_argument(\n      '--convert_tf1_model',\n      type=bool,\n      default=False,\n      help='support TRT conversion for TF1 models')\n  parser_convert_with_tensorrt.set_defaults(func=convert_with_tensorrt)", "target": 0}, {"function": "def _parse_common_freeze_and_aot(parser_compile):\n  \"\"\"Parse arguments shared by freeze model and aot_compile.\"\"\"\n  parser_compile.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to convert')\n  parser_compile.add_argument(\n      '--output_prefix',\n      type=str,\n      required=True,\n      help=('output directory + filename prefix for the resulting header(s) '\n            'and object file(s)'))\n  parser_compile.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to convert, separated by \\',\\'')\n  parser_compile.add_argument(\n      '--signature_def_key',\n      type=str,\n      default=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY,\n      help=('signature_def key to use.  '\n            'default: DEFAULT_SERVING_SIGNATURE_DEF_KEY'))\n  parser_compile.add_argument(\n      '--checkpoint_path',\n      type=str,\n      default=None,\n      help='Custom checkpoint to use (default: use the SavedModel variables)')\n  parser_compile.add_argument(\n      '--variables_to_feed',\n      type=str,\n      default='',\n      help=('The names of variables that will be fed into the network.  '\n            'Options are: empty (default; all variables are frozen, none may '\n            'be fed), \\'all\\' (all variables may be fed), or a '\n            'comma-delimited list of names of variables that may be fed.  In '\n            'the last case, the non-fed variables will be frozen in the graph.'\n            '**NOTE** Any variables passed to `variables_to_feed` *must be set '\n            'by the user*.  These variables will NOT be frozen and their '\n            'values will be uninitialized in the compiled object '\n            '(this applies to all input arguments from the signature as '\n            'well).'))", "target": 0}, {"function": "def add_freeze_model_subparser(subparsers):\n  \"\"\"Add parser for `freeze_model`.\"\"\"\n  compile_msg = '\\n'.join(\n      ['Usage example:',\n       'To freeze a SavedModel in preparation for tfcompile:',\n       '$saved_model_cli freeze_model \\\\',\n       '   --dir /tmp/saved_model \\\\',\n       '   --tag_set serve \\\\',\n       '   --output_prefix /tmp/saved_model_xla_aot',\n      ])\n\n  parser_compile = subparsers.add_parser(\n      'freeze_model',\n      description=compile_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  _parse_common_freeze_and_aot(parser_compile)\n  parser_compile.set_defaults(func=freeze_model)", "target": 0}, {"function": "def add_aot_compile_cpu_subparser(subparsers):\n  \"\"\"Add parser for `aot_compile_cpu`.\"\"\"\n  compile_msg = '\\n'.join(\n      ['Usage example:',\n       'To compile a SavedModel signature via (CPU) XLA AOT:',\n       '$saved_model_cli aot_compile_cpu \\\\',\n       '   --dir /tmp/saved_model \\\\',\n       '   --tag_set serve \\\\',\n       '   --output_dir /tmp/saved_model_xla_aot',\n       '', '',\n       'Note: Additional XLA compilation options are available by setting the ',\n       'XLA_FLAGS environment variable.  See the XLA debug options flags for ',\n       'all the options: ',\n       '  {}'.format(_XLA_DEBUG_OPTIONS_URL),\n       '',\n       'For example, to disable XLA fast math when compiling:',\n       '',\n       'XLA_FLAGS=\"--xla_cpu_enable_fast_math=false\" $saved_model_cli '\n       'aot_compile_cpu ...',\n       '',\n       'Some possibly useful flags:',\n       '  --xla_cpu_enable_fast_math=false',\n       '  --xla_force_host_platform_device_count=<num threads>',\n       '    (useful in conjunction with disabling multi threading)'\n      ])\n\n  parser_compile = subparsers.add_parser(\n      'aot_compile_cpu',\n      description=compile_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  _parse_common_freeze_and_aot(parser_compile)\n  parser_compile.add_argument(\n      '--target_triple',\n      type=str,\n      default='x86_64-pc-linux',\n      help=('Target triple for LLVM during AOT compilation.  Examples: '\n            'x86_64-none-darwin, x86_64-apple-ios, arm64-none-ios, '\n            'armv7-none-android.  More examples are available in tfcompile.bzl '\n            'in the tensorflow codebase.'))\n  parser_compile.add_argument(\n      '--target_cpu',\n      type=str,\n      default='',\n      help=('Target cpu name for LLVM during AOT compilation.  Examples: '\n            'x86_64, skylake, haswell, westmere, <empty> (unknown).  For '\n            'a complete list of options, run (for x86 targets): '\n            '`llc -march=x86 -mcpu=help`'))\n  parser_compile.add_argument(\n      '--cpp_class',\n      type=str,\n      required=True,\n      help=('The name of the generated C++ class, wrapping the generated '\n            'function.  The syntax of this flag is '\n            '[[<optional_namespace>::],...]<class_name>.  This mirrors the '\n            'C++ syntax for referring to a class, where multiple namespaces '\n            'may precede the class name, separated by double-colons.  '\n            'The class will be generated in the given namespace(s), or if no '\n            'namespaces are given, within the global namespace.'))\n  parser_compile.add_argument(\n      '--multithreading',\n      type=str,\n      default='False',\n      help=('Enable multithreading in the compiled computation.  '\n            'Note that if using this option, the resulting object files '\n            'may have external dependencies on multithreading libraries '\n            'like nsync.'))\n\n  parser_compile.set_defaults(func=aot_compile_cpu)", "target": 0}, {"function": "def create_parser():\n  \"\"\"Creates a parser that parse the command line arguments.\n\n  Returns:\n    A namespace parsed from command line arguments.\n  \"\"\"\n  parser = argparse.ArgumentParser(\n      description='saved_model_cli: Command-line interface for SavedModel')\n  parser.add_argument('-v', '--version', action='version', version='0.1.0')\n\n  subparsers = parser.add_subparsers(\n      title='commands', description='valid commands', help='additional help')\n\n  # show command\n  add_show_subparser(subparsers)\n\n  # run command\n  add_run_subparser(subparsers)\n\n  # scan command\n  add_scan_subparser(subparsers)\n\n  # tensorrt convert command\n  add_convert_subparser(subparsers)\n\n  # aot_compile_cpu command\n  add_aot_compile_cpu_subparser(subparsers)\n\n  # freeze_model command\n  add_freeze_model_subparser(subparsers)\n  return parser", "target": 0}, {"function": "def main():\n  logging.set_verbosity(logging.INFO)\n  parser = create_parser()\n  args = parser.parse_args()\n  if not hasattr(args, 'func'):\n    parser.error('too few arguments')\n  args.func(args)", "target": 0}], "function_after": [{"function": "def _show_tag_sets(saved_model_dir):\n  \"\"\"Prints the tag-sets stored in SavedModel directory.\n\n  Prints all the tag-sets for MetaGraphs stored in SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  tag_sets = saved_model_utils.get_saved_model_tag_sets(saved_model_dir)\n  print('The given SavedModel contains the following tag-sets:')\n  for tag_set in sorted(tag_sets):\n    print('%r' % ', '.join(sorted(tag_set)))", "target": 0}, {"function": "def _show_signature_def_map_keys(saved_model_dir, tag_set):\n  \"\"\"Prints the keys for each SignatureDef in the SignatureDef map.\n\n  Prints the list of SignatureDef keys from the SignatureDef map specified by\n  the given tag-set and SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n    tag_set: Group of tag(s) of the MetaGraphDef to get SignatureDef map from,\n        in string format, separated by ','. For tag-set contains multiple tags,\n        all tags must be passed in.\n  \"\"\"\n  signature_def_map = get_signature_def_map(saved_model_dir, tag_set)\n  print('The given SavedModel MetaGraphDef contains SignatureDefs with the '\n        'following keys:')\n  for signature_def_key in sorted(signature_def_map.keys()):\n    print('SignatureDef key: \\\"%s\\\"' % signature_def_key)", "target": 0}, {"function": "def _get_inputs_tensor_info_from_meta_graph_def(meta_graph_def,\n                                                signature_def_key):\n  \"\"\"Gets TensorInfo for all inputs of the SignatureDef.\n\n  Returns a dictionary that maps each input key to its TensorInfo for the given\n  signature_def_key in the meta_graph_def\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer with the SignatureDef map to\n        look up SignatureDef key.\n    signature_def_key: A SignatureDef key string.\n\n  Returns:\n    A dictionary that maps input tensor keys to TensorInfos.\n\n  Raises:\n    ValueError if `signature_def_key` is not found in the MetaGraphDef.\n  \"\"\"\n  if signature_def_key not in meta_graph_def.signature_def:\n    raise ValueError(\n        f'Could not find signature \"{signature_def_key}\". Please choose from: '\n        f'{\", \".join(meta_graph_def.signature_def.keys())}')\n  return meta_graph_def.signature_def[signature_def_key].inputs", "target": 0}, {"function": "def _get_outputs_tensor_info_from_meta_graph_def(meta_graph_def,\n                                                 signature_def_key):\n  \"\"\"Gets TensorInfos for all outputs of the SignatureDef.\n\n  Returns a dictionary that maps each output key to its TensorInfo for the given\n  signature_def_key in the meta_graph_def.\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer with the SignatureDefmap to\n    look up signature_def_key.\n    signature_def_key: A SignatureDef key string.\n\n  Returns:\n    A dictionary that maps output tensor keys to TensorInfos.\n  \"\"\"\n  return meta_graph_def.signature_def[signature_def_key].outputs", "target": 0}, {"function": "def _show_inputs_outputs(saved_model_dir, tag_set, signature_def_key, indent=0):\n  \"\"\"Prints input and output TensorInfos.\n\n  Prints the details of input and output TensorInfos for the SignatureDef mapped\n  by the given signature_def_key.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n    tag_set: Group of tag(s) of the MetaGraphDef, in string format, separated by\n        ','. For tag-set contains multiple tags, all tags must be passed in.\n    signature_def_key: A SignatureDef key string.\n    indent: How far (in increments of 2 spaces) to indent each line of output.\n  \"\"\"\n  meta_graph_def = saved_model_utils.get_meta_graph_def(saved_model_dir,\n                                                        tag_set)\n  inputs_tensor_info = _get_inputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n  outputs_tensor_info = _get_outputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n\n  indent_str = '  ' * indent\n  def in_print(s):\n    print(indent_str + s)\n\n  in_print('The given SavedModel SignatureDef contains the following input(s):')\n  for input_key, input_tensor in sorted(inputs_tensor_info.items()):\n    in_print('  inputs[\\'%s\\'] tensor_info:' % input_key)\n    _print_tensor_info(input_tensor, indent+1)\n\n  in_print('The given SavedModel SignatureDef contains the following '\n           'output(s):')\n  for output_key, output_tensor in sorted(outputs_tensor_info.items()):\n    in_print('  outputs[\\'%s\\'] tensor_info:' % output_key)\n    _print_tensor_info(output_tensor, indent+1)\n\n  in_print('Method name is: %s' %\n           meta_graph_def.signature_def[signature_def_key].method_name)", "target": 0}, {"function": "def _show_defined_functions(saved_model_dir):\n  \"\"\"Prints the callable concrete and polymorphic functions of the Saved Model.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  meta_graphs = saved_model_utils.read_saved_model(saved_model_dir).meta_graphs\n  has_object_graph_def = False\n\n  for meta_graph_def in meta_graphs:\n    has_object_graph_def |= meta_graph_def.HasField('object_graph_def')\n  if not has_object_graph_def:\n    return\n  with ops_lib.Graph().as_default():\n    trackable_object = load.load(saved_model_dir)\n\n  print('\\nConcrete Functions:', end='')\n  children = list(\n      save._AugmentedGraphView(trackable_object)  # pylint: disable=protected-access\n      .list_children(trackable_object))\n  children = sorted(children, key=lambda x: x.name)\n  for name, child in children:\n    concrete_functions = []\n    if isinstance(child, defun.ConcreteFunction):\n      concrete_functions.append(child)\n    elif isinstance(child, def_function.Function):\n      concrete_functions.extend(\n          child._list_all_concrete_functions_for_serialization())  # pylint: disable=protected-access\n    else:\n      continue\n    print('\\n  Function Name: \\'%s\\'' % name)\n    concrete_functions = sorted(concrete_functions, key=lambda x: x.name)\n    for index, concrete_function in enumerate(concrete_functions, 1):\n      args, kwargs = None, None\n      if concrete_function.structured_input_signature:\n        args, kwargs = concrete_function.structured_input_signature\n      elif concrete_function._arg_keywords:  # pylint: disable=protected-access\n        # For pure ConcreteFunctions we might have nothing better than\n        # _arg_keywords.\n        args = concrete_function._arg_keywords  # pylint: disable=protected-access\n      if args:\n        print('    Option #%d' % index)\n        print('      Callable with:')\n        _print_args(args, indent=4)\n      if kwargs:\n        _print_args(kwargs, 'Named Argument', indent=4)", "target": 0}, {"function": "def _print_args(arguments, argument_type='Argument', indent=0):\n  \"\"\"Formats and prints the argument of the concrete functions defined in the model.\n\n  Args:\n    arguments: Arguments to format print.\n    argument_type: Type of arguments.\n    indent: How far (in increments of 2 spaces) to indent each line of\n     output.\n  \"\"\"\n  indent_str = '  ' * indent\n\n  def _maybe_add_quotes(value):\n    is_quotes = '\\'' * isinstance(value, str)\n    return is_quotes + str(value) + is_quotes\n\n  def in_print(s, end='\\n'):\n    print(indent_str + s, end=end)\n\n  for index, element in enumerate(arguments, 1):\n    if indent == 4:\n      in_print('%s #%d' % (argument_type, index))\n    if isinstance(element, six.string_types):\n      in_print('  %s' % element)\n    elif isinstance(element, tensor_spec.TensorSpec):\n      print((indent + 1) * '  ' + '%s: %s' % (element.name, repr(element)))\n    elif (isinstance(element, collections_abc.Iterable) and\n          not isinstance(element, dict)):\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: [', end='')\n      for value in element:\n        print('%s' % _maybe_add_quotes(value), end=', ')\n      print('\\b\\b]')\n    elif isinstance(element, dict):\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: {', end='')\n      for (key, value) in element.items():\n        print('\\'%s\\': %s' % (str(key), _maybe_add_quotes(value)), end=', ')\n      print('\\b\\b}')\n    else:\n      in_print('  DType: %s' % type(element).__name__)\n      in_print('  Value: %s' % str(element))", "target": 0}, {"function": "def _print_tensor_info(tensor_info, indent=0):\n  \"\"\"Prints details of the given tensor_info.\n\n  Args:\n    tensor_info: TensorInfo object to be printed.\n    indent: How far (in increments of 2 spaces) to indent each line output\n  \"\"\"\n  indent_str = '  ' * indent\n  def in_print(s):\n    print(indent_str + s)\n\n  in_print('    dtype: ' +\n           {value: key\n            for (key, value) in types_pb2.DataType.items()}[tensor_info.dtype])\n  # Display shape as tuple.\n  if tensor_info.tensor_shape.unknown_rank:\n    shape = 'unknown_rank'\n  else:\n    dims = [str(dim.size) for dim in tensor_info.tensor_shape.dim]\n    shape = ', '.join(dims)\n    shape = '(' + shape + ')'\n  in_print('    shape: ' + shape)\n  in_print('    name: ' + tensor_info.name)", "target": 0}, {"function": "def _show_all(saved_model_dir):\n  \"\"\"Prints tag-set, SignatureDef and Inputs/Outputs information in SavedModel.\n\n  Prints all tag-set, SignatureDef and Inputs/Outputs information stored in\n  SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect.\n  \"\"\"\n  tag_sets = saved_model_utils.get_saved_model_tag_sets(saved_model_dir)\n  for tag_set in sorted(tag_sets):\n    print(\"\\nMetaGraphDef with tag-set: '%s' \"\n          \"contains the following SignatureDefs:\" % ', '.join(tag_set))\n\n    tag_set = ','.join(tag_set)\n    signature_def_map = get_signature_def_map(saved_model_dir, tag_set)\n    for signature_def_key in sorted(signature_def_map.keys()):\n      print('\\nsignature_def[\\'' + signature_def_key + '\\']:')\n      _show_inputs_outputs(saved_model_dir, tag_set, signature_def_key,\n                           indent=1)\n  _show_defined_functions(saved_model_dir)", "target": 0}, {"function": "def get_meta_graph_def(saved_model_dir, tag_set):\n  \"\"\"DEPRECATED: Use saved_model_utils.get_meta_graph_def instead.\n\n  Gets MetaGraphDef from SavedModel. Returns the MetaGraphDef for the given\n  tag-set and SavedModel directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect or execute.\n    tag_set: Group of tag(s) of the MetaGraphDef to load, in string format,\n        separated by ','. For tag-set contains multiple tags, all tags must be\n        passed in.\n\n  Raises:\n    RuntimeError: An error when the given tag-set does not exist in the\n        SavedModel.\n\n  Returns:\n    A MetaGraphDef corresponding to the tag-set.\n  \"\"\"\n  return saved_model_utils.get_meta_graph_def(saved_model_dir, tag_set)", "target": 0}, {"function": "def get_signature_def_map(saved_model_dir, tag_set):\n  \"\"\"Gets SignatureDef map from a MetaGraphDef in a SavedModel.\n\n  Returns the SignatureDef map for the given tag-set in the SavedModel\n  directory.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to inspect or execute.\n    tag_set: Group of tag(s) of the MetaGraphDef with the SignatureDef map, in\n        string format, separated by ','. For tag-set contains multiple tags, all\n        tags must be passed in.\n\n  Returns:\n    A SignatureDef map that maps from string keys to SignatureDefs.\n  \"\"\"\n  meta_graph = saved_model_utils.get_meta_graph_def(saved_model_dir, tag_set)\n  return meta_graph.signature_def", "target": 0}, {"function": "def scan_meta_graph_def(meta_graph_def):\n  \"\"\"Scans meta_graph_def and reports if there are ops on denylist.\n\n  Print ops if they are on black list, or print success if no denylisted ops\n  found.\n\n  Args:\n    meta_graph_def: MetaGraphDef protocol buffer.\n  \"\"\"\n  all_ops_set = set(\n      meta_graph_lib.ops_used_by_graph_def(meta_graph_def.graph_def))\n  denylisted_ops = _OP_DENYLIST & all_ops_set\n  if denylisted_ops:\n    # TODO(yifeif): print more warnings\n    print(\n        'MetaGraph with tag set %s contains the following denylisted ops:' %\n        meta_graph_def.meta_info_def.tags, denylisted_ops)\n  else:\n    print('MetaGraph with tag set %s does not contain denylisted ops.' %\n          meta_graph_def.meta_info_def.tags)", "target": 0}, {"function": "def run_saved_model_with_feed_dict(saved_model_dir,\n                                   tag_set,\n                                   signature_def_key,\n                                   input_tensor_key_feed_dict,\n                                   outdir,\n                                   overwrite_flag,\n                                   worker=None,\n                                   init_tpu=False,\n                                   use_tfrt=False,\n                                   tf_debug=False):\n  \"\"\"Runs SavedModel and fetch all outputs.\n\n  Runs the input dictionary through the MetaGraphDef within a SavedModel\n  specified by the given tag_set and SignatureDef. Also save the outputs to file\n  if outdir is not None.\n\n  Args:\n    saved_model_dir: Directory containing the SavedModel to execute.\n    tag_set: Group of tag(s) of the MetaGraphDef with the SignatureDef map, in\n        string format, separated by ','. For tag-set contains multiple tags, all\n        tags must be passed in.\n    signature_def_key: A SignatureDef key string.\n    input_tensor_key_feed_dict: A dictionary maps input keys to numpy ndarrays.\n    outdir: A directory to save the outputs to. If the directory doesn't exist,\n        it will be created.\n    overwrite_flag: A boolean flag to allow overwrite output file if file with\n        the same name exists.\n    worker: If provided, the session will be run on the worker.  Valid worker\n        specification is a bns or gRPC path.\n    init_tpu: If true, the TPU system will be initialized after the session\n        is created.\n    use_tfrt: If true, TFRT session will be used.\n    tf_debug: A boolean flag to use TensorFlow Debugger (TFDBG) to observe the\n        intermediate Tensor values and runtime GraphDefs while running the\n        SavedModel.\n\n  Raises:\n    ValueError: When any of the input tensor keys is not valid.\n    RuntimeError: An error when output file already exists and overwrite is not\n    enabled.\n  \"\"\"\n  # Get a list of output tensor names.\n  meta_graph_def = saved_model_utils.get_meta_graph_def(saved_model_dir,\n                                                        tag_set)\n\n  # Re-create feed_dict based on input tensor name instead of key as session.run\n  # uses tensor name.\n  inputs_tensor_info = _get_inputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n\n  # Check if input tensor keys are valid.\n  for input_key_name in input_tensor_key_feed_dict.keys():\n    if input_key_name not in inputs_tensor_info:\n      raise ValueError(\n          '\"%s\" is not a valid input key. Please choose from %s, or use '\n          '--show option.' %\n          (input_key_name, '\"' + '\", \"'.join(inputs_tensor_info.keys()) + '\"'))\n\n  inputs_feed_dict = {\n      inputs_tensor_info[key].name: tensor\n      for key, tensor in input_tensor_key_feed_dict.items()\n  }\n  # Get outputs\n  outputs_tensor_info = _get_outputs_tensor_info_from_meta_graph_def(\n      meta_graph_def, signature_def_key)\n  # Sort to preserve order because we need to go from value to key later.\n  output_tensor_keys_sorted = sorted(outputs_tensor_info.keys())\n  output_tensor_names_sorted = [\n      outputs_tensor_info[tensor_key].name\n      for tensor_key in output_tensor_keys_sorted\n  ]\n\n  config = None\n  if use_tfrt:\n    logging.info('Using TFRT session.')\n    config = config_pb2.ConfigProto(\n        experimental=config_pb2.ConfigProto.Experimental(use_tfrt=True))\n  with session.Session(worker, graph=ops_lib.Graph(), config=config) as sess:\n    if init_tpu:\n      print('Initializing TPU System ...')\n      # This is needed for freshly started worker, or if the job\n      # restarts after a preemption.\n      sess.run(tpu.initialize_system())\n\n    loader.load(sess, tag_set.split(','), saved_model_dir)\n\n    if tf_debug:\n      sess = local_cli_wrapper.LocalCLIDebugWrapperSession(sess)\n\n    outputs = sess.run(output_tensor_names_sorted, feed_dict=inputs_feed_dict)\n\n    for i, output in enumerate(outputs):\n      output_tensor_key = output_tensor_keys_sorted[i]\n      print('Result for output key %s:\\n%s' % (output_tensor_key, output))\n\n      # Only save if outdir is specified.\n      if outdir:\n        # Create directory if outdir does not exist\n        if not os.path.isdir(outdir):\n          os.makedirs(outdir)\n        output_full_path = os.path.join(outdir, output_tensor_key + '.npy')\n\n        # If overwrite not enabled and file already exist, error out\n        if not overwrite_flag and os.path.exists(output_full_path):\n          raise RuntimeError(\n              'Output file %s already exists. Add \\\"--overwrite\\\" to overwrite'\n              ' the existing output files.' % output_full_path)\n\n        np.save(output_full_path, output)\n        print('Output %s is saved to %s' % (output_tensor_key,\n                                            output_full_path))", "target": 0}, {"function": "def preprocess_inputs_arg_string(inputs_str):\n  \"\"\"Parses input arg into dictionary that maps input to file/variable tuple.\n\n  Parses input string in the format of, for example,\n  \"input1=filename1[variable_name1],input2=filename2\" into a\n  dictionary looks like\n  {'input_key1': (filename1, variable_name1),\n   'input_key2': (file2, None)}\n  , which maps input keys to a tuple of file name and variable name(None if\n  empty).\n\n  Args:\n    inputs_str: A string that specified where to load inputs. Inputs are\n    separated by semicolons.\n        * For each input key:\n            '<input_key>=<filename>' or\n            '<input_key>=<filename>[<variable_name>]'\n        * The optional 'variable_name' key will be set to None if not specified.\n\n  Returns:\n    A dictionary that maps input keys to a tuple of file name and variable name.\n\n  Raises:\n    RuntimeError: An error when the given input string is in a bad format.\n  \"\"\"\n  input_dict = {}\n  inputs_raw = inputs_str.split(';')\n  for input_raw in filter(bool, inputs_raw):  # skip empty strings\n    # Format of input=filename[variable_name]'\n    match = re.match(r'([^=]+)=([^\\[\\]]+)\\[([^\\[\\]]+)\\]$', input_raw)\n\n    if match:\n      input_dict[match.group(1)] = match.group(2), match.group(3)\n    else:\n      # Format of input=filename'\n      match = re.match(r'([^=]+)=([^\\[\\]]+)$', input_raw)\n      if match:\n        input_dict[match.group(1)] = match.group(2), None\n      else:\n        raise RuntimeError(\n            '--inputs \"%s\" format is incorrect. Please follow'\n            '\"<input_key>=<filename>\", or'\n            '\"<input_key>=<filename>[<variable_name>]\"' % input_raw)\n\n  return input_dict", "target": 0}, {"function": "def preprocess_input_exprs_arg_string(input_exprs_str, safe=True):\n  \"\"\"Parses input arg into dictionary that maps input key to python expression.\n\n  Parses input string in the format of 'input_key=<python expression>' into a\n  dictionary that maps each input_key to its python expression.\n\n  Args:\n    input_exprs_str: A string that specifies python expression for input keys.\n      Each input is separated by semicolon. For each input key:\n        'input_key=<python expression>'\n    safe: Whether to evaluate the python expression as literals or allow\n      arbitrary calls (e.g. numpy usage).\n\n  Returns:\n    A dictionary that maps input keys to their values.\n\n  Raises:\n    RuntimeError: An error when the given input string is in a bad format.\n  \"\"\"\n  input_dict = {}\n\n  for input_raw in filter(bool, input_exprs_str.split(';')):\n    if '=' not in input_exprs_str:\n      raise RuntimeError('--input_exprs \"%s\" format is incorrect. Please follow'\n                         '\"<input_key>=<python expression>\"' % input_exprs_str)\n    input_key, expr = input_raw.split('=', 1)\n    if safe:\n      try:\n        input_dict[input_key] = ast.literal_eval(expr)\n      except:\n        raise RuntimeError(\n            f'Expression \"{expr}\" is not a valid python literal.')\n    else:\n      # ast.literal_eval does not work with numpy expressions\n      input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\n  return input_dict", "target": 0}, {"function": "def preprocess_input_examples_arg_string(input_examples_str):\n  \"\"\"Parses input into dict that maps input keys to lists of tf.Example.\n\n  Parses input string in the format of 'input_key1=[{feature_name:\n  feature_list}];input_key2=[{feature_name:feature_list}];' into a dictionary\n  that maps each input_key to its list of serialized tf.Example.\n\n  Args:\n    input_examples_str: A string that specifies a list of dictionaries of\n    feature_names and their feature_lists for each input.\n    Each input is separated by semicolon. For each input key:\n      'input=[{feature_name1: feature_list1, feature_name2:feature_list2}]'\n      items in feature_list can be the type of float, int, long or str.\n\n  Returns:\n    A dictionary that maps input keys to lists of serialized tf.Example.\n\n  Raises:\n    ValueError: An error when the given tf.Example is not a list.\n  \"\"\"\n  input_dict = preprocess_input_exprs_arg_string(input_examples_str)\n  for input_key, example_list in input_dict.items():\n    if not isinstance(example_list, list):\n      raise ValueError(\n          'tf.Example input must be a list of dictionaries, but \"%s\" is %s' %\n          (example_list, type(example_list)))\n    input_dict[input_key] = [\n        _create_example_string(example) for example in example_list\n    ]\n  return input_dict", "target": 0}, {"function": "def _create_example_string(example_dict):\n  \"\"\"Create a serialized tf.example from feature dictionary.\"\"\"\n  example = example_pb2.Example()\n  for feature_name, feature_list in example_dict.items():\n    if not isinstance(feature_list, list):\n      raise ValueError('feature value must be a list, but %s: \"%s\" is %s' %\n                       (feature_name, feature_list, type(feature_list)))\n    if isinstance(feature_list[0], float):\n      example.features.feature[feature_name].float_list.value.extend(\n          feature_list)\n    elif isinstance(feature_list[0], str):\n      example.features.feature[feature_name].bytes_list.value.extend(\n          [f.encode('utf8') for f in feature_list])\n    elif isinstance(feature_list[0], bytes):\n      example.features.feature[feature_name].bytes_list.value.extend(\n          feature_list)\n    elif isinstance(feature_list[0], six.integer_types):\n      example.features.feature[feature_name].int64_list.value.extend(\n          feature_list)\n    else:\n      raise ValueError(\n          'Type %s for value %s is not supported for tf.train.Feature.' %\n          (type(feature_list[0]), feature_list[0]))\n  return example.SerializeToString()", "target": 0}, {"function": "def load_inputs_from_input_arg_string(inputs_str, input_exprs_str,\n                                      input_examples_str):\n  \"\"\"Parses input arg strings and create inputs feed_dict.\n\n  Parses '--inputs' string for inputs to be loaded from file, and parses\n  '--input_exprs' string for inputs to be evaluated from python expression.\n  '--input_examples' string for inputs to be created from tf.example feature\n  dictionary list.\n\n  Args:\n    inputs_str: A string that specified where to load inputs. Each input is\n        separated by semicolon.\n        * For each input key:\n            '<input_key>=<filename>' or\n            '<input_key>=<filename>[<variable_name>]'\n        * The optional 'variable_name' key will be set to None if not specified.\n        * File specified by 'filename' will be loaded using numpy.load. Inputs\n            can be loaded from only .npy, .npz or pickle files.\n        * The \"[variable_name]\" key is optional depending on the input file type\n            as descripted in more details below.\n        When loading from a npy file, which always contains a numpy ndarray, the\n        content will be directly assigned to the specified input tensor. If a\n        variable_name is specified, it will be ignored and a warning will be\n        issued.\n        When loading from a npz zip file, user can specify which variable within\n        the zip file to load for the input tensor inside the square brackets. If\n        nothing is specified, this function will check that only one file is\n        included in the zip and load it for the specified input tensor.\n        When loading from a pickle file, if no variable_name is specified in the\n        square brackets, whatever that is inside the pickle file will be passed\n        to the specified input tensor, else SavedModel CLI will assume a\n        dictionary is stored in the pickle file and the value corresponding to\n        the variable_name will be used.\n    input_exprs_str: A string that specifies python expressions for inputs.\n        * In the format of: '<input_key>=<python expression>'.\n        * numpy module is available as np.\n    input_examples_str: A string that specifies tf.Example with dictionary.\n        * In the format of: '<input_key>=<[{feature:value list}]>'\n\n  Returns:\n    A dictionary that maps input tensor keys to numpy ndarrays.\n\n  Raises:\n    RuntimeError: An error when a key is specified, but the input file contains\n        multiple numpy ndarrays, none of which matches the given key.\n    RuntimeError: An error when no key is specified, but the input file contains\n        more than one numpy ndarrays.\n  \"\"\"\n  tensor_key_feed_dict = {}\n\n  inputs = preprocess_inputs_arg_string(inputs_str)\n  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str)\n  input_examples = preprocess_input_examples_arg_string(input_examples_str)\n\n  for input_tensor_key, (filename, variable_name) in inputs.items():\n    data = np.load(file_io.FileIO(filename, mode='rb'), allow_pickle=True)  # pylint: disable=unexpected-keyword-arg\n\n    # When a variable_name key is specified for the input file\n    if variable_name:\n      # if file contains a single ndarray, ignore the input name\n      if isinstance(data, np.ndarray):\n        logging.warn(\n            'Input file %s contains a single ndarray. Name key \\\"%s\\\" ignored.'\n            % (filename, variable_name))\n        tensor_key_feed_dict[input_tensor_key] = data\n      else:\n        if variable_name in data:\n          tensor_key_feed_dict[input_tensor_key] = data[variable_name]\n        else:\n          raise RuntimeError(\n              'Input file %s does not contain variable with name \\\"%s\\\".' %\n              (filename, variable_name))\n    # When no key is specified for the input file.\n    else:\n      # Check if npz file only contains a single numpy ndarray.\n      if isinstance(data, np.lib.npyio.NpzFile):\n        variable_name_list = data.files\n        if len(variable_name_list) != 1:\n          raise RuntimeError(\n              'Input file %s contains more than one ndarrays. Please specify '\n              'the name of ndarray to use.' % filename)\n        tensor_key_feed_dict[input_tensor_key] = data[variable_name_list[0]]\n      else:\n        tensor_key_feed_dict[input_tensor_key] = data\n\n  # When input is a python expression:\n  for input_tensor_key, py_expr_evaluated in input_exprs.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified with both --inputs and --input_exprs'\n          ' options. Value in --input_exprs will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = py_expr_evaluated\n\n  # When input is a tf.Example:\n  for input_tensor_key, example in input_examples.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified in multiple options. Value in '\n          '--input_examples will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = example\n  return tensor_key_feed_dict", "target": 0}, {"function": "def show(args):\n  \"\"\"Function triggered by show command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  # If all tag is specified, display all information.\n  if args.all:\n    _show_all(args.dir)\n  else:\n    # If no tag is specified, display all tag_set, if no signature_def key is\n    # specified, display all SignatureDef keys, else show input output tensor\n    # information corresponding to the given SignatureDef key\n    if args.tag_set is None:\n      _show_tag_sets(args.dir)\n    else:\n      if args.signature_def is None:\n        _show_signature_def_map_keys(args.dir, args.tag_set)\n      else:\n        _show_inputs_outputs(args.dir, args.tag_set, args.signature_def)", "target": 0}, {"function": "def run(args):\n  \"\"\"Function triggered by run command.\n\n  Args:\n    args: A namespace parsed from command line.\n\n  Raises:\n    AttributeError: An error when neither --inputs nor --input_exprs is passed\n    to run command.\n  \"\"\"\n  if not args.inputs and not args.input_exprs and not args.input_examples:\n    raise AttributeError(\n        'At least one of --inputs, --input_exprs or --input_examples must be '\n        'required')\n  tensor_key_feed_dict = load_inputs_from_input_arg_string(\n      args.inputs, args.input_exprs, args.input_examples)\n  run_saved_model_with_feed_dict(\n      args.dir,\n      args.tag_set,\n      args.signature_def,\n      tensor_key_feed_dict,\n      args.outdir,\n      args.overwrite,\n      worker=args.worker,\n      init_tpu=args.init_tpu,\n      use_tfrt=args.use_tfrt,\n      tf_debug=args.tf_debug)", "target": 0}, {"function": "def scan(args):\n  \"\"\"Function triggered by scan command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  if args.tag_set:\n    scan_meta_graph_def(\n        saved_model_utils.get_meta_graph_def(args.dir, args.tag_set))\n  else:\n    saved_model = saved_model_utils.read_saved_model(args.dir)\n    for meta_graph_def in saved_model.meta_graphs:\n      scan_meta_graph_def(meta_graph_def)", "target": 0}, {"function": "def convert_with_tensorrt(args):\n  \"\"\"Function triggered by 'convert tensorrt' command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  # Import here instead of at top, because this will crash if TensorRT is\n  # not installed\n  from tensorflow.python.compiler.tensorrt import trt_convert as trt  # pylint: disable=g-import-not-at-top\n\n  if not args.convert_tf1_model:\n    params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n        max_workspace_size_bytes=args.max_workspace_size_bytes,\n        precision_mode=args.precision_mode,\n        minimum_segment_size=args.minimum_segment_size)\n    converter = trt.TrtGraphConverterV2(\n        input_saved_model_dir=args.dir,\n        input_saved_model_tags=args.tag_set.split(','),\n        **params._asdict())\n    try:\n      converter.convert()\n    except Exception as e:\n      raise RuntimeError(\n          '{}. Try passing \"--convert_tf1_model=True\".'.format(e))\n    converter.save(output_saved_model_dir=args.output_dir)\n  else:\n    trt.create_inference_graph(\n        None,\n        None,\n        max_batch_size=1,\n        max_workspace_size_bytes=args.max_workspace_size_bytes,\n        precision_mode=args.precision_mode,\n        minimum_segment_size=args.minimum_segment_size,\n        is_dynamic_op=True,\n        input_saved_model_dir=args.dir,\n        input_saved_model_tags=args.tag_set.split(','),\n        output_saved_model_dir=args.output_dir)", "target": 0}, {"function": "def freeze_model(args):\n  \"\"\"Function triggered by freeze_model command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  checkpoint_path = (\n      args.checkpoint_path\n      or os.path.join(args.dir, 'variables/variables'))\n  if not args.variables_to_feed:\n    variables_to_feed = []\n  elif args.variables_to_feed.lower() == 'all':\n    variables_to_feed = None  # We will identify them after.\n  else:\n    variables_to_feed = args.variables_to_feed.split(',')\n\n  saved_model_aot_compile.freeze_model(\n      checkpoint_path=checkpoint_path,\n      meta_graph_def=saved_model_utils.get_meta_graph_def(\n          args.dir, args.tag_set),\n      signature_def_key=args.signature_def_key,\n      variables_to_feed=variables_to_feed,\n      output_prefix=args.output_prefix)", "target": 0}, {"function": "def aot_compile_cpu(args):\n  \"\"\"Function triggered by aot_compile_cpu command.\n\n  Args:\n    args: A namespace parsed from command line.\n  \"\"\"\n  checkpoint_path = (\n      args.checkpoint_path\n      or os.path.join(args.dir, 'variables/variables'))\n  if not args.variables_to_feed:\n    variables_to_feed = []\n  elif args.variables_to_feed.lower() == 'all':\n    variables_to_feed = None  # We will identify them after.\n  else:\n    variables_to_feed = args.variables_to_feed.split(',')\n\n  saved_model_aot_compile.aot_compile_cpu_meta_graph_def(\n      checkpoint_path=checkpoint_path,\n      meta_graph_def=saved_model_utils.get_meta_graph_def(\n          args.dir, args.tag_set),\n      signature_def_key=args.signature_def_key,\n      variables_to_feed=variables_to_feed,\n      output_prefix=args.output_prefix,\n      target_triple=args.target_triple,\n      target_cpu=args.target_cpu,\n      cpp_class=args.cpp_class,\n      multithreading=args.multithreading.lower() not in ('f', 'false', '0'))", "target": 0}, {"function": "def add_show_subparser(subparsers):\n  \"\"\"Add parser for `show`.\"\"\"\n  show_msg = (\n      'Usage examples:\\n'\n      'To show all tag-sets in a SavedModel:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model\\n\\n'\n      'To show all available SignatureDef keys in a '\n      'MetaGraphDef specified by its tag-set:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve\\n\\n'\n      'For a MetaGraphDef with multiple tags in the tag-set, all tags must be '\n      'passed in, separated by \\';\\':\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve,gpu\\n\\n'\n      'To show all inputs and outputs TensorInfo for a specific'\n      ' SignatureDef specified by the SignatureDef key in a'\n      ' MetaGraph.\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --tag_set serve'\n      ' --signature_def serving_default\\n\\n'\n      'To show all available information in the SavedModel:\\n'\n      '$saved_model_cli show --dir /tmp/saved_model --all')\n  parser_show = subparsers.add_parser(\n      'show',\n      description=show_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_show.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to inspect')\n  parser_show.add_argument(\n      '--all',\n      action='store_true',\n      help='if set, will output all information in given SavedModel')\n  parser_show.add_argument(\n      '--tag_set',\n      type=str,\n      default=None,\n      help='tag-set of graph in SavedModel to show, separated by \\',\\'')\n  parser_show.add_argument(\n      '--signature_def',\n      type=str,\n      default=None,\n      metavar='SIGNATURE_DEF_KEY',\n      help='key of SignatureDef to display input(s) and output(s) for')\n  parser_show.set_defaults(func=show)", "target": 0}, {"function": "def add_run_subparser(subparsers):\n  \"\"\"Add parser for `run`.\"\"\"\n  run_msg = ('Usage example:\\n'\n             'To run input tensors from files through a MetaGraphDef and save'\n             ' the output tensors to files:\\n'\n             '$saved_model_cli show --dir /tmp/saved_model --tag_set serve \\\\\\n'\n             '   --signature_def serving_default \\\\\\n'\n             '   --inputs input1_key=/tmp/124.npz[x],input2_key=/tmp/123.npy '\n             '\\\\\\n'\n             '   --input_exprs \\'input3_key=np.ones(2)\\' \\\\\\n'\n             '   --input_examples '\n             '\\'input4_key=[{\"id\":[26],\"weights\":[0.5, 0.5]}]\\' \\\\\\n'\n             '   --outdir=/out\\n\\n'\n             'For more information about input file format, please see:\\n'\n             'https://www.tensorflow.org/guide/saved_model_cli\\n')\n  parser_run = subparsers.add_parser(\n      'run', description=run_msg, formatter_class=argparse.RawTextHelpFormatter)\n  parser_run.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to execute')\n  parser_run.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to load, separated by \\',\\'')\n  parser_run.add_argument(\n      '--signature_def',\n      type=str,\n      required=True,\n      metavar='SIGNATURE_DEF_KEY',\n      help='key of SignatureDef to run')\n  msg = ('Loading inputs from files, in the format of \\'<input_key>=<filename>,'\n         ' or \\'<input_key>=<filename>[<variable_name>]\\', separated by \\';\\'.'\n         ' The file format can only be from .npy, .npz or pickle.')\n  parser_run.add_argument('--inputs', type=str, default='', help=msg)\n  msg = ('Specifying inputs by python expressions, in the format of'\n         ' \"<input_key>=\\'<python expression>\\'\", separated by \\';\\'. '\n         'numpy module is available as \\'np\\'. Please note that the expression '\n         'will be evaluated as-is, and is susceptible to code injection. '\n         'When this is set, the value will override duplicate input keys from '\n         '--inputs option.')\n  parser_run.add_argument('--input_exprs', type=str, default='', help=msg)\n  msg = (\n      'Specifying tf.Example inputs as list of dictionaries. For example: '\n      '<input_key>=[{feature0:value_list,feature1:value_list}]. Use \";\" to '\n      'separate input keys. Will override duplicate input keys from --inputs '\n      'and --input_exprs option.')\n  parser_run.add_argument('--input_examples', type=str, default='', help=msg)\n  parser_run.add_argument(\n      '--outdir',\n      type=str,\n      default=None,\n      help='if specified, output tensor(s) will be saved to given directory')\n  parser_run.add_argument(\n      '--overwrite',\n      action='store_true',\n      help='if set, output file will be overwritten if it already exists.')\n  parser_run.add_argument(\n      '--tf_debug',\n      action='store_true',\n      help='if set, will use TensorFlow Debugger (tfdbg) to watch the '\n           'intermediate Tensors and runtime GraphDefs while running the '\n           'SavedModel.')\n  parser_run.add_argument(\n      '--worker',\n      type=str,\n      default=None,\n      help='if specified, a Session will be run on the worker. '\n           'Valid worker specification is a bns or gRPC path.')\n  parser_run.add_argument(\n      '--init_tpu',\n      action='store_true',\n      default=None,\n      help='if specified, tpu.initialize_system will be called on the Session. '\n           'This option should be only used if the worker is a TPU job.')\n  parser_run.add_argument(\n      '--use_tfrt',\n      action='store_true',\n      default=None,\n      help='if specified, TFRT session will be used, instead of TF1 session.')\n  parser_run.set_defaults(func=run)", "target": 0}, {"function": "def add_scan_subparser(subparsers):\n  \"\"\"Add parser for `scan`.\"\"\"\n  scan_msg = ('Usage example:\\n'\n              'To scan for denylisted ops in SavedModel:\\n'\n              '$saved_model_cli scan --dir /tmp/saved_model\\n'\n              'To scan a specific MetaGraph, pass in --tag_set\\n')\n  parser_scan = subparsers.add_parser(\n      'scan',\n      description=scan_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_scan.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to execute')\n  parser_scan.add_argument(\n      '--tag_set',\n      type=str,\n      help='tag-set of graph in SavedModel to scan, separated by \\',\\'')\n  parser_scan.set_defaults(func=scan)", "target": 0}, {"function": "def add_convert_subparser(subparsers):\n  \"\"\"Add parser for `convert`.\"\"\"\n  convert_msg = ('Usage example:\\n'\n                 'To convert the SavedModel to one that have TensorRT ops:\\n'\n                 '$saved_model_cli convert \\\\\\n'\n                 '   --dir /tmp/saved_model \\\\\\n'\n                 '   --tag_set serve \\\\\\n'\n                 '   --output_dir /tmp/saved_model_trt \\\\\\n'\n                 '   tensorrt \\n')\n  parser_convert = subparsers.add_parser(\n      'convert',\n      description=convert_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_convert.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to convert')\n  parser_convert.add_argument(\n      '--output_dir',\n      type=str,\n      required=True,\n      help='output directory for the converted SavedModel')\n  parser_convert.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to convert, separated by \\',\\'')\n  convert_subparsers = parser_convert.add_subparsers(\n      title='conversion methods',\n      description='valid conversion methods',\n      help='the conversion to run with the SavedModel')\n  parser_convert_with_tensorrt = convert_subparsers.add_parser(\n      'tensorrt',\n      description='Convert the SavedModel with Tensorflow-TensorRT integration',\n      formatter_class=argparse.RawTextHelpFormatter)\n  parser_convert_with_tensorrt.add_argument(\n      '--max_workspace_size_bytes',\n      type=int,\n      default=2 << 20,\n      help=('the maximum GPU temporary memory which the TRT engine can use at '\n            'execution time'))\n  parser_convert_with_tensorrt.add_argument(\n      '--precision_mode',\n      type=str,\n      default='FP32',\n      help='one of FP32, FP16 and INT8')\n  parser_convert_with_tensorrt.add_argument(\n      '--minimum_segment_size',\n      type=int,\n      default=3,\n      help=('the minimum number of nodes required for a subgraph to be replaced'\n            'in a TensorRT node'))\n  parser_convert_with_tensorrt.add_argument(\n      '--convert_tf1_model',\n      type=bool,\n      default=False,\n      help='support TRT conversion for TF1 models')\n  parser_convert_with_tensorrt.set_defaults(func=convert_with_tensorrt)", "target": 0}, {"function": "def _parse_common_freeze_and_aot(parser_compile):\n  \"\"\"Parse arguments shared by freeze model and aot_compile.\"\"\"\n  parser_compile.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to convert')\n  parser_compile.add_argument(\n      '--output_prefix',\n      type=str,\n      required=True,\n      help=('output directory + filename prefix for the resulting header(s) '\n            'and object file(s)'))\n  parser_compile.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to convert, separated by \\',\\'')\n  parser_compile.add_argument(\n      '--signature_def_key',\n      type=str,\n      default=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY,\n      help=('signature_def key to use.  '\n            'default: DEFAULT_SERVING_SIGNATURE_DEF_KEY'))\n  parser_compile.add_argument(\n      '--checkpoint_path',\n      type=str,\n      default=None,\n      help='Custom checkpoint to use (default: use the SavedModel variables)')\n  parser_compile.add_argument(\n      '--variables_to_feed',\n      type=str,\n      default='',\n      help=('The names of variables that will be fed into the network.  '\n            'Options are: empty (default; all variables are frozen, none may '\n            'be fed), \\'all\\' (all variables may be fed), or a '\n            'comma-delimited list of names of variables that may be fed.  In '\n            'the last case, the non-fed variables will be frozen in the graph.'\n            '**NOTE** Any variables passed to `variables_to_feed` *must be set '\n            'by the user*.  These variables will NOT be frozen and their '\n            'values will be uninitialized in the compiled object '\n            '(this applies to all input arguments from the signature as '\n            'well).'))", "target": 0}, {"function": "def add_freeze_model_subparser(subparsers):\n  \"\"\"Add parser for `freeze_model`.\"\"\"\n  compile_msg = '\\n'.join(\n      ['Usage example:',\n       'To freeze a SavedModel in preparation for tfcompile:',\n       '$saved_model_cli freeze_model \\\\',\n       '   --dir /tmp/saved_model \\\\',\n       '   --tag_set serve \\\\',\n       '   --output_prefix /tmp/saved_model_xla_aot',\n      ])\n\n  parser_compile = subparsers.add_parser(\n      'freeze_model',\n      description=compile_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  _parse_common_freeze_and_aot(parser_compile)\n  parser_compile.set_defaults(func=freeze_model)", "target": 0}, {"function": "def add_aot_compile_cpu_subparser(subparsers):\n  \"\"\"Add parser for `aot_compile_cpu`.\"\"\"\n  compile_msg = '\\n'.join(\n      ['Usage example:',\n       'To compile a SavedModel signature via (CPU) XLA AOT:',\n       '$saved_model_cli aot_compile_cpu \\\\',\n       '   --dir /tmp/saved_model \\\\',\n       '   --tag_set serve \\\\',\n       '   --output_dir /tmp/saved_model_xla_aot',\n       '', '',\n       'Note: Additional XLA compilation options are available by setting the ',\n       'XLA_FLAGS environment variable.  See the XLA debug options flags for ',\n       'all the options: ',\n       '  {}'.format(_XLA_DEBUG_OPTIONS_URL),\n       '',\n       'For example, to disable XLA fast math when compiling:',\n       '',\n       'XLA_FLAGS=\"--xla_cpu_enable_fast_math=false\" $saved_model_cli '\n       'aot_compile_cpu ...',\n       '',\n       'Some possibly useful flags:',\n       '  --xla_cpu_enable_fast_math=false',\n       '  --xla_force_host_platform_device_count=<num threads>',\n       '    (useful in conjunction with disabling multi threading)'\n      ])\n\n  parser_compile = subparsers.add_parser(\n      'aot_compile_cpu',\n      description=compile_msg,\n      formatter_class=argparse.RawTextHelpFormatter)\n  _parse_common_freeze_and_aot(parser_compile)\n  parser_compile.add_argument(\n      '--target_triple',\n      type=str,\n      default='x86_64-pc-linux',\n      help=('Target triple for LLVM during AOT compilation.  Examples: '\n            'x86_64-none-darwin, x86_64-apple-ios, arm64-none-ios, '\n            'armv7-none-android.  More examples are available in tfcompile.bzl '\n            'in the tensorflow codebase.'))\n  parser_compile.add_argument(\n      '--target_cpu',\n      type=str,\n      default='',\n      help=('Target cpu name for LLVM during AOT compilation.  Examples: '\n            'x86_64, skylake, haswell, westmere, <empty> (unknown).  For '\n            'a complete list of options, run (for x86 targets): '\n            '`llc -march=x86 -mcpu=help`'))\n  parser_compile.add_argument(\n      '--cpp_class',\n      type=str,\n      required=True,\n      help=('The name of the generated C++ class, wrapping the generated '\n            'function.  The syntax of this flag is '\n            '[[<optional_namespace>::],...]<class_name>.  This mirrors the '\n            'C++ syntax for referring to a class, where multiple namespaces '\n            'may precede the class name, separated by double-colons.  '\n            'The class will be generated in the given namespace(s), or if no '\n            'namespaces are given, within the global namespace.'))\n  parser_compile.add_argument(\n      '--multithreading',\n      type=str,\n      default='False',\n      help=('Enable multithreading in the compiled computation.  '\n            'Note that if using this option, the resulting object files '\n            'may have external dependencies on multithreading libraries '\n            'like nsync.'))\n\n  parser_compile.set_defaults(func=aot_compile_cpu)", "target": 0}, {"function": "def create_parser():\n  \"\"\"Creates a parser that parse the command line arguments.\n\n  Returns:\n    A namespace parsed from command line arguments.\n  \"\"\"\n  parser = argparse.ArgumentParser(\n      description='saved_model_cli: Command-line interface for SavedModel')\n  parser.add_argument('-v', '--version', action='version', version='0.1.0')\n\n  subparsers = parser.add_subparsers(\n      title='commands', description='valid commands', help='additional help')\n\n  # show command\n  add_show_subparser(subparsers)\n\n  # run command\n  add_run_subparser(subparsers)\n\n  # scan command\n  add_scan_subparser(subparsers)\n\n  # tensorrt convert command\n  add_convert_subparser(subparsers)\n\n  # aot_compile_cpu command\n  add_aot_compile_cpu_subparser(subparsers)\n\n  # freeze_model command\n  add_freeze_model_subparser(subparsers)\n  return parser", "target": 0}, {"function": "def main():\n  logging.set_verbosity(logging.INFO)\n  parser = create_parser()\n  args = parser.parse_args()\n  if not hasattr(args, 'func'):\n    parser.error('too few arguments')\n  args.func(args)", "target": 0}]}, {"raw_url": "https://github.com/tensorflow/tensorflow/raw/c5da7af048611aa29e9382371f0aed5018516cac/tensorflow%2Fpython%2Ftools%2Fsaved_model_cli_test.py", "code": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for SavedModelCLI tool.\"\"\"\nimport contextlib\nimport os\nimport pickle\nimport platform\nimport shutil\nimport sys\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six import StringIO\n\nfrom tensorflow.core.example import example_pb2\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.core.protobuf import meta_graph_pb2\nfrom tensorflow.python.debug.wrappers import local_cli_wrapper\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.saved_model import save\nfrom tensorflow.python.tools import saved_model_cli\nfrom tensorflow.python.training.tracking import tracking\n\nSAVED_MODEL_PATH = ('cc/saved_model/testdata/half_plus_two/00000123')\n\n\n@contextlib.contextmanager\ndef captured_output():\n  new_out, new_err = StringIO(), StringIO()\n  old_out, old_err = sys.stdout, sys.stderr\n  try:\n    sys.stdout, sys.stderr = new_out, new_err\n    yield sys.stdout, sys.stderr\n  finally:\n    sys.stdout, sys.stderr = old_out, old_err\n\n\nclass SavedModelCLITestCase(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(SavedModelCLITestCase, self).setUp()\n    if platform.system() == 'Windows':\n      self.skipTest('Skipping failing tests on Windows.')\n\n  def testShowCommandAll(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', base_path, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    # pylint: disable=line-too-long\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['classify_x2_to_y3']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x2:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y3:0\n  Method name is: tensorflow/serving/classify\n\nsignature_def['classify_x_to_y']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/classify\n\nsignature_def['regress_x2_to_y3']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x2:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y3:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['regress_x_to_y']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['regress_x_to_y2']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y2:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['x'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['y'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/predict\"\"\"\n    # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowAllWithFunctions(self):\n\n    class DummyModel(tracking.AutoTrackable):\n      \"\"\"Model with callable polymorphic functions specified.\"\"\"\n\n      @def_function.function\n      def func1(self, a, b, c):\n        if c:\n          return a + b\n        else:\n          return a * b\n\n      @def_function.function(input_signature=[\n          tensor_spec.TensorSpec(shape=(2, 2), dtype=dtypes.float32)\n      ])\n      def func2(self, x):\n        return x + 2\n\n      @def_function.function\n      def __call__(self, y, c=7):\n        return y + 2 * c\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = DummyModel()\n    # Call with specific values to create new polymorphic function traces.\n    dummy_model.func1(constant_op.constant(5), constant_op.constant(9), True)\n    dummy_model(constant_op.constant(5))\n    save.save(dummy_model, saved_model_dir)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', saved_model_dir, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['x'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 2)\n        name: serving_default_x:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['output_0'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 2)\n        name: PartitionedCall:0\n  Method name is: tensorflow/serving/predict\n\nConcrete Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          y: TensorSpec(shape=(), dtype=tf.int32, name='y')\n        Argument #2\n          DType: int\n          Value: 7\n\n  Function Name: 'func1'\n    Option #1\n      Callable with:\n        Argument #1\n          a: TensorSpec(shape=(), dtype=tf.int32, name='a')\n        Argument #2\n          b: TensorSpec(shape=(), dtype=tf.int32, name='b')\n        Argument #3\n          DType: bool\n          Value: True\n\n  Function Name: 'func2'\n    Option #1\n      Callable with:\n        Argument #1\n          x: TensorSpec(shape=(2, 2), dtype=tf.float32, name='x')\n\"\"\".strip()  # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowAllWithPureConcreteFunction(self):\n\n    class DummyModel(tracking.AutoTrackable):\n      \"\"\"Model with a callable concrete function.\"\"\"\n\n      def __init__(self):\n        function = def_function.function(\n            self.multiply,\n            input_signature=[\n                tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n                tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32)\n            ])\n        self.pure_concrete_function = function.get_concrete_function()\n        super(DummyModel, self).__init__()\n\n      def multiply(self, a, b):\n        return a * b\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = DummyModel()\n    save.save(dummy_model, saved_model_dir)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', saved_model_dir, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['a'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: serving_default_a:0\n    inputs['b'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: serving_default_b:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['output_0'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: PartitionedCall:0\n  Method name is: tensorflow/serving/predict\n\nConcrete Functions:\n  Function Name: 'pure_concrete_function'\n    Option #1\n      Callable with:\n        Argument #1\n          a: TensorSpec(shape=(), dtype=tf.float32, name='a')\n        Argument #2\n          b: TensorSpec(shape=(), dtype=tf.float32, name='b')\n\"\"\".strip()  # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandTags(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', base_path])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = 'The given SavedModel contains the following tag-sets:\\n\\'serve\\''\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandSignature(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(\n        ['show', '--dir', base_path, '--tag_set', 'serve'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_header = ('The given SavedModel MetaGraphDef contains SignatureDefs '\n                  'with the following keys:')\n    exp_start = 'SignatureDef key: '\n    exp_keys = [\n        '\"classify_x2_to_y3\"', '\"classify_x_to_y\"', '\"regress_x2_to_y3\"',\n        '\"regress_x_to_y\"', '\"regress_x_to_y2\"', '\"serving_default\"'\n    ]\n    # Order of signatures does not matter\n    self.assertMultiLineEqual(\n        output,\n        '\\n'.join([exp_header] + [exp_start + exp_key for exp_key in exp_keys]))\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandErrorNoTagSet(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(\n        ['show', '--dir', base_path, '--tag_set', 'badtagset'])\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.show(args)\n\n  def testShowCommandInputsOutputs(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args([\n        'show', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default'\n    ])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    expected_output = (\n        'The given SavedModel SignatureDef contains the following input(s):\\n'\n        '  inputs[\\'x\\'] tensor_info:\\n'\n        '      dtype: DT_FLOAT\\n      shape: (-1, 1)\\n      name: x:0\\n'\n        'The given SavedModel SignatureDef contains the following output(s):\\n'\n        '  outputs[\\'y\\'] tensor_info:\\n'\n        '      dtype: DT_FLOAT\\n      shape: (-1, 1)\\n      name: y:0\\n'\n        'Method name is: tensorflow/serving/predict')\n    self.assertEqual(output, expected_output)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testPrintREFTypeTensor(self):\n    ref_tensor_info = meta_graph_pb2.TensorInfo()\n    ref_tensor_info.dtype = types_pb2.DT_FLOAT_REF\n    with captured_output() as (out, err):\n      saved_model_cli._print_tensor_info(ref_tensor_info)\n    self.assertTrue('DT_FLOAT_REF' in out.getvalue().strip())\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testInputPreProcessFormats(self):\n    input_str = 'input1=/path/file.txt[ab3];input2=file2'\n    input_expr_str = 'input3=np.zeros([2,2]);input4=[4,5]'\n    input_dict = saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_expr_dict = saved_model_cli.preprocess_input_exprs_arg_string(\n        input_expr_str, safe=False)\n    self.assertTrue(input_dict['input1'] == ('/path/file.txt', 'ab3'))\n    self.assertTrue(input_dict['input2'] == ('file2', None))\n    print(input_expr_dict['input3'])\n    self.assertAllClose(input_expr_dict['input3'], np.zeros([2, 2]))\n    self.assertAllClose(input_expr_dict['input4'], [4, 5])\n    self.assertTrue(len(input_dict) == 2)\n    self.assertTrue(len(input_expr_dict) == 2)\n\n  def testInputPreProcessExamplesWithStrAndBytes(self):\n    input_examples_str = 'inputs=[{\"text\":[\"foo\"], \"bytes\":[b\"bar\"]}]'\n    input_dict = saved_model_cli.preprocess_input_examples_arg_string(\n        input_examples_str)\n    feature = example_pb2.Example.FromString(input_dict['inputs'][0])\n    self.assertProtoEquals(\n        \"\"\"\n          features {\n            feature {\n              key: \"bytes\"\n              value {\n                bytes_list {\n                  value: \"bar\"\n                }\n              }\n            }\n            feature {\n              key: \"text\"\n              value {\n                bytes_list {\n                  value: \"foo\"\n                }\n              }\n            }\n          }\n    \"\"\", feature)\n\n  def testInputPreprocessExampleWithCodeInjection(self):\n    input_examples_str = 'inputs=os.system(\"echo hacked\")'\n    with self.assertRaisesRegex(RuntimeError, 'not a valid python literal.'):\n      saved_model_cli.preprocess_input_examples_arg_string(input_examples_str)\n\n  def testInputPreProcessFileNames(self):\n    input_str = (r'inputx=C:\\Program Files\\data.npz[v:0];'\n                 r'input:0=c:\\PROGRA~1\\data.npy')\n    input_dict = saved_model_cli.preprocess_inputs_arg_string(input_str)\n    self.assertTrue(input_dict['inputx'] == (r'C:\\Program Files\\data.npz',\n                                             'v:0'))\n    self.assertTrue(input_dict['input:0'] == (r'c:\\PROGRA~1\\data.npy', None))\n\n  def testInputPreProcessErrorBadFormat(self):\n    input_str = 'inputx=file[[v1]v2'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_str = 'inputx:file'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_str = 'inputx:np.zeros((5))'\n    with self.assertRaisesRegex(RuntimeError, 'format is incorrect'):\n      saved_model_cli.preprocess_input_exprs_arg_string(input_str, safe=False)\n\n  def testInputParserNPY(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(6)).reshape(2, 3)\n    input0_path = os.path.join(test.get_temp_dir(), 'input0.npy')\n    input1_path = os.path.join(test.get_temp_dir(), 'input1.npy')\n    np.save(input0_path, x0)\n    np.save(input1_path, x1)\n    input_str = 'x0=' + input0_path + '[x0];x1=' + input1_path\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x0'] == x0))\n    self.assertTrue(np.all(feed_dict['x1'] == x1))\n\n  def testInputParserNPZ(self):\n    x0 = np.array([[1], [2]])\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0)\n    input_str = 'x=' + input_path + '[a];y=' + input_path\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x'] == x0))\n    self.assertTrue(np.all(feed_dict['y'] == x0))\n\n  def testInputParserPickle(self):\n    pkl0 = {'a': 5, 'b': np.array(range(4))}\n    pkl1 = np.array([1])\n    pkl2 = np.array([[1], [3]])\n    input_path0 = os.path.join(test.get_temp_dir(), 'pickle0.pkl')\n    input_path1 = os.path.join(test.get_temp_dir(), 'pickle1.pkl')\n    input_path2 = os.path.join(test.get_temp_dir(), 'pickle2.pkl')\n    with open(input_path0, 'wb') as f:\n      pickle.dump(pkl0, f)\n    with open(input_path1, 'wb') as f:\n      pickle.dump(pkl1, f)\n    with open(input_path2, 'wb') as f:\n      pickle.dump(pkl2, f)\n    input_str = 'x=' + input_path0 + '[b];y=' + input_path1 + '[c];'\n    input_str += 'z=' + input_path2\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x'] == pkl0['b']))\n    self.assertTrue(np.all(feed_dict['y'] == pkl1))\n    self.assertTrue(np.all(feed_dict['z'] == pkl2))\n\n  def testInputParserErrorNoName(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(5))\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0, b=x1)\n    input_str = 'x=' + input_path\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.load_inputs_from_input_arg_string(input_str, '', '')\n\n  def testInputParserErrorWrongName(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(5))\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0, b=x1)\n    input_str = 'x=' + input_path + '[c]'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.load_inputs_from_input_arg_string(input_str, '', '')\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamples(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(),\n                              'new_dir' + ('tfrt' if use_tfrt else ''))\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples',\n        'inputs=[{\"x\":[8.0],\"x2\":[5.0]}, {\"x\":[4.0],\"x2\":[3.0]}]', '--outdir',\n        output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(os.path.join(output_dir, 'outputs.npy'))\n    y_expected = np.array([[6.0], [4.0]])\n    self.assertAllEqual(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandExistingOutdir(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(), 'testRunCommand_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'outputs.npy')\n    if os.path.exists(output_file):\n      os.remove(output_file)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x2_to_y3', '--inputs', 'inputs=' + input_path +\n        '[x0]', '--outdir',\n        test.get_temp_dir()\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(output_file)\n    y_expected = np.array([[3.5], [4.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandNewOutdir(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandNewOutdir_inputs.npz')\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    if os.path.isdir(output_dir):\n      shutil.rmtree(output_dir)\n    np.savez(input_path, x0=x, x1=x_notused)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path +\n        '[x0]', '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(os.path.join(output_dir, 'y.npy'))\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandOutOverwrite(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandOutOverwrite_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'y.npy')\n    open(output_file, 'a').close()\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path + '[x0]', '--outdir',\n        test.get_temp_dir(), '--overwrite'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(output_file)\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInvalidInputKeyError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x2_to_y3', '--input_exprs', 'x2=[1,2,3]'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(ValueError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInvalidSignature(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'INVALID_SIGNATURE', '--input_exprs', 'x2=[1,2,3]'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError,\n                                'Could not find signature \"INVALID_SIGNATURE\"'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesNotListError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs={\"x\":8.0,\"x2\":5.0}',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'must be a list'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesFeatureValueNotListError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs=[{\"x\":8.0,\"x2\":5.0}]',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'feature value must be a list'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesFeatureBadType(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs=[{\"x\":[[1],[2]]}]',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'is not supported'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandOutputFileExistError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandOutOverwrite_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'y.npy')\n    open(output_file, 'a').close()\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path + '[x0]', '--outdir',\n        test.get_temp_dir()\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputNotGivenError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(AttributeError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandWithDebuggerEnabled(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandNewOutdir_inputs.npz')\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    if os.path.isdir(output_dir):\n      shutil.rmtree(output_dir)\n    np.savez(input_path, x0=x, x1=x_notused)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path +\n        '[x0]', '--outdir', output_dir, '--tf_debug'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n\n    def fake_wrapper_session(sess):\n      return sess\n\n    with test.mock.patch.object(\n        local_cli_wrapper,\n        'LocalCLIDebugWrapperSession',\n        side_effect=fake_wrapper_session,\n        autospec=True) as fake:\n      saved_model_cli.run(args)\n      fake.assert_called_with(test.mock.ANY)\n\n    y_actual = np.load(os.path.join(output_dir, 'y.npy'))\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  def testScanCommand(self):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args(['scan', '--dir', base_path])\n    with captured_output() as (out, _):\n      saved_model_cli.scan(args)\n    output = out.getvalue().strip()\n    self.assertTrue('does not contain denylisted ops' in output)\n\n  def testScanCommandFoundDenylistedOp(self):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args(\n        ['scan', '--dir', base_path, '--tag_set', 'serve'])\n    op_denylist = saved_model_cli._OP_DENYLIST\n    saved_model_cli._OP_DENYLIST = set(['VariableV2'])\n    with captured_output() as (out, _):\n      saved_model_cli.scan(args)\n    saved_model_cli._OP_DENYLIST = op_denylist\n    output = out.getvalue().strip()\n    self.assertTrue('\\'VariableV2\\'' in output)\n\n  def testAOTCompileCPUWrongSignatureDefKey(self):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir')\n    args = self.parser.parse_args([\n        'aot_compile_cpu', '--dir', base_path, '--tag_set', 'serve',\n        '--output_prefix', output_dir, '--cpp_class', 'Compiled',\n        '--signature_def_key', 'MISSING'\n    ])\n    with self.assertRaisesRegex(ValueError, 'Unable to find signature_def'):\n      saved_model_cli.aot_compile_cpu(args)\n\n  class AOTCompileDummyModel(tracking.AutoTrackable):\n    \"\"\"Model compatible with XLA compilation.\"\"\"\n\n    def __init__(self):\n      self.var = variables.Variable(1.0, name='my_var')\n      self.write_var = variables.Variable(1.0, name='write_var')\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=(2, 2), dtype=dtypes.float32),\n        # Test unused inputs.\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func2(self, x, y):\n      del y\n      return {'res': x + self.var}\n\n    @def_function.function(input_signature=[\n        # Test large inputs.\n        tensor_spec.TensorSpec(shape=(2048, 16), dtype=dtypes.float32),\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func3(self, x, y):\n      del y\n      return {'res': x + self.var}\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func_write(self, x, y):\n      del y\n      self.write_var.assign(x + self.var)\n      return {'res': self.write_var}\n\n  @parameterized.named_parameters(\n      ('VariablesToFeedNone', '', 'func2', None),\n      ('VariablesToFeedNoneTargetAarch64Linux', '', 'func2',\n       'aarch64-none-linux-gnu'),\n      ('VariablesToFeedNoneTargetAarch64Android', '', 'func2',\n       'aarch64-none-android'),\n      ('VariablesToFeedAll', 'all', 'func2', None),\n      ('VariablesToFeedMyVar', 'my_var', 'func2', None),\n      ('VariablesToFeedNoneLargeConstant', '', 'func3', None),\n      ('WriteToWriteVar', 'all', 'func_write', None),\n  )\n  def testAOTCompileCPUFreezesAndCompiles(self, variables_to_feed, func,\n                                          target_triple):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = self.AOTCompileDummyModel()\n    func = getattr(dummy_model, func)\n    with self.cached_session():\n      self.evaluate(dummy_model.var.initializer)\n      self.evaluate(dummy_model.write_var.initializer)\n      save.save(dummy_model, saved_model_dir, signatures={'func': func})\n\n    self.parser = saved_model_cli.create_parser()\n    output_prefix = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir/out')\n    args = [  # Use the default seving signature_key.\n        'aot_compile_cpu', '--dir', saved_model_dir, '--tag_set', 'serve',\n        '--signature_def_key', 'func', '--output_prefix', output_prefix,\n        '--variables_to_feed', variables_to_feed, '--cpp_class', 'Generated'\n    ]\n    if target_triple:\n      args.extend(['--target_triple', target_triple])\n    args = self.parser.parse_args(args)\n    with test.mock.patch.object(logging, 'warn') as captured_warn:\n      saved_model_cli.aot_compile_cpu(args)\n    self.assertRegex(\n        str(captured_warn.call_args),\n        'Signature input key \\'y\\'.*has been pruned while freezing the graph.')\n    self.assertTrue(file_io.file_exists('{}.o'.format(output_prefix)))\n    self.assertTrue(file_io.file_exists('{}.h'.format(output_prefix)))\n    self.assertTrue(file_io.file_exists('{}_metadata.o'.format(output_prefix)))\n    self.assertTrue(\n        file_io.file_exists('{}_makefile.inc'.format(output_prefix)))\n    header_contents = file_io.read_file_to_string('{}.h'.format(output_prefix))\n    self.assertIn('class Generated', header_contents)\n    self.assertIn('arg_feed_x_data', header_contents)\n    self.assertIn('result_fetch_res_data', header_contents)\n    # arg_y got filtered out as it's not used by the output.\n    self.assertNotIn('arg_feed_y_data', header_contents)\n    if variables_to_feed:\n      # Read-only-variables' setters preserve constness.\n      self.assertIn('set_var_param_my_var_data(const float', header_contents)\n      self.assertNotIn('set_var_param_my_var_data(float', header_contents)\n    if func == dummy_model.func_write:\n      # Writeable variables setters do not preserve constness.\n      self.assertIn('set_var_param_write_var_data(float', header_contents)\n      self.assertNotIn('set_var_param_write_var_data(const float',\n                       header_contents)\n\n    makefile_contents = file_io.read_file_to_string(\n        '{}_makefile.inc'.format(output_prefix))\n    self.assertIn('-D_GLIBCXX_USE_CXX11_ABI=', makefile_contents)\n\n  def testFreezeModel(self):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    variables_to_feed = 'all'\n    func = 'func2'\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = self.AOTCompileDummyModel()\n    func = getattr(dummy_model, func)\n    with self.cached_session():\n      self.evaluate(dummy_model.var.initializer)\n      self.evaluate(dummy_model.write_var.initializer)\n      save.save(dummy_model, saved_model_dir, signatures={'func': func})\n\n    self.parser = saved_model_cli.create_parser()\n    output_prefix = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir/out')\n    args = [  # Use the default seving signature_key.\n        'freeze_model', '--dir', saved_model_dir, '--tag_set', 'serve',\n        '--signature_def_key', 'func', '--output_prefix', output_prefix,\n        '--variables_to_feed', variables_to_feed\n    ]\n    args = self.parser.parse_args(args)\n    with test.mock.patch.object(logging, 'warn'):\n      saved_model_cli.freeze_model(args)\n    self.assertTrue(\n        file_io.file_exists(os.path.join(output_prefix, 'frozen_graph.pb')))\n    self.assertTrue(\n        file_io.file_exists(os.path.join(output_prefix, 'config.pbtxt')))\n\n\nif __name__ == '__main__':\n  test.main()\n", "code_before": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for SavedModelCLI tool.\"\"\"\nimport contextlib\nimport os\nimport pickle\nimport platform\nimport shutil\nimport sys\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six import StringIO\n\nfrom tensorflow.core.example import example_pb2\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.core.protobuf import meta_graph_pb2\nfrom tensorflow.python.debug.wrappers import local_cli_wrapper\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.saved_model import save\nfrom tensorflow.python.tools import saved_model_cli\nfrom tensorflow.python.training.tracking import tracking\n\nSAVED_MODEL_PATH = ('cc/saved_model/testdata/half_plus_two/00000123')\n\n\n@contextlib.contextmanager\ndef captured_output():\n  new_out, new_err = StringIO(), StringIO()\n  old_out, old_err = sys.stdout, sys.stderr\n  try:\n    sys.stdout, sys.stderr = new_out, new_err\n    yield sys.stdout, sys.stderr\n  finally:\n    sys.stdout, sys.stderr = old_out, old_err\n\n\nclass SavedModelCLITestCase(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(SavedModelCLITestCase, self).setUp()\n    if platform.system() == 'Windows':\n      self.skipTest('Skipping failing tests on Windows.')\n\n  def testShowCommandAll(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', base_path, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    # pylint: disable=line-too-long\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['classify_x2_to_y3']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x2:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y3:0\n  Method name is: tensorflow/serving/classify\n\nsignature_def['classify_x_to_y']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/classify\n\nsignature_def['regress_x2_to_y3']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x2:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y3:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['regress_x_to_y']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['regress_x_to_y2']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y2:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['x'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['y'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/predict\"\"\"\n    # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowAllWithFunctions(self):\n\n    class DummyModel(tracking.AutoTrackable):\n      \"\"\"Model with callable polymorphic functions specified.\"\"\"\n\n      @def_function.function\n      def func1(self, a, b, c):\n        if c:\n          return a + b\n        else:\n          return a * b\n\n      @def_function.function(input_signature=[\n          tensor_spec.TensorSpec(shape=(2, 2), dtype=dtypes.float32)\n      ])\n      def func2(self, x):\n        return x + 2\n\n      @def_function.function\n      def __call__(self, y, c=7):\n        return y + 2 * c\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = DummyModel()\n    # Call with specific values to create new polymorphic function traces.\n    dummy_model.func1(constant_op.constant(5), constant_op.constant(9), True)\n    dummy_model(constant_op.constant(5))\n    save.save(dummy_model, saved_model_dir)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', saved_model_dir, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['x'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 2)\n        name: serving_default_x:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['output_0'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 2)\n        name: PartitionedCall:0\n  Method name is: tensorflow/serving/predict\n\nConcrete Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          y: TensorSpec(shape=(), dtype=tf.int32, name='y')\n        Argument #2\n          DType: int\n          Value: 7\n\n  Function Name: 'func1'\n    Option #1\n      Callable with:\n        Argument #1\n          a: TensorSpec(shape=(), dtype=tf.int32, name='a')\n        Argument #2\n          b: TensorSpec(shape=(), dtype=tf.int32, name='b')\n        Argument #3\n          DType: bool\n          Value: True\n\n  Function Name: 'func2'\n    Option #1\n      Callable with:\n        Argument #1\n          x: TensorSpec(shape=(2, 2), dtype=tf.float32, name='x')\n\"\"\".strip()  # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowAllWithPureConcreteFunction(self):\n\n    class DummyModel(tracking.AutoTrackable):\n      \"\"\"Model with a callable concrete function.\"\"\"\n\n      def __init__(self):\n        function = def_function.function(\n            self.multiply,\n            input_signature=[\n                tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n                tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32)\n            ])\n        self.pure_concrete_function = function.get_concrete_function()\n        super(DummyModel, self).__init__()\n\n      def multiply(self, a, b):\n        return a * b\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = DummyModel()\n    save.save(dummy_model, saved_model_dir)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', saved_model_dir, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['a'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: serving_default_a:0\n    inputs['b'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: serving_default_b:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['output_0'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: PartitionedCall:0\n  Method name is: tensorflow/serving/predict\n\nConcrete Functions:\n  Function Name: 'pure_concrete_function'\n    Option #1\n      Callable with:\n        Argument #1\n          a: TensorSpec(shape=(), dtype=tf.float32, name='a')\n        Argument #2\n          b: TensorSpec(shape=(), dtype=tf.float32, name='b')\n\"\"\".strip()  # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandTags(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', base_path])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = 'The given SavedModel contains the following tag-sets:\\n\\'serve\\''\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandSignature(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(\n        ['show', '--dir', base_path, '--tag_set', 'serve'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_header = ('The given SavedModel MetaGraphDef contains SignatureDefs '\n                  'with the following keys:')\n    exp_start = 'SignatureDef key: '\n    exp_keys = [\n        '\"classify_x2_to_y3\"', '\"classify_x_to_y\"', '\"regress_x2_to_y3\"',\n        '\"regress_x_to_y\"', '\"regress_x_to_y2\"', '\"serving_default\"'\n    ]\n    # Order of signatures does not matter\n    self.assertMultiLineEqual(\n        output,\n        '\\n'.join([exp_header] + [exp_start + exp_key for exp_key in exp_keys]))\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandErrorNoTagSet(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(\n        ['show', '--dir', base_path, '--tag_set', 'badtagset'])\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.show(args)\n\n  def testShowCommandInputsOutputs(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args([\n        'show', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default'\n    ])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    expected_output = (\n        'The given SavedModel SignatureDef contains the following input(s):\\n'\n        '  inputs[\\'x\\'] tensor_info:\\n'\n        '      dtype: DT_FLOAT\\n      shape: (-1, 1)\\n      name: x:0\\n'\n        'The given SavedModel SignatureDef contains the following output(s):\\n'\n        '  outputs[\\'y\\'] tensor_info:\\n'\n        '      dtype: DT_FLOAT\\n      shape: (-1, 1)\\n      name: y:0\\n'\n        'Method name is: tensorflow/serving/predict')\n    self.assertEqual(output, expected_output)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testPrintREFTypeTensor(self):\n    ref_tensor_info = meta_graph_pb2.TensorInfo()\n    ref_tensor_info.dtype = types_pb2.DT_FLOAT_REF\n    with captured_output() as (out, err):\n      saved_model_cli._print_tensor_info(ref_tensor_info)\n    self.assertTrue('DT_FLOAT_REF' in out.getvalue().strip())\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testInputPreProcessFormats(self):\n    input_str = 'input1=/path/file.txt[ab3];input2=file2'\n    input_expr_str = 'input3=np.zeros([2,2]);input4=[4,5]'\n    input_dict = saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_expr_dict = saved_model_cli.preprocess_input_exprs_arg_string(\n        input_expr_str, safe=False)\n    self.assertTrue(input_dict['input1'] == ('/path/file.txt', 'ab3'))\n    self.assertTrue(input_dict['input2'] == ('file2', None))\n    print(input_expr_dict['input3'])\n    self.assertAllClose(input_expr_dict['input3'], np.zeros([2, 2]))\n    self.assertAllClose(input_expr_dict['input4'], [4, 5])\n    self.assertTrue(len(input_dict) == 2)\n    self.assertTrue(len(input_expr_dict) == 2)\n\n  def testInputPreProcessExamplesWithStrAndBytes(self):\n    input_examples_str = 'inputs=[{\"text\":[\"foo\"], \"bytes\":[b\"bar\"]}]'\n    input_dict = saved_model_cli.preprocess_input_examples_arg_string(\n        input_examples_str)\n    feature = example_pb2.Example.FromString(input_dict['inputs'][0])\n    self.assertProtoEquals(\n        \"\"\"\n          features {\n            feature {\n              key: \"bytes\"\n              value {\n                bytes_list {\n                  value: \"bar\"\n                }\n              }\n            }\n            feature {\n              key: \"text\"\n              value {\n                bytes_list {\n                  value: \"foo\"\n                }\n              }\n            }\n          }\n    \"\"\", feature)\n\n  def testInputPreprocessExampleWithCodeInjection(self):\n    input_examples_str = 'inputs=os.system(\"echo hacked\")'\n    with self.assertRaisesRegex(RuntimeError, 'not a valid python literal.'):\n      saved_model_cli.preprocess_input_examples_arg_string(input_examples_str)\n\n  def testInputPreProcessFileNames(self):\n    input_str = (r'inputx=C:\\Program Files\\data.npz[v:0];'\n                 r'input:0=c:\\PROGRA~1\\data.npy')\n    input_dict = saved_model_cli.preprocess_inputs_arg_string(input_str)\n    self.assertTrue(input_dict['inputx'] == (r'C:\\Program Files\\data.npz',\n                                             'v:0'))\n    self.assertTrue(input_dict['input:0'] == (r'c:\\PROGRA~1\\data.npy', None))\n\n  def testInputPreProcessErrorBadFormat(self):\n    input_str = 'inputx=file[[v1]v2'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_str = 'inputx:file'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_str = 'inputx:np.zeros((5))'\n    with self.assertRaisesRegex(RuntimeError, 'format is incorrect'):\n      saved_model_cli.preprocess_input_exprs_arg_string(input_str, safe=False)\n\n  def testInputParserNPY(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(6)).reshape(2, 3)\n    input0_path = os.path.join(test.get_temp_dir(), 'input0.npy')\n    input1_path = os.path.join(test.get_temp_dir(), 'input1.npy')\n    np.save(input0_path, x0)\n    np.save(input1_path, x1)\n    input_str = 'x0=' + input0_path + '[x0];x1=' + input1_path\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x0'] == x0))\n    self.assertTrue(np.all(feed_dict['x1'] == x1))\n\n  def testInputParserNPZ(self):\n    x0 = np.array([[1], [2]])\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0)\n    input_str = 'x=' + input_path + '[a];y=' + input_path\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x'] == x0))\n    self.assertTrue(np.all(feed_dict['y'] == x0))\n\n  def testInputParserPickle(self):\n    pkl0 = {'a': 5, 'b': np.array(range(4))}\n    pkl1 = np.array([1])\n    pkl2 = np.array([[1], [3]])\n    input_path0 = os.path.join(test.get_temp_dir(), 'pickle0.pkl')\n    input_path1 = os.path.join(test.get_temp_dir(), 'pickle1.pkl')\n    input_path2 = os.path.join(test.get_temp_dir(), 'pickle2.pkl')\n    with open(input_path0, 'wb') as f:\n      pickle.dump(pkl0, f)\n    with open(input_path1, 'wb') as f:\n      pickle.dump(pkl1, f)\n    with open(input_path2, 'wb') as f:\n      pickle.dump(pkl2, f)\n    input_str = 'x=' + input_path0 + '[b];y=' + input_path1 + '[c];'\n    input_str += 'z=' + input_path2\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x'] == pkl0['b']))\n    self.assertTrue(np.all(feed_dict['y'] == pkl1))\n    self.assertTrue(np.all(feed_dict['z'] == pkl2))\n\n  def testInputParserPythonExpression(self):\n    x1 = np.ones([2, 10])\n    x2 = np.array([[1], [2], [3]])\n    x3 = np.mgrid[0:5, 0:5]\n    x4 = [[3], [4]]\n    input_expr_str = ('x1=np.ones([2,10]);x2=np.array([[1],[2],[3]]);'\n                      'x3=np.mgrid[0:5,0:5];x4=[[3],[4]]')\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        '', input_expr_str, '')\n    self.assertTrue(np.all(feed_dict['x1'] == x1))\n    self.assertTrue(np.all(feed_dict['x2'] == x2))\n    self.assertTrue(np.all(feed_dict['x3'] == x3))\n    self.assertTrue(np.all(feed_dict['x4'] == x4))\n\n  def testInputParserBoth(self):\n    x0 = np.array([[1], [2]])\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0)\n    x1 = np.ones([2, 10])\n    input_str = 'x0=' + input_path + '[a]'\n    input_expr_str = 'x1=np.ones([2,10])'\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, input_expr_str, '')\n    self.assertTrue(np.all(feed_dict['x0'] == x0))\n    self.assertTrue(np.all(feed_dict['x1'] == x1))\n\n  def testInputParserBothDuplicate(self):\n    x0 = np.array([[1], [2]])\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0)\n    x1 = np.ones([2, 10])\n    input_str = 'x0=' + input_path + '[a]'\n    input_expr_str = 'x0=np.ones([2,10])'\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, input_expr_str, '')\n    self.assertTrue(np.all(feed_dict['x0'] == x1))\n\n  def testInputParserErrorNoName(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(5))\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0, b=x1)\n    input_str = 'x=' + input_path\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.load_inputs_from_input_arg_string(input_str, '', '')\n\n  def testInputParserErrorWrongName(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(5))\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0, b=x1)\n    input_str = 'x=' + input_path + '[c]'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.load_inputs_from_input_arg_string(input_str, '', '')\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamples(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(),\n                              'new_dir' + ('tfrt' if use_tfrt else ''))\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples',\n        'inputs=[{\"x\":[8.0],\"x2\":[5.0]}, {\"x\":[4.0],\"x2\":[3.0]}]', '--outdir',\n        output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(os.path.join(output_dir, 'outputs.npy'))\n    y_expected = np.array([[6.0], [4.0]])\n    self.assertAllEqual(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandExistingOutdir(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(), 'testRunCommand_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'outputs.npy')\n    if os.path.exists(output_file):\n      os.remove(output_file)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x2_to_y3', '--inputs', 'inputs=' + input_path +\n        '[x0]', '--outdir',\n        test.get_temp_dir()\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(output_file)\n    y_expected = np.array([[3.5], [4.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandNewOutdir(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandNewOutdir_inputs.npz')\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    if os.path.isdir(output_dir):\n      shutil.rmtree(output_dir)\n    np.savez(input_path, x0=x, x1=x_notused)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path +\n        '[x0]', '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(os.path.join(output_dir, 'y.npy'))\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandOutOverwrite(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandOutOverwrite_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'y.npy')\n    open(output_file, 'a').close()\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path + '[x0]', '--outdir',\n        test.get_temp_dir(), '--overwrite'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(output_file)\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInvalidInputKeyError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x2_to_y3', '--input_exprs', 'x2=np.ones((3,1))'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(ValueError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInvalidSignature(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'INVALID_SIGNATURE', '--input_exprs', 'x2=np.ones((3,1))'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError,\n                                'Could not find signature \"INVALID_SIGNATURE\"'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesNotListError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs={\"x\":8.0,\"x2\":5.0}',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'must be a list'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesFeatureValueNotListError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs=[{\"x\":8.0,\"x2\":5.0}]',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'feature value must be a list'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesFeatureBadType(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs=[{\"x\":[[1],[2]]}]',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'is not supported'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandOutputFileExistError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandOutOverwrite_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'y.npy')\n    open(output_file, 'a').close()\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path + '[x0]', '--outdir',\n        test.get_temp_dir()\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputNotGivenError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(AttributeError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandWithDebuggerEnabled(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandNewOutdir_inputs.npz')\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    if os.path.isdir(output_dir):\n      shutil.rmtree(output_dir)\n    np.savez(input_path, x0=x, x1=x_notused)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path +\n        '[x0]', '--outdir', output_dir, '--tf_debug'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n\n    def fake_wrapper_session(sess):\n      return sess\n\n    with test.mock.patch.object(\n        local_cli_wrapper,\n        'LocalCLIDebugWrapperSession',\n        side_effect=fake_wrapper_session,\n        autospec=True) as fake:\n      saved_model_cli.run(args)\n      fake.assert_called_with(test.mock.ANY)\n\n    y_actual = np.load(os.path.join(output_dir, 'y.npy'))\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  def testScanCommand(self):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args(['scan', '--dir', base_path])\n    with captured_output() as (out, _):\n      saved_model_cli.scan(args)\n    output = out.getvalue().strip()\n    self.assertTrue('does not contain denylisted ops' in output)\n\n  def testScanCommandFoundDenylistedOp(self):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args(\n        ['scan', '--dir', base_path, '--tag_set', 'serve'])\n    op_denylist = saved_model_cli._OP_DENYLIST\n    saved_model_cli._OP_DENYLIST = set(['VariableV2'])\n    with captured_output() as (out, _):\n      saved_model_cli.scan(args)\n    saved_model_cli._OP_DENYLIST = op_denylist\n    output = out.getvalue().strip()\n    self.assertTrue('\\'VariableV2\\'' in output)\n\n  def testAOTCompileCPUWrongSignatureDefKey(self):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir')\n    args = self.parser.parse_args([\n        'aot_compile_cpu', '--dir', base_path, '--tag_set', 'serve',\n        '--output_prefix', output_dir, '--cpp_class', 'Compiled',\n        '--signature_def_key', 'MISSING'\n    ])\n    with self.assertRaisesRegex(ValueError, 'Unable to find signature_def'):\n      saved_model_cli.aot_compile_cpu(args)\n\n  class AOTCompileDummyModel(tracking.AutoTrackable):\n    \"\"\"Model compatible with XLA compilation.\"\"\"\n\n    def __init__(self):\n      self.var = variables.Variable(1.0, name='my_var')\n      self.write_var = variables.Variable(1.0, name='write_var')\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=(2, 2), dtype=dtypes.float32),\n        # Test unused inputs.\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func2(self, x, y):\n      del y\n      return {'res': x + self.var}\n\n    @def_function.function(input_signature=[\n        # Test large inputs.\n        tensor_spec.TensorSpec(shape=(2048, 16), dtype=dtypes.float32),\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func3(self, x, y):\n      del y\n      return {'res': x + self.var}\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func_write(self, x, y):\n      del y\n      self.write_var.assign(x + self.var)\n      return {'res': self.write_var}\n\n  @parameterized.named_parameters(\n      ('VariablesToFeedNone', '', 'func2', None),\n      ('VariablesToFeedNoneTargetAarch64Linux', '', 'func2',\n       'aarch64-none-linux-gnu'),\n      ('VariablesToFeedNoneTargetAarch64Android', '', 'func2',\n       'aarch64-none-android'),\n      ('VariablesToFeedAll', 'all', 'func2', None),\n      ('VariablesToFeedMyVar', 'my_var', 'func2', None),\n      ('VariablesToFeedNoneLargeConstant', '', 'func3', None),\n      ('WriteToWriteVar', 'all', 'func_write', None),\n  )\n  def testAOTCompileCPUFreezesAndCompiles(self, variables_to_feed, func,\n                                          target_triple):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = self.AOTCompileDummyModel()\n    func = getattr(dummy_model, func)\n    with self.cached_session():\n      self.evaluate(dummy_model.var.initializer)\n      self.evaluate(dummy_model.write_var.initializer)\n      save.save(dummy_model, saved_model_dir, signatures={'func': func})\n\n    self.parser = saved_model_cli.create_parser()\n    output_prefix = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir/out')\n    args = [  # Use the default seving signature_key.\n        'aot_compile_cpu', '--dir', saved_model_dir, '--tag_set', 'serve',\n        '--signature_def_key', 'func', '--output_prefix', output_prefix,\n        '--variables_to_feed', variables_to_feed, '--cpp_class', 'Generated'\n    ]\n    if target_triple:\n      args.extend(['--target_triple', target_triple])\n    args = self.parser.parse_args(args)\n    with test.mock.patch.object(logging, 'warn') as captured_warn:\n      saved_model_cli.aot_compile_cpu(args)\n    self.assertRegex(\n        str(captured_warn.call_args),\n        'Signature input key \\'y\\'.*has been pruned while freezing the graph.')\n    self.assertTrue(file_io.file_exists('{}.o'.format(output_prefix)))\n    self.assertTrue(file_io.file_exists('{}.h'.format(output_prefix)))\n    self.assertTrue(file_io.file_exists('{}_metadata.o'.format(output_prefix)))\n    self.assertTrue(\n        file_io.file_exists('{}_makefile.inc'.format(output_prefix)))\n    header_contents = file_io.read_file_to_string('{}.h'.format(output_prefix))\n    self.assertIn('class Generated', header_contents)\n    self.assertIn('arg_feed_x_data', header_contents)\n    self.assertIn('result_fetch_res_data', header_contents)\n    # arg_y got filtered out as it's not used by the output.\n    self.assertNotIn('arg_feed_y_data', header_contents)\n    if variables_to_feed:\n      # Read-only-variables' setters preserve constness.\n      self.assertIn('set_var_param_my_var_data(const float', header_contents)\n      self.assertNotIn('set_var_param_my_var_data(float', header_contents)\n    if func == dummy_model.func_write:\n      # Writeable variables setters do not preserve constness.\n      self.assertIn('set_var_param_write_var_data(float', header_contents)\n      self.assertNotIn('set_var_param_write_var_data(const float',\n                       header_contents)\n\n    makefile_contents = file_io.read_file_to_string(\n        '{}_makefile.inc'.format(output_prefix))\n    self.assertIn('-D_GLIBCXX_USE_CXX11_ABI=', makefile_contents)\n\n  def testFreezeModel(self):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    variables_to_feed = 'all'\n    func = 'func2'\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = self.AOTCompileDummyModel()\n    func = getattr(dummy_model, func)\n    with self.cached_session():\n      self.evaluate(dummy_model.var.initializer)\n      self.evaluate(dummy_model.write_var.initializer)\n      save.save(dummy_model, saved_model_dir, signatures={'func': func})\n\n    self.parser = saved_model_cli.create_parser()\n    output_prefix = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir/out')\n    args = [  # Use the default seving signature_key.\n        'freeze_model', '--dir', saved_model_dir, '--tag_set', 'serve',\n        '--signature_def_key', 'func', '--output_prefix', output_prefix,\n        '--variables_to_feed', variables_to_feed\n    ]\n    args = self.parser.parse_args(args)\n    with test.mock.patch.object(logging, 'warn'):\n      saved_model_cli.freeze_model(args)\n    self.assertTrue(\n        file_io.file_exists(os.path.join(output_prefix, 'frozen_graph.pb')))\n    self.assertTrue(\n        file_io.file_exists(os.path.join(output_prefix, 'config.pbtxt')))\n\n\nif __name__ == '__main__':\n  test.main()\n", "patch": "@@ -486,43 +486,6 @@ def testInputParserPickle(self):\n     self.assertTrue(np.all(feed_dict['y'] == pkl1))\n     self.assertTrue(np.all(feed_dict['z'] == pkl2))\n \n-  def testInputParserPythonExpression(self):\n-    x1 = np.ones([2, 10])\n-    x2 = np.array([[1], [2], [3]])\n-    x3 = np.mgrid[0:5, 0:5]\n-    x4 = [[3], [4]]\n-    input_expr_str = ('x1=np.ones([2,10]);x2=np.array([[1],[2],[3]]);'\n-                      'x3=np.mgrid[0:5,0:5];x4=[[3],[4]]')\n-    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n-        '', input_expr_str, '')\n-    self.assertTrue(np.all(feed_dict['x1'] == x1))\n-    self.assertTrue(np.all(feed_dict['x2'] == x2))\n-    self.assertTrue(np.all(feed_dict['x3'] == x3))\n-    self.assertTrue(np.all(feed_dict['x4'] == x4))\n-\n-  def testInputParserBoth(self):\n-    x0 = np.array([[1], [2]])\n-    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n-    np.savez(input_path, a=x0)\n-    x1 = np.ones([2, 10])\n-    input_str = 'x0=' + input_path + '[a]'\n-    input_expr_str = 'x1=np.ones([2,10])'\n-    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n-        input_str, input_expr_str, '')\n-    self.assertTrue(np.all(feed_dict['x0'] == x0))\n-    self.assertTrue(np.all(feed_dict['x1'] == x1))\n-\n-  def testInputParserBothDuplicate(self):\n-    x0 = np.array([[1], [2]])\n-    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n-    np.savez(input_path, a=x0)\n-    x1 = np.ones([2, 10])\n-    input_str = 'x0=' + input_path + '[a]'\n-    input_expr_str = 'x0=np.ones([2,10])'\n-    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n-        input_str, input_expr_str, '')\n-    self.assertTrue(np.all(feed_dict['x0'] == x1))\n-\n   def testInputParserErrorNoName(self):\n     x0 = np.array([[1], [2]])\n     x1 = np.array(range(5))\n@@ -629,7 +592,7 @@ def testRunCommandInvalidInputKeyError(self, use_tfrt):\n     base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n     args = self.parser.parse_args([\n         'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n-        'regress_x2_to_y3', '--input_exprs', 'x2=np.ones((3,1))'\n+        'regress_x2_to_y3', '--input_exprs', 'x2=[1,2,3]'\n     ] + (['--use_tfrt'] if use_tfrt else []))\n     with self.assertRaises(ValueError):\n       saved_model_cli.run(args)\n@@ -640,7 +603,7 @@ def testRunCommandInvalidSignature(self, use_tfrt):\n     base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n     args = self.parser.parse_args([\n         'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n-        'INVALID_SIGNATURE', '--input_exprs', 'x2=np.ones((3,1))'\n+        'INVALID_SIGNATURE', '--input_exprs', 'x2=[1,2,3]'\n     ] + (['--use_tfrt'] if use_tfrt else []))\n     with self.assertRaisesRegex(ValueError,\n                                 'Could not find signature \"INVALID_SIGNATURE\"'):", "file_path": "files/2022_5/223", "file_language": "py", "file_name": "tensorflow/python/tools/saved_model_cli_test.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class SavedModelCLITestCase(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(SavedModelCLITestCase, self).setUp()\n    if platform.system() == 'Windows':\n      self.skipTest('Skipping failing tests on Windows.')\n\n  def testShowCommandAll(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', base_path, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    # pylint: disable=line-too-long\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['classify_x2_to_y3']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x2:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y3:0\n  Method name is: tensorflow/serving/classify\n\nsignature_def['classify_x_to_y']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/classify\n\nsignature_def['regress_x2_to_y3']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x2:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y3:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['regress_x_to_y']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['regress_x_to_y2']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y2:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['x'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['y'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/predict\"\"\"\n    # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowAllWithFunctions(self):\n\n    class DummyModel(tracking.AutoTrackable):\n      \"\"\"Model with callable polymorphic functions specified.\"\"\"\n\n      @def_function.function\n      def func1(self, a, b, c):\n        if c:\n          return a + b\n        else:\n          return a * b\n\n      @def_function.function(input_signature=[\n          tensor_spec.TensorSpec(shape=(2, 2), dtype=dtypes.float32)\n      ])\n      def func2(self, x):\n        return x + 2\n\n      @def_function.function\n      def __call__(self, y, c=7):\n        return y + 2 * c\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = DummyModel()\n    # Call with specific values to create new polymorphic function traces.\n    dummy_model.func1(constant_op.constant(5), constant_op.constant(9), True)\n    dummy_model(constant_op.constant(5))\n    save.save(dummy_model, saved_model_dir)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', saved_model_dir, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['x'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 2)\n        name: serving_default_x:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['output_0'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 2)\n        name: PartitionedCall:0\n  Method name is: tensorflow/serving/predict\n\nConcrete Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          y: TensorSpec(shape=(), dtype=tf.int32, name='y')\n        Argument #2\n          DType: int\n          Value: 7\n\n  Function Name: 'func1'\n    Option #1\n      Callable with:\n        Argument #1\n          a: TensorSpec(shape=(), dtype=tf.int32, name='a')\n        Argument #2\n          b: TensorSpec(shape=(), dtype=tf.int32, name='b')\n        Argument #3\n          DType: bool\n          Value: True\n\n  Function Name: 'func2'\n    Option #1\n      Callable with:\n        Argument #1\n          x: TensorSpec(shape=(2, 2), dtype=tf.float32, name='x')\n\"\"\".strip()  # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowAllWithPureConcreteFunction(self):\n\n    class DummyModel(tracking.AutoTrackable):\n      \"\"\"Model with a callable concrete function.\"\"\"\n\n      def __init__(self):\n        function = def_function.function(\n            self.multiply,\n            input_signature=[\n                tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n                tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32)\n            ])\n        self.pure_concrete_function = function.get_concrete_function()\n        super(DummyModel, self).__init__()\n\n      def multiply(self, a, b):\n        return a * b\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = DummyModel()\n    save.save(dummy_model, saved_model_dir)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', saved_model_dir, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['a'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: serving_default_a:0\n    inputs['b'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: serving_default_b:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['output_0'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: PartitionedCall:0\n  Method name is: tensorflow/serving/predict\n\nConcrete Functions:\n  Function Name: 'pure_concrete_function'\n    Option #1\n      Callable with:\n        Argument #1\n          a: TensorSpec(shape=(), dtype=tf.float32, name='a')\n        Argument #2\n          b: TensorSpec(shape=(), dtype=tf.float32, name='b')\n\"\"\".strip()  # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandTags(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', base_path])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = 'The given SavedModel contains the following tag-sets:\\n\\'serve\\''\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandSignature(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(\n        ['show', '--dir', base_path, '--tag_set', 'serve'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_header = ('The given SavedModel MetaGraphDef contains SignatureDefs '\n                  'with the following keys:')\n    exp_start = 'SignatureDef key: '\n    exp_keys = [\n        '\"classify_x2_to_y3\"', '\"classify_x_to_y\"', '\"regress_x2_to_y3\"',\n        '\"regress_x_to_y\"', '\"regress_x_to_y2\"', '\"serving_default\"'\n    ]\n    # Order of signatures does not matter\n    self.assertMultiLineEqual(\n        output,\n        '\\n'.join([exp_header] + [exp_start + exp_key for exp_key in exp_keys]))\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandErrorNoTagSet(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(\n        ['show', '--dir', base_path, '--tag_set', 'badtagset'])\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.show(args)\n\n  def testShowCommandInputsOutputs(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args([\n        'show', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default'\n    ])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    expected_output = (\n        'The given SavedModel SignatureDef contains the following input(s):\\n'\n        '  inputs[\\'x\\'] tensor_info:\\n'\n        '      dtype: DT_FLOAT\\n      shape: (-1, 1)\\n      name: x:0\\n'\n        'The given SavedModel SignatureDef contains the following output(s):\\n'\n        '  outputs[\\'y\\'] tensor_info:\\n'\n        '      dtype: DT_FLOAT\\n      shape: (-1, 1)\\n      name: y:0\\n'\n        'Method name is: tensorflow/serving/predict')\n    self.assertEqual(output, expected_output)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testPrintREFTypeTensor(self):\n    ref_tensor_info = meta_graph_pb2.TensorInfo()\n    ref_tensor_info.dtype = types_pb2.DT_FLOAT_REF\n    with captured_output() as (out, err):\n      saved_model_cli._print_tensor_info(ref_tensor_info)\n    self.assertTrue('DT_FLOAT_REF' in out.getvalue().strip())\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testInputPreProcessFormats(self):\n    input_str = 'input1=/path/file.txt[ab3];input2=file2'\n    input_expr_str = 'input3=np.zeros([2,2]);input4=[4,5]'\n    input_dict = saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_expr_dict = saved_model_cli.preprocess_input_exprs_arg_string(\n        input_expr_str, safe=False)\n    self.assertTrue(input_dict['input1'] == ('/path/file.txt', 'ab3'))\n    self.assertTrue(input_dict['input2'] == ('file2', None))\n    print(input_expr_dict['input3'])\n    self.assertAllClose(input_expr_dict['input3'], np.zeros([2, 2]))\n    self.assertAllClose(input_expr_dict['input4'], [4, 5])\n    self.assertTrue(len(input_dict) == 2)\n    self.assertTrue(len(input_expr_dict) == 2)\n\n  def testInputPreProcessExamplesWithStrAndBytes(self):\n    input_examples_str = 'inputs=[{\"text\":[\"foo\"], \"bytes\":[b\"bar\"]}]'\n    input_dict = saved_model_cli.preprocess_input_examples_arg_string(\n        input_examples_str)\n    feature = example_pb2.Example.FromString(input_dict['inputs'][0])\n    self.assertProtoEquals(\n        \"\"\"\n          features {\n            feature {\n              key: \"bytes\"\n              value {\n                bytes_list {\n                  value: \"bar\"\n                }\n              }\n            }\n            feature {\n              key: \"text\"\n              value {\n                bytes_list {\n                  value: \"foo\"\n                }\n              }\n            }\n          }\n    \"\"\", feature)\n\n  def testInputPreprocessExampleWithCodeInjection(self):\n    input_examples_str = 'inputs=os.system(\"echo hacked\")'\n    with self.assertRaisesRegex(RuntimeError, 'not a valid python literal.'):\n      saved_model_cli.preprocess_input_examples_arg_string(input_examples_str)\n\n  def testInputPreProcessFileNames(self):\n    input_str = (r'inputx=C:\\Program Files\\data.npz[v:0];'\n                 r'input:0=c:\\PROGRA~1\\data.npy')\n    input_dict = saved_model_cli.preprocess_inputs_arg_string(input_str)\n    self.assertTrue(input_dict['inputx'] == (r'C:\\Program Files\\data.npz',\n                                             'v:0'))\n    self.assertTrue(input_dict['input:0'] == (r'c:\\PROGRA~1\\data.npy', None))\n\n  def testInputPreProcessErrorBadFormat(self):\n    input_str = 'inputx=file[[v1]v2'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_str = 'inputx:file'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_str = 'inputx:np.zeros((5))'\n    with self.assertRaisesRegex(RuntimeError, 'format is incorrect'):\n      saved_model_cli.preprocess_input_exprs_arg_string(input_str, safe=False)\n\n  def testInputParserNPY(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(6)).reshape(2, 3)\n    input0_path = os.path.join(test.get_temp_dir(), 'input0.npy')\n    input1_path = os.path.join(test.get_temp_dir(), 'input1.npy')\n    np.save(input0_path, x0)\n    np.save(input1_path, x1)\n    input_str = 'x0=' + input0_path + '[x0];x1=' + input1_path\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x0'] == x0))\n    self.assertTrue(np.all(feed_dict['x1'] == x1))\n\n  def testInputParserNPZ(self):\n    x0 = np.array([[1], [2]])\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0)\n    input_str = 'x=' + input_path + '[a];y=' + input_path\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x'] == x0))\n    self.assertTrue(np.all(feed_dict['y'] == x0))\n\n  def testInputParserPickle(self):\n    pkl0 = {'a': 5, 'b': np.array(range(4))}\n    pkl1 = np.array([1])\n    pkl2 = np.array([[1], [3]])\n    input_path0 = os.path.join(test.get_temp_dir(), 'pickle0.pkl')\n    input_path1 = os.path.join(test.get_temp_dir(), 'pickle1.pkl')\n    input_path2 = os.path.join(test.get_temp_dir(), 'pickle2.pkl')\n    with open(input_path0, 'wb') as f:\n      pickle.dump(pkl0, f)\n    with open(input_path1, 'wb') as f:\n      pickle.dump(pkl1, f)\n    with open(input_path2, 'wb') as f:\n      pickle.dump(pkl2, f)\n    input_str = 'x=' + input_path0 + '[b];y=' + input_path1 + '[c];'\n    input_str += 'z=' + input_path2\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x'] == pkl0['b']))\n    self.assertTrue(np.all(feed_dict['y'] == pkl1))\n    self.assertTrue(np.all(feed_dict['z'] == pkl2))\n\n  def testInputParserPythonExpression(self):\n    x1 = np.ones([2, 10])\n    x2 = np.array([[1], [2], [3]])\n    x3 = np.mgrid[0:5, 0:5]\n    x4 = [[3], [4]]\n    input_expr_str = ('x1=np.ones([2,10]);x2=np.array([[1],[2],[3]]);'\n                      'x3=np.mgrid[0:5,0:5];x4=[[3],[4]]')\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        '', input_expr_str, '')\n    self.assertTrue(np.all(feed_dict['x1'] == x1))\n    self.assertTrue(np.all(feed_dict['x2'] == x2))\n    self.assertTrue(np.all(feed_dict['x3'] == x3))\n    self.assertTrue(np.all(feed_dict['x4'] == x4))\n\n  def testInputParserBoth(self):\n    x0 = np.array([[1], [2]])\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0)\n    x1 = np.ones([2, 10])\n    input_str = 'x0=' + input_path + '[a]'\n    input_expr_str = 'x1=np.ones([2,10])'\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, input_expr_str, '')\n    self.assertTrue(np.all(feed_dict['x0'] == x0))\n    self.assertTrue(np.all(feed_dict['x1'] == x1))\n\n  def testInputParserBothDuplicate(self):\n    x0 = np.array([[1], [2]])\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0)\n    x1 = np.ones([2, 10])\n    input_str = 'x0=' + input_path + '[a]'\n    input_expr_str = 'x0=np.ones([2,10])'\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, input_expr_str, '')\n    self.assertTrue(np.all(feed_dict['x0'] == x1))\n\n  def testInputParserErrorNoName(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(5))\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0, b=x1)\n    input_str = 'x=' + input_path\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.load_inputs_from_input_arg_string(input_str, '', '')\n\n  def testInputParserErrorWrongName(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(5))\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0, b=x1)\n    input_str = 'x=' + input_path + '[c]'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.load_inputs_from_input_arg_string(input_str, '', '')\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamples(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(),\n                              'new_dir' + ('tfrt' if use_tfrt else ''))\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples',\n        'inputs=[{\"x\":[8.0],\"x2\":[5.0]}, {\"x\":[4.0],\"x2\":[3.0]}]', '--outdir',\n        output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(os.path.join(output_dir, 'outputs.npy'))\n    y_expected = np.array([[6.0], [4.0]])\n    self.assertAllEqual(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandExistingOutdir(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(), 'testRunCommand_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'outputs.npy')\n    if os.path.exists(output_file):\n      os.remove(output_file)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x2_to_y3', '--inputs', 'inputs=' + input_path +\n        '[x0]', '--outdir',\n        test.get_temp_dir()\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(output_file)\n    y_expected = np.array([[3.5], [4.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandNewOutdir(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandNewOutdir_inputs.npz')\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    if os.path.isdir(output_dir):\n      shutil.rmtree(output_dir)\n    np.savez(input_path, x0=x, x1=x_notused)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path +\n        '[x0]', '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(os.path.join(output_dir, 'y.npy'))\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandOutOverwrite(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandOutOverwrite_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'y.npy')\n    open(output_file, 'a').close()\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path + '[x0]', '--outdir',\n        test.get_temp_dir(), '--overwrite'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(output_file)\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInvalidInputKeyError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x2_to_y3', '--input_exprs', 'x2=np.ones((3,1))'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(ValueError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInvalidSignature(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'INVALID_SIGNATURE', '--input_exprs', 'x2=np.ones((3,1))'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError,\n                                'Could not find signature \"INVALID_SIGNATURE\"'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesNotListError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs={\"x\":8.0,\"x2\":5.0}',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'must be a list'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesFeatureValueNotListError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs=[{\"x\":8.0,\"x2\":5.0}]',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'feature value must be a list'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesFeatureBadType(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs=[{\"x\":[[1],[2]]}]',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'is not supported'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandOutputFileExistError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandOutOverwrite_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'y.npy')\n    open(output_file, 'a').close()\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path + '[x0]', '--outdir',\n        test.get_temp_dir()\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputNotGivenError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(AttributeError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandWithDebuggerEnabled(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandNewOutdir_inputs.npz')\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    if os.path.isdir(output_dir):\n      shutil.rmtree(output_dir)\n    np.savez(input_path, x0=x, x1=x_notused)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path +\n        '[x0]', '--outdir', output_dir, '--tf_debug'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n\n    def fake_wrapper_session(sess):\n      return sess\n\n    with test.mock.patch.object(\n        local_cli_wrapper,\n        'LocalCLIDebugWrapperSession',\n        side_effect=fake_wrapper_session,\n        autospec=True) as fake:\n      saved_model_cli.run(args)\n      fake.assert_called_with(test.mock.ANY)\n\n    y_actual = np.load(os.path.join(output_dir, 'y.npy'))\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  def testScanCommand(self):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args(['scan', '--dir', base_path])\n    with captured_output() as (out, _):\n      saved_model_cli.scan(args)\n    output = out.getvalue().strip()\n    self.assertTrue('does not contain denylisted ops' in output)\n\n  def testScanCommandFoundDenylistedOp(self):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args(\n        ['scan', '--dir', base_path, '--tag_set', 'serve'])\n    op_denylist = saved_model_cli._OP_DENYLIST\n    saved_model_cli._OP_DENYLIST = set(['VariableV2'])\n    with captured_output() as (out, _):\n      saved_model_cli.scan(args)\n    saved_model_cli._OP_DENYLIST = op_denylist\n    output = out.getvalue().strip()\n    self.assertTrue('\\'VariableV2\\'' in output)\n\n  def testAOTCompileCPUWrongSignatureDefKey(self):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir')\n    args = self.parser.parse_args([\n        'aot_compile_cpu', '--dir', base_path, '--tag_set', 'serve',\n        '--output_prefix', output_dir, '--cpp_class', 'Compiled',\n        '--signature_def_key', 'MISSING'\n    ])\n    with self.assertRaisesRegex(ValueError, 'Unable to find signature_def'):\n      saved_model_cli.aot_compile_cpu(args)\n\n  class AOTCompileDummyModel(tracking.AutoTrackable):\n    \"\"\"Model compatible with XLA compilation.\"\"\"\n\n    def __init__(self):\n      self.var = variables.Variable(1.0, name='my_var')\n      self.write_var = variables.Variable(1.0, name='write_var')\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=(2, 2), dtype=dtypes.float32),\n        # Test unused inputs.\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func2(self, x, y):\n      del y\n      return {'res': x + self.var}\n\n    @def_function.function(input_signature=[\n        # Test large inputs.\n        tensor_spec.TensorSpec(shape=(2048, 16), dtype=dtypes.float32),\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func3(self, x, y):\n      del y\n      return {'res': x + self.var}\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func_write(self, x, y):\n      del y\n      self.write_var.assign(x + self.var)\n      return {'res': self.write_var}\n\n  @parameterized.named_parameters(\n      ('VariablesToFeedNone', '', 'func2', None),\n      ('VariablesToFeedNoneTargetAarch64Linux', '', 'func2',\n       'aarch64-none-linux-gnu'),\n      ('VariablesToFeedNoneTargetAarch64Android', '', 'func2',\n       'aarch64-none-android'),\n      ('VariablesToFeedAll', 'all', 'func2', None),\n      ('VariablesToFeedMyVar', 'my_var', 'func2', None),\n      ('VariablesToFeedNoneLargeConstant', '', 'func3', None),\n      ('WriteToWriteVar', 'all', 'func_write', None),\n  )\n  def testAOTCompileCPUFreezesAndCompiles(self, variables_to_feed, func,\n                                          target_triple):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = self.AOTCompileDummyModel()\n    func = getattr(dummy_model, func)\n    with self.cached_session():\n      self.evaluate(dummy_model.var.initializer)\n      self.evaluate(dummy_model.write_var.initializer)\n      save.save(dummy_model, saved_model_dir, signatures={'func': func})\n\n    self.parser = saved_model_cli.create_parser()\n    output_prefix = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir/out')\n    args = [  # Use the default seving signature_key.\n        'aot_compile_cpu', '--dir', saved_model_dir, '--tag_set', 'serve',\n        '--signature_def_key', 'func', '--output_prefix', output_prefix,\n        '--variables_to_feed', variables_to_feed, '--cpp_class', 'Generated'\n    ]\n    if target_triple:\n      args.extend(['--target_triple', target_triple])\n    args = self.parser.parse_args(args)\n    with test.mock.patch.object(logging, 'warn') as captured_warn:\n      saved_model_cli.aot_compile_cpu(args)\n    self.assertRegex(\n        str(captured_warn.call_args),\n        'Signature input key \\'y\\'.*has been pruned while freezing the graph.')\n    self.assertTrue(file_io.file_exists('{}.o'.format(output_prefix)))\n    self.assertTrue(file_io.file_exists('{}.h'.format(output_prefix)))\n    self.assertTrue(file_io.file_exists('{}_metadata.o'.format(output_prefix)))\n    self.assertTrue(\n        file_io.file_exists('{}_makefile.inc'.format(output_prefix)))\n    header_contents = file_io.read_file_to_string('{}.h'.format(output_prefix))\n    self.assertIn('class Generated', header_contents)\n    self.assertIn('arg_feed_x_data', header_contents)\n    self.assertIn('result_fetch_res_data', header_contents)\n    # arg_y got filtered out as it's not used by the output.\n    self.assertNotIn('arg_feed_y_data', header_contents)\n    if variables_to_feed:\n      # Read-only-variables' setters preserve constness.\n      self.assertIn('set_var_param_my_var_data(const float', header_contents)\n      self.assertNotIn('set_var_param_my_var_data(float', header_contents)\n    if func == dummy_model.func_write:\n      # Writeable variables setters do not preserve constness.\n      self.assertIn('set_var_param_write_var_data(float', header_contents)\n      self.assertNotIn('set_var_param_write_var_data(const float',\n                       header_contents)\n\n    makefile_contents = file_io.read_file_to_string(\n        '{}_makefile.inc'.format(output_prefix))\n    self.assertIn('-D_GLIBCXX_USE_CXX11_ABI=', makefile_contents)\n\n  def testFreezeModel(self):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    variables_to_feed = 'all'\n    func = 'func2'\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = self.AOTCompileDummyModel()\n    func = getattr(dummy_model, func)\n    with self.cached_session():\n      self.evaluate(dummy_model.var.initializer)\n      self.evaluate(dummy_model.write_var.initializer)\n      save.save(dummy_model, saved_model_dir, signatures={'func': func})\n\n    self.parser = saved_model_cli.create_parser()\n    output_prefix = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir/out')\n    args = [  # Use the default seving signature_key.\n        'freeze_model', '--dir', saved_model_dir, '--tag_set', 'serve',\n        '--signature_def_key', 'func', '--output_prefix', output_prefix,\n        '--variables_to_feed', variables_to_feed\n    ]\n    args = self.parser.parse_args(args)\n    with test.mock.patch.object(logging, 'warn'):\n      saved_model_cli.freeze_model(args)\n    self.assertTrue(\n        file_io.file_exists(os.path.join(output_prefix, 'frozen_graph.pb')))\n    self.assertTrue(\n        file_io.file_exists(os.path.join(output_prefix, 'config.pbtxt')))", "target": 0}], "function_after": [{"function": "class SavedModelCLITestCase(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(SavedModelCLITestCase, self).setUp()\n    if platform.system() == 'Windows':\n      self.skipTest('Skipping failing tests on Windows.')\n\n  def testShowCommandAll(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', base_path, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    # pylint: disable=line-too-long\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['classify_x2_to_y3']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x2:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y3:0\n  Method name is: tensorflow/serving/classify\n\nsignature_def['classify_x_to_y']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/classify\n\nsignature_def['regress_x2_to_y3']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x2:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y3:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['regress_x_to_y']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['regress_x_to_y2']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: tf_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['outputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y2:0\n  Method name is: tensorflow/serving/regress\n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['x'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['y'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/predict\"\"\"\n    # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowAllWithFunctions(self):\n\n    class DummyModel(tracking.AutoTrackable):\n      \"\"\"Model with callable polymorphic functions specified.\"\"\"\n\n      @def_function.function\n      def func1(self, a, b, c):\n        if c:\n          return a + b\n        else:\n          return a * b\n\n      @def_function.function(input_signature=[\n          tensor_spec.TensorSpec(shape=(2, 2), dtype=dtypes.float32)\n      ])\n      def func2(self, x):\n        return x + 2\n\n      @def_function.function\n      def __call__(self, y, c=7):\n        return y + 2 * c\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = DummyModel()\n    # Call with specific values to create new polymorphic function traces.\n    dummy_model.func1(constant_op.constant(5), constant_op.constant(9), True)\n    dummy_model(constant_op.constant(5))\n    save.save(dummy_model, saved_model_dir)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', saved_model_dir, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['x'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 2)\n        name: serving_default_x:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['output_0'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 2)\n        name: PartitionedCall:0\n  Method name is: tensorflow/serving/predict\n\nConcrete Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          y: TensorSpec(shape=(), dtype=tf.int32, name='y')\n        Argument #2\n          DType: int\n          Value: 7\n\n  Function Name: 'func1'\n    Option #1\n      Callable with:\n        Argument #1\n          a: TensorSpec(shape=(), dtype=tf.int32, name='a')\n        Argument #2\n          b: TensorSpec(shape=(), dtype=tf.int32, name='b')\n        Argument #3\n          DType: bool\n          Value: True\n\n  Function Name: 'func2'\n    Option #1\n      Callable with:\n        Argument #1\n          x: TensorSpec(shape=(2, 2), dtype=tf.float32, name='x')\n\"\"\".strip()  # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowAllWithPureConcreteFunction(self):\n\n    class DummyModel(tracking.AutoTrackable):\n      \"\"\"Model with a callable concrete function.\"\"\"\n\n      def __init__(self):\n        function = def_function.function(\n            self.multiply,\n            input_signature=[\n                tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n                tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32)\n            ])\n        self.pure_concrete_function = function.get_concrete_function()\n        super(DummyModel, self).__init__()\n\n      def multiply(self, a, b):\n        return a * b\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = DummyModel()\n    save.save(dummy_model, saved_model_dir)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', saved_model_dir, '--all'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = \"\"\"MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['a'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: serving_default_a:0\n    inputs['b'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: serving_default_b:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['output_0'] tensor_info:\n        dtype: DT_FLOAT\n        shape: ()\n        name: PartitionedCall:0\n  Method name is: tensorflow/serving/predict\n\nConcrete Functions:\n  Function Name: 'pure_concrete_function'\n    Option #1\n      Callable with:\n        Argument #1\n          a: TensorSpec(shape=(), dtype=tf.float32, name='a')\n        Argument #2\n          b: TensorSpec(shape=(), dtype=tf.float32, name='b')\n\"\"\".strip()  # pylint: enable=line-too-long\n    self.maxDiff = None  # Produce a useful error msg if the comparison fails\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandTags(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(['show', '--dir', base_path])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_out = 'The given SavedModel contains the following tag-sets:\\n\\'serve\\''\n    self.assertMultiLineEqual(output, exp_out)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandSignature(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(\n        ['show', '--dir', base_path, '--tag_set', 'serve'])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    exp_header = ('The given SavedModel MetaGraphDef contains SignatureDefs '\n                  'with the following keys:')\n    exp_start = 'SignatureDef key: '\n    exp_keys = [\n        '\"classify_x2_to_y3\"', '\"classify_x_to_y\"', '\"regress_x2_to_y3\"',\n        '\"regress_x_to_y\"', '\"regress_x_to_y2\"', '\"serving_default\"'\n    ]\n    # Order of signatures does not matter\n    self.assertMultiLineEqual(\n        output,\n        '\\n'.join([exp_header] + [exp_start + exp_key for exp_key in exp_keys]))\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testShowCommandErrorNoTagSet(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args(\n        ['show', '--dir', base_path, '--tag_set', 'badtagset'])\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.show(args)\n\n  def testShowCommandInputsOutputs(self):\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    self.parser = saved_model_cli.create_parser()\n    args = self.parser.parse_args([\n        'show', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default'\n    ])\n    with captured_output() as (out, err):\n      saved_model_cli.show(args)\n    output = out.getvalue().strip()\n    expected_output = (\n        'The given SavedModel SignatureDef contains the following input(s):\\n'\n        '  inputs[\\'x\\'] tensor_info:\\n'\n        '      dtype: DT_FLOAT\\n      shape: (-1, 1)\\n      name: x:0\\n'\n        'The given SavedModel SignatureDef contains the following output(s):\\n'\n        '  outputs[\\'y\\'] tensor_info:\\n'\n        '      dtype: DT_FLOAT\\n      shape: (-1, 1)\\n      name: y:0\\n'\n        'Method name is: tensorflow/serving/predict')\n    self.assertEqual(output, expected_output)\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testPrintREFTypeTensor(self):\n    ref_tensor_info = meta_graph_pb2.TensorInfo()\n    ref_tensor_info.dtype = types_pb2.DT_FLOAT_REF\n    with captured_output() as (out, err):\n      saved_model_cli._print_tensor_info(ref_tensor_info)\n    self.assertTrue('DT_FLOAT_REF' in out.getvalue().strip())\n    self.assertEqual(err.getvalue().strip(), '')\n\n  def testInputPreProcessFormats(self):\n    input_str = 'input1=/path/file.txt[ab3];input2=file2'\n    input_expr_str = 'input3=np.zeros([2,2]);input4=[4,5]'\n    input_dict = saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_expr_dict = saved_model_cli.preprocess_input_exprs_arg_string(\n        input_expr_str, safe=False)\n    self.assertTrue(input_dict['input1'] == ('/path/file.txt', 'ab3'))\n    self.assertTrue(input_dict['input2'] == ('file2', None))\n    print(input_expr_dict['input3'])\n    self.assertAllClose(input_expr_dict['input3'], np.zeros([2, 2]))\n    self.assertAllClose(input_expr_dict['input4'], [4, 5])\n    self.assertTrue(len(input_dict) == 2)\n    self.assertTrue(len(input_expr_dict) == 2)\n\n  def testInputPreProcessExamplesWithStrAndBytes(self):\n    input_examples_str = 'inputs=[{\"text\":[\"foo\"], \"bytes\":[b\"bar\"]}]'\n    input_dict = saved_model_cli.preprocess_input_examples_arg_string(\n        input_examples_str)\n    feature = example_pb2.Example.FromString(input_dict['inputs'][0])\n    self.assertProtoEquals(\n        \"\"\"\n          features {\n            feature {\n              key: \"bytes\"\n              value {\n                bytes_list {\n                  value: \"bar\"\n                }\n              }\n            }\n            feature {\n              key: \"text\"\n              value {\n                bytes_list {\n                  value: \"foo\"\n                }\n              }\n            }\n          }\n    \"\"\", feature)\n\n  def testInputPreprocessExampleWithCodeInjection(self):\n    input_examples_str = 'inputs=os.system(\"echo hacked\")'\n    with self.assertRaisesRegex(RuntimeError, 'not a valid python literal.'):\n      saved_model_cli.preprocess_input_examples_arg_string(input_examples_str)\n\n  def testInputPreProcessFileNames(self):\n    input_str = (r'inputx=C:\\Program Files\\data.npz[v:0];'\n                 r'input:0=c:\\PROGRA~1\\data.npy')\n    input_dict = saved_model_cli.preprocess_inputs_arg_string(input_str)\n    self.assertTrue(input_dict['inputx'] == (r'C:\\Program Files\\data.npz',\n                                             'v:0'))\n    self.assertTrue(input_dict['input:0'] == (r'c:\\PROGRA~1\\data.npy', None))\n\n  def testInputPreProcessErrorBadFormat(self):\n    input_str = 'inputx=file[[v1]v2'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_str = 'inputx:file'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_str = 'inputx:np.zeros((5))'\n    with self.assertRaisesRegex(RuntimeError, 'format is incorrect'):\n      saved_model_cli.preprocess_input_exprs_arg_string(input_str, safe=False)\n\n  def testInputParserNPY(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(6)).reshape(2, 3)\n    input0_path = os.path.join(test.get_temp_dir(), 'input0.npy')\n    input1_path = os.path.join(test.get_temp_dir(), 'input1.npy')\n    np.save(input0_path, x0)\n    np.save(input1_path, x1)\n    input_str = 'x0=' + input0_path + '[x0];x1=' + input1_path\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x0'] == x0))\n    self.assertTrue(np.all(feed_dict['x1'] == x1))\n\n  def testInputParserNPZ(self):\n    x0 = np.array([[1], [2]])\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0)\n    input_str = 'x=' + input_path + '[a];y=' + input_path\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x'] == x0))\n    self.assertTrue(np.all(feed_dict['y'] == x0))\n\n  def testInputParserPickle(self):\n    pkl0 = {'a': 5, 'b': np.array(range(4))}\n    pkl1 = np.array([1])\n    pkl2 = np.array([[1], [3]])\n    input_path0 = os.path.join(test.get_temp_dir(), 'pickle0.pkl')\n    input_path1 = os.path.join(test.get_temp_dir(), 'pickle1.pkl')\n    input_path2 = os.path.join(test.get_temp_dir(), 'pickle2.pkl')\n    with open(input_path0, 'wb') as f:\n      pickle.dump(pkl0, f)\n    with open(input_path1, 'wb') as f:\n      pickle.dump(pkl1, f)\n    with open(input_path2, 'wb') as f:\n      pickle.dump(pkl2, f)\n    input_str = 'x=' + input_path0 + '[b];y=' + input_path1 + '[c];'\n    input_str += 'z=' + input_path2\n    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n        input_str, '', '')\n    self.assertTrue(np.all(feed_dict['x'] == pkl0['b']))\n    self.assertTrue(np.all(feed_dict['y'] == pkl1))\n    self.assertTrue(np.all(feed_dict['z'] == pkl2))\n\n  def testInputParserErrorNoName(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(5))\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0, b=x1)\n    input_str = 'x=' + input_path\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.load_inputs_from_input_arg_string(input_str, '', '')\n\n  def testInputParserErrorWrongName(self):\n    x0 = np.array([[1], [2]])\n    x1 = np.array(range(5))\n    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n    np.savez(input_path, a=x0, b=x1)\n    input_str = 'x=' + input_path + '[c]'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.load_inputs_from_input_arg_string(input_str, '', '')\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamples(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(),\n                              'new_dir' + ('tfrt' if use_tfrt else ''))\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples',\n        'inputs=[{\"x\":[8.0],\"x2\":[5.0]}, {\"x\":[4.0],\"x2\":[3.0]}]', '--outdir',\n        output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(os.path.join(output_dir, 'outputs.npy'))\n    y_expected = np.array([[6.0], [4.0]])\n    self.assertAllEqual(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandExistingOutdir(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(), 'testRunCommand_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'outputs.npy')\n    if os.path.exists(output_file):\n      os.remove(output_file)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x2_to_y3', '--inputs', 'inputs=' + input_path +\n        '[x0]', '--outdir',\n        test.get_temp_dir()\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(output_file)\n    y_expected = np.array([[3.5], [4.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandNewOutdir(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandNewOutdir_inputs.npz')\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    if os.path.isdir(output_dir):\n      shutil.rmtree(output_dir)\n    np.savez(input_path, x0=x, x1=x_notused)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path +\n        '[x0]', '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(os.path.join(output_dir, 'y.npy'))\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandOutOverwrite(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandOutOverwrite_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'y.npy')\n    open(output_file, 'a').close()\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path + '[x0]', '--outdir',\n        test.get_temp_dir(), '--overwrite'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    saved_model_cli.run(args)\n    y_actual = np.load(output_file)\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInvalidInputKeyError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x2_to_y3', '--input_exprs', 'x2=[1,2,3]'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(ValueError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInvalidSignature(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'INVALID_SIGNATURE', '--input_exprs', 'x2=[1,2,3]'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError,\n                                'Could not find signature \"INVALID_SIGNATURE\"'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesNotListError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs={\"x\":8.0,\"x2\":5.0}',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'must be a list'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesFeatureValueNotListError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs=[{\"x\":8.0,\"x2\":5.0}]',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'feature value must be a list'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputExamplesFeatureBadType(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'regress_x_to_y', '--input_examples', 'inputs=[{\"x\":[[1],[2]]}]',\n        '--outdir', output_dir\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaisesRegex(ValueError, 'is not supported'):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandOutputFileExistError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandOutOverwrite_inputs.npz')\n    np.savez(input_path, x0=x, x1=x_notused)\n    output_file = os.path.join(test.get_temp_dir(), 'y.npy')\n    open(output_file, 'a').close()\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path + '[x0]', '--outdir',\n        test.get_temp_dir()\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandInputNotGivenError(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n    with self.assertRaises(AttributeError):\n      saved_model_cli.run(args)\n\n  @parameterized.named_parameters(('non_tfrt', False))\n  def testRunCommandWithDebuggerEnabled(self, use_tfrt):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    x = np.array([[1], [2]])\n    x_notused = np.zeros((6, 3))\n    input_path = os.path.join(test.get_temp_dir(),\n                              'testRunCommandNewOutdir_inputs.npz')\n    output_dir = os.path.join(test.get_temp_dir(), 'new_dir')\n    if os.path.isdir(output_dir):\n      shutil.rmtree(output_dir)\n    np.savez(input_path, x0=x, x1=x_notused)\n    args = self.parser.parse_args([\n        'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n        'serving_default', '--inputs', 'x=' + input_path +\n        '[x0]', '--outdir', output_dir, '--tf_debug'\n    ] + (['--use_tfrt'] if use_tfrt else []))\n\n    def fake_wrapper_session(sess):\n      return sess\n\n    with test.mock.patch.object(\n        local_cli_wrapper,\n        'LocalCLIDebugWrapperSession',\n        side_effect=fake_wrapper_session,\n        autospec=True) as fake:\n      saved_model_cli.run(args)\n      fake.assert_called_with(test.mock.ANY)\n\n    y_actual = np.load(os.path.join(output_dir, 'y.npy'))\n    y_expected = np.array([[2.5], [3.0]])\n    self.assertAllClose(y_expected, y_actual)\n\n  def testScanCommand(self):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args(['scan', '--dir', base_path])\n    with captured_output() as (out, _):\n      saved_model_cli.scan(args)\n    output = out.getvalue().strip()\n    self.assertTrue('does not contain denylisted ops' in output)\n\n  def testScanCommandFoundDenylistedOp(self):\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    args = self.parser.parse_args(\n        ['scan', '--dir', base_path, '--tag_set', 'serve'])\n    op_denylist = saved_model_cli._OP_DENYLIST\n    saved_model_cli._OP_DENYLIST = set(['VariableV2'])\n    with captured_output() as (out, _):\n      saved_model_cli.scan(args)\n    saved_model_cli._OP_DENYLIST = op_denylist\n    output = out.getvalue().strip()\n    self.assertTrue('\\'VariableV2\\'' in output)\n\n  def testAOTCompileCPUWrongSignatureDefKey(self):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    self.parser = saved_model_cli.create_parser()\n    base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n    output_dir = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir')\n    args = self.parser.parse_args([\n        'aot_compile_cpu', '--dir', base_path, '--tag_set', 'serve',\n        '--output_prefix', output_dir, '--cpp_class', 'Compiled',\n        '--signature_def_key', 'MISSING'\n    ])\n    with self.assertRaisesRegex(ValueError, 'Unable to find signature_def'):\n      saved_model_cli.aot_compile_cpu(args)\n\n  class AOTCompileDummyModel(tracking.AutoTrackable):\n    \"\"\"Model compatible with XLA compilation.\"\"\"\n\n    def __init__(self):\n      self.var = variables.Variable(1.0, name='my_var')\n      self.write_var = variables.Variable(1.0, name='write_var')\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=(2, 2), dtype=dtypes.float32),\n        # Test unused inputs.\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func2(self, x, y):\n      del y\n      return {'res': x + self.var}\n\n    @def_function.function(input_signature=[\n        # Test large inputs.\n        tensor_spec.TensorSpec(shape=(2048, 16), dtype=dtypes.float32),\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func3(self, x, y):\n      del y\n      return {'res': x + self.var}\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n        tensor_spec.TensorSpec(shape=(), dtype=dtypes.float32),\n    ])\n    def func_write(self, x, y):\n      del y\n      self.write_var.assign(x + self.var)\n      return {'res': self.write_var}\n\n  @parameterized.named_parameters(\n      ('VariablesToFeedNone', '', 'func2', None),\n      ('VariablesToFeedNoneTargetAarch64Linux', '', 'func2',\n       'aarch64-none-linux-gnu'),\n      ('VariablesToFeedNoneTargetAarch64Android', '', 'func2',\n       'aarch64-none-android'),\n      ('VariablesToFeedAll', 'all', 'func2', None),\n      ('VariablesToFeedMyVar', 'my_var', 'func2', None),\n      ('VariablesToFeedNoneLargeConstant', '', 'func3', None),\n      ('WriteToWriteVar', 'all', 'func_write', None),\n  )\n  def testAOTCompileCPUFreezesAndCompiles(self, variables_to_feed, func,\n                                          target_triple):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = self.AOTCompileDummyModel()\n    func = getattr(dummy_model, func)\n    with self.cached_session():\n      self.evaluate(dummy_model.var.initializer)\n      self.evaluate(dummy_model.write_var.initializer)\n      save.save(dummy_model, saved_model_dir, signatures={'func': func})\n\n    self.parser = saved_model_cli.create_parser()\n    output_prefix = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir/out')\n    args = [  # Use the default seving signature_key.\n        'aot_compile_cpu', '--dir', saved_model_dir, '--tag_set', 'serve',\n        '--signature_def_key', 'func', '--output_prefix', output_prefix,\n        '--variables_to_feed', variables_to_feed, '--cpp_class', 'Generated'\n    ]\n    if target_triple:\n      args.extend(['--target_triple', target_triple])\n    args = self.parser.parse_args(args)\n    with test.mock.patch.object(logging, 'warn') as captured_warn:\n      saved_model_cli.aot_compile_cpu(args)\n    self.assertRegex(\n        str(captured_warn.call_args),\n        'Signature input key \\'y\\'.*has been pruned while freezing the graph.')\n    self.assertTrue(file_io.file_exists('{}.o'.format(output_prefix)))\n    self.assertTrue(file_io.file_exists('{}.h'.format(output_prefix)))\n    self.assertTrue(file_io.file_exists('{}_metadata.o'.format(output_prefix)))\n    self.assertTrue(\n        file_io.file_exists('{}_makefile.inc'.format(output_prefix)))\n    header_contents = file_io.read_file_to_string('{}.h'.format(output_prefix))\n    self.assertIn('class Generated', header_contents)\n    self.assertIn('arg_feed_x_data', header_contents)\n    self.assertIn('result_fetch_res_data', header_contents)\n    # arg_y got filtered out as it's not used by the output.\n    self.assertNotIn('arg_feed_y_data', header_contents)\n    if variables_to_feed:\n      # Read-only-variables' setters preserve constness.\n      self.assertIn('set_var_param_my_var_data(const float', header_contents)\n      self.assertNotIn('set_var_param_my_var_data(float', header_contents)\n    if func == dummy_model.func_write:\n      # Writeable variables setters do not preserve constness.\n      self.assertIn('set_var_param_write_var_data(float', header_contents)\n      self.assertNotIn('set_var_param_write_var_data(const float',\n                       header_contents)\n\n    makefile_contents = file_io.read_file_to_string(\n        '{}_makefile.inc'.format(output_prefix))\n    self.assertIn('-D_GLIBCXX_USE_CXX11_ABI=', makefile_contents)\n\n  def testFreezeModel(self):\n    if not test.is_built_with_xla():\n      self.skipTest('Skipping test because XLA is not compiled in.')\n\n    variables_to_feed = 'all'\n    func = 'func2'\n    saved_model_dir = os.path.join(test.get_temp_dir(), 'dummy_model')\n    dummy_model = self.AOTCompileDummyModel()\n    func = getattr(dummy_model, func)\n    with self.cached_session():\n      self.evaluate(dummy_model.var.initializer)\n      self.evaluate(dummy_model.write_var.initializer)\n      save.save(dummy_model, saved_model_dir, signatures={'func': func})\n\n    self.parser = saved_model_cli.create_parser()\n    output_prefix = os.path.join(test.get_temp_dir(), 'aot_compile_cpu_dir/out')\n    args = [  # Use the default seving signature_key.\n        'freeze_model', '--dir', saved_model_dir, '--tag_set', 'serve',\n        '--signature_def_key', 'func', '--output_prefix', output_prefix,\n        '--variables_to_feed', variables_to_feed\n    ]\n    args = self.parser.parse_args(args)\n    with test.mock.patch.object(logging, 'warn'):\n      saved_model_cli.freeze_model(args)\n    self.assertTrue(\n        file_io.file_exists(os.path.join(output_prefix, 'frozen_graph.pb')))\n    self.assertTrue(\n        file_io.file_exists(os.path.join(output_prefix, 'config.pbtxt')))", "target": 0}]}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
