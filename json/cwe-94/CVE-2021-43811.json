{"index": 6965, "cve_id": "CVE-2021-43811", "cwe_id": ["CWE-94"], "cve_language": "Python", "cve_description": "Sockeye is an open-source sequence-to-sequence framework for Neural Machine Translation built on PyTorch. Sockeye uses YAML to store model and data configurations on disk. Versions below 2.3.24 use unsafe YAML loading, which can be made to execute arbitrary code embedded in config files. An attacker can add malicious code to the config file of a trained model and attempt to convince users to download and run it. If users run the model, the embedded code will run locally. The issue is fixed in version 2.3.24.", "cvss": "7.8", "publish_date": "December 8, 2021", "AV": "LOCAL", "AC": "LOCAL", "PR": "NONE", "UI": "REQUIRED", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "35dd717a80ae1f04128d79bd0bcf340e2e9d1427", "commit_message": "Use safe yaml loader (#964)\n\n* WIP: yaml fix.\r\n\r\n* CHANGELOG.", "commit_date": "2021-10-21T15:53:54Z", "project": "awslabs/sockeye", "url": "https://api.github.com/repos/awslabs/sockeye/commits/35dd717a80ae1f04128d79bd0bcf340e2e9d1427", "html_url": "https://github.com/awslabs/sockeye/commit/35dd717a80ae1f04128d79bd0bcf340e2e9d1427", "windows_before": [{"commit_id": "565cc71d42ef983eb17d07f3c7befce67c1331ce", "commit_date": "Tue Oct 19 17:31:45 2021 +0200", "commit_message": "Do not sort BIAS_STATE in beam search (#971)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/beam_search.py"]}, {"commit_id": "33eb25fe0f3dfa8477e42acf57606fa3ac015ea5", "commit_date": "Thu Sep 30 19:27:42 2021 +0200", "commit_message": "Convert classes to dataclasses (#969)", "files_name": ["sockeye/data_io.py", "sockeye/inference.py"]}, {"commit_id": "a0bc3f0108a488d8432c6d19d8662da4cd4e5479", "commit_date": "Thu Sep 23 14:50:13 2021 +0200", "commit_message": "data_io: make BucketBatchSize a dataclass (#966)", "files_name": ["sockeye/data_io.py"]}, {"commit_id": "4730ca8985bfdfc58673a20d372bbe041f526258", "commit_date": "Tue Sep 21 16:30:08 2021 +0200", "commit_message": "Fix: Vocabulary creation. (#965)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/arguments.py", "sockeye/vocab.py", "test/unit/test_data_io.py", "test/unit/test_vocab.py"]}, {"commit_id": "6a91906c936c12fc7a6e465139b0960aad8a0710", "commit_date": "Mon Sep 13 11:42:05 2021 +0200", "commit_message": "More publications citing sockeye (#963)", "files_name": ["README.md"]}, {"commit_id": "06fe1ee4557dfab8896b490b419f653f1195c88f", "commit_date": "Fri Sep 10 16:37:29 2021 +0200", "commit_message": "Parallelized prepare_data (#962)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/data_io.py", "sockeye/prepare_data.py", "sockeye/train.py", "sockeye/utils.py", "sockeye/vocab.py", "test/unit/test_data_io.py", "test/unit/test_utils.py", "test/unit/test_vocab.py"]}, {"commit_id": "4b6dd659f82454cc3a7e1002c0e61057f1d94c52", "commit_date": "Wed Sep 1 15:03:04 2021 +0200", "commit_message": "Add debug logging for restrict_lexicon lookups (#961)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/beam_search.py", "sockeye/lexicon.py"]}, {"commit_id": "74d9b799f070ee99141b4310ce3329f48b84358f", "commit_date": "Tue Aug 31 19:12:16 2021 +0200", "commit_message": "Update README.md with a new publication (#960)", "files_name": ["README.md"]}, {"commit_id": "94f3b74c1efb31e8a4c42079f72016e8e481ae92", "commit_date": "Sat Aug 28 10:34:12 2021 -0500", "commit_message": "Turn off autograd when training only the decoder (#959)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/model.py", "sockeye/train.py", "sockeye/utils.py", "test/integration/test_seq_copy_int.py"]}, {"commit_id": "a9c5fceb01a24d41648d723057df597066418d48", "commit_date": "Thu Aug 26 18:01:01 2021 +0200", "commit_message": "Update README.md with recent publications using Sockeye (#958)", "files_name": ["README.md"]}, {"commit_id": "21ee321eae55a75c983a1e9e58817bc9ccc4d547", "commit_date": "Wed Aug 11 18:27:16 2021 -0500", "commit_message": "Update Dockerfiles (#957)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye_contrib/docker/Dockerfile.cpu", "sockeye_contrib/docker/Dockerfile.gpu", "sockeye_contrib/docker/README.md", "sockeye_contrib/docker/build.py", "sockeye_contrib/docker/entrypoint.sh"]}, {"commit_id": "575121665702b269b50002d950112c8753f45d42", "commit_date": "Wed Jul 14 08:32:09 2021 -0400", "commit_message": "Minor typo in evaluate. (#954)", "files_name": ["sockeye/evaluate.py"]}, {"commit_id": "ef908e3c5751ef072b2554f327f8081e935d9731", "commit_date": "Sun May 30 22:04:06 2021 +0200", "commit_message": "Alternative, faster greedy search (#952)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/arguments.py", "sockeye/beam_search.py", "sockeye/inference.py", "sockeye/translate.py", "test/common.py", "test/integration/test_seq_copy_int.py", "test/system/test_seq_copy_sys.py", "test/unit/test_arguments.py", "test/unit/test_beam_search.py", "test/unit/test_inference.py"]}, {"commit_id": "ba8f849804e99c9fac3ff31ce4dac3758fcda95b", "commit_date": "Mon May 10 09:07:47 2021 -0500", "commit_message": "Add option --transformer-feed-forward-use-glu (#951)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/arguments.py", "sockeye/train.py", "sockeye/transformer.py", "test/unit/test_arguments.py", "test/unit/test_transformer.py"]}, {"commit_id": "feb30db31f36fde696f258bd881869e2c980b41d", "commit_date": "Thu May 6 06:30:01 2021 +0200", "commit_message": "Decoder class is now a complete HybridBlock (#950)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/decoder.py", "sockeye/model.py"]}, {"commit_id": "51539beccf69fa255ab5595282be94306762f7f2", "commit_date": "Wed Apr 7 14:00:29 2021 +0200", "commit_message": "Prevent empty references in test data when running sacrebleu, fix reading of outputs (#948)", "files_name": ["test/integration/test_seq_copy_int.py"]}, {"commit_id": "587eb7f33c6f79ae8b309dd257a94ecde69885cc", "commit_date": "Tue Apr 6 14:21:35 2021 +0200", "commit_message": "Update python-publish.yml to Python 3.7 (#949)", "files_name": [".github/workflows/python-publish.yml"]}, {"commit_id": "25e240b13f6079977e4ad4431ac2881b1036ab4f", "commit_date": "Tue Apr 6 12:48:10 2021 +0200", "commit_message": "Simplify mxnet nightly build action (#947)", "files_name": [".github/workflows/mxnet_nightly.yml", ".github/workflows/push_pr.yml"]}, {"commit_id": "a92030e42588cf5c729d96b3b5f8619364b19e48", "commit_date": "Mon Apr 5 20:42:54 2021 +0200", "commit_message": "Update to MXNet 1.8 & Python 3.7 (#945)", "files_name": [".github/workflows/mxnet_nightly.yml", ".github/workflows/push_pr.yml", "CHANGELOG.md", "docs/setup.md", "requirements/requirements.dev.txt", "requirements/requirements.docs.txt", "requirements/requirements.gpu-cu100.txt", "requirements/requirements.gpu-cu101.txt", "requirements/requirements.gpu-cu102.txt", "requirements/requirements.gpu-cu110.txt", "requirements/requirements.gpu-cu112.txt", "requirements/requirements.txt", "sockeye/__init__.py"]}, {"commit_id": "1586c9ec050e391bbdbaabf83d2f091271ae88e9", "commit_date": "Wed Mar 24 20:30:55 2021 +0100", "commit_message": "rename pylintrc to .pylintrc and move settings into .pylintrc (#946)", "files_name": [".github/workflows/push_pr.yml", ".pylintrc", "MANIFEST.in", "style-check.sh"]}, {"commit_id": "3e2b0171a3576fe5677ac0f868936b03f4ab4cd5", "commit_date": "Wed Mar 17 18:37:12 2021 +0100", "commit_message": "Added collection of nbest target factors into TranslatorOutput object (and json output) (#944)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/inference.py", "test/unit/test_output_handler.py"]}, {"commit_id": "2b527e81087d27bc0b6517ccde9ffedab91291f4", "commit_date": "Tue Mar 16 09:37:59 2021 +0100", "commit_message": "prepare_data now supports storage of CLI flags in yaml file (like sockeye_train) (#941)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/prepare_data.py"]}, {"commit_id": "795ea2746720adb8658b0f78eb475ba46b9c75ad", "commit_date": "Fri Mar 12 09:34:14 2021 +0100", "commit_message": "Adding an option to avoid generate <unk> during decoding (#936)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/arguments.py", "sockeye/beam_search.py", "sockeye/inference.py", "sockeye/translate.py", "test/system/test_seq_copy_sys.py", "test/unit/test_arguments.py", "test/unit/test_beam_search.py"]}, {"commit_id": "e8c77b975127a1245671770ad1b7ee95e76ac9a2", "commit_date": "Wed Mar 10 19:27:21 2021 +0100", "commit_message": "Add another publication (#939)", "files_name": ["README.md"]}, {"commit_id": "bd29357303438fa07e1a68734999368ed2698494", "commit_date": "Wed Mar 10 09:39:21 2021 +0100", "commit_message": "Added a few publications using Sockeye (#935)", "files_name": ["README.md"]}, {"commit_id": "552d8660ed769021d5cc9e247911ea9e0d9ccc59", "commit_date": "Tue Mar 9 17:35:02 2021 +0100", "commit_message": "Attempt to fix pylint error: sockeye/data_io.py:1834:15: E1102: self.shard_iter.next is not callable (not-callable) (#937)", "files_name": [".github/workflows/push_pr.yml", "sockeye/data_io.py", "style-check.sh"]}, {"commit_id": "c3870e38f0a446ae5bf3920d5872908d282444d4", "commit_date": "Fri Feb 5 11:25:14 2021 +0100", "commit_message": "Avoid circular import: move cleanup method to training.py (#932)", "files_name": ["sockeye/arguments.py", "sockeye/average.py", "sockeye/test_utils.py", "sockeye/training.py", "sockeye/utils.py", "test/unit/test_lexicon.py", "test/unit/test_params.py"]}, {"commit_id": "f2d5f57750ef538b84b67e09c6568ab49c5a3009", "commit_date": "Thu Feb 4 07:46:24 2021 -0500", "commit_message": "add facility to cache 'best' parameter sets to a sub-directory (#929)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/arguments.py", "sockeye/average.py", "sockeye/constants.py", "sockeye/train.py", "sockeye/training.py", "sockeye/utils.py", "test/unit/test_arguments.py", "test/unit/test_average.py", "test/unit/test_params.py"]}, {"commit_id": "9bc35c6338932d4b9910e3b1db0d4023c3b721e9", "commit_date": "Thu Feb 4 13:12:28 2021 +0100", "commit_message": "Fix mypy issues with numpy 1.20.0 (#931)", "files_name": ["sockeye/beam_search.py", "sockeye/inference.py", "sockeye/init_embedding.py", "sockeye/lexical_constraints.py", "sockeye/lexicon.py"]}, {"commit_id": "f55f5abb009141e30fdea356619ac327faff3a23", "commit_date": "Wed Jan 27 09:59:02 2021 -0600", "commit_message": "Scripts for processing benchmark files (#930)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye_contrib/benchmark/benchmark_to_output.py", "sockeye_contrib/benchmark/benchmark_to_percentiles.py"]}], "windows_after": [{"commit_id": "ec6350015dfab895a271ed619b239f3ef0460136", "commit_date": "Sun Nov 7 03:31:50 2021 -0500", "commit_message": "Update wmt.md (#972)", "files_name": ["docs/tutorials/wmt.md"]}, {"commit_id": "1bef6722872d9bc513084265db1e4ae68c22a4e8", "commit_date": "Mon Nov 8 10:55:10 2021 +0100", "commit_message": "Update push_pr.yml to point to main branch (#975)", "files_name": [".github/workflows/push_pr.yml"]}, {"commit_id": "aea9084c4c763bae2d01205d3eb7200671e949e7", "commit_date": "Mon Nov 8 11:18:06 2021 +0100", "commit_message": "master->main (#976)", "files_name": ["docs/development.md", "docs/index.md", "docs/inference.md", "docs/setup.md", "docs/training.md", "docs/tutorials/seqcopy.md"]}, {"commit_id": "c44f1262252ead39b426e15b5a62abecdde603fd", "commit_date": "Tue Nov 30 10:42:15 2021 +0100", "commit_message": "Sockeye 3 - Fast Neural Machine Translation with PyTorch (#977)", "files_name": [".github/workflows/push_pr.yml", ".github/workflows/torch_nightly.yml", ".pylintrc", "CHANGELOG.md", "README.md", "docs/development.md", "docs/faq.md", "docs/index.md", "docs/inference.md", "docs/setup.md", "docs/training.md", "docs/tutorials.md", "docs/tutorials/cpu_process_per_core_translation.py", "docs/tutorials/multilingual.md", "docs/tutorials/wmt.md", "docs/tutorials/wmt_large.md", "requirements/requirements.gpu-cu100.txt", "requirements/requirements.gpu-cu101.txt", "requirements/requirements.gpu-cu102.txt", "requirements/requirements.gpu-cu110.txt", "requirements/requirements.gpu-cu112.txt", "requirements/requirements.horovod.txt", "requirements/requirements.txt", "setup.py", "sockeye/__init__.py", "sockeye/arguments.py", "sockeye/average.py", "sockeye/beam_search.py", "sockeye/beam_search_pt.py", "sockeye/checkpoint_decoder.py", "sockeye/checkpoint_decoder_pt.py", "sockeye/constants.py", "sockeye/data_io.py", "sockeye/data_io_pt.py", "sockeye/decoder.py", "sockeye/decoder_pt.py", "sockeye/embeddings.py", "sockeye/encoder.py", "sockeye/encoder_pt.py", "sockeye/evaluate.py", "sockeye/extract_parameters.py", "sockeye/inference.py", "sockeye/inference_pt.py", "sockeye/init_embedding.py", "sockeye/initial_setup.py", "sockeye/layers.py", "sockeye/layers_pt.py", "sockeye/lexical_constraints.py", "sockeye/lexicon.py", "sockeye/log.py", "sockeye/loss.py", "sockeye/loss_pt.py", "sockeye/lr_scheduler.py", "sockeye/model.py", "sockeye/model_pt.py", "sockeye/mx_to_pt.py", "sockeye/optimizers.py", "sockeye/output_handler.py", "sockeye/prepare_data.py", "sockeye/prepare_data_pt.py", "sockeye/quantization.py", "sockeye/rerank.py", "sockeye/score.py", "sockeye/score_pt.py", "sockeye/scoring.py", "sockeye/scoring_pt.py", "sockeye/test_utils.py", "sockeye/train.py", "sockeye/train_pt.py", "sockeye/training.py", "sockeye/training_pt.py", "sockeye/transformer.py", "sockeye/transformer_pt.py", "sockeye/translate.py", "sockeye/translate_pt.py", "sockeye/utils.py", "sockeye_contrib/docker/Dockerfile.cpu", "sockeye_contrib/docker/Dockerfile.gpu", "sockeye_contrib/docker/README.md", "sockeye_contrib/docker/build.py", "sockeye_contrib/rouge.py", "test/common.py", "test/data/model_2.3.x/params.best", "test/data/model_2.3.x/params.best.mx", "test/integration/test_backwards_compatibility.py", "test/integration/test_constraints_int.py", "test/integration/test_seq_copy_int.py", "test/system/test_seq_copy_sys.py", "test/unit/test_arguments.py", "test/unit/test_average.py", "test/unit/test_beam_search.py", "test/unit/test_constraints.py", "test/unit/test_data_io.py", "test/unit/test_data_io_pt.py", "test/unit/test_decoder.py", "test/unit/test_encoder.py", "test/unit/test_fixed_param_strategy.py", "test/unit/test_inference.py", "test/unit/test_init_embedding.py", "test/unit/test_layers.py", "test/unit/test_loss.py", "test/unit/test_lr_scheduler.py", "test/unit/test_model.py", "test/unit/test_output_handler.py", "test/unit/test_params.py", "test/unit/test_scoring.py", "test/unit/test_transformer.py", "test/unit/test_translate.py", "test/unit/test_utils.py", "typechecked-files"]}, {"commit_id": "2b2d900ecb61605eac12335803c5808ed22bb5b7", "commit_date": "Wed Dec 1 12:33:35 2021 +0100", "commit_message": "Use relative links with .md extension (automatically converted to .html for github-pages) (#979)", "files_name": ["docs/_config.yml", "docs/index.md", "docs/inference.md", "docs/scoring.md", "docs/training.md", "docs/tutorials.md", "docs/tutorials/adapt.md", "docs/tutorials/multilingual.md", "docs/tutorials/seqcopy_tutorial.md", "docs/tutorials/wmt.md"]}, {"commit_id": "2bc83bd1580e8fa49acd329072c3b7ea9e400276", "commit_date": "Wed Dec 8 13:50:49 2021 +0100", "commit_message": "Update wmt.md (#982)", "files_name": ["docs/tutorials/wmt.md"]}, {"commit_id": "adc3f4a8b885e965f7cd04e9093efce7ff360d85", "commit_date": "Fri Dec 10 02:23:17 2021 -0600", "commit_message": "Always load parameters to average on CPU (#984)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/average.py"]}, {"commit_id": "097c237003fad5e8a9d215b7f76d9564615be964", "commit_date": "Fri Dec 10 21:35:09 2021 +0100", "commit_message": "Return target factor sequence scores in beam search & scoring (#980)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/beam_search_pt.py", "sockeye/inference_pt.py", "sockeye/layers_pt.py", "sockeye/output_handler.py", "sockeye/score_pt.py", "sockeye/scoring_pt.py", "test/common.py", "test/unit/test_beam_search.py", "test/unit/test_inference.py", "test/unit/test_output_handler.py"]}, {"commit_id": "fa23b9f6bb3d7cf7e94ceebed98b1183047967ed", "commit_date": "Mon Dec 13 10:36:07 2021 +0100", "commit_message": "Fix EnsembleInference for models without target factors (#987)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/beam_search_pt.py"]}, {"commit_id": "8e5033be2a2f09d935c682f33703eff34cf8b3f4", "commit_date": "Mon Dec 13 18:22:22 2021 +0100", "commit_message": "Convert data permutations to int64 (long) (#990)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/data_io_pt.py"]}, {"commit_id": "1decce5169c7f12337ecd942617aeb04bdd16bb9", "commit_date": "Tue Dec 14 10:06:53 2021 +0100", "commit_message": "fix multilingual tutorial data download (#991)", "files_name": ["CHANGELOG.md", "docs/tutorials/multilingual.md", "docs/tutorials/multilingual/prepare-iwslt17-multilingual.sh", "sockeye/__init__.py"]}, {"commit_id": "64c3b09b3402fffb63799e9ad33c81c6b97d8629", "commit_date": "Tue Dec 14 15:48:58 2021 +0100", "commit_message": "Reduce number of tensor operations and split operations (#992)", "files_name": ["sockeye/beam_search_pt.py", "sockeye/data_io_pt.py", "sockeye/encoder_pt.py"]}, {"commit_id": "02268714886c4f9e417c9507a3b66b0d218ec181", "commit_date": "Thu Dec 16 23:06:35 2021 +0100", "commit_message": "Multilingual tutorial: pull data from main branch (#994)", "files_name": ["docs/tutorials/multilingual.md"]}, {"commit_id": "71527d7161705108033122d492c87df2479da8eb", "commit_date": "Fri Dec 17 13:56:37 2021 -0600", "commit_message": "Fix issue with checkpoint decoder metrics for distributed training (#997)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/training_pt.py"]}, {"commit_id": "6905c787325e087537ce523c9d977ee4de188606", "commit_date": "Sun Dec 19 17:49:18 2021 +0100", "commit_message": "Fused Multihead Attention for training (#985)", "files_name": ["CHANGELOG.md", "sockeye/__init__.py", "sockeye/decoder_pt.py", "sockeye/encoder_pt.py", "sockeye/layers_pt.py"]}], "parents": [{"commit_id_before": "565cc71d42ef983eb17d07f3c7befce67c1331ce", "url_before": "https://api.github.com/repos/awslabs/sockeye/commits/565cc71d42ef983eb17d07f3c7befce67c1331ce", "html_url_before": "https://github.com/awslabs/sockeye/commit/565cc71d42ef983eb17d07f3c7befce67c1331ce"}], "details": [{"raw_url": "https://github.com/awslabs/sockeye/raw/35dd717a80ae1f04128d79bd0bcf340e2e9d1427/CHANGELOG.md", "code": "# Changelog\n\nAll notable changes to the project are documented in this file.\n\nVersion numbers are of the form `1.0.0`.\nAny version bump in the last digit is backwards-compatible, in that a model trained with the previous version can still\nbe used for translation with the new version.\nAny bump in the second digit indicates a backwards-incompatible change,\ne.g. due to changing the architecture or simply modifying model parameter names.\nNote that Sockeye has checks in place to not translate with an old model that was trained with an incompatible version.\n\nEach version section may have have subsections for: _Added_, _Changed_, _Removed_, _Deprecated_, and _Fixed_.\n\n\n## [2.3.24]\n### Added\n\n- Use of the safe yaml loader for the model configuration files.\n\n## [2.3.23]\n### Changed\n\n- Do not sort BIAS_STATE in beam search. It is constant across decoder steps.\n\n## [2.3.22]\n### Fixed\n\n- The previous commit introduced a regression for vocab creation. The results was that the vocabulary was created on the input characters rather than on tokens.\n\n## [2.3.21]\n### Added\n\n- Extended parallelization of data preparation to vocabulary and statistics creation while minimizing the overhead of sharding.\n\n## [2.3.20]\n### Added\n\n- Added debug logging for restrict_lexicon lookups\n\n## [2.3.19]\n### Changed\n\n- When training only the decoder (`--fixed-param-strategy all_except_decoder`), disable autograd for the encoder and embeddings to save memory.\n\n## [2.3.18]\n### Changed\n\n- Updated Docker builds and documentation.  See [sockeye_contrib/docker](sockeye_contrib/docker).\n\n## [2.3.17]\n### Added\n- Added an alternative, faster implementation of greedy search. The '--greedy' flag to `sockeye.translate` will enable it. This implementation does not support hypothesis scores, batch decoding, or lexical constraints.\"\n\n## [2.3.16]\n\n### Added\n- Added option `--transformer-feed-forward-use-glu` to use Gated Linear Units in transformer feed forward networks ([Dauphin et al., 2016](https://arxiv.org/abs/1612.08083); [Shazeer, 2020](https://arxiv.org/abs/2002.05202)).\n\n## [2.3.15]\n\n### Changed\n- Optimization: Decoder class is now a complete HybridBlock (no forward method).\n\n## [2.3.14]\n\n### Changed\n- Updated to [MXNet 1.8.0](https://github.com/apache/incubator-mxnet/tree/1.8.0)\n- Removed dependency support for Cuda 9.2 (no longer supported by MXNet 1.8).\n- Added dependency support for Cuda 11.0 and 11.2.\n- Updated Python requirement to 3.7 and later. (Removed backporting `dataclasses` requirement)\n\n## [2.3.13]\n\n### Added\n- Target factors are now also collected for nbest translations (and stored in the JSON output handler).\n\n## [2.3.12]\n\n### Added\n- Added `--config` option to `prepare_data` CLI to allow setting commandline flags via a yaml config.\n- Flags for the `prepare_data` CLI are now stored in the output folder under `args.yaml`\n  (equivalent to the behavior of `sockeye_train`)\n\n## [2.3.11]\n\n### Added\n- Added option `prevent_unk` to avoid generating `<unk>` token in beam search.\n\n## [2.3.10]\n\n### Changed\n\n- Make sure that the top N best params files retained, even if N > --keep-last-params. This ensures that model\n  averaging will not be crippled when keeping only a few params files during training. This can result in a\n  significant savings of disk space during training.\n\n## [2.3.9]\n\n### Added\n\n- Added scripts for processing Sockeye benchmark output (`--output-type benchmark`):\n  - [benchmark_to_output.py](sockeye_contrib/benchmark/benchmark_to_output.py) extracts translations\n  - [benchmark_to_percentiles.py](sockeye_contrib/benchmark/benchmark_to_percentiles.py) computes percentiles\n\n## [2.3.8]\n\n### Fixed\n\n- Fix problem identified in issue #925 that caused learning rate\n  warmup to fail in some instances when doing continued training\n\n## [2.3.7]\n\n### Changed\n\n- Use dataclass module to simplify Config classes. No functional change.\n\n## [2.3.6]\n\n### Fixed\n\n- Fixes the problem identified in issue #890, where the lr_scheduler\n  does not behave as expected when continuing training. The problem is\n  that the lr_scheduler is kept as part of the optimizer, but the\n  optimizer is not saved when saving state. Therefore, every time\n  training is restarted, a new lr_scheduler is created with initial\n  parameter settings. Fix by saving and restoring the lr_scheduling\n  separately.\n\n## [2.3.5]\n\n### Fixed\n\n- Fixed issue with LearningRateSchedulerPlateauReduce.__repr__ printing\n\tout num_not_improved instead of reduce_num_not_improved.\n\n## [2.3.4]\n\n### Fixed\n\n- Fixed issue with dtype mismatch in beam search when translating with `--dtype float16`.\n\n## [2.3.3]\n\n### Changed\n\n- Upgraded `SacreBLEU` dependency of Sockeye to a newer version (`1.4.14`).\n\n## [2.3.2]\n### Fixed\n\n- Fixed edge case that unintentionally skips softmax for sampling if beam size is 1.\n\n## [2.3.1]\n### Fixed\n\n- Optimizing for BLEU/CHRF with horovod required the secondary workers to also create checkpoint decoders.\n\n## [2.3.0]\n\n### Added\n\n- Added support for target factors.\n  If provided with additional target-side tokens/features (token-parallel to the regular target-side) at training time,\n  the model can now learn to predict these in a multi-task setting. You can provide target factor data similar to source\n  factors: `--target-factors <factor_file1> [<factor_fileN>]`. During training, Sockeye optimizes one loss per factor\n  in a multi-task setting. The weight of the losses can be controlled by `--target-factors-weight`.\n  At inference, target factors are decoded greedily, they do not participate in beam search.\n  The predicted factor at each time step is the argmax over its separate output\n  layer distribution. To receive the target factor predictions at inference time, use\n  `--output-type translation_with_factors`.\n\n### Changed\n\n- `load_model(s)` now returns a list of target vocabs.\n- Default source factor combination changed to `sum` (was `concat` before).\n- `SockeyeModel` class has three new properties: `num_target_factors`, `target_factor_configs`,\n  and `factor_output_layers`.\n\n## [2.2.8]\n\n### Changed\n- Make source/target data parameters required for the scoring CLI to avoid cryptic error messages.\n\n## [2.2.7]\n\n### Added\n\n- Added an argument to specify the log level of secondary workers. Defaults to ERROR to hide any logs except for exceptions.\n\n## [2.2.6]\n\n### Fixed\n- Avoid a crash due to an edge case when no model improvement has been observed by the time the learning rate gets reduced for the first time.\n\n## [2.2.5]\n\n### Fixed\n- Enforce sentence batching for sockeye score tool, set default batch size to 56\n\n## [2.2.4]\n\n### Changed\n- Use softmax with length in DotAttentionCell.\n- Use `contrib.arange_like` in AutoRegressiveBias block to reduce number of ops.\n\n## [2.2.3]\n\n### Added\n\n- Log the absolute number of `<unk>` tokens in source and target data\n\n## [2.2.2]\n\n### Fixed\n\n- Fix: Guard against null division for small batch sizes.\n\n## [2.2.1]\n\n## Fixed\n\n- Fixes a corner case bug by which the beam decoder can wrongly return a best hypothesis with -infinite score.\n\n## [2.2.0]\n\n### Changed\n\n- Replaced multi-head attention with [interleaved_matmul_encdec](https://github.com/apache/incubator-mxnet/pull/16408) operators, which removes previously needed transposes and improves performance.\n\n- Beam search states and model layers now assume time-major format.\n\n## [2.1.26]\n\n### Fixed\n\n- Fixes a backwards incompatibility introduced in 2.1.17, which would prevent models trained with prior versions to be used for inference.\n\n## [2.1.25]\n\n### Changed\n\n- Reverting PR #772 as it causes issues with `amp`.\n\n## [2.1.24]\n\n### Changed\n\n- Make sure to write a final checkpoint when stopping with `--max-updates`, `--max-samples` or `--max-num-epochs`.\n\n## [2.1.23]\n\n### Changed\n\n- Updated to [MXNet 1.7.0](https://github.com/apache/incubator-mxnet/tree/1.7.0).\n- Re-introduced use of softmax with length parameter in DotAttentionCell (see PR #772).\n\n## [2.1.22]\n\n### Added\n\n- Re-introduced `--softmax-temperature` flag for `sockeye.score` and `sockeye.translate`.\n\n## [2.1.21]\n\n### Added\n\n- Added an optional ability to cache encoder outputs of model.\n\n## [2.1.20]\n\n### Fixed\n\n- Fixed a bug where the training state object was saved to disk before training metrics were added to it, leading to an inconsistency between the training state object and the metrics file (see #859).\n\n## [2.1.19]\n\n### Fixed\n\n- When loading a shard in Horovod mode, there is now a check that each non-empty bucket contains enough sentences to cover each worker's slice. If not, the bucket's sentences are replicated to guarantee coverage.\n\n## [2.1.18]\n\n### Fixed\n\n- Fixed a bug where sampling translation fails because an array is created in the wrong context.\n\n## [2.1.17]\n\n### Added\n\n- Added `layers.SSRU`, which implements a Simpler Simple Recurrent Unit as described in\nKim et al, \"From Research to Production and Back: Ludicrously Fast Neural Machine Translation\" WNGT 2019.\n\n- Added `ssru_transformer` option to `--decoder`, which enables the usage of SSRUs as a replacement for the decoder-side self-attention layers.\n\n### Changed\n\n- Reduced the number of arguments for `MultiHeadSelfAttention.hybrid_forward()`.\n `previous_keys` and `previous_values` should now be input together as `previous_states`, a list containing two symbols.\n\n## [2.1.16]\n\n### Fixed\n\n- Fixed batch sizing error introduced in version 2.1.12 (c00da52) that caused batch sizes to be multiplied by the number of devices. Batch sizing now works as documented (same as pre-2.1.12 versions).\n- Fixed `max-word` batching to properly size batches to a multiple of both `--batch-sentences-multiple-of` and the number of devices.\n\n## [2.1.15]\n\n### Added\n\n- Inference option `--mc-dropout` to use dropout during inference, leading to non-deterministic output. This option uses the same dropout parameters present in the model config file.\n\n## [2.1.14]\n\n### Added\n\n- Added `sockeye.rerank` option `--output` to specify output file.\n- Added `sockeye.rerank` option `--output-reference-instead-of-blank` to output reference line instead of best hypothesis when best hypothesis is blank.\n\n\n## [2.1.13]\n\n### Added\n\n- Training option `--quiet-secondary-workers` that suppresses console output for secondary workers when training with Horovod/MPI.\n- Set version of isort to `<5.0.0` in requirements.dev.txt to avoid incompatibility between newer versions of isort and pylint.\n\n## [2.1.12]\n\n### Added\n\n- Batch type option `max-word` for max number of words including padding tokens (more predictable memory usage than `word`).\n- Batching option `--batch-sentences-multiple-of` that is similar to `--round-batch-sizes-to-multiple-of` but always rounds down (more predictable memory usage).\n\n### Changed\n\n- Default bucketing settings changed to width 8, max sequence length 95 (96 including BOS/EOS tokens), and no bucket scaling.\n- Argument `--no-bucket-scaling` replaced with `--bucket-scaling` which is False by default.\n\n## [2.1.11]\n\n### Changed\n\n- Updated `sockeye.rerank` module to use \"add-k\" smoothing for sentence-level BLEU.\n\n### Fixed\n\n- Updated `sockeye.rerank` module to use current N-best format.\n\n## [2.1.10]\n\n### Changed\n\n- Changed to a cross-entropy loss implementation that avoids the use of SoftmaxOutput.\n\n## [2.1.9]\n\n### Added\n\n- Added training argument `--ignore-extra-params` to ignore extra parameters when loading models.  The primary use case is continuing training with a model that has already been annotated with scaling factors (`sockeye.quantize`).\n\n### Fixed\n\n- Properly pass `allow_missing` flag to `model.load_parameters()`\n\n## [2.1.8]\n\n### Changed\n\n- Update to sacrebleu=1.4.10\n\n## [2.1.7]\n\n### Changed\n\n- Optimize prepare_data by saving the shards in parallel. The prepare_data script accepts a new parameter `--max-processes` to control the level of parallelism with which shards are written to disk.\n\n## [2.1.6]\n\n### Changed\n\n- Updated Dockerfiles optimized for CPU (intgemm int8 inference, full MKL support) and GPU (distributed training with Horovod).  See [sockeye_contrib/docker](sockeye_contrib/docker).\n\n### Added\n\n- Official support for int8 quantization with [intgemm](https://github.com/kpu/intgemm):\n  - This requires the \"intgemm\" fork of MXNet ([kpuatamazon/incubator-mxnet/intgemm](https://github.com/kpuatamazon/incubator-mxnet/tree/intgemm)).  This is the version of MXNet used in the Sockeye CPU docker image (see [sockeye_contrib/docker](sockeye_contrib/docker)).\n  - Use `sockeye.translate --dtype int8` to quantize a trained float32 model at runtime.\n  - Use the `sockeye.quantize` CLI to annotate a float32 model with int8 scaling factors for fast runtime quantization.\n\n## [2.1.5]\n\n### Changed\n\n- Changed state caching for transformer models during beam search to cache states with attention heads already separated out. This avoids repeated transpose operations during decoding, leading to faster inference.\n\n## [2.1.4]\n\n### Added\n\n- Added Dockerfiles that build an experimental CPU-optimized Sockeye image:\n  - Uses the latest versions of [kpuatamazon/incubator-mxnet](https://github.com/kpuatamazon/incubator-mxnet) (supports [intgemm](https://github.com/kpu/intgemm) and makes full use of Intel MKL) and [kpuatamazon/sockeye](https://github.com/kpuatamazon/sockeye) (supports int8 quantization for inference).\n  - See [sockeye_contrib/docker](sockeye_contrib/docker).\n\n## [2.1.3]\n\n### Changed\n\n- Performance optimizations to beam search inference\n  - Remove unneeded take ops on encoder states\n  - Gathering input data before sending to GPU, rather than sending each batch element individually\n  - All of beam search can be done in fp16, if specified by the model\n  - Other small miscellaneous optimizations\n- Model states are now a flat list in ensemble inference, structure of states provided by `state_structure()`\n\n## [2.1.2]\n\n### Changed\n\n- Updated to [MXNet 1.6.0](https://github.com/apache/incubator-mxnet/tree/1.6.0)\n\n### Added\n\n- Added support for CUDA 10.2\n\n### Removed\n\n- Removed support for CUDA<9.1 / CUDNN<7.5\n\n## [2.1.1]\n\n### Added\n- Ability to set environment variables from training/translate CLIs before MXNet is imported. For example, users can\n  configure MXNet as such: `--env \"OMP_NUM_THREADS=1;MXNET_ENGINE_TYPE=NaiveEngine\"`\n\n## [2.1.0]\n\n### Changed\n\n- Version bump, which should have been included in commit b0461b due to incompatible models.\n\n## [2.0.1]\n\n### Changed\n\n- Inference defaults to using the max input length observed in training (versus scaling down based on mean length ratio and standard deviations).\n\n### Added\n\n- Additional parameter fixing strategies:\n  - `all_except_feed_forward`: Only train feed forward layers.\n  - `encoder_and_source_embeddings`: Only train the decoder (decoder layers, output layer, and target embeddings).\n  - `encoder_half_and_source_embeddings`: Train the latter half of encoder layers and the decoder.\n- Option to specify the number of CPU threads without using an environment variable (`--omp-num-threads`).\n- More flexibility for source factors combination\n\n## [2.0.0]\n\n### Changed\n\n- Update to [MXNet 1.5.0](https://github.com/apache/incubator-mxnet/tree/1.5.0)\n- Moved `SockeyeModel` implementation and all layers to [Gluon API](http://mxnet.incubator.apache.org/versions/master/gluon/index.html)\n- Removed support for Python 3.4.\n- Removed image captioning module\n- Removed outdated Autopilot module\n- Removed unused training options: Eve, Nadam, RMSProp, Nag, Adagrad, and Adadelta optimizers, `fixed-step` and `fixed-rate-inv-t` learning rate schedulers\n- Updated and renamed learning rate scheduler `fixed-rate-inv-sqrt-t` -> `inv-sqrt-decay`\n- Added script for plotting metrics files: [sockeye_contrib/plot_metrics.py](sockeye_contrib/plot_metrics.py)\n- Removed option `--weight-tying`.  Weight tying is enabled by default, disable with `--weight-tying-type none`.\n\n### Added\n\n- Added distributed training support with Horovod/MPI.  Use `horovodrun` and the `--horovod` training flag.\n- Added Dockerfiles that build a Sockeye image with all features enabled.  See [sockeye_contrib/docker](sockeye_contrib/docker).\n- Added `none` learning rate scheduler (use a fixed rate throughout training)\n- Added `linear-decay` learning rate scheduler\n- Added training option `--learning-rate-t-scale` for time-based decay schedulers\n- Added support for MXNet's [Automatic Mixed Precision](https://mxnet.incubator.apache.org/versions/master/tutorials/amp/amp_tutorial.html).  Activate with the `--amp` training flag.  For best results, make sure as many model dimensions are possible are multiples of 8.\n- Added options for making various model dimensions multiples of a given value.  For example, use `--pad-vocab-to-multiple-of 8`, `--bucket-width 8 --no-bucket-scaling`, and `--round-batch-sizes-to-multiple-of 8` with AMP training.\n- Added [GluonNLP](http://gluon-nlp.mxnet.io/)'s BERTAdam optimizer, an implementation of the Adam variant used by Devlin et al. ([2018](https://arxiv.org/pdf/1810.04805.pdf)).  Use `--optimizer bertadam`.\n- Added training option `--checkpoint-improvement-threshold` to set the amount of metric improvement required over the window of previous checkpoints to be considered actual model improvement (used with `--max-num-checkpoint-not-improved`).\n\n## [1.18.103]\n### Added\n- Added ability to score image-sentence pairs by extending the scoring feature originally implemented for machine\n  translation to the image captioning module.\n\n## [1.18.102]\n### Fixed\n- Fixed loading of more than 10 source vocabulary files to be in the right, numerical order.\n\n## [1.18.101]\n### Changed\n- Update to Sacrebleu 1.3.6\n\n## [1.18.100]\n### Fixed\n- Always initializing the multiprocessing context. This should fix issues observed when running `sockeye-train`.\n\n## [1.18.99]\n### Changed\n- Updated to [MXNet 1.4.1](https://github.com/apache/incubator-mxnet/tree/1.4.1)\n\n## [1.18.98]\n### Changed\n- Converted several transformer-related layer implementations to Gluon HybridBlocks. No functional change.\n\n## [1.18.97]\n### Changed\n- Updated to PyYAML 5.1\n\n## [1.18.96]\n### Changed\n- Extracted prepare vocab functionality in the build vocab step into its own function. This matches the pattern in prepare data and train where the main() function only has argparsing, and it invokes a separate function to do the work. This is to allow modules that import this one to circumvent the command line.\n\n## [1.18.95]\n### Changed\n- Removed custom operators from transformer models and replaced them with symbolic operators.\n  Improves Performance.\n\n## [1.18.94]\n### Added\n- Added ability to accumulate gradients over multiple batches (--update-interval). This allows simulation of large\n  batch sizes on environments with limited memory. For example: training with `--batch-size 4096 --update-interval 2`\n  should be close to training with `--batch-size 8192` at smaller memory footprint.\n\n## [1.18.93]\n### Fixed\n- Made `brevity_penalty` argument in `Translator` class optional to ensure backwards compatibility.\n\n## [1.18.92]\n### Added\n- Added sentence length (and length ratio) prediction to be able to discourage hypotheses that are too short at inference time. Can be enabled for training with `--length-task` and with `--brevity-penalty-type` during inference.\n\n## [1.18.91]\n### Changed\n- Multiple lexicons can now be specified with the `--restrict-lexicon` option:\n  - For a single lexicon: `--restrict-lexicon /path/to/lexicon`.\n  - For multiple lexicons: `--restrict-lexicon key1:/path/to/lexicon1 key2:/path/to/lexicon2 ...`.\n  - Use `--json-input` to specify the lexicon to use for each input, ex: `{\"text\": \"some input string\", \"restrict_lexicon\": \"key1\"}`.\n\n## [1.18.90]\n### Changed\n- Updated to [MXNet 1.4.0](https://github.com/apache/incubator-mxnet/tree/1.4.0)\n- Integration tests no longer check for equivalence of outputs with batch size 2\n\n## [1.18.89]\n### Fixed\n- Made the length ratios per bucket change backwards compatible.\n\n## [1.18.88]\n### Changed\n- Made sacrebleu a pip dependency and removed it from `sockeye_contrib`.\n\n## [1.18.87]\n### Added\n- Data statistics at training time now compute mean and standard deviation of length ratios per bucket.\n  This information is stored in the model's config, but not used at the moment.\n\n## [1.18.86]\n### Added\n- Added the `--fixed-param-strategy` option that allows fixing various model parameters during training via named strategies.\n  These include some of the simpler combinations from [Wuebker et al. (2018)](https://arxiv.org/abs/1811.01990) such as fixing everything except the first and last layers of the encoder and decoder (`all_except_outer_layers`).  See the help message for a full list of strategies.\n\n## [1.18.85]\n### Changed\n- Disabled dynamic batching for `Translator.translate()` by default due to increased memory usage. The default is to\n  fill-up batches to `Translator.max_batch_size`.\n  Dynamic batching can still be enabled if `fill_up_batches` is set to False.\n### Added\n- Added parameter to force training to stop after a given number of checkpoints. Useful when forced to share limited GPU resources.\n\n## [1.18.84]\n### Fixed\n- Fixed lexical constraints bugs that broke batching and caused large drop in BLEU.\n  These were introduced with sampling (1.18.64).\n\n## [1.18.83]\n### Changed\n - The embedding size is automatically adjusted to the Transformer model size in case it is not specified on the command line.\n\n## [1.18.82]\n### Fixed\n- Fixed type conversion in metrics file reading introduced in 1.18.79.\n\n## [1.18.81]\n### Fixed\n- Making sure the training pickled training state contains the checkpoint decoder's BLEU score of the last checkpoint.\n\n## [1.18.80]\n### Fixed\n- Fixed a bug introduced in 1.18.77 where blank lines in the training data resulted in failure.\n\n## [1.18.79]\n### Added\n- Writing of the convergence/divergence status to the metrics file and guarding against numpy.histogram's errors for NaNs during divergent behaviour.\n\n## [1.18.78]\n### Changed\n- Dynamic batch sizes: `Translator.translate()` will adjust batch size in beam search to the actual number of inputs without using padding.\n\n## [1.18.77]\n### Added\n- `sockeye.score` now loads data on demand and doesn't skip any input lines\n\n## [1.18.76]\n### Changed\n- Do not compare scores from translation and scoring in integration tests.\n\n### Added\n- Adding the option via the flag `--stop-training-on-decoder-failure` to stop training in case the checkpoint decoder dies (e.g. because there is not enough memory).\nIn case this is turned on a checkpoint decoder is launched right when training starts in order to fail as early as possible.\n\n## [1.18.75]\n### Changed\n- Do not create dropout layers for inference models for performance reasons.\n\n## [1.18.74]\n### Changed\n- Revert change in 1.18.72 as no memory saving could be observed.\n\n## [1.18.73]\n### Fixed\n- Fixed a bug where `source-factors-num-embed` was not correctly adjusted to `num-embed`\n  when using prepared data & `source-factor-combine` sum.\n\n## [1.18.72]\n### Changed\n- Removed use of `expand_dims` in favor of `reshape` to save memory.\n\n## [1.18.71]\n### Fixed\n- Fixed default setting of source factor combination to be 'concat' for backwards compatibility.\n\n## [1.18.70]\n### Added\n- Sockeye now outputs fields found in a JSON input object, if they are not overwritten by Sockeye. This behavior can be enabled by selecting `--json-input` (to read input as a JSON object) and `--output-type json` (to write a JSON object to output).\n\n## [1.18.69]\n### Added\n- Source factors can now be added to the embeddings instead of concatenated with `--source-factors-combine sum` (default: concat)\n\n## [1.18.68]\n- Fixed training crashes with `--learning-rate-decay-optimizer-states-reset initial` option.\n\n## [1.18.67]\n### Added\n- Added `fertility` as a further type of attention coverage.\n- Added an option for training to keep the initializations of the model via `--keep-initializations`. When set, the trainer will avoid deleting the params file for the first checkpoint, no matter what `--keep-last-params` is set to.\n\n## [1.18.66]\n### Fixed\n- Fix to argument names that are allowed to differ for resuming training.\n\n## [1.18.65]\n### Changed\n- More informative error message about inconsistent --shared-vocab setting.\n\n## [1.18.64]\n### Added\n- Adding translation sampling via `--sample [N]`. This causes the decoder to sample each next step from the target distribution probabilities at each\n  timestep. An optional value of `N` causes the decoder to sample only from the top `N` vocabulary items for each hypothesis at each timestep (the\n  default is 0, meaning to sample from the entire vocabulary).\n\n## [1.18.63]\n### Changed\n- The checkpoint decoder and nvidia-smi subprocess are now launched from a forkserver, allowing for a better separation between processes.\n\n## [1.18.62]\n### Added\n- Add option to make `TranslatorInputs` directly from a dict.\n\n## [1.18.61]\n### Changed\n- Update to MXNet 1.3.1. Removed requirements/requirements.gpu-cu{75,91}.txt as CUDA 7.5 and 9.1 are deprecated.\n\n## [1.18.60]\n### Fixed\n- Performance optimization to skip the softmax operation for single model greedy decoding is now only applied if no translation scores are required in the output.\n\n## [1.18.59]\n### Added\n- Full training state is now returned from EarlyStoppingTrainer's fit().\n### Changed\n- Training state cleanup will not be performed for training runs that did not converge yet.\n- Switched to portalocker for locking files (Windows compatibility).\n\n## [1.18.58]\n### Added\n- Added nbest translation, exposed as `--nbest-size`. Nbest translation means to not only output the most probable translation according to a model, but the top n most probable hypotheses. If `--nbest-size > 1` and the option `--output-type` is not explicitly specified, the output type will be changed to one JSON list of nbest translations per line. `--nbest-size` can never be larger than `--beam-size`.\n\n### Changed\n- Changed `sockeye.rerank` CLI to be compatible with nbest translation JSON output format.\n\n## [1.18.57]\n### Added\n- Added `sockeye.score` CLI for quickly scoring existing translations ([documentation](tutorials/scoring.md)).\n### Fixed\n- Entry-point clean-up after the contrib/ rename\n\n## [1.18.56]\n### Changed\n- Update to MXNet 1.3.0.post0\n\n## [1.18.55]\n- Renamed `contrib` to less-generic `sockeye_contrib`\n\n## [1.18.54]\n### Added\n- `--source-factor-vocabs` can be set to provide source factor vocabularies.\n\n## [1.18.53]\n### Added\n- Always skipping softmax for greedy decoding by default, only for single models.\n- Added option `--skip-topk` for greedy decoding.\n\n## [1.18.52]\n### Fixed\n- Fixed bug in constrained decoding to make sure best hypothesis satifies all constraints.\n\n## [1.18.51]\n### Added\n- Added a CLI for reranking of an nbest list of translations.\n\n## [1.18.50]\n### Fixed\n- Check for equivalency of training and validation source factors was incorrectly indented.\n\n## [1.18.49]\n### Changed\n- Removed dependence on the nvidia-smi tool. The number of GPUs is now determined programatically.\n\n## [1.18.48]\n### Changed\n- Translator.max_input_length now reports correct maximum input length for TranslatorInput objects, independent of the internal representation, where an additional EOS gets added.\n\n## [1.18.47]\n### Changed\n- translate CLI: no longer rely on external, user-given input id for sorting translations. Also allow string ids for sentences.\n\n## [1.18.46]\n### Fixed\n- Fixed issue with `--num-words 0:0` in image captioning and another issue related to loading all features to memory with variable length.\n\n## [1.18.45]\n### Added\n- Added an 8 layer LSTM model similar (but not exactly identical) to the 'GNMT' architecture to autopilot.\n\n## [1.18.44]\n### Fixed\n- Fixed an issue with `--max-num-epochs` causing training to stop before the update/batch that actually completes the epoch was made.\n\n## [1.18.43]\n### Added\n- `<s>` now supported as the first token in a multi-word negative constraint\n  (e.g., `<s> I think` to prevent a sentence from starting with `I think`)\n### Fixed\n- Bugfix in resetting the state of a multiple-word negative constraint\n\n## [1.18.42]\n### Changed\n- Simplified gluon blocks for length calculation\n\n## [1.18.41]\n### Changed\n- Require numpy 1.14 or later to avoid MKL conflicts between numpy as mxnet-mkl.\n\n## [1.18.40]\n### Fixed\n- Fixed bad check for existence of negative constraints.\n- Resolved conflict for phrases that are both positive and negative constraints.\n- Fixed softmax temperature at inference time.\n\n## [1.18.39]\n### Added\n- Image Captioning now supports constrained decoding.\n- Image Captioning: zero padding of features now allows input features of different shape for each image.\n\n## [1.18.38]\n### Fixed\n- Fixed issue with the incorrect order of translations when empty inputs are present and translating in chunks.\n\n## [1.18.37]\n### Fixed\n- Determining the max output length for each sentence in a batch by the bucket length rather than the actual in order to match the behavior of a single sentence translation.\n\n## [1.18.36]\n### Changed\n- Updated to [MXNet 1.2.1](https://github.com/apache/incubator-mxnet/tree/1.2.1)\n\n## [1.18.35]\n### Added\n- ROUGE scores are now available in `sockeye-evaluate`.\n- Enabled CHRF as an early-stopping metric.\n\n## [1.18.34]\n### Added\n- Added support for `--beam-search-stop first` for decoding jobs with `--batch-size > 1`.\n\n## [1.18.33]\n### Added\n- Now supports negative constraints, which are phrases that must *not* appear in the output.\n   - Global constraints can be listed in a (pre-processed) file, one per line: `--avoid-list FILE`\n   - Per-sentence constraints are passed using the `avoid` keyword in the JSON object, with a list of strings as its field value.\n\n## [1.18.32]\n### Added\n- Added option to pad vocabulary to a multiple of x: e.g. `--pad-vocab-to-multiple-of 16`.\n\n## [1.18.31]\n### Added\n- Pre-training the RNN decoder. Usage:\n  1. Train with flag `--decoder-only`.\n  2. Feed identical source/target training data.\n\n## [1.18.30]\n### Fixed\n- Preserving max output length for each sentence to allow having identical translations for both with and without batching.\n\n## [1.18.29]\n### Changed\n- No longer restrict the vocabulary to 50,000 words by default, but rather create the vocabulary from all words which occur at least `--word-min-count` times. Specifying `--num-words` explicitly will still lead to a restricted\n  vocabulary.\n\n## [1.18.28]\n### Changed\n- Temporarily fixing the pyyaml version to 3.12 as version 4.1 introduced some backwards incompatible changes.\n\n## [1.18.27]\n### Fixed\n- Fix silent failing of NDArray splits during inference by using a version that always returns a list. This was causing incorrect behavior when using lexicon restriction and batch inference with a single source factor.\n\n## [1.18.26]\n### Added\n- ROUGE score evaluation. It can be used as the stopping criterion for tasks such as summarization.\n\n## [1.18.25]\n### Changed\n- Update requirements to use MKL versions of MXNet for fast CPU operation.\n\n## [1.18.24]\n### Added\n- Dockerfiles and convenience scripts for running `fast_align` to generate lexical tables.\nThese tables can be used to create top-K lexicons for faster decoding via vocabulary selection ([documentation](https://github.com/awslabs/sockeye/tree/master/contrib/fast_align)).\n\n### Changed\n- Updated default top-K lexicon size from 20 to 200.\n\n## [1.18.23]\n### Fixed\n- Correctly create the convolutional embedding layers when the encoder is set to `transformer-with-conv-embed`. Previously\nno convolutional layers were added so that a standard Transformer model was trained instead.\n\n## [1.18.22]\n### Fixed\n- Make sure the default bucket is large enough with word based batching when the source is longer than the target (Previously\nthere was an edge case where the memory usage was sub-optimal with word based batching and longer source than target sentences).\n\n## [1.18.21]\n### Fixed\n- Constrained decoding was missing a crucial cast\n- Fixed test cases that should have caught this\n\n## [1.18.20]\n### Changed\n- Transformer parametrization flags (model size, # of attention heads, feed-forward layer size) can now optionally\n  defined separately for encoder & decoder. For example, to use a different transformer model size for the encoder,\n  pass `--transformer-model-size 1024:512`.\n\n## [1.18.19]\n### Added\n- LHUC is now supported in transformer models\n\n## [1.18.18]\n### Added\n- \\[Experimental\\] Introducing the image captioning module. Type of models supported: ConvNet encoder - Sockeye NMT decoders. This includes also a feature extraction script,\nan image-text iterator that loads features, training and inference pipelines and a visualization script that loads images and captions.\nSee [this tutorial](tutorials/image_captioning) for its usage. This module is experimental therefore its maintenance is not fully guaranteed.\n\n## [1.18.17]\n### Changed\n- Updated to MXNet 1.2\n- Use of the new LayerNormalization operator to save GPU memory.\n\n## [1.18.16]\n### Fixed\n- Removed summation of gradient arrays when logging gradients.\n  This clogged the memory on the primary GPU device over time when many checkpoints were done.\n  Gradient histograms are now logged to Tensorboard separated by device.\n\n## [1.18.15]\n### Added\n- Added decoding with target-side lexical constraints (documentation in `tutorials/constraints`).\n\n## [1.18.14]\n### Added\n- Introduced Sockeye Autopilot for single-command end-to-end system building.\nSee the [Autopilot documentation]((https://github.com/awslabs/sockeye/tree/master/contrib/autopilot)) and run with: `sockeye-autopilot`.\nAutopilot is a `contrib` module with its own tests that are run periodically.\nIt is not included in the comprehensive tests run for every commit.\n\n## [1.18.13]\n### Fixed\n- Fixed two bugs with training resumption:\n  1. removed overly strict assertion in the data iterator for model states before the first checkpoint.\n  2. removed deletion of Tensorboard log directory.\n\n### Added\n- Added support for config files. Command line parameters have precedence over the values read from the config file.\n  Minimal working example:\n  `python -m sockeye.train --config config.yaml` with contents of `config.yaml` as follows:\n  ```yaml\n  source: source.txt\n  target: target.txt\n  output: out\n  validation_source: valid.source.txt\n  validation_target: valid.target.txt\n  ```\n### Changed\n  The full set of arguments is serialized to `out/args.yaml` at the beginning of training (before json was used).\n\n## [1.18.12]\n### Changed\n- All source side sequences now get appended an additional end-of-sentence (EOS) symbol. This change is backwards\n  compatible meaning that inference with older models will still work without the EOS symbol.\n\n## [1.18.11]\n### Changed\n- Default training parameters have been changed to reflect the setup used in our arXiv paper. Specifically, the default\n  is now to train a 6 layer Transformer model with word based batching. The only difference to the paper is that weight\n  tying is still turned off by default, as there may be use cases in which tying the source and target vocabularies is\n  not appropriate. Turn it on using `--weight-tying --weight-tying-type=src_trg_softmax`. Additionally, BLEU scores from\n  a checkpoint decoder are now monitored by default.\n\n## [1.18.10]\n### Fixed\n- Re-allow early stopping w.r.t BLEU\n\n## [1.18.9]\n### Fixed\n- Fixed a problem with lhuc boolean flags passed as None.\n\n### Added\n- Reorganized beam search. Normalization is applied only to completed hypotheses, and pruning of\n  hypotheses (logprob against highest-scoring completed hypothesis) can be specified with\n  `--beam-prune X`\n- Enabled stopping at first completed hypothesis with `--beam-search-stop first` (default is 'all')\n\n## [1.18.8]\n### Removed\n- Removed tensorboard logging of embedding & output parameters at every checkpoint. This used a lot of disk space.\n\n## [1.18.7]\n### Added\n- Added support for LHUC in RNN models (David Vilar, \"Learning Hidden Unit\n  Contribution for Adapting Neural Machine Translation Models\" NAACL 2018)\n\n### Fixed\n- Word based batching with very small batch sizes.\n\n## [1.18.6]\n### Fixed\n- Fixed a problem with learning rate scheduler not properly being loaded when resuming training.\n\n## [1.18.5]\n### Fixed\n- Fixed a problem with trainer not waiting for the last checkpoint decoder (#367).\n\n## [1.18.4]\n### Added\n- Added options to control training length w.r.t number of updates/batches or number of samples:\n  `--min-updates`, `--max-updates`, `--min-samples`, `--max-samples`.\n\n## [1.18.3]\n### Changed\n- Training now supports training and validation data that contains empty segments. If a segment is empty, it is skipped\n  during loading and a warning message including the number of empty segments is printed.\n\n## [1.18.2]\n### Changed\n- Removed combined linear projection of keys & values in source attention transformer layers for\n  performance improvements.\n- The topk operator is performed in a single operation during batch decoding instead of running in a loop over each\nsentence, bringing speed benefits in batch decoding.\n\n## [1.18.1]\n### Added\n- Added Tensorboard logging for all parameter values and gradients as histograms/distributions. The logged values\n  correspond to the current batch at checkpoint time.\n\n### Changed\n- Tensorboard logging now is done with the MXNet compatible 'mxboard' that supports logging of all kinds of events\n  (scalars, histograms, embeddings, etc.). If installed, training events are written out to Tensorboard compatible\n  even files automatically.\n\n### Removed\n- Removed the `--use-tensorboard` argument from `sockeye.train`. Tensorboard logging is now enabled by default if\n  `mxboard` is installed.\n\n## [1.18.0]\n### Changed\n- Change default target vocab name in model folder to `vocab.trg.0.json`\n- Changed serialization format of top-k lexica to pickle/Numpy instead of JSON.\n- `sockeye-lexicon` now supports two subcommands: create & inspect.\n  The former provides the same functionality as the previous CLI.\n  The latter allows users to pass source words to the top-k lexicon to inspect the set of allowed target words.\n\n### Added\n- Added ability to choose a smaller `k` at decoding runtime for lexicon restriction.\n\n## [1.17.5]\n### Added\n- Added a flag `--strip-unknown-words` to `sockeye.translate` to remove any `<unk>` symbols from the output strings.\n\n## [1.17.4]\n### Added\n- Added a flag `--fixed-param-names` to prevent certain parameters from being optimized during training.\n  This is useful if you want to keep pre-trained embeddings fixed during training.\n- Added a flag `--dry-run` to `sockeye.train` to not perform any actual training, but print statistics about the model\n  and mode of operation.\n\n## [1.17.3]\n### Changed\n- `sockeye.evaluate` can now handle multiple hypotheses files by simply specifying `--hypotheses file1 file2...`.\nFor each metric the mean and standard deviation will be reported across files.\n\n## [1.17.2]\n### Added\n- Optionally store the beam search history to a `json` output using the `beam_store` output handler.\n\n### Changed\n- Use stack operator instead of expand_dims + concat in RNN decoder. Reduces memory usage.\n\n## [1.17.1]\n### Changed\n - Updated to [MXNet 1.1.0](https://github.com/apache/incubator-mxnet/tree/1.1.0)\n\n## [1.17.0]\n### Added\n - Source factors, as described in\n\n   Linguistic Input Features Improve Neural Machine Translation (Sennrich \\& Haddow, WMT 2016)\n   [PDF](http://www.aclweb.org/anthology/W16-2209.pdf) [bibtex](http://www.aclweb.org/anthology/W16-2209.bib)\n\n   Additional source factors are enabled by passing `--source-factors file1 [file2 ...]` (`-sf`), where file1, etc. are\n   token-parallel to the source (`-s`).\n   An analogous parameter, `--validation-source-factors`, is used to pass factors for validation data.\n   The flag `--source-factors-num-embed D1 [D2 ...]` denotes the embedding dimensions and is required if source factor\n   files are given. Factor embeddings are concatenated to the source embeddings dimension (`--num-embed`).\n\n   At test time, the input sentence and its factors can be passed in via STDIN or command-line arguments.\n   - For STDIN, the input and factors should be in a token-based factored format, e.g.,\n     `word1|factor1|factor2|... w2|f1|f2|... ...1`.\n   - You can also use file arguments, which mirrors training: `--input` takes the path to a file containing the source,\n     and `--input-factors` a list of files containing token-parallel factors.\n   At test time, an exception is raised if the number of expected factors does not\n   match the factors passed along with the input.\n\n - Removed bias parameters from multi-head attention layers of the transformer.\n\n## [1.16.6]\n### Changed\n - Loading/Saving auxiliary parameters of the models. Before aux parameters were not saved or used for initialization.\n Therefore the parameters of certain layers were ignored (e.g., BatchNorm) and randomly initialized. This change\n enables to properly load, save and initialize the layers which use auxiliary parameters.\n\n## [1.16.5]\n### Changed\n - Device locking: Only one process will be acquiring GPUs at a time.\n This will lead to consecutive device ids whenever possible.\n\n## [1.16.4]\n### Changed\n - Internal change: Standardized all data to be batch-major both at training and at inference time.\n\n## [1.16.3]\n### Changed\n - When a device lock file exists and the process has no write permissions for the lock file we assume that the device\n is locked. Previously this lead to an permission denied exception. Please note that in this scenario we an not detect\n if the original Sockeye process did not shut down gracefully. This is not an issue when the sockeye process has write\n permissions on existing lock files as in that case locking is based on file system locks, which cease to exist when a\n process exits.\n\n## [1.16.2]\n### Changed\n - Changed to a custom speedometer that tracks samples/sec AND words/sec. The original MXNet speedometer did not take\n variable batch sizes due to word-based batching into account.\n\n## [1.16.1]\n### Fixed\n - Fixed entry points in `setup.py`.\n\n## [1.16.0]\n### Changed\n - Update to [MXNet 1.0.0](https://github.com/apache/incubator-mxnet/tree/1.0.0) which adds more advanced indexing\nfeatures, benefitting the beam search implementation.\n - `--kvstore` now accepts 'nccl' value. Only works if MXNet was compiled with `USE_NCCL=1`.\n\n### Added\n - `--gradient-compression-type` and `--gradient-compression-threshold` flags to use gradient compression.\n  See [MXNet FAQ on Gradient Compression](https://mxnet.incubator.apache.org/versions/master/faq/gradient_compression.html).\n\n## [1.15.8]\n### Fixed\n - Taking the BOS and EOS tag into account when calculating the maximum input length at inference.\n\n## [1.15.7]\n### Fixed\n - fixed a problem with `--num-samples-per-shard` flag not being parsed as int.\n\n## [1.15.6]\n### Added\n - New CLI `sockeye.prepare_data` for preprocessing the training data only once before training,\n potentially splitting large datasets into shards. At training time only one shard is loaded into memory at a time,\n limiting the maximum memory usage.\n\n### Changed\n - Instead of using the ```--source``` and ```--target``` arguments ```sockeye.train``` now accepts a\n ```--prepared-data``` argument pointing to the folder containing the preprocessed and sharded data. Using the raw\n training data is still possible and now consumes less memory.\n\n## [1.15.5]\n### Added\n - Optionally apply query, key and value projections to the source and target hidden vectors in the CNN model\n before applying the attention mechanism. CLI parameter: `--cnn-project-qkv`.\n\n## [1.15.4]\n### Added\n - A warning will be printed if the checkpoint decoder slows down training.\n\n## [1.15.3]\n### Added\n - Exposing the xavier random number generator through `--weight-init-xavier-rand-type`.\n\n## [1.15.2]\n### Added\n - Exposing MXNet's Nesterov Accelerated Gradient, Adadelta and Adadelta optimizers.\n\n## [1.15.1]\n### Added\n - A tool that initializes embedding weights with pretrained word representations, `sockeye.init_embedding`.\n\n## [1.15.0]\n### Added\n- Added support for Swish-1 (SiLU) activation to transformer models\n([Ramachandran et al. 2017: Searching for Activation Functions](https://arxiv.org/pdf/1710.05941.pdf),\n[Elfwing et al. 2017: Sigmoid-Weighted Linear Units for Neural Network Function Approximation\nin Reinforcement Learning](https://arxiv.org/pdf/1702.03118.pdf)).  Use `--transformer-activation-type swish1`.\n- Added support for GELU activation to transformer models ([Hendrycks and Gimpel 2016: Bridging Nonlinearities and\nStochastic Regularizers with Gaussian Error Linear Units](https://arxiv.org/pdf/1606.08415.pdf).\nUse `--transformer-activation-type gelu`.\n\n## [1.14.3]\n### Changed\n- Fast decoding for transformer models. Caches keys and values of self-attention before softmax.\nChanged decoding flag `--bucket-width` to apply only to source length.\n\n## [1.14.2]\n### Added\n - Gradient norm clipping (`--gradient-clipping-type`) and monitoring.\n### Changed\n - Changed `--clip-gradient` to `--gradient-clipping-threshold` for consistency.\n\n## [1.14.1]\n### Changed\n - Sorting sentences during decoding before splitting them into batches.\n - Default chunk size: The default chunk size when batching is enabled is now batch_size * 500 during decoding to avoid\n  users accidentally forgetting to increase the chunk size.\n\n## [1.14.0]\n### Changed\n - Downscaled fixed positional embeddings for CNN models.\n - Renamed `--monitor-bleu` flag to `--decode-and-evaluate` to illustrate that it computes\n other metrics in addition to BLEU.\n\n### Added\n - `--decode-and-evaluate-use-cpu` flag to use CPU for decoding validation data.\n - `--decode-and-evaluate-device-id` flag to use a separate GPU device for validation decoding. If not specified, the\n existing and still default behavior is to use the last acquired GPU for training.\n\n## [1.13.2]\n### Added\n - A tool that extracts specified parameters from params.x into a .npz file for downstream applications or analysis.\n\n## [1.13.1]\n### Added\n - Added chrF metric\n([Popovic 2015: chrF: character n-gram F-score for automatic MT evaluation](http://www.statmt.org/wmt15/pdf/WMT49.pdf)) to Sockeye.\nsockeye.evaluate now accepts `bleu` and `chrf` as values for `--metrics`\n\n## [1.13.0]\n### Fixed\n - Transformer models do not ignore `--num-embed` anymore as they did silently before.\n As a result there is an error thrown if `--num-embed` != `--transformer-model-size`.\n - Fixed the attention in upper layers (`--rnn-attention-in-upper-layers`), which was previously not passed correctly\n to the decoder.\n### Removed\n - Removed RNN parameter (un-)packing and support for FusedRNNCells (removed `--use-fused-rnns` flag).\n These were not used, not correctly initialized, and performed worse than regular RNN cells. Moreover,\n they made the code much more complex. RNN models trained with previous versions are no longer compatible.\n- Removed the lexical biasing functionality (Arthur ETAL'16) (removed arguments `--lexical-bias`\n and `--learn-lexical-bias`).\n\n## [1.12.2]\n### Changed\n - Updated to [MXNet 0.12.1](https://github.com/apache/incubator-mxnet/releases/tag/0.12.1), which includes an important\n bug fix for CPU decoding.\n\n## [1.12.1]\n### Changed\n - Removed dependency on sacrebleu pip package. Now imports directly from `contrib/`.\n\n## [1.12.0]\n### Changed\n - Transformers now always use the linear output transformation after combining attention heads, even if input & output\n depth do not differ.\n\n## [1.11.2]\n### Fixed\n - Fixed a bug where vocabulary slice padding was defaulting to CPU context.  This was affecting decoding on GPUs with\n very small vocabularies.\n\n## [1.11.1]\n### Fixed\n - Fixed an issue with the use of `ignore` in `CrossEntropyMetric::cross_entropy_smoothed`. This was affecting\n runs with Eve optimizer and label smoothing. Thanks @kobenaxie for reporting.\n\n## [1.11.0]\n### Added\n - Lexicon-based target vocabulary restriction for faster decoding. New CLI for top-k lexicon creation, sockeye.lexicon.\n New translate CLI argument `--restrict-lexicon`.\n\n### Changed\n - Bleu computation based on Sacrebleu.\n\n## [1.10.5]\n### Fixed\n - Fixed yet another bug with the data iterator.\n\n## [1.10.4]\n### Fixed\n - Fixed a bug with the revised data iterator not correctly appending EOS symbols for variable-length batches.\n This reverts part of the commit added in 1.10.1 but is now correct again.\n\n## [1.10.3]\n### Changed\n - Fixed a bug with max_observed_{source,target}_len being computed on the complete data set, not only on the\n sentences actually added to the buckets based on `--max_seq_len`.\n\n## [1.10.2]\n### Added\n - `--max-num-epochs` flag to train for a maximum number of passes through the training data.\n\n## [1.10.1]\n### Changed\n - Reduced memory footprint when creating data iterators: integer sequences\n are streamed from disk when being assigned to buckets.\n\n## [1.10.0]\n### Changed\n - Updated MXNet dependency to 0.12 (w/ MKL support by default).\n - Changed `--smoothed-cross-entropy-alpha` to `--label-smoothing`.\n Label smoothing should now require significantly less memory due to its addition to MXNet's `SoftmaxOutput` operator.\n - `--weight-normalization` now applies not only to convolutional weight matrices, but to output layers of all decoders.\n It is also independent of weight tying.\n - Transformers now use `--embed-dropout`. Before they were using `--transformer-dropout-prepost` for this.\n - Transformers now scale their embedding vectors before adding fixed positional embeddings.\n This turns out to be crucial for effective learning.\n - `.param` files now use 5 digit identifiers to reduce risk of overflowing with many checkpoints.\n\n### Added\n - Added CUDA 9.0 requirements file.\n - `--loss-normalization-type`. Added a new flag to control loss normalization. New default is to normalize\n by the number of valid, non-PAD tokens instead of the batch size.\n - `--weight-init-xavier-factor-type`. Added new flag to control Xavier factor type when `--weight-init=xavier`.\n - `--embed-weight-init`. Added new flag for initialization of embeddings matrices.\n\n### Removed\n - `--smoothed-cross-entropy-alpha` argument. See above.\n - `--normalize-loss` argument. See above.\n\n## [1.9.0]\n### Added\n - Batch decoding. New options for the translate CLI: ``--batch-size`` and ``--chunk-size``. Translator.translate()\n now accepts and returns lists of inputs and outputs.\n\n## [1.8.4]\n### Added\n - Exposing the MXNet KVStore through the ``--kvstore`` argument, potentially enabling distributed training.\n\n## [1.8.3]\n### Added\n - Optional smart rollback of parameters and optimizer states after updating the learning rate\n if not improved for x checkpoints. New flags: ``--learning-rate-decay-param-reset``,\n ``--learning-rate-decay-optimizer-states-reset``\n\n## [1.8.2]\n### Fixed\n - The RNN variational dropout mask is now independent of the input\n (previously any zero initial state led to the first state being canceled).\n - Correctly pass `self.dropout_inputs` float to `mx.sym.Dropout` in `VariationalDropoutCell`.\n\n## [1.8.1]\n### Changed\n - Instead of truncating sentences exceeding the maximum input length they are now translated in chunks.\n\n## [1.8.0]\n### Added\n - Convolutional decoder.\n - Weight normalization (for CNN only so far).\n - Learned positional embeddings for the transformer.\n\n### Changed\n - `--attention-*` CLI params renamed to `--rnn-attention-*`.\n - `--transformer-no-positional-encodings` generalized to `--transformer-positional-embedding-type`.\n", "code_before": "# Changelog\n\nAll notable changes to the project are documented in this file.\n\nVersion numbers are of the form `1.0.0`.\nAny version bump in the last digit is backwards-compatible, in that a model trained with the previous version can still\nbe used for translation with the new version.\nAny bump in the second digit indicates a backwards-incompatible change,\ne.g. due to changing the architecture or simply modifying model parameter names.\nNote that Sockeye has checks in place to not translate with an old model that was trained with an incompatible version.\n\nEach version section may have have subsections for: _Added_, _Changed_, _Removed_, _Deprecated_, and _Fixed_.\n\n## [2.3.23]\n### Changed\n\n- Do not sort BIAS_STATE in beam search. It is constant across decoder steps.\n\n## [2.3.22]\n### Fixed\n\n- The previous commit introduced a regression for vocab creation. The results was that the vocabulary was created on the input characters rather than on tokens.\n\n\n## [2.3.21]\n### Added\n\n- Extended parallelization of data preparation to vocabulary and statistics creation while minimizing the overhead of sharding.\n\n## [2.3.20]\n### Added\n\n- Added debug logging for restrict_lexicon lookups\n\n## [2.3.19]\n### Changed\n\n- When training only the decoder (`--fixed-param-strategy all_except_decoder`), disable autograd for the encoder and embeddings to save memory.\n\n## [2.3.18]\n### Changed\n\n- Updated Docker builds and documentation.  See [sockeye_contrib/docker](sockeye_contrib/docker).\n\n## [2.3.17]\n### Added\n- Added an alternative, faster implementation of greedy search. The '--greedy' flag to `sockeye.translate` will enable it. This implementation does not support hypothesis scores, batch decoding, or lexical constraints.\"\n\n## [2.3.16]\n\n### Added\n- Added option `--transformer-feed-forward-use-glu` to use Gated Linear Units in transformer feed forward networks ([Dauphin et al., 2016](https://arxiv.org/abs/1612.08083); [Shazeer, 2020](https://arxiv.org/abs/2002.05202)).\n\n## [2.3.15]\n\n### Changed\n- Optimization: Decoder class is now a complete HybridBlock (no forward method).\n\n## [2.3.14]\n\n### Changed\n- Updated to [MXNet 1.8.0](https://github.com/apache/incubator-mxnet/tree/1.8.0)\n- Removed dependency support for Cuda 9.2 (no longer supported by MXNet 1.8).\n- Added dependency support for Cuda 11.0 and 11.2.\n- Updated Python requirement to 3.7 and later. (Removed backporting `dataclasses` requirement)\n\n## [2.3.13]\n\n### Added\n- Target factors are now also collected for nbest translations (and stored in the JSON output handler).\n\n## [2.3.12]\n\n### Added\n- Added `--config` option to `prepare_data` CLI to allow setting commandline flags via a yaml config.\n- Flags for the `prepare_data` CLI are now stored in the output folder under `args.yaml`\n  (equivalent to the behavior of `sockeye_train`)\n\n## [2.3.11]\n\n### Added\n- Added option `prevent_unk` to avoid generating `<unk>` token in beam search.\n\n## [2.3.10]\n\n### Changed\n\n- Make sure that the top N best params files retained, even if N > --keep-last-params. This ensures that model\n  averaging will not be crippled when keeping only a few params files during training. This can result in a\n  significant savings of disk space during training.\n\n## [2.3.9]\n\n### Added\n\n- Added scripts for processing Sockeye benchmark output (`--output-type benchmark`):\n  - [benchmark_to_output.py](sockeye_contrib/benchmark/benchmark_to_output.py) extracts translations\n  - [benchmark_to_percentiles.py](sockeye_contrib/benchmark/benchmark_to_percentiles.py) computes percentiles\n\n## [2.3.8]\n\n### Fixed\n\n- Fix problem identified in issue #925 that caused learning rate\n  warmup to fail in some instances when doing continued training\n\n## [2.3.7]\n\n### Changed\n\n- Use dataclass module to simplify Config classes. No functional change.\n\n## [2.3.6]\n\n### Fixed\n\n- Fixes the problem identified in issue #890, where the lr_scheduler\n  does not behave as expected when continuing training. The problem is\n  that the lr_scheduler is kept as part of the optimizer, but the\n  optimizer is not saved when saving state. Therefore, every time\n  training is restarted, a new lr_scheduler is created with initial\n  parameter settings. Fix by saving and restoring the lr_scheduling\n  separately.\n\n## [2.3.5]\n\n### Fixed\n\n- Fixed issue with LearningRateSchedulerPlateauReduce.__repr__ printing\n\tout num_not_improved instead of reduce_num_not_improved.\n\n## [2.3.4]\n\n### Fixed\n\n- Fixed issue with dtype mismatch in beam search when translating with `--dtype float16`.\n\n## [2.3.3]\n\n### Changed\n\n- Upgraded `SacreBLEU` dependency of Sockeye to a newer version (`1.4.14`).\n\n## [2.3.2]\n### Fixed\n\n- Fixed edge case that unintentionally skips softmax for sampling if beam size is 1.\n\n## [2.3.1]\n### Fixed\n\n- Optimizing for BLEU/CHRF with horovod required the secondary workers to also create checkpoint decoders.\n\n## [2.3.0]\n\n### Added\n\n- Added support for target factors.\n  If provided with additional target-side tokens/features (token-parallel to the regular target-side) at training time,\n  the model can now learn to predict these in a multi-task setting. You can provide target factor data similar to source\n  factors: `--target-factors <factor_file1> [<factor_fileN>]`. During training, Sockeye optimizes one loss per factor\n  in a multi-task setting. The weight of the losses can be controlled by `--target-factors-weight`.\n  At inference, target factors are decoded greedily, they do not participate in beam search.\n  The predicted factor at each time step is the argmax over its separate output\n  layer distribution. To receive the target factor predictions at inference time, use\n  `--output-type translation_with_factors`.\n\n### Changed\n\n- `load_model(s)` now returns a list of target vocabs.\n- Default source factor combination changed to `sum` (was `concat` before).\n- `SockeyeModel` class has three new properties: `num_target_factors`, `target_factor_configs`,\n  and `factor_output_layers`.\n\n## [2.2.8]\n\n### Changed\n- Make source/target data parameters required for the scoring CLI to avoid cryptic error messages.\n\n## [2.2.7]\n\n### Added\n\n- Added an argument to specify the log level of secondary workers. Defaults to ERROR to hide any logs except for exceptions.\n\n## [2.2.6]\n\n### Fixed\n- Avoid a crash due to an edge case when no model improvement has been observed by the time the learning rate gets reduced for the first time.\n\n## [2.2.5]\n\n### Fixed\n- Enforce sentence batching for sockeye score tool, set default batch size to 56\n\n## [2.2.4]\n\n### Changed\n- Use softmax with length in DotAttentionCell.\n- Use `contrib.arange_like` in AutoRegressiveBias block to reduce number of ops.\n\n## [2.2.3]\n\n### Added\n\n- Log the absolute number of `<unk>` tokens in source and target data\n\n## [2.2.2]\n\n### Fixed\n\n- Fix: Guard against null division for small batch sizes.\n\n## [2.2.1]\n\n## Fixed\n\n- Fixes a corner case bug by which the beam decoder can wrongly return a best hypothesis with -infinite score.\n\n## [2.2.0]\n\n### Changed\n\n- Replaced multi-head attention with [interleaved_matmul_encdec](https://github.com/apache/incubator-mxnet/pull/16408) operators, which removes previously needed transposes and improves performance.\n\n- Beam search states and model layers now assume time-major format.\n\n## [2.1.26]\n\n### Fixed\n\n- Fixes a backwards incompatibility introduced in 2.1.17, which would prevent models trained with prior versions to be used for inference.\n\n## [2.1.25]\n\n### Changed\n\n- Reverting PR #772 as it causes issues with `amp`.\n\n## [2.1.24]\n\n### Changed\n\n- Make sure to write a final checkpoint when stopping with `--max-updates`, `--max-samples` or `--max-num-epochs`.\n\n## [2.1.23]\n\n### Changed\n\n- Updated to [MXNet 1.7.0](https://github.com/apache/incubator-mxnet/tree/1.7.0).\n- Re-introduced use of softmax with length parameter in DotAttentionCell (see PR #772).\n\n## [2.1.22]\n\n### Added\n\n- Re-introduced `--softmax-temperature` flag for `sockeye.score` and `sockeye.translate`.\n\n## [2.1.21]\n\n### Added\n\n- Added an optional ability to cache encoder outputs of model.\n\n## [2.1.20]\n\n### Fixed\n\n- Fixed a bug where the training state object was saved to disk before training metrics were added to it, leading to an inconsistency between the training state object and the metrics file (see #859).\n\n## [2.1.19]\n\n### Fixed\n\n- When loading a shard in Horovod mode, there is now a check that each non-empty bucket contains enough sentences to cover each worker's slice. If not, the bucket's sentences are replicated to guarantee coverage.\n\n## [2.1.18]\n\n### Fixed\n\n- Fixed a bug where sampling translation fails because an array is created in the wrong context.\n\n## [2.1.17]\n\n### Added\n\n- Added `layers.SSRU`, which implements a Simpler Simple Recurrent Unit as described in\nKim et al, \"From Research to Production and Back: Ludicrously Fast Neural Machine Translation\" WNGT 2019.\n\n- Added `ssru_transformer` option to `--decoder`, which enables the usage of SSRUs as a replacement for the decoder-side self-attention layers.\n\n### Changed\n\n- Reduced the number of arguments for `MultiHeadSelfAttention.hybrid_forward()`.\n `previous_keys` and `previous_values` should now be input together as `previous_states`, a list containing two symbols.\n\n## [2.1.16]\n\n### Fixed\n\n- Fixed batch sizing error introduced in version 2.1.12 (c00da52) that caused batch sizes to be multiplied by the number of devices. Batch sizing now works as documented (same as pre-2.1.12 versions).\n- Fixed `max-word` batching to properly size batches to a multiple of both `--batch-sentences-multiple-of` and the number of devices.\n\n## [2.1.15]\n\n### Added\n\n- Inference option `--mc-dropout` to use dropout during inference, leading to non-deterministic output. This option uses the same dropout parameters present in the model config file.\n\n## [2.1.14]\n\n### Added\n\n- Added `sockeye.rerank` option `--output` to specify output file.\n- Added `sockeye.rerank` option `--output-reference-instead-of-blank` to output reference line instead of best hypothesis when best hypothesis is blank.\n\n\n## [2.1.13]\n\n### Added\n\n- Training option `--quiet-secondary-workers` that suppresses console output for secondary workers when training with Horovod/MPI.\n- Set version of isort to `<5.0.0` in requirements.dev.txt to avoid incompatibility between newer versions of isort and pylint.\n\n## [2.1.12]\n\n### Added\n\n- Batch type option `max-word` for max number of words including padding tokens (more predictable memory usage than `word`).\n- Batching option `--batch-sentences-multiple-of` that is similar to `--round-batch-sizes-to-multiple-of` but always rounds down (more predictable memory usage).\n\n### Changed\n\n- Default bucketing settings changed to width 8, max sequence length 95 (96 including BOS/EOS tokens), and no bucket scaling.\n- Argument `--no-bucket-scaling` replaced with `--bucket-scaling` which is False by default.\n\n## [2.1.11]\n\n### Changed\n\n- Updated `sockeye.rerank` module to use \"add-k\" smoothing for sentence-level BLEU.\n\n### Fixed\n\n- Updated `sockeye.rerank` module to use current N-best format.\n\n## [2.1.10]\n\n### Changed\n\n- Changed to a cross-entropy loss implementation that avoids the use of SoftmaxOutput.\n\n## [2.1.9]\n\n### Added\n\n- Added training argument `--ignore-extra-params` to ignore extra parameters when loading models.  The primary use case is continuing training with a model that has already been annotated with scaling factors (`sockeye.quantize`).\n\n### Fixed\n\n- Properly pass `allow_missing` flag to `model.load_parameters()`\n\n## [2.1.8]\n\n### Changed\n\n- Update to sacrebleu=1.4.10\n\n## [2.1.7]\n\n### Changed\n\n- Optimize prepare_data by saving the shards in parallel. The prepare_data script accepts a new parameter `--max-processes` to control the level of parallelism with which shards are written to disk.\n\n## [2.1.6]\n\n### Changed\n\n- Updated Dockerfiles optimized for CPU (intgemm int8 inference, full MKL support) and GPU (distributed training with Horovod).  See [sockeye_contrib/docker](sockeye_contrib/docker).\n\n### Added\n\n- Official support for int8 quantization with [intgemm](https://github.com/kpu/intgemm):\n  - This requires the \"intgemm\" fork of MXNet ([kpuatamazon/incubator-mxnet/intgemm](https://github.com/kpuatamazon/incubator-mxnet/tree/intgemm)).  This is the version of MXNet used in the Sockeye CPU docker image (see [sockeye_contrib/docker](sockeye_contrib/docker)).\n  - Use `sockeye.translate --dtype int8` to quantize a trained float32 model at runtime.\n  - Use the `sockeye.quantize` CLI to annotate a float32 model with int8 scaling factors for fast runtime quantization.\n\n## [2.1.5]\n\n### Changed\n\n- Changed state caching for transformer models during beam search to cache states with attention heads already separated out. This avoids repeated transpose operations during decoding, leading to faster inference.\n\n## [2.1.4]\n\n### Added\n\n- Added Dockerfiles that build an experimental CPU-optimized Sockeye image:\n  - Uses the latest versions of [kpuatamazon/incubator-mxnet](https://github.com/kpuatamazon/incubator-mxnet) (supports [intgemm](https://github.com/kpu/intgemm) and makes full use of Intel MKL) and [kpuatamazon/sockeye](https://github.com/kpuatamazon/sockeye) (supports int8 quantization for inference).\n  - See [sockeye_contrib/docker](sockeye_contrib/docker).\n\n## [2.1.3]\n\n### Changed\n\n- Performance optimizations to beam search inference\n  - Remove unneeded take ops on encoder states\n  - Gathering input data before sending to GPU, rather than sending each batch element individually\n  - All of beam search can be done in fp16, if specified by the model\n  - Other small miscellaneous optimizations\n- Model states are now a flat list in ensemble inference, structure of states provided by `state_structure()`\n\n## [2.1.2]\n\n### Changed\n\n- Updated to [MXNet 1.6.0](https://github.com/apache/incubator-mxnet/tree/1.6.0)\n\n### Added\n\n- Added support for CUDA 10.2\n\n### Removed\n\n- Removed support for CUDA<9.1 / CUDNN<7.5\n\n## [2.1.1]\n\n### Added\n- Ability to set environment variables from training/translate CLIs before MXNet is imported. For example, users can\n  configure MXNet as such: `--env \"OMP_NUM_THREADS=1;MXNET_ENGINE_TYPE=NaiveEngine\"`\n\n## [2.1.0]\n\n### Changed\n\n- Version bump, which should have been included in commit b0461b due to incompatible models.\n\n## [2.0.1]\n\n### Changed\n\n- Inference defaults to using the max input length observed in training (versus scaling down based on mean length ratio and standard deviations).\n\n### Added\n\n- Additional parameter fixing strategies:\n  - `all_except_feed_forward`: Only train feed forward layers.\n  - `encoder_and_source_embeddings`: Only train the decoder (decoder layers, output layer, and target embeddings).\n  - `encoder_half_and_source_embeddings`: Train the latter half of encoder layers and the decoder.\n- Option to specify the number of CPU threads without using an environment variable (`--omp-num-threads`).\n- More flexibility for source factors combination\n\n## [2.0.0]\n\n### Changed\n\n- Update to [MXNet 1.5.0](https://github.com/apache/incubator-mxnet/tree/1.5.0)\n- Moved `SockeyeModel` implementation and all layers to [Gluon API](http://mxnet.incubator.apache.org/versions/master/gluon/index.html)\n- Removed support for Python 3.4.\n- Removed image captioning module\n- Removed outdated Autopilot module\n- Removed unused training options: Eve, Nadam, RMSProp, Nag, Adagrad, and Adadelta optimizers, `fixed-step` and `fixed-rate-inv-t` learning rate schedulers\n- Updated and renamed learning rate scheduler `fixed-rate-inv-sqrt-t` -> `inv-sqrt-decay`\n- Added script for plotting metrics files: [sockeye_contrib/plot_metrics.py](sockeye_contrib/plot_metrics.py)\n- Removed option `--weight-tying`.  Weight tying is enabled by default, disable with `--weight-tying-type none`.\n\n### Added\n\n- Added distributed training support with Horovod/MPI.  Use `horovodrun` and the `--horovod` training flag.\n- Added Dockerfiles that build a Sockeye image with all features enabled.  See [sockeye_contrib/docker](sockeye_contrib/docker).\n- Added `none` learning rate scheduler (use a fixed rate throughout training)\n- Added `linear-decay` learning rate scheduler\n- Added training option `--learning-rate-t-scale` for time-based decay schedulers\n- Added support for MXNet's [Automatic Mixed Precision](https://mxnet.incubator.apache.org/versions/master/tutorials/amp/amp_tutorial.html).  Activate with the `--amp` training flag.  For best results, make sure as many model dimensions are possible are multiples of 8.\n- Added options for making various model dimensions multiples of a given value.  For example, use `--pad-vocab-to-multiple-of 8`, `--bucket-width 8 --no-bucket-scaling`, and `--round-batch-sizes-to-multiple-of 8` with AMP training.\n- Added [GluonNLP](http://gluon-nlp.mxnet.io/)'s BERTAdam optimizer, an implementation of the Adam variant used by Devlin et al. ([2018](https://arxiv.org/pdf/1810.04805.pdf)).  Use `--optimizer bertadam`.\n- Added training option `--checkpoint-improvement-threshold` to set the amount of metric improvement required over the window of previous checkpoints to be considered actual model improvement (used with `--max-num-checkpoint-not-improved`).\n\n## [1.18.103]\n### Added\n- Added ability to score image-sentence pairs by extending the scoring feature originally implemented for machine\n  translation to the image captioning module.\n\n## [1.18.102]\n### Fixed\n- Fixed loading of more than 10 source vocabulary files to be in the right, numerical order.\n\n## [1.18.101]\n### Changed\n- Update to Sacrebleu 1.3.6\n\n## [1.18.100]\n### Fixed\n- Always initializing the multiprocessing context. This should fix issues observed when running `sockeye-train`.\n\n## [1.18.99]\n### Changed\n- Updated to [MXNet 1.4.1](https://github.com/apache/incubator-mxnet/tree/1.4.1)\n\n## [1.18.98]\n### Changed\n- Converted several transformer-related layer implementations to Gluon HybridBlocks. No functional change.\n\n## [1.18.97]\n### Changed\n- Updated to PyYAML 5.1\n\n## [1.18.96]\n### Changed\n- Extracted prepare vocab functionality in the build vocab step into its own function. This matches the pattern in prepare data and train where the main() function only has argparsing, and it invokes a separate function to do the work. This is to allow modules that import this one to circumvent the command line.\n\n## [1.18.95]\n### Changed\n- Removed custom operators from transformer models and replaced them with symbolic operators.\n  Improves Performance.\n\n## [1.18.94]\n### Added\n- Added ability to accumulate gradients over multiple batches (--update-interval). This allows simulation of large\n  batch sizes on environments with limited memory. For example: training with `--batch-size 4096 --update-interval 2`\n  should be close to training with `--batch-size 8192` at smaller memory footprint.\n\n## [1.18.93]\n### Fixed\n- Made `brevity_penalty` argument in `Translator` class optional to ensure backwards compatibility.\n\n## [1.18.92]\n### Added\n- Added sentence length (and length ratio) prediction to be able to discourage hypotheses that are too short at inference time. Can be enabled for training with `--length-task` and with `--brevity-penalty-type` during inference.\n\n## [1.18.91]\n### Changed\n- Multiple lexicons can now be specified with the `--restrict-lexicon` option:\n  - For a single lexicon: `--restrict-lexicon /path/to/lexicon`.\n  - For multiple lexicons: `--restrict-lexicon key1:/path/to/lexicon1 key2:/path/to/lexicon2 ...`.\n  - Use `--json-input` to specify the lexicon to use for each input, ex: `{\"text\": \"some input string\", \"restrict_lexicon\": \"key1\"}`.\n\n## [1.18.90]\n### Changed\n- Updated to [MXNet 1.4.0](https://github.com/apache/incubator-mxnet/tree/1.4.0)\n- Integration tests no longer check for equivalence of outputs with batch size 2\n\n## [1.18.89]\n### Fixed\n- Made the length ratios per bucket change backwards compatible.\n\n## [1.18.88]\n### Changed\n- Made sacrebleu a pip dependency and removed it from `sockeye_contrib`.\n\n## [1.18.87]\n### Added\n- Data statistics at training time now compute mean and standard deviation of length ratios per bucket.\n  This information is stored in the model's config, but not used at the moment.\n\n## [1.18.86]\n### Added\n- Added the `--fixed-param-strategy` option that allows fixing various model parameters during training via named strategies.\n  These include some of the simpler combinations from [Wuebker et al. (2018)](https://arxiv.org/abs/1811.01990) such as fixing everything except the first and last layers of the encoder and decoder (`all_except_outer_layers`).  See the help message for a full list of strategies.\n\n## [1.18.85]\n### Changed\n- Disabled dynamic batching for `Translator.translate()` by default due to increased memory usage. The default is to\n  fill-up batches to `Translator.max_batch_size`.\n  Dynamic batching can still be enabled if `fill_up_batches` is set to False.\n### Added\n- Added parameter to force training to stop after a given number of checkpoints. Useful when forced to share limited GPU resources.\n\n## [1.18.84]\n### Fixed\n- Fixed lexical constraints bugs that broke batching and caused large drop in BLEU.\n  These were introduced with sampling (1.18.64).\n\n## [1.18.83]\n### Changed\n - The embedding size is automatically adjusted to the Transformer model size in case it is not specified on the command line.\n\n## [1.18.82]\n### Fixed\n- Fixed type conversion in metrics file reading introduced in 1.18.79.\n\n## [1.18.81]\n### Fixed\n- Making sure the training pickled training state contains the checkpoint decoder's BLEU score of the last checkpoint.\n\n## [1.18.80]\n### Fixed\n- Fixed a bug introduced in 1.18.77 where blank lines in the training data resulted in failure.\n\n## [1.18.79]\n### Added\n- Writing of the convergence/divergence status to the metrics file and guarding against numpy.histogram's errors for NaNs during divergent behaviour.\n\n## [1.18.78]\n### Changed\n- Dynamic batch sizes: `Translator.translate()` will adjust batch size in beam search to the actual number of inputs without using padding.\n\n## [1.18.77]\n### Added\n- `sockeye.score` now loads data on demand and doesn't skip any input lines\n\n## [1.18.76]\n### Changed\n- Do not compare scores from translation and scoring in integration tests.\n\n### Added\n- Adding the option via the flag `--stop-training-on-decoder-failure` to stop training in case the checkpoint decoder dies (e.g. because there is not enough memory).\nIn case this is turned on a checkpoint decoder is launched right when training starts in order to fail as early as possible.\n\n## [1.18.75]\n### Changed\n- Do not create dropout layers for inference models for performance reasons.\n\n## [1.18.74]\n### Changed\n- Revert change in 1.18.72 as no memory saving could be observed.\n\n## [1.18.73]\n### Fixed\n- Fixed a bug where `source-factors-num-embed` was not correctly adjusted to `num-embed`\n  when using prepared data & `source-factor-combine` sum.\n\n## [1.18.72]\n### Changed\n- Removed use of `expand_dims` in favor of `reshape` to save memory.\n\n## [1.18.71]\n### Fixed\n- Fixed default setting of source factor combination to be 'concat' for backwards compatibility.\n\n## [1.18.70]\n### Added\n- Sockeye now outputs fields found in a JSON input object, if they are not overwritten by Sockeye. This behavior can be enabled by selecting `--json-input` (to read input as a JSON object) and `--output-type json` (to write a JSON object to output).\n\n## [1.18.69]\n### Added\n- Source factors can now be added to the embeddings instead of concatenated with `--source-factors-combine sum` (default: concat)\n\n## [1.18.68]\n- Fixed training crashes with `--learning-rate-decay-optimizer-states-reset initial` option.\n\n## [1.18.67]\n### Added\n- Added `fertility` as a further type of attention coverage.\n- Added an option for training to keep the initializations of the model via `--keep-initializations`. When set, the trainer will avoid deleting the params file for the first checkpoint, no matter what `--keep-last-params` is set to.\n\n## [1.18.66]\n### Fixed\n- Fix to argument names that are allowed to differ for resuming training.\n\n## [1.18.65]\n### Changed\n- More informative error message about inconsistent --shared-vocab setting.\n\n## [1.18.64]\n### Added\n- Adding translation sampling via `--sample [N]`. This causes the decoder to sample each next step from the target distribution probabilities at each\n  timestep. An optional value of `N` causes the decoder to sample only from the top `N` vocabulary items for each hypothesis at each timestep (the\n  default is 0, meaning to sample from the entire vocabulary).\n\n## [1.18.63]\n### Changed\n- The checkpoint decoder and nvidia-smi subprocess are now launched from a forkserver, allowing for a better separation between processes.\n\n## [1.18.62]\n### Added\n- Add option to make `TranslatorInputs` directly from a dict.\n\n## [1.18.61]\n### Changed\n- Update to MXNet 1.3.1. Removed requirements/requirements.gpu-cu{75,91}.txt as CUDA 7.5 and 9.1 are deprecated.\n\n## [1.18.60]\n### Fixed\n- Performance optimization to skip the softmax operation for single model greedy decoding is now only applied if no translation scores are required in the output.\n\n## [1.18.59]\n### Added\n- Full training state is now returned from EarlyStoppingTrainer's fit().\n### Changed\n- Training state cleanup will not be performed for training runs that did not converge yet.\n- Switched to portalocker for locking files (Windows compatibility).\n\n## [1.18.58]\n### Added\n- Added nbest translation, exposed as `--nbest-size`. Nbest translation means to not only output the most probable translation according to a model, but the top n most probable hypotheses. If `--nbest-size > 1` and the option `--output-type` is not explicitly specified, the output type will be changed to one JSON list of nbest translations per line. `--nbest-size` can never be larger than `--beam-size`.\n\n### Changed\n- Changed `sockeye.rerank` CLI to be compatible with nbest translation JSON output format.\n\n## [1.18.57]\n### Added\n- Added `sockeye.score` CLI for quickly scoring existing translations ([documentation](tutorials/scoring.md)).\n### Fixed\n- Entry-point clean-up after the contrib/ rename\n\n## [1.18.56]\n### Changed\n- Update to MXNet 1.3.0.post0\n\n## [1.18.55]\n- Renamed `contrib` to less-generic `sockeye_contrib`\n\n## [1.18.54]\n### Added\n- `--source-factor-vocabs` can be set to provide source factor vocabularies.\n\n## [1.18.53]\n### Added\n- Always skipping softmax for greedy decoding by default, only for single models.\n- Added option `--skip-topk` for greedy decoding.\n\n## [1.18.52]\n### Fixed\n- Fixed bug in constrained decoding to make sure best hypothesis satifies all constraints.\n\n## [1.18.51]\n### Added\n- Added a CLI for reranking of an nbest list of translations.\n\n## [1.18.50]\n### Fixed\n- Check for equivalency of training and validation source factors was incorrectly indented.\n\n## [1.18.49]\n### Changed\n- Removed dependence on the nvidia-smi tool. The number of GPUs is now determined programatically.\n\n## [1.18.48]\n### Changed\n- Translator.max_input_length now reports correct maximum input length for TranslatorInput objects, independent of the internal representation, where an additional EOS gets added.\n\n## [1.18.47]\n### Changed\n- translate CLI: no longer rely on external, user-given input id for sorting translations. Also allow string ids for sentences.\n\n## [1.18.46]\n### Fixed\n- Fixed issue with `--num-words 0:0` in image captioning and another issue related to loading all features to memory with variable length.\n\n## [1.18.45]\n### Added\n- Added an 8 layer LSTM model similar (but not exactly identical) to the 'GNMT' architecture to autopilot.\n\n## [1.18.44]\n### Fixed\n- Fixed an issue with `--max-num-epochs` causing training to stop before the update/batch that actually completes the epoch was made.\n\n## [1.18.43]\n### Added\n- `<s>` now supported as the first token in a multi-word negative constraint\n  (e.g., `<s> I think` to prevent a sentence from starting with `I think`)\n### Fixed\n- Bugfix in resetting the state of a multiple-word negative constraint\n\n## [1.18.42]\n### Changed\n- Simplified gluon blocks for length calculation\n\n## [1.18.41]\n### Changed\n- Require numpy 1.14 or later to avoid MKL conflicts between numpy as mxnet-mkl.\n\n## [1.18.40]\n### Fixed\n- Fixed bad check for existence of negative constraints.\n- Resolved conflict for phrases that are both positive and negative constraints.\n- Fixed softmax temperature at inference time.\n\n## [1.18.39]\n### Added\n- Image Captioning now supports constrained decoding.\n- Image Captioning: zero padding of features now allows input features of different shape for each image.\n\n## [1.18.38]\n### Fixed\n- Fixed issue with the incorrect order of translations when empty inputs are present and translating in chunks.\n\n## [1.18.37]\n### Fixed\n- Determining the max output length for each sentence in a batch by the bucket length rather than the actual in order to match the behavior of a single sentence translation.\n\n## [1.18.36]\n### Changed\n- Updated to [MXNet 1.2.1](https://github.com/apache/incubator-mxnet/tree/1.2.1)\n\n## [1.18.35]\n### Added\n- ROUGE scores are now available in `sockeye-evaluate`.\n- Enabled CHRF as an early-stopping metric.\n\n## [1.18.34]\n### Added\n- Added support for `--beam-search-stop first` for decoding jobs with `--batch-size > 1`.\n\n## [1.18.33]\n### Added\n- Now supports negative constraints, which are phrases that must *not* appear in the output.\n   - Global constraints can be listed in a (pre-processed) file, one per line: `--avoid-list FILE`\n   - Per-sentence constraints are passed using the `avoid` keyword in the JSON object, with a list of strings as its field value.\n\n## [1.18.32]\n### Added\n- Added option to pad vocabulary to a multiple of x: e.g. `--pad-vocab-to-multiple-of 16`.\n\n## [1.18.31]\n### Added\n- Pre-training the RNN decoder. Usage:\n  1. Train with flag `--decoder-only`.\n  2. Feed identical source/target training data.\n\n## [1.18.30]\n### Fixed\n- Preserving max output length for each sentence to allow having identical translations for both with and without batching.\n\n## [1.18.29]\n### Changed\n- No longer restrict the vocabulary to 50,000 words by default, but rather create the vocabulary from all words which occur at least `--word-min-count` times. Specifying `--num-words` explicitly will still lead to a restricted\n  vocabulary.\n\n## [1.18.28]\n### Changed\n- Temporarily fixing the pyyaml version to 3.12 as version 4.1 introduced some backwards incompatible changes.\n\n## [1.18.27]\n### Fixed\n- Fix silent failing of NDArray splits during inference by using a version that always returns a list. This was causing incorrect behavior when using lexicon restriction and batch inference with a single source factor.\n\n## [1.18.26]\n### Added\n- ROUGE score evaluation. It can be used as the stopping criterion for tasks such as summarization.\n\n## [1.18.25]\n### Changed\n- Update requirements to use MKL versions of MXNet for fast CPU operation.\n\n## [1.18.24]\n### Added\n- Dockerfiles and convenience scripts for running `fast_align` to generate lexical tables.\nThese tables can be used to create top-K lexicons for faster decoding via vocabulary selection ([documentation](https://github.com/awslabs/sockeye/tree/master/contrib/fast_align)).\n\n### Changed\n- Updated default top-K lexicon size from 20 to 200.\n\n## [1.18.23]\n### Fixed\n- Correctly create the convolutional embedding layers when the encoder is set to `transformer-with-conv-embed`. Previously\nno convolutional layers were added so that a standard Transformer model was trained instead.\n\n## [1.18.22]\n### Fixed\n- Make sure the default bucket is large enough with word based batching when the source is longer than the target (Previously\nthere was an edge case where the memory usage was sub-optimal with word based batching and longer source than target sentences).\n\n## [1.18.21]\n### Fixed\n- Constrained decoding was missing a crucial cast\n- Fixed test cases that should have caught this\n\n## [1.18.20]\n### Changed\n- Transformer parametrization flags (model size, # of attention heads, feed-forward layer size) can now optionally\n  defined separately for encoder & decoder. For example, to use a different transformer model size for the encoder,\n  pass `--transformer-model-size 1024:512`.\n\n## [1.18.19]\n### Added\n- LHUC is now supported in transformer models\n\n## [1.18.18]\n### Added\n- \\[Experimental\\] Introducing the image captioning module. Type of models supported: ConvNet encoder - Sockeye NMT decoders. This includes also a feature extraction script,\nan image-text iterator that loads features, training and inference pipelines and a visualization script that loads images and captions.\nSee [this tutorial](tutorials/image_captioning) for its usage. This module is experimental therefore its maintenance is not fully guaranteed.\n\n## [1.18.17]\n### Changed\n- Updated to MXNet 1.2\n- Use of the new LayerNormalization operator to save GPU memory.\n\n## [1.18.16]\n### Fixed\n- Removed summation of gradient arrays when logging gradients.\n  This clogged the memory on the primary GPU device over time when many checkpoints were done.\n  Gradient histograms are now logged to Tensorboard separated by device.\n\n## [1.18.15]\n### Added\n- Added decoding with target-side lexical constraints (documentation in `tutorials/constraints`).\n\n## [1.18.14]\n### Added\n- Introduced Sockeye Autopilot for single-command end-to-end system building.\nSee the [Autopilot documentation]((https://github.com/awslabs/sockeye/tree/master/contrib/autopilot)) and run with: `sockeye-autopilot`.\nAutopilot is a `contrib` module with its own tests that are run periodically.\nIt is not included in the comprehensive tests run for every commit.\n\n## [1.18.13]\n### Fixed\n- Fixed two bugs with training resumption:\n  1. removed overly strict assertion in the data iterator for model states before the first checkpoint.\n  2. removed deletion of Tensorboard log directory.\n\n### Added\n- Added support for config files. Command line parameters have precedence over the values read from the config file.\n  Minimal working example:\n  `python -m sockeye.train --config config.yaml` with contents of `config.yaml` as follows:\n  ```yaml\n  source: source.txt\n  target: target.txt\n  output: out\n  validation_source: valid.source.txt\n  validation_target: valid.target.txt\n  ```\n### Changed\n  The full set of arguments is serialized to `out/args.yaml` at the beginning of training (before json was used).\n\n## [1.18.12]\n### Changed\n- All source side sequences now get appended an additional end-of-sentence (EOS) symbol. This change is backwards\n  compatible meaning that inference with older models will still work without the EOS symbol.\n\n## [1.18.11]\n### Changed\n- Default training parameters have been changed to reflect the setup used in our arXiv paper. Specifically, the default\n  is now to train a 6 layer Transformer model with word based batching. The only difference to the paper is that weight\n  tying is still turned off by default, as there may be use cases in which tying the source and target vocabularies is\n  not appropriate. Turn it on using `--weight-tying --weight-tying-type=src_trg_softmax`. Additionally, BLEU scores from\n  a checkpoint decoder are now monitored by default.\n\n## [1.18.10]\n### Fixed\n- Re-allow early stopping w.r.t BLEU\n\n## [1.18.9]\n### Fixed\n- Fixed a problem with lhuc boolean flags passed as None.\n\n### Added\n- Reorganized beam search. Normalization is applied only to completed hypotheses, and pruning of\n  hypotheses (logprob against highest-scoring completed hypothesis) can be specified with\n  `--beam-prune X`\n- Enabled stopping at first completed hypothesis with `--beam-search-stop first` (default is 'all')\n\n## [1.18.8]\n### Removed\n- Removed tensorboard logging of embedding & output parameters at every checkpoint. This used a lot of disk space.\n\n## [1.18.7]\n### Added\n- Added support for LHUC in RNN models (David Vilar, \"Learning Hidden Unit\n  Contribution for Adapting Neural Machine Translation Models\" NAACL 2018)\n\n### Fixed\n- Word based batching with very small batch sizes.\n\n## [1.18.6]\n### Fixed\n- Fixed a problem with learning rate scheduler not properly being loaded when resuming training.\n\n## [1.18.5]\n### Fixed\n- Fixed a problem with trainer not waiting for the last checkpoint decoder (#367).\n\n## [1.18.4]\n### Added\n- Added options to control training length w.r.t number of updates/batches or number of samples:\n  `--min-updates`, `--max-updates`, `--min-samples`, `--max-samples`.\n\n## [1.18.3]\n### Changed\n- Training now supports training and validation data that contains empty segments. If a segment is empty, it is skipped\n  during loading and a warning message including the number of empty segments is printed.\n\n## [1.18.2]\n### Changed\n- Removed combined linear projection of keys & values in source attention transformer layers for\n  performance improvements.\n- The topk operator is performed in a single operation during batch decoding instead of running in a loop over each\nsentence, bringing speed benefits in batch decoding.\n\n## [1.18.1]\n### Added\n- Added Tensorboard logging for all parameter values and gradients as histograms/distributions. The logged values\n  correspond to the current batch at checkpoint time.\n\n### Changed\n- Tensorboard logging now is done with the MXNet compatible 'mxboard' that supports logging of all kinds of events\n  (scalars, histograms, embeddings, etc.). If installed, training events are written out to Tensorboard compatible\n  even files automatically.\n\n### Removed\n- Removed the `--use-tensorboard` argument from `sockeye.train`. Tensorboard logging is now enabled by default if\n  `mxboard` is installed.\n\n## [1.18.0]\n### Changed\n- Change default target vocab name in model folder to `vocab.trg.0.json`\n- Changed serialization format of top-k lexica to pickle/Numpy instead of JSON.\n- `sockeye-lexicon` now supports two subcommands: create & inspect.\n  The former provides the same functionality as the previous CLI.\n  The latter allows users to pass source words to the top-k lexicon to inspect the set of allowed target words.\n\n### Added\n- Added ability to choose a smaller `k` at decoding runtime for lexicon restriction.\n\n## [1.17.5]\n### Added\n- Added a flag `--strip-unknown-words` to `sockeye.translate` to remove any `<unk>` symbols from the output strings.\n\n## [1.17.4]\n### Added\n- Added a flag `--fixed-param-names` to prevent certain parameters from being optimized during training.\n  This is useful if you want to keep pre-trained embeddings fixed during training.\n- Added a flag `--dry-run` to `sockeye.train` to not perform any actual training, but print statistics about the model\n  and mode of operation.\n\n## [1.17.3]\n### Changed\n- `sockeye.evaluate` can now handle multiple hypotheses files by simply specifying `--hypotheses file1 file2...`.\nFor each metric the mean and standard deviation will be reported across files.\n\n## [1.17.2]\n### Added\n- Optionally store the beam search history to a `json` output using the `beam_store` output handler.\n\n### Changed\n- Use stack operator instead of expand_dims + concat in RNN decoder. Reduces memory usage.\n\n## [1.17.1]\n### Changed\n - Updated to [MXNet 1.1.0](https://github.com/apache/incubator-mxnet/tree/1.1.0)\n\n## [1.17.0]\n### Added\n - Source factors, as described in\n\n   Linguistic Input Features Improve Neural Machine Translation (Sennrich \\& Haddow, WMT 2016)\n   [PDF](http://www.aclweb.org/anthology/W16-2209.pdf) [bibtex](http://www.aclweb.org/anthology/W16-2209.bib)\n\n   Additional source factors are enabled by passing `--source-factors file1 [file2 ...]` (`-sf`), where file1, etc. are\n   token-parallel to the source (`-s`).\n   An analogous parameter, `--validation-source-factors`, is used to pass factors for validation data.\n   The flag `--source-factors-num-embed D1 [D2 ...]` denotes the embedding dimensions and is required if source factor\n   files are given. Factor embeddings are concatenated to the source embeddings dimension (`--num-embed`).\n\n   At test time, the input sentence and its factors can be passed in via STDIN or command-line arguments.\n   - For STDIN, the input and factors should be in a token-based factored format, e.g.,\n     `word1|factor1|factor2|... w2|f1|f2|... ...1`.\n   - You can also use file arguments, which mirrors training: `--input` takes the path to a file containing the source,\n     and `--input-factors` a list of files containing token-parallel factors.\n   At test time, an exception is raised if the number of expected factors does not\n   match the factors passed along with the input.\n\n - Removed bias parameters from multi-head attention layers of the transformer.\n\n## [1.16.6]\n### Changed\n - Loading/Saving auxiliary parameters of the models. Before aux parameters were not saved or used for initialization.\n Therefore the parameters of certain layers were ignored (e.g., BatchNorm) and randomly initialized. This change\n enables to properly load, save and initialize the layers which use auxiliary parameters.\n\n## [1.16.5]\n### Changed\n - Device locking: Only one process will be acquiring GPUs at a time.\n This will lead to consecutive device ids whenever possible.\n\n## [1.16.4]\n### Changed\n - Internal change: Standardized all data to be batch-major both at training and at inference time.\n\n## [1.16.3]\n### Changed\n - When a device lock file exists and the process has no write permissions for the lock file we assume that the device\n is locked. Previously this lead to an permission denied exception. Please note that in this scenario we an not detect\n if the original Sockeye process did not shut down gracefully. This is not an issue when the sockeye process has write\n permissions on existing lock files as in that case locking is based on file system locks, which cease to exist when a\n process exits.\n\n## [1.16.2]\n### Changed\n - Changed to a custom speedometer that tracks samples/sec AND words/sec. The original MXNet speedometer did not take\n variable batch sizes due to word-based batching into account.\n\n## [1.16.1]\n### Fixed\n - Fixed entry points in `setup.py`.\n\n## [1.16.0]\n### Changed\n - Update to [MXNet 1.0.0](https://github.com/apache/incubator-mxnet/tree/1.0.0) which adds more advanced indexing\nfeatures, benefitting the beam search implementation.\n - `--kvstore` now accepts 'nccl' value. Only works if MXNet was compiled with `USE_NCCL=1`.\n\n### Added\n - `--gradient-compression-type` and `--gradient-compression-threshold` flags to use gradient compression.\n  See [MXNet FAQ on Gradient Compression](https://mxnet.incubator.apache.org/versions/master/faq/gradient_compression.html).\n\n## [1.15.8]\n### Fixed\n - Taking the BOS and EOS tag into account when calculating the maximum input length at inference.\n\n## [1.15.7]\n### Fixed\n - fixed a problem with `--num-samples-per-shard` flag not being parsed as int.\n\n## [1.15.6]\n### Added\n - New CLI `sockeye.prepare_data` for preprocessing the training data only once before training,\n potentially splitting large datasets into shards. At training time only one shard is loaded into memory at a time,\n limiting the maximum memory usage.\n\n### Changed\n - Instead of using the ```--source``` and ```--target``` arguments ```sockeye.train``` now accepts a\n ```--prepared-data``` argument pointing to the folder containing the preprocessed and sharded data. Using the raw\n training data is still possible and now consumes less memory.\n\n## [1.15.5]\n### Added\n - Optionally apply query, key and value projections to the source and target hidden vectors in the CNN model\n before applying the attention mechanism. CLI parameter: `--cnn-project-qkv`.\n\n## [1.15.4]\n### Added\n - A warning will be printed if the checkpoint decoder slows down training.\n\n## [1.15.3]\n### Added\n - Exposing the xavier random number generator through `--weight-init-xavier-rand-type`.\n\n## [1.15.2]\n### Added\n - Exposing MXNet's Nesterov Accelerated Gradient, Adadelta and Adadelta optimizers.\n\n## [1.15.1]\n### Added\n - A tool that initializes embedding weights with pretrained word representations, `sockeye.init_embedding`.\n\n## [1.15.0]\n### Added\n- Added support for Swish-1 (SiLU) activation to transformer models\n([Ramachandran et al. 2017: Searching for Activation Functions](https://arxiv.org/pdf/1710.05941.pdf),\n[Elfwing et al. 2017: Sigmoid-Weighted Linear Units for Neural Network Function Approximation\nin Reinforcement Learning](https://arxiv.org/pdf/1702.03118.pdf)).  Use `--transformer-activation-type swish1`.\n- Added support for GELU activation to transformer models ([Hendrycks and Gimpel 2016: Bridging Nonlinearities and\nStochastic Regularizers with Gaussian Error Linear Units](https://arxiv.org/pdf/1606.08415.pdf).\nUse `--transformer-activation-type gelu`.\n\n## [1.14.3]\n### Changed\n- Fast decoding for transformer models. Caches keys and values of self-attention before softmax.\nChanged decoding flag `--bucket-width` to apply only to source length.\n\n## [1.14.2]\n### Added\n - Gradient norm clipping (`--gradient-clipping-type`) and monitoring.\n### Changed\n - Changed `--clip-gradient` to `--gradient-clipping-threshold` for consistency.\n\n## [1.14.1]\n### Changed\n - Sorting sentences during decoding before splitting them into batches.\n - Default chunk size: The default chunk size when batching is enabled is now batch_size * 500 during decoding to avoid\n  users accidentally forgetting to increase the chunk size.\n\n## [1.14.0]\n### Changed\n - Downscaled fixed positional embeddings for CNN models.\n - Renamed `--monitor-bleu` flag to `--decode-and-evaluate` to illustrate that it computes\n other metrics in addition to BLEU.\n\n### Added\n - `--decode-and-evaluate-use-cpu` flag to use CPU for decoding validation data.\n - `--decode-and-evaluate-device-id` flag to use a separate GPU device for validation decoding. If not specified, the\n existing and still default behavior is to use the last acquired GPU for training.\n\n## [1.13.2]\n### Added\n - A tool that extracts specified parameters from params.x into a .npz file for downstream applications or analysis.\n\n## [1.13.1]\n### Added\n - Added chrF metric\n([Popovic 2015: chrF: character n-gram F-score for automatic MT evaluation](http://www.statmt.org/wmt15/pdf/WMT49.pdf)) to Sockeye.\nsockeye.evaluate now accepts `bleu` and `chrf` as values for `--metrics`\n\n## [1.13.0]\n### Fixed\n - Transformer models do not ignore `--num-embed` anymore as they did silently before.\n As a result there is an error thrown if `--num-embed` != `--transformer-model-size`.\n - Fixed the attention in upper layers (`--rnn-attention-in-upper-layers`), which was previously not passed correctly\n to the decoder.\n### Removed\n - Removed RNN parameter (un-)packing and support for FusedRNNCells (removed `--use-fused-rnns` flag).\n These were not used, not correctly initialized, and performed worse than regular RNN cells. Moreover,\n they made the code much more complex. RNN models trained with previous versions are no longer compatible.\n- Removed the lexical biasing functionality (Arthur ETAL'16) (removed arguments `--lexical-bias`\n and `--learn-lexical-bias`).\n\n## [1.12.2]\n### Changed\n - Updated to [MXNet 0.12.1](https://github.com/apache/incubator-mxnet/releases/tag/0.12.1), which includes an important\n bug fix for CPU decoding.\n\n## [1.12.1]\n### Changed\n - Removed dependency on sacrebleu pip package. Now imports directly from `contrib/`.\n\n## [1.12.0]\n### Changed\n - Transformers now always use the linear output transformation after combining attention heads, even if input & output\n depth do not differ.\n\n## [1.11.2]\n### Fixed\n - Fixed a bug where vocabulary slice padding was defaulting to CPU context.  This was affecting decoding on GPUs with\n very small vocabularies.\n\n## [1.11.1]\n### Fixed\n - Fixed an issue with the use of `ignore` in `CrossEntropyMetric::cross_entropy_smoothed`. This was affecting\n runs with Eve optimizer and label smoothing. Thanks @kobenaxie for reporting.\n\n## [1.11.0]\n### Added\n - Lexicon-based target vocabulary restriction for faster decoding. New CLI for top-k lexicon creation, sockeye.lexicon.\n New translate CLI argument `--restrict-lexicon`.\n\n### Changed\n - Bleu computation based on Sacrebleu.\n\n## [1.10.5]\n### Fixed\n - Fixed yet another bug with the data iterator.\n\n## [1.10.4]\n### Fixed\n - Fixed a bug with the revised data iterator not correctly appending EOS symbols for variable-length batches.\n This reverts part of the commit added in 1.10.1 but is now correct again.\n\n## [1.10.3]\n### Changed\n - Fixed a bug with max_observed_{source,target}_len being computed on the complete data set, not only on the\n sentences actually added to the buckets based on `--max_seq_len`.\n\n## [1.10.2]\n### Added\n - `--max-num-epochs` flag to train for a maximum number of passes through the training data.\n\n## [1.10.1]\n### Changed\n - Reduced memory footprint when creating data iterators: integer sequences\n are streamed from disk when being assigned to buckets.\n\n## [1.10.0]\n### Changed\n - Updated MXNet dependency to 0.12 (w/ MKL support by default).\n - Changed `--smoothed-cross-entropy-alpha` to `--label-smoothing`.\n Label smoothing should now require significantly less memory due to its addition to MXNet's `SoftmaxOutput` operator.\n - `--weight-normalization` now applies not only to convolutional weight matrices, but to output layers of all decoders.\n It is also independent of weight tying.\n - Transformers now use `--embed-dropout`. Before they were using `--transformer-dropout-prepost` for this.\n - Transformers now scale their embedding vectors before adding fixed positional embeddings.\n This turns out to be crucial for effective learning.\n - `.param` files now use 5 digit identifiers to reduce risk of overflowing with many checkpoints.\n\n### Added\n - Added CUDA 9.0 requirements file.\n - `--loss-normalization-type`. Added a new flag to control loss normalization. New default is to normalize\n by the number of valid, non-PAD tokens instead of the batch size.\n - `--weight-init-xavier-factor-type`. Added new flag to control Xavier factor type when `--weight-init=xavier`.\n - `--embed-weight-init`. Added new flag for initialization of embeddings matrices.\n\n### Removed\n - `--smoothed-cross-entropy-alpha` argument. See above.\n - `--normalize-loss` argument. See above.\n\n## [1.9.0]\n### Added\n - Batch decoding. New options for the translate CLI: ``--batch-size`` and ``--chunk-size``. Translator.translate()\n now accepts and returns lists of inputs and outputs.\n\n## [1.8.4]\n### Added\n - Exposing the MXNet KVStore through the ``--kvstore`` argument, potentially enabling distributed training.\n\n## [1.8.3]\n### Added\n - Optional smart rollback of parameters and optimizer states after updating the learning rate\n if not improved for x checkpoints. New flags: ``--learning-rate-decay-param-reset``,\n ``--learning-rate-decay-optimizer-states-reset``\n\n## [1.8.2]\n### Fixed\n - The RNN variational dropout mask is now independent of the input\n (previously any zero initial state led to the first state being canceled).\n - Correctly pass `self.dropout_inputs` float to `mx.sym.Dropout` in `VariationalDropoutCell`.\n\n## [1.8.1]\n### Changed\n - Instead of truncating sentences exceeding the maximum input length they are now translated in chunks.\n\n## [1.8.0]\n### Added\n - Convolutional decoder.\n - Weight normalization (for CNN only so far).\n - Learned positional embeddings for the transformer.\n\n### Changed\n - `--attention-*` CLI params renamed to `--rnn-attention-*`.\n - `--transformer-no-positional-encodings` generalized to `--transformer-positional-embedding-type`.\n", "patch": "@@ -11,6 +11,12 @@ Note that Sockeye has checks in place to not translate with an old model that wa\n \n Each version section may have have subsections for: _Added_, _Changed_, _Removed_, _Deprecated_, and _Fixed_.\n \n+\n+## [2.3.24]\n+### Added\n+\n+- Use of the safe yaml loader for the model configuration files.\n+\n ## [2.3.23]\n ### Changed\n \n@@ -21,7 +27,6 @@ Each version section may have have subsections for: _Added_, _Changed_, _Removed\n \n - The previous commit introduced a regression for vocab creation. The results was that the vocabulary was created on the input characters rather than on tokens.\n \n-\n ## [2.3.21]\n ### Added\n ", "file_path": "files/2021_12/782", "file_language": "md", "file_name": "CHANGELOG.md", "outdated_file_modify": 1, "outdated_file_before": 1, "outdated_file_after": 0}, {"raw_url": "https://github.com/awslabs/sockeye/raw/35dd717a80ae1f04128d79bd0bcf340e2e9d1427/sockeye%2F__init__.py", "code": "# Copyright 2017--2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You may not\n# use this file except in compliance with the License. A copy of the License\n# is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is distributed on\n# an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\n\n__version__ = '2.3.24'\n", "code_before": "# Copyright 2017--2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You may not\n# use this file except in compliance with the License. A copy of the License\n# is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is distributed on\n# an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\n\n__version__ = '2.3.23'\n", "patch": "@@ -11,4 +11,4 @@\n # express or implied. See the License for the specific language governing\n # permissions and limitations under the License.\n \n-__version__ = '2.3.23'\n+__version__ = '2.3.24'", "file_path": "files/2021_12/783", "file_language": "py", "file_name": "sockeye/__init__.py", "outdated_file_modify": 0, "outdated_file_before": 1, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/awslabs/sockeye/raw/35dd717a80ae1f04128d79bd0bcf340e2e9d1427/sockeye%2Fconfig.py", "code": "# Copyright 2017--2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You may not\n# use this file except in compliance with the License. A copy of the License\n# is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is distributed on\n# an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\n\nimport copy\nimport logging\nfrom dataclasses import dataclass\n\nimport yaml\n\nlogger = logging.getLogger(__name__)\n\n\nclass TaggedYamlObjectMetaclass(yaml.YAMLObjectMetaclass):\n    def __init__(cls, name, bases, kwds):\n        cls.yaml_tag = \"!\" + name\n        new_kwds = {}\n        new_kwds.update(kwds)\n        new_kwds['yaml_tag'] = \"!\" + name\n        super().__init__(name, bases, new_kwds)\n\n\nclass SafeLoaderWithTuple(yaml.SafeLoader):\n    def construct_python_tuple(self, node):\n        return tuple(self.construct_sequence(node))\n\n\nSafeLoaderWithTuple.add_constructor(\n    u'tag:yaml.org,2002:python/tuple',\n    SafeLoaderWithTuple.construct_python_tuple\n)\n\n\n@dataclass\nclass Config(yaml.YAMLObject, metaclass=TaggedYamlObjectMetaclass):\n    \"\"\"\n    Base configuration object YAML (de-)serialization.\n    Actual Configuration should subclass this object.\n    \"\"\"\n    yaml_loader = SafeLoaderWithTuple  # type: ignore\n\n    def save(self, fname: str):\n        \"\"\"\n        Saves this Config to a file called fname.\n\n        :param fname: Name of file to store this Config in.\n        \"\"\"\n        obj = copy.deepcopy(self)\n        with open(fname, 'w') as out:\n            yaml.dump(obj, out, default_flow_style=False)\n\n    @staticmethod\n    def load(fname: str) -> 'Config':\n        \"\"\"\n        Returns a Config object loaded from a file.\n\n        :param fname: Name of file to load the Config from.\n        :return: Configuration.\n        \"\"\"\n        with open(fname) as inp:\n            obj = yaml.load(inp, Loader=SafeLoaderWithTuple)  # type: ignore\n            return obj\n\n    def copy(self, **kwargs):\n        \"\"\"\n        Create a copy of the config object, optionally modifying some of the attributes.\n        For example `nn_config.copy(num_hidden=512)` will create a copy of `nn_config` where the attribute `num_hidden`\n        will be set to the new value of num_hidden.\n\n        :param kwargs:\n        :return: A deep copy of the config object.\n        \"\"\"\n        copy_obj = copy.deepcopy(self)\n        for name, value in kwargs.items():\n            object.__setattr__(copy_obj, name, value)\n        return copy_obj\n\n    def disable_dropout(self):\n        \"\"\"\n        Sets the value of all float-valued attributes in this config (or any of its children) that contain 'dropout'\n        in their name to 0.0.\n        \"\"\"\n        for attr, val in self.__dict__.items():\n            if isinstance(val, Config):\n                val.disable_dropout()\n            elif 'dropout' in attr and isinstance(val, float):\n                logger.debug(\"Setting %s to 0.0\", attr)\n                setattr(self, attr, 0.0)\n", "code_before": "# Copyright 2017--2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You may not\n# use this file except in compliance with the License. A copy of the License\n# is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is distributed on\n# an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\n\nimport copy\nimport logging\nfrom dataclasses import dataclass\n\nimport yaml\n\nlogger = logging.getLogger(__name__)\n\n\nclass TaggedYamlObjectMetaclass(yaml.YAMLObjectMetaclass):\n    def __init__(cls, name, bases, kwds):\n        cls.yaml_tag = \"!\" + name\n        new_kwds = {}\n        new_kwds.update(kwds)\n        new_kwds['yaml_tag'] = \"!\" + name\n        super().__init__(name, bases, new_kwds)\n\n\n@dataclass\nclass Config(yaml.YAMLObject, metaclass=TaggedYamlObjectMetaclass):\n    \"\"\"\n    Base configuration object YAML (de-)serialization.\n    Actual Configuration should subclass this object.\n    \"\"\"\n    yaml_loader = yaml.UnsafeLoader  # type: ignore\n\n    def save(self, fname: str):\n        \"\"\"\n        Saves this Config to a file called fname.\n\n        :param fname: Name of file to store this Config in.\n        \"\"\"\n        obj = copy.deepcopy(self)\n        with open(fname, 'w') as out:\n            yaml.dump(obj, out, default_flow_style=False)\n\n    @staticmethod\n    def load(fname: str) -> 'Config':\n        \"\"\"\n        Returns a Config object loaded from a file.\n\n        :param fname: Name of file to load the Config from.\n        :return: Configuration.\n        \"\"\"\n        with open(fname) as inp:\n            obj = yaml.load(inp, Loader=yaml.UnsafeLoader)  # type: ignore\n            return obj\n\n    def copy(self, **kwargs):\n        \"\"\"\n        Create a copy of the config object, optionally modifying some of the attributes.\n        For example `nn_config.copy(num_hidden=512)` will create a copy of `nn_config` where the attribute `num_hidden`\n        will be set to the new value of num_hidden.\n\n        :param kwargs:\n        :return: A deep copy of the config object.\n        \"\"\"\n        copy_obj = copy.deepcopy(self)\n        for name, value in kwargs.items():\n            object.__setattr__(copy_obj, name, value)\n        return copy_obj\n\n    def disable_dropout(self):\n        \"\"\"\n        Sets the value of all float-valued attributes in this config (or any of its children) that contain 'dropout'\n        in their name to 0.0.\n        \"\"\"\n        for attr, val in self.__dict__.items():\n            if isinstance(val, Config):\n                val.disable_dropout()\n            elif 'dropout' in attr and isinstance(val, float):\n                logger.debug(\"Setting %s to 0.0\", attr)\n                setattr(self, attr, 0.0)\n", "patch": "@@ -29,13 +29,24 @@ def __init__(cls, name, bases, kwds):\n         super().__init__(name, bases, new_kwds)\n \n \n+class SafeLoaderWithTuple(yaml.SafeLoader):\n+    def construct_python_tuple(self, node):\n+        return tuple(self.construct_sequence(node))\n+\n+\n+SafeLoaderWithTuple.add_constructor(\n+    u'tag:yaml.org,2002:python/tuple',\n+    SafeLoaderWithTuple.construct_python_tuple\n+)\n+\n+\n @dataclass\n class Config(yaml.YAMLObject, metaclass=TaggedYamlObjectMetaclass):\n     \"\"\"\n     Base configuration object YAML (de-)serialization.\n     Actual Configuration should subclass this object.\n     \"\"\"\n-    yaml_loader = yaml.UnsafeLoader  # type: ignore\n+    yaml_loader = SafeLoaderWithTuple  # type: ignore\n \n     def save(self, fname: str):\n         \"\"\"\n@@ -56,7 +67,7 @@ def load(fname: str) -> 'Config':\n         :return: Configuration.\n         \"\"\"\n         with open(fname) as inp:\n-            obj = yaml.load(inp, Loader=yaml.UnsafeLoader)  # type: ignore\n+            obj = yaml.load(inp, Loader=SafeLoaderWithTuple)  # type: ignore\n             return obj\n \n     def copy(self, **kwargs):", "file_path": "files/2021_12/784", "file_language": "py", "file_name": "sockeye/config.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 1, "static": {"rats": [false, []], "semgrep": [true, ["       python.lang.security.deserialization.avoid-pyyaml-load.avoid-pyyaml-load                      \n          Detected a possible YAML deserialization vulnerability. `yaml.unsafe_load`, `yaml.Loader`, \n          `yaml.CLoader`, and `yaml.UnsafeLoader` are all known to be unsafe methods of deserializing\n          YAML. An attacker with control over the YAML input could create special YAML input that    \n          allows the attacker to run arbitrary Python code. This would allow the attacker to steal   \n          files, download and install malware, or otherwise take over the machine. Use               \n          `yaml.safe_load` or `yaml.SafeLoader` instead.                                             \n          Details: https://sg.run/we9Y                                                               \n\n           \u25b6\u25b6\u2506 Autofix \u25b6 yaml.load(inp, Loader=yaml.UnsafeLoader)           59\u2506 obj = yaml.load(inp, Loader=yaml.UnsafeLoader)  # type: ignore"]]}, "target": -1, "function_before": [], "function_after": []}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
