{"index": 12664, "cve_id": "CVE-2023-48699", "cwe_id": ["CWE-94", "CWE-95"], "cve_language": "Python", "cve_description": "fastbots is a library for fast bot and scraper development using selenium and the Page Object Model (POM) design. Prior to version 0.1.5, an attacker could modify the locators.ini locator file with python code that without proper validation it's executed and it could lead to rce. The vulnerability is in the function `def __locator__(self, locator_name: str)` in `page.py`. In order to mitigate this issue, upgrade to fastbots version 0.1.5 or above.", "cvss": "9.8", "publish_date": "November 21, 2023", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "NONE", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "73eb03bd75365e112b39877e26ef52853f5e9f57", "commit_message": "Merge pull request #3 from ubertidavide/develop\n\nDevelop", "commit_date": "2023-11-20T21:55:52Z", "project": "ubertidavide/fastbots", "url": "https://api.github.com/repos/ubertidavide/fastbots/commits/73eb03bd75365e112b39877e26ef52853f5e9f57", "html_url": "https://github.com/ubertidavide/fastbots/commit/73eb03bd75365e112b39877e26ef52853f5e9f57", "windows_before": [{"commit_id": "cc96c41afbd0ecf0d69147e0f35c4e31839dc29b", "commit_date": "Mon Nov 20 22:51:16 2023 +0100", "commit_message": "fixed locator parsing function", "files_name": ["fastbots/page.py"]}, {"commit_id": "681e9c21c1b5380ddbfe9db1247d0dc511572606", "commit_date": "Mon Nov 20 22:33:39 2023 +0100", "commit_message": "fixed issue", "files_name": ["fastbots/page.py"]}, {"commit_id": "4dc9068d532f595e104bb506fce9165c8ecb9278", "commit_date": "Mon Nov 20 22:28:46 2023 +0100", "commit_message": "fixed vulnerability that could lead to rce from config settings loading", "files_name": ["fastbots/page.py"]}, {"commit_id": "14eeff4e26269eff765983b2f47eb255258df98b", "commit_date": "Mon Nov 20 21:40:53 2023 +0100", "commit_message": "added download file docs", "files_name": ["README.md"]}, {"commit_id": "e3aee6e4d77d64a0ce81546321893e5b730ff1fb", "commit_date": "Mon Nov 20 21:31:32 2023 +0100", "commit_message": "added the ability to wait for a download and get the download file path", "files_name": ["fastbots/bot.py", "fastbots/config.py", "fastbots/exceptions.py"]}, {"commit_id": "ca25f6388005288aae7ed4978f37e7f1b02fc5e1", "commit_date": "Sun Nov 19 21:57:05 2023 +0100", "commit_message": "Merge pull request #2 from ubertidavide/feature/0.1.3", "files_name": ["37822c6550a6f0a8ef3150fce3cdc16cfeb6c73c - Sun Nov 19 19:18:51 2023 +0100 : Merge branch 'feature/tests' into develop", "bedb65b0ad4d5d4e15d55e6c0dc9fc703a4c2692 - Sun Nov 19 19:13:52 2023 +0100 : test", ".gitignore", "cookies.pkl"]}, {"commit_id": "1f4b56fb7a7a7026d136f64caf237f6982a81b1f", "commit_date": "Sun Nov 19 19:13:24 2023 +0100", "commit_message": "added tests", "files_name": [".gitignore", "cookies.pkl", "fastbots/task.py", "locators.ini"]}, {"commit_id": "e482df8d435525caec99c93fce17f133b2a8fdaf", "commit_date": "Sun Nov 19 18:39:19 2023 +0100", "commit_message": "removed cookies", "files_name": ["cookies.pkl"]}, {"commit_id": "001678b233df66893fdf86925206a9ea6cc1a793", "commit_date": "Sun Nov 19 18:38:05 2023 +0100", "commit_message": "passing chrome_bot and firefox_bot tests", "files_name": ["cookies.pkl", "fastbots/bot.py", "fastbots/chrome_bot.py", "fastbots/firefox_bot.py", "main.py", "tests/test_chrome_bot.py", "tests/test_firefox_bot.py"]}, {"commit_id": "d65103e3c59cdf877bbd2b979d6e4a65e2b37e35", "commit_date": "Sun Nov 19 17:23:12 2023 +0100", "commit_message": "added bastic test", "files_name": [".gitignore", "pyproject.toml", "tests/test_chrome_bot.py", "tests/test_firefox_bot.py"]}, {"commit_id": "2b87677af039c8686b49a8f71b6bd1faf8906e94", "commit_date": "Sun Nov 19 16:43:16 2023 +0100", "commit_message": "added basic bot implementations tests", "files_name": ["fastbots/chrome_bot.py", "fastbots/firefox_bot.py", "tests/test_bot.py", "tests/test_chrome_bot.py", "tests/test_firefox_bot.py"]}, {"commit_id": "ef2fb33d54234cfb297eefdd62babdfad09a15c5", "commit_date": "Sun Nov 19 16:16:05 2023 +0100", "commit_message": "added firefox and chrome bot as child class of bot", "files_name": ["fastbots/bot.py", "fastbots/chrome_bot.py", "fastbots/firefox_bot.py", "fastbots/task.py"]}, {"commit_id": "2185342b03a1063aedadca03bc5f2179509f2ca9", "commit_date": "Sun Nov 19 15:58:03 2023 +0100", "commit_message": "added tests, code refactoring", "files_name": ["README.md", "fastbots/bot.py", "fastbots/config.py", "fastbots/firefox_bot.py", "fastbots/task.py", "locators.ini", "preferences.json", "settings.ini", "tests/bot.py", "tests/test_bot.py"]}, {"commit_id": "1c181b15f880bfdc30b781fce1446f9174ae9ce0", "commit_date": "Fri Nov 17 23:31:00 2023 +0100", "commit_message": "fix #1", "files_name": ["fastbots/bot.py"]}, {"commit_id": "52fb574fd71ec53db193d84fa4e4d3f153f911b1", "commit_date": "Fri Nov 17 20:55:00 2023 +0100", "commit_message": "Merge tag '0.1.2' into develop", "files_name": ["0d761032423b241fe8c7bf9023e0e844d1b41303 - Fri Nov 17 20:54:48 2023 +0100 : Merge branch 'release/0.1.2'", "c4831271e5f5369df0a69aa428aa366d984770b1 - Fri Nov 17 20:54:30 2023 +0100 : fixed structure", "Dockerfile", "README.md", "entrypoint.sh", "examples/locators.ini", "examples/main.py", "examples/preferences.json", "examples/settings.ini", "pyproject.toml", "tests/__init__.py", "tests/bot.py"]}, {"commit_id": "a88fd50d5e1b04f7447eb65a83f8efc209c100a4", "commit_date": "Fri Nov 17 20:51:16 2023 +0100", "commit_message": "rm dockerignore", "files_name": [".dockerignore"]}, {"commit_id": "e10b61716ca4a88f7a163f77db3e870ac170435f", "commit_date": "Fri Nov 17 20:50:47 2023 +0100", "commit_message": "fixed project structure", "files_name": ["Dockerfile", "README.md", "entrypoint.sh", "examples/locators.ini", "examples/main.py", "examples/preferences.json", "examples/settings.ini", "pyproject.toml", "tests/__init__.py", "tests/bot.py"]}, {"commit_id": "25c91478198c69e403322befc9a3f17dc34adc12", "commit_date": "Fri Nov 17 17:45:55 2023 +0100", "commit_message": "fixed typos, improved version", "files_name": ["README.md", "pyproject.toml"]}, {"commit_id": "ef5196bd36847c4d16b89ce86cbdbfd8b8a97980", "commit_date": "Fri Nov 17 17:38:32 2023 +0100", "commit_message": "added chromedirver and firefoxdriver interoperability, changed docs, cleaned some code", "files_name": ["README.md", "fastbots/bot.py", "fastbots/config.py", "fastbots/page.py", "fastbots/task.py", "locators.ini", "main.py", "preferences.json"]}, {"commit_id": "e0928e299e87431fc5cbadcf8960c5b17d784c5d", "commit_date": "Fri Nov 17 00:26:15 2023 +0100", "commit_message": "added cookiecutter note", "files_name": ["README.md"]}, {"commit_id": "18e26db7d991a90a139915daba110cbe1310694e", "commit_date": "Fri Nov 17 00:26:02 2023 +0100", "commit_message": "added cookiecutter note", "files_name": ["README.md"]}, {"commit_id": "6e1aa6f48718f3d045449b8235d8392db9185eee", "commit_date": "Fri Nov 17 00:01:39 2023 +0100", "commit_message": "added readme preferences doc", "files_name": ["README.md", "firefox_preferences.json"]}, {"commit_id": "acb6af31b332ad100118f2fab34ed66c6928655a", "commit_date": "Thu Nov 16 23:55:17 2023 +0100", "commit_message": "fixed readme", "files_name": ["README.md"]}, {"commit_id": "0cf148d7dc673face66736a165194c9d471bf823", "commit_date": "Thu Nov 16 23:46:42 2023 +0100", "commit_message": "renamed project", "files_name": ["README.md"]}, {"commit_id": "5e8554172da729f8f2fac2cf2c94a753b724965e", "commit_date": "Thu Nov 16 23:46:07 2023 +0100", "commit_message": "renamed project", "files_name": ["fastbots/__init__.py", "fastbots/bot.py", "fastbots/config.py", "fastbots/exceptions.py", "fastbots/logger.py", "fastbots/page.py", "fastbots/task.py", "main.py", "pyproject.toml"]}, {"commit_id": "d90322128d7f2e70c2c1df90f1178a0106519856", "commit_date": "Thu Nov 16 23:23:22 2023 +0100", "commit_message": "added docker and fixed readme", "files_name": [".dockerignore", "Dockerfile", "README.md", "entrypoint.sh"]}, {"commit_id": "b5ee148c60e50e6c61f1a965266f0041c65ca1a4", "commit_date": "Thu Nov 16 22:15:23 2023 +0100", "commit_message": "fixed path and logging", "files_name": [".gitignore", "Dockerfile", "fastbot/config.py", "fastbot/logger.py", "locators.ini"]}, {"commit_id": "6ccaf190ddd036e1034f1513ff192a69e8789fb5", "commit_date": "Thu Nov 16 22:05:27 2023 +0100", "commit_message": "fixed issues", "files_name": [".gitignore", "Dockerfile", "README.md", "fastbot/__init__.py", "fastbot/bot.py", "fastbot/task.py", "locators.ini", "main.py", "pyproject.toml"]}, {"commit_id": "12e8a208b63158dfa58e658f668d16890e63b90a", "commit_date": "Thu Nov 16 11:38:23 2023 +0100", "commit_message": "added config docs", "files_name": ["README.md"]}, {"commit_id": "cc0410268cd998cc324a3a725c6659e86d26d4d8", "commit_date": "Thu Nov 16 11:32:21 2023 +0100", "commit_message": "added installation docs", "files_name": ["README.md"]}, {"commit_id": "4c13960cde4769b4fb0af6f19e24f07a36712f54", "commit_date": "Thu Nov 16 11:30:55 2023 +0100", "commit_message": "fixed license", "files_name": [".gitignore", "pyproject.toml"]}, {"commit_id": "f186f7237eafd56aeb4bba93420b8c3b827927ee", "commit_date": "Thu Nov 16 11:23:05 2023 +0100", "commit_message": "modified imports", "files_name": ["README.md", "fastbot/__init__.py", "main.py"]}, {"commit_id": "cd15dd380f477006fdd0bc582750369d74323b51", "commit_date": "Thu Nov 16 11:15:51 2023 +0100", "commit_message": "modified readme", "files_name": ["README.md"]}, {"commit_id": "ff03bdd6a0fa1c8cab427ae636ffcfcb5aecec64", "commit_date": "Thu Nov 16 11:14:44 2023 +0100", "commit_message": "added payload, log on error, wait, better example", "files_name": ["README.md", "fastbot/bot.py", "fastbot/config.py", "fastbot/task.py", "locators.ini", "main.py"]}, {"commit_id": "42e8d259460a6c0eca47a21f69aff3561a1e9b55", "commit_date": "Thu Nov 16 00:45:06 2023 +0100", "commit_message": "added basic components", "files_name": []}], "windows_after": [{"commit_id": "a5dd8d40a08e0385ee207522925e27072bf9160a", "commit_date": "Mon Nov 20 23:16:13 2023 +0100", "commit_message": "fixed page.py", "files_name": ["fastbots/page.py"]}, {"commit_id": "b94b51a8f4280a3ccbd533fd01e1de14e01f89c7", "commit_date": "Mon Nov 20 23:17:03 2023 +0100", "commit_message": "Merge pull request #6 from ubertidavide/main", "files_name": ["10984edc794f86c8a29ad739cab9bc3a16269fed - Mon Nov 20 23:19:41 2023 +0100 : Merge branch 'develop' of https://github.com/ubertidavide/fastbot into develop", "4df56b832ee78a04261dc494350914e8916991c2 - Mon Nov 20 23:21:39 2023 +0100 : added new version", "pyproject.toml"]}, {"commit_id": "a871a22e2f43f6497e1ab2a938739be88792f806", "commit_date": "Tue Nov 21 20:55:29 2023 +0100", "commit_message": "Create SECURITY.md", "files_name": ["SECURITY.md"]}, {"commit_id": "4ae06711eb60afcd4f4119156b8caa0a6389010a", "commit_date": "Tue Nov 21 20:58:05 2023 +0100", "commit_message": "Merge pull request #7 from ubertidavide/develop", "files_name": ["ac7a2000c943d6a34f8d74894ec10021eed282f1 - Tue Nov 21 20:59:11 2023 +0100 : Merge pull request #8 from ubertidavide/main", "5ecf8896b303889efee4f22174ae18b80de586b1 - Tue Nov 21 21:16:22 2023 +0100 : Create CODE_OF_CONDUCT.md", "CODE_OF_CONDUCT.md"]}, {"commit_id": "bc9ee03a1bce78a44906151f0ec974c7ab15c657", "commit_date": "Tue Nov 21 21:17:23 2023 +0100", "commit_message": "Update issue templates", "files_name": [".github/ISSUE_TEMPLATE/bug_report.md", ".github/ISSUE_TEMPLATE/feature_request.md"]}, {"commit_id": "29893b245436324821c1867e0c38c6c797837911", "commit_date": "Tue Nov 21 21:32:54 2023 +0100", "commit_message": "Create CONTRIBUTING.md", "files_name": ["CONTRIBUTING.md"]}, {"commit_id": "7f1fdcb1c48d856add158ab3d54911c1c2b0e9c6", "commit_date": "Tue Nov 21 21:34:20 2023 +0100", "commit_message": "Merge pull request #9 from ubertidavide/main", "files_name": ["ea59887a3f196f5700ff4e3e5a4952a97de3b6c7 - Thu Nov 23 11:43:00 2023 +0100 : Update README.md", "README.md"]}, {"commit_id": "e481135d53c25794033a33603cdd6eee1902c6e2", "commit_date": "Thu Nov 23 20:01:14 2023 +0100", "commit_message": "Update README.md", "files_name": ["README.md"]}, {"commit_id": "4f86fd0cb9e691146873357d54bc03e0bc90adeb", "commit_date": "Thu Nov 23 20:11:02 2023 +0100", "commit_message": "Update bot.py", "files_name": ["fastbots/bot.py"]}, {"commit_id": "7175230946bc3a1ca245b25f61cd672fff04a230", "commit_date": "Thu Nov 23 20:12:30 2023 +0100", "commit_message": "Merge pull request #10 from ubertidavide/ubertidavide-patch-2", "files_name": ["b83b5b4a7ea302f8db71986e207da2fd4b654f81 - Thu Nov 23 20:13:10 2023 +0100 : Merge pull request #11 from ubertidavide/ubertidavide-patch-1", "b5b441f324d5b92d05f15a85bc051e74f7689786 - Thu Nov 23 20:14:42 2023 +0100 : Update main.py", "main.py"]}, {"commit_id": "1f45511afe171d21df2f7716478598605dea2c1d", "commit_date": "Thu Nov 23 20:16:11 2023 +0100", "commit_message": "Update task.py", "files_name": ["fastbots/task.py"]}, {"commit_id": "fa6a3747bfd51266f5658aa67c88c996c955dbad", "commit_date": "Thu Nov 23 20:18:04 2023 +0100", "commit_message": "Update page.py", "files_name": ["fastbots/page.py"]}, {"commit_id": "4679833788d45456aeac214b0ab6240ec1371e03", "commit_date": "Thu Nov 23 20:18:54 2023 +0100", "commit_message": "Update logger.py", "files_name": ["fastbots/logger.py"]}, {"commit_id": "1592166f0b5c3cef28cf8facc674fdf0927f0b2d", "commit_date": "Thu Nov 23 20:19:54 2023 +0100", "commit_message": "Update firefox_bot.py", "files_name": ["fastbots/firefox_bot.py"]}, {"commit_id": "4e3a4f47119698803f0e75420df732c28fcef769", "commit_date": "Thu Nov 23 20:20:32 2023 +0100", "commit_message": "Update exceptions.py", "files_name": ["fastbots/exceptions.py"]}, {"commit_id": "75c24d290ce79258ca39223566737c6411b6522e", "commit_date": "Thu Nov 23 20:24:17 2023 +0100", "commit_message": "Update config.py", "files_name": ["fastbots/config.py"]}, {"commit_id": "fbd908f953320c439a2e3d428c3f1d52ee11c92f", "commit_date": "Thu Nov 23 20:25:23 2023 +0100", "commit_message": "Update chrome_bot.py", "files_name": ["fastbots/chrome_bot.py"]}, {"commit_id": "aed3f2875dead41d88c0faf81364f05d5d28305b", "commit_date": "Thu Nov 23 20:27:04 2023 +0100", "commit_message": "Update locators.ini", "files_name": ["locators.ini"]}, {"commit_id": "9e83216af5c6ded8344655a5e0ef3b698730b346", "commit_date": "Thu Nov 23 20:27:58 2023 +0100", "commit_message": "Update settings.ini", "files_name": ["settings.ini"]}, {"commit_id": "6aa5806a3fc09802d8c6ea5359b5b4b46535c577", "commit_date": "Thu Nov 23 20:29:02 2023 +0100", "commit_message": "Update pyproject.toml", "files_name": ["pyproject.toml"]}, {"commit_id": "1f0f87b78f7831217ddf9f83eb7a801601a83b01", "commit_date": "Thu Nov 23 20:29:47 2023 +0100", "commit_message": "Update SECURITY.md", "files_name": ["SECURITY.md"]}, {"commit_id": "bfb8a517560ac7fdbb259c7924bad27d6078f3d0", "commit_date": "Thu Nov 23 20:32:24 2023 +0100", "commit_message": "Update bot.py", "files_name": ["fastbots/bot.py"]}, {"commit_id": "01b6bb870708a1aedee296de7c262f3f93d85999", "commit_date": "Thu Nov 23 20:38:10 2023 +0100", "commit_message": "Merge pull request #12 from ubertidavide/ubertidavide-patch-1", "files_name": ["19422719c29f5c4e812ba49a6e528504f28d2ec8 - Thu Nov 23 20:39:45 2023 +0100 : Merge pull request #13 from ubertidavide/main", "28bd248a027769a89ff8f404bc078ecad4258325 - Thu Nov 23 23:31:05 2023 +0100 : Merge branch 'develop' of https://github.com/ubertidavide/fastbot into develop", "d1b7d86a4dbda90c6d8a984eac97e1b3d3168d7a - Fri Nov 24 00:02:51 2023 +0100 : added docs", ".gitignore", "docs/index.md", "mkdocs.yml", "pyproject.toml"]}, {"commit_id": "67eb6a4cd12683c17bd6993afff9890fe1555f6a", "commit_date": "Fri Nov 24 00:05:16 2023 +0100", "commit_message": "Merge pull request #14 from ubertidavide/develop", "files_name": ["772e2cda158ff125c2ca651d17daf2b21c917524 - Fri Nov 24 16:20:44 2023 +0100 : added new docs", "README.md", "docs/index.md", "docs/reference/bot.md", "docs/reference/chrome_bot.md", "docs/reference/config.md", "docs/reference/firefox_bot.md", "docs/reference/page.md", "docs/reference/task.md", "mkdocs.yml", "pyproject.toml"]}, {"commit_id": "c7c6a8458a9f693faaf9772557923c66430ee488", "commit_date": "Fri Nov 24 16:21:20 2023 +0100", "commit_message": "Merge pull request #15 from ubertidavide/develop", "files_name": ["571333b67c93b84098fbe60926d7b47a28221798 - Fri Nov 24 16:28:23 2023 +0100 : added new docs", ".gitignore", "site/assets/_mkdocstrings.css", "site/css/fonts/Roboto-Slab-Bold.woff", "site/css/fonts/Roboto-Slab-Bold.woff2", "site/css/fonts/Roboto-Slab-Regular.woff", "site/css/fonts/Roboto-Slab-Regular.woff2", "site/css/fonts/fontawesome-webfont.eot", "site/css/fonts/fontawesome-webfont.svg", "site/css/fonts/fontawesome-webfont.ttf", "site/css/fonts/fontawesome-webfont.woff", "site/css/fonts/fontawesome-webfont.woff2", "site/css/fonts/lato-bold-italic.woff", "site/css/fonts/lato-bold-italic.woff2", "site/css/fonts/lato-bold.woff", "site/css/fonts/lato-bold.woff2", "site/css/fonts/lato-normal-italic.woff", "site/css/fonts/lato-normal-italic.woff2", "site/css/fonts/lato-normal.woff", "site/css/fonts/lato-normal.woff2", "site/css/theme.css", "site/css/theme_extra.css", "site/img/favicon.ico", "site/js/html5shiv.min.js", "site/js/jquery-3.6.0.min.js", "site/js/theme.js", "site/js/theme_extra.js", "site/objects.inv", "site/sitemap.xml", "site/sitemap.xml.gz"]}, {"commit_id": "9b59aaf8c9bc2d4b1f42ec47a7313a855d28ae69", "commit_date": "Fri Nov 24 16:32:38 2023 +0100", "commit_message": "Merge pull request #16 from ubertidavide/main", "files_name": ["9bf1c8aa254fd8657386ce31cd47f38005c7317c - Fri Nov 24 16:33:36 2023 +0100 : Merge pull request #17 from ubertidavide/develop", "114cdf5343ee4084a67fa7f69b50a8b42d59f578 - Fri Nov 24 16:40:03 2023 +0100 : remove site folder", "site/assets/_mkdocstrings.css", "site/css/fonts/Roboto-Slab-Bold.woff", "site/css/fonts/Roboto-Slab-Bold.woff2", "site/css/fonts/Roboto-Slab-Regular.woff", "site/css/fonts/Roboto-Slab-Regular.woff2", "site/css/fonts/fontawesome-webfont.eot", "site/css/fonts/fontawesome-webfont.svg", "site/css/fonts/fontawesome-webfont.ttf", "site/css/fonts/fontawesome-webfont.woff", "site/css/fonts/fontawesome-webfont.woff2", "site/css/fonts/lato-bold-italic.woff", "site/css/fonts/lato-bold-italic.woff2", "site/css/fonts/lato-bold.woff", "site/css/fonts/lato-bold.woff2", "site/css/fonts/lato-normal-italic.woff", "site/css/fonts/lato-normal-italic.woff2", "site/css/fonts/lato-normal.woff", "site/css/fonts/lato-normal.woff2", "site/css/theme.css", "site/css/theme_extra.css", "site/img/favicon.ico", "site/js/html5shiv.min.js", "site/js/jquery-3.6.0.min.js", "site/js/theme.js", "site/js/theme_extra.js", "site/objects.inv", "site/sitemap.xml", "site/sitemap.xml.gz"]}, {"commit_id": "79c1b18625f77316a0e5babd87225de3807a81ce", "commit_date": "Fri Nov 24 16:46:01 2023 +0100", "commit_message": "added docs", "files_name": [".gitignore", "README.md"]}, {"commit_id": "d2603cb96ee89a557c5fd1aa7ebefb10e1816f07", "commit_date": "Fri Nov 24 16:47:07 2023 +0100", "commit_message": "Merge pull request #18 from ubertidavide/develop", "files_name": ["b2ed3ffbaea60cda5020c543404a8fe565a28681 - Tue Dec 5 21:18:24 2023 +0100 : Update __init__.py", "fastbots/__init__.py"]}, {"commit_id": "6fe79ab6e8e1e26a43e07b29063348245ae83721", "commit_date": "Tue Dec 5 21:26:31 2023 +0100", "commit_message": "Update README.md", "files_name": ["README.md"]}, {"commit_id": "d96f956f55b8dd868c68a6744b664842380c4bbe", "commit_date": "Tue Dec 5 21:26:49 2023 +0100", "commit_message": "Merge pull request #19 from ubertidavide/ubertidavide-patch-1", "files_name": ["1f9c77f0ba2c425a3fe4dea1ff716a0263274dd9 - Tue Dec 5 21:29:13 2023 +0100 : Update __init__.py", "fastbots/__init__.py"]}, {"commit_id": "06b220cb44107edad96ca027edd487f53790e950", "commit_date": "Tue Dec 5 21:29:47 2023 +0100", "commit_message": "Merge pull request #20 from ubertidavide/develop", "files_name": ["dd86bde2eddd3b065faef2e714b47c8784f808a5 - Tue Dec 5 21:35:07 2023 +0100 : removed unused files", "locators.ini", "main.py", "pyproject.toml", "settings.ini"]}, {"commit_id": "4ecfd62a05cdda44c868ee355a599dbd38c7b98a", "commit_date": "Tue Dec 5 21:36:07 2023 +0100", "commit_message": "upgraded version", "files_name": ["locators.ini"]}, {"commit_id": "fa1a342fff0b29f3291e385cc8b21e47b19419f9", "commit_date": "Tue Dec 5 21:38:06 2023 +0100", "commit_message": "Merge pull request #21 from ubertidavide/develop", "files_name": ["6e36d2cab5ac407f5de5ef4f8d3be3cc3ec93333 - Wed Dec 6 14:52:04 2023 +0100 : Update README.md", "README.md"]}, {"commit_id": "4a7111e9529084215c09bf4e095ba768e6236038", "commit_date": "Wed Dec 6 14:58:36 2023 +0100", "commit_message": "Update task.py", "files_name": ["fastbots/task.py"]}, {"commit_id": "6fbf84746b3448be23146c452f721fb8eef30083", "commit_date": "Wed Dec 6 14:58:54 2023 +0100", "commit_message": "Merge pull request #22 from ubertidavide/develop", "files_name": ["155d99ed22e459023cb9667bc264ddda3a9c3f72 - Wed Dec 6 17:06:41 2023 +0100 : upgraded"]}], "parents": [{"commit_id_before": "ca25f6388005288aae7ed4978f37e7f1b02fc5e1", "url_before": "https://api.github.com/repos/ubertidavide/fastbots/commits/ca25f6388005288aae7ed4978f37e7f1b02fc5e1", "html_url_before": "https://github.com/ubertidavide/fastbots/commit/ca25f6388005288aae7ed4978f37e7f1b02fc5e1"}, {"commit_id_before": "9577f99fb0de33769fed1727ca0cd65a1124d790", "url_before": "https://api.github.com/repos/ubertidavide/fastbots/commits/9577f99fb0de33769fed1727ca0cd65a1124d790", "html_url_before": "https://github.com/ubertidavide/fastbots/commit/9577f99fb0de33769fed1727ca0cd65a1124d790"}], "details": [{"raw_url": "https://github.com/ubertidavide/fastbots/raw/73eb03bd75365e112b39877e26ef52853f5e9f57/README.md", "code": "# fastbots\nA simple library for fast bot and scraper development using selenium and the POM (Page Object Model) design.\nIncrease your productivity, focusing only one the scraping, less boilerplate code and don't require directly driver managment related code, with browser independent settings.\nIf the site locators changes, this don't require changes in the code, only in the configuration.\n\n## Installation:\nThe installation is a simple process with pip on the\nPyPy repository.\n```bash\npip install fastbots\n```\n\n## Showcase:\nCheck out the full example at the repo: [cookiecutter-fastbots](https://github.com/ubertidavide/cookiecutter-fastbots).\n\n### Main Code\nAll the main code example:\n```python\n-- main.py\nimport logging\n\nfrom fastbots import Task, Bot, Page, EC, WebElement, Keys\n\n\nclass ProductPage(Page):\n\n    # page name it's the page_name used in the locators file, see below\n    def __init__(self, bot: Bot, page_name: str = 'product_page'): \n        super().__init__(bot, page_name)\n\n    def forward(self) -> None:\n        logging.info('DO THINGS')\n\n        # using the locators specified in the file give more flexibility and less code changes\n        name_element: WebElement = self.bot.wait.until(EC.element_to_be_clickable(self.__locator__('name_locator')))\n        # store data in the payload section, useful when i need to retrieve data on success\n        self.bot.payload['result'] = name_element.text\n\n        # end the chain of pages interactins\n        return None\n\nclass SearchPage(Page):\n\n    # page name it's the page_name used in the locators file, see below\n    def __init__(self, bot: Bot, page_name: str = 'search_page'):\n        super().__init__(bot, page_name)\n\n    def forward(self) -> ProductPage:\n        logging.info('DO THINGS')\n\n        # using the locators specified in the file give more flexibility and less code changes\n        search_element: WebElement = self.bot.wait.until(EC.element_to_be_clickable(self.__locator__('search_locator')))\n        search_element.send_keys('Selenium with Python Simplified For Beginners')\n        search_element.send_keys(Keys.ENTER)\n\n        # product_element: WebElement = self.bot.driver.find_element(*self.__locator__('product_locator'))\n        product_element: WebElement = self.bot.wait.until(EC.element_to_be_clickable(self.__locator__('product_locator')))\n        product_element.click()\n\n        # continue the chain interaction in the next page\n        return ProductPage(bot=self.bot)\n\nclass TestTask(Task):\n\n    # main task code\n    def run(self, bot: Bot) -> bool:\n        logging.info('DO THINGS')\n\n        # open the search page do things and go forward\n        page: Page = SearchPage(bot=bot).forward()\n\n        # for every page founded do things and go forward\n        while page:\n            page = page.forward()\n\n        # for default it will succeed\n        return True\n\n    # method executed on bot success, with it's payload\n    def on_success(self, payload):\n        logging.info(f'SUCCESS {payload}')\n    \n    # method executed on bot failure\n    def on_failure(self, payload):\n        logging.info(f'FAILED {payload}')\n        \nif __name__ == '__main__':\n    # start the above task\n    TestTask()()\n```\n\n### Locators File\nIn the locators configuration file there is all the required locators config.\nThis could be changed easily without rebuild or make modifications at the code.\n```ini\n-- locators.ini\n[pages_url] # pages_url required url settings\nstart_url=https://www.amazon.com/ #start_url it's the first page driver.get()\nsearch_page=https://www.amazon.com/ #*_page it's the first page url used for the page_name parameter with it's url that need to match\nproduct_page=https://www.amazon.com/s?k=Selenium+with+Python#*_page it's the second page url used for the page_name parameter with it's url that need to match\n\n[search_page] #*_page first page_name parameter, with it's related locators\nsearch_locator=(By.ID, \"twotabsearchtextbox\")\nproduct_locator=(By.XPATH, '//*[@id=\"search\"]/div[1]/div[1]/div/span[1]/div[1]/div[2]')\n\n[product_page]#*_page second page_name parameter, with it's related locators\nname_locator=(By.ID, \"title\")\n```\n\n### Browser and Drivers (Optional)\nFor default config, the selected browser is Firefox, but it could be changed from the config file:\n```ini\n-- settings.ini\n[settings]\n#BOT_DRIVER_TYPE=FIREFOX\nBOT_DRIVER_TYPE=CHROME\n```\n**The correct browser installed for the driver selected it's required.**\nThe browser installation path is autodetected by system env variables, the driver download process and it's related installation path settings are managed automatically.\n\n### Retry and Debug (Optional)\nFor default every task will be retryed 2 times waiting 10 seconds, when all the two try fail, the task execute the on_error method else it will execute the on_success method.\nThis behaviour could be modified in the settings file:\n```ini\n-- settings.ini\n[settings]\nBOT_MAX_RETRIES=2 #sec default\nBOT_RETRY_DELAY=10 #sec default\n```\nWhen the task is failed the library store the screenshot and the html of the page in the debug folder, useful for debug.\nIt will store also all the logs in the log.log file.\n\n### Page Url Check (Automatic)\nEvery defined page must have a page url and when it's instantiate and reaced by the bot, the library check that the \nspecified url in the config are the same as the reached page during the navigation, to reduce navigation errors.  \nIf you want to disable this function see the Global Wait Section below.\n\n### File Download Wait (Functions)\nThis library have the bot.wait_downloaded_file_path(file_extension, new_name_file=None) method that could be used afer a button download click in order\nto wait and get the path of the downloaded file, it will give the ability also to rename the file.  \nThe extension is used to check that the file downloaded it's the correct and it's not corrupted.\n\n### Download Folder and other Folders (Optional)\n```ini\n-- settings.ini\n[settings]\nBOT_DOWNLOAD_FOLDER_PATH='/usr/...' #override the default download path used for the browser\nBOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH='/debug' # default\nBOT_HTML_DOWNLOAD_FOLDER_PATH='/debug'\n```\n\n### Global Wait (Optional)\nThe default configured wait are showed below:\n- The implicit wait used for inital page loading.\n- The wait for the url check that matches the specified in the locators file\n- The default wait used by the self.bot.wait function\n```ini\n-- settings.ini\n[settings]\nSELENIUM_GLOBAL_IMPLICIT_WAIT=5 #sec default\nSELENIUM_EXPECTED_URL_TIMEOUT=5 #sec default\nSELENIUM_DEFAULT_WAIT=5 #sec default\nSELENIUM_FILE_DOWNLOAD_TIMEOUT=20 #sec default\n\nSELENIUM_EXPECTED_URL_CHECK=False #disable the automatic page url check, the default value it's True\n```\n\n### Proxy (Optional)\nConfigure the proxy settings.\n```ini\n-- settings.ini\n[settings]\nBOT_PROXY_ENABLED=True\nBOT_HTTP_PROXY=127.0.0.1:8080\nBOT_HTTPS_PROXY=127.0.0.1:8080\n```\n\n### User Agent (Optional)\nConfigure the user agent used for the requests.\n```ini\n-- settings.ini\n[settings]\nBOT_USER_AGENT=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n```\n\n### Arguments (Optional)\nConfigure Firefox Arguments, store them in the config file, the format it's the same for all the supported drivers, check carefully that the exact arg it's implemented for the selected driver.\n\n#### Firefox args:\n```ini\n-- settings.ini\n[settings]\nBOT_ARGUMENTS=[\"--headless\", \"--disable-gpu\"]\n```\n\n#### Chrome args\n```ini\n-- settings.ini\n[settings]\nBOT_ARGUMENTS=[\"--no-sandbox\"]\n```\n\n### Store Preferences (Optional)\nStore preferences in a json file, the format it's the same for all the supported drivers, check carefully that the exact string and value it's implemented for the selected driver.\n\n#### Firefox prefs:\n```json\n-- preferences.json \n{\n    \"browser.download.manager.showWhenStarting\": false, # Don't show download\n    \"browser.helperApps.neverAsk.saveToDisk\": \"application/pdf\" # Automatic save pdf files\n}\n```\n\n#### Chrome prefs:\n```json\n-- preferences.json \n{\n    \"profile.default_content_setting_values.notifications\": 2,  # Disable notifications\n    \"profile.default_content_settings.popups\": 0  # Allow popups\n}\n```", "code_before": "# fastbots\nA simple library for fast bot and scraper development using selenium and the POM (Page Object Model) design.\nIncrease your productivity, focusing only one the scraping, less boilerplate code and don't require directly driver managment related code, with browser independent settings.\nIf the site locators changes, this don't require changes in the code, only in the configuration.\n\n## Installation:\nThe installation is a simple process with pip on the\nPyPy repository.\n```bash\npip install fastbots\n```\n\n## Showcase:\nCheck out the full example at the repo: [cookiecutter-fastbots](https://github.com/ubertidavide/cookiecutter-fastbots).\n\n### Main Code\nAll the main code example:\n```python\n-- main.py\nimport logging\n\nfrom fastbots import Task, Bot, Page, EC, WebElement, Keys\n\n\nclass ProductPage(Page):\n\n    # page name it's the page_name used in the locators file, see below\n    def __init__(self, bot: Bot, page_name: str = 'product_page'): \n        super().__init__(bot, page_name)\n\n    def forward(self) -> None:\n        logging.info('DO THINGS')\n\n        # using the locators specified in the file give more flexibility and less code changes\n        name_element: WebElement = self.bot.wait.until(EC.element_to_be_clickable(self.__locator__('name_locator')))\n        # store data in the payload section, useful when i need to retrieve data on success\n        self.bot.payload['result'] = name_element.text\n\n        # end the chain of pages interactins\n        return None\n\nclass SearchPage(Page):\n\n    # page name it's the page_name used in the locators file, see below\n    def __init__(self, bot: Bot, page_name: str = 'search_page'):\n        super().__init__(bot, page_name)\n\n    def forward(self) -> ProductPage:\n        logging.info('DO THINGS')\n\n        # using the locators specified in the file give more flexibility and less code changes\n        search_element: WebElement = self.bot.wait.until(EC.element_to_be_clickable(self.__locator__('search_locator')))\n        search_element.send_keys('Selenium with Python Simplified For Beginners')\n        search_element.send_keys(Keys.ENTER)\n\n        # product_element: WebElement = self.bot.driver.find_element(*self.__locator__('product_locator'))\n        product_element: WebElement = self.bot.wait.until(EC.element_to_be_clickable(self.__locator__('product_locator')))\n        product_element.click()\n\n        # continue the chain interaction in the next page\n        return ProductPage(bot=self.bot)\n\nclass TestTask(Task):\n\n    # main task code\n    def run(self, bot: Bot) -> bool:\n        logging.info('DO THINGS')\n\n        # open the search page do things and go forward\n        page: Page = SearchPage(bot=bot).forward()\n\n        # for every page founded do things and go forward\n        while page:\n            page = page.forward()\n\n        # for default it will succeed\n        return True\n\n    # method executed on bot success, with it's payload\n    def on_success(self, payload):\n        logging.info(f'SUCCESS {payload}')\n    \n    # method executed on bot failure\n    def on_failure(self, payload):\n        logging.info(f'FAILED {payload}')\n        \nif __name__ == '__main__':\n    # start the above task\n    TestTask()()\n```\n\n### Locators File\nIn the locators configuration file there is all the required locators config.\nThis could be changed easily without rebuild or make modifications at the code.\n```ini\n-- locators.ini\n[pages_url] # pages_url required url settings\nstart_url=https://www.amazon.com/ #start_url it's the first page driver.get()\nsearch_page=https://www.amazon.com/ #*_page it's the first page url used for the page_name parameter with it's url that need to match\nproduct_page=https://www.amazon.com/s?k=Selenium+with+Python#*_page it's the second page url used for the page_name parameter with it's url that need to match\n\n[search_page] #*_page first page_name parameter, with it's related locators\nsearch_locator=(By.ID, \"twotabsearchtextbox\")\nproduct_locator=(By.XPATH, '//*[@id=\"search\"]/div[1]/div[1]/div/span[1]/div[1]/div[2]')\n\n[product_page]#*_page second page_name parameter, with it's related locators\nname_locator=(By.ID, \"title\")\n```\n\n### Browser and Drivers (Optional)\nFor default config, the selected browser is Firefox, but it could be changed from the config file:\n```ini\n-- settings.ini\n[settings]\n#BOT_DRIVER_TYPE=FIREFOX\nBOT_DRIVER_TYPE=CHROME\n```\n**The correct browser installed for the driver selected it's required.**\nThe browser installation path is autodetected by system env variables, the driver download process and it's related installation path settings are managed automatically.\n\n### Retry and Debug (Optional)\nFor default every task will be retryed 2 times waiting 10 seconds, when all the two try fail, the task execute the on_error method else it will execute the on_success method.\nThis behaviour could be modified in the settings file:\n```ini\n-- settings.ini\n[settings]\nBOT_MAX_RETRIES=2 #sec default\nBOT_RETRY_DELAY=10 #sec default\n```\nWhen the task is failed the library store the screenshot and the html of the page in the debug folder, useful for debug.\nIt will store also all the logs in the log.log file.\n\n### Page Url Check (Automatic)\nEvery defined page must have a page url and when it's instantiate and reaced by the bot, the library check that the \nspecified url in the config are the same as the reached page during the navigation, to reduce navigation errors.  \nIf you want to disable this function see the Global Wait Section below.\n\n### File Download Wait (Functions)\nThis library have the bot.wait_downloaded_file_path(file_extension, new_name_file=None) method that could be used afer a button download click in order\nto wait and get the path of the downloaded file, it will give the ability also to rename the file.  \nThe extension is used to check that the file downloaded it's the correct and it's not corrupted.\n\n### Download Folder and other Folders (Optional)\n```ini\n-- settings.ini\n[settings]\nBOT_DOWNLOAD_FOLDER_PATH='/usr/...' #override the default download path used for the browser\nBOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH='/debug' # default\nBOT_HTML_DOWNLOAD_FOLDER_PATH='/debug'\n```\n\n### Global Wait (Optional)\nThe default configured wait are showed below:\n- The implicit wait used for inital page loading.\n- The wait for the url check that matches the specified in the locators file\n- The default wait used by the self.bot.wait function\n```ini\n-- settings.ini\n[settings]\nSELENIUM_GLOBAL_IMPLICIT_WAIT=5 #sec default\nSELENIUM_EXPECTED_URL_TIMEOUT=5 #sec default\nSELENIUM_DEFAULT_WAIT=5 #sec default\nSELENIUM_FILE_DOWNLOAD_TIMEOUT=20 #sec default\n\nSELENIUM_EXPECTED_URL_CHECK=False #disable the automatic page url check, the default value it's True\n```\n\n### Proxy (Optional)\nConfigure the proxy settings.\n```ini\n-- settings.ini\n[settings]\nBOT_PROXY_ENABLED=True\nBOT_HTTP_PROXY=127.0.0.1:8080\nBOT_HTTPS_PROXY=127.0.0.1:8080\n```\n\n### User Agent (Optional)\nConfigure the user agent used for the requests.\n```ini\n-- settings.ini\n[settings]\nBOT_USER_AGENT=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n```\n\n### Arguments (Optional)\nConfigure Firefox Arguments, store them in the config file, the format it's the same for all the supported drivers, check carefully that the exact arg it's implemented for the selected driver.\n\n#### Firefox args:\n```ini\n-- settings.ini\n[settings]\nBOT_ARGUMENTS=[\"--headless\", \"--disable-gpu\"]\n```\n\n#### Chrome args\n```ini\n-- settings.ini\n[settings]\nBOT_ARGUMENTS=[\"--no-sandbox\"]\n```\n\n### Store Preferences (Optional)\nStore preferences in a json file, the format it's the same for all the supported drivers, check carefully that the exact string and value it's implemented for the selected driver.\n\n#### Firefox prefs:\n```json\n-- preferences.json \n{\n    \"browser.download.manager.showWhenStarting\": false, # Don't show download\n    \"browser.helperApps.neverAsk.saveToDisk\": \"application/pdf\" # Automatic save pdf files\n}\n```\n\n#### Chrome prefs:\n```json\n-- preferences.json \n{\n    \"profile.default_content_setting_values.notifications\": 2,  # Disable notifications\n    \"profile.default_content_settings.popups\": 0  # Allow popups\n}\n```", "patch": "@@ -130,6 +130,16 @@ BOT_RETRY_DELAY=10 #sec default\n When the task is failed the library store the screenshot and the html of the page in the debug folder, useful for debug.\n It will store also all the logs in the log.log file.\n \n+### Page Url Check (Automatic)\n+Every defined page must have a page url and when it's instantiate and reaced by the bot, the library check that the \n+specified url in the config are the same as the reached page during the navigation, to reduce navigation errors.  \n+If you want to disable this function see the Global Wait Section below.\n+\n+### File Download Wait (Functions)\n+This library have the bot.wait_downloaded_file_path(file_extension, new_name_file=None) method that could be used afer a button download click in order\n+to wait and get the path of the downloaded file, it will give the ability also to rename the file.  \n+The extension is used to check that the file downloaded it's the correct and it's not corrupted.\n+\n ### Download Folder and other Folders (Optional)\n ```ini\n -- settings.ini\n@@ -150,6 +160,7 @@ The default configured wait are showed below:\n SELENIUM_GLOBAL_IMPLICIT_WAIT=5 #sec default\n SELENIUM_EXPECTED_URL_TIMEOUT=5 #sec default\n SELENIUM_DEFAULT_WAIT=5 #sec default\n+SELENIUM_FILE_DOWNLOAD_TIMEOUT=20 #sec default\n \n SELENIUM_EXPECTED_URL_CHECK=False #disable the automatic page url check, the default value it's True\n ```", "file_path": "files/2023_11/214", "file_language": "md", "file_name": "README.md", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/ubertidavide/fastbots/raw/73eb03bd75365e112b39877e26ef52853f5e9f57/fastbots%2Fbot.py", "code": "import tempfile\nimport shutil\nimport pickle\nfrom typing import List, Union\nfrom pathlib import Path\nfrom datetime import datetime\nfrom configparser import ConfigParser\nimport logging\nfrom typing import Type\nfrom abc import ABC, abstractmethod\n\nfrom selenium.webdriver.firefox.options import Options as FirefoxOptions\nfrom selenium.webdriver.chrome.options import Options as ChromeOptions\nfrom selenium.webdriver.firefox.firefox_profile import FirefoxProfile\nfrom selenium.webdriver.support.wait import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.remote.webdriver import WebDriver\n\nfrom fastbots import config, logger\nfrom fastbots.exceptions import ExpectedUrlError, DownloadFileError\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Bot(ABC):\n    \"\"\"\n    Bot\n\n    Class used to specify a bot blueprint.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Bot\n\n        Initialize all the attributes of the Bot instance\n        \"\"\"\n        super().__init__()\n\n        # use a temporary directory as default download folder\n        self._temp_dir: str = tempfile.mkdtemp()\n\n        # official downloaded file folder\n        if config.BOT_DOWNLOAD_FOLDER_PATH != 'None':\n            self._download_dir: str = tempfile.mkdtemp(dir=config.BOT_DOWNLOAD_FOLDER_PATH)\n        else:\n            self._download_dir: str = tempfile.mkdtemp()\n\n        # load all the locators\n        self._locators: ConfigParser = self.__load_locators__()\n        # data store\n        self._payload: dict = {}\n\n    @property\n    def driver(self) -> WebDriver:\n        \"\"\"\n        Driver Getter\n        \"\"\"\n        return self._driver\n    \n    @property\n    def wait(self) -> WebDriverWait:\n        \"\"\"\n        Wait Getter\n        \"\"\"\n        return self._wait\n    \n    @property\n    def payload(self) -> dict:\n        \"\"\"\n        Payload Getter\n        \"\"\"\n        return self._payload\n\n    def __enter__(self) -> Type['Bot']:\n        \"\"\"\n        Enter\n\n        Load and configure all the needed resources.\n        \"\"\"\n        # default global driver settings\n        self._driver.implicitly_wait(config.SELENIUM_GLOBAL_IMPLICIT_WAIT)\n\n        # load the start page\n        self._driver.get(self.locator('pages_url', 'start_url'))\n\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_tb):\n        \"\"\"\n        Exit\n        \n        Clean all the used resources.\n        \"\"\"\n        shutil.rmtree(self._temp_dir)\n        self._driver.close()\n\n    def check_page_url(self, expected_page_url: str):\n        \"\"\"\n        Check Page Url\n\n        Check that the browser it't at the expected page.\n        \"\"\"\n        try:\n            # polling that the page url is the expected\n            WebDriverWait(driver=self._driver, timeout=config.SELENIUM_EXPECTED_URL_TIMEOUT, poll_frequency=1).until(\n                EC.url_to_be(expected_page_url)\n            )\n\n        except TimeoutException as te:\n            # if not the expected url raises an exception\n            raise ExpectedUrlError(current_url=self._driver.current_url, expected_url=expected_page_url)\n\n    def locator(self, page_name: str, locator_name: str) -> str:\n        \"\"\"\n        Locator\n\n        Getter that get the locator used for a page locator\n        \"\"\"\n        if not self._locators.has_section(page_name):\n            raise ValueError(f'The specified page_name: {page_name} is not declared in locators config.')\n        \n        if not self._locators.has_option(page_name, locator_name):\n            raise ValueError(f'The specified locator_name: {locator_name} is not declared in locators config.')\n        \n        return self._locators.get(page_name, locator_name)\n        \n    def wait_downloaded_file_path(self, file_extension: str, new_file_name: str | None = None) -> str:\n        \"\"\"\n        Wait Downloaded File Path\n\n        This method allow to wait for a specific downloaded file to be completely available in the download folder.\n        It uses the file extension in order to wait the full download finish.\n        It will also give the ability to rename the downloaded file.\n\n        The file_extension must be specified without the dot \".\" (ex .png become png)\n        \"\"\"\n        try:\n            # polling that the page url is the expected, it uses the extension because the temp part file cache by browser\n            # usally have a specific extension that isn't the usally of the files\n            WebDriverWait(driver=self._driver, timeout=config.SELENIUM_FILE_DOWNLOAD_TIMEOUT, poll_frequency=1).until(\n                lambda driver: len(list(Path(self._temp_dir).glob(f'*.{file_extension}'))) == 1\n            )\n\n            # get the latest downloaded file\n            latest_file: Path = max(list(Path(self._temp_dir).glob(f'*.{file_extension}')), key=lambda x: x.stat().st_ctime)\n\n            # build the download path based on renamed file or \n            downloaded_file_path: Path = None\n            if new_file_name is None:\n                downloaded_file_path = Path(config.BOT_DOWNLOAD_FOLDER_PATH) / latest_file.name\n            else:\n                downloaded_file_path = Path(config.BOT_DOWNLOAD_FOLDER_PATH) / f'{new_file_name}.{file_extension}'\n                \n            # move to the download folder the file name\n            shutil.move(src=str(latest_file.absolute()), dst=str(downloaded_file_path.absolute()))\n\n            # remove the temporary downloaded file\n            latest_file.unlink()\n\n            # return the path and filename as string\n            return str(downloaded_file_path.absolute())\n\n        except TimeoutException as te:\n            # if not the expected url raises an exception\n            file_count: int = len(list(Path(self._temp_dir).glob(f'*.{file_extension}')))\n\n            # error string based on the specific error\n            if file_count == 0:\n                raise DownloadFileError('File not founded in the download folder, an error with the download occurs.')\n            elif file_count > 1:\n                raise DownloadFileError(f'Too many downloaded files founded, files number : {file_count}.')\n\n            raise DownloadFileError()\n\n    def save_screenshot(self):\n        \"\"\"\n        Save Screenshot\n\n        Save the browser's screenshot to a png file, the path could be specified in the settings.\n        \"\"\"\n        if not Path(config.BOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH).exists():\n            Path(config.BOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH).mkdir(exist_ok=True, parents=True)\n\n        file_path: Path = Path(config.BOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH) / f'{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.png'\n        self._driver.save_screenshot(str(file_path.absolute()))\n\n    def save_html(self):\n        \"\"\"\n        Save Html\n\n        Save the browser's html page to a file, the path could be specified in the settings.\n        \"\"\"\n        if not Path(config.BOT_HTML_DOWNLOAD_FOLDER_PATH).exists():\n            Path(config.BOT_HTML_DOWNLOAD_FOLDER_PATH).mkdir(exist_ok=True, parents=True)\n\n        file_path: Path = Path(config.BOT_HTML_DOWNLOAD_FOLDER_PATH) / f'{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.html'\n        with open(str(file_path.absolute()), \"w\", encoding=\"utf-8\") as file:\n            file.write(self._driver.page_source)\n\n    def save_cookies(self):\n        \"\"\"\n        Save Cookies\n\n        Save all the cookies founded in the file.\n        \"\"\"\n        cookies: List[dict] = self._driver.get_cookies()\n\n        with open(config.BOT_COOKIES_FILE_PATH, 'wb') as file:\n            pickle.dump(cookies, file)\n    \n    def load_cookies(self):\n        \"\"\"\n        Load Cookies\n\n        Add all the cookies founded in the file.\n        \"\"\"\n        if Path(config.BOT_COOKIES_FILE_PATH).is_file():\n            with open(config.BOT_COOKIES_FILE_PATH, 'rb') as file:\n                cookies = pickle.load(file)\n\n                for cookie in cookies:\n                    self._driver.add_cookie(cookie)\n\n    def __load_locators__(self) -> ConfigParser:\n        \"\"\"\n        Load Locators\n\n        Load a file that contains all the locators\n        \"\"\"\n        if not Path(config.SELENIUM_LOCATORS_FILE).is_file():\n            return ValueError(f'Erorr, locators file not founded at path: {config.SELENIUM_LOCATORS_FILE}')\n        \n        config_parser: ConfigParser = ConfigParser()\n        config_parser.read(config.SELENIUM_LOCATORS_FILE)\n        return config_parser\n\n    @abstractmethod\n    def __load_preferences__(self) -> Union[FirefoxProfile, dict]:\n        \"\"\"\n        Load Preferences\n\n        Load all the preferences stored in a json file,\n        specified in the config.\n        \"\"\"\n        return NotImplementedError('Bot must define this method.')\n\n    @abstractmethod\n    def __load_options__(self) -> Union[FirefoxOptions, ChromeOptions]:\n        \"\"\"\n        Load Options\n\n        Load all the default options\n        \"\"\"\n        return NotImplementedError('Bot must define this method.')\n    \n    @abstractmethod\n    def __load_driver__(self) -> WebDriver:\n        \"\"\"\n        Load Driver\n\n        Load and configure all the options for the driver.\n        \"\"\"\n        return NotImplementedError('Bot must define this method.')", "code_before": "import tempfile\nimport shutil\nimport pickle\nfrom typing import List, Union\nfrom pathlib import Path\nfrom datetime import datetime\nfrom configparser import ConfigParser\nimport logging\nfrom typing import Type\nfrom abc import ABC, abstractmethod\n\nfrom selenium.webdriver.firefox.options import Options as FirefoxOptions\nfrom selenium.webdriver.chrome.options import Options as ChromeOptions\nfrom selenium.webdriver.firefox.firefox_profile import FirefoxProfile\nfrom selenium.webdriver.support.wait import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.remote.webdriver import WebDriver\n\nfrom fastbots import config, logger\nfrom fastbots.exceptions import ExpectedUrlError, DownloadFileError\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Bot(ABC):\n    \"\"\"\n    Bot\n\n    Class used to specify a bot blueprint.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Bot\n\n        Initialize all the attributes of the Bot instance\n        \"\"\"\n        super().__init__()\n\n        # use a temporary directory as default download folder\n        self._temp_dir: str = tempfile.mkdtemp()\n\n        # official downloaded file folder\n        if config.BOT_DOWNLOAD_FOLDER_PATH != 'None':\n            self._download_dir: str = tempfile.mkdtemp(dir=config.BOT_DOWNLOAD_FOLDER_PATH)\n        else:\n            self._download_dir: str = tempfile.mkdtemp()\n\n        # load all the locators\n        self._locators: ConfigParser = self.__load_locators__()\n        # data store\n        self._payload: dict = {}\n\n    @property\n    def driver(self) -> WebDriver:\n        \"\"\"\n        Driver Getter\n        \"\"\"\n        return self._driver\n    \n    @property\n    def wait(self) -> WebDriverWait:\n        \"\"\"\n        Wait Getter\n        \"\"\"\n        return self._wait\n    \n    @property\n    def payload(self) -> dict:\n        \"\"\"\n        Payload Getter\n        \"\"\"\n        return self._payload\n\n    def __enter__(self) -> Type['Bot']:\n        \"\"\"\n        Enter\n\n        Load and configure all the needed resources.\n        \"\"\"\n        # default global driver settings\n        self._driver.implicitly_wait(config.SELENIUM_GLOBAL_IMPLICIT_WAIT)\n\n        # load the start page\n        self._driver.get(self.locator('pages_url', 'start_url'))\n\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_tb):\n        \"\"\"\n        Exit\n        \n        Clean all the used resources.\n        \"\"\"\n        shutil.rmtree(self._temp_dir)\n        self._driver.close()\n\n    def check_page_url(self, expected_page_url: str):\n        \"\"\"\n        Check Page Url\n\n        Check that the browser it't at the expected page.\n        \"\"\"\n        try:\n            # polling that the page url is the expected\n            WebDriverWait(driver=self._driver, timeout=config.SELENIUM_EXPECTED_URL_TIMEOUT, poll_frequency=1).until(\n                EC.url_to_be(expected_page_url)\n            )\n\n        except TimeoutException as te:\n            # if not the expected url raises an exception\n            raise ExpectedUrlError(current_url=self._driver.current_url, expected_url=expected_page_url)\n\n    def locator(self, page_name: str, locator_name: str) -> str:\n        \"\"\"\n        Locator\n\n        Getter that get the locator used for a page locator\n        \"\"\"\n        if not self._locators.has_section(page_name):\n            raise ValueError(f'The specified page_name: {page_name} is not declared in locators config.')\n        \n        if not self._locators.has_option(page_name, locator_name):\n            raise ValueError(f'The specified locator_name: {locator_name} is not declared in locators config.')\n        \n        return self._locators.get(page_name, locator_name)\n        \n    def wait_downloaded_file_path(self, file_extension: str, new_file_name: str | None = None) -> str:\n        \"\"\"\n        Wait Downloaded File Path\n\n        This method allow to wait for a specific downloaded file to be completely available in the download folder.\n        It uses the file extension in order to wait the full download finish.\n        It will also give the ability to rename the downloaded file.\n\n        The file_extension must be specified without the dot \".\" (ex .png become png)\n        \"\"\"\n        try:\n            # polling that the page url is the expected, it uses the extension because the temp part file cache by browser\n            # usally have a specific extension that isn't the usally of the files\n            WebDriverWait(driver=self._driver, timeout=config.SELENIUM_FILE_DOWNLOAD_TIMEOUT, poll_frequency=1).until(\n                lambda driver: len(list(Path(self._temp_dir).glob(f'*.{file_extension}'))) == 1\n            )\n\n            # get the latest downloaded file\n            latest_file: Path = max(list(Path(self._temp_dir).glob(f'*.{file_extension}')), key=lambda x: x.stat().st_ctime)\n\n            # build the download path based on renamed file or \n            downloaded_file_path: Path = None\n            if new_file_name is None:\n                downloaded_file_path = Path(config.BOT_DOWNLOAD_FOLDER_PATH) / latest_file.name\n            else:\n                downloaded_file_path = Path(config.BOT_DOWNLOAD_FOLDER_PATH) / f'{new_file_name}.{file_extension}'\n                \n            # move to the download folder the file name\n            shutil.move(src=str(latest_file.absolute()), dst=str(downloaded_file_path.absolute()))\n\n            # remove the temporary downloaded file\n            latest_file.unlink()\n\n            # return the path and filename as string\n            return str(downloaded_file_path.absolute())\n\n        except TimeoutException as te:\n            # if not the expected url raises an exception\n            file_count: int = len(list(Path(self._temp_dir).glob(f'*.{file_extension}')))\n\n            # error string based on the specific error\n            if file_count == 0:\n                raise DownloadFileError('File not founded in the download folder, an error with the download occurs.')\n            elif file_count > 1:\n                raise DownloadFileError(f'Too many downloaded files founded, files number : {file_count}.')\n\n            raise DownloadFileError()\n\n    def save_screenshot(self):\n        \"\"\"\n        Save Screenshot\n\n        Save the browser's screenshot to a png file, the path could be specified in the settings.\n        \"\"\"\n        if not Path(config.BOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH).exists():\n            Path(config.BOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH).mkdir(exist_ok=True, parents=True)\n\n        file_path: Path = Path(config.BOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH) / f'{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.png'\n        self._driver.save_screenshot(str(file_path.absolute()))\n\n    def save_html(self):\n        \"\"\"\n        Save Html\n\n        Save the browser's html page to a file, the path could be specified in the settings.\n        \"\"\"\n        if not Path(config.BOT_HTML_DOWNLOAD_FOLDER_PATH).exists():\n            Path(config.BOT_HTML_DOWNLOAD_FOLDER_PATH).mkdir(exist_ok=True, parents=True)\n\n        file_path: Path = Path(config.BOT_HTML_DOWNLOAD_FOLDER_PATH) / f'{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.html'\n        with open(str(file_path.absolute()), \"w\", encoding=\"utf-8\") as file:\n            file.write(self._driver.page_source)\n\n    def save_cookies(self):\n        \"\"\"\n        Save Cookies\n\n        Save all the cookies founded in the file.\n        \"\"\"\n        cookies: List[dict] = self._driver.get_cookies()\n\n        with open(config.BOT_COOKIES_FILE_PATH, 'wb') as file:\n            pickle.dump(cookies, file)\n    \n    def load_cookies(self):\n        \"\"\"\n        Load Cookies\n\n        Add all the cookies founded in the file.\n        \"\"\"\n        if Path(config.BOT_COOKIES_FILE_PATH).is_file():\n            with open(config.BOT_COOKIES_FILE_PATH, 'rb') as file:\n                cookies = pickle.load(file)\n\n                for cookie in cookies:\n                    self._driver.add_cookie(cookie)\n\n    def __load_locators__(self) -> ConfigParser:\n        \"\"\"\n        Load Locators\n\n        Load a file that contains all the locators\n        \"\"\"\n        if not Path(config.SELENIUM_LOCATORS_FILE).is_file():\n            return ValueError(f'Erorr, locators file not founded at path: {config.SELENIUM_LOCATORS_FILE}')\n        \n        config_parser: ConfigParser = ConfigParser()\n        config_parser.read(config.SELENIUM_LOCATORS_FILE)\n        return config_parser\n\n    @abstractmethod\n    def __load_preferences__(self) -> Union[FirefoxProfile, dict]:\n        \"\"\"\n        Load Preferences\n\n        Load all the preferences stored in a json file,\n        specified in the config.\n        \"\"\"\n        return NotImplementedError('Bot must define this method.')\n\n    @abstractmethod\n    def __load_options__(self) -> Union[FirefoxOptions, ChromeOptions]:\n        \"\"\"\n        Load Options\n\n        Load all the default options\n        \"\"\"\n        return NotImplementedError('Bot must define this method.')\n    \n    @abstractmethod\n    def __load_driver__(self) -> WebDriver:\n        \"\"\"\n        Load Driver\n\n        Load and configure all the options for the driver.\n        \"\"\"\n        return NotImplementedError('Bot must define this method.')", "patch": "@@ -17,8 +17,8 @@\n from selenium.common.exceptions import TimeoutException\n from selenium.webdriver.remote.webdriver import WebDriver\n \n-from fastbots import config\n-from fastbots.exceptions import ExpectedUrlError\n+from fastbots import config, logger\n+from fastbots.exceptions import ExpectedUrlError, DownloadFileError\n \n \n logger = logging.getLogger(__name__)\n@@ -40,10 +40,13 @@ def __init__(self) -> None:\n         super().__init__()\n \n         # use a temporary directory as default download folder\n+        self._temp_dir: str = tempfile.mkdtemp()\n+\n+        # official downloaded file folder\n         if config.BOT_DOWNLOAD_FOLDER_PATH != 'None':\n-            self._temp_dir: str = tempfile.mkdtemp(dir=config.BOT_DOWNLOAD_FOLDER_PATH)\n+            self._download_dir: str = tempfile.mkdtemp(dir=config.BOT_DOWNLOAD_FOLDER_PATH)\n         else:\n-            self._temp_dir: str = tempfile.mkdtemp()\n+            self._download_dir: str = tempfile.mkdtemp()\n \n         # load all the locators\n         self._locators: ConfigParser = self.__load_locators__()\n@@ -118,10 +121,60 @@ def locator(self, page_name: str, locator_name: str) -> str:\n         \"\"\"\n         if not self._locators.has_section(page_name):\n             raise ValueError(f'The specified page_name: {page_name} is not declared in locators config.')\n+        \n         if not self._locators.has_option(page_name, locator_name):\n             raise ValueError(f'The specified locator_name: {locator_name} is not declared in locators config.')\n+        \n         return self._locators.get(page_name, locator_name)\n         \n+    def wait_downloaded_file_path(self, file_extension: str, new_file_name: str | None = None) -> str:\n+        \"\"\"\n+        Wait Downloaded File Path\n+\n+        This method allow to wait for a specific downloaded file to be completely available in the download folder.\n+        It uses the file extension in order to wait the full download finish.\n+        It will also give the ability to rename the downloaded file.\n+\n+        The file_extension must be specified without the dot \".\" (ex .png become png)\n+        \"\"\"\n+        try:\n+            # polling that the page url is the expected, it uses the extension because the temp part file cache by browser\n+            # usally have a specific extension that isn't the usally of the files\n+            WebDriverWait(driver=self._driver, timeout=config.SELENIUM_FILE_DOWNLOAD_TIMEOUT, poll_frequency=1).until(\n+                lambda driver: len(list(Path(self._temp_dir).glob(f'*.{file_extension}'))) == 1\n+            )\n+\n+            # get the latest downloaded file\n+            latest_file: Path = max(list(Path(self._temp_dir).glob(f'*.{file_extension}')), key=lambda x: x.stat().st_ctime)\n+\n+            # build the download path based on renamed file or \n+            downloaded_file_path: Path = None\n+            if new_file_name is None:\n+                downloaded_file_path = Path(config.BOT_DOWNLOAD_FOLDER_PATH) / latest_file.name\n+            else:\n+                downloaded_file_path = Path(config.BOT_DOWNLOAD_FOLDER_PATH) / f'{new_file_name}.{file_extension}'\n+                \n+            # move to the download folder the file name\n+            shutil.move(src=str(latest_file.absolute()), dst=str(downloaded_file_path.absolute()))\n+\n+            # remove the temporary downloaded file\n+            latest_file.unlink()\n+\n+            # return the path and filename as string\n+            return str(downloaded_file_path.absolute())\n+\n+        except TimeoutException as te:\n+            # if not the expected url raises an exception\n+            file_count: int = len(list(Path(self._temp_dir).glob(f'*.{file_extension}')))\n+\n+            # error string based on the specific error\n+            if file_count == 0:\n+                raise DownloadFileError('File not founded in the download folder, an error with the download occurs.')\n+            elif file_count > 1:\n+                raise DownloadFileError(f'Too many downloaded files founded, files number : {file_count}.')\n+\n+            raise DownloadFileError()\n+\n     def save_screenshot(self):\n         \"\"\"\n         Save Screenshot", "file_path": "files/2023_11/215", "file_language": "py", "file_name": "fastbots/bot.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/ubertidavide/fastbots/raw/73eb03bd75365e112b39877e26ef52853f5e9f57/fastbots%2Fconfig.py", "code": "import logging\nfrom typing import List\nfrom enum import Enum\n\nfrom decouple import config\n\n\nclass DriverType(Enum):\n    FIREFOX = 1\n    CHROME = 2\n\n# static config\nENV_DEVELOPMENT: str = 'development'\nENV_RELEASE: str = 'release'\n\n# dynamic config\n\n# project settings\nLOG_LEVEL: int = config('LOGLEVEL', default=logging.DEBUG, cast=int)\nENV: str = config('ENV', default=ENV_DEVELOPMENT, cast=str)\n\nPROJECT_NAME: str = config('PROJECT_NAME', default='fastbot', cast=str)\nAPP_VERSION: str = config('APP_VERSION', default='0.1.0', cast=str)\n\n# firefox settings\nBOT_DRIVER_TYPE: DriverType = config('BOT_DRIVER_TYPE', default=DriverType.FIREFOX, cast=DriverType)\nBOT_DOWNLOAD_FOLDER_PATH: str = config('BOT_DOWNLOAD_FOLDER_PATH', default=None, cast=str)\n\n# comma separated list of arguments (ex: --headless, --disable-gui)\nBOT_ARGUMENTS: List[str] = config('BOT_ARGUMENTS', default=[])\n\nBOT_USER_AGENT: str = config('BOT_USER_AGENT', default=f'{PROJECT_NAME} {APP_VERSION}', cast=str)\n\nBOT_PROXY_ENABLED: bool = config('BOT_PROXY_ENABLED', default=False, cast=bool)\nBOT_HTTP_PROXY: str = config('BOT_HTTP_PROXY', default=None, cast=str)\nBOT_HTTPS_PROXY: str = config('BOT_HTTPS_PROXY', default=BOT_HTTP_PROXY, cast=str)\n\nBOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH: str = config('BOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH', default='debug/', cast=str)\nBOT_HTML_DOWNLOAD_FOLDER_PATH: str = config('BOT_HTML_DOWNLOAD_FOLDER_PATH', default='debug/', cast=str)\nBOT_COOKIES_FILE_PATH: str = config('BOT_COOKIES_FILE_PATH', default='cookies.pkl', cast=str)\n\nBOT_PREFERENCES_FILE_PATH: str = config('BOT_PREFERENCES_FILE_PATH', default='preferences.json', cast=str)\n\nBOT_MAX_RETRIES: int = config('BOT_MAX_RETRIES', default=2, cast=int)\nBOT_RETRY_DELAY: int = config('BOT_RETRY_DELAY', default=10, cast=int)\n\n# selenium\nSELENIUM_GLOBAL_IMPLICIT_WAIT: int = config('SELENIUM_GLOBAL_IMPLICIT_WAIT', default=5, cast=int)\nSELENIUM_EXPECTED_URL_CHECK: bool = config('SELENIUM_EXPECTED_URL_CHECK', default=True, cast=bool)\nSELENIUM_EXPECTED_URL_TIMEOUT: int = config('SELENIUM_EXPECTED_URL_TIMEOUT', default=5, cast=int)\nSELENIUM_DEFAULT_WAIT: int = config('SELENIUM_DEFAULT_WAIT', default=5, cast=int)\nSELENIUM_FILE_DOWNLOAD_TIMEOUT: int = config('SELENIUM_FILE_DOWNLOAD_TIMEOUT', default=20, cast=int)\nSELENIUM_LOCATORS_FILE: str = config('SELENIUM_LOCATORS_FILE', default='locators.ini', cast=str)", "code_before": "import logging\nfrom typing import List\nfrom enum import Enum\n\nfrom decouple import config\n\n\nclass DriverType(Enum):\n    FIREFOX = 1\n    CHROME = 2\n\n# static config\nENV_DEVELOPMENT: str = 'development'\nENV_RELEASE: str = 'release'\n\n# dynamic config\n\n# project settings\nLOG_LEVEL: int = config('LOGLEVEL', default=logging.DEBUG, cast=int)\nENV: str = config('ENV', default=ENV_DEVELOPMENT, cast=str)\n\nPROJECT_NAME: str = config('PROJECT_NAME', default='fastbot', cast=str)\nAPP_VERSION: str = config('APP_VERSION', default='0.1.0', cast=str)\n\n# firefox settings\nBOT_DRIVER_TYPE: DriverType = config('BOT_DRIVER_TYPE', default=DriverType.FIREFOX, cast=DriverType)\nBOT_DOWNLOAD_FOLDER_PATH: str = config('BOT_DOWNLOAD_FOLDER_PATH', default=None, cast=str)\n\n# comma separated list of arguments (ex: --headless, --disable-gui)\nBOT_ARGUMENTS: List[str] = config('BOT_ARGUMENTS', default=[])\n\nBOT_USER_AGENT: str = config('BOT_USER_AGENT', default=f'{PROJECT_NAME} {APP_VERSION}', cast=str)\n\nBOT_PROXY_ENABLED: bool = config('BOT_PROXY_ENABLED', default=False, cast=bool)\nBOT_HTTP_PROXY: str = config('BOT_HTTP_PROXY', default=None, cast=str)\nBOT_HTTPS_PROXY: str = config('BOT_HTTPS_PROXY', default=BOT_HTTP_PROXY, cast=str)\n\nBOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH: str = config('BOT_SCREENSHOT_DOWNLOAD_FOLDER_PATH', default='debug/', cast=str)\nBOT_HTML_DOWNLOAD_FOLDER_PATH: str = config('BOT_HTML_DOWNLOAD_FOLDER_PATH', default='debug/', cast=str)\nBOT_COOKIES_FILE_PATH: str = config('BOT_COOKIES_FILE_PATH', default='cookies.pkl', cast=str)\n\nBOT_PREFERENCES_FILE_PATH: str = config('BOT_PREFERENCES_FILE_PATH', default='preferences.json', cast=str)\n\nBOT_MAX_RETRIES: int = config('BOT_MAX_RETRIES', default=2, cast=int)\nBOT_RETRY_DELAY: int = config('BOT_RETRY_DELAY', default=10, cast=int)\n\n# selenium\nSELENIUM_GLOBAL_IMPLICIT_WAIT: int = config('SELENIUM_GLOBAL_IMPLICIT_WAIT', default=5, cast=int)\nSELENIUM_EXPECTED_URL_CHECK: bool = config('SELENIUM_EXPECTED_URL_CHECK', default=True, cast=bool)\nSELENIUM_EXPECTED_URL_TIMEOUT: int = config('SELENIUM_EXPECTED_URL_TIMEOUT', default=5, cast=int)\nSELENIUM_DEFAULT_WAIT: int = config('SELENIUM_DEFAULT_WAIT', default=5, cast=int)\nSELENIUM_FILE_DOWNLOAD_TIMEOUT: int = config('SELENIUM_FILE_DOWNLOAD_TIMEOUT', default=20, cast=int)\nSELENIUM_LOCATORS_FILE: str = config('SELENIUM_LOCATORS_FILE', default='locators.ini', cast=str)", "patch": "@@ -49,4 +49,5 @@ class DriverType(Enum):\n SELENIUM_EXPECTED_URL_CHECK: bool = config('SELENIUM_EXPECTED_URL_CHECK', default=True, cast=bool)\n SELENIUM_EXPECTED_URL_TIMEOUT: int = config('SELENIUM_EXPECTED_URL_TIMEOUT', default=5, cast=int)\n SELENIUM_DEFAULT_WAIT: int = config('SELENIUM_DEFAULT_WAIT', default=5, cast=int)\n+SELENIUM_FILE_DOWNLOAD_TIMEOUT: int = config('SELENIUM_FILE_DOWNLOAD_TIMEOUT', default=20, cast=int)\n SELENIUM_LOCATORS_FILE: str = config('SELENIUM_LOCATORS_FILE', default='locators.ini', cast=str)\n\\ No newline at end of file", "file_path": "files/2023_11/216", "file_language": "py", "file_name": "fastbots/config.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class DriverType(Enum):\n    FIREFOX = 1\n    CHROME = 2", "target": 0}], "function_after": [{"function": "class DriverType(Enum):\n    FIREFOX = 1\n    CHROME = 2", "target": 0}]}, {"raw_url": "https://github.com/ubertidavide/fastbots/raw/73eb03bd75365e112b39877e26ef52853f5e9f57/fastbots%2Fexceptions.py", "code": "class GenericError(Exception):\n    \"\"\" Generic Error \"\"\"\n    \n    def __init__(self, message: str = 'Generic Error') -> None:\n        self.message: str = message\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message\n\nclass ExpectedUrlError(GenericError):\n    \"\"\" \n    Expected Url Error\n\n    Happen when the current url is not the expected. \n    \"\"\"\n    \n    def __init__(self, current_url: str, expected_url: str) -> None:\n        self.current_url: str = current_url\n        self.expected_url: str = expected_url\n        self.message: str = f'The current url: {self.current_url} of the browser is not the expected: {self.expected_url}'\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message\n    \nclass DownloadFileError(GenericError):\n    \"\"\"\n    Download File Error\n\n    Happen when an error occurs in the downloading process.\n    \"\"\"\n\n    def __init__(self, message: str = 'Download File Error') -> None:\n        self.message: str = message\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message\n", "code_before": "class GenericError(Exception):\n    \"\"\" Generic Error \"\"\"\n    \n    def __init__(self, message: str = 'Generic Error') -> None:\n        self.message: str = message\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message\n\nclass ExpectedUrlError(GenericError):\n    \"\"\" \n    Expected Url Error\n\n    Happen when the current url is not the expected. \n    \"\"\"\n    \n    def __init__(self, current_url: str, expected_url: str) -> None:\n        self.current_url: str = current_url\n        self.expected_url: str = expected_url\n        self.message: str = f'The current url: {self.current_url} of the browser is not the expected: {self.expected_url}'\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message\n    \nclass DownloadFileError(GenericError):\n    \"\"\"\n    Download File Error\n\n    Happen when an error occurs in the downloading process.\n    \"\"\"\n\n    def __init__(self, message: str = 'Download File Error') -> None:\n        self.message: str = message\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message\n", "patch": "@@ -23,3 +23,17 @@ def __init__(self, current_url: str, expected_url: str) -> None:\n \n     def __str__(self) -> str:\n         return self.message\n+    \n+class DownloadFileError(GenericError):\n+    \"\"\"\n+    Download File Error\n+\n+    Happen when an error occurs in the downloading process.\n+    \"\"\"\n+\n+    def __init__(self, message: str = 'Download File Error') -> None:\n+        self.message: str = message\n+        super().__init__(self.message)\n+\n+    def __str__(self) -> str:\n+        return self.message", "file_path": "files/2023_11/217", "file_language": "py", "file_name": "fastbots/exceptions.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class GenericError(Exception):\n    \"\"\" Generic Error \"\"\"\n    \n    def __init__(self, message: str = 'Generic Error') -> None:\n        self.message: str = message\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message", "target": 0}, {"function": "class ExpectedUrlError(GenericError):\n    \"\"\" \n    Expected Url Error\n\n    Happen when the current url is not the expected. \n    \"\"\"\n    \n    def __init__(self, current_url: str, expected_url: str) -> None:\n        self.current_url: str = current_url\n        self.expected_url: str = expected_url\n        self.message: str = f'The current url: {self.current_url} of the browser is not the expected: {self.expected_url}'\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message", "target": 0}, {"function": "class DownloadFileError(GenericError):\n    \"\"\"\n    Download File Error\n\n    Happen when an error occurs in the downloading process.\n    \"\"\"\n\n    def __init__(self, message: str = 'Download File Error') -> None:\n        self.message: str = message\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message", "target": 0}], "function_after": [{"function": "class GenericError(Exception):\n    \"\"\" Generic Error \"\"\"\n    \n    def __init__(self, message: str = 'Generic Error') -> None:\n        self.message: str = message\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message", "target": 0}, {"function": "class ExpectedUrlError(GenericError):\n    \"\"\" \n    Expected Url Error\n\n    Happen when the current url is not the expected. \n    \"\"\"\n    \n    def __init__(self, current_url: str, expected_url: str) -> None:\n        self.current_url: str = current_url\n        self.expected_url: str = expected_url\n        self.message: str = f'The current url: {self.current_url} of the browser is not the expected: {self.expected_url}'\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message", "target": 0}, {"function": "class DownloadFileError(GenericError):\n    \"\"\"\n    Download File Error\n\n    Happen when an error occurs in the downloading process.\n    \"\"\"\n\n    def __init__(self, message: str = 'Download File Error') -> None:\n        self.message: str = message\n        super().__init__(self.message)\n\n    def __str__(self) -> str:\n        return self.message", "target": 0}]}, {"raw_url": "https://github.com/ubertidavide/fastbots/raw/73eb03bd75365e112b39877e26ef52853f5e9f57/fastbots%2Fpage.py", "code": "import logging\nfrom abc import ABC, abstractmethod\nfrom typing import Type, Union, Dict\n\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.remote.webelement import WebElement\n\nfrom fastbots.bot import Bot\nfrom fastbots import config\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Page(ABC):\n    \"\"\"\n    Page\n\n    A web page blueprint used to forward pages and\n    make a series of actions on it.\n    \"\"\"\n\n    def __init__(self, bot: Bot, page_name: str = 'page_name'):\n        \"\"\"\n        Initialize the page class.\n\n        In the locators file must be declared the page in the below format:\n\n        [pages_url]\n        page_name=https://example.com/page\n        \"\"\"\n        super().__init__()\n\n        self._bot: Bot = bot\n        self._page_name: str = page_name\n        \n        # load the pages url from the locators file\n        self._page_url: str = self._bot.locator('pages_url', self._page_name)\n\n        # check that the current page is the expected\n        if config.SELENIUM_EXPECTED_URL_CHECK and self._page_url != 'None':\n            self._bot.check_page_url(expected_page_url=self._page_url)\n\n    @property\n    def bot(self):\n        return self._bot\n\n    def __locator__(self, locator_name: str) -> tuple:\n        \"\"\"\n        Locator\n\n        Utility used to load the locator.\n\n        The locators in the file must be in the below format:\n\n        [page_name]\n        locator_name=(By.XPATH, \"//html//input\")\n\n        \"\"\"\n        # load the locators from file and interprete that as code\n        full_locator: str = self._bot.locator(self._page_name, locator_name)\n\n        if not full_locator.startswith('(') or not full_locator.endswith(')'):\n            raise ValueError('The locator must be enclosed in round brackets.')\n\n        # declared locators\n        locator_list: Dict[str, By] = ['By.ID', 'By.XPATH', 'By.NAME', 'By.CLASS_NAME', 'By.CSS_SELECTOR', \n                                       'By.LINK_TEXT', 'By.PARTIAL_LINK_TEXT', 'By.TAG_NAME']\n\n        # check the used locator\n        parsed_locator: tuple = None\n        for locator in locator_list:\n            # check that the first characters are them of the locators and the next one of the comma \n            if full_locator[1:-1].strip().startswith(locator) and full_locator[1:-1].strip()[len(locator):].strip().startswith(','):\n                # extract the tuple required as locator\n                parsed_locator = (eval(locator), full_locator[1:-1].strip()[len(locator):].strip()[1:].strip()[1:-1])\n\n                logging.info(f'{parsed_locator}')\n\n                return parsed_locator\n            \n        else:\n            raise ValueError('The specified locator is unknown or worng, check by, brackets and commas.')\n\n    @abstractmethod\n    def forward(self) -> Union[Type['Page'], None]:\n        \"\"\"\n        Forward\n\n        This method represents a series of action in one page,\n        in order to pass on another page or when the task it's done\n        it will return None, in order to finish.\n        \"\"\"\n        raise NotImplementedError('Tasks must define this method.')", "code_before": "import logging\nfrom abc import ABC, abstractmethod\nfrom typing import Type, Union, Dict\n\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.remote.webelement import WebElement\n\nfrom fastbots.bot import Bot\nfrom fastbots import config\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Page(ABC):\n    \"\"\"\n    Page\n\n    A web page blueprint used to forward pages and\n    make a series of actions on it.\n    \"\"\"\n\n    def __init__(self, bot: Bot, page_name: str = 'page_name'):\n        \"\"\"\n        Initialize the page class.\n\n        In the locators file must be declared the page in the below format:\n\n        [pages_url]\n        page_name=https://example.com/page\n        \"\"\"\n        super().__init__()\n\n        self._bot: Bot = bot\n        self._page_name: str = page_name\n        \n        # load the pages url from the locators file\n        self._page_url: str = self._bot.locator('pages_url', self._page_name)\n\n        # check that the current page is the expected\n        if config.SELENIUM_EXPECTED_URL_CHECK and self._page_url != 'None':\n            self._bot.check_page_url(expected_page_url=self._page_url)\n\n    @property\n    def bot(self):\n        return self._bot\n\n    def __locator__(self, locator_name: str) -> tuple:\n        \"\"\"\n        Locator\n\n        Utility used to load the locator.\n\n        The locators in the file must be in the below format:\n\n        [page_name]\n        locator_name=(By.XPATH, \"//html//input\")\n\n        \"\"\"\n        # load the locators from file and interprete that as code\n        full_locator: str = self._bot.locator(self._page_name, locator_name)\n\n        if not full_locator.startswith('(') or not full_locator.endswith(')'):\n            raise ValueError('The locator must be enclosed in round brackets.')\n\n        # declared locators\n        locator_list: Dict[str, By] = ['By.ID', 'By.XPATH', 'By.NAME', 'By.CLASS_NAME', 'By.CSS_SELECTOR', \n                                       'By.LINK_TEXT', 'By.PARTIAL_LINK_TEXT', 'By.TAG_NAME']\n\n        # check the used locator\n        parsed_locator: tuple = None\n        for locator in locator_list:\n            # check that the first characters are them of the locators and the next one of the comma \n            if full_locator[1:-1].strip().startswith(locator) and full_locator[1:-1].strip()[len(locator):].strip().startswith(','):\n                # extract the tuple required as locator\n                parsed_locator = (eval(locator), full_locator[1:-1].strip()[len(locator):].strip()[1:].strip()[1:-1])\n\n                logging.info(f'{parsed_locator}')\n\n                return parsed_locator\n            \n        else:\n            raise ValueError('The specified locator is unknown or worng, check by, brackets and commas.')\n\n    @abstractmethod\n    def forward(self) -> Union[Type['Page'], None]:\n        \"\"\"\n        Forward\n\n        This method represents a series of action in one page,\n        in order to pass on another page or when the task it's done\n        it will return None, in order to finish.\n        \"\"\"\n        raise NotImplementedError('Tasks must define this method.')", "patch": "@@ -1,6 +1,6 @@\n import logging\n from abc import ABC, abstractmethod\n-from typing import Type, Union\n+from typing import Type, Union, Dict\n \n from selenium.webdriver.common.by import By\n from selenium.webdriver.remote.webelement import WebElement\n@@ -58,7 +58,29 @@ def __locator__(self, locator_name: str) -> tuple:\n \n         \"\"\"\n         # load the locators from file and interprete that as code\n-        return eval(self._bot.locator(self._page_name, locator_name))\n+        full_locator: str = self._bot.locator(self._page_name, locator_name)\n+\n+        if not full_locator.startswith('(') or not full_locator.endswith(')'):\n+            raise ValueError('The locator must be enclosed in round brackets.')\n+\n+        # declared locators\n+        locator_list: Dict[str, By] = ['By.ID', 'By.XPATH', 'By.NAME', 'By.CLASS_NAME', 'By.CSS_SELECTOR', \n+                                       'By.LINK_TEXT', 'By.PARTIAL_LINK_TEXT', 'By.TAG_NAME']\n+\n+        # check the used locator\n+        parsed_locator: tuple = None\n+        for locator in locator_list:\n+            # check that the first characters are them of the locators and the next one of the comma \n+            if full_locator[1:-1].strip().startswith(locator) and full_locator[1:-1].strip()[len(locator):].strip().startswith(','):\n+                # extract the tuple required as locator\n+                parsed_locator = (eval(locator), full_locator[1:-1].strip()[len(locator):].strip()[1:].strip()[1:-1])\n+\n+                logging.info(f'{parsed_locator}')\n+\n+                return parsed_locator\n+            \n+        else:\n+            raise ValueError('The specified locator is unknown or worng, check by, brackets and commas.')\n \n     @abstractmethod\n     def forward(self) -> Union[Type['Page'], None]:", "file_path": "files/2023_11/218", "file_language": "py", "file_name": "fastbots/page.py", "outdated_file_modify": 0, "outdated_file_before": 1, "outdated_file_after": 1, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/ubertidavide/fastbots/raw/73eb03bd75365e112b39877e26ef52853f5e9f57/pyproject.toml", "code": "[tool.poetry]\nname = \"fastbots\"\nversion = \"0.1.4\"\ndescription = \"A simple library for fast bot and scraper development using selenium and the POM (Page Object Model) design.\"\nauthors = [\"Uberti Davide <24529587+ubertidavide@users.noreply.github.com>\"]\nlicense = \"LICENSE\"\nreadme = \"README.md\"\n\n[tool.poetry.dependencies]\npython = \"^3.11\"\nselenium = \"^4.15.2\"\npython-decouple = \"^3.8\"\ntenacity = \"^8.2.3\"\nselenium-wire = \"^5.1.0\"\n\n[tool.poetry.group.dev.dependencies]\nsetuptools = \"^68.2.2\"\npytest = \"^7.4.3\"\npytest-mock = \"^3.12.0\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n", "code_before": "[tool.poetry]\nname = \"fastbots\"\nversion = \"0.1.4\"\ndescription = \"A simple library for fast bot and scraper development using selenium and the POM (Page Object Model) design.\"\nauthors = [\"Uberti Davide <24529587+ubertidavide@users.noreply.github.com>\"]\nlicense = \"LICENSE\"\nreadme = \"README.md\"\n\n[tool.poetry.dependencies]\npython = \"^3.11\"\nselenium = \"^4.15.2\"\npython-decouple = \"^3.8\"\ntenacity = \"^8.2.3\"\nselenium-wire = \"^5.1.0\"\n\n[tool.poetry.group.dev.dependencies]\nsetuptools = \"^68.2.2\"\npytest = \"^7.4.3\"\npytest-mock = \"^3.12.0\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n", "patch": "@@ -1,7 +1,7 @@\n [tool.poetry]\n name = \"fastbots\"\n-version = \"0.1.3\"\n-description = \"A simple library for bot development using selenium and the POM (Page Object Model) design.\"\n+version = \"0.1.4\"\n+description = \"A simple library for fast bot and scraper development using selenium and the POM (Page Object Model) design.\"\n authors = [\"Uberti Davide <24529587+ubertidavide@users.noreply.github.com>\"]\n license = \"LICENSE\"\n readme = \"README.md\"", "file_path": "files/2023_11/219", "file_language": "toml", "file_name": "pyproject.toml", "outdated_file_modify": 1, "outdated_file_before": 0, "outdated_file_after": 0}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
