{"index": 204, "cve_id": "CVE-2012-4406", "cwe_id": ["CWE-94"], "cve_language": "Python", "cve_description": "OpenStack Object Storage (swift) before 1.7.0 uses the loads function in the pickle Python module unsafely when storing and loading metadata in memcached, which allows remote attackers to execute arbitrary code via a crafted pickle object.", "cvss": "7.3", "publish_date": "October 22, 2012", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "NONE", "S": "UNCHANGED", "C": "LOW", "I": "LOW", "A": "LOW", "commit_id": "e1ff51c04554d51616d2845f92ab726cb0e5831a", "commit_message": "Do not use pickle for serialization in memcache, but JSON\n\nWe don't want to use pickle as it can execute arbitrary code. JSON is\nsafer. However, note that it supports serialization for only some\nspecific subset of object types; this should be enough for what we need,\nthough.\n\nTo avoid issues on upgrades (unability to read pickled values, and cache\npoisoning for old servers not understanding JSON), we add a\nmemcache_serialization_support configuration option, with the following\nvalues:\n\n 0 = older, insecure pickle serialization\n 1 = json serialization but pickles can still be read (still insecure)\n 2 = json serialization only (secure and the default)\n\nTo avoid an instant full cache flush, existing installations should\nupgrade with 0, then set to 1 and reload, then after some time (24\nhours) set to 2 and reload. Support for 0 and 1 will be removed in\nfuture versions.\n\nPart of bug 1006414.\n\nChange-Id: Id7d6d547b103b4f23ebf5be98b88f09ec6027ce4", "commit_date": "2012-08-03T14:22:21Z", "project": "openstack/swift", "url": "https://api.github.com/repos/openstack/swift/commits/e1ff51c04554d51616d2845f92ab726cb0e5831a", "html_url": "https://github.com/openstack/swift/commit/e1ff51c04554d51616d2845f92ab726cb0e5831a", "windows_before": [{"commit_id": "ef3f8bb335de47439211e3e72835ffa50beb3559", "commit_date": "Thu Aug 2 14:49:02 2012 +0000", "commit_message": "Ensure parameters sent to db are utf8 strs", "files_name": ["swift/common/db.py"]}, {"commit_id": "d2b51dd2b66afc796f297ba73ea438ce5b66ec98", "commit_date": "Wed Aug 1 21:07:32 2012 +0000", "commit_message": "Updated use of iter_nodes for handoff logging", "files_name": ["swift/proxy/server.py"]}, {"commit_id": "7923c56afadf272da0db1a34810bbc029ea4d047", "commit_date": "Wed Aug 1 00:16:12 2012 +0000", "commit_message": "Fixed proxy logging.", "files_name": ["swift/common/middleware/proxy_logging.py", "test/unit/common/middleware/test_proxy_logging.py"]}, {"commit_id": "0d38c710a57228f6c8dadb994ae2ce79d6a7058e", "commit_date": "Tue Jul 31 04:22:12 2012 +0000", "commit_message": "Cleaner fix to format regression fix", "files_name": ["swift/account/server.py", "swift/common/constraints.py", "swift/container/server.py"]}, {"commit_id": "de9b81baee2b9cbd87c28812ae85bc5020b06774", "commit_date": "Tue Jul 31 02:26:39 2012 +0000", "commit_message": "Fixes regression with format=somethingelse", "files_name": ["swift/account/server.py", "swift/container/server.py", "test/unit/account/test_server.py", "test/unit/container/test_server.py"]}, {"commit_id": "f2f6b2f26ca82c369629146ab1b97cd1b15871d1", "commit_date": "Mon Jul 30 15:09:14 2012 -0500", "commit_message": "Report all unmounted drives", "files_name": ["bin/swift-recon"]}, {"commit_id": "ceaf7606fe25f77cf31deb2946a16ae7a6fec05c", "commit_date": "Sun Jul 29 18:55:19 2012 -0700", "commit_message": "updated changelog with patch from review", "files_name": ["CHANGELOG"]}, {"commit_id": "f20d5fdfc28410392782a7d3c05c4c9936245e7c", "commit_date": "Mon Jul 30 01:40:54 2012 +0000", "commit_message": "Merge \"Logging improvements: handoffs and thread locals\"", "files_name": ["a1a4d35362bda35112e7e41da70952ddf032f93a - Sun Jul 29 17:22:18 2012 +0000 : Logging improvements: handoffs and thread locals", "doc/source/deployment_guide.rst", "etc/proxy-server.conf-sample", "swift/common/utils.py", "swift/proxy/server.py", "test/unit/common/test_utils.py", "test/unit/proxy/test_server.py"]}, {"commit_id": "3800fdc1a97efd796593441f4ba20d62906a0c26", "commit_date": "Sun Jul 29 18:17:14 2012 +0000", "commit_message": "Fixed bug in staticweb with log_headers", "files_name": ["swift/common/middleware/staticweb.py", "test/unit/common/middleware/test_staticweb.py"]}, {"commit_id": "21616c2fc995ff5928b2d5e5c6ffc71cdfa763c2", "commit_date": "Wed Jul 25 09:25:11 2012 -0700", "commit_message": "1.6.1 version bump", "files_name": ["swift/__init__.py"]}, {"commit_id": "53e35ba4ffeab4d2614b790a0d8fb615b9359374", "commit_date": "Fri Jun 22 11:44:23 2012 -0500", "commit_message": "changelog for 1.6.0", "files_name": ["AUTHORS", "CHANGELOG"]}, {"commit_id": "a2bec22ac051e5ac04427295ac0a004ec1323092", "commit_date": "Wed Jul 25 20:14:27 2012 +0400", "commit_message": "Fix typing in DiskFile put method comment", "files_name": ["swift/obj/server.py"]}, {"commit_id": "03cec57e5db036169c79c04e66abff49b3451350", "commit_date": "Wed Jul 25 08:30:58 2012 -0700", "commit_message": "version bump to reflect current dev effort", "files_name": ["swift/__init__.py"]}, {"commit_id": "0da1b4f82d105e89729e41e45a22cf5ddc3d719a", "commit_date": "Fri Jul 13 17:13:53 2012 -0500", "commit_message": "Create and configure /var/cache/swift in SAIO", "files_name": ["doc/source/development_saio.rst"]}, {"commit_id": "6d4ba010eb88a7e5a06eabdd1da8d9d28f134236", "commit_date": "Mon Jul 23 18:24:16 2012 +0000", "commit_message": "Merge \"Fix problem with logger and libc loader\"", "files_name": ["baa04b9bda3b96e7ba4169eb2ac3a3484cdfaaa3 - Fri Jul 20 20:20:16 2012 +0000 : Merge \"ensure that accessing the ring devs reloads the ring if necessary\"", "a9887c0e158f3c632e7c555865b31a5d7ba8c6cd - Fri Jul 20 17:30:01 2012 +0000 : Merge \"Move swift_auth middleware from keystone to swift.\"", "de3b663a735f2cd200a7b140bdf2ea2b25a4f2d3 - Fri Jul 20 09:15:34 2012 -0700 : ensure that accessing the ring devs reloads the ring if necessary", "swift/common/ring/ring.py", "test/unit/common/ring/test_ring.py"]}, {"commit_id": "43437558fe20fa677b218137757e00ba5eeb9091", "commit_date": "Thu Jul 19 23:16:52 2012 +0400", "commit_message": "Fix problem with logger and libc loader", "files_name": ["swift/common/utils.py"]}, {"commit_id": "faff4ae769f71f79f066774f959028a4391fe689", "commit_date": "Thu Jul 5 15:43:14 2012 +0200", "commit_message": "Forbid substrings based on a regexp in name_filter middleware", "files_name": ["doc/manpages/proxy-server.conf.5", "etc/proxy-server.conf-sample", "swift/common/middleware/name_check.py", "test/unit/common/middleware/test_name_check.py"]}, {"commit_id": "31ff3da485d50ed67567ccee8479f1a21acc534a", "commit_date": "Thu Jul 19 01:21:38 2012 +0400", "commit_message": "Fix wrong assert in manager unit test, self._assert(len(m.servers), 4) don't check that number of servers equal 4.", "files_name": ["test/unit/common/test_manager.py"]}, {"commit_id": "a5242dbe2c6749134404843ed18b6fae5b8aa0a6", "commit_date": "Tue Jul 17 06:33:59 2012 +0000", "commit_message": "Merge \"Fix Dispersion report and swift-bench on saio\"", "files_name": ["de4d23c2a517050ab0afd0c8d0dbab817139b055 - Wed Jun 6 03:39:53 2012 +0900 : Adapt Swift for WebOb 1.2", "swift/account/server.py", "swift/common/bufferedhttp.py", "swift/common/constraints.py", "swift/common/db.py", "swift/common/middleware/healthcheck.py", "swift/common/middleware/proxy_logging.py", "swift/common/utils.py", "swift/container/server.py", "swift/obj/server.py", "swift/proxy/server.py", "test/functional/swift.py", "test/functional/tests.py", "test/unit/account/test_server.py", "test/unit/common/middleware/test_tempauth.py", "test/unit/common/middleware/test_tempurl.py", "test/unit/common/test_constraints.py", "test/unit/common/test_utils.py", "test/unit/container/test_server.py", "test/unit/proxy/test_server.py", "tools/pip-requires"]}, {"commit_id": "5f72a8db4a9c31d0139b523e6b5a4e0c966294bc", "commit_date": "Fri Jul 13 17:48:37 2012 -0500", "commit_message": "Fix Dispersion report and swift-bench on saio", "files_name": ["bin/swift-bench", "doc/source/admin_guide.rst", "etc/dispersion.conf-sample"]}, {"commit_id": "0ab4f2ab4ab14e480505aaa69e8d3e5d7c68003f", "commit_date": "Thu Jul 12 20:55:07 2012 +0000", "commit_message": "Merge \"add UDP protocol support for logger\"", "files_name": ["47385a2d8f6b961ac7f022c76b93e31d19ef5968 - Mon Jul 9 21:57:08 2012 +0000 : Merge \"Validate devices and partitions to avoid directory traversals\"", "4a9d19197c5084c2a2a804ac2564950c913d5c2e - Sat Jun 30 20:23:24 2012 +0000 : Updated probe tests", "test/probe/common.py", "test/probe/test_account_failures.py", "test/probe/test_container_failures.py", "test/probe/test_object_async_update.py", "test/probe/test_object_failures.py", "test/probe/test_object_handoff.py", "test/probe/test_running_with_each_type_down.py"]}, {"commit_id": "afa4f70024be30cb1a0b84b9744f044f2532904f", "commit_date": "Wed Jun 20 16:37:30 2012 +0100", "commit_message": "Move swift_auth middleware from keystone to swift.", "files_name": ["doc/source/overview_auth.rst", "etc/proxy-server.conf-sample", "setup.py", "swift/common/middleware/keystoneauth.py", "test/unit/common/middleware/test_keystoneauth.py"]}, {"commit_id": "cc1907eef5a8ba7f6c6a53e5f44752ae88111e6b", "commit_date": "Tue Jun 19 12:11:06 2012 +0200", "commit_message": "Validate devices and partitions to avoid directory traversals", "files_name": ["swift/account/server.py", "swift/common/utils.py", "swift/container/server.py", "swift/obj/server.py", "test/unit/common/test_utils.py"]}, {"commit_id": "d8c2d0e1bc7b804b54c3060a83c7866419b247b2", "commit_date": "Thu Jun 28 20:27:15 2012 +0000", "commit_message": "FormPost logging bugfix and slight refactor", "files_name": ["swift/common/middleware/formpost.py", "test/unit/common/middleware/test_formpost.py"]}, {"commit_id": "329b1da07b12fa7260c866c842ffad3c80104385", "commit_date": "Tue Jul 3 16:40:20 2012 +0000", "commit_message": "Merge \"Fixed bug 1011636 with segmented objects\"", "files_name": ["501a3e492972a2d95ce1ea3d81229a3ce0b38400 - Tue Jul 3 16:37:24 2012 +0000 : Merge \"Remove ambiguity in memcache_servers documentation\"", "217676c06c25e5d0f5d24d29ec9cc256a3d44507 - Mon Jul 2 18:49:13 2012 +0000 : Merge \"swift-bench should be able to use auth version 2.0\"", "f2a61ab50ed755b0a3c8bf57bf6795d25ff47851 - Thu Jun 28 23:25:27 2012 +0000 : Fixed bug 1011636 with segmented objects", "swift/proxy/server.py", "test/unit/proxy/test_server.py"]}, {"commit_id": "49a9cc7e68c1558b1fabb71ebdf6c6c70a9b6a3e", "commit_date": "Sat Jun 23 05:19:00 2012 +0900", "commit_message": "Made ranged requests on large objects working correctly when size of manifest file is not 0 byte.", "files_name": ["swift/proxy/server.py", "test/unit/proxy/test_server.py"]}, {"commit_id": "c16b70bb3d5bdad88148a3bcf95c877baec8f305", "commit_date": "Thu Jun 28 15:31:21 2012 +0000", "commit_message": "Merge \"Remove hard coded man page paths.\"", "files_name": ["1125368624ae70b84a4ef3da3bb515d36f29173f - Thu Jun 28 16:09:10 2012 +0200 : Remove ambiguity in memcache_servers documentation", "doc/manpages/proxy-server.conf.5", "etc/proxy-server.conf-sample"]}, {"commit_id": "c67e56848cec310193f17b4e006948a17006a442", "commit_date": "Mon Jun 18 23:40:50 2012 +0900", "commit_message": "Use buffered gzip stream with python 2.7.", "files_name": ["swift/common/ring/ring.py"]}, {"commit_id": "f0eb25a973585e6a6cdb7c69a342fc6a38055e0d", "commit_date": "Fri Jun 22 04:47:52 2012 -0700", "commit_message": "add UDP protocol support for logger", "files_name": ["AUTHORS", "swift/common/middleware/proxy_logging.py", "swift/common/utils.py", "swift/proxy/server.py"]}, {"commit_id": "fcab7b73586c1b4ad5887f76a6ce9d24bb000f4a", "commit_date": "Wed Jun 27 16:45:03 2012 +0000", "commit_message": "Merge \"Fixes for probe tests\"", "files_name": ["7538ff496a902ff9f4ada0f5bbd39c6a19c8eedc - Wed Jun 27 16:42:00 2012 +0000 : Merge \"use sort_key to compare the number of partitions the device wants\"", "04b77fdf4d6752c5ff3883ccd56b6c9c402c9014 - Wed Jun 27 16:41:58 2012 +0000 : Merge \"Make proxy-logging more like eventlet.posthook\"", "57008e553aad0d4eab94072b83de3ef8bb92144c - Wed Jun 27 16:41:43 2012 +0000 : Merge \"Patch for Swift Solaris (Illumos) compability.\"", "be79b0884eb7ba91836c5296aec92cf6066be767 - Wed Jun 27 05:01:55 2012 +0000 : Fixes for probe tests", "test/probe/common.py", "test/probe/test_account_failures.py", "test/probe/test_container_failures.py", "test/probe/test_object_async_update.py", "test/probe/test_object_failures.py", "test/probe/test_object_handoff.py", "test/probe/test_running_with_each_type_down.py"]}, {"commit_id": "69fd05f439ffeafa022d5152f0c69c0912598a84", "commit_date": "Fri Jun 22 13:05:53 2012 -0400", "commit_message": "Remove hard coded man page paths.", "files_name": ["doc/manpages/swift-account-auditor.1", "doc/manpages/swift-account-reaper.1", "doc/manpages/swift-account-replicator.1", "doc/manpages/swift-account-server.1", "doc/manpages/swift-container-auditor.1", "doc/manpages/swift-container-replicator.1", "doc/manpages/swift-container-server.1", "doc/manpages/swift-container-sync.1", "doc/manpages/swift-container-updater.1", "doc/manpages/swift-init.1", "doc/manpages/swift-object-auditor.1", "doc/manpages/swift-object-expirer.1", "doc/manpages/swift-object-replicator.1", "doc/manpages/swift-object-server.1", "doc/manpages/swift-object-updater.1", "doc/manpages/swift-proxy-server.1"]}, {"commit_id": "a4ebc465c3df971e9db5a3943325a1119f8ec995", "commit_date": "Fri Jun 22 01:34:39 2012 +0900", "commit_message": "use sort_key to compare the number of partitions the device wants", "files_name": ["swift/common/ring/builder.py"]}], "windows_after": [{"commit_id": "aad7cdc36441b91b517006a34bbd034bc15d2af4", "commit_date": "Fri Aug 3 11:47:28 2012 -0700", "commit_message": "To simulate the real workloads, the objects to be uploaded could be created in the random sizes, which are bounded (lower_object_size and upper_object_size) by the user inputs.", "files_name": ["bin/swift-bench", "swift/common/bench.py"]}, {"commit_id": "f8ce43a21891ae2cc00d0770895b556eea9c7845", "commit_date": "Sun Aug 5 00:51:49 2012 -0700", "commit_message": "Use custom encoding for RingData, not pickle.", "files_name": ["bin/swift-ring-builder", "swift/common/ring/builder.py", "swift/common/ring/ring.py", "test/unit/common/ring/test_ring.py"]}, {"commit_id": "d8c5c0c8975a6991fb53dd916e579ac762a1bc80", "commit_date": "Mon Aug 6 09:02:45 2012 -0400", "commit_message": "all in one setup rc.local needs -p for mkdir command", "files_name": ["doc/source/development_saio.rst"]}, {"commit_id": "cb03022366d661940243d0f05571c665d5e84cb3", "commit_date": "Mon Aug 6 14:41:07 2012 +0000", "commit_message": "Merge \"Report all unmounted drives\"", "files_name": ["b06bfa69a6be450026447a596dae47cba7b057ef - Mon Aug 6 17:46:04 2012 +0000 : Merge \"all in one setup rc.local needs -p for mkdir command\"", "61932d85066e646b9d7fabb58fc5ac1dd2ab9eb3 - Mon Aug 6 20:53:24 2012 +0000 : Fixed bug where expirer would get confused by...", "swift/obj/server.py", "test/unit/obj/test_server.py"]}, {"commit_id": "3c82151c4408dbec62d0c54c772e6d13b07e8e58", "commit_date": "Tue Aug 7 16:26:38 2012 -0700", "commit_message": "update object versions docs", "files_name": ["doc/source/overview_object_versioning.rst"]}, {"commit_id": "edd38035be4f64d547938f44fb2d96a72eeb672a", "commit_date": "Fri Aug 10 14:00:09 2012 +0900", "commit_message": "handle exception correctly in _make_app_iter_reader", "files_name": ["swift/proxy/server.py"]}, {"commit_id": "f2f3b75802754b387af8355364c922d1d48c3275", "commit_date": "Mon Aug 13 15:52:56 2012 +0000", "commit_message": "Merge \"Use custom encoding for RingData, not pickle.\"", "files_name": ["fa661d5b210004d89e53a03bac5076aabd5eda76 - Mon Aug 13 15:58:01 2012 +0000 : ring.py: Uses simplejson if avail and pep8 1.3.1", "swift/common/ring/ring.py"]}, {"commit_id": "e7c2ae985ad579200874111c4bfb57306396091c", "commit_date": "Fri Aug 17 13:03:48 2012 -0500", "commit_message": "Cleanup after LockTimeout", "files_name": ["swift/common/utils.py"]}, {"commit_id": "eabe9ba4442dd272bcee98115c11245cf0114d08", "commit_date": "Fri Aug 17 23:44:21 2012 +0000", "commit_message": "Merge \"update object versions docs\"", "files_name": ["ba9158886e1021aa117e7a35f46760df085373e5 - Fri Aug 17 23:47:32 2012 +0000 : Merge \"To simulate the real workloads, the objects to be uploaded could be created in the random sizes, which are bounded (lower_object_size and upper_object_size) by the user inputs.\"", "73b00ca9de888bd7936b72d3b92e7d217c38d929 - Fri Aug 17 23:54:32 2012 +0000 : Merge \"handle exception correctly in _make_app_iter_reader\"", "d9443d68f910cfbddf492cf8f071ef4fef9c0d7f - Mon Aug 20 16:02:27 2012 -0700 : A few misc. code cleanups.", "swift/common/db.py", "swift/common/middleware/tempauth.py", "swift/obj/server.py"]}, {"commit_id": "6dbac845d5096063d6b344cd0314d0b50f7c50b0", "commit_date": "Fri Aug 17 19:44:21 2012 +0800", "commit_message": "Retuen a reasonable response to client.", "files_name": ["swift/proxy/server.py", "test/unit/proxy/test_server.py"]}, {"commit_id": "af2ff124eb60ea9ba01f25f401bcd174e00a3052", "commit_date": "Tue Aug 21 10:03:42 2012 -0700", "commit_message": "Update docs for new ring serialization.", "files_name": ["doc/source/admin_guide.rst", "doc/source/deployment_guide.rst", "doc/source/overview_ring.rst"]}, {"commit_id": "a0a8b484cc927472903f99190bb80db2957ec68d", "commit_date": "Tue Aug 21 20:53:42 2012 +0000", "commit_message": "Merge \"Updated probe tests\"", "files_name": ["2352b9d66f7a767376d36bd5b79d4b2c328ea45a - Tue Aug 21 14:43:08 2012 -0700 : Add nosehtmloutput as a test dependency.", "tools/test-requires"]}, {"commit_id": "b10430a356cd997f8b286d084e60cf8370518019", "commit_date": "Tue Aug 21 23:12:19 2012 +0000", "commit_message": "Merge \"Fixed bug where expirer would get confused by...\"", "files_name": ["56bb00f7c9cc17f6b138ab4a5cc969b2ad4b91ea - Tue Aug 21 14:54:12 2012 -0700 : A couple minor tweaks to the SAIO guide.", "doc/source/development_saio.rst"]}, {"commit_id": "2eb56c5150b9972b7770916d8474e1987ddcdd83", "commit_date": "Wed Aug 22 00:35:09 2012 +0000", "commit_message": "Merge \"A few misc. code cleanups.\"", "files_name": ["8c369a3361c0cc844422629d0a6a54426a8ddca9 - Wed Aug 22 00:39:43 2012 +0000 : Merge \"A couple minor tweaks to the SAIO guide.\"", "1678da2f59782bc6eff605a7150db2266875e586 - Wed Aug 22 00:53:39 2012 +0000 : Merge \"Add nosehtmloutput as a test dependency.\"", "7ebb81c0f64a866bb55176462a6147363ee352c5 - Wed Aug 22 12:20:30 2012 +0800 : Fix PEP8 issues.", "doc/source/conf.py"]}, {"commit_id": "665556b5003d5e78322855b38f264e2d549c2627", "commit_date": "Wed Aug 22 19:12:49 2012 +0000", "commit_message": "Merge \"Update docs for new ring serialization.\"", "files_name": ["82f1d550b6be61adcdba47f023553f3c98b34476 - Wed Aug 22 19:47:15 2012 +0000 : Merge \"Fix PEP8 issues.\"", "da0e013d988094636f2d3026dbf5c9449c9986d9 - Tue Aug 21 12:51:59 2012 -0700 : make obj replicator locking more optimistic", "swift/obj/replicator.py", "swift/obj/server.py", "test/unit/__init__.py", "test/unit/obj/test_replicator.py", "test/unit/obj/test_server.py"]}, {"commit_id": "66400b7337867403cbc7ec24e66f18e5826094a3", "commit_date": "Fri Aug 17 17:00:50 2012 -0700", "commit_message": "Add device name to *-replicator.removes for DBs", "files_name": ["doc/source/admin_guide.rst", "swift/common/db_replicator.py", "test/unit/__init__.py", "test/unit/common/test_db_replicator.py"]}, {"commit_id": "4f0226e4f8b65034c8e1b88901336997c6a901de", "commit_date": "Wed Aug 22 21:00:14 2012 +0000", "commit_message": "Merge \"Retuen a reasonable response to client.\"", "files_name": ["1712135a7ea64120b86d7eb5f25003380d5ccc70 - Wed Aug 22 21:11:18 2012 +0000 : Merge \"Add device name to *-replicator.removes for DBs\"", "eb4af8f84097ae6f1c0057313dd8c0c002b3d49d - Thu Aug 23 12:38:09 2012 -0700 : split proxy controllers into individual modules", "swift/common/middleware/ratelimit.py", "swift/proxy/controllers/__init__.py", "swift/proxy/controllers/account.py", "swift/proxy/controllers/base.py", "swift/proxy/controllers/container.py", "swift/proxy/controllers/obj.py", "swift/proxy/server.py", "test/unit/common/middleware/test_ratelimit.py", "test/unit/proxy/test_server.py"]}, {"commit_id": "3f01f889d5d61c0352ba73e7c1bca32b4b48d952", "commit_date": "Thu Aug 23 22:06:35 2012 +0000", "commit_message": "Merge \"make obj replicator locking more optimistic\"", "files_name": ["d8c02dccc098177c9b63536353b986b27645909d - Thu Aug 23 23:08:03 2012 +0000 : Merge \"split proxy controllers into individual modules\"", "1a6c42fccd0b277c98f7a8325d49500c5e87d8bb - Mon Aug 20 22:51:46 2012 -0700 : Fix when rate_limit_after_segment kicks in.", "etc/proxy-server.conf-sample", "swift/proxy/controllers/obj.py", "test/unit/proxy/test_server.py"]}, {"commit_id": "28bff36b814a30c07e7ff67d896edd0d3562416f", "commit_date": "Mon Aug 27 09:55:06 2012 +0800", "commit_message": "Remove redefinition of import traceback", "files_name": ["AUTHORS", "swift/obj/server.py"]}, {"commit_id": "e630e7c9d6c275494d7382d0a288a4e8bad0b816", "commit_date": "Mon Aug 27 20:23:11 2012 +0000", "commit_message": "Merge \"Remove redefinition of import traceback\"", "files_name": ["9bda92d54a4d80b7c274fa75f02a1c4ae293d6e7 - Sat Aug 25 16:02:45 2012 -0700 : Misc. swift-bench improvements.", "bin/swift-bench", "etc/swift-bench.conf-sample", "swift/common/bench.py"]}, {"commit_id": "9290471b61a98a1882f0d9e5ce7d883428e2ff36", "commit_date": "Fri Aug 24 20:20:14 2012 +0300", "commit_message": "x-newest cleanup code with test. Fixes bug 1037337", "files_name": ["swift/proxy/controllers/base.py", "test/unit/proxy/test_server.py"]}, {"commit_id": "2a38a040924b621bce4e999b3c7dab692f497a21", "commit_date": "Tue Aug 28 10:51:49 2012 +0800", "commit_message": "Remove the gettext wrapper of server_type.", "files_name": ["swift/proxy/controllers/account.py", "swift/proxy/controllers/base.py", "swift/proxy/controllers/container.py", "swift/proxy/controllers/obj.py"]}, {"commit_id": "eb5f89ac25fb3edf5d6f0dcc238d4dea4cbbc7c8", "commit_date": "Mon Aug 27 14:44:41 2012 -0700", "commit_message": "fallocate call error handling", "files_name": ["swift/common/utils.py", "swift/obj/server.py", "test/unit/obj/test_server.py"]}, {"commit_id": "78dacc4663fd0062f70da8fb4ff68ea77218003f", "commit_date": "Tue Aug 28 20:09:00 2012 +0000", "commit_message": "Merge \"x-newest cleanup code with test. Fixes bug 1037337\"", "files_name": ["e375e7edd7caeec0484bd119d0182ebd14abc41b - Tue Aug 28 21:39:57 2012 +0000 : Merge \"Do not use pickle for serialization in memcache, but JSON\"", "6e524a3a725b23ad093d9cc8c590d8ec7f5f5bc8 - Wed Aug 29 01:28:50 2012 +0000 : Merge \"Misc. swift-bench improvements.\"", "859afd6f49619318f2497731b6f14dfa4067c9aa - Wed Aug 29 18:19:17 2012 +0000 : Merge \"Remove the gettext wrapper of server_type.\"", "c509ac23711edb70caf62697592e11f56e8d7b95 - Wed Aug 29 19:57:26 2012 +0000 : Added ability to disable fallocate", "doc/source/development_saio.rst", "etc/account-server.conf-sample", "etc/container-server.conf-sample", "etc/object-server.conf-sample", "swift/common/daemon.py", "swift/common/utils.py", "swift/common/wsgi.py"]}, {"commit_id": "4a2ae2b46034e8027bee20dcfaaf38a9ee0c5c17", "commit_date": "Sun Aug 19 17:44:43 2012 -0700", "commit_message": "Upating proxy-server StatsD logging.", "files_name": ["doc/source/admin_guide.rst", "doc/source/development_saio.rst", "etc/account-server.conf-sample", "etc/container-server.conf-sample", "etc/object-expirer.conf-sample", "etc/object-server.conf-sample", "etc/proxy-server.conf-sample", "swift/common/middleware/proxy_logging.py", "swift/common/middleware/staticweb.py", "swift/common/utils.py", "swift/proxy/controllers/account.py", "swift/proxy/controllers/base.py", "swift/proxy/controllers/container.py", "swift/proxy/controllers/obj.py", "swift/proxy/server.py", "test/unit/common/middleware/test_proxy_logging.py", "test/unit/proxy/test_server.py"]}, {"commit_id": "ed3b12d05caa8d807e8fea1036b8d21013bcab80", "commit_date": "Sat Aug 25 23:02:53 2012 -0700", "commit_message": "Can run swift-bench across multiple cores/servers.", "files_name": ["bin/swift-bench", "bin/swift-bench-client", "setup.py", "swift/common/bench.py"]}, {"commit_id": "7b664c99e57ed3069032759d7c16bd9382603ae3", "commit_date": "Fri Aug 31 11:24:46 2012 +0800", "commit_message": "Fix PEP8 issues in ./test/unit/common .", "files_name": ["test/unit/common/test_bufferedhttp.py", "test/unit/common/test_constraints.py", "test/unit/common/test_db.py", "test/unit/common/test_db_replicator.py", "test/unit/common/test_init.py"]}, {"commit_id": "54a290718ce31d3e0e89eac6199b41510c45f11b", "commit_date": "Fri Aug 31 14:22:56 2012 -0700", "commit_message": "bumped version to 1.7.0 to reflect current dev effort", "files_name": ["swift/__init__.py"]}, {"commit_id": "0bb5d6da135c4a234dcd5443ac0dff7147f56992", "commit_date": "Sun Sep 2 23:55:53 2012 -0700", "commit_message": "use simplejson to serialize acct/cont results", "files_name": ["swift/account/server.py", "swift/container/server.py"]}, {"commit_id": "314d3b7a853779fa88e7223021b05896f63105f2", "commit_date": "Tue Sep 4 15:34:47 2012 +0200", "commit_message": "Fall back to UDP if /dev/log does not exist.", "files_name": ["swift/common/utils.py"]}, {"commit_id": "2e16b1f1e5837ed0bd10cac4fba08ed9d72a36cb", "commit_date": "Tue Sep 4 14:41:12 2012 +0000", "commit_message": "Update swift.common.db to us swift.cmmn.utils.json", "files_name": ["swift/common/db.py"]}, {"commit_id": "a8b8798f720dcab872220a7219fe204608ed1357", "commit_date": "Tue Sep 4 14:48:59 2012 +0000", "commit_message": "Merge \"Fix PEP8 issues in ./test/unit/common .\"", "files_name": ["93766cd28d2d90aa18d88a39866236256497b40f - Tue Sep 4 15:35:37 2012 +0000 : Merge \"Update swift.common.db to us swift.cmmn.utils.json\"", "341da755744d9be62e1a52200428d4531aa82e36 - Fri Aug 31 14:21:39 2012 -0700 : changelog and authors updates for 1.7 release", ".mailmap", "AUTHORS", "CHANGELOG"]}, {"commit_id": "07b4c21b42374721092c2a365ec81bc0cfbe873e", "commit_date": "Tue Sep 4 09:16:29 2012 -0700", "commit_message": "1.7.1 version bump", "files_name": ["swift/__init__.py"]}], "parents": [{"commit_id_before": "ef3f8bb335de47439211e3e72835ffa50beb3559", "url_before": "https://api.github.com/repos/openstack/swift/commits/ef3f8bb335de47439211e3e72835ffa50beb3559", "html_url_before": "https://github.com/openstack/swift/commit/ef3f8bb335de47439211e3e72835ffa50beb3559"}], "details": [{"raw_url": "https://github.com/openstack/swift/raw/e1ff51c04554d51616d2845f92ab726cb0e5831a/doc%2Fmanpages%2Fproxy-server.conf.5", "code": ".\\\"\n.\\\" Author: Joao Marcelo Martins <marcelo.martins@rackspace.com> or <btorch@gmail.com>\n.\\\" Copyright (c) 2010-2012 OpenStack, LLC.\n.\\\"\n.\\\" Licensed under the Apache License, Version 2.0 (the \"License\");\n.\\\" you may not use this file except in compliance with the License.\n.\\\" You may obtain a copy of the License at\n.\\\"\n.\\\"    http://www.apache.org/licenses/LICENSE-2.0\n.\\\"\n.\\\" Unless required by applicable law or agreed to in writing, software\n.\\\" distributed under the License is distributed on an \"AS IS\" BASIS,\n.\\\" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n.\\\" implied.\n.\\\" See the License for the specific language governing permissions and\n.\\\" limitations under the License.\n.\\\"\n.TH proxy-server.conf 5 \"8/26/2011\" \"Linux\" \"OpenStack Swift\"\n\n.SH NAME\n.LP\n.B proxy-server.conf\n\\- configuration file for the openstack-swift proxy server\n\n\n\n.SH SYNOPSIS\n.LP\n.B proxy-server.conf\n\n\n\n.SH DESCRIPTION\n.PP\nThis is the configuration file used by the proxy server and other proxy middlewares.\n\nThe configuration file follows the python-pastedeploy syntax. The file is divided\ninto sections, which are enclosed by square brackets. Each section will contain a\ncertain number of key/value parameters which are described later.\n\nAny line that begins with a '#' symbol is ignored.\n\nYou can find more information about python-pastedeploy configuration format at\n\\fIhttp://pythonpaste.org/deploy/#config-format\\fR\n\n\n\n.SH GLOBAL SECTION\n.PD 1\n.RS 0\nThis is indicated by section named [DEFAULT]. Below are the parameters that\nare acceptable within this section.\n\n.IP \"\\fBbind_ip\\fR\"\nIP address the proxy server should bind to. The default is 0.0.0.0 which will make\nit bind to all available addresses.\n.IP \"\\fBbind_port\\fR\"\nTCP port the proxy server should bind to. The default is 80.\n.IP \\fBbacklog\\fR\nTCP backlog.  Maximum number of allowed pending connections. The default value is 4096.\n.IP \\fBworkers\\fR\nNumber of container server workers to fork. The default is 1.\n.IP \\fBuser\\fR\nThe system user that the container server will run as. The default is swift.\n.IP \\fBswift_dir\\fR\nSwift configuration directory. The default is /etc/swift.\n.IP \\fBcert_file\\fR\nLocation of the SSL certificate file. The default path is /etc/swift/proxy.crt. This is\ndisabled by default.\n.IP \\fBkey_file\\fR\nLocation of the SSL certificate key file. The default path is /etc/swift/proxy.key. This is\ndisabled by default.\n.IP \\fBlog_name\\fR\nLabel used when logging. The default is swift.\n.IP \\fBlog_facility\\fR\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \\fBlog_level\\fR\nLogging level. The default is INFO.\n.IP \\fBlog_address\\fR\nLogging address. The default is /dev/log.\n.RE\n.PD\n\n\n\n.SH PIPELINE SECTION\n.PD 1\n.RS 0\nThis is indicated by section name [pipeline:main]. Below are the parameters that\nare acceptable within this section.\n\n.IP \"\\fBpipeline\\fR\"\nIt is used when you need apply a number of filters. It is a list of filters\nended by an application. The default should be \\fB\"catch_errors healthcheck\ncache ratelimit tempauth proxy-server\"\\fR\n.RE\n.PD\n\n\n\n.SH FILTER SECTION\n.PD 1\n.RS 0\nAny section that has its name prefixed by \"filter:\" indicates a filter section.\nFilters are used to specify configuration parameters for specific swift middlewares.\nBelow are the filters available and respective acceptable parameters.\n.IP \"\\fB[filter:healthcheck]\\fR\"\n.RE\n.RS 3\n.IP \"\\fBuse\\fR\"\nEntry point for paste.deploy for the healthcheck middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#healthcheck\\fR.\n.RE\n\n\n.RS 0\n.IP \"\\fB[filter:tempauth]\\fR\"\n.RE\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the tempauth middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#tempauth\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is tempauth.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR \"\nEnables the ability to log request headers. The default is False.\n.IP \\fBreseller_prefix\\fR\nThe reseller prefix will verify a token begins with this prefix before even\nattempting to validate it. Also, with authorization, only Swift storage accounts\nwith this prefix will be authorized by this middleware. Useful if multiple auth\nsystems are in use for one Swift cluster. The default is AUTH.\n.IP \\fBauth_prefix\\fR\nThe auth prefix will cause requests beginning with this prefix to be routed\nto the auth subsystem, for granting tokens, etc. The default is /auth/.\n.IP \\fBtoken_life\\fR\nThis is the time in seconds before the token expires. The default is 86400.\n.IP \\fBallowed_sync_hosts\\fR\nThis is a comma separated list of hosts allowed to send X-Container-Sync-Key requests.\n.IP \\fBuser_<account>_<user>\\fR\nLastly, you need to list all the accounts/users you want here. The format is:\nuser_<account>_<user> = <key> [group] [group] [...] [storage_url]\n\nThere are special groups of: \\fI.reseller_admin\\fR who can do anything to any account for this auth\nand also \\fI.admin\\fR who can do anything within the account.\n\nIf neither of these groups are specified, the user can only access containers that\nhave been explicitly allowed for them by a \\fI.admin\\fR or \\fI.reseller_admin\\fR.\nThe trailing optional storage_url allows you to specify an alternate url to hand\nback to the user upon authentication. If not specified, this defaults to\n\\fIhttp[s]://<ip>:<port>/v1/<reseller_prefix>_<account>\\fR where http or https depends\non whether cert_file is specified in the [DEFAULT] section, <ip> and <port> are based\non the [DEFAULT] section's bind_ip and bind_port (falling back to 127.0.0.1 and 8080),\n<reseller_prefix> is from this section, and <account> is from the user_<account>_<user> name.\n\nHere are example entries, required for running the tests:\n.RE\n\n.PD 0\n.RS 10\n.IP \"user_admin_admin = admin .admin .reseller_admin\"\n.IP \"user_test_tester = testing .admin\"\n.IP \"user_test2_tester2 = testing2 .admin\"\n.IP \"user_test_tester3 = testing3\"\n.RE\n.PD\n\n.RS 0\n.IP \"\\fB[filter:healthcheck]\\fR\"\n.RE\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the healthcheck middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#healthcheck\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is healthcheck.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR \"\nEnables the ability to log request headers. The default is False.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:cache]\\fR\"\n.RE\n\nCaching middleware that manages caching in swift.\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the memcache middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#memcache\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is memcache.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR \"\nEnables the ability to log request headers. The default is False.\n.IP \\fBmemcache_servers\\fR\nIf not set in the configuration file, the value for memcache_servers will be read from /etc/swift/memcache.conf (see memcache.conf-sample) or lacking that file, it will default to the value below. You can specify multiple servers separated with commas, as in: 10.1.2.3:11211,10.1.2.4:11211. This can be a list separated by commas. The default is 127.0.0.1:11211.\n.IP \\fBmemcache_serialization_support\\fR\nThis sets how memcache values are serialized and deserialized:\n.RE\n\n.PD 0\n.RS 10\n.IP \"0 = older, insecure pickle serialization\"\n.IP \"1 = json serialization but pickles can still be read (still insecure)\"\n.IP \"2 = json serialization only (secure and the default)\"\n.RE\n\n.RS 10\nTo avoid an instant full cache flush, existing installations should upgrade with 0, then set to 1 and reload, then after some time (24 hours) set to 2 and reload. In the future, the ability to use pickle serialization will be removed.\n\nIf not set in the configuration file, the value for memcache_serialization_support will be read from /etc/swift/memcache.conf if it exists (see memcache.conf-sample). Otherwise, the default value as indicated above will be used.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:ratelimit]\\fR\"\n.RE\n\nRate limits requests on both an Account and Container level.  Limits are configurable.\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the ratelimit middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#ratelimit\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is ratelimit.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR \"\nEnables the ability to log request headers. The default is False.\n.IP \\fBclock_accuracy\\fR\nThis should represent how accurate the proxy servers' system clocks are with each other.\n1000 means that all the proxies' clock are accurate to each other within 1 millisecond.\nNo ratelimit should be higher than the clock accuracy. The default is 1000.\n.IP \\fBmax_sleep_time_seconds\\fR\nApp will immediately return a 498 response if the necessary sleep time ever exceeds\nthe given max_sleep_time_seconds. The default is 60 seconds.\n.IP \\fBlog_sleep_time_seconds\\fR\nTo allow visibility into rate limiting set this value > 0 and all sleeps greater than\nthe number will be logged. If set to 0 means disabled. The default is 0.\n.IP \\fBrate_buffer_seconds\\fR\nNumber of seconds the rate counter can drop and be allowed to catch up\n(at a faster than listed rate). A larger number will result in larger spikes in\nrate but better average accuracy. The default is 5.\n.IP \\fBaccount_ratelimit\\fR\nIf set, will limit PUT and DELETE requests to /account_name/container_name. Number is\nin requests per second. If set to 0 means disabled. The default is 0.\n.IP \\fBaccount_whitelist\\fR\nComma separated lists of account names that will not be rate limited. The default is ''.\n.IP \\fBaccount_blacklist\\fR\nComma separated lists of account names that will not be allowed. Returns a 497 response.\nThe default is ''.\n.IP \\fBcontainer_ratelimit_size\\fR\nWhen set with container_limit_x = r: for containers of size x, limit requests per second\nto r. Will limit PUT, DELETE, and POST requests to /a/c/o. The default is ''.\n.RE\n\n\n.RS 0\n.IP \"\\fB[filter:domain_remap]\\fR\"\n.RE\n\nMiddleware that translates container and account parts of a domain to path parameters that the proxy server understands. The container.account.storageurl/object gets translated to container.account.storageurl/path_root/account/container/object and account.storageurl/path_root/container/object gets translated to account.storageurl/path_root/account/container/object\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the domain_remap middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#domain_remap\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is domain_remap.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR\"\nEnables the ability to log request headers. The default is False.\n.IP \\fBstorage_domain\\fR\nThe domain to be used by the middleware.\n.IP \\fBpath_root\\fR\nThe path root value for the storage URL. The default is v1.\n.IP \\fBreseller_prefixes\\fR\nBrowsers can convert a host header to lowercase, so check that reseller\nprefix on the account is the correct case. This is done by comparing the\nitems in the reseller_prefixes config option to the found prefix. If they\nmatch except for case, the item from reseller_prefixes will be used\ninstead of the found reseller prefix. The reseller_prefixes list is exclusive.\nIf defined, any request with an account prefix not in that list will be ignored\nby this middleware. Defaults to 'AUTH'.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:catch_errors]\\fR\"\n.RE\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the catch_errors middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#catch_errors\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is catch_errors.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR \"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR\"\nEnables the ability to log request headers. The default is False.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:cname_lookup]\\fR\"\n.RE\n\nNote: this middleware requires python-dnspython\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the cname_lookup middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#cname_lookup\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is cname_lookup.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR\"\nEnables the ability to log request headers. The default is False.\n.IP \\fBstorage_domain\\fR\nThe domain to be used by the middleware.\n.IP \\fBlookup_depth\\fR\nHow deep in the CNAME chain to look for something that matches the storage domain.\nThe default is 1.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:staticweb]\\fR\"\n.RE\n\nNote: Put staticweb just after your auth filter(s) in the pipeline\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the staticweb middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#staticweb\\fR.\n.IP \\fBcache_timeout\\fR\nSeconds to cache container x-container-meta-web-* header values. The default is 300 seconds.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is staticweb.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR \"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR\"\nEnables the ability to log request headers. The default is False.\n.IP \"\\fBset access_log_name\\fR\"\nLabel used when logging. The default is staticweb.\n.IP \"\\fBset access_log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset access_log_level\\fR \"\nLogging level. The default is INFO.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:tempurl]\\fR\"\n.RE\n\nNote: Put tempurl just before your auth filter(s) in the pipeline\n\n.RS 3\n.IP \\fBincoming_remove_headers\\fR\nThe headers to remove from incoming requests. Simply a whitespace delimited list of header names and names can optionally end with '*' to indicate a prefix match. incoming_allow_headers is a list of exceptions to these removals.\n.IP \\fBincoming_allow_headers\\fR\nThe headers allowed as exceptions to incoming_remove_headers. Simply a whitespace delimited list of header names and names can optionally end with '*' to indicate a prefix match.\n.IP \"\\fBoutgoing_remove_headers\\fR\"\nThe headers to remove from outgoing responses. Simply a whitespace delimited list of header names and names can optionally end with '*' to indicate a prefix match. outgoing_allow_headers is a list of exceptions to these removals.\n.IP \"\\fBoutgoing_allow_headers\\fR\"\nThe headers allowed as exceptions to outgoing_remove_headers. Simply a whitespace delimited list of header names and names can optionally end with '*' to indicate a prefix match.\n.IP \"\\fBset log_level\\fR \"\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:formpost]\\fR\"\n.RE\n\nNote: Put formpost just before your auth filter(s) in the pipeline\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the formpost middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#formpost\\fR.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:name_check]\\fR\"\n.RE\n\nNote: Just needs to be placed before the proxy-server in the pipeline.\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the name_check middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#name_check\\fR.\n.IP \\fBforbidden_chars\\fR\nCharacters that will not be allowed in a name.\n.IP \\fBmaximum_length\\fR\nMaximum number of characters that can be in the name.\n.IP \\fBforbidden_regexp\\fR\nPython regular expressions of substrings that will not be allowed in a name.\n.RE\n\n\n.PD\n\n\n\n\n.SH APP SECTION\n.PD 1\n.RS 0\nThis is indicated by section name [app:proxy-server]. Below are the parameters\nthat are acceptable within this section.\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the proxy server. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#proxy\\fR.\n.IP \"\\fBset log_name\\fR\nLabel used when logging. The default is proxy-server.\n.IP \"\\fBset log_facility\\fR\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fB set log_level\\fR\nLogging level. The default is INFO.\n.IP \"\\fB set log_address\\fR\nLogging address. The default is /dev/log.\n.IP \"\\fBset access_log_name\\fR\"\nLabel used when logging. The default is proxy-server.\n.IP \"\\fBset access_log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset access_log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fB set log_requests\\fR\nEnables request logging. The default is False.\n.IP \\fBrecheck_account_existence\\fR\nCache timeout in seconds to send memcached for account existence. The default is 60 seconds.\n.IP \\fBrecheck_container_existence\\fR\nCache timeout in seconds to send memcached for container existence. The default is 60 seconds.\n.IP \\fBobject_chunk_size\\fR\nChunk size to read from object servers. The default is 8192.\n.IP \\fBclient_chunk_size\\fR\nChunk size to read from clients. The default is 8192.\n.IP \\fBnode_timeout\\fR\nRequest timeout to external services. The default is 10 seconds.\n.IP \\fBclient_timeoutt\\fR\nTimeout to read one chunk from a client. The default is 60 seconds.\n.IP \\fBconn_timeout\\fR\nConnection timeout to external services. The default is 0.5 seconds.\n.IP \\fBerror_suppression_interval\\fR\nTime in seconds that must elapse since the last error for a node to\nbe considered no longer error limited. The default is 60 seconds.\n.IP \\fBerror_suppression_limit\\fR\nError count to consider a node error limited. The default is 10.\n.IP \\fBallow_account_management\\fR\nWhether account PUTs and DELETEs are even callable. If set to 'true' any authorized\nuser may create and delete accounts; if 'false' no one, even authorized, can. The default\nis false.\n.IP \\fBobject_post_as_copy\\fR\nSet object_post_as_copy = false to turn on fast posts where only the metadata changes\nare stored as new and the original data file is kept in place. This makes for quicker\nposts; but since the container metadata isn't updated in this mode, features like\ncontainer sync won't be able to sync posts. The default is True.\n.IP \\fBaccount_autocreate\\fR\nIf set to 'true' authorized accounts that do not yet exist within the Swift cluster\nwill be automatically created. The default is set to false.\n.IP \\fBrate_limit_after_segment\\fR\nRate limit the download of large object segments after this segment is\ndownloaded. The default is 10 segments.\n.IP \\fBrate_limit_segments_per_sec\\fR\nRate limit large object downlods at this rate.  The default is 1.\n.RE\n.PD\n\n\n\n.SH DOCUMENTATION\n.LP\nMore in depth documentation about the swift-proxy-server and\nalso Openstack-Swift as a whole can be found at\n.BI http://swift.openstack.org/admin_guide.html\nand\n.BI http://swift.openstack.org\n\n\n.SH \"SEE ALSO\"\n.BR swift-proxy-server(1),\n\n\n", "code_before": ".\\\"\n.\\\" Author: Joao Marcelo Martins <marcelo.martins@rackspace.com> or <btorch@gmail.com>\n.\\\" Copyright (c) 2010-2012 OpenStack, LLC.\n.\\\"\n.\\\" Licensed under the Apache License, Version 2.0 (the \"License\");\n.\\\" you may not use this file except in compliance with the License.\n.\\\" You may obtain a copy of the License at\n.\\\"\n.\\\"    http://www.apache.org/licenses/LICENSE-2.0\n.\\\"\n.\\\" Unless required by applicable law or agreed to in writing, software\n.\\\" distributed under the License is distributed on an \"AS IS\" BASIS,\n.\\\" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n.\\\" implied.\n.\\\" See the License for the specific language governing permissions and\n.\\\" limitations under the License.\n.\\\"\n.TH proxy-server.conf 5 \"8/26/2011\" \"Linux\" \"OpenStack Swift\"\n\n.SH NAME\n.LP\n.B proxy-server.conf\n\\- configuration file for the openstack-swift proxy server\n\n\n\n.SH SYNOPSIS\n.LP\n.B proxy-server.conf\n\n\n\n.SH DESCRIPTION\n.PP\nThis is the configuration file used by the proxy server and other proxy middlewares.\n\nThe configuration file follows the python-pastedeploy syntax. The file is divided\ninto sections, which are enclosed by square brackets. Each section will contain a\ncertain number of key/value parameters which are described later.\n\nAny line that begins with a '#' symbol is ignored.\n\nYou can find more information about python-pastedeploy configuration format at\n\\fIhttp://pythonpaste.org/deploy/#config-format\\fR\n\n\n\n.SH GLOBAL SECTION\n.PD 1\n.RS 0\nThis is indicated by section named [DEFAULT]. Below are the parameters that\nare acceptable within this section.\n\n.IP \"\\fBbind_ip\\fR\"\nIP address the proxy server should bind to. The default is 0.0.0.0 which will make\nit bind to all available addresses.\n.IP \"\\fBbind_port\\fR\"\nTCP port the proxy server should bind to. The default is 80.\n.IP \\fBbacklog\\fR\nTCP backlog.  Maximum number of allowed pending connections. The default value is 4096.\n.IP \\fBworkers\\fR\nNumber of container server workers to fork. The default is 1.\n.IP \\fBuser\\fR\nThe system user that the container server will run as. The default is swift.\n.IP \\fBswift_dir\\fR\nSwift configuration directory. The default is /etc/swift.\n.IP \\fBcert_file\\fR\nLocation of the SSL certificate file. The default path is /etc/swift/proxy.crt. This is\ndisabled by default.\n.IP \\fBkey_file\\fR\nLocation of the SSL certificate key file. The default path is /etc/swift/proxy.key. This is\ndisabled by default.\n.IP \\fBlog_name\\fR\nLabel used when logging. The default is swift.\n.IP \\fBlog_facility\\fR\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \\fBlog_level\\fR\nLogging level. The default is INFO.\n.IP \\fBlog_address\\fR\nLogging address. The default is /dev/log.\n.RE\n.PD\n\n\n\n.SH PIPELINE SECTION\n.PD 1\n.RS 0\nThis is indicated by section name [pipeline:main]. Below are the parameters that\nare acceptable within this section.\n\n.IP \"\\fBpipeline\\fR\"\nIt is used when you need apply a number of filters. It is a list of filters\nended by an application. The default should be \\fB\"catch_errors healthcheck\ncache ratelimit tempauth proxy-server\"\\fR\n.RE\n.PD\n\n\n\n.SH FILTER SECTION\n.PD 1\n.RS 0\nAny section that has its name prefixed by \"filter:\" indicates a filter section.\nFilters are used to specify configuration parameters for specific swift middlewares.\nBelow are the filters available and respective acceptable parameters.\n.IP \"\\fB[filter:healthcheck]\\fR\"\n.RE\n.RS 3\n.IP \"\\fBuse\\fR\"\nEntry point for paste.deploy for the healthcheck middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#healthcheck\\fR.\n.RE\n\n\n.RS 0\n.IP \"\\fB[filter:tempauth]\\fR\"\n.RE\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the tempauth middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#tempauth\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is tempauth.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR \"\nEnables the ability to log request headers. The default is False.\n.IP \\fBreseller_prefix\\fR\nThe reseller prefix will verify a token begins with this prefix before even\nattempting to validate it. Also, with authorization, only Swift storage accounts\nwith this prefix will be authorized by this middleware. Useful if multiple auth\nsystems are in use for one Swift cluster. The default is AUTH.\n.IP \\fBauth_prefix\\fR\nThe auth prefix will cause requests beginning with this prefix to be routed\nto the auth subsystem, for granting tokens, etc. The default is /auth/.\n.IP \\fBtoken_life\\fR\nThis is the time in seconds before the token expires. The default is 86400.\n.IP \\fBallowed_sync_hosts\\fR\nThis is a comma separated list of hosts allowed to send X-Container-Sync-Key requests.\n.IP \\fBuser_<account>_<user>\\fR\nLastly, you need to list all the accounts/users you want here. The format is:\nuser_<account>_<user> = <key> [group] [group] [...] [storage_url]\n\nThere are special groups of: \\fI.reseller_admin\\fR who can do anything to any account for this auth\nand also \\fI.admin\\fR who can do anything within the account.\n\nIf neither of these groups are specified, the user can only access containers that\nhave been explicitly allowed for them by a \\fI.admin\\fR or \\fI.reseller_admin\\fR.\nThe trailing optional storage_url allows you to specify an alternate url to hand\nback to the user upon authentication. If not specified, this defaults to\n\\fIhttp[s]://<ip>:<port>/v1/<reseller_prefix>_<account>\\fR where http or https depends\non whether cert_file is specified in the [DEFAULT] section, <ip> and <port> are based\non the [DEFAULT] section's bind_ip and bind_port (falling back to 127.0.0.1 and 8080),\n<reseller_prefix> is from this section, and <account> is from the user_<account>_<user> name.\n\nHere are example entries, required for running the tests:\n.RE\n\n.PD 0\n.RS 10\n.IP \"user_admin_admin = admin .admin .reseller_admin\"\n.IP \"user_test_tester = testing .admin\"\n.IP \"user_test2_tester2 = testing2 .admin\"\n.IP \"user_test_tester3 = testing3\"\n.RE\n.PD\n\n.RS 0\n.IP \"\\fB[filter:healthcheck]\\fR\"\n.RE\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the healthcheck middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#healthcheck\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is healthcheck.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR \"\nEnables the ability to log request headers. The default is False.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:cache]\\fR\"\n.RE\n\nCaching middleware that manages caching in swift.\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the memcache middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#memcache\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is memcache.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR \"\nEnables the ability to log request headers. The default is False.\n.IP \\fBmemcache_servers\\fR\nIf not set in the configuration file, the value for memcache_servers will be read from /etc/swift/memcache.conf (see memcache.conf-sample) or lacking that file, it will default to the value below. You can specify multiple servers separated with commas, as in: 10.1.2.3:11211,10.1.2.4:11211. This can be a list separated by commas. The default is 127.0.0.1:11211.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:ratelimit]\\fR\"\n.RE\n\nRate limits requests on both an Account and Container level.  Limits are configurable.\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the ratelimit middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#ratelimit\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is ratelimit.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR \"\nEnables the ability to log request headers. The default is False.\n.IP \\fBclock_accuracy\\fR\nThis should represent how accurate the proxy servers' system clocks are with each other.\n1000 means that all the proxies' clock are accurate to each other within 1 millisecond.\nNo ratelimit should be higher than the clock accuracy. The default is 1000.\n.IP \\fBmax_sleep_time_seconds\\fR\nApp will immediately return a 498 response if the necessary sleep time ever exceeds\nthe given max_sleep_time_seconds. The default is 60 seconds.\n.IP \\fBlog_sleep_time_seconds\\fR\nTo allow visibility into rate limiting set this value > 0 and all sleeps greater than\nthe number will be logged. If set to 0 means disabled. The default is 0.\n.IP \\fBrate_buffer_seconds\\fR\nNumber of seconds the rate counter can drop and be allowed to catch up\n(at a faster than listed rate). A larger number will result in larger spikes in\nrate but better average accuracy. The default is 5.\n.IP \\fBaccount_ratelimit\\fR\nIf set, will limit PUT and DELETE requests to /account_name/container_name. Number is\nin requests per second. If set to 0 means disabled. The default is 0.\n.IP \\fBaccount_whitelist\\fR\nComma separated lists of account names that will not be rate limited. The default is ''.\n.IP \\fBaccount_blacklist\\fR\nComma separated lists of account names that will not be allowed. Returns a 497 response.\nThe default is ''.\n.IP \\fBcontainer_ratelimit_size\\fR\nWhen set with container_limit_x = r: for containers of size x, limit requests per second\nto r. Will limit PUT, DELETE, and POST requests to /a/c/o. The default is ''.\n.RE\n\n\n.RS 0\n.IP \"\\fB[filter:domain_remap]\\fR\"\n.RE\n\nMiddleware that translates container and account parts of a domain to path parameters that the proxy server understands. The container.account.storageurl/object gets translated to container.account.storageurl/path_root/account/container/object and account.storageurl/path_root/container/object gets translated to account.storageurl/path_root/account/container/object\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the domain_remap middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#domain_remap\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is domain_remap.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR\"\nEnables the ability to log request headers. The default is False.\n.IP \\fBstorage_domain\\fR\nThe domain to be used by the middleware.\n.IP \\fBpath_root\\fR\nThe path root value for the storage URL. The default is v1.\n.IP \\fBreseller_prefixes\\fR\nBrowsers can convert a host header to lowercase, so check that reseller\nprefix on the account is the correct case. This is done by comparing the\nitems in the reseller_prefixes config option to the found prefix. If they\nmatch except for case, the item from reseller_prefixes will be used\ninstead of the found reseller prefix. The reseller_prefixes list is exclusive.\nIf defined, any request with an account prefix not in that list will be ignored\nby this middleware. Defaults to 'AUTH'.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:catch_errors]\\fR\"\n.RE\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the catch_errors middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#catch_errors\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is catch_errors.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR \"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR\"\nEnables the ability to log request headers. The default is False.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:cname_lookup]\\fR\"\n.RE\n\nNote: this middleware requires python-dnspython\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the cname_lookup middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#cname_lookup\\fR.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is cname_lookup.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR\"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR\"\nEnables the ability to log request headers. The default is False.\n.IP \\fBstorage_domain\\fR\nThe domain to be used by the middleware.\n.IP \\fBlookup_depth\\fR\nHow deep in the CNAME chain to look for something that matches the storage domain.\nThe default is 1.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:staticweb]\\fR\"\n.RE\n\nNote: Put staticweb just after your auth filter(s) in the pipeline\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the staticweb middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#staticweb\\fR.\n.IP \\fBcache_timeout\\fR\nSeconds to cache container x-container-meta-web-* header values. The default is 300 seconds.\n.IP \"\\fBset log_name\\fR\"\nLabel used when logging. The default is staticweb.\n.IP \"\\fBset log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fBset log_address\\fR \"\nLogging address. The default is /dev/log.\n.IP \"\\fBset log_headers\\fR\"\nEnables the ability to log request headers. The default is False.\n.IP \"\\fBset access_log_name\\fR\"\nLabel used when logging. The default is staticweb.\n.IP \"\\fBset access_log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset access_log_level\\fR \"\nLogging level. The default is INFO.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:tempurl]\\fR\"\n.RE\n\nNote: Put tempurl just before your auth filter(s) in the pipeline\n\n.RS 3\n.IP \\fBincoming_remove_headers\\fR\nThe headers to remove from incoming requests. Simply a whitespace delimited list of header names and names can optionally end with '*' to indicate a prefix match. incoming_allow_headers is a list of exceptions to these removals.\n.IP \\fBincoming_allow_headers\\fR\nThe headers allowed as exceptions to incoming_remove_headers. Simply a whitespace delimited list of header names and names can optionally end with '*' to indicate a prefix match.\n.IP \"\\fBoutgoing_remove_headers\\fR\"\nThe headers to remove from outgoing responses. Simply a whitespace delimited list of header names and names can optionally end with '*' to indicate a prefix match. outgoing_allow_headers is a list of exceptions to these removals.\n.IP \"\\fBoutgoing_allow_headers\\fR\"\nThe headers allowed as exceptions to outgoing_remove_headers. Simply a whitespace delimited list of header names and names can optionally end with '*' to indicate a prefix match.\n.IP \"\\fBset log_level\\fR \"\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:formpost]\\fR\"\n.RE\n\nNote: Put formpost just before your auth filter(s) in the pipeline\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the formpost middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#formpost\\fR.\n.RE\n\n\n\n.RS 0\n.IP \"\\fB[filter:name_check]\\fR\"\n.RE\n\nNote: Just needs to be placed before the proxy-server in the pipeline.\n\n.RS 3\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the name_check middleware. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#name_check\\fR.\n.IP \\fBforbidden_chars\\fR\nCharacters that will not be allowed in a name.\n.IP \\fBmaximum_length\\fR\nMaximum number of characters that can be in the name.\n.IP \\fBforbidden_regexp\\fR\nPython regular expressions of substrings that will not be allowed in a name.\n.RE\n\n\n.PD\n\n\n\n\n.SH APP SECTION\n.PD 1\n.RS 0\nThis is indicated by section name [app:proxy-server]. Below are the parameters\nthat are acceptable within this section.\n.IP \\fBuse\\fR\nEntry point for paste.deploy for the proxy server. This is the reference to the installed python egg.\nThe default is \\fBegg:swift#proxy\\fR.\n.IP \"\\fBset log_name\\fR\nLabel used when logging. The default is proxy-server.\n.IP \"\\fBset log_facility\\fR\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fB set log_level\\fR\nLogging level. The default is INFO.\n.IP \"\\fB set log_address\\fR\nLogging address. The default is /dev/log.\n.IP \"\\fBset access_log_name\\fR\"\nLabel used when logging. The default is proxy-server.\n.IP \"\\fBset access_log_facility\\fR\"\nSyslog log facility. The default is LOG_LOCAL0.\n.IP \"\\fBset access_log_level\\fR \"\nLogging level. The default is INFO.\n.IP \"\\fB set log_requests\\fR\nEnables request logging. The default is False.\n.IP \\fBrecheck_account_existence\\fR\nCache timeout in seconds to send memcached for account existence. The default is 60 seconds.\n.IP \\fBrecheck_container_existence\\fR\nCache timeout in seconds to send memcached for container existence. The default is 60 seconds.\n.IP \\fBobject_chunk_size\\fR\nChunk size to read from object servers. The default is 8192.\n.IP \\fBclient_chunk_size\\fR\nChunk size to read from clients. The default is 8192.\n.IP \\fBnode_timeout\\fR\nRequest timeout to external services. The default is 10 seconds.\n.IP \\fBclient_timeoutt\\fR\nTimeout to read one chunk from a client. The default is 60 seconds.\n.IP \\fBconn_timeout\\fR\nConnection timeout to external services. The default is 0.5 seconds.\n.IP \\fBerror_suppression_interval\\fR\nTime in seconds that must elapse since the last error for a node to\nbe considered no longer error limited. The default is 60 seconds.\n.IP \\fBerror_suppression_limit\\fR\nError count to consider a node error limited. The default is 10.\n.IP \\fBallow_account_management\\fR\nWhether account PUTs and DELETEs are even callable. If set to 'true' any authorized\nuser may create and delete accounts; if 'false' no one, even authorized, can. The default\nis false.\n.IP \\fBobject_post_as_copy\\fR\nSet object_post_as_copy = false to turn on fast posts where only the metadata changes\nare stored as new and the original data file is kept in place. This makes for quicker\nposts; but since the container metadata isn't updated in this mode, features like\ncontainer sync won't be able to sync posts. The default is True.\n.IP \\fBaccount_autocreate\\fR\nIf set to 'true' authorized accounts that do not yet exist within the Swift cluster\nwill be automatically created. The default is set to false.\n.IP \\fBrate_limit_after_segment\\fR\nRate limit the download of large object segments after this segment is\ndownloaded. The default is 10 segments.\n.IP \\fBrate_limit_segments_per_sec\\fR\nRate limit large object downlods at this rate.  The default is 1.\n.RE\n.PD\n\n\n\n.SH DOCUMENTATION\n.LP\nMore in depth documentation about the swift-proxy-server and\nalso Openstack-Swift as a whole can be found at\n.BI http://swift.openstack.org/admin_guide.html\nand\n.BI http://swift.openstack.org\n\n\n.SH \"SEE ALSO\"\n.BR swift-proxy-server(1),\n\n\n", "patch": "@@ -213,6 +213,21 @@ Logging address. The default is /dev/log.\n Enables the ability to log request headers. The default is False.\n .IP \\fBmemcache_servers\\fR\n If not set in the configuration file, the value for memcache_servers will be read from /etc/swift/memcache.conf (see memcache.conf-sample) or lacking that file, it will default to the value below. You can specify multiple servers separated with commas, as in: 10.1.2.3:11211,10.1.2.4:11211. This can be a list separated by commas. The default is 127.0.0.1:11211.\n+.IP \\fBmemcache_serialization_support\\fR\n+This sets how memcache values are serialized and deserialized:\n+.RE\n+\n+.PD 0\n+.RS 10\n+.IP \"0 = older, insecure pickle serialization\"\n+.IP \"1 = json serialization but pickles can still be read (still insecure)\"\n+.IP \"2 = json serialization only (secure and the default)\"\n+.RE\n+\n+.RS 10\n+To avoid an instant full cache flush, existing installations should upgrade with 0, then set to 1 and reload, then after some time (24 hours) set to 2 and reload. In the future, the ability to use pickle serialization will be removed.\n+\n+If not set in the configuration file, the value for memcache_serialization_support will be read from /etc/swift/memcache.conf if it exists (see memcache.conf-sample). Otherwise, the default value as indicated above will be used.\n .RE\n \n ", "file_path": "files/2012_10/9", "file_language": "5", "file_name": "doc/manpages/proxy-server.conf.5", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/openstack/swift/raw/e1ff51c04554d51616d2845f92ab726cb0e5831a/etc%2Fmemcache.conf-sample", "code": "[memcache]\n# You can use this single conf file instead of having memcache_servers set in\n# several other conf files under [filter:cache] for example. You can specify\n# multiple servers separated with commas, as in: 10.1.2.3:11211,10.1.2.4:11211\n# memcache_servers = 127.0.0.1:11211\n#\n# Sets how memcache values are serialized and deserialized:\n# 0 = older, insecure pickle serialization\n# 1 = json serialization but pickles can still be read (still insecure)\n# 2 = json serialization only (secure and the default)\n# To avoid an instant full cache flush, existing installations should\n# upgrade with 0, then set to 1 and reload, then after some time (24 hours)\n# set to 2 and reload.\n# In the future, the ability to use pickle serialization will be removed.\n# memcache_serialization_support = 2\n", "code_before": "[memcache]\n# You can use this single conf file instead of having memcache_servers set in\n# several other conf files under [filter:cache] for example. You can specify\n# multiple servers separated with commas, as in: 10.1.2.3:11211,10.1.2.4:11211\n# memcache_servers = 127.0.0.1:11211\n", "patch": "@@ -3,3 +3,13 @@\n # several other conf files under [filter:cache] for example. You can specify\n # multiple servers separated with commas, as in: 10.1.2.3:11211,10.1.2.4:11211\n # memcache_servers = 127.0.0.1:11211\n+#\n+# Sets how memcache values are serialized and deserialized:\n+# 0 = older, insecure pickle serialization\n+# 1 = json serialization but pickles can still be read (still insecure)\n+# 2 = json serialization only (secure and the default)\n+# To avoid an instant full cache flush, existing installations should\n+# upgrade with 0, then set to 1 and reload, then after some time (24 hours)\n+# set to 2 and reload.\n+# In the future, the ability to use pickle serialization will be removed.\n+# memcache_serialization_support = 2", "file_path": "files/2012_10/10", "file_language": "conf-sample", "file_name": "etc/memcache.conf-sample", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/openstack/swift/raw/e1ff51c04554d51616d2845f92ab726cb0e5831a/etc%2Fproxy-server.conf-sample", "code": "[DEFAULT]\n# bind_ip = 0.0.0.0\n# bind_port = 80\n# backlog = 4096\n# swift_dir = /etc/swift\n# workers = 1\n# user = swift\n# Set the following two lines to enable SSL. This is for testing only.\n# cert_file = /etc/swift/proxy.crt\n# key_file = /etc/swift/proxy.key\n# expiring_objects_container_divisor = 86400\n# You can specify default log routing here if you want:\n# log_name = swift\n# log_facility = LOG_LOCAL0\n# log_level = INFO\n# log_address = /dev/log\n# You can enable default statsD logging here and/or override it in sections\n# below:\n# log_statsd_host = localhost\n# log_statsd_port = 8125\n# log_statsd_default_sample_rate = 1\n# log_statsd_metric_prefix =\n\n[pipeline:main]\npipeline = catch_errors healthcheck cache ratelimit tempauth proxy-logging proxy-server\n\n[app:proxy-server]\nuse = egg:swift#proxy\n# You can override the default log routing for this app here:\n# set log_name = proxy-server\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_address = /dev/log\n# set access_log_name = proxy-server\n# set access_log_facility = LOG_LOCAL0\n# set access_log_level = INFO\n# set log_headers = False\n# set log_handoffs = True\n# recheck_account_existence = 60\n# recheck_container_existence = 60\n# object_chunk_size = 8192\n# client_chunk_size = 8192\n# node_timeout = 10\n# client_timeout = 60\n# conn_timeout = 0.5\n# How long without an error before a node's error count is reset. This will\n# also be how long before a node is reenabled after suppression is triggered.\n# error_suppression_interval = 60\n# How many errors can accumulate before a node is temporarily ignored.\n# error_suppression_limit = 10\n# If set to 'true' any authorized user may create and delete accounts; if\n# 'false' no one, even authorized, can.\n# allow_account_management = false\n# Set object_post_as_copy = false to turn on fast posts where only the metadata\n# changes are stored anew and the original data file is kept in place. This\n# makes for quicker posts; but since the container metadata isn't updated in\n# this mode, features like container sync won't be able to sync posts.\n# object_post_as_copy = true\n# If set to 'true' authorized accounts that do not yet exist within the Swift\n# cluster will be automatically created.\n# account_autocreate = false\n# If set to a positive value, trying to create a container when the account\n# already has at least this maximum containers will result in a 403 Forbidden.\n# Note: This is a soft limit, meaning a user might exceed the cap for\n# recheck_account_existence before the 403s kick in.\n# max_containers_per_account = 0\n# This is a comma separated list of account hashes that ignore the\n# max_containers_per_account cap.\n# max_containers_whitelist =\n# comma separated list of Host headers the proxy will be deny requests to\n# deny_host_headers =\n# prefix used when automatically creating accounts\n# auto_create_account_prefix = .\n# depth of the proxy put queue\n# put_queue_depth = 10\n\n[filter:tempauth]\nuse = egg:swift#tempauth\n# You can override the default log routing for this filter here:\n# set log_name = tempauth\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n# The reseller prefix will verify a token begins with this prefix before even\n# attempting to validate it. Also, with authorization, only Swift storage\n# accounts with this prefix will be authorized by this middleware. Useful if\n# multiple auth systems are in use for one Swift cluster.\n# reseller_prefix = AUTH\n# The auth prefix will cause requests beginning with this prefix to be routed\n# to the auth subsystem, for granting tokens, etc.\n# auth_prefix = /auth/\n# token_life = 86400\n# This is a comma separated list of hosts allowed to send X-Container-Sync-Key\n# requests.\n# allowed_sync_hosts = 127.0.0.1\n# This allows middleware higher in the WSGI pipeline to override auth\n# processing, useful for middleware such as tempurl and formpost. If you know\n# you're not going to use such middleware and you want a bit of extra security,\n# you can set this to false.\n# allow_overrides = true\n# Lastly, you need to list all the accounts/users you want here. The format is:\n#   user_<account>_<user> = <key> [group] [group] [...] [storage_url]\n# There are special groups of:\n#   .reseller_admin = can do anything to any account for this auth\n#   .admin = can do anything within the account\n# If neither of these groups are specified, the user can only access containers\n# that have been explicitly allowed for them by a .admin or .reseller_admin.\n# The trailing optional storage_url allows you to specify an alternate url to\n# hand back to the user upon authentication. If not specified, this defaults to\n# http[s]://<ip>:<port>/v1/<reseller_prefix>_<account> where http or https\n# depends on whether cert_file is specified in the [DEFAULT] section, <ip> and\n# <port> are based on the [DEFAULT] section's bind_ip and bind_port (falling\n# back to 127.0.0.1 and 8080), <reseller_prefix> is from this section, and\n# <account> is from the user_<account>_<user> name.\n# Here are example entries, required for running the tests:\nuser_admin_admin = admin .admin .reseller_admin\nuser_test_tester = testing .admin\nuser_test2_tester2 = testing2 .admin\nuser_test_tester3 = testing3\n\n# To enable Keystone authentication you need to have the auth token\n# middleware first to be configured. Here is an example below, please\n# refer to the keystone's documentation for details about the\n# different settings.\n#\n# You'll need to have as well the keystoneauth middleware enabled\n# and have it in your main pipeline so instead of having tempauth in\n# there you can change it to: authtoken keystone\n#\n# [filter:authtoken]\n# paste.filter_factory = keystone.middleware.auth_token:filter_factory\n# auth_host = keystonehost\n# auth_port = 35357\n# auth_protocol = http\n# auth_uri = http://keystonehost:5000/\n# admin_tenant_name = service\n# admin_user = swift\n# admin_password = password\n# delay_auth_decision = 1\n#\n# [filter:keystoneauth]\n# use = egg:swift#keystoneauth\n# Operator roles is the role which user would be allowed to manage a\n# tenant and be able to create container or give ACL to others.\n# operator_roles = admin, swiftoperator\n\n[filter:healthcheck]\nuse = egg:swift#healthcheck\n# You can override the default log routing for this filter here:\n# set log_name = healthcheck\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n\n[filter:cache]\nuse = egg:swift#memcache\n# You can override the default log routing for this filter here:\n# set log_name = cache\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n# If not set here, the value for memcache_servers will be read from\n# memcache.conf (see memcache.conf-sample) or lacking that file, it will\n# default to the value below. You can specify multiple servers separated with\n# commas, as in: 10.1.2.3:11211,10.1.2.4:11211\n# memcache_servers = 127.0.0.1:11211\n#\n# Sets how memcache values are serialized and deserialized:\n# 0 = older, insecure pickle serialization\n# 1 = json serialization but pickles can still be read (still insecure)\n# 2 = json serialization only (secure and the default)\n# If not set here, the value for memcache_serialization_support will be read\n# from /etc/swift/memcache.conf (see memcache.conf-sample).\n# To avoid an instant full cache flush, existing installations should\n# upgrade with 0, then set to 1 and reload, then after some time (24 hours)\n# set to 2 and reload.\n# In the future, the ability to use pickle serialization will be removed.\n# memcache_serialization_support = 2\n\n[filter:ratelimit]\nuse = egg:swift#ratelimit\n# You can override the default log routing for this filter here:\n# set log_name = ratelimit\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n# clock_accuracy should represent how accurate the proxy servers' system clocks\n# are with each other. 1000 means that all the proxies' clock are accurate to\n# each other within 1 millisecond.  No ratelimit should be higher than the\n# clock accuracy.\n# clock_accuracy = 1000\n# max_sleep_time_seconds = 60\n# log_sleep_time_seconds of 0 means disabled\n# log_sleep_time_seconds = 0\n# allows for slow rates (e.g. running up to 5 sec's behind) to catch up.\n# rate_buffer_seconds = 5\n# account_ratelimit of 0 means disabled\n# account_ratelimit = 0\n\n# these are comma separated lists of account names\n# account_whitelist = a,b\n# account_blacklist = c,d\n\n# with container_limit_x = r\n# for containers of size x limit requests per second to r.  The container\n# rate will be linearly interpolated from the values given. With the values\n# below, a container of size 5 will get a rate of 75.\n# container_ratelimit_0 = 100\n# container_ratelimit_10 = 50\n# container_ratelimit_50 = 20\n\n[filter:domain_remap]\nuse = egg:swift#domain_remap\n# You can override the default log routing for this filter here:\n# set log_name = domain_remap\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n# storage_domain = example.com\n# path_root = v1\n# reseller_prefixes = AUTH\n\n[filter:catch_errors]\nuse = egg:swift#catch_errors\n# You can override the default log routing for this filter here:\n# set log_name = catch_errors\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n\n[filter:cname_lookup]\n# Note: this middleware requires python-dnspython\nuse = egg:swift#cname_lookup\n# You can override the default log routing for this filter here:\n# set log_name = cname_lookup\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n# storage_domain = example.com\n# lookup_depth = 1\n\n# Note: Put staticweb just after your auth filter(s) in the pipeline\n[filter:staticweb]\nuse = egg:swift#staticweb\n# Seconds to cache container x-container-meta-web-* header values.\n# cache_timeout = 300\n# You can override the default log routing for this filter here:\n# set log_name = staticweb\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_address = /dev/log\n# set access_log_name = staticweb\n# set access_log_facility = LOG_LOCAL0\n# set access_log_level = INFO\n# set log_headers = False\n\n# Note: Put tempurl just before your auth filter(s) in the pipeline\n[filter:tempurl]\nuse = egg:swift#tempurl\n#\n# The headers to remove from incoming requests. Simply a whitespace delimited\n# list of header names and names can optionally end with '*' to indicate a\n# prefix match. incoming_allow_headers is a list of exceptions to these\n# removals.\n# incoming_remove_headers = x-timestamp\n#\n# The headers allowed as exceptions to incoming_remove_headers. Simply a\n# whitespace delimited list of header names and names can optionally end with\n# '*' to indicate a prefix match.\n# incoming_allow_headers =\n#\n# The headers to remove from outgoing responses. Simply a whitespace delimited\n# list of header names and names can optionally end with '*' to indicate a\n# prefix match. outgoing_allow_headers is a list of exceptions to these\n# removals.\n# outgoing_remove_headers = x-object-meta-*\n#\n# The headers allowed as exceptions to outgoing_remove_headers. Simply a\n# whitespace delimited list of header names and names can optionally end with\n# '*' to indicate a prefix match.\n# outgoing_allow_headers = x-object-meta-public-*\n\n# Note: Put formpost just before your auth filter(s) in the pipeline\n[filter:formpost]\nuse = egg:swift#formpost\n\n# Note: Just needs to be placed before the proxy-server in the pipeline.\n[filter:name_check]\nuse = egg:swift#name_check\n# forbidden_chars = '\"`<>\n# maximum_length = 255\n# forbidden_regexp = /\\./|/\\.\\./|/\\.$|/\\.\\.$\n\n[filter:proxy-logging]\nuse = egg:swift#proxy_logging\n", "code_before": "[DEFAULT]\n# bind_ip = 0.0.0.0\n# bind_port = 80\n# backlog = 4096\n# swift_dir = /etc/swift\n# workers = 1\n# user = swift\n# Set the following two lines to enable SSL. This is for testing only.\n# cert_file = /etc/swift/proxy.crt\n# key_file = /etc/swift/proxy.key\n# expiring_objects_container_divisor = 86400\n# You can specify default log routing here if you want:\n# log_name = swift\n# log_facility = LOG_LOCAL0\n# log_level = INFO\n# log_address = /dev/log\n# You can enable default statsD logging here and/or override it in sections\n# below:\n# log_statsd_host = localhost\n# log_statsd_port = 8125\n# log_statsd_default_sample_rate = 1\n# log_statsd_metric_prefix =\n\n[pipeline:main]\npipeline = catch_errors healthcheck cache ratelimit tempauth proxy-logging proxy-server\n\n[app:proxy-server]\nuse = egg:swift#proxy\n# You can override the default log routing for this app here:\n# set log_name = proxy-server\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_address = /dev/log\n# set access_log_name = proxy-server\n# set access_log_facility = LOG_LOCAL0\n# set access_log_level = INFO\n# set log_headers = False\n# set log_handoffs = True\n# recheck_account_existence = 60\n# recheck_container_existence = 60\n# object_chunk_size = 8192\n# client_chunk_size = 8192\n# node_timeout = 10\n# client_timeout = 60\n# conn_timeout = 0.5\n# How long without an error before a node's error count is reset. This will\n# also be how long before a node is reenabled after suppression is triggered.\n# error_suppression_interval = 60\n# How many errors can accumulate before a node is temporarily ignored.\n# error_suppression_limit = 10\n# If set to 'true' any authorized user may create and delete accounts; if\n# 'false' no one, even authorized, can.\n# allow_account_management = false\n# Set object_post_as_copy = false to turn on fast posts where only the metadata\n# changes are stored anew and the original data file is kept in place. This\n# makes for quicker posts; but since the container metadata isn't updated in\n# this mode, features like container sync won't be able to sync posts.\n# object_post_as_copy = true\n# If set to 'true' authorized accounts that do not yet exist within the Swift\n# cluster will be automatically created.\n# account_autocreate = false\n# If set to a positive value, trying to create a container when the account\n# already has at least this maximum containers will result in a 403 Forbidden.\n# Note: This is a soft limit, meaning a user might exceed the cap for\n# recheck_account_existence before the 403s kick in.\n# max_containers_per_account = 0\n# This is a comma separated list of account hashes that ignore the\n# max_containers_per_account cap.\n# max_containers_whitelist =\n# comma separated list of Host headers the proxy will be deny requests to\n# deny_host_headers =\n# prefix used when automatically creating accounts\n# auto_create_account_prefix = .\n# depth of the proxy put queue\n# put_queue_depth = 10\n\n[filter:tempauth]\nuse = egg:swift#tempauth\n# You can override the default log routing for this filter here:\n# set log_name = tempauth\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n# The reseller prefix will verify a token begins with this prefix before even\n# attempting to validate it. Also, with authorization, only Swift storage\n# accounts with this prefix will be authorized by this middleware. Useful if\n# multiple auth systems are in use for one Swift cluster.\n# reseller_prefix = AUTH\n# The auth prefix will cause requests beginning with this prefix to be routed\n# to the auth subsystem, for granting tokens, etc.\n# auth_prefix = /auth/\n# token_life = 86400\n# This is a comma separated list of hosts allowed to send X-Container-Sync-Key\n# requests.\n# allowed_sync_hosts = 127.0.0.1\n# This allows middleware higher in the WSGI pipeline to override auth\n# processing, useful for middleware such as tempurl and formpost. If you know\n# you're not going to use such middleware and you want a bit of extra security,\n# you can set this to false.\n# allow_overrides = true\n# Lastly, you need to list all the accounts/users you want here. The format is:\n#   user_<account>_<user> = <key> [group] [group] [...] [storage_url]\n# There are special groups of:\n#   .reseller_admin = can do anything to any account for this auth\n#   .admin = can do anything within the account\n# If neither of these groups are specified, the user can only access containers\n# that have been explicitly allowed for them by a .admin or .reseller_admin.\n# The trailing optional storage_url allows you to specify an alternate url to\n# hand back to the user upon authentication. If not specified, this defaults to\n# http[s]://<ip>:<port>/v1/<reseller_prefix>_<account> where http or https\n# depends on whether cert_file is specified in the [DEFAULT] section, <ip> and\n# <port> are based on the [DEFAULT] section's bind_ip and bind_port (falling\n# back to 127.0.0.1 and 8080), <reseller_prefix> is from this section, and\n# <account> is from the user_<account>_<user> name.\n# Here are example entries, required for running the tests:\nuser_admin_admin = admin .admin .reseller_admin\nuser_test_tester = testing .admin\nuser_test2_tester2 = testing2 .admin\nuser_test_tester3 = testing3\n\n# To enable Keystone authentication you need to have the auth token\n# middleware first to be configured. Here is an example below, please\n# refer to the keystone's documentation for details about the\n# different settings.\n#\n# You'll need to have as well the keystoneauth middleware enabled\n# and have it in your main pipeline so instead of having tempauth in\n# there you can change it to: authtoken keystone\n#\n# [filter:authtoken]\n# paste.filter_factory = keystone.middleware.auth_token:filter_factory\n# auth_host = keystonehost\n# auth_port = 35357\n# auth_protocol = http\n# auth_uri = http://keystonehost:5000/\n# admin_tenant_name = service\n# admin_user = swift\n# admin_password = password\n# delay_auth_decision = 1\n#\n# [filter:keystoneauth]\n# use = egg:swift#keystoneauth\n# Operator roles is the role which user would be allowed to manage a\n# tenant and be able to create container or give ACL to others.\n# operator_roles = admin, swiftoperator\n\n[filter:healthcheck]\nuse = egg:swift#healthcheck\n# You can override the default log routing for this filter here:\n# set log_name = healthcheck\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n\n[filter:cache]\nuse = egg:swift#memcache\n# You can override the default log routing for this filter here:\n# set log_name = cache\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n# If not set here, the value for memcache_servers will be read from\n# memcache.conf (see memcache.conf-sample) or lacking that file, it will\n# default to the value below. You can specify multiple servers separated with\n# commas, as in: 10.1.2.3:11211,10.1.2.4:11211\n# memcache_servers = 127.0.0.1:11211\n\n[filter:ratelimit]\nuse = egg:swift#ratelimit\n# You can override the default log routing for this filter here:\n# set log_name = ratelimit\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n# clock_accuracy should represent how accurate the proxy servers' system clocks\n# are with each other. 1000 means that all the proxies' clock are accurate to\n# each other within 1 millisecond.  No ratelimit should be higher than the\n# clock accuracy.\n# clock_accuracy = 1000\n# max_sleep_time_seconds = 60\n# log_sleep_time_seconds of 0 means disabled\n# log_sleep_time_seconds = 0\n# allows for slow rates (e.g. running up to 5 sec's behind) to catch up.\n# rate_buffer_seconds = 5\n# account_ratelimit of 0 means disabled\n# account_ratelimit = 0\n\n# these are comma separated lists of account names\n# account_whitelist = a,b\n# account_blacklist = c,d\n\n# with container_limit_x = r\n# for containers of size x limit requests per second to r.  The container\n# rate will be linearly interpolated from the values given. With the values\n# below, a container of size 5 will get a rate of 75.\n# container_ratelimit_0 = 100\n# container_ratelimit_10 = 50\n# container_ratelimit_50 = 20\n\n[filter:domain_remap]\nuse = egg:swift#domain_remap\n# You can override the default log routing for this filter here:\n# set log_name = domain_remap\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n# storage_domain = example.com\n# path_root = v1\n# reseller_prefixes = AUTH\n\n[filter:catch_errors]\nuse = egg:swift#catch_errors\n# You can override the default log routing for this filter here:\n# set log_name = catch_errors\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n\n[filter:cname_lookup]\n# Note: this middleware requires python-dnspython\nuse = egg:swift#cname_lookup\n# You can override the default log routing for this filter here:\n# set log_name = cname_lookup\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_headers = False\n# set log_address = /dev/log\n# storage_domain = example.com\n# lookup_depth = 1\n\n# Note: Put staticweb just after your auth filter(s) in the pipeline\n[filter:staticweb]\nuse = egg:swift#staticweb\n# Seconds to cache container x-container-meta-web-* header values.\n# cache_timeout = 300\n# You can override the default log routing for this filter here:\n# set log_name = staticweb\n# set log_facility = LOG_LOCAL0\n# set log_level = INFO\n# set log_address = /dev/log\n# set access_log_name = staticweb\n# set access_log_facility = LOG_LOCAL0\n# set access_log_level = INFO\n# set log_headers = False\n\n# Note: Put tempurl just before your auth filter(s) in the pipeline\n[filter:tempurl]\nuse = egg:swift#tempurl\n#\n# The headers to remove from incoming requests. Simply a whitespace delimited\n# list of header names and names can optionally end with '*' to indicate a\n# prefix match. incoming_allow_headers is a list of exceptions to these\n# removals.\n# incoming_remove_headers = x-timestamp\n#\n# The headers allowed as exceptions to incoming_remove_headers. Simply a\n# whitespace delimited list of header names and names can optionally end with\n# '*' to indicate a prefix match.\n# incoming_allow_headers =\n#\n# The headers to remove from outgoing responses. Simply a whitespace delimited\n# list of header names and names can optionally end with '*' to indicate a\n# prefix match. outgoing_allow_headers is a list of exceptions to these\n# removals.\n# outgoing_remove_headers = x-object-meta-*\n#\n# The headers allowed as exceptions to outgoing_remove_headers. Simply a\n# whitespace delimited list of header names and names can optionally end with\n# '*' to indicate a prefix match.\n# outgoing_allow_headers = x-object-meta-public-*\n\n# Note: Put formpost just before your auth filter(s) in the pipeline\n[filter:formpost]\nuse = egg:swift#formpost\n\n# Note: Just needs to be placed before the proxy-server in the pipeline.\n[filter:name_check]\nuse = egg:swift#name_check\n# forbidden_chars = '\"`<>\n# maximum_length = 255\n# forbidden_regexp = /\\./|/\\.\\./|/\\.$|/\\.\\.$\n\n[filter:proxy-logging]\nuse = egg:swift#proxy_logging\n", "patch": "@@ -167,6 +167,18 @@ use = egg:swift#memcache\n # default to the value below. You can specify multiple servers separated with\n # commas, as in: 10.1.2.3:11211,10.1.2.4:11211\n # memcache_servers = 127.0.0.1:11211\n+#\n+# Sets how memcache values are serialized and deserialized:\n+# 0 = older, insecure pickle serialization\n+# 1 = json serialization but pickles can still be read (still insecure)\n+# 2 = json serialization only (secure and the default)\n+# If not set here, the value for memcache_serialization_support will be read\n+# from /etc/swift/memcache.conf (see memcache.conf-sample).\n+# To avoid an instant full cache flush, existing installations should\n+# upgrade with 0, then set to 1 and reload, then after some time (24 hours)\n+# set to 2 and reload.\n+# In the future, the ability to use pickle serialization will be removed.\n+# memcache_serialization_support = 2\n \n [filter:ratelimit]\n use = egg:swift#ratelimit", "file_path": "files/2012_10/11", "file_language": "conf-sample", "file_name": "etc/proxy-server.conf-sample", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/openstack/swift/raw/e1ff51c04554d51616d2845f92ab726cb0e5831a/swift%2Fcommon%2Fmemcached.py", "code": "# Copyright (c) 2010-2012 OpenStack, LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nLucid comes with memcached: v1.4.2.  Protocol documentation for that\nversion is at:\n\nhttp://github.com/memcached/memcached/blob/1.4.2/doc/protocol.txt\n\"\"\"\n\nimport cPickle as pickle\nimport logging\nimport socket\nimport time\nfrom bisect import bisect\nfrom hashlib import md5\n\ntry:\n    import simplejson as json\nexcept ImportError:\n    import json\n\nDEFAULT_MEMCACHED_PORT = 11211\n\nCONN_TIMEOUT = 0.3\nIO_TIMEOUT = 2.0\nPICKLE_FLAG = 1\nJSON_FLAG = 2\nNODE_WEIGHT = 50\nPICKLE_PROTOCOL = 2\nTRY_COUNT = 3\n\n# if ERROR_LIMIT_COUNT errors occur in ERROR_LIMIT_TIME seconds, the server\n# will be considered failed for ERROR_LIMIT_DURATION seconds.\nERROR_LIMIT_COUNT = 10\nERROR_LIMIT_TIME = 60\nERROR_LIMIT_DURATION = 60\n\n\ndef md5hash(key):\n    return md5(key).hexdigest()\n\n\nclass MemcacheConnectionError(Exception):\n    pass\n\n\nclass MemcacheRing(object):\n    \"\"\"\n    Simple, consistent-hashed memcache client.\n    \"\"\"\n\n    def __init__(self, servers, connect_timeout=CONN_TIMEOUT,\n                 io_timeout=IO_TIMEOUT, tries=TRY_COUNT,\n                 allow_pickle=False, allow_unpickle=False):\n        self._ring = {}\n        self._errors = dict(((serv, []) for serv in servers))\n        self._error_limited = dict(((serv, 0) for serv in servers))\n        for server in sorted(servers):\n            for i in xrange(NODE_WEIGHT):\n                self._ring[md5hash('%s-%s' % (server, i))] = server\n        self._tries = tries if tries <= len(servers) else len(servers)\n        self._sorted = sorted(self._ring.keys())\n        self._client_cache = dict(((server, []) for server in servers))\n        self._connect_timeout = connect_timeout\n        self._io_timeout = io_timeout\n        self._allow_pickle = allow_pickle\n        self._allow_unpickle = allow_unpickle or allow_pickle\n\n    def _exception_occurred(self, server, e, action='talking'):\n        if isinstance(e, socket.timeout):\n            logging.error(_(\"Timeout %(action)s to memcached: %(server)s\"),\n                {'action': action, 'server': server})\n        else:\n            logging.exception(_(\"Error %(action)s to memcached: %(server)s\"),\n                {'action': action, 'server': server})\n        now = time.time()\n        self._errors[server].append(time.time())\n        if len(self._errors[server]) > ERROR_LIMIT_COUNT:\n            self._errors[server] = [err for err in self._errors[server]\n                                          if err > now - ERROR_LIMIT_TIME]\n            if len(self._errors[server]) > ERROR_LIMIT_COUNT:\n                self._error_limited[server] = now + ERROR_LIMIT_DURATION\n                logging.error(_('Error limiting server %s'), server)\n\n    def _get_conns(self, key):\n        \"\"\"\n        Retrieves a server conn from the pool, or connects a new one.\n        Chooses the server based on a consistent hash of \"key\".\n        \"\"\"\n        pos = bisect(self._sorted, key)\n        served = []\n        while len(served) < self._tries:\n            pos = (pos + 1) % len(self._sorted)\n            server = self._ring[self._sorted[pos]]\n            if server in served:\n                continue\n            served.append(server)\n            if self._error_limited[server] > time.time():\n                continue\n            try:\n                fp, sock = self._client_cache[server].pop()\n                yield server, fp, sock\n            except IndexError:\n                try:\n                    if ':' in server:\n                        host, port = server.split(':')\n                    else:\n                        host = server\n                        port = DEFAULT_MEMCACHED_PORT\n                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                    sock.settimeout(self._connect_timeout)\n                    sock.connect((host, int(port)))\n                    sock.settimeout(self._io_timeout)\n                    yield server, sock.makefile(), sock\n                except Exception, e:\n                    self._exception_occurred(server, e, 'connecting')\n\n    def _return_conn(self, server, fp, sock):\n        \"\"\" Returns a server connection to the pool \"\"\"\n        self._client_cache[server].append((fp, sock))\n\n    def set(self, key, value, serialize=True, timeout=0):\n        \"\"\"\n        Set a key/value pair in memcache\n\n        :param key: key\n        :param value: value\n        :param serialize: if True, value is serialized with JSON before sending\n                          to memcache, or with pickle if configured to use\n                          pickle instead of JSON (to avoid cache poisoning)\n        :param timeout: ttl in memcache\n        \"\"\"\n        key = md5hash(key)\n        if timeout > 0:\n            timeout += time.time()\n        flags = 0\n        if serialize and self._allow_pickle:\n            value = pickle.dumps(value, PICKLE_PROTOCOL)\n            flags |= PICKLE_FLAG\n        elif serialize:\n            value = json.dumps(value)\n            flags |= JSON_FLAG\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('set %s %d %d %s noreply\\r\\n%s\\r\\n' % \\\n                              (key, flags, timeout, len(value), value))\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def get(self, key):\n        \"\"\"\n        Gets the object specified by key.  It will also unserialize the object\n        before returning if it is serialized in memcache with JSON, or if it\n        is pickled and unpickling is allowed.\n\n        :param key: key\n        :returns: value of the key in memcache\n        \"\"\"\n        key = md5hash(key)\n        value = None\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('get %s\\r\\n' % key)\n                line = fp.readline().strip().split()\n                while line[0].upper() != 'END':\n                    if line[0].upper() == 'VALUE' and line[1] == key:\n                        size = int(line[3])\n                        value = fp.read(size)\n                        if int(line[2]) & PICKLE_FLAG:\n                            if self._allow_unpickle:\n                                value = pickle.loads(value)\n                            else:\n                                value = None\n                        elif int(line[2]) & JSON_FLAG:\n                            value = json.loads(value)\n                        fp.readline()\n                    line = fp.readline().strip().split()\n                self._return_conn(server, fp, sock)\n                return value\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def incr(self, key, delta=1, timeout=0):\n        \"\"\"\n        Increments a key which has a numeric value by delta.\n        If the key can't be found, it's added as delta or 0 if delta < 0.\n        If passed a negative number, will use memcached's decr. Returns\n        the int stored in memcached\n        Note: The data memcached stores as the result of incr/decr is\n        an unsigned int.  decr's that result in a number below 0 are\n        stored as 0.\n\n        :param key: key\n        :param delta: amount to add to the value of key (or set as the value\n                      if the key is not found) will be cast to an int\n        :param timeout: ttl in memcache\n        :raises MemcacheConnectionError:\n        \"\"\"\n        key = md5hash(key)\n        command = 'incr'\n        if delta < 0:\n            command = 'decr'\n        delta = str(abs(int(delta)))\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('%s %s %s\\r\\n' % (command, key, delta))\n                line = fp.readline().strip().split()\n                if line[0].upper() == 'NOT_FOUND':\n                    add_val = delta\n                    if command == 'decr':\n                        add_val = '0'\n                    sock.sendall('add %s %d %d %s\\r\\n%s\\r\\n' % \\\n                                  (key, 0, timeout, len(add_val), add_val))\n                    line = fp.readline().strip().split()\n                    if line[0].upper() == 'NOT_STORED':\n                        sock.sendall('%s %s %s\\r\\n' % (command, key, delta))\n                        line = fp.readline().strip().split()\n                        ret = int(line[0].strip())\n                    else:\n                        ret = int(add_val)\n                else:\n                    ret = int(line[0].strip())\n                self._return_conn(server, fp, sock)\n                return ret\n            except Exception, e:\n                self._exception_occurred(server, e)\n        raise MemcacheConnectionError(\"No Memcached connections succeeded.\")\n\n    def decr(self, key, delta=1, timeout=0):\n        \"\"\"\n        Decrements a key which has a numeric value by delta. Calls incr with\n        -delta.\n\n        :param key: key\n        :param delta: amount to subtract to the value of key (or set the\n                      value to 0 if the key is not found) will be cast to\n                      an int\n        :param timeout: ttl in memcache\n        :raises MemcacheConnectionError:\n        \"\"\"\n        self.incr(key, delta=-delta, timeout=timeout)\n\n    def delete(self, key):\n        \"\"\"\n        Deletes a key/value pair from memcache.\n\n        :param key: key to be deleted\n        \"\"\"\n        key = md5hash(key)\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('delete %s noreply\\r\\n' % key)\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def set_multi(self, mapping, server_key, serialize=True, timeout=0):\n        \"\"\"\n        Sets multiple key/value pairs in memcache.\n\n        :param mapping: dictonary of keys and values to be set in memcache\n        :param servery_key: key to use in determining which server in the ring\n                            is used\n        :param serialize: if True, value is serialized with JSON before sending\n                          to memcache, or with pickle if configured to use\n                          pickle instead of JSON (to avoid cache poisoning)\n        :param timeout: ttl for memcache\n        \"\"\"\n        server_key = md5hash(server_key)\n        if timeout > 0:\n            timeout += time.time()\n        msg = ''\n        for key, value in mapping.iteritems():\n            key = md5hash(key)\n            flags = 0\n            if serialize and self._allow_pickle:\n                value = pickle.dumps(value, PICKLE_PROTOCOL)\n                flags |= PICKLE_FLAG\n            elif serialize:\n                value = json.dumps(value)\n                flags |= JSON_FLAG\n            msg += ('set %s %d %d %s noreply\\r\\n%s\\r\\n' %\n                    (key, flags, timeout, len(value), value))\n        for (server, fp, sock) in self._get_conns(server_key):\n            try:\n                sock.sendall(msg)\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def get_multi(self, keys, server_key):\n        \"\"\"\n        Gets multiple values from memcache for the given keys.\n\n        :param keys: keys for values to be retrieved from memcache\n        :param servery_key: key to use in determining which server in the ring\n                            is used\n        :returns: list of values\n        \"\"\"\n        server_key = md5hash(server_key)\n        keys = [md5hash(key) for key in keys]\n        for (server, fp, sock) in self._get_conns(server_key):\n            try:\n                sock.sendall('get %s\\r\\n' % ' '.join(keys))\n                line = fp.readline().strip().split()\n                responses = {}\n                while line[0].upper() != 'END':\n                    if line[0].upper() == 'VALUE':\n                        size = int(line[3])\n                        value = fp.read(size)\n                        if int(line[2]) & PICKLE_FLAG:\n                            if self._allow_unpickle:\n                                value = pickle.loads(value)\n                            else:\n                                value = None\n                        elif int(line[2]) & JSON_FLAG:\n                            value = json.loads(value)\n                        responses[line[1]] = value\n                        fp.readline()\n                    line = fp.readline().strip().split()\n                values = []\n                for key in keys:\n                    if key in responses:\n                        values.append(responses[key])\n                    else:\n                        values.append(None)\n                self._return_conn(server, fp, sock)\n                return values\n            except Exception, e:\n                self._exception_occurred(server, e)\n", "code_before": "# Copyright (c) 2010-2012 OpenStack, LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nLucid comes with memcached: v1.4.2.  Protocol documentation for that\nversion is at:\n\nhttp://github.com/memcached/memcached/blob/1.4.2/doc/protocol.txt\n\"\"\"\n\nimport cPickle as pickle\nimport logging\nimport socket\nimport time\nfrom bisect import bisect\nfrom hashlib import md5\n\nDEFAULT_MEMCACHED_PORT = 11211\n\nCONN_TIMEOUT = 0.3\nIO_TIMEOUT = 2.0\nPICKLE_FLAG = 1\nNODE_WEIGHT = 50\nPICKLE_PROTOCOL = 2\nTRY_COUNT = 3\n\n# if ERROR_LIMIT_COUNT errors occur in ERROR_LIMIT_TIME seconds, the server\n# will be considered failed for ERROR_LIMIT_DURATION seconds.\nERROR_LIMIT_COUNT = 10\nERROR_LIMIT_TIME = 60\nERROR_LIMIT_DURATION = 60\n\n\ndef md5hash(key):\n    return md5(key).hexdigest()\n\n\nclass MemcacheConnectionError(Exception):\n    pass\n\n\nclass MemcacheRing(object):\n    \"\"\"\n    Simple, consistent-hashed memcache client.\n    \"\"\"\n\n    def __init__(self, servers, connect_timeout=CONN_TIMEOUT,\n                 io_timeout=IO_TIMEOUT, tries=TRY_COUNT):\n        self._ring = {}\n        self._errors = dict(((serv, []) for serv in servers))\n        self._error_limited = dict(((serv, 0) for serv in servers))\n        for server in sorted(servers):\n            for i in xrange(NODE_WEIGHT):\n                self._ring[md5hash('%s-%s' % (server, i))] = server\n        self._tries = tries if tries <= len(servers) else len(servers)\n        self._sorted = sorted(self._ring.keys())\n        self._client_cache = dict(((server, []) for server in servers))\n        self._connect_timeout = connect_timeout\n        self._io_timeout = io_timeout\n\n    def _exception_occurred(self, server, e, action='talking'):\n        if isinstance(e, socket.timeout):\n            logging.error(_(\"Timeout %(action)s to memcached: %(server)s\"),\n                {'action': action, 'server': server})\n        else:\n            logging.exception(_(\"Error %(action)s to memcached: %(server)s\"),\n                {'action': action, 'server': server})\n        now = time.time()\n        self._errors[server].append(time.time())\n        if len(self._errors[server]) > ERROR_LIMIT_COUNT:\n            self._errors[server] = [err for err in self._errors[server]\n                                          if err > now - ERROR_LIMIT_TIME]\n            if len(self._errors[server]) > ERROR_LIMIT_COUNT:\n                self._error_limited[server] = now + ERROR_LIMIT_DURATION\n                logging.error(_('Error limiting server %s'), server)\n\n    def _get_conns(self, key):\n        \"\"\"\n        Retrieves a server conn from the pool, or connects a new one.\n        Chooses the server based on a consistent hash of \"key\".\n        \"\"\"\n        pos = bisect(self._sorted, key)\n        served = []\n        while len(served) < self._tries:\n            pos = (pos + 1) % len(self._sorted)\n            server = self._ring[self._sorted[pos]]\n            if server in served:\n                continue\n            served.append(server)\n            if self._error_limited[server] > time.time():\n                continue\n            try:\n                fp, sock = self._client_cache[server].pop()\n                yield server, fp, sock\n            except IndexError:\n                try:\n                    if ':' in server:\n                        host, port = server.split(':')\n                    else:\n                        host = server\n                        port = DEFAULT_MEMCACHED_PORT\n                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                    sock.settimeout(self._connect_timeout)\n                    sock.connect((host, int(port)))\n                    sock.settimeout(self._io_timeout)\n                    yield server, sock.makefile(), sock\n                except Exception, e:\n                    self._exception_occurred(server, e, 'connecting')\n\n    def _return_conn(self, server, fp, sock):\n        \"\"\" Returns a server connection to the pool \"\"\"\n        self._client_cache[server].append((fp, sock))\n\n    def set(self, key, value, serialize=True, timeout=0):\n        \"\"\"\n        Set a key/value pair in memcache\n\n        :param key: key\n        :param value: value\n        :param serialize: if True, value is pickled before sending to memcache\n        :param timeout: ttl in memcache\n        \"\"\"\n        key = md5hash(key)\n        if timeout > 0:\n            timeout += time.time()\n        flags = 0\n        if serialize:\n            value = pickle.dumps(value, PICKLE_PROTOCOL)\n            flags |= PICKLE_FLAG\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('set %s %d %d %s noreply\\r\\n%s\\r\\n' % \\\n                              (key, flags, timeout, len(value), value))\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def get(self, key):\n        \"\"\"\n        Gets the object specified by key.  It will also unpickle the object\n        before returning if it is pickled in memcache.\n\n        :param key: key\n        :returns: value of the key in memcache\n        \"\"\"\n        key = md5hash(key)\n        value = None\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('get %s\\r\\n' % key)\n                line = fp.readline().strip().split()\n                while line[0].upper() != 'END':\n                    if line[0].upper() == 'VALUE' and line[1] == key:\n                        size = int(line[3])\n                        value = fp.read(size)\n                        if int(line[2]) & PICKLE_FLAG:\n                            value = pickle.loads(value)\n                        fp.readline()\n                    line = fp.readline().strip().split()\n                self._return_conn(server, fp, sock)\n                return value\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def incr(self, key, delta=1, timeout=0):\n        \"\"\"\n        Increments a key which has a numeric value by delta.\n        If the key can't be found, it's added as delta or 0 if delta < 0.\n        If passed a negative number, will use memcached's decr. Returns\n        the int stored in memcached\n        Note: The data memcached stores as the result of incr/decr is\n        an unsigned int.  decr's that result in a number below 0 are\n        stored as 0.\n\n        :param key: key\n        :param delta: amount to add to the value of key (or set as the value\n                      if the key is not found) will be cast to an int\n        :param timeout: ttl in memcache\n        :raises MemcacheConnectionError:\n        \"\"\"\n        key = md5hash(key)\n        command = 'incr'\n        if delta < 0:\n            command = 'decr'\n        delta = str(abs(int(delta)))\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('%s %s %s\\r\\n' % (command, key, delta))\n                line = fp.readline().strip().split()\n                if line[0].upper() == 'NOT_FOUND':\n                    add_val = delta\n                    if command == 'decr':\n                        add_val = '0'\n                    sock.sendall('add %s %d %d %s\\r\\n%s\\r\\n' % \\\n                                  (key, 0, timeout, len(add_val), add_val))\n                    line = fp.readline().strip().split()\n                    if line[0].upper() == 'NOT_STORED':\n                        sock.sendall('%s %s %s\\r\\n' % (command, key, delta))\n                        line = fp.readline().strip().split()\n                        ret = int(line[0].strip())\n                    else:\n                        ret = int(add_val)\n                else:\n                    ret = int(line[0].strip())\n                self._return_conn(server, fp, sock)\n                return ret\n            except Exception, e:\n                self._exception_occurred(server, e)\n        raise MemcacheConnectionError(\"No Memcached connections succeeded.\")\n\n    def decr(self, key, delta=1, timeout=0):\n        \"\"\"\n        Decrements a key which has a numeric value by delta. Calls incr with\n        -delta.\n\n        :param key: key\n        :param delta: amount to subtract to the value of key (or set the\n                      value to 0 if the key is not found) will be cast to\n                      an int\n        :param timeout: ttl in memcache\n        :raises MemcacheConnectionError:\n        \"\"\"\n        self.incr(key, delta=-delta, timeout=timeout)\n\n    def delete(self, key):\n        \"\"\"\n        Deletes a key/value pair from memcache.\n\n        :param key: key to be deleted\n        \"\"\"\n        key = md5hash(key)\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('delete %s noreply\\r\\n' % key)\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def set_multi(self, mapping, server_key, serialize=True, timeout=0):\n        \"\"\"\n        Sets multiple key/value pairs in memcache.\n\n        :param mapping: dictonary of keys and values to be set in memcache\n        :param servery_key: key to use in determining which server in the ring\n                            is used\n        :param serialize: if True, value is pickled before sending to memcache\n        :param timeout: ttl for memcache\n        \"\"\"\n        server_key = md5hash(server_key)\n        if timeout > 0:\n            timeout += time.time()\n        msg = ''\n        for key, value in mapping.iteritems():\n            key = md5hash(key)\n            flags = 0\n            if serialize:\n                value = pickle.dumps(value, PICKLE_PROTOCOL)\n                flags |= PICKLE_FLAG\n            msg += ('set %s %d %d %s noreply\\r\\n%s\\r\\n' %\n                    (key, flags, timeout, len(value), value))\n        for (server, fp, sock) in self._get_conns(server_key):\n            try:\n                sock.sendall(msg)\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def get_multi(self, keys, server_key):\n        \"\"\"\n        Gets multiple values from memcache for the given keys.\n\n        :param keys: keys for values to be retrieved from memcache\n        :param servery_key: key to use in determining which server in the ring\n                            is used\n        :returns: list of values\n        \"\"\"\n        server_key = md5hash(server_key)\n        keys = [md5hash(key) for key in keys]\n        for (server, fp, sock) in self._get_conns(server_key):\n            try:\n                sock.sendall('get %s\\r\\n' % ' '.join(keys))\n                line = fp.readline().strip().split()\n                responses = {}\n                while line[0].upper() != 'END':\n                    if line[0].upper() == 'VALUE':\n                        size = int(line[3])\n                        value = fp.read(size)\n                        if int(line[2]) & PICKLE_FLAG:\n                            value = pickle.loads(value)\n                        responses[line[1]] = value\n                        fp.readline()\n                    line = fp.readline().strip().split()\n                values = []\n                for key in keys:\n                    if key in responses:\n                        values.append(responses[key])\n                    else:\n                        values.append(None)\n                self._return_conn(server, fp, sock)\n                return values\n            except Exception, e:\n                self._exception_occurred(server, e)\n", "patch": "@@ -27,11 +27,17 @@\n from bisect import bisect\n from hashlib import md5\n \n+try:\n+    import simplejson as json\n+except ImportError:\n+    import json\n+\n DEFAULT_MEMCACHED_PORT = 11211\n \n CONN_TIMEOUT = 0.3\n IO_TIMEOUT = 2.0\n PICKLE_FLAG = 1\n+JSON_FLAG = 2\n NODE_WEIGHT = 50\n PICKLE_PROTOCOL = 2\n TRY_COUNT = 3\n@@ -57,7 +63,8 @@ class MemcacheRing(object):\n     \"\"\"\n \n     def __init__(self, servers, connect_timeout=CONN_TIMEOUT,\n-                 io_timeout=IO_TIMEOUT, tries=TRY_COUNT):\n+                 io_timeout=IO_TIMEOUT, tries=TRY_COUNT,\n+                 allow_pickle=False, allow_unpickle=False):\n         self._ring = {}\n         self._errors = dict(((serv, []) for serv in servers))\n         self._error_limited = dict(((serv, 0) for serv in servers))\n@@ -69,6 +76,8 @@ def __init__(self, servers, connect_timeout=CONN_TIMEOUT,\n         self._client_cache = dict(((server, []) for server in servers))\n         self._connect_timeout = connect_timeout\n         self._io_timeout = io_timeout\n+        self._allow_pickle = allow_pickle\n+        self._allow_unpickle = allow_unpickle or allow_pickle\n \n     def _exception_occurred(self, server, e, action='talking'):\n         if isinstance(e, socket.timeout):\n@@ -130,16 +139,21 @@ def set(self, key, value, serialize=True, timeout=0):\n \n         :param key: key\n         :param value: value\n-        :param serialize: if True, value is pickled before sending to memcache\n+        :param serialize: if True, value is serialized with JSON before sending\n+                          to memcache, or with pickle if configured to use\n+                          pickle instead of JSON (to avoid cache poisoning)\n         :param timeout: ttl in memcache\n         \"\"\"\n         key = md5hash(key)\n         if timeout > 0:\n             timeout += time.time()\n         flags = 0\n-        if serialize:\n+        if serialize and self._allow_pickle:\n             value = pickle.dumps(value, PICKLE_PROTOCOL)\n             flags |= PICKLE_FLAG\n+        elif serialize:\n+            value = json.dumps(value)\n+            flags |= JSON_FLAG\n         for (server, fp, sock) in self._get_conns(key):\n             try:\n                 sock.sendall('set %s %d %d %s noreply\\r\\n%s\\r\\n' % \\\n@@ -151,8 +165,9 @@ def set(self, key, value, serialize=True, timeout=0):\n \n     def get(self, key):\n         \"\"\"\n-        Gets the object specified by key.  It will also unpickle the object\n-        before returning if it is pickled in memcache.\n+        Gets the object specified by key.  It will also unserialize the object\n+        before returning if it is serialized in memcache with JSON, or if it\n+        is pickled and unpickling is allowed.\n \n         :param key: key\n         :returns: value of the key in memcache\n@@ -168,7 +183,12 @@ def get(self, key):\n                         size = int(line[3])\n                         value = fp.read(size)\n                         if int(line[2]) & PICKLE_FLAG:\n-                            value = pickle.loads(value)\n+                            if self._allow_unpickle:\n+                                value = pickle.loads(value)\n+                            else:\n+                                value = None\n+                        elif int(line[2]) & JSON_FLAG:\n+                            value = json.loads(value)\n                         fp.readline()\n                     line = fp.readline().strip().split()\n                 self._return_conn(server, fp, sock)\n@@ -258,7 +278,9 @@ def set_multi(self, mapping, server_key, serialize=True, timeout=0):\n         :param mapping: dictonary of keys and values to be set in memcache\n         :param servery_key: key to use in determining which server in the ring\n                             is used\n-        :param serialize: if True, value is pickled before sending to memcache\n+        :param serialize: if True, value is serialized with JSON before sending\n+                          to memcache, or with pickle if configured to use\n+                          pickle instead of JSON (to avoid cache poisoning)\n         :param timeout: ttl for memcache\n         \"\"\"\n         server_key = md5hash(server_key)\n@@ -268,9 +290,12 @@ def set_multi(self, mapping, server_key, serialize=True, timeout=0):\n         for key, value in mapping.iteritems():\n             key = md5hash(key)\n             flags = 0\n-            if serialize:\n+            if serialize and self._allow_pickle:\n                 value = pickle.dumps(value, PICKLE_PROTOCOL)\n                 flags |= PICKLE_FLAG\n+            elif serialize:\n+                value = json.dumps(value)\n+                flags |= JSON_FLAG\n             msg += ('set %s %d %d %s noreply\\r\\n%s\\r\\n' %\n                     (key, flags, timeout, len(value), value))\n         for (server, fp, sock) in self._get_conns(server_key):\n@@ -302,7 +327,12 @@ def get_multi(self, keys, server_key):\n                         size = int(line[3])\n                         value = fp.read(size)\n                         if int(line[2]) & PICKLE_FLAG:\n-                            value = pickle.loads(value)\n+                            if self._allow_unpickle:\n+                                value = pickle.loads(value)\n+                            else:\n+                                value = None\n+                        elif int(line[2]) & JSON_FLAG:\n+                            value = json.loads(value)\n                         responses[line[1]] = value\n                         fp.readline()\n                     line = fp.readline().strip().split()", "file_path": "files/2012_10/12", "file_language": "py", "file_name": "swift/common/memcached.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 1, "static": {"rats": [false, []], "semgrep": [true, ["       python.lang.security.deserialization.pickle.avoid-pickle                                       \n          Avoid using `pickle`, which is known to lead to code execution vulnerabilities. When        \n          unpickling, the serialized data could be manipulated to run arbitrary code. Instead,        \n          consider serializing the relevant data as JSON or a similar text-based serialization format.\n          Details: https://sg.run/OPwB                                                                \n          305\u2506 value = pickle.loads(value)", "       python.lang.security.deserialization.pickle.avoid-cPickle                                      \n          Avoid using `cPickle`, which is known to lead to code execution vulnerabilities. When       \n          unpickling, the serialized data could be manipulated to run arbitrary code. Instead,        \n          consider serializing the relevant data as JSON or a similar text-based serialization format.\n          Details: https://sg.run/eLxb                                                                \n\n          305\u2506 value = pickle.loads(value)", "       python.lang.security.deserialization.pickle.avoid-pickle                                       \n          Avoid using `pickle`, which is known to lead to code execution vulnerabilities. When        \n          unpickling, the serialized data could be manipulated to run arbitrary code. Instead,        \n          consider serializing the relevant data as JSON or a similar text-based serialization format.\n          Details: https://sg.run/OPwB                                                                \n\n          272\u2506 value = pickle.dumps(value, PICKLE_PROTOCOL)", "       python.lang.security.deserialization.pickle.avoid-cPickle                                      \n          Avoid using `cPickle`, which is known to lead to code execution vulnerabilities. When       \n          unpickling, the serialized data could be manipulated to run arbitrary code. Instead,        \n          consider serializing the relevant data as JSON or a similar text-based serialization format.\n          Details: https://sg.run/eLxb                                                                \n\n          272\u2506 value = pickle.dumps(value, PICKLE_PROTOCOL)", "       python.lang.security.deserialization.pickle.avoid-pickle                                       \n          Avoid using `pickle`, which is known to lead to code execution vulnerabilities. When        \n          unpickling, the serialized data could be manipulated to run arbitrary code. Instead,        \n          consider serializing the relevant data as JSON or a similar text-based serialization format.\n          Details: https://sg.run/OPwB                                                                \n\n          171\u2506 value = pickle.loads(value)", "       python.lang.security.deserialization.pickle.avoid-cPickle                                      \n          Avoid using `cPickle`, which is known to lead to code execution vulnerabilities. When       \n          unpickling, the serialized data could be manipulated to run arbitrary code. Instead,        \n          consider serializing the relevant data as JSON or a similar text-based serialization format.\n          Details: https://sg.run/eLxb                                                                \n\n          171\u2506 value = pickle.loads(value)", "       python.lang.security.deserialization.pickle.avoid-pickle                                       \n          Avoid using `pickle`, which is known to lead to code execution vulnerabilities. When        \n          unpickling, the serialized data could be manipulated to run arbitrary code. Instead,        \n          consider serializing the relevant data as JSON or a similar text-based serialization format.\n          Details: https://sg.run/OPwB                                                                \n\n          141\u2506 value = pickle.dumps(value, PICKLE_PROTOCOL)", "       python.lang.security.deserialization.pickle.avoid-cPickle                                      \n          Avoid using `cPickle`, which is known to lead to code execution vulnerabilities. When       \n          unpickling, the serialized data could be manipulated to run arbitrary code. Instead,        \n          consider serializing the relevant data as JSON or a similar text-based serialization format.\n          Details: https://sg.run/eLxb                                                                \n\n          141\u2506 value = pickle.dumps(value, PICKLE_PROTOCOL)"]]}, "target": 1, "function_before": [{"function": "def md5hash(key):\n    return md5(key).hexdigest()", "target": 0}, {"function": "class MemcacheConnectionError(Exception):\n    pass", "target": 0}, {"function": "class MemcacheRing(object):\n    \"\"\"\n    Simple, consistent-hashed memcache client.\n    \"\"\"\n\n    def __init__(self, servers, connect_timeout=CONN_TIMEOUT,\n                 io_timeout=IO_TIMEOUT, tries=TRY_COUNT):\n        self._ring = {}\n        self._errors = dict(((serv, []) for serv in servers))\n        self._error_limited = dict(((serv, 0) for serv in servers))\n        for server in sorted(servers):\n            for i in xrange(NODE_WEIGHT):\n                self._ring[md5hash('%s-%s' % (server, i))] = server\n        self._tries = tries if tries <= len(servers) else len(servers)\n        self._sorted = sorted(self._ring.keys())\n        self._client_cache = dict(((server, []) for server in servers))\n        self._connect_timeout = connect_timeout\n        self._io_timeout = io_timeout\n\n    def _exception_occurred(self, server, e, action='talking'):\n        if isinstance(e, socket.timeout):\n            logging.error(_(\"Timeout %(action)s to memcached: %(server)s\"),\n                {'action': action, 'server': server})\n        else:\n            logging.exception(_(\"Error %(action)s to memcached: %(server)s\"),\n                {'action': action, 'server': server})\n        now = time.time()\n        self._errors[server].append(time.time())\n        if len(self._errors[server]) > ERROR_LIMIT_COUNT:\n            self._errors[server] = [err for err in self._errors[server]\n                                          if err > now - ERROR_LIMIT_TIME]\n            if len(self._errors[server]) > ERROR_LIMIT_COUNT:\n                self._error_limited[server] = now + ERROR_LIMIT_DURATION\n                logging.error(_('Error limiting server %s'), server)\n\n    def _get_conns(self, key):\n        \"\"\"\n        Retrieves a server conn from the pool, or connects a new one.\n        Chooses the server based on a consistent hash of \"key\".\n        \"\"\"\n        pos = bisect(self._sorted, key)\n        served = []\n        while len(served) < self._tries:\n            pos = (pos + 1) % len(self._sorted)\n            server = self._ring[self._sorted[pos]]\n            if server in served:\n                continue\n            served.append(server)\n            if self._error_limited[server] > time.time():\n                continue\n            try:\n                fp, sock = self._client_cache[server].pop()\n                yield server, fp, sock\n            except IndexError:\n                try:\n                    if ':' in server:\n                        host, port = server.split(':')\n                    else:\n                        host = server\n                        port = DEFAULT_MEMCACHED_PORT\n                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                    sock.settimeout(self._connect_timeout)\n                    sock.connect((host, int(port)))\n                    sock.settimeout(self._io_timeout)\n                    yield server, sock.makefile(), sock\n                except Exception, e:\n                    self._exception_occurred(server, e, 'connecting')\n\n    def _return_conn(self, server, fp, sock):\n        \"\"\" Returns a server connection to the pool \"\"\"\n        self._client_cache[server].append((fp, sock))\n\n    def set(self, key, value, serialize=True, timeout=0):\n        \"\"\"\n        Set a key/value pair in memcache\n\n        :param key: key\n        :param value: value\n        :param serialize: if True, value is pickled before sending to memcache\n        :param timeout: ttl in memcache\n        \"\"\"\n        key = md5hash(key)\n        if timeout > 0:\n            timeout += time.time()\n        flags = 0\n        if serialize:\n            value = pickle.dumps(value, PICKLE_PROTOCOL)\n            flags |= PICKLE_FLAG\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('set %s %d %d %s noreply\\r\\n%s\\r\\n' % \\\n                              (key, flags, timeout, len(value), value))\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def get(self, key):\n        \"\"\"\n        Gets the object specified by key.  It will also unpickle the object\n        before returning if it is pickled in memcache.\n\n        :param key: key\n        :returns: value of the key in memcache\n        \"\"\"\n        key = md5hash(key)\n        value = None\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('get %s\\r\\n' % key)\n                line = fp.readline().strip().split()\n                while line[0].upper() != 'END':\n                    if line[0].upper() == 'VALUE' and line[1] == key:\n                        size = int(line[3])\n                        value = fp.read(size)\n                        if int(line[2]) & PICKLE_FLAG:\n                            value = pickle.loads(value)\n                        fp.readline()\n                    line = fp.readline().strip().split()\n                self._return_conn(server, fp, sock)\n                return value\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def incr(self, key, delta=1, timeout=0):\n        \"\"\"\n        Increments a key which has a numeric value by delta.\n        If the key can't be found, it's added as delta or 0 if delta < 0.\n        If passed a negative number, will use memcached's decr. Returns\n        the int stored in memcached\n        Note: The data memcached stores as the result of incr/decr is\n        an unsigned int.  decr's that result in a number below 0 are\n        stored as 0.\n\n        :param key: key\n        :param delta: amount to add to the value of key (or set as the value\n                      if the key is not found) will be cast to an int\n        :param timeout: ttl in memcache\n        :raises MemcacheConnectionError:\n        \"\"\"\n        key = md5hash(key)\n        command = 'incr'\n        if delta < 0:\n            command = 'decr'\n        delta = str(abs(int(delta)))\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('%s %s %s\\r\\n' % (command, key, delta))\n                line = fp.readline().strip().split()\n                if line[0].upper() == 'NOT_FOUND':\n                    add_val = delta\n                    if command == 'decr':\n                        add_val = '0'\n                    sock.sendall('add %s %d %d %s\\r\\n%s\\r\\n' % \\\n                                  (key, 0, timeout, len(add_val), add_val))\n                    line = fp.readline().strip().split()\n                    if line[0].upper() == 'NOT_STORED':\n                        sock.sendall('%s %s %s\\r\\n' % (command, key, delta))\n                        line = fp.readline().strip().split()\n                        ret = int(line[0].strip())\n                    else:\n                        ret = int(add_val)\n                else:\n                    ret = int(line[0].strip())\n                self._return_conn(server, fp, sock)\n                return ret\n            except Exception, e:\n                self._exception_occurred(server, e)\n        raise MemcacheConnectionError(\"No Memcached connections succeeded.\")\n\n    def decr(self, key, delta=1, timeout=0):\n        \"\"\"\n        Decrements a key which has a numeric value by delta. Calls incr with\n        -delta.\n\n        :param key: key\n        :param delta: amount to subtract to the value of key (or set the\n                      value to 0 if the key is not found) will be cast to\n                      an int\n        :param timeout: ttl in memcache\n        :raises MemcacheConnectionError:\n        \"\"\"\n        self.incr(key, delta=-delta, timeout=timeout)\n\n    def delete(self, key):\n        \"\"\"\n        Deletes a key/value pair from memcache.\n\n        :param key: key to be deleted\n        \"\"\"\n        key = md5hash(key)\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('delete %s noreply\\r\\n' % key)\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def set_multi(self, mapping, server_key, serialize=True, timeout=0):\n        \"\"\"\n        Sets multiple key/value pairs in memcache.\n\n        :param mapping: dictonary of keys and values to be set in memcache\n        :param servery_key: key to use in determining which server in the ring\n                            is used\n        :param serialize: if True, value is pickled before sending to memcache\n        :param timeout: ttl for memcache\n        \"\"\"\n        server_key = md5hash(server_key)\n        if timeout > 0:\n            timeout += time.time()\n        msg = ''\n        for key, value in mapping.iteritems():\n            key = md5hash(key)\n            flags = 0\n            if serialize:\n                value = pickle.dumps(value, PICKLE_PROTOCOL)\n                flags |= PICKLE_FLAG\n            msg += ('set %s %d %d %s noreply\\r\\n%s\\r\\n' %\n                    (key, flags, timeout, len(value), value))\n        for (server, fp, sock) in self._get_conns(server_key):\n            try:\n                sock.sendall(msg)\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def get_multi(self, keys, server_key):\n        \"\"\"\n        Gets multiple values from memcache for the given keys.\n\n        :param keys: keys for values to be retrieved from memcache\n        :param servery_key: key to use in determining which server in the ring\n                            is used\n        :returns: list of values\n        \"\"\"\n        server_key = md5hash(server_key)\n        keys = [md5hash(key) for key in keys]\n        for (server, fp, sock) in self._get_conns(server_key):\n            try:\n                sock.sendall('get %s\\r\\n' % ' '.join(keys))\n                line = fp.readline().strip().split()\n                responses = {}\n                while line[0].upper() != 'END':\n                    if line[0].upper() == 'VALUE':\n                        size = int(line[3])\n                        value = fp.read(size)\n                        if int(line[2]) & PICKLE_FLAG:\n                            value = pickle.loads(value)\n                        responses[line[1]] = value\n                        fp.readline()\n                    line = fp.readline().strip().split()\n                values = []\n                for key in keys:\n                    if key in responses:\n                        values.append(responses[key])\n                    else:\n                        values.append(None)\n                self._return_conn(server, fp, sock)\n                return values\n            except Exception, e:\n                self._exception_occurred(server, e)", "target": 1, "line": "@@  -57,7 +63,8  @@ class MemcacheRing(object):\n     \"\"\"\n \n     def __init__(self, servers, connect_timeout=CONN_TIMEOUT,\n-                 io_timeout=IO_TIMEOUT, tries=TRY_COUNT):\n+                 io_timeout=IO_TIMEOUT, tries=TRY_COUNT,\n+                 allow_pickle=False, allow_unpickle=False):\n         self._ring = {}\n         self._errors = dict(((serv, []) for serv in servers))\n         self._error_limited = dict(((serv, 0) for serv in servers))\n@@  -69,6 +76,8  @@ def __init__(self, servers, connect_timeout=CONN_TIMEOUT,\n         self._client_cache = dict(((server, []) for server in servers))\n         self._connect_timeout = connect_timeout\n         self._io_timeout = io_timeout\n+        self._allow_pickle = allow_pickle\n+        self._allow_unpickle = allow_unpickle or allow_pickle\n \n     def _exception_occurred(self, server, e, action='talking'):\n         if isinstance(e, socket.timeout):\n@@  -130,16 +139,21  @@ def set(self, key, value, serialize=True, timeout=0):\n \n         :param key: key\n         :param value: value\n-        :param serialize: if True, value is pickled before sending to memcache\n+        :param serialize: if True, value is serialized with JSON before sending\n+                          to memcache, or with pickle if configured to use\n+                          pickle instead of JSON (to avoid cache poisoning)\n         :param timeout: ttl in memcache\n         \"\"\"\n         key = md5hash(key)\n         if timeout > 0:\n             timeout += time.time()\n         flags = 0\n-        if serialize:\n+        if serialize and self._allow_pickle:\n             value = pickle.dumps(value, PICKLE_PROTOCOL)\n             flags |= PICKLE_FLAG\n+        elif serialize:\n+            value = json.dumps(value)\n+            flags |= JSON_FLAG\n         for (server, fp, sock) in self._get_conns(key):\n             try:\n                 sock.sendall('set %s %d %d %s noreply\\r\\n%s\\r\\n' % \\\n@@  -151,8 +165,9  @@ def set(self, key, value, serialize=True, timeout=0):\n \n     def get(self, key):\n         \"\"\"\n-        Gets the object specified by key.  It will also unpickle the object\n-        before returning if it is pickled in memcache.\n+        Gets the object specified by key.  It will also unserialize the object\n+        before returning if it is serialized in memcache with JSON, or if it\n+        is pickled and unpickling is allowed.\n \n         :param key: key\n         :returns: value of the key in memcache\n@@  -168,7 +183,12  @@ def get(self, key):\n                         size = int(line[3])\n                         value = fp.read(size)\n                         if int(line[2]) & PICKLE_FLAG:\n-                            value = pickle.loads(value)\n+                            if self._allow_unpickle:\n+                                value = pickle.loads(value)\n+                            else:\n+                                value = None\n+                        elif int(line[2]) & JSON_FLAG:\n+                            value = json.loads(value)\n                         fp.readline()\n                     line = fp.readline().strip().split()\n                 self._return_conn(server, fp, sock)\n@@  -258,7 +278,9  @@ def set_multi(self, mapping, server_key, serialize=True, timeout=0):\n         :param mapping: dictonary of keys and values to be set in memcache\n         :param servery_key: key to use in determining which server in the ring\n                             is used\n-        :param serialize: if True, value is pickled before sending to memcache\n+        :param serialize: if True, value is serialized with JSON before sending\n+                          to memcache, or with pickle if configured to use\n+                          pickle instead of JSON (to avoid cache poisoning)\n         :param timeout: ttl for memcache\n         \"\"\"\n         server_key = md5hash(server_key)\n@@  -268,9 +290,12  @@ def set_multi(self, mapping, server_key, serialize=True, timeout=0):\n         for key, value in mapping.iteritems():\n             key = md5hash(key)\n             flags = 0\n-            if serialize:\n+            if serialize and self._allow_pickle:\n                 value = pickle.dumps(value, PICKLE_PROTOCOL)\n                 flags |= PICKLE_FLAG\n+            elif serialize:\n+                value = json.dumps(value)\n+                flags |= JSON_FLAG\n             msg += ('set %s %d %d %s noreply\\r\\n%s\\r\\n' %\n                     (key, flags, timeout, len(value), value))\n         for (server, fp, sock) in self._get_conns(server_key):\n@@  -302,7 +327,12  @@ def get_multi(self, keys, server_key):\n                         size = int(line[3])\n                         value = fp.read(size)\n                         if int(line[2]) & PICKLE_FLAG:\n-                            value = pickle.loads(value)\n+                            if self._allow_unpickle:\n+                                value = pickle.loads(value)\n+                            else:\n+                                value = None\n+                        elif int(line[2]) & JSON_FLAG:\n+                            value = json.loads(value)\n                         responses[line[1]] = value\n                         fp.readline()\n                     line = fp.readline().strip().split()"}], "function_after": [{"function": "def md5hash(key):\n    return md5(key).hexdigest()", "target": 0}, {"function": "class MemcacheConnectionError(Exception):\n    pass", "target": 0}, {"function": "class MemcacheRing(object):\n    \"\"\"\n    Simple, consistent-hashed memcache client.\n    \"\"\"\n\n    def __init__(self, servers, connect_timeout=CONN_TIMEOUT,\n                 io_timeout=IO_TIMEOUT, tries=TRY_COUNT,\n                 allow_pickle=False, allow_unpickle=False):\n        self._ring = {}\n        self._errors = dict(((serv, []) for serv in servers))\n        self._error_limited = dict(((serv, 0) for serv in servers))\n        for server in sorted(servers):\n            for i in xrange(NODE_WEIGHT):\n                self._ring[md5hash('%s-%s' % (server, i))] = server\n        self._tries = tries if tries <= len(servers) else len(servers)\n        self._sorted = sorted(self._ring.keys())\n        self._client_cache = dict(((server, []) for server in servers))\n        self._connect_timeout = connect_timeout\n        self._io_timeout = io_timeout\n        self._allow_pickle = allow_pickle\n        self._allow_unpickle = allow_unpickle or allow_pickle\n\n    def _exception_occurred(self, server, e, action='talking'):\n        if isinstance(e, socket.timeout):\n            logging.error(_(\"Timeout %(action)s to memcached: %(server)s\"),\n                {'action': action, 'server': server})\n        else:\n            logging.exception(_(\"Error %(action)s to memcached: %(server)s\"),\n                {'action': action, 'server': server})\n        now = time.time()\n        self._errors[server].append(time.time())\n        if len(self._errors[server]) > ERROR_LIMIT_COUNT:\n            self._errors[server] = [err for err in self._errors[server]\n                                          if err > now - ERROR_LIMIT_TIME]\n            if len(self._errors[server]) > ERROR_LIMIT_COUNT:\n                self._error_limited[server] = now + ERROR_LIMIT_DURATION\n                logging.error(_('Error limiting server %s'), server)\n\n    def _get_conns(self, key):\n        \"\"\"\n        Retrieves a server conn from the pool, or connects a new one.\n        Chooses the server based on a consistent hash of \"key\".\n        \"\"\"\n        pos = bisect(self._sorted, key)\n        served = []\n        while len(served) < self._tries:\n            pos = (pos + 1) % len(self._sorted)\n            server = self._ring[self._sorted[pos]]\n            if server in served:\n                continue\n            served.append(server)\n            if self._error_limited[server] > time.time():\n                continue\n            try:\n                fp, sock = self._client_cache[server].pop()\n                yield server, fp, sock\n            except IndexError:\n                try:\n                    if ':' in server:\n                        host, port = server.split(':')\n                    else:\n                        host = server\n                        port = DEFAULT_MEMCACHED_PORT\n                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                    sock.settimeout(self._connect_timeout)\n                    sock.connect((host, int(port)))\n                    sock.settimeout(self._io_timeout)\n                    yield server, sock.makefile(), sock\n                except Exception, e:\n                    self._exception_occurred(server, e, 'connecting')\n\n    def _return_conn(self, server, fp, sock):\n        \"\"\" Returns a server connection to the pool \"\"\"\n        self._client_cache[server].append((fp, sock))\n\n    def set(self, key, value, serialize=True, timeout=0):\n        \"\"\"\n        Set a key/value pair in memcache\n\n        :param key: key\n        :param value: value\n        :param serialize: if True, value is serialized with JSON before sending\n                          to memcache, or with pickle if configured to use\n                          pickle instead of JSON (to avoid cache poisoning)\n        :param timeout: ttl in memcache\n        \"\"\"\n        key = md5hash(key)\n        if timeout > 0:\n            timeout += time.time()\n        flags = 0\n        if serialize and self._allow_pickle:\n            value = pickle.dumps(value, PICKLE_PROTOCOL)\n            flags |= PICKLE_FLAG\n        elif serialize:\n            value = json.dumps(value)\n            flags |= JSON_FLAG\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('set %s %d %d %s noreply\\r\\n%s\\r\\n' % \\\n                              (key, flags, timeout, len(value), value))\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def get(self, key):\n        \"\"\"\n        Gets the object specified by key.  It will also unserialize the object\n        before returning if it is serialized in memcache with JSON, or if it\n        is pickled and unpickling is allowed.\n\n        :param key: key\n        :returns: value of the key in memcache\n        \"\"\"\n        key = md5hash(key)\n        value = None\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('get %s\\r\\n' % key)\n                line = fp.readline().strip().split()\n                while line[0].upper() != 'END':\n                    if line[0].upper() == 'VALUE' and line[1] == key:\n                        size = int(line[3])\n                        value = fp.read(size)\n                        if int(line[2]) & PICKLE_FLAG:\n                            if self._allow_unpickle:\n                                value = pickle.loads(value)\n                            else:\n                                value = None\n                        elif int(line[2]) & JSON_FLAG:\n                            value = json.loads(value)\n                        fp.readline()\n                    line = fp.readline().strip().split()\n                self._return_conn(server, fp, sock)\n                return value\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def incr(self, key, delta=1, timeout=0):\n        \"\"\"\n        Increments a key which has a numeric value by delta.\n        If the key can't be found, it's added as delta or 0 if delta < 0.\n        If passed a negative number, will use memcached's decr. Returns\n        the int stored in memcached\n        Note: The data memcached stores as the result of incr/decr is\n        an unsigned int.  decr's that result in a number below 0 are\n        stored as 0.\n\n        :param key: key\n        :param delta: amount to add to the value of key (or set as the value\n                      if the key is not found) will be cast to an int\n        :param timeout: ttl in memcache\n        :raises MemcacheConnectionError:\n        \"\"\"\n        key = md5hash(key)\n        command = 'incr'\n        if delta < 0:\n            command = 'decr'\n        delta = str(abs(int(delta)))\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('%s %s %s\\r\\n' % (command, key, delta))\n                line = fp.readline().strip().split()\n                if line[0].upper() == 'NOT_FOUND':\n                    add_val = delta\n                    if command == 'decr':\n                        add_val = '0'\n                    sock.sendall('add %s %d %d %s\\r\\n%s\\r\\n' % \\\n                                  (key, 0, timeout, len(add_val), add_val))\n                    line = fp.readline().strip().split()\n                    if line[0].upper() == 'NOT_STORED':\n                        sock.sendall('%s %s %s\\r\\n' % (command, key, delta))\n                        line = fp.readline().strip().split()\n                        ret = int(line[0].strip())\n                    else:\n                        ret = int(add_val)\n                else:\n                    ret = int(line[0].strip())\n                self._return_conn(server, fp, sock)\n                return ret\n            except Exception, e:\n                self._exception_occurred(server, e)\n        raise MemcacheConnectionError(\"No Memcached connections succeeded.\")\n\n    def decr(self, key, delta=1, timeout=0):\n        \"\"\"\n        Decrements a key which has a numeric value by delta. Calls incr with\n        -delta.\n\n        :param key: key\n        :param delta: amount to subtract to the value of key (or set the\n                      value to 0 if the key is not found) will be cast to\n                      an int\n        :param timeout: ttl in memcache\n        :raises MemcacheConnectionError:\n        \"\"\"\n        self.incr(key, delta=-delta, timeout=timeout)\n\n    def delete(self, key):\n        \"\"\"\n        Deletes a key/value pair from memcache.\n\n        :param key: key to be deleted\n        \"\"\"\n        key = md5hash(key)\n        for (server, fp, sock) in self._get_conns(key):\n            try:\n                sock.sendall('delete %s noreply\\r\\n' % key)\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def set_multi(self, mapping, server_key, serialize=True, timeout=0):\n        \"\"\"\n        Sets multiple key/value pairs in memcache.\n\n        :param mapping: dictonary of keys and values to be set in memcache\n        :param servery_key: key to use in determining which server in the ring\n                            is used\n        :param serialize: if True, value is serialized with JSON before sending\n                          to memcache, or with pickle if configured to use\n                          pickle instead of JSON (to avoid cache poisoning)\n        :param timeout: ttl for memcache\n        \"\"\"\n        server_key = md5hash(server_key)\n        if timeout > 0:\n            timeout += time.time()\n        msg = ''\n        for key, value in mapping.iteritems():\n            key = md5hash(key)\n            flags = 0\n            if serialize and self._allow_pickle:\n                value = pickle.dumps(value, PICKLE_PROTOCOL)\n                flags |= PICKLE_FLAG\n            elif serialize:\n                value = json.dumps(value)\n                flags |= JSON_FLAG\n            msg += ('set %s %d %d %s noreply\\r\\n%s\\r\\n' %\n                    (key, flags, timeout, len(value), value))\n        for (server, fp, sock) in self._get_conns(server_key):\n            try:\n                sock.sendall(msg)\n                self._return_conn(server, fp, sock)\n                return\n            except Exception, e:\n                self._exception_occurred(server, e)\n\n    def get_multi(self, keys, server_key):\n        \"\"\"\n        Gets multiple values from memcache for the given keys.\n\n        :param keys: keys for values to be retrieved from memcache\n        :param servery_key: key to use in determining which server in the ring\n                            is used\n        :returns: list of values\n        \"\"\"\n        server_key = md5hash(server_key)\n        keys = [md5hash(key) for key in keys]\n        for (server, fp, sock) in self._get_conns(server_key):\n            try:\n                sock.sendall('get %s\\r\\n' % ' '.join(keys))\n                line = fp.readline().strip().split()\n                responses = {}\n                while line[0].upper() != 'END':\n                    if line[0].upper() == 'VALUE':\n                        size = int(line[3])\n                        value = fp.read(size)\n                        if int(line[2]) & PICKLE_FLAG:\n                            if self._allow_unpickle:\n                                value = pickle.loads(value)\n                            else:\n                                value = None\n                        elif int(line[2]) & JSON_FLAG:\n                            value = json.loads(value)\n                        responses[line[1]] = value\n                        fp.readline()\n                    line = fp.readline().strip().split()\n                values = []\n                for key in keys:\n                    if key in responses:\n                        values.append(responses[key])\n                    else:\n                        values.append(None)\n                self._return_conn(server, fp, sock)\n                return values\n            except Exception, e:\n                self._exception_occurred(server, e)", "target": 0}]}, {"raw_url": "https://github.com/openstack/swift/raw/e1ff51c04554d51616d2845f92ab726cb0e5831a/swift%2Fcommon%2Fmiddleware%2Fmemcache.py", "code": "# Copyright (c) 2010-2012 OpenStack, LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom ConfigParser import ConfigParser, NoSectionError, NoOptionError\n\nfrom swift.common.memcached import MemcacheRing\n\n\nclass MemcacheMiddleware(object):\n    \"\"\"\n    Caching middleware that manages caching in swift.\n    \"\"\"\n\n    def __init__(self, app, conf):\n        self.app = app\n        self.memcache_servers = conf.get('memcache_servers')\n        serialization_format = conf.get('memcache_serialization_support')\n\n        if not self.memcache_servers or serialization_format is None:\n            path = os.path.join(conf.get('swift_dir', '/etc/swift'),\n                                'memcache.conf')\n            memcache_conf = ConfigParser()\n            if memcache_conf.read(path):\n                if not self.memcache_servers:\n                    try:\n                        self.memcache_servers = \\\n                            memcache_conf.get('memcache', 'memcache_servers')\n                    except (NoSectionError, NoOptionError):\n                        pass\n                if serialization_format is None:\n                    try:\n                        serialization_format = \\\n                            memcache_conf.get('memcache',\n                                              'memcache_serialization_support')\n                    except (NoSectionError, NoOptionError):\n                        pass\n\n        if not self.memcache_servers:\n            self.memcache_servers = '127.0.0.1:11211'\n        if serialization_format is None:\n            serialization_format = 2\n\n        self.memcache = MemcacheRing(\n            [s.strip() for s in self.memcache_servers.split(',') if s.strip()],\n            allow_pickle=(serialization_format == 0),\n            allow_unpickle=(serialization_format <= 1))\n\n    def __call__(self, env, start_response):\n        env['swift.cache'] = self.memcache\n        return self.app(env, start_response)\n\n\ndef filter_factory(global_conf, **local_conf):\n    conf = global_conf.copy()\n    conf.update(local_conf)\n\n    def cache_filter(app):\n        return MemcacheMiddleware(app, conf)\n\n    return cache_filter\n", "code_before": "# Copyright (c) 2010-2012 OpenStack, LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom ConfigParser import ConfigParser, NoSectionError, NoOptionError\n\nfrom swift.common.memcached import MemcacheRing\n\n\nclass MemcacheMiddleware(object):\n    \"\"\"\n    Caching middleware that manages caching in swift.\n    \"\"\"\n\n    def __init__(self, app, conf):\n        self.app = app\n        self.memcache_servers = conf.get('memcache_servers')\n        if not self.memcache_servers:\n            path = os.path.join(conf.get('swift_dir', '/etc/swift'),\n                                'memcache.conf')\n            memcache_conf = ConfigParser()\n            if memcache_conf.read(path):\n                try:\n                    self.memcache_servers = \\\n                        memcache_conf.get('memcache', 'memcache_servers')\n                except (NoSectionError, NoOptionError):\n                    pass\n        if not self.memcache_servers:\n            self.memcache_servers = '127.0.0.1:11211'\n        self.memcache = MemcacheRing(\n            [s.strip() for s in self.memcache_servers.split(',') if s.strip()])\n\n    def __call__(self, env, start_response):\n        env['swift.cache'] = self.memcache\n        return self.app(env, start_response)\n\n\ndef filter_factory(global_conf, **local_conf):\n    conf = global_conf.copy()\n    conf.update(local_conf)\n\n    def cache_filter(app):\n        return MemcacheMiddleware(app, conf)\n\n    return cache_filter\n", "patch": "@@ -27,20 +27,36 @@ class MemcacheMiddleware(object):\n     def __init__(self, app, conf):\n         self.app = app\n         self.memcache_servers = conf.get('memcache_servers')\n-        if not self.memcache_servers:\n+        serialization_format = conf.get('memcache_serialization_support')\n+\n+        if not self.memcache_servers or serialization_format is None:\n             path = os.path.join(conf.get('swift_dir', '/etc/swift'),\n                                 'memcache.conf')\n             memcache_conf = ConfigParser()\n             if memcache_conf.read(path):\n-                try:\n-                    self.memcache_servers = \\\n-                        memcache_conf.get('memcache', 'memcache_servers')\n-                except (NoSectionError, NoOptionError):\n-                    pass\n+                if not self.memcache_servers:\n+                    try:\n+                        self.memcache_servers = \\\n+                            memcache_conf.get('memcache', 'memcache_servers')\n+                    except (NoSectionError, NoOptionError):\n+                        pass\n+                if serialization_format is None:\n+                    try:\n+                        serialization_format = \\\n+                            memcache_conf.get('memcache',\n+                                              'memcache_serialization_support')\n+                    except (NoSectionError, NoOptionError):\n+                        pass\n+\n         if not self.memcache_servers:\n             self.memcache_servers = '127.0.0.1:11211'\n+        if serialization_format is None:\n+            serialization_format = 2\n+\n         self.memcache = MemcacheRing(\n-            [s.strip() for s in self.memcache_servers.split(',') if s.strip()])\n+            [s.strip() for s in self.memcache_servers.split(',') if s.strip()],\n+            allow_pickle=(serialization_format == 0),\n+            allow_unpickle=(serialization_format <= 1))\n \n     def __call__(self, env, start_response):\n         env['swift.cache'] = self.memcache", "file_path": "files/2012_10/13", "file_language": "py", "file_name": "swift/common/middleware/memcache.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/openstack/swift/raw/e1ff51c04554d51616d2845f92ab726cb0e5831a/test%2Funit%2Fcommon%2Fmiddleware%2Ftest_memcache.py", "code": "# Copyright (c) 2010-2012 OpenStack, LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom ConfigParser import NoSectionError, NoOptionError\n\nfrom webob import Request\n\nfrom swift.common.middleware import memcache\nfrom swift.common.memcached import MemcacheRing\n\nclass FakeApp(object):\n    def __call__(self, env, start_response):\n        return env\n\n\nclass ExcConfigParser(object):\n\n    def read(self, path):\n        raise Exception('read called with %r' % path)\n\n\nclass EmptyConfigParser(object):\n\n    def read(self, path):\n        return False\n\n\nclass SetConfigParser(object):\n\n    def read(self, path):\n        return True\n\n    def get(self, section, option):\n        if section == 'memcache':\n            if option == 'memcache_servers':\n                return '1.2.3.4:5'\n            elif option == 'memcache_serialization_support':\n                return '2'\n            else:\n                raise NoOptionError(option)\n        else:\n            raise NoSectionError(option)\n\n\ndef start_response(*args):\n    pass\n\nclass TestCacheMiddleware(unittest.TestCase):\n\n    def setUp(self):\n        self.app = memcache.MemcacheMiddleware(FakeApp(), {})\n\n    def test_cache_middleware(self):\n        req = Request.blank('/something', environ={'REQUEST_METHOD': 'GET'})\n        resp = self.app(req.environ, start_response)\n        self.assertTrue('swift.cache' in resp)\n        self.assertTrue(isinstance(resp['swift.cache'], MemcacheRing))\n\n    def test_conf_default_read(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = ExcConfigParser\n        exc = None\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        except Exception, err:\n            exc = err\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(str(exc),\n            \"read called with '/etc/swift/memcache.conf'\")\n\n    def test_conf_set_no_read(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = ExcConfigParser\n        exc = None\n        try:\n            app = memcache.MemcacheMiddleware(\n                    FakeApp(), {'memcache_servers': '1.2.3.4:5',\n                                'memcache_serialization_support': '2'})\n        except Exception, err:\n            exc = err\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(exc, None)\n\n    def test_conf_default(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = EmptyConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '127.0.0.1:11211')\n\n    def test_conf_from_extra_conf(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = SetConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '1.2.3.4:5')\n\n    def test_conf_from_inline_conf(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = SetConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(\n                    FakeApp(), {'memcache_servers': '6.7.8.9:10'})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '6.7.8.9:10')\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "code_before": "# Copyright (c) 2010-2012 OpenStack, LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom ConfigParser import NoSectionError, NoOptionError\n\nfrom webob import Request\n\nfrom swift.common.middleware import memcache\nfrom swift.common.memcached import MemcacheRing\n\nclass FakeApp(object):\n    def __call__(self, env, start_response):\n        return env\n\n\nclass ExcConfigParser(object):\n\n    def read(self, path):\n        raise Exception('read called with %r' % path)\n\n\nclass EmptyConfigParser(object):\n\n    def read(self, path):\n        return False\n\n\nclass SetConfigParser(object):\n\n    def read(self, path):\n        return True\n\n    def get(self, section, option):\n        if section == 'memcache':\n            if option == 'memcache_servers':\n                return '1.2.3.4:5'\n            else:\n                raise NoOptionError(option)\n        else:\n            raise NoSectionError(option)\n\n\ndef start_response(*args):\n    pass\n\nclass TestCacheMiddleware(unittest.TestCase):\n\n    def setUp(self):\n        self.app = memcache.MemcacheMiddleware(FakeApp(), {})\n\n    def test_cache_middleware(self):\n        req = Request.blank('/something', environ={'REQUEST_METHOD': 'GET'})\n        resp = self.app(req.environ, start_response)\n        self.assertTrue('swift.cache' in resp)\n        self.assertTrue(isinstance(resp['swift.cache'], MemcacheRing))\n\n    def test_conf_default_read(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = ExcConfigParser\n        exc = None\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        except Exception, err:\n            exc = err\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(str(exc),\n            \"read called with '/etc/swift/memcache.conf'\")\n\n    def test_conf_set_no_read(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = ExcConfigParser\n        exc = None\n        try:\n            app = memcache.MemcacheMiddleware(\n                    FakeApp(), {'memcache_servers': '1.2.3.4:5'})\n        except Exception, err:\n            exc = err\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(exc, None)\n\n    def test_conf_default(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = EmptyConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '127.0.0.1:11211')\n\n    def test_conf_from_extra_conf(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = SetConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '1.2.3.4:5')\n\n    def test_conf_from_inline_conf(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = SetConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(\n                    FakeApp(), {'memcache_servers': '6.7.8.9:10'})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '6.7.8.9:10')\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "patch": "@@ -47,6 +47,8 @@ def get(self, section, option):\n         if section == 'memcache':\n             if option == 'memcache_servers':\n                 return '1.2.3.4:5'\n+            elif option == 'memcache_serialization_support':\n+                return '2'\n             else:\n                 raise NoOptionError(option)\n         else:\n@@ -86,7 +88,8 @@ def test_conf_set_no_read(self):\n         exc = None\n         try:\n             app = memcache.MemcacheMiddleware(\n-                    FakeApp(), {'memcache_servers': '1.2.3.4:5'})\n+                    FakeApp(), {'memcache_servers': '1.2.3.4:5',\n+                                'memcache_serialization_support': '2'})\n         except Exception, err:\n             exc = err\n         finally:", "file_path": "files/2012_10/14", "file_language": "py", "file_name": "test/unit/common/middleware/test_memcache.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class FakeApp(object):\n    def __call__(self, env, start_response):\n        return env", "target": 0}, {"function": "class ExcConfigParser(object):\n\n    def read(self, path):\n        raise Exception('read called with %r' % path)", "target": 0}, {"function": "class EmptyConfigParser(object):\n\n    def read(self, path):\n        return False", "target": 0}, {"function": "class SetConfigParser(object):\n\n    def read(self, path):\n        return True\n\n    def get(self, section, option):\n        if section == 'memcache':\n            if option == 'memcache_servers':\n                return '1.2.3.4:5'\n            else:\n                raise NoOptionError(option)\n        else:\n            raise NoSectionError(option)", "target": 0}, {"function": "def start_response(*args):\n    pass", "target": 0}, {"function": "class TestCacheMiddleware(unittest.TestCase):\n\n    def setUp(self):\n        self.app = memcache.MemcacheMiddleware(FakeApp(), {})\n\n    def test_cache_middleware(self):\n        req = Request.blank('/something', environ={'REQUEST_METHOD': 'GET'})\n        resp = self.app(req.environ, start_response)\n        self.assertTrue('swift.cache' in resp)\n        self.assertTrue(isinstance(resp['swift.cache'], MemcacheRing))\n\n    def test_conf_default_read(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = ExcConfigParser\n        exc = None\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        except Exception, err:\n            exc = err\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(str(exc),\n            \"read called with '/etc/swift/memcache.conf'\")\n\n    def test_conf_set_no_read(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = ExcConfigParser\n        exc = None\n        try:\n            app = memcache.MemcacheMiddleware(\n                    FakeApp(), {'memcache_servers': '1.2.3.4:5'})\n        except Exception, err:\n            exc = err\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(exc, None)\n\n    def test_conf_default(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = EmptyConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '127.0.0.1:11211')\n\n    def test_conf_from_extra_conf(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = SetConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '1.2.3.4:5')\n\n    def test_conf_from_inline_conf(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = SetConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(\n                    FakeApp(), {'memcache_servers': '6.7.8.9:10'})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '6.7.8.9:10')", "target": 0}], "function_after": [{"function": "class FakeApp(object):\n    def __call__(self, env, start_response):\n        return env", "target": 0}, {"function": "class ExcConfigParser(object):\n\n    def read(self, path):\n        raise Exception('read called with %r' % path)", "target": 0}, {"function": "class EmptyConfigParser(object):\n\n    def read(self, path):\n        return False", "target": 0}, {"function": "class SetConfigParser(object):\n\n    def read(self, path):\n        return True\n\n    def get(self, section, option):\n        if section == 'memcache':\n            if option == 'memcache_servers':\n                return '1.2.3.4:5'\n            elif option == 'memcache_serialization_support':\n                return '2'\n            else:\n                raise NoOptionError(option)\n        else:\n            raise NoSectionError(option)", "target": 0}, {"function": "def start_response(*args):\n    pass", "target": 0}, {"function": "class TestCacheMiddleware(unittest.TestCase):\n\n    def setUp(self):\n        self.app = memcache.MemcacheMiddleware(FakeApp(), {})\n\n    def test_cache_middleware(self):\n        req = Request.blank('/something', environ={'REQUEST_METHOD': 'GET'})\n        resp = self.app(req.environ, start_response)\n        self.assertTrue('swift.cache' in resp)\n        self.assertTrue(isinstance(resp['swift.cache'], MemcacheRing))\n\n    def test_conf_default_read(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = ExcConfigParser\n        exc = None\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        except Exception, err:\n            exc = err\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(str(exc),\n            \"read called with '/etc/swift/memcache.conf'\")\n\n    def test_conf_set_no_read(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = ExcConfigParser\n        exc = None\n        try:\n            app = memcache.MemcacheMiddleware(\n                    FakeApp(), {'memcache_servers': '1.2.3.4:5',\n                                'memcache_serialization_support': '2'})\n        except Exception, err:\n            exc = err\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(exc, None)\n\n    def test_conf_default(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = EmptyConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '127.0.0.1:11211')\n\n    def test_conf_from_extra_conf(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = SetConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '1.2.3.4:5')\n\n    def test_conf_from_inline_conf(self):\n        orig_parser = memcache.ConfigParser\n        memcache.ConfigParser = SetConfigParser\n        try:\n            app = memcache.MemcacheMiddleware(\n                    FakeApp(), {'memcache_servers': '6.7.8.9:10'})\n        finally:\n            memcache.ConfigParser = orig_parser\n        self.assertEquals(app.memcache_servers, '6.7.8.9:10')", "target": 0}]}, {"raw_url": "https://github.com/openstack/swift/raw/e1ff51c04554d51616d2845f92ab726cb0e5831a/test%2Funit%2Fcommon%2Ftest_memcached.py", "code": " # -*- coding: utf8 -*-\n# Copyright (c) 2010-2012 OpenStack, LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\" Tests for swift.common.utils \"\"\"\n\nfrom __future__ import with_statement\nimport hashlib\nimport logging\nimport socket\nimport time\nimport unittest\nfrom uuid import uuid4\n\nfrom swift.common import memcached\nfrom test.unit import NullLoggingHandler\n\n\nclass ExplodingMockMemcached(object):\n    exploded = False\n    def sendall(self, string):\n        self.exploded = True\n        raise socket.error()\n    def readline(self):\n        self.exploded = True\n        raise socket.error()\n    def read(self, size):\n        self.exploded = True\n        raise socket.error()\n\nclass MockMemcached(object):\n    def __init__(self):\n        self.inbuf = ''\n        self.outbuf = ''\n        self.cache = {}\n        self.down = False\n        self.exc_on_delete = False\n        self.read_return_none = False\n\n    def sendall(self, string):\n        if self.down:\n            raise Exception('mock is down')\n        self.inbuf += string\n        while '\\n' in self.inbuf:\n            cmd, self.inbuf = self.inbuf.split('\\n', 1)\n            parts = cmd.split()\n            if parts[0].lower() == 'set':\n                self.cache[parts[1]] = parts[2], parts[3], \\\n                        self.inbuf[:int(parts[4])]\n                self.inbuf = self.inbuf[int(parts[4])+2:]\n                if len(parts) < 6 or parts[5] != 'noreply':\n                    self.outbuf += 'STORED\\r\\n'\n            elif parts[0].lower() == 'add':\n                value = self.inbuf[:int(parts[4])]\n                self.inbuf = self.inbuf[int(parts[4])+2:]\n                if parts[1] in self.cache:\n                    if len(parts) < 6 or parts[5] != 'noreply':\n                        self.outbuf += 'NOT_STORED\\r\\n'\n                else:\n                    self.cache[parts[1]] = parts[2], parts[3], value\n                    if len(parts) < 6 or parts[5] != 'noreply':\n                        self.outbuf += 'STORED\\r\\n'\n            elif parts[0].lower() == 'delete':\n                if self.exc_on_delete:\n                    raise Exception('mock is has exc_on_delete set')\n                if parts[1] in self.cache:\n                    del self.cache[parts[1]]\n                    if 'noreply' not in parts:\n                        self.outbuf += 'DELETED\\r\\n'\n                elif 'noreply' not in parts:\n                    self.outbuf += 'NOT_FOUND\\r\\n'\n            elif parts[0].lower() == 'get':\n                for key in parts[1:]:\n                    if key in self.cache:\n                        val = self.cache[key]\n                        self.outbuf += 'VALUE %s %s %s\\r\\n' % (key, val[0], len(val[2]))\n                        self.outbuf += val[2] + '\\r\\n'\n                self.outbuf += 'END\\r\\n'\n            elif parts[0].lower() == 'incr':\n                if parts[1] in self.cache:\n                    val = list(self.cache[parts[1]])\n                    val[2] = str(int(val[2]) + int(parts[2]))\n                    self.cache[parts[1]] = val\n                    self.outbuf += str(val[2]) + '\\r\\n'\n                else:\n                    self.outbuf += 'NOT_FOUND\\r\\n'\n            elif parts[0].lower() == 'decr':\n                if parts[1] in self.cache:\n                    val = list(self.cache[parts[1]])\n                    if int(val[2]) - int(parts[2]) > 0:\n                        val[2] = str(int(val[2]) - int(parts[2]))\n                    else:\n                        val[2] = '0'\n                    self.cache[parts[1]] = val\n                    self.outbuf += str(val[2]) + '\\r\\n'\n                else:\n                    self.outbuf += 'NOT_FOUND\\r\\n'\n    def readline(self):\n        if self.read_return_none:\n            return None\n        if self.down:\n            raise Exception('mock is down')\n        if '\\n' in self.outbuf:\n            response, self.outbuf = self.outbuf.split('\\n', 1)\n            return response+'\\n'\n    def read(self, size):\n        if self.down:\n            raise Exception('mock is down')\n        if len(self.outbuf) >= size:\n            response = self.outbuf[:size]\n            self.outbuf = self.outbuf[size:]\n            return response\n\nclass TestMemcached(unittest.TestCase):\n    \"\"\" Tests for swift.common.memcached\"\"\"\n\n    def test_get_conns(self):\n        sock1 = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock1.bind(('127.0.0.1', 0))\n        sock1.listen(1)\n        sock1ipport = '%s:%s' % sock1.getsockname()\n        sock2 = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock2.bind(('127.0.0.1', 0))\n        sock2.listen(1)\n        orig_port = memcached.DEFAULT_MEMCACHED_PORT\n        try:\n            sock2ip, memcached.DEFAULT_MEMCACHED_PORT = sock2.getsockname()\n            sock2ipport = '%s:%s' % (sock2ip, memcached.DEFAULT_MEMCACHED_PORT)\n            # We're deliberately using sock2ip (no port) here to test that the\n            # default port is used.\n            memcache_client = memcached.MemcacheRing([sock1ipport, sock2ip])\n            one = two = True\n            while one or two:  # Run until we match hosts one and two\n                key = uuid4().hex\n                for conn in memcache_client._get_conns(key):\n                    peeripport = '%s:%s' % conn[2].getpeername()\n                    self.assert_(peeripport in (sock1ipport, sock2ipport))\n                    if peeripport == sock1ipport:\n                        one = False\n                    if peeripport == sock2ipport:\n                        two = False\n        finally:\n            memcached.DEFAULT_MEMCACHED_PORT = orig_port\n\n    def test_set_get(self):\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'])\n        mock = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n        memcache_client.set('some_key', [1, 2, 3])\n        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n        memcache_client.set('some_key', [4, 5, 6])\n        self.assertEquals(memcache_client.get('some_key'), [4, 5, 6])\n        memcache_client.set('some_key', ['simple str', 'utf8 str \u00e9\u00e0'])\n        # As per http://wiki.openstack.org/encoding, we should expect to have unicode\n        self.assertEquals(memcache_client.get('some_key'), ['simple str', u'utf8 str \u00e9\u00e0'])\n        self.assert_(float(mock.cache.values()[0][1]) == 0)\n        esttimeout = time.time() + 10\n        memcache_client.set('some_key', [1, 2, 3], timeout=10)\n        self.assert_(-1 <= float(mock.cache.values()[0][1]) - esttimeout <= 1)\n\n    def test_incr(self):\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'])\n        mock = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n        memcache_client.incr('some_key', delta=5)\n        self.assertEquals(memcache_client.get('some_key'), '5')\n        memcache_client.incr('some_key', delta=5)\n        self.assertEquals(memcache_client.get('some_key'), '10')\n        memcache_client.incr('some_key', delta=1)\n        self.assertEquals(memcache_client.get('some_key'), '11')\n        memcache_client.incr('some_key', delta=-5)\n        self.assertEquals(memcache_client.get('some_key'), '6')\n        memcache_client.incr('some_key', delta=-15)\n        self.assertEquals(memcache_client.get('some_key'), '0')\n        mock.read_return_none = True\n        self.assertRaises(memcached.MemcacheConnectionError,\n                          memcache_client.incr, 'some_key', delta=-15)\n\n    def test_decr(self):\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'])\n        mock = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n        memcache_client.decr('some_key', delta=5)\n        self.assertEquals(memcache_client.get('some_key'), '0')\n        memcache_client.incr('some_key', delta=15)\n        self.assertEquals(memcache_client.get('some_key'), '15')\n        memcache_client.decr('some_key', delta=4)\n        self.assertEquals(memcache_client.get('some_key'), '11')\n        memcache_client.decr('some_key', delta=15)\n        self.assertEquals(memcache_client.get('some_key'), '0')\n        mock.read_return_none = True\n        self.assertRaises(memcached.MemcacheConnectionError,\n                          memcache_client.decr, 'some_key', delta=15)\n\n\n    def test_retry(self):\n        logging.getLogger().addHandler(NullLoggingHandler())\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211', '1.2.3.5:11211'])\n        mock1 = ExplodingMockMemcached()\n        mock2 = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock2, mock2)]\n        memcache_client._client_cache['1.2.3.5:11211'] = [(mock1, mock1)]\n        memcache_client.set('some_key', [1, 2, 3])\n        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n        self.assertEquals(mock1.exploded, True)\n\n    def test_delete(self):\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'])\n        mock = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n        memcache_client.set('some_key', [1, 2, 3])\n        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n        memcache_client.delete('some_key')\n        self.assertEquals(memcache_client.get('some_key'), None)\n\n    def test_multi(self):\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'])\n        mock = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n        memcache_client.set_multi(\n            {'some_key1': [1, 2, 3], 'some_key2': [4, 5, 6]}, 'multi_key')\n        self.assertEquals(\n            memcache_client.get_multi(('some_key2', 'some_key1'), 'multi_key'),\n            [[4, 5, 6], [1, 2, 3]])\n        esttimeout = time.time() + 10\n        memcache_client.set_multi(\n            {'some_key1': [1, 2, 3], 'some_key2': [4, 5, 6]}, 'multi_key',\n            timeout=10)\n        self.assert_(-1 <= float(mock.cache.values()[0][1]) - esttimeout <= 1)\n        self.assert_(-1 <= float(mock.cache.values()[1][1]) - esttimeout <= 1)\n        self.assertEquals(memcache_client.get_multi(('some_key2', 'some_key1',\n            'not_exists'), 'multi_key'), [[4, 5, 6], [1, 2, 3], None])\n\n    def test_serialization(self):\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'],\n                                                 allow_pickle=True)\n        mock = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n        memcache_client.set('some_key', [1, 2, 3])\n        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n        memcache_client._allow_pickle = False\n        memcache_client._allow_unpickle = True\n        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n        memcache_client._allow_unpickle = False\n        self.assertEquals(memcache_client.get('some_key'), None)\n        memcache_client.set('some_key', [1, 2, 3])\n        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n        memcache_client._allow_unpickle = True\n        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n        memcache_client._allow_pickle = True\n        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n\nif __name__ == '__main__':\n    unittest.main()\n\n", "code_before": "# Copyright (c) 2010-2012 OpenStack, LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\" Tests for swift.common.utils \"\"\"\n\nfrom __future__ import with_statement\nimport hashlib\nimport logging\nimport socket\nimport time\nimport unittest\nfrom uuid import uuid4\n\nfrom swift.common import memcached\nfrom test.unit import NullLoggingHandler\n\n\nclass ExplodingMockMemcached(object):\n    exploded = False\n    def sendall(self, string):\n        self.exploded = True\n        raise socket.error()\n    def readline(self):\n        self.exploded = True\n        raise socket.error()\n    def read(self, size):\n        self.exploded = True\n        raise socket.error()\n\nclass MockMemcached(object):\n    def __init__(self):\n        self.inbuf = ''\n        self.outbuf = ''\n        self.cache = {}\n        self.down = False\n        self.exc_on_delete = False\n        self.read_return_none = False\n\n    def sendall(self, string):\n        if self.down:\n            raise Exception('mock is down')\n        self.inbuf += string\n        while '\\n' in self.inbuf:\n            cmd, self.inbuf = self.inbuf.split('\\n', 1)\n            parts = cmd.split()\n            if parts[0].lower() == 'set':\n                self.cache[parts[1]] = parts[2], parts[3], \\\n                        self.inbuf[:int(parts[4])]\n                self.inbuf = self.inbuf[int(parts[4])+2:]\n                if len(parts) < 6 or parts[5] != 'noreply':\n                    self.outbuf += 'STORED\\r\\n'\n            elif parts[0].lower() == 'add':\n                value = self.inbuf[:int(parts[4])]\n                self.inbuf = self.inbuf[int(parts[4])+2:]\n                if parts[1] in self.cache:\n                    if len(parts) < 6 or parts[5] != 'noreply':\n                        self.outbuf += 'NOT_STORED\\r\\n'\n                else:\n                    self.cache[parts[1]] = parts[2], parts[3], value\n                    if len(parts) < 6 or parts[5] != 'noreply':\n                        self.outbuf += 'STORED\\r\\n'\n            elif parts[0].lower() == 'delete':\n                if self.exc_on_delete:\n                    raise Exception('mock is has exc_on_delete set')\n                if parts[1] in self.cache:\n                    del self.cache[parts[1]]\n                    if 'noreply' not in parts:\n                        self.outbuf += 'DELETED\\r\\n'\n                elif 'noreply' not in parts:\n                    self.outbuf += 'NOT_FOUND\\r\\n'\n            elif parts[0].lower() == 'get':\n                for key in parts[1:]:\n                    if key in self.cache:\n                        val = self.cache[key]\n                        self.outbuf += 'VALUE %s %s %s\\r\\n' % (key, val[0], len(val[2]))\n                        self.outbuf += val[2] + '\\r\\n'\n                self.outbuf += 'END\\r\\n'\n            elif parts[0].lower() == 'incr':\n                if parts[1] in self.cache:\n                    val = list(self.cache[parts[1]])\n                    val[2] = str(int(val[2]) + int(parts[2]))\n                    self.cache[parts[1]] = val\n                    self.outbuf += str(val[2]) + '\\r\\n'\n                else:\n                    self.outbuf += 'NOT_FOUND\\r\\n'\n            elif parts[0].lower() == 'decr':\n                if parts[1] in self.cache:\n                    val = list(self.cache[parts[1]])\n                    if int(val[2]) - int(parts[2]) > 0:\n                        val[2] = str(int(val[2]) - int(parts[2]))\n                    else:\n                        val[2] = '0'\n                    self.cache[parts[1]] = val\n                    self.outbuf += str(val[2]) + '\\r\\n'\n                else:\n                    self.outbuf += 'NOT_FOUND\\r\\n'\n    def readline(self):\n        if self.read_return_none:\n            return None\n        if self.down:\n            raise Exception('mock is down')\n        if '\\n' in self.outbuf:\n            response, self.outbuf = self.outbuf.split('\\n', 1)\n            return response+'\\n'\n    def read(self, size):\n        if self.down:\n            raise Exception('mock is down')\n        if len(self.outbuf) >= size:\n            response = self.outbuf[:size]\n            self.outbuf = self.outbuf[size:]\n            return response\n\nclass TestMemcached(unittest.TestCase):\n    \"\"\" Tests for swift.common.memcached\"\"\"\n\n    def test_get_conns(self):\n        sock1 = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock1.bind(('127.0.0.1', 0))\n        sock1.listen(1)\n        sock1ipport = '%s:%s' % sock1.getsockname()\n        sock2 = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock2.bind(('127.0.0.1', 0))\n        sock2.listen(1)\n        orig_port = memcached.DEFAULT_MEMCACHED_PORT\n        try:\n            sock2ip, memcached.DEFAULT_MEMCACHED_PORT = sock2.getsockname()\n            sock2ipport = '%s:%s' % (sock2ip, memcached.DEFAULT_MEMCACHED_PORT)\n            # We're deliberately using sock2ip (no port) here to test that the\n            # default port is used.\n            memcache_client = memcached.MemcacheRing([sock1ipport, sock2ip])\n            one = two = True\n            while one or two:  # Run until we match hosts one and two\n                key = uuid4().hex\n                for conn in memcache_client._get_conns(key):\n                    peeripport = '%s:%s' % conn[2].getpeername()\n                    self.assert_(peeripport in (sock1ipport, sock2ipport))\n                    if peeripport == sock1ipport:\n                        one = False\n                    if peeripport == sock2ipport:\n                        two = False\n        finally:\n            memcached.DEFAULT_MEMCACHED_PORT = orig_port\n\n    def test_set_get(self):\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'])\n        mock = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n        memcache_client.set('some_key', [1, 2, 3])\n        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n        memcache_client.set('some_key', [4, 5, 6])\n        self.assertEquals(memcache_client.get('some_key'), [4, 5, 6])\n        self.assert_(float(mock.cache.values()[0][1]) == 0)\n        esttimeout = time.time() + 10\n        memcache_client.set('some_key', [1, 2, 3], timeout=10)\n        self.assert_(-1 <= float(mock.cache.values()[0][1]) - esttimeout <= 1)\n\n    def test_incr(self):\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'])\n        mock = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n        memcache_client.incr('some_key', delta=5)\n        self.assertEquals(memcache_client.get('some_key'), '5')\n        memcache_client.incr('some_key', delta=5)\n        self.assertEquals(memcache_client.get('some_key'), '10')\n        memcache_client.incr('some_key', delta=1)\n        self.assertEquals(memcache_client.get('some_key'), '11')\n        memcache_client.incr('some_key', delta=-5)\n        self.assertEquals(memcache_client.get('some_key'), '6')\n        memcache_client.incr('some_key', delta=-15)\n        self.assertEquals(memcache_client.get('some_key'), '0')\n        mock.read_return_none = True\n        self.assertRaises(memcached.MemcacheConnectionError,\n                          memcache_client.incr, 'some_key', delta=-15)\n\n    def test_decr(self):\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'])\n        mock = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n        memcache_client.decr('some_key', delta=5)\n        self.assertEquals(memcache_client.get('some_key'), '0')\n        memcache_client.incr('some_key', delta=15)\n        self.assertEquals(memcache_client.get('some_key'), '15')\n        memcache_client.decr('some_key', delta=4)\n        self.assertEquals(memcache_client.get('some_key'), '11')\n        memcache_client.decr('some_key', delta=15)\n        self.assertEquals(memcache_client.get('some_key'), '0')\n        mock.read_return_none = True\n        self.assertRaises(memcached.MemcacheConnectionError,\n                          memcache_client.decr, 'some_key', delta=15)\n\n\n    def test_retry(self):\n        logging.getLogger().addHandler(NullLoggingHandler())\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211', '1.2.3.5:11211'])\n        mock1 = ExplodingMockMemcached()\n        mock2 = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock2, mock2)]\n        memcache_client._client_cache['1.2.3.5:11211'] = [(mock1, mock1)]\n        memcache_client.set('some_key', [1, 2, 3])\n        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n        self.assertEquals(mock1.exploded, True)\n\n    def test_delete(self):\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'])\n        mock = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n        memcache_client.set('some_key', [1, 2, 3])\n        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n        memcache_client.delete('some_key')\n        self.assertEquals(memcache_client.get('some_key'), None)\n\n    def test_multi(self):\n        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'])\n        mock = MockMemcached()\n        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n        memcache_client.set_multi(\n            {'some_key1': [1, 2, 3], 'some_key2': [4, 5, 6]}, 'multi_key')\n        self.assertEquals(\n            memcache_client.get_multi(('some_key2', 'some_key1'), 'multi_key'),\n            [[4, 5, 6], [1, 2, 3]])\n        esttimeout = time.time() + 10\n        memcache_client.set_multi(\n            {'some_key1': [1, 2, 3], 'some_key2': [4, 5, 6]}, 'multi_key',\n            timeout=10)\n        self.assert_(-1 <= float(mock.cache.values()[0][1]) - esttimeout <= 1)\n        self.assert_(-1 <= float(mock.cache.values()[1][1]) - esttimeout <= 1)\n        self.assertEquals(memcache_client.get_multi(('some_key2', 'some_key1',\n            'not_exists'), 'multi_key'), [[4, 5, 6], [1, 2, 3], None])\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n", "patch": "@@ -1,3 +1,4 @@\n+ # -*- coding: utf8 -*-\n # Copyright (c) 2010-2012 OpenStack, LLC.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -161,6 +162,9 @@ def test_set_get(self):\n         self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n         memcache_client.set('some_key', [4, 5, 6])\n         self.assertEquals(memcache_client.get('some_key'), [4, 5, 6])\n+        memcache_client.set('some_key', ['simple str', 'utf8 str \u00e9\u00e0'])\n+        # As per http://wiki.openstack.org/encoding, we should expect to have unicode\n+        self.assertEquals(memcache_client.get('some_key'), ['simple str', u'utf8 str \u00e9\u00e0'])\n         self.assert_(float(mock.cache.values()[0][1]) == 0)\n         esttimeout = time.time() + 10\n         memcache_client.set('some_key', [1, 2, 3], timeout=10)\n@@ -239,6 +243,24 @@ def test_multi(self):\n         self.assertEquals(memcache_client.get_multi(('some_key2', 'some_key1',\n             'not_exists'), 'multi_key'), [[4, 5, 6], [1, 2, 3], None])\n \n+    def test_serialization(self):\n+        memcache_client = memcached.MemcacheRing(['1.2.3.4:11211'],\n+                                                 allow_pickle=True)\n+        mock = MockMemcached()\n+        memcache_client._client_cache['1.2.3.4:11211'] = [(mock, mock)] * 2\n+        memcache_client.set('some_key', [1, 2, 3])\n+        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n+        memcache_client._allow_pickle = False\n+        memcache_client._allow_unpickle = True\n+        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n+        memcache_client._allow_unpickle = False\n+        self.assertEquals(memcache_client.get('some_key'), None)\n+        memcache_client.set('some_key', [1, 2, 3])\n+        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n+        memcache_client._allow_unpickle = True\n+        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n+        memcache_client._allow_pickle = True\n+        self.assertEquals(memcache_client.get('some_key'), [1, 2, 3])\n \n if __name__ == '__main__':\n     unittest.main()", "file_path": "files/2012_10/15", "file_language": "py", "file_name": "test/unit/common/test_memcached.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
