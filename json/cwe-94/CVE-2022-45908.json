{"index": 9592, "cve_id": "CVE-2022-45908", "cwe_id": ["CWE-94"], "cve_language": "Python", "cve_description": "In PaddlePaddle before 2.4, paddle.audio.functional.get_window is vulnerable to code injection because it calls eval on a user-supplied winstr. This may lead to arbitrary code execution.", "cvss": "9.8", "publish_date": "November 25, 2022", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "NONE", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "26c419ca386aeae3c461faf2b828d00b48e908eb", "commit_message": "[audio]fix audio get_window security error (#47386)\n\n* fix window security error\r\n\r\n* format", "commit_date": "2022-10-28T10:15:55Z", "project": "paddlepaddle/paddle", "url": "https://api.github.com/repos/PaddlePaddle/Paddle/commits/26c419ca386aeae3c461faf2b828d00b48e908eb", "html_url": "https://github.com/PaddlePaddle/Paddle/commit/26c419ca386aeae3c461faf2b828d00b48e908eb", "windows_before": [{"commit_id": "0f649b32397dcd043e51b414904ccfa730b52603", "commit_date": "Fri Oct 28 14:20:32 2022 +0800", "commit_message": "remove tcp store barrier (#47184)", "files_name": ["python/paddle/distributed/collective.py"]}, {"commit_id": "e48b6dcfa656c7b40dd4d53c41d494b42fa276fe", "commit_date": "Fri Oct 28 14:07:15 2022 +0800", "commit_message": "[JITLayer]Enable OneDNN on CPU and Fix zero shape (#47428)", "files_name": ["paddle/fluid/inference/api/analysis_predictor.cc", "paddle/fluid/jit/engine/predictor_engine.cc"]}, {"commit_id": "57d5ffa5fd340831f90d0a6f15f7cab930cd8842", "commit_date": "Fri Oct 28 12:59:25 2022 +0800", "commit_message": "[Dygraph] Fix memory bugs of no sync and SplitTensors in DataParallel (#47369)", "files_name": ["paddle/fluid/distributed/collective/ProcessGroup.cc", "paddle/fluid/distributed/collective/ProcessGroup.h", "paddle/fluid/distributed/collective/ProcessGroupGloo.h", "paddle/fluid/distributed/collective/ProcessGroupNCCL.cc", "paddle/fluid/distributed/collective/ProcessGroupNCCL.h", "paddle/fluid/distributed/collective/ProcessGroupStream.cc", "paddle/fluid/distributed/collective/ProcessGroupStream.h", "paddle/fluid/distributed/collective/Utils.h", "paddle/fluid/distributed/collective/reducer.cc", "paddle/fluid/distributed/collective/reducer.h", "paddle/fluid/imperative/reducer.cc", "paddle/fluid/imperative/reducer.h", "paddle/fluid/pybind/distributed_py.cc", "paddle/fluid/pybind/imperative.cc", "python/paddle/fluid/dygraph/parallel.py"]}, {"commit_id": "6baeb2d1066b58be0f64d3f864b6e3aea0f5974d", "commit_date": "Fri Oct 28 11:26:44 2022 +0800", "commit_message": "Generate static graph code for some activation ops by Yaml (#47382)", "files_name": [".pre-commit-config.yaml", "paddle/fluid/operators/activation_op.cc", "paddle/phi/api/yaml/backward.yaml", "paddle/phi/api/yaml/legacy_backward.yaml", "paddle/phi/api/yaml/legacy_ops.yaml", "paddle/phi/api/yaml/op_compat.yaml", "paddle/phi/api/yaml/ops.yaml", "paddle/phi/ops/compat/activation_sig.cc", "python/paddle/tensor/ops.py"]}, {"commit_id": "533f6cbda3fc17814a828c88ead876775b94c2c1", "commit_date": "Fri Oct 28 11:15:06 2022 +0800", "commit_message": "Revert \"Optimiza params sync between CPU and GPU. (#45805)\" (#47356)", "files_name": ["paddle/fluid/inference/analysis/passes/ir_params_sync_among_devices_pass.cc"]}, {"commit_id": "6b77bfff91281c542d9dd244d580392501ef37de", "commit_date": "Fri Oct 28 11:03:16 2022 +0800", "commit_message": "fix default setting of dygraph PTQ (#47413)", "files_name": ["python/paddle/fluid/contrib/slim/quantization/imperative/ptq.py"]}, {"commit_id": "800e05346a69f94bb594609fd1c5532715f37e15", "commit_date": "Fri Oct 28 10:16:09 2022 +0800", "commit_message": "fix pragma-pack warning on macos (#47399)", "files_name": ["cmake/flags.cmake", "paddle/fluid/operators/math/bloomfilter.h"]}, {"commit_id": "b160d09eeb9453d05575beb88bc82703791ec977", "commit_date": "Thu Oct 27 21:03:28 2022 +0800", "commit_message": "[JIT] Add Predictor for JITLayer (#47379)", "files_name": ["paddle/fluid/inference/analysis/argument.h", "paddle/fluid/inference/analysis/passes/ir_graph_build_pass.cc", "paddle/fluid/inference/analysis/passes/ir_graph_build_pass.h", "paddle/fluid/inference/api/analysis_config.cc", "paddle/fluid/inference/api/analysis_predictor.cc", "paddle/fluid/inference/api/paddle_analysis_config.h", "paddle/fluid/inference/io.cc", "paddle/fluid/inference/io.h", "paddle/fluid/jit/CMakeLists.txt", "paddle/fluid/jit/engine/CMakeLists.txt", "paddle/fluid/jit/engine/predictor_engine.cc", "paddle/fluid/jit/engine/predictor_engine.h", "paddle/fluid/jit/function_schema.cc", "paddle/fluid/jit/function_schema.h", "paddle/fluid/jit/serializer.cc", "paddle/fluid/operators/collective/CMakeLists.txt", "paddle/fluid/operators/sequence_ops/CMakeLists.txt", "paddle/fluid/platform/flags.cc"]}, {"commit_id": "0972d6ac78e8e7696c256da8dc961b1d7ed8fe93", "commit_date": "Thu Oct 27 20:56:40 2022 +0800", "commit_message": "[Paddle Inference] improve convert_to_mixed_precision (#47333)", "files_name": ["paddle/fluid/inference/analysis/passes/convert_to_mixed_precision.cc", "paddle/fluid/inference/analysis/passes/convert_to_mixed_precision.h"]}, {"commit_id": "5429d145978118da922d5c0e610a7761ee450ca5", "commit_date": "Thu Oct 27 19:21:33 2022 +0800", "commit_message": "update dygraph PTQ export_model api (#47284)", "files_name": ["python/paddle/fluid/contrib/slim/quantization/imperative/ptq.py", "python/paddle/fluid/contrib/slim/quantization/imperative/ptq_registry.py"]}, {"commit_id": "b68c4a1e6dc442a248f8f51650249f0559dbd5bd", "commit_date": "Thu Oct 27 19:18:04 2022 +0800", "commit_message": "[Dy2St]Fix abnormal growth of memory in train mode and no_grad for Dy2St (#47398)", "files_name": ["paddle/fluid/eager/to_static/run_program_op_node.h", "python/paddle/fluid/dygraph/io.py"]}, {"commit_id": "8775545a7dd4e54d46ad5ba48db6d31e0aea0dd2", "commit_date": "Thu Oct 27 19:06:17 2022 +0800", "commit_message": "support prepare_data for selected_rows in c++ api (#47380)", "files_name": ["paddle/phi/api/lib/data_transform.cc", "paddle/phi/api/lib/data_transform.h", "paddle/phi/api/yaml/generator/api_base.py"]}, {"commit_id": "2096448bb1268ffef91be4e013842cc685605683", "commit_date": "Thu Oct 27 17:06:22 2022 +0800", "commit_message": "make all cpp tests dynamic linked to libpaddle.so [except windows] (#47088)", "files_name": ["CMakeLists.txt", "cmake/generic.cmake", "paddle/CMakeLists.txt", "paddle/fluid/distributed/fleet_executor/test/CMakeLists.txt", "paddle/fluid/distributed/test/CMakeLists.txt", "paddle/fluid/eager/tests/data_structure_tests/CMakeLists.txt", "paddle/fluid/eager/tests/performance_tests/CMakeLists.txt", "paddle/fluid/framework/CMakeLists.txt", "paddle/fluid/framework/details/CMakeLists.txt", "paddle/fluid/framework/details/build_strategy_test.cc", "paddle/fluid/framework/ir/CMakeLists.txt", "paddle/fluid/framework/ir/graph_test.cc", "paddle/fluid/framework/ir/mkldnn/conv_activation_mkldnn_fuse_pass_tester.cc", "paddle/fluid/framework/new_executor/CMakeLists.txt", "paddle/fluid/framework/paddle2cinn/CMakeLists.txt", "paddle/fluid/framework/var_type_inference_test.cc", "paddle/fluid/inference/CMakeLists.txt", "paddle/fluid/inference/api/CMakeLists.txt", "paddle/fluid/inference/lite/CMakeLists.txt", "paddle/fluid/inference/utils/CMakeLists.txt", "paddle/fluid/inference/utils/table_printer.cc", "paddle/fluid/jit/CMakeLists.txt", "paddle/fluid/memory/allocation/CMakeLists.txt", "paddle/fluid/operators/benchmark/CMakeLists.txt", "paddle/fluid/operators/benchmark/op_tester_config.cc", "paddle/fluid/operators/benchmark/op_tester_config.h", "paddle/fluid/operators/cinn/CMakeLists.txt", "paddle/fluid/operators/copy_cross_scope_test.cc", "paddle/fluid/operators/lite/CMakeLists.txt", "paddle/fluid/operators/mkldnn/nhwc_op_tests.cmake", "paddle/fluid/operators/prim_ops/CMakeLists.txt", "paddle/fluid/operators/pscore/CMakeLists.txt", "paddle/fluid/pybind/CMakeLists.txt", "paddle/phi/tests/core/CMakeLists.txt", "paddle/phi/tests/ops/test_op_signature.cc", "paddle/testing/CMakeLists.txt"]}, {"commit_id": "539f30061e1feb4ddbe17b9ce7a82679db18e395", "commit_date": "Thu Oct 27 16:31:14 2022 +0800", "commit_message": "clean gelu cudnn (#47378)", "files_name": ["paddle/phi/api/yaml/op_compat.yaml"]}, {"commit_id": "4d5c8a69a99f35643e737060fb7b9ce5b6ff87d0", "commit_date": "Thu Oct 27 16:31:06 2022 +0800", "commit_message": "clean angle cudnn (#47375)", "files_name": ["paddle/phi/api/yaml/op_compat.yaml"]}, {"commit_id": "8607a180e1adb66e286336165bd4ddc4c1a8ec8e", "commit_date": "Thu Oct 27 16:30:58 2022 +0800", "commit_message": "clean abs cudnn (#47374)", "files_name": ["paddle/phi/api/yaml/op_compat.yaml"]}, {"commit_id": "23c9c8857bb3a02028160fd012a6516ea2140b08", "commit_date": "Thu Oct 27 16:28:21 2022 +0800", "commit_message": "precise_test_logic_update (#47387)", "files_name": ["paddle/scripts/paddle_build.sh", "tools/final_ut_parallel_rule.py", "tools/get_pr_ut.py", "tools/get_single_test_cov.py", "tools/get_ut_file_map.py"]}, {"commit_id": "daf98c15451f70caae5626baa43d006348bc6410", "commit_date": "Thu Oct 27 16:27:59 2022 +0800", "commit_message": "New precise map (#47389)", "files_name": ["tools/coverage/paddle_coverage.sh", "tools/get_pr_ut.py"]}, {"commit_id": "8dca988214a2539235251b70d78a79c19f3f1492", "commit_date": "Thu Oct 27 15:38:09 2022 +0800", "commit_message": "Fix the symbol missing bug about cinn. (#47347)", "files_name": ["paddle/fluid/framework/paddle2cinn/build_cinn_pass.cc", "paddle/fluid/inference/paddle_inference_custom_device.map"]}, {"commit_id": "13181fd975bccb25d1d1126347872ef68279d567", "commit_date": "Thu Oct 27 14:54:41 2022 +0800", "commit_message": "Add launch_bounds (#47285)", "files_name": ["paddle/fluid/operators/fused/fused_dropout_act_bias.h"]}, {"commit_id": "493fbfd75b0983de4a08afb859be104398a0af22", "commit_date": "Thu Oct 27 08:22:46 2022 +0200", "commit_message": "Update of PHI transpose_grad (#47311)", "files_name": ["paddle/phi/backends/onednn/onednn_reuse.h", "paddle/phi/kernels/onednn/transpose_grad_kernel.cc"]}, {"commit_id": "77dbb318e77146aa043be3f276959e0c42d3911c", "commit_date": "Thu Oct 27 13:38:06 2022 +0800", "commit_message": "fix reduce_any kernel data race on sharedMem (#47233)", "files_name": ["paddle/phi/kernels/primitive/compute_primitives.h"]}, {"commit_id": "cb74666531b5371fdb6d2135d02fa5c10a7099ab", "commit_date": "Thu Oct 27 11:28:50 2022 +0800", "commit_message": "delete GetKernelTypeForVar mkldnn hardcode (#47360)", "files_name": ["paddle/fluid/operators/prelu_op.cc"]}, {"commit_id": "19feba386c0ddb5eaf16c1e75dc264a347d55a1b", "commit_date": "Wed Oct 26 21:51:49 2022 -0500", "commit_message": "Fix compile error of mkldnn and tensorrt (#47388)", "files_name": ["paddle/fluid/framework/ir/mkldnn/operator_scale_onednn_fuse_pass.cc", "paddle/fluid/inference/tensorrt/convert/preln_layernorm_shift_partition_op.cc"]}, {"commit_id": "54dd19be8a7a27ce5660ab8cfc3d381fb4114b92", "commit_date": "Thu Oct 27 10:42:38 2022 +0800", "commit_message": "[Docs]fix return_type issue (#47371)", "files_name": ["python/paddle/tensor/manipulation.py"]}, {"commit_id": "d17d0cd10296a34c3f2b21840c7f3d91fe417187", "commit_date": "Wed Oct 26 20:22:04 2022 +0800", "commit_message": "Preln_Layernorm_Shift_Partition (#47099)", "files_name": ["paddle/fluid/framework/ir/CMakeLists.txt", "paddle/fluid/framework/ir/preln_layernorm_x_fuse_pass.cc", "paddle/fluid/framework/ir/preln_layernorm_x_fuse_pass.h", "paddle/fluid/inference/api/analysis_predictor.cc", "paddle/fluid/inference/api/paddle_pass_builder.cc", "paddle/fluid/inference/tensorrt/convert/CMakeLists.txt", "paddle/fluid/inference/tensorrt/convert/preln_layernorm_shift_partition_op.cc", "paddle/fluid/inference/tensorrt/op_teller.cc", "paddle/fluid/inference/tensorrt/plugin/CMakeLists.txt", "paddle/fluid/inference/tensorrt/plugin/prelnlayernorm_shift_partition_op.cu", "paddle/fluid/inference/tensorrt/plugin/prelnlayernorm_shift_partition_op.h", "python/paddle/fluid/tests/unittests/ir/inference/CMakeLists.txt", "python/paddle/fluid/tests/unittests/ir/inference/test_preln_layernorm_x_fuse_pass.py"]}, {"commit_id": "c1c2be2da42b8a74cfda96ea552d564fc51388d0", "commit_date": "Wed Oct 26 13:27:57 2022 +0200", "commit_message": "FC/matmul(v2) + scale fuse pass (#47127)", "files_name": ["paddle/fluid/framework/ir/CMakeLists.txt", "paddle/fluid/framework/ir/mkldnn/operator_scale_onednn_fuse_pass.cc", "paddle/fluid/framework/ir/mkldnn/operator_scale_onednn_fuse_pass.h", "paddle/fluid/inference/api/paddle_pass_builder.cc", "paddle/fluid/operators/mkldnn/fc_mkldnn_op.cc", "paddle/fluid/platform/mkldnn_reuse.h", "python/paddle/fluid/tests/unittests/ir/inference/test_mkldnn_matmul_activation_fuse_pass.py", "python/paddle/fluid/tests/unittests/ir/inference/test_mkldnn_matmul_v2_activation_fuse_pass.py", "python/paddle/fluid/tests/unittests/ir/inference/test_onednn_fc_activation_fuse_pass.py"]}, {"commit_id": "d78dd7ea82fdd9572ad277dd8fcaa91e6b0ee35a", "commit_date": "Wed Oct 26 19:07:13 2022 +0800", "commit_message": "[MKLDNN] Delete mkldnn hard code of prior_box (#47068)", "files_name": ["paddle/fluid/operators/detection/prior_box_op.cc", "paddle/fluid/operators/detection/prior_box_op.h", "paddle/fluid/platform/mkldnn_op_list.h", "paddle/phi/core/dense_tensor.cc"]}, {"commit_id": "40ce7f4af6c042cbbdff830350f1e9f384bf1537", "commit_date": "Wed Oct 26 02:58:38 2022 -0700", "commit_message": "Wandb callback (#46918)", "files_name": []}], "windows_after": [{"commit_id": "315ef26505c4c1b5bf534b5ff9c8643fcce57f54", "commit_date": "Fri Oct 28 20:08:03 2022 +0800", "commit_message": "[AutoParallel] fix engine _build and cost method (#47263)", "files_name": ["python/paddle/distributed/auto_parallel/cost/comp_op_cost.py", "python/paddle/distributed/auto_parallel/cost/estimate_cost.py", "python/paddle/distributed/auto_parallel/engine.py", "python/paddle/distributed/auto_parallel/utils.py", "python/paddle/fluid/tests/unittests/auto_parallel/CMakeLists.txt", "python/paddle/fluid/tests/unittests/auto_parallel/engine_api.py", "python/paddle/fluid/tests/unittests/auto_parallel/test_engine_api_error.py"]}, {"commit_id": "e77c062ef1d2503fb1dd4139821488c5e7441701", "commit_date": "Fri Oct 28 21:19:12 2022 +0800", "commit_message": "[Dygraph] Finish fixing mem bugs of no sync in DataParallel (#47444)", "files_name": ["paddle/fluid/distributed/collective/reducer.cc", "paddle/fluid/distributed/collective/reducer.h", "paddle/fluid/imperative/reducer.cc", "paddle/fluid/imperative/reducer.h", "paddle/fluid/pybind/distributed_py.cc", "paddle/fluid/pybind/imperative.cc", "python/paddle/fluid/dygraph/parallel.py"]}, {"commit_id": "17fb92b355a7f8d0f505c3221087f69d16571f94", "commit_date": "Fri Oct 28 21:39:47 2022 +0800", "commit_message": "generate static graph code for some ops by yaml (#47416)", "files_name": ["paddle/fluid/operators/angle_op.cc", "paddle/fluid/operators/argsort_op.cc", "paddle/fluid/operators/bmm_op.cc", "paddle/fluid/operators/bmm_op.h", "paddle/fluid/operators/determinant_op.cc", "paddle/phi/api/yaml/backward.yaml", "paddle/phi/api/yaml/generator/templates/operator_utils.c.j2", "paddle/phi/api/yaml/legacy_backward.yaml", "paddle/phi/api/yaml/legacy_ops.yaml", "paddle/phi/api/yaml/op_compat.yaml", "paddle/phi/api/yaml/ops.yaml", "paddle/phi/kernels/cpu/angle_grad_kernel.cc", "paddle/phi/kernels/gpu/angle_grad_kernel.cu", "paddle/phi/ops/compat/angle_sig.cc", "paddle/phi/ops/compat/argsort_sig.cc", "paddle/phi/ops/compat/bmm_sig.cc", "paddle/phi/ops/compat/determinant_sig.cc"]}, {"commit_id": "c036c5c0b9f28bcd7a48592b9f5dc78046837924", "commit_date": "Fri Oct 28 22:45:35 2022 +0800", "commit_message": "Add fused_allreduce_gradients_with_group for PPFleetX (#47447)", "files_name": ["python/paddle/distributed/fleet/utils/hybrid_parallel_util.py"]}, {"commit_id": "67ca9d45879f24bc974191dbb01b6d9c1069c833", "commit_date": "Sat Oct 29 22:58:04 2022 +0800", "commit_message": "[INCUBATE] Add dist save/load for sharding stage2 (#46908)", "files_name": ["python/paddle/fluid/tests/unittests/collective/fleet/CMakeLists.txt", "python/paddle/fluid/tests/unittests/collective/fleet/dygraph_dist_save_load.py", "python/paddle/fluid/tests/unittests/collective/fleet/test_dygraph_dist_save_load.py", "python/paddle/fluid/tests/unittests/collective/fleet/testslist.csv", "python/paddle/incubate/distributed/utils/io/__init__.py", "python/paddle/incubate/distributed/utils/io/dist_load.py", "python/paddle/incubate/distributed/utils/io/dist_save.py", "python/setup.py.in"]}, {"commit_id": "605b3f98636918bfbc9d9480aafa9bd973b580fa", "commit_date": "Sun Oct 30 23:18:54 2022 +0800", "commit_message": "Fix gen cmake (#47457)", "files_name": ["python/paddle/fluid/tests/unittests/collective/fleet/testslist.csv", "tools/gen_ut_cmakelists.py"]}, {"commit_id": "2b6bccc51a430cc46db3f8269d93b0ce56fc2e7f", "commit_date": "Mon Oct 31 09:49:56 2022 +0800", "commit_message": "Fix the problem of printing log (#47474)", "files_name": ["tools/handle_h_cu_file.py"]}, {"commit_id": "31b677bda2f4938806e54682cd65bd0c77b95f14", "commit_date": "Mon Oct 31 09:56:51 2022 +0800", "commit_message": "apply new precise_card_test to coverage_ci (#47473)", "files_name": ["tools/get_pr_ut.py"]}, {"commit_id": "1e2a371cbb52087d9e2b3b9641d4840e114aa957", "commit_date": "Mon Oct 31 09:58:37 2022 +0800", "commit_message": "repair log bugs that keeps printing warnings (#47467)", "files_name": ["python/paddle/fluid/contrib/slim/quantization/post_training_quantization.py"]}, {"commit_id": "91096ae22ca91e8da1a90d652f2fa09f17ec7fe3", "commit_date": "Mon Oct 31 10:32:04 2022 +0800", "commit_message": "remove boost compiler flags in flags.cmake (#47468)", "files_name": ["cmake/flags.cmake", "paddle/fluid/operators/dgc_op.h", "paddle/fluid/operators/mlu/mlu_baseop.cc", "paddle/fluid/operators/optimizers/dgc_momentum_op.h", "paddle/fluid/operators/reduce_ops/logsumexp_op_xpu.cc", "paddle/phi/kernels/cpu/layer_norm_kernel.cc", "paddle/phi/kernels/impl/logsumexp_grad_kernel_impl.h", "paddle/phi/kernels/impl/logsumexp_kernel_impl.h"]}, {"commit_id": "d4b68dafd3ca54b0e16d5b23261b849cdc42395f", "commit_date": "Mon Oct 31 10:43:40 2022 +0800", "commit_message": "[audio] rm kaiser window in audio get_window function && rm audio utils (#47469)", "files_name": ["python/paddle/audio/functional/window.py", "python/paddle/audio/utils/__init__.py", "python/paddle/audio/utils/error.py", "python/paddle/tests/test_audio_functions.py"]}, {"commit_id": "81b93ebbc3c6e772eda9623b79c9d04365b88100", "commit_date": "Mon Oct 31 10:58:56 2022 +0800", "commit_message": "fix python module not found bug (#47438)", "files_name": ["paddle/scripts/paddle_build.sh", "python/paddle/fluid/tests/unittests/__init__.py"]}, {"commit_id": "c8fc33798a1a28354b90d78feac181000b96451b", "commit_date": "Mon Oct 31 11:01:16 2022 +0800", "commit_message": "[Zero-Dim] support input 0D Tensor for reduce_sum/reduce_mean (#47219)", "files_name": ["paddle/phi/infermeta/unary.cc", "paddle/phi/kernels/funcs/broadcast_function.h", "paddle/phi/kernels/funcs/reduce_function.h", "paddle/phi/kernels/gpu/reduce_mean_grad_kernel.cu", "paddle/phi/kernels/gpu/reduce_sum_grad_kernel.cu", "paddle/phi/kernels/reduce_mean_kernel.cc", "python/paddle/fluid/layers/nn.py", "python/paddle/fluid/tests/unittests/test_mean_op.py", "python/paddle/fluid/tests/unittests/test_reduce_op.py", "python/paddle/fluid/tests/unittests/test_zero_dim_shape.py", "python/paddle/tensor/math.py", "python/paddle/tensor/stat.py"]}, {"commit_id": "f5912d0c7ee3f73183e9801fd0bbcfe48a5d22e3", "commit_date": "Mon Oct 31 11:15:50 2022 +0800", "commit_message": "fix typos for `True` and `False` (#47477)", "files_name": ["paddle/fluid/framework/naive_executor.h", "paddle/fluid/memory/allocation/allocator_facade.cc", "paddle/fluid/operators/detection/yolo_box_op.cc", "paddle/fluid/operators/fused/fused_dropout_helper.h", "paddle/fluid/operators/select_op_helper.h", "paddle/fluid/operators/tensorrt/tensorrt_engine_op.h", "paddle/fluid/operators/unique_op.cc", "python/paddle/fluid/contrib/sparsity/asp.py", "python/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py", "python/paddle/fluid/dygraph/nn.py", "python/paddle/fluid/layers/control_flow.py", "python/paddle/fluid/layers/nn.py", "python/paddle/nn/layer/norm.py", "python/paddle/sparse/nn/layer/norm.py", "python/paddle/vision/ops.py"]}, {"commit_id": "bb6356e884a24a5d583b9053be02e97d135eb865", "commit_date": "Mon Oct 31 11:32:27 2022 +0800", "commit_message": "[MLU] fix compile error & add mlu blacklist function. (#47439)", "files_name": ["paddle/fluid/imperative/prepared_operator.cc", "paddle/fluid/operators/strided_slice_op_mlu.cc"]}, {"commit_id": "3b219e5ea393a65622750e705f978113a40b0f2e", "commit_date": "Mon Oct 31 11:53:55 2022 +0800", "commit_message": "[ControlFlow] replace executor in run method of control flow ops with standalone_executor (#45696)", "files_name": ["paddle/fluid/framework/new_executor/interpreter/execution_config.h", "paddle/fluid/framework/new_executor/interpreter/interpreter_util.cc", "paddle/fluid/framework/new_executor/interpreter/interpreter_util.h", "paddle/fluid/framework/new_executor/interpretercore.cc", "paddle/fluid/framework/new_executor/interpretercore.h", "paddle/fluid/framework/new_executor/new_executor_defs.h", "paddle/fluid/framework/new_executor/standalone_executor.cc", "paddle/fluid/operators/controlflow/CMakeLists.txt", "paddle/fluid/operators/controlflow/conditional_block_op.cc", "paddle/phi/kernels/transfer_layout_kernel.cc"]}, {"commit_id": "b03b4a3c30be813cc26a1ea66c57dbae419583d9", "commit_date": "Mon Oct 31 12:54:38 2022 +0800", "commit_message": "[Auto Parallel] Improve the c++ dist attr (#47358)", "files_name": ["paddle/fluid/distributed/auto_parallel/dist_attr.cc", "paddle/fluid/distributed/auto_parallel/dist_attr.h", "paddle/fluid/distributed/auto_parallel/test/dist_attr_test.cc", "paddle/fluid/distributed/auto_parallel/utils.h", "paddle/fluid/framework/attribute.cc", "paddle/fluid/framework/attribute.h", "paddle/fluid/framework/op_desc.cc", "paddle/fluid/framework/var_desc.cc", "paddle/fluid/framework/var_desc.h", "paddle/fluid/pybind/auto_parallel_py.cc", "python/paddle/distributed/auto_parallel/dist_context.py", "python/paddle/distributed/auto_parallel/utils.py", "python/paddle/fluid/framework.py", "python/paddle/fluid/tests/unittests/auto_parallel/CMakeLists.txt", "python/paddle/fluid/tests/unittests/auto_parallel/test_dist_attr_v2.py", "python/paddle/fluid/tests/unittests/auto_parallel/test_serialization.py", "python/paddle/fluid/tests/unittests/test_program.py"]}, {"commit_id": "520adc0e4962c544af1b2f5327b1be71c397c571", "commit_date": "Mon Oct 31 13:00:35 2022 +0800", "commit_message": "optimize: vit 384 (#47432)", "files_name": ["paddle/fluid/inference/tensorrt/plugin/qkv_to_context_plugin.cu", "paddle/fluid/operators/fused/multihead_matmul_op.cu", "paddle/fluid/operators/math/bert_encoder_functor.cu", "paddle/fluid/operators/math/bert_encoder_functor.h"]}, {"commit_id": "34d13d6abb4ddf9ef407a61f3efc1894989d8bfa", "commit_date": "Mon Oct 31 14:06:28 2022 +0800", "commit_message": "[CustomDevice] GetCCLComm add custom device support (#47168)", "files_name": ["paddle/fluid/distributed/collective/CMakeLists.txt", "paddle/fluid/distributed/collective/ProcessGroupCustom.cc", "paddle/fluid/distributed/collective/ProcessGroupCustom.h", "paddle/phi/backends/CMakeLists.txt", "paddle/phi/backends/processgroup_comm_utils.cc", "paddle/phi/kernels/CMakeLists.txt", "paddle/phi/kernels/gpu/sync_batch_norm_kernel.cu"]}, {"commit_id": "2953b708a03d023b6b6b1fecde7ac431f8f48a94", "commit_date": "Mon Oct 31 14:23:40 2022 +0800", "commit_message": "feat: add int8 support for vit (#47330)", "files_name": ["paddle/fluid/framework/ir/vit_attention_fuse_pass.cc", "paddle/fluid/inference/tensorrt/convert/multihead_matmul_op.cc", "python/paddle/fluid/tests/unittests/ir/inference/test_trt_convert_multihead_matmul.py"]}, {"commit_id": "de4a79119fd5a063d6e1acccfa87f9db01462a30", "commit_date": "Mon Oct 31 17:14:01 2022 +0800", "commit_message": "fix predictor memory write overflow (#47485)", "files_name": ["paddle/fluid/inference/api/analysis_predictor.cc", "paddle/fluid/inference/api/api_impl.cc", "paddle/fluid/inference/tests/api/analyzer_transformer_tester_helper.h"]}, {"commit_id": "266283b21d76e1a5d4e36a44447befc103972ccb", "commit_date": "Mon Oct 31 17:40:02 2022 +0800", "commit_message": "remove postprocess in dygraph ptq export (#47487)", "files_name": ["python/paddle/fluid/contrib/slim/quantization/imperative/ptq.py"]}, {"commit_id": "6e1c14e357e4dc88a6c484cc79529aff3d8911c7", "commit_date": "Mon Oct 31 17:45:56 2022 +0800", "commit_message": "[Einsum] Einsum support repeated labels. (#47290)", "files_name": ["paddle/phi/infermeta/unary.cc", "paddle/phi/kernels/cpu/diagonal_grad_kernel.cc", "paddle/phi/kernels/cpu/diagonal_kernel.cc", "paddle/phi/kernels/diagonal_kernel.h", "paddle/phi/kernels/fill_diagonal_tensor_kernel.h", "paddle/phi/kernels/gpu/diagonal_grad_kernel.cu", "paddle/phi/kernels/gpu/diagonal_kernel.cu", "paddle/phi/kernels/gpu/fill_diagonal_tensor_grad_kernel.cu", "paddle/phi/kernels/gpu/fill_diagonal_tensor_kernel.cu", "paddle/phi/kernels/impl/einsum_grad_impl.h", "paddle/phi/kernels/impl/einsum_impl.h", "python/paddle/fluid/tests/unittests/test_einsum_op.py", "python/paddle/fluid/tests/unittests/test_einsum_v2.py", "python/paddle/tensor/einsum.py"]}, {"commit_id": "60e0c506a55820084481890d6266cca9e822af83", "commit_date": "Mon Oct 31 18:37:56 2022 +0800", "commit_message": "[PHI]Standardise some C++ API (#47385)", "files_name": ["paddle/fluid/operators/activation_op.h", "paddle/fluid/operators/activation_op.kps", "paddle/fluid/operators/crop_tensor_op.cc"]}], "parents": [{"commit_id_before": "0f649b32397dcd043e51b414904ccfa730b52603", "url_before": "https://api.github.com/repos/PaddlePaddle/Paddle/commits/0f649b32397dcd043e51b414904ccfa730b52603", "html_url_before": "https://github.com/PaddlePaddle/Paddle/commit/0f649b32397dcd043e51b414904ccfa730b52603"}], "details": [{"raw_url": "https://github.com/PaddlePaddle/Paddle/raw/26c419ca386aeae3c461faf2b828d00b48e908eb/python%2Fpaddle%2Faudio%2Ffunctional%2Fwindow.py", "code": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\nimport math\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Union\n\nimport paddle\nfrom paddle import Tensor\n\n\nclass WindowFunctionRegister(object):\n    def __init__(self):\n        self._functions_dict = dict()\n\n    def register(self, func=None):\n        def add_subfunction(func):\n            name = func.__name__\n            self._functions_dict[name] = func\n            return func\n\n        return add_subfunction\n\n    def get(self, name):\n        return self._functions_dict[name]\n\n\nwindow_function_register = WindowFunctionRegister()\n\n\n@window_function_register.register()\ndef _cat(x: List[Tensor], data_type: str) -> Tensor:\n    l = [paddle.to_tensor(_, data_type) for _ in x]\n    return paddle.concat(l)\n\n\n@window_function_register.register()\ndef _acosh(x: Union[Tensor, float]) -> Tensor:\n    if isinstance(x, float):\n        return math.log(x + math.sqrt(x**2 - 1))\n    return paddle.log(x + paddle.sqrt(paddle.square(x) - 1))\n\n\n@window_function_register.register()\ndef _extend(M: int, sym: bool) -> bool:\n    \"\"\"Extend window by 1 sample if needed for DFT-even symmetry.\"\"\"\n    if not sym:\n        return M + 1, True\n    else:\n        return M, False\n\n\n@window_function_register.register()\ndef _len_guards(M: int) -> bool:\n    \"\"\"Handle small or incorrect window lengths.\"\"\"\n    if int(M) != M or M < 0:\n        raise ValueError('Window length M must be a non-negative integer')\n\n    return M <= 1\n\n\n@window_function_register.register()\ndef _truncate(w: Tensor, needed: bool) -> Tensor:\n    \"\"\"Truncate window by 1 sample if needed for DFT-even symmetry.\"\"\"\n    if needed:\n        return w[:-1]\n    else:\n        return w\n\n\n@window_function_register.register()\ndef _general_gaussian(\n    M: int, p, sig, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a window with a generalized Gaussian shape.\n    This function is consistent with scipy.signal.windows.general_gaussian().\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n\n    n = paddle.arange(0, M, dtype=dtype) - (M - 1.0) / 2.0\n    w = paddle.exp(-0.5 * paddle.abs(n / sig) ** (2 * p))\n\n    return _truncate(w, needs_trunc)\n\n\n@window_function_register.register()\ndef _general_cosine(\n    M: int, a: float, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a generic weighted sum of cosine terms window.\n    This function is consistent with scipy.signal.windows.general_cosine().\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n    fac = paddle.linspace(-math.pi, math.pi, M, dtype=dtype)\n    w = paddle.zeros((M,), dtype=dtype)\n    for k in range(len(a)):\n        w += a[k] * paddle.cos(k * fac)\n    return _truncate(w, needs_trunc)\n\n\n@window_function_register.register()\ndef _general_hamming(\n    M: int, alpha: float, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a generalized Hamming window.\n    This function is consistent with scipy.signal.windows.general_hamming()\n    \"\"\"\n    return _general_cosine(M, [alpha, 1.0 - alpha], sym, dtype=dtype)\n\n\n@window_function_register.register()\ndef _taylor(\n    M: int, nbar=4, sll=30, norm=True, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a Taylor window.\n    The Taylor window taper function approximates the Dolph-Chebyshev window's\n    constant sidelobe level for a parameterized number of near-in sidelobes.\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n    # Original text uses a negative sidelobe level parameter and then negates\n    # it in the calculation of B. To keep consistent with other methods we\n    # assume the sidelobe level parameter to be positive.\n    B = 10 ** (sll / 20)\n    A = _acosh(B) / math.pi\n    s2 = nbar**2 / (A**2 + (nbar - 0.5) ** 2)\n    ma = paddle.arange(1, nbar, dtype=dtype)\n\n    Fm = paddle.empty((nbar - 1,), dtype=dtype)\n    signs = paddle.empty_like(ma)\n    signs[::2] = 1\n    signs[1::2] = -1\n    m2 = ma * ma\n    for mi in range(len(ma)):\n        numer = signs[mi] * paddle.prod(\n            1 - m2[mi] / s2 / (A**2 + (ma - 0.5) ** 2)\n        )\n        if mi == 0:\n            denom = 2 * paddle.prod(1 - m2[mi] / m2[mi + 1 :])\n        elif mi == len(ma) - 1:\n            denom = 2 * paddle.prod(1 - m2[mi] / m2[:mi])\n        else:\n            denom = (\n                2\n                * paddle.prod(1 - m2[mi] / m2[:mi])\n                * paddle.prod(1 - m2[mi] / m2[mi + 1 :])\n            )\n\n        Fm[mi] = numer / denom\n\n    def W(n):\n        return 1 + 2 * paddle.matmul(\n            Fm.unsqueeze(0),\n            paddle.cos(2 * math.pi * ma.unsqueeze(1) * (n - M / 2.0 + 0.5) / M),\n        )\n\n    w = W(paddle.arange(0, M, dtype=dtype))\n\n    # normalize (Note that this is not described in the original text [1])\n    if norm:\n        scale = 1.0 / W((M - 1) / 2)\n        w *= scale\n    w = w.squeeze()\n    return _truncate(w, needs_trunc)\n\n\n@window_function_register.register()\ndef _hamming(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a Hamming window.\n    The Hamming window is a taper formed by using a raised cosine with\n    non-zero endpoints, optimized to minimize the nearest side lobe.\n    \"\"\"\n    return _general_hamming(M, 0.54, sym, dtype=dtype)\n\n\n@window_function_register.register()\ndef _hann(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a Hann window.\n    The Hann window is a taper formed by using a raised cosine or sine-squared\n    with ends that touch zero.\n    \"\"\"\n    return _general_hamming(M, 0.5, sym, dtype=dtype)\n\n\n@window_function_register.register()\ndef _tukey(\n    M: int, alpha=0.5, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a Tukey window.\n    The Tukey window is also known as a tapered cosine window.\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n\n    if alpha <= 0:\n        return paddle.ones((M,), dtype=dtype)\n    elif alpha >= 1.0:\n        return hann(M, sym=sym)\n\n    M, needs_trunc = _extend(M, sym)\n\n    n = paddle.arange(0, M, dtype=dtype)\n    width = int(alpha * (M - 1) / 2.0)\n    n1 = n[0 : width + 1]\n    n2 = n[width + 1 : M - width - 1]\n    n3 = n[M - width - 1 :]\n\n    w1 = 0.5 * (1 + paddle.cos(math.pi * (-1 + 2.0 * n1 / alpha / (M - 1))))\n    w2 = paddle.ones(n2.shape, dtype=dtype)\n    w3 = 0.5 * (\n        1\n        + paddle.cos(math.pi * (-2.0 / alpha + 1 + 2.0 * n3 / alpha / (M - 1)))\n    )\n    w = paddle.concat([w1, w2, w3])\n\n    return _truncate(w, needs_trunc)\n\n\n@window_function_register.register()\ndef _kaiser(\n    M: int, beta: float, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a Kaiser window.\n    The Kaiser window is a taper formed by using a Bessel function.\n    \"\"\"\n    raise NotImplementedError()\n\n\n@window_function_register.register()\ndef _gaussian(\n    M: int, std: float, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a Gaussian window.\n    The Gaussian widows has a Gaussian shape defined by the standard deviation(std).\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n\n    n = paddle.arange(0, M, dtype=dtype) - (M - 1.0) / 2.0\n    sig2 = 2 * std * std\n    w = paddle.exp(-(n**2) / sig2)\n\n    return _truncate(w, needs_trunc)\n\n\n@window_function_register.register()\ndef _exponential(\n    M: int, center=None, tau=1.0, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute an exponential (or Poisson) window.\"\"\"\n    if sym and center is not None:\n        raise ValueError(\"If sym==True, center must be None.\")\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n\n    if center is None:\n        center = (M - 1) / 2\n\n    n = paddle.arange(0, M, dtype=dtype)\n    w = paddle.exp(-paddle.abs(n - center) / tau)\n\n    return _truncate(w, needs_trunc)\n\n\n@window_function_register.register()\ndef _triang(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a triangular window.\"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n\n    n = paddle.arange(1, (M + 1) // 2 + 1, dtype=dtype)\n    if M % 2 == 0:\n        w = (2 * n - 1.0) / M\n        w = paddle.concat([w, w[::-1]])\n    else:\n        w = 2 * n / (M + 1.0)\n        w = paddle.concat([w, w[-2::-1]])\n\n    return _truncate(w, needs_trunc)\n\n\n@window_function_register.register()\ndef _bohman(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a Bohman window.\n    The Bohman window is the autocorrelation of a cosine window.\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n\n    fac = paddle.abs(paddle.linspace(-1, 1, M, dtype=dtype)[1:-1])\n    w = (1 - fac) * paddle.cos(math.pi * fac) + 1.0 / math.pi * paddle.sin(\n        math.pi * fac\n    )\n    w = _cat([0, w, 0], dtype)\n\n    return _truncate(w, needs_trunc)\n\n\n@window_function_register.register()\ndef _blackman(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a Blackman window.\n    The Blackman window is a taper formed by using the first three terms of\n    a summation of cosines. It was designed to have close to the minimal\n    leakage possible.  It is close to optimal, only slightly worse than a\n    Kaiser window.\n    \"\"\"\n    return _general_cosine(M, [0.42, 0.50, 0.08], sym, dtype=dtype)\n\n\n@window_function_register.register()\ndef _cosine(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a window with a simple cosine shape.\"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n    w = paddle.sin(math.pi / M * (paddle.arange(0, M, dtype=dtype) + 0.5))\n\n    return _truncate(w, needs_trunc)\n\n\ndef get_window(\n    window: Union[str, Tuple[str, float]],\n    win_length: int,\n    fftbins: bool = True,\n    dtype: str = 'float64',\n) -> Tensor:\n    \"\"\"Return a window of a given length and type.\n\n    Args:\n        window (Union[str, Tuple[str, float]]): The window function applied to the signal before the Fourier transform. Supported window functions: 'hamming', 'hann', 'kaiser', 'gaussian', 'general_gaussian', 'exponential', 'triang', 'bohman', 'blackman', 'cosine', 'tukey', 'taylor'.\n        win_length (int): Number of samples.\n        fftbins (bool, optional): If True, create a \"periodic\" window. Otherwise, create a \"symmetric\" window, for use in filter design. Defaults to True.\n        dtype (str, optional): The data type of the return window. Defaults to 'float64'.\n\n    Returns:\n        Tensor: The window represented as a tensor.\n\n    Examples:\n        .. code-block:: python\n\n            import paddle\n\n            n_fft = 512\n            cosine_window = paddle.audio.functional.get_window('cosine', n_fft)\n\n            std = 7\n            gussian_window = paddle.audio.functional.get_window(('gaussian',std), n_fft)\n    \"\"\"\n    sym = not fftbins\n\n    args = ()\n    if isinstance(window, tuple):\n        winstr = window[0]\n        if len(window) > 1:\n            args = window[1:]\n    elif isinstance(window, str):\n        if window in ['gaussian', 'exponential']:\n            raise ValueError(\n                \"The '\" + window + \"' window needs one or \"\n                \"more parameters -- pass a tuple.\"\n            )\n        else:\n            winstr = window\n    else:\n        raise ValueError(\n            \"%s as window type is not supported.\" % str(type(window))\n        )\n\n    try:\n        winfunc = window_function_register.get('_' + winstr)\n    except KeyError as e:\n        raise ValueError(\"Unknown window type.\") from e\n\n    params = (win_length,) + args\n    kwargs = {'sym': sym}\n    return winfunc(*params, dtype=dtype, **kwargs)\n", "code_before": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\nimport math\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Union\n\nimport paddle\nfrom paddle import Tensor\n\n\ndef _cat(x: List[Tensor], data_type: str) -> Tensor:\n    l = [paddle.to_tensor(_, data_type) for _ in x]\n    return paddle.concat(l)\n\n\ndef _acosh(x: Union[Tensor, float]) -> Tensor:\n    if isinstance(x, float):\n        return math.log(x + math.sqrt(x**2 - 1))\n    return paddle.log(x + paddle.sqrt(paddle.square(x) - 1))\n\n\ndef _extend(M: int, sym: bool) -> bool:\n    \"\"\"Extend window by 1 sample if needed for DFT-even symmetry.\"\"\"\n    if not sym:\n        return M + 1, True\n    else:\n        return M, False\n\n\ndef _len_guards(M: int) -> bool:\n    \"\"\"Handle small or incorrect window lengths.\"\"\"\n    if int(M) != M or M < 0:\n        raise ValueError('Window length M must be a non-negative integer')\n\n    return M <= 1\n\n\ndef _truncate(w: Tensor, needed: bool) -> Tensor:\n    \"\"\"Truncate window by 1 sample if needed for DFT-even symmetry.\"\"\"\n    if needed:\n        return w[:-1]\n    else:\n        return w\n\n\ndef _general_gaussian(\n    M: int, p, sig, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a window with a generalized Gaussian shape.\n    This function is consistent with scipy.signal.windows.general_gaussian().\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n\n    n = paddle.arange(0, M, dtype=dtype) - (M - 1.0) / 2.0\n    w = paddle.exp(-0.5 * paddle.abs(n / sig) ** (2 * p))\n\n    return _truncate(w, needs_trunc)\n\n\ndef _general_cosine(\n    M: int, a: float, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a generic weighted sum of cosine terms window.\n    This function is consistent with scipy.signal.windows.general_cosine().\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n    fac = paddle.linspace(-math.pi, math.pi, M, dtype=dtype)\n    w = paddle.zeros((M,), dtype=dtype)\n    for k in range(len(a)):\n        w += a[k] * paddle.cos(k * fac)\n    return _truncate(w, needs_trunc)\n\n\ndef _general_hamming(\n    M: int, alpha: float, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a generalized Hamming window.\n    This function is consistent with scipy.signal.windows.general_hamming()\n    \"\"\"\n    return _general_cosine(M, [alpha, 1.0 - alpha], sym, dtype=dtype)\n\n\ndef _taylor(\n    M: int, nbar=4, sll=30, norm=True, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a Taylor window.\n    The Taylor window taper function approximates the Dolph-Chebyshev window's\n    constant sidelobe level for a parameterized number of near-in sidelobes.\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n    # Original text uses a negative sidelobe level parameter and then negates\n    # it in the calculation of B. To keep consistent with other methods we\n    # assume the sidelobe level parameter to be positive.\n    B = 10 ** (sll / 20)\n    A = _acosh(B) / math.pi\n    s2 = nbar**2 / (A**2 + (nbar - 0.5) ** 2)\n    ma = paddle.arange(1, nbar, dtype=dtype)\n\n    Fm = paddle.empty((nbar - 1,), dtype=dtype)\n    signs = paddle.empty_like(ma)\n    signs[::2] = 1\n    signs[1::2] = -1\n    m2 = ma * ma\n    for mi in range(len(ma)):\n        numer = signs[mi] * paddle.prod(\n            1 - m2[mi] / s2 / (A**2 + (ma - 0.5) ** 2)\n        )\n        if mi == 0:\n            denom = 2 * paddle.prod(1 - m2[mi] / m2[mi + 1 :])\n        elif mi == len(ma) - 1:\n            denom = 2 * paddle.prod(1 - m2[mi] / m2[:mi])\n        else:\n            denom = (\n                2\n                * paddle.prod(1 - m2[mi] / m2[:mi])\n                * paddle.prod(1 - m2[mi] / m2[mi + 1 :])\n            )\n\n        Fm[mi] = numer / denom\n\n    def W(n):\n        return 1 + 2 * paddle.matmul(\n            Fm.unsqueeze(0),\n            paddle.cos(2 * math.pi * ma.unsqueeze(1) * (n - M / 2.0 + 0.5) / M),\n        )\n\n    w = W(paddle.arange(0, M, dtype=dtype))\n\n    # normalize (Note that this is not described in the original text [1])\n    if norm:\n        scale = 1.0 / W((M - 1) / 2)\n        w *= scale\n    w = w.squeeze()\n    return _truncate(w, needs_trunc)\n\n\ndef _hamming(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a Hamming window.\n    The Hamming window is a taper formed by using a raised cosine with\n    non-zero endpoints, optimized to minimize the nearest side lobe.\n    \"\"\"\n    return _general_hamming(M, 0.54, sym, dtype=dtype)\n\n\ndef _hann(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a Hann window.\n    The Hann window is a taper formed by using a raised cosine or sine-squared\n    with ends that touch zero.\n    \"\"\"\n    return _general_hamming(M, 0.5, sym, dtype=dtype)\n\n\ndef _tukey(\n    M: int, alpha=0.5, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a Tukey window.\n    The Tukey window is also known as a tapered cosine window.\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n\n    if alpha <= 0:\n        return paddle.ones((M,), dtype=dtype)\n    elif alpha >= 1.0:\n        return hann(M, sym=sym)\n\n    M, needs_trunc = _extend(M, sym)\n\n    n = paddle.arange(0, M, dtype=dtype)\n    width = int(alpha * (M - 1) / 2.0)\n    n1 = n[0 : width + 1]\n    n2 = n[width + 1 : M - width - 1]\n    n3 = n[M - width - 1 :]\n\n    w1 = 0.5 * (1 + paddle.cos(math.pi * (-1 + 2.0 * n1 / alpha / (M - 1))))\n    w2 = paddle.ones(n2.shape, dtype=dtype)\n    w3 = 0.5 * (\n        1\n        + paddle.cos(math.pi * (-2.0 / alpha + 1 + 2.0 * n3 / alpha / (M - 1)))\n    )\n    w = paddle.concat([w1, w2, w3])\n\n    return _truncate(w, needs_trunc)\n\n\ndef _kaiser(\n    M: int, beta: float, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a Kaiser window.\n    The Kaiser window is a taper formed by using a Bessel function.\n    \"\"\"\n    raise NotImplementedError()\n\n\ndef _gaussian(\n    M: int, std: float, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute a Gaussian window.\n    The Gaussian widows has a Gaussian shape defined by the standard deviation(std).\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n\n    n = paddle.arange(0, M, dtype=dtype) - (M - 1.0) / 2.0\n    sig2 = 2 * std * std\n    w = paddle.exp(-(n**2) / sig2)\n\n    return _truncate(w, needs_trunc)\n\n\ndef _exponential(\n    M: int, center=None, tau=1.0, sym: bool = True, dtype: str = 'float64'\n) -> Tensor:\n    \"\"\"Compute an exponential (or Poisson) window.\"\"\"\n    if sym and center is not None:\n        raise ValueError(\"If sym==True, center must be None.\")\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n\n    if center is None:\n        center = (M - 1) / 2\n\n    n = paddle.arange(0, M, dtype=dtype)\n    w = paddle.exp(-paddle.abs(n - center) / tau)\n\n    return _truncate(w, needs_trunc)\n\n\ndef _triang(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a triangular window.\"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n\n    n = paddle.arange(1, (M + 1) // 2 + 1, dtype=dtype)\n    if M % 2 == 0:\n        w = (2 * n - 1.0) / M\n        w = paddle.concat([w, w[::-1]])\n    else:\n        w = 2 * n / (M + 1.0)\n        w = paddle.concat([w, w[-2::-1]])\n\n    return _truncate(w, needs_trunc)\n\n\ndef _bohman(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a Bohman window.\n    The Bohman window is the autocorrelation of a cosine window.\n    \"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n\n    fac = paddle.abs(paddle.linspace(-1, 1, M, dtype=dtype)[1:-1])\n    w = (1 - fac) * paddle.cos(math.pi * fac) + 1.0 / math.pi * paddle.sin(\n        math.pi * fac\n    )\n    w = _cat([0, w, 0], dtype)\n\n    return _truncate(w, needs_trunc)\n\n\ndef _blackman(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a Blackman window.\n    The Blackman window is a taper formed by using the first three terms of\n    a summation of cosines. It was designed to have close to the minimal\n    leakage possible.  It is close to optimal, only slightly worse than a\n    Kaiser window.\n    \"\"\"\n    return _general_cosine(M, [0.42, 0.50, 0.08], sym, dtype=dtype)\n\n\ndef _cosine(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n    \"\"\"Compute a window with a simple cosine shape.\"\"\"\n    if _len_guards(M):\n        return paddle.ones((M,), dtype=dtype)\n    M, needs_trunc = _extend(M, sym)\n    w = paddle.sin(math.pi / M * (paddle.arange(0, M, dtype=dtype) + 0.5))\n\n    return _truncate(w, needs_trunc)\n\n\ndef get_window(\n    window: Union[str, Tuple[str, float]],\n    win_length: int,\n    fftbins: bool = True,\n    dtype: str = 'float64',\n) -> Tensor:\n    \"\"\"Return a window of a given length and type.\n\n    Args:\n        window (Union[str, Tuple[str, float]]): The window function applied to the signal before the Fourier transform. Supported window functions: 'hamming', 'hann', 'kaiser', 'gaussian', 'exponential', 'triang', 'bohman', 'blackman', 'cosine', 'tukey', 'taylor'.\n        win_length (int): Number of samples.\n        fftbins (bool, optional): If True, create a \"periodic\" window. Otherwise, create a \"symmetric\" window, for use in filter design. Defaults to True.\n        dtype (str, optional): The data type of the return window. Defaults to 'float64'.\n\n    Returns:\n        Tensor: The window represented as a tensor.\n\n    Examples:\n        .. code-block:: python\n\n            import paddle\n\n            n_fft = 512\n            cosine_window = paddle.audio.functional.get_window('cosine', n_fft)\n\n            std = 7\n            gussian_window = paddle.audio.functional.get_window(('gaussian',std), n_fft)\n    \"\"\"\n    sym = not fftbins\n\n    args = ()\n    if isinstance(window, tuple):\n        winstr = window[0]\n        if len(window) > 1:\n            args = window[1:]\n    elif isinstance(window, str):\n        if window in ['gaussian', 'exponential']:\n            raise ValueError(\n                \"The '\" + window + \"' window needs one or \"\n                \"more parameters -- pass a tuple.\"\n            )\n        else:\n            winstr = window\n    else:\n        raise ValueError(\n            \"%s as window type is not supported.\" % str(type(window))\n        )\n\n    try:\n        winfunc = eval('_' + winstr)\n    except NameError as e:\n        raise ValueError(\"Unknown window type.\") from e\n\n    params = (win_length,) + args\n    kwargs = {'sym': sym}\n    return winfunc(*params, dtype=dtype, **kwargs)\n", "patch": "@@ -19,17 +19,39 @@\n from paddle import Tensor\n \n \n+class WindowFunctionRegister(object):\n+    def __init__(self):\n+        self._functions_dict = dict()\n+\n+    def register(self, func=None):\n+        def add_subfunction(func):\n+            name = func.__name__\n+            self._functions_dict[name] = func\n+            return func\n+\n+        return add_subfunction\n+\n+    def get(self, name):\n+        return self._functions_dict[name]\n+\n+\n+window_function_register = WindowFunctionRegister()\n+\n+\n+@window_function_register.register()\n def _cat(x: List[Tensor], data_type: str) -> Tensor:\n     l = [paddle.to_tensor(_, data_type) for _ in x]\n     return paddle.concat(l)\n \n \n+@window_function_register.register()\n def _acosh(x: Union[Tensor, float]) -> Tensor:\n     if isinstance(x, float):\n         return math.log(x + math.sqrt(x**2 - 1))\n     return paddle.log(x + paddle.sqrt(paddle.square(x) - 1))\n \n \n+@window_function_register.register()\n def _extend(M: int, sym: bool) -> bool:\n     \"\"\"Extend window by 1 sample if needed for DFT-even symmetry.\"\"\"\n     if not sym:\n@@ -38,6 +60,7 @@ def _extend(M: int, sym: bool) -> bool:\n         return M, False\n \n \n+@window_function_register.register()\n def _len_guards(M: int) -> bool:\n     \"\"\"Handle small or incorrect window lengths.\"\"\"\n     if int(M) != M or M < 0:\n@@ -46,6 +69,7 @@ def _len_guards(M: int) -> bool:\n     return M <= 1\n \n \n+@window_function_register.register()\n def _truncate(w: Tensor, needed: bool) -> Tensor:\n     \"\"\"Truncate window by 1 sample if needed for DFT-even symmetry.\"\"\"\n     if needed:\n@@ -54,6 +78,7 @@ def _truncate(w: Tensor, needed: bool) -> Tensor:\n         return w\n \n \n+@window_function_register.register()\n def _general_gaussian(\n     M: int, p, sig, sym: bool = True, dtype: str = 'float64'\n ) -> Tensor:\n@@ -70,6 +95,7 @@ def _general_gaussian(\n     return _truncate(w, needs_trunc)\n \n \n+@window_function_register.register()\n def _general_cosine(\n     M: int, a: float, sym: bool = True, dtype: str = 'float64'\n ) -> Tensor:\n@@ -86,6 +112,7 @@ def _general_cosine(\n     return _truncate(w, needs_trunc)\n \n \n+@window_function_register.register()\n def _general_hamming(\n     M: int, alpha: float, sym: bool = True, dtype: str = 'float64'\n ) -> Tensor:\n@@ -95,6 +122,7 @@ def _general_hamming(\n     return _general_cosine(M, [alpha, 1.0 - alpha], sym, dtype=dtype)\n \n \n+@window_function_register.register()\n def _taylor(\n     M: int, nbar=4, sll=30, norm=True, sym: bool = True, dtype: str = 'float64'\n ) -> Tensor:\n@@ -151,6 +179,7 @@ def W(n):\n     return _truncate(w, needs_trunc)\n \n \n+@window_function_register.register()\n def _hamming(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n     \"\"\"Compute a Hamming window.\n     The Hamming window is a taper formed by using a raised cosine with\n@@ -159,6 +188,7 @@ def _hamming(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n     return _general_hamming(M, 0.54, sym, dtype=dtype)\n \n \n+@window_function_register.register()\n def _hann(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n     \"\"\"Compute a Hann window.\n     The Hann window is a taper formed by using a raised cosine or sine-squared\n@@ -167,6 +197,7 @@ def _hann(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n     return _general_hamming(M, 0.5, sym, dtype=dtype)\n \n \n+@window_function_register.register()\n def _tukey(\n     M: int, alpha=0.5, sym: bool = True, dtype: str = 'float64'\n ) -> Tensor:\n@@ -200,6 +231,7 @@ def _tukey(\n     return _truncate(w, needs_trunc)\n \n \n+@window_function_register.register()\n def _kaiser(\n     M: int, beta: float, sym: bool = True, dtype: str = 'float64'\n ) -> Tensor:\n@@ -209,6 +241,7 @@ def _kaiser(\n     raise NotImplementedError()\n \n \n+@window_function_register.register()\n def _gaussian(\n     M: int, std: float, sym: bool = True, dtype: str = 'float64'\n ) -> Tensor:\n@@ -226,6 +259,7 @@ def _gaussian(\n     return _truncate(w, needs_trunc)\n \n \n+@window_function_register.register()\n def _exponential(\n     M: int, center=None, tau=1.0, sym: bool = True, dtype: str = 'float64'\n ) -> Tensor:\n@@ -245,6 +279,7 @@ def _exponential(\n     return _truncate(w, needs_trunc)\n \n \n+@window_function_register.register()\n def _triang(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n     \"\"\"Compute a triangular window.\"\"\"\n     if _len_guards(M):\n@@ -262,6 +297,7 @@ def _triang(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n     return _truncate(w, needs_trunc)\n \n \n+@window_function_register.register()\n def _bohman(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n     \"\"\"Compute a Bohman window.\n     The Bohman window is the autocorrelation of a cosine window.\n@@ -279,6 +315,7 @@ def _bohman(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n     return _truncate(w, needs_trunc)\n \n \n+@window_function_register.register()\n def _blackman(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n     \"\"\"Compute a Blackman window.\n     The Blackman window is a taper formed by using the first three terms of\n@@ -289,6 +326,7 @@ def _blackman(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n     return _general_cosine(M, [0.42, 0.50, 0.08], sym, dtype=dtype)\n \n \n+@window_function_register.register()\n def _cosine(M: int, sym: bool = True, dtype: str = 'float64') -> Tensor:\n     \"\"\"Compute a window with a simple cosine shape.\"\"\"\n     if _len_guards(M):\n@@ -308,7 +346,7 @@ def get_window(\n     \"\"\"Return a window of a given length and type.\n \n     Args:\n-        window (Union[str, Tuple[str, float]]): The window function applied to the signal before the Fourier transform. Supported window functions: 'hamming', 'hann', 'kaiser', 'gaussian', 'exponential', 'triang', 'bohman', 'blackman', 'cosine', 'tukey', 'taylor'.\n+        window (Union[str, Tuple[str, float]]): The window function applied to the signal before the Fourier transform. Supported window functions: 'hamming', 'hann', 'kaiser', 'gaussian', 'general_gaussian', 'exponential', 'triang', 'bohman', 'blackman', 'cosine', 'tukey', 'taylor'.\n         win_length (int): Number of samples.\n         fftbins (bool, optional): If True, create a \"periodic\" window. Otherwise, create a \"symmetric\" window, for use in filter design. Defaults to True.\n         dtype (str, optional): The data type of the return window. Defaults to 'float64'.\n@@ -348,8 +386,8 @@ def get_window(\n         )\n \n     try:\n-        winfunc = eval('_' + winstr)\n-    except NameError as e:\n+        winfunc = window_function_register.get('_' + winstr)\n+    except KeyError as e:\n         raise ValueError(\"Unknown window type.\") from e\n \n     params = (win_length,) + args", "file_path": "files/2022_11/55", "file_language": "py", "file_name": "python/paddle/audio/functional/window.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 1, "static": {"rats": [true, ["/data/rdhu/other/Static/tmp/2022_11_55.py:351: High: eval\n        winfunc = eval('_' + winstr)\nArgument 1 to this function call should be checked to ensure that it does not\ncome from an untrusted source without first verifying that it contains nothing\ndangerous."]], "semgrep": [true, ["       python.lang.security.audit.eval-detected.eval-detected                                        \n          Detected the use of eval(). eval() can be dangerous if used to evaluate dynamic content. If\n          this content can be input from outside the program, this may be a code injection           \n          vulnerability. Ensure evaluated content is not definable by external sources.              \n          Details: https://sg.run/ZvrD                                                               \n          351\u2506 winfunc = eval('_' + winstr)"]]}, "target": -1, "function_before": [], "function_after": []}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
