{"index": 4865, "cve_id": "CVE-2020-11078", "cwe_id": ["CWE-93", "CWE-74"], "cve_language": "Python", "cve_description": "In httplib2 before version 0.18.0, an attacker controlling unescaped part of uri for `httplib2.Http.request()` could change request headers and body, send additional hidden requests to same server. This vulnerability impacts software that uses httplib2 with uri constructed by string concatenation, as opposed to proper urllib building with escaping. This has been fixed in 0.18.0.", "cvss": "6.8", "publish_date": "August 19, 2020", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "NONE", "S": "CHANGED", "C": "NONE", "I": "HIGH", "A": "NONE", "commit_id": "a1457cc31f3206cf691d11d2bf34e98865873e9e", "commit_message": "IMPORTANT security vulnerability CWE-93 CRLF injection\n\nForce %xx quote of space, CR, LF characters in uri.\n\nSpecial thanks to Recar https://github.com/Ciyfly for discrete notification.\n\nhttps://cwe.mitre.org/data/definitions/93.html", "commit_date": "2020-05-20T12:00:24Z", "project": "httplib2/httplib2", "url": "https://api.github.com/repos/httplib2/httplib2/commits/a1457cc31f3206cf691d11d2bf34e98865873e9e", "html_url": "https://github.com/httplib2/httplib2/commit/a1457cc31f3206cf691d11d2bf34e98865873e9e", "windows_before": [{"commit_id": "9413ffc973a2dc90abf787509ee82238345d5602", "commit_date": "Tue May 19 17:28:48 2020 +0300", "commit_message": "v0.17.4 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "fe3136ac369199abd9d6afd2d2a61a11da9e32ac", "commit_date": "Tue May 19 15:32:25 2020 +0200", "commit_message": "Ship new test suite in source dist", "files_name": ["MANIFEST.in"]}, {"commit_id": "f5684876ef5e3b57c81f716c08b316fa36684f08", "commit_date": "Wed Apr 22 12:09:01 2020 +0300", "commit_message": "v0.17.3 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "fa0c4d2cb98ece0a7b156583d308ddf4370dc7d7", "commit_date": "Thu Apr 16 07:46:38 2020 -0700", "commit_message": "Switched the iri2uri import to a relative import", "files_name": ["python2/httplib2/__init__.py"]}, {"commit_id": "067b3f250769e921dc397e28f3aadd0ffa14e17b", "commit_date": "Sat Apr 11 02:16:46 2020 +0300", "commit_message": "v0.17.2 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "59586b5f0ccfa9d570d8c7d908d00d34cf1f3c89", "commit_date": "Sat Apr 11 00:00:47 2020 +0100", "commit_message": "Fix debug in HTTPSConnectionWithTimeout.connect", "files_name": ["python3/httplib2/__init__.py"]}, {"commit_id": "0d490f692ca5f73cd2caaf9cc5e42e9f7702e453", "commit_date": "Thu Apr 2 20:37:22 2020 +0300", "commit_message": "travis says matrix is alias for jobs now", "files_name": [".travis.yml", "script/compile-py3-openssl11.sh", "tests/test_proxy.py"]}, {"commit_id": "0effb930cb8e72537076a88e3713ce63b67e4f85", "commit_date": "Thu Apr 2 20:11:05 2020 +0300", "commit_message": "v0.17.1 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "e9a98f93c9e29a886231cf02a10b4dafa6793ef2", "commit_date": "Thu Apr 2 19:27:42 2020 +0300", "commit_message": "python3: no_proxy was not checked with https", "files_name": ["python3/httplib2/__init__.py", "tests/test_proxy.py"]}, {"commit_id": "67463426cf79af766176b1f318fefefd3d5edc23", "commit_date": "Fri Jan 24 16:57:26 2020 +0300", "commit_message": "v0.17.0 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "ea0e1f151bae5fb4bb58629ecf2909f277cef761", "commit_date": "Thu Jan 23 12:25:01 2020 +0300", "commit_message": "feature: Http().redirect_codes set, works after follow(_all)_redirects check", "files_name": ["python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "tests/test_http.py"]}, {"commit_id": "a128b94898068fb9357bb693f134c5d0a2443871", "commit_date": "Tue Jan 14 00:27:24 2020 +0300", "commit_message": "v0.16.0 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "fa9a3bbfb5667968531613b24529a364f4d16b29", "commit_date": "Tue Jan 14 00:11:51 2020 +0300", "commit_message": "proxy: username/password as str compatible with pysocks; Thanks to Lorenzo Mentaschi", "files_name": ["python2/httplib2/socks.py", "python3/httplib2/__init__.py", "python3/httplib2/socks.py", "tests/test_proxy.py"]}, {"commit_id": "a8716b1d48d57b56ed634b215edf56a3f114f244", "commit_date": "Mon Dec 23 17:08:41 2019 +1100", "commit_message": "Fix simple typo: usefull -> useful", "files_name": ["python3/httplib2/__init__.py"]}, {"commit_id": "45441b2f0ea6180853b087f77169f44ce726a508", "commit_date": "Thu Dec 19 14:02:28 2019 +0300", "commit_message": "IMPORTANT cache invalidation change, fix 307 keep method, add 308 redirect", "files_name": ["python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "tests/__init__.py", "tests/test_http.py"]}, {"commit_id": "095494db43abe8ddfd53b3c389baaadbfcae4f12", "commit_date": "Thu Dec 19 14:28:16 2019 +0300", "commit_message": "v0.15.0 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "d12344af52d57060284cca7fbe85deb6581a6942", "commit_date": "Wed Nov 13 05:09:05 2019 -0500", "commit_message": "python2: regression in connect() error handling", "files_name": ["python2/httplib2/__init__.py", "tests/test_other.py"]}, {"commit_id": "54ee0ef8ff80a5d95a96c76bdc0187cab9e76971", "commit_date": "Tue Oct 15 11:35:22 2019 +0300", "commit_message": "feature: Http.close() to clean persistent connections and sensitive data", "files_name": ["python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "tests/test_other.py"]}, {"commit_id": "4009e8e4922bb592f6ab93cb1f3aa0e4d650e847", "commit_date": "Mon Oct 21 07:28:11 2019 -0700", "commit_message": "add support for password protected certificate files", "files_name": ["python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "tests/test_https.py"]}, {"commit_id": "3a6d7cd9bc9ff7a82e9dc560c9118ddcc4607dec", "commit_date": "Thu Sep 26 22:11:25 2019 +0300", "commit_message": "tests: TLS/https support", "files_name": ["script/generate-tls", "tests/__init__.py", "tests/test_external.py", "tests/test_http.py", "tests/test_https.py", "tests/tls/ca.key", "tests/tls/ca.pem", "tests/tls/ca.srl", "tests/tls/ca_unused.pem", "tests/tls/client.crt", "tests/tls/client.key", "tests/tls/client.pem", "tests/tls/client_chain.pem", "tests/tls/client_encrypted.crt", "tests/tls/client_encrypted.key", "tests/tls/client_encrypted.pem", "tests/tls/server.crt", "tests/tls/server.key", "tests/tls/server.pem", "tests/tls/server_chain.pem"]}, {"commit_id": "761558788d540b9442cd1614570074c61aa0bfca", "commit_date": "Thu Sep 26 08:29:12 2019 +0300", "commit_message": "v0.14.0 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "cfba1201736e0060a9cb82eab73ad49988ab7416", "commit_date": "Mon Sep 16 02:10:07 2019 +0500", "commit_message": "PROXY_TYPE_SOCKS5 with str user/pass raised TypeError (#145)", "files_name": [".gitignore", "python3/httplib2/__init__.py", "python3/httplib2/socks.py", "tests/__init__.py", "tests/test_proxy.py"]}, {"commit_id": "d498ee35017a9498cff3e9aa9887b95b988e6ca9", "commit_date": "Sun Jul 28 07:04:29 2019 +0500", "commit_message": "v0.13.1 release (#141)", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "5d9e6492af7557b4fd7f4be1fbfb9f60e27c189d", "commit_date": "Sat Jul 27 08:47:46 2019 -0400", "commit_message": "Python3: Use no_proxy (#140)", "files_name": ["Makefile", "python3/httplib2/__init__.py"]}, {"commit_id": "e92155b85c719405fefe69b045f3bcf55d4fbcc1", "commit_date": "Fri Jun 7 00:40:38 2019 +0500", "commit_message": "v0.13.0 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "2a806cb4bc731058541ab902101ea7750f350282", "commit_date": "Thu Jun 6 02:59:21 2019 -0400", "commit_message": "Allow setting TLS max/min versions https://github.com/httplib2/httplib2/pull/138", "files_name": [".travis.yml", "python3/httplib2/__init__.py", "script/compile-py3-openssl11.sh", "tests/test_external.py", "tests/test_http.py"]}, {"commit_id": "dd741d17b595ef47a12f06f7db823fb85edab603", "commit_date": "Tue Apr 23 16:26:20 2019 +0500", "commit_message": "v0.12.3 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "e120afcc1b7d83319726e02c5842d11bd9ece4d6", "commit_date": "Tue Apr 23 16:19:36 2019 +0500", "commit_message": "release non-universal wheels", "files_name": [".travis.yml", "script/release", "setup.cfg"]}, {"commit_id": "1b96297db986773903ea4c41769895f1ae322258", "commit_date": "Tue Apr 23 16:08:55 2019 +0500", "commit_message": "travis: upgrade service jobs python 3.6 -> 3.7 for no good reason; fix indent in script/release", "files_name": [".travis.yml", "script/release"]}, {"commit_id": "51bcdd26291343a9c29cac122048a937d4d93895", "commit_date": "Fri Apr 12 15:54:43 2019 +0500", "commit_message": "release: clean --all helps to make wheels https://github.com/httplib2/httplib2/pull/29", "files_name": ["script/release"]}, {"commit_id": "23a7e13fe24ee33855263dd947484b14dcb169fa", "commit_date": "Thu Feb 14 21:38:07 2019 +0500", "commit_message": "v0.12.1 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "724c6d01a4952fee3c77a4aae77108a144524098", "commit_date": "Fri Feb 15 02:30:10 2019 +0800", "commit_message": "Fix a warning under Python 3.7", "files_name": ["setup.py"]}, {"commit_id": "d26ed028c0eccdfdc94316eaaf07982d8520ee9e", "commit_date": "Sat Dec 1 08:23:50 2018 +0100", "commit_message": "officially support Python 3.7", "files_name": [".travis.yml", "CHANGELOG", "setup.py"]}, {"commit_id": "806de52a2939d95800e43b1ab96be45a33b882d1", "commit_date": "Mon Nov 19 15:53:04 2018 +1300", "commit_message": "Catch socket timeouts and clear dead connection", "files_name": ["python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "tests/test_other.py"]}, {"commit_id": "86c3b4ea8614cbbddd86fc556781f2f79cd7c2ac", "commit_date": "Wed Nov 14 09:40:05 2018 +0500", "commit_message": "v0.12.0 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}], "windows_after": [{"commit_id": "9fef207e85eef0534574d71fe1338c01874eba46", "commit_date": "Wed May 20 15:32:51 2020 +0300", "commit_message": "pyproject.toml", "files_name": ["pyproject.toml", "script/release"]}, {"commit_id": "8373177d3a9e4dd9c956f9bded22a5f96a00957b", "commit_date": "Wed May 20 15:40:47 2020 +0300", "commit_message": "v0.18.0 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "828c26d8ca1e7e3c9c3e154885c9bf3a13426cbe", "commit_date": "Wed May 20 20:44:00 2020 +0300", "commit_message": "Security Policy", "files_name": ["SECURITY.md"]}, {"commit_id": "94f48efe2ffb1caa3fbcba0598e7583df02b832a", "commit_date": "Wed May 20 22:42:35 2020 +0300", "commit_message": "check-manifest build tool", "files_name": [".travis.yml", "MANIFEST.in", "pyproject.toml", "script/release"]}, {"commit_id": "cb2940a5046c34b6c3568054e8679ae064da4f72", "commit_date": "Wed May 20 22:42:42 2020 +0300", "commit_message": "explicit build-backend workaround pip build isolation bug 6264", "files_name": ["pyproject.toml"]}, {"commit_id": "9bf300cdc372938f4237150d5b9b615879eb51a1", "commit_date": "Wed May 20 22:56:40 2020 +0300", "commit_message": "v0.18.1 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "595e248d0958c00e83cb28f136a2a54772772b50", "commit_date": "Tue Sep 29 09:21:04 2020 -0600", "commit_message": "auth: WSSE token needs to be string not bytes", "files_name": ["python3/httplib2/__init__.py", "tests/__init__.py", "tests/test_auth.py"]}, {"commit_id": "33090ab8f40ee6d4507d602df4641b5eaad84e1e", "commit_date": "Thu Jan 21 06:41:17 2021 +0000", "commit_message": "initial fuzz testing integration with OSS-Fuzz", "files_name": ["tests/fuzz_request.py", "tests/fuzz_url.py"]}, {"commit_id": "bd9ee252c8f099608019709e22c0d705e98d26bc", "commit_date": "Wed Jan 6 17:13:38 2021 +0300", "commit_message": "parse auth headers using pyparsing instead of regexp", "files_name": ["python2/httplib2/__init__.py", "python2/httplib2/auth.py", "python2/httplib2/error.py", "python3/httplib2/__init__.py", "python3/httplib2/auth.py", "python3/httplib2/error.py", "requirements.txt", "script/release", "script/test", "setup.py", "tests/__init__.py", "tests/test_auth.py"]}, {"commit_id": "c3aed1eea5ddcc90344ad5c483d3aee22a135473", "commit_date": "Sun Feb 7 00:37:02 2021 +0300", "commit_message": "fix release script, interactive part", "files_name": ["script/release"]}, {"commit_id": "81e80d00c2b3cb51de6019cfd28b793184d7699a", "commit_date": "Sun Feb 7 17:49:33 2021 +0300", "commit_message": "v0.19.0 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "f88fe0a1142f71215fea95be9900eaecb546f7b5", "commit_date": "Wed Mar 3 22:07:25 2021 +0100", "commit_message": "Use mock from the standard library", "files_name": ["requirements-test.txt", "tests/test_cacerts_from_env.py", "tests/test_http.py", "tests/test_other.py", "tests/test_proxy.py"]}, {"commit_id": "77464f88ad6ed5a56ad4c395f1811331cad3cfcd", "commit_date": "Tue Mar 30 00:26:09 2021 +0300", "commit_message": "auth header parsing performance optimizations; Thanks to Paul McGuire", "files_name": ["python2/httplib2/auth.py", "python3/httplib2/auth.py", "tests/__init__.py", "tests/test_auth.py"]}, {"commit_id": "58e6f66804665884393ebbe83968c0c2c16c9f70", "commit_date": "Tue Mar 30 19:03:13 2021 +0300", "commit_message": "v0.19.1 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "df7639fdd08d92d20b962aed0456fca784c1b2ad", "commit_date": "Thu Apr 8 08:54:08 2021 +0100", "commit_message": "tls: accept min/max ssl.TLSVersion enum values", "files_name": ["python3/httplib2/__init__.py", "tests/test_https.py"]}, {"commit_id": "08d6993b69256fbc6c0b1c615c24910803c4d610", "commit_date": "Thu May 13 19:09:52 2021 +0300", "commit_message": "tests: test_inject_space failed after bpo-43882 fix", "files_name": ["tests/test_http.py"]}, {"commit_id": "37f809f2bf1d2f99107b1f8eedc512d8bca04eeb", "commit_date": "Thu May 13 23:35:09 2021 +0300", "commit_message": "travis: upgrade pypy OS to fix setuptools_scm could not be found", "files_name": [".travis.yml"]}, {"commit_id": "f35d8fb0b270997ebdfd584ed3505f2a1c1cf53e", "commit_date": "Fri Aug 20 14:07:15 2021 +0100", "commit_message": "fuzzer: update for new atheris version", "files_name": ["tests/fuzz_request.py", "tests/fuzz_url.py"]}, {"commit_id": "da32041f6fda4edb045f88ef22cfbc90e09857d8", "commit_date": "Mon Jun 28 14:07:29 2021 +0300", "commit_message": "Add support for Python 3.8 and 3.9", "files_name": [".travis.yml", "setup.py"]}, {"commit_id": "ddd0c68985522ca805c6dae0e947ef9bba2a0280", "commit_date": "Mon Jun 28 14:07:41 2021 +0300", "commit_message": "Add python_requires to help pip", "files_name": ["setup.py"]}, {"commit_id": "f6c2da9ba54df21c2fb68fe890260f97c2a3b3af", "commit_date": "Sun Oct 3 23:13:25 2021 +0200", "commit_message": "cacerts: Security fix Remove DST Root CA X3, it expires on 2021-09-30", "files_name": ["python2/httplib2/cacerts.txt", "python3/httplib2/cacerts.txt"]}, {"commit_id": "4b7f7811b2eed4ebbe1fbf56569671ab526bbd03", "commit_date": "Mon Oct 4 00:08:47 2021 +0300", "commit_message": "cacerts: add ISRG Root X1, X2", "files_name": ["python2/httplib2/cacerts.txt", "python3/httplib2/cacerts.txt"]}, {"commit_id": "a0d1e28d22ba8df56ef32f01830e12f47f39dccc", "commit_date": "Thu Oct 7 11:50:11 2021 +0300", "commit_message": "v0.20.0 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "41804463a5d1d56fd39511e4bcc215cf0ac1e357", "commit_date": "Thu Oct 7 15:57:31 2021 +0300", "commit_message": "v0.20.1 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "dce3376fc25775451ce93e2456806459f4cec0df", "commit_date": "Mon Oct 11 10:53:38 2021 +0300", "commit_message": "proxy: correct extraction of errno from pysocks ProxyConnectionError", "files_name": ["python3/httplib2/__init__.py"]}, {"commit_id": "130cd39fd7f4a4ca464549b3ff776bea4ffbef89", "commit_date": "Tue Nov 2 00:35:15 2021 -0600", "commit_message": "auth: support pyparsing v3 API rename (pp.common.downcaseTokens)", "files_name": ["python3/httplib2/auth.py", "requirements.txt"]}, {"commit_id": "f4efdff5003aa99799f304540bf139d7e08a92f2", "commit_date": "Tue Nov 2 10:04:35 2021 +0300", "commit_message": "v0.20.2 release", "files_name": ["CHANGELOG", "python2/httplib2/__init__.py", "python3/httplib2/__init__.py", "setup.py"]}, {"commit_id": "be13bba53e11148a77a1dc4f587c456dc15dfab4", "commit_date": "Mon Oct 4 00:00:57 2021 +0000", "commit_message": "tests: regenerated testing keys/certs", "files_name": ["requirements-test.txt", "script/generate-tls", "tests/__init__.py", "tests/test_https.py", "tests/tls/ca.pem", "tests/tls/ca.srl", "tests/tls/ca_unused.pem", "tests/tls/client.crt", "tests/tls/client.pem", "tests/tls/client_chain.pem", "tests/tls/client_encrypted.crt", "tests/tls/client_encrypted.pem", "tests/tls/server.crt", "tests/tls/server.pem", "tests/tls/server_chain.pem"]}, {"commit_id": "f77f29cc339d5b0a3da71d0aceaaadad1e15db65", "commit_date": "Thu Nov 18 10:32:02 2021 +0100", "commit_message": "tests: bump min/max tls versions, remove problematic test_min_tls_version", "files_name": ["tests/test_https.py"]}, {"commit_id": "37779fd2bb0425040daed53afcc8b9d6f790a35f", "commit_date": "Fri Sep 24 23:05:15 2021 +0300", "commit_message": "ci: switch from Travis to Github Actions", "files_name": [".github/workflows/publish.yaml", ".github/workflows/style.yaml", ".github/workflows/test.yaml", ".travis.yml", "codecov.yml", "pyproject.toml", "requirements-test.txt", "script/compile-py3-openssl11.sh", "script/release", "script/test", "setup.cfg", "tests/fuzz_request.py", "tests/test_http.py", "tests/test_https.py", "tests/test_other.py", "tests/test_proxy.py"]}, {"commit_id": "c0640d6af2f3a5e550cbce0593cd060bd9a90745", "commit_date": "Mon Apr 2 08:03:44 2018 +0300", "commit_message": "tests: remove old test files, use `script/test`", "files_name": ["MANIFEST.in", "Makefile", "python2/httplib2/test/brokensocket/socket.py", "python2/httplib2/test/server.key", "python2/httplib2/test/server.pem", "python2/httplib2test.py", "python2/ssl_protocol_test.py", "python3/httplib2/test/other_cacerts.txt", "python3/httplib2test.py", "setup.cfg", "test/.htaccess", "test/300/final-destination.txt", "test/300/with-location-header.asis", "test/300/without-location-header.asis", "test/301/final-destination.txt", "test/301/onestep.asis", "test/302/.myhtaccess", "test/302/final-destination.txt", "test/302/no-location.asis", "test/302/onestep.asis", "test/302/twostep.asis", "test/303/303.cgi"]}], "parents": [{"commit_id_before": "9413ffc973a2dc90abf787509ee82238345d5602", "url_before": "https://api.github.com/repos/httplib2/httplib2/commits/9413ffc973a2dc90abf787509ee82238345d5602", "html_url_before": "https://github.com/httplib2/httplib2/commit/9413ffc973a2dc90abf787509ee82238345d5602"}], "details": [{"raw_url": "https://github.com/httplib2/httplib2/raw/a1457cc31f3206cf691d11d2bf34e98865873e9e/python2%2Fhttplib2%2F__init__.py", "code": "\"\"\"Small, fast HTTP client library for Python.\n\nFeatures persistent connections, cache, and Google App Engine Standard\nEnvironment support.\n\"\"\"\n\nfrom __future__ import print_function\n\n__author__ = \"Joe Gregorio (joe@bitworking.org)\"\n__copyright__ = \"Copyright 2006, Joe Gregorio\"\n__contributors__ = [\n    \"Thomas Broyer (t.broyer@ltgt.net)\",\n    \"James Antill\",\n    \"Xavier Verges Farrero\",\n    \"Jonathan Feinberg\",\n    \"Blair Zajac\",\n    \"Sam Ruby\",\n    \"Louis Nyffenegger\",\n    \"Alex Yu\",\n]\n__license__ = \"MIT\"\n__version__ = '0.17.4'\n\nimport base64\nimport calendar\nimport copy\nimport email\nimport email.FeedParser\nimport email.Message\nimport email.Utils\nimport errno\nimport gzip\nimport httplib\nimport os\nimport random\nimport re\nimport StringIO\nimport sys\nimport time\nimport urllib\nimport urlparse\nimport zlib\n\ntry:\n    from hashlib import sha1 as _sha, md5 as _md5\nexcept ImportError:\n    # prior to Python 2.5, these were separate modules\n    import sha\n    import md5\n\n    _sha = sha.new\n    _md5 = md5.new\nimport hmac\nfrom gettext import gettext as _\nimport socket\n\ntry:\n    from httplib2 import socks\nexcept ImportError:\n    try:\n        import socks\n    except (ImportError, AttributeError):\n        socks = None\n\n# Build the appropriate socket wrapper for ssl\nssl = None\nssl_SSLError = None\nssl_CertificateError = None\ntry:\n    import ssl  # python 2.6\nexcept ImportError:\n    pass\nif ssl is not None:\n    ssl_SSLError = getattr(ssl, \"SSLError\", None)\n    ssl_CertificateError = getattr(ssl, \"CertificateError\", None)\n\n\ndef _ssl_wrap_socket(\n    sock, key_file, cert_file, disable_validation, ca_certs, ssl_version, hostname, key_password\n):\n    if disable_validation:\n        cert_reqs = ssl.CERT_NONE\n    else:\n        cert_reqs = ssl.CERT_REQUIRED\n    if ssl_version is None:\n        ssl_version = ssl.PROTOCOL_SSLv23\n\n    if hasattr(ssl, \"SSLContext\"):  # Python 2.7.9\n        context = ssl.SSLContext(ssl_version)\n        context.verify_mode = cert_reqs\n        context.check_hostname = cert_reqs != ssl.CERT_NONE\n        if cert_file:\n            if key_password:\n                context.load_cert_chain(cert_file, key_file, key_password)\n            else:\n                context.load_cert_chain(cert_file, key_file)\n        if ca_certs:\n            context.load_verify_locations(ca_certs)\n        return context.wrap_socket(sock, server_hostname=hostname)\n    else:\n        if key_password:\n            raise NotSupportedOnThisPlatform(\"Certificate with password is not supported.\")\n        return ssl.wrap_socket(\n            sock,\n            keyfile=key_file,\n            certfile=cert_file,\n            cert_reqs=cert_reqs,\n            ca_certs=ca_certs,\n            ssl_version=ssl_version,\n        )\n\n\ndef _ssl_wrap_socket_unsupported(\n    sock, key_file, cert_file, disable_validation, ca_certs, ssl_version, hostname, key_password\n):\n    if not disable_validation:\n        raise CertificateValidationUnsupported(\n            \"SSL certificate validation is not supported without \"\n            \"the ssl module installed. To avoid this error, install \"\n            \"the ssl module, or explicity disable validation.\"\n        )\n    if key_password:\n        raise NotSupportedOnThisPlatform(\"Certificate with password is not supported.\")\n    ssl_sock = socket.ssl(sock, key_file, cert_file)\n    return httplib.FakeSocket(sock, ssl_sock)\n\n\nif ssl is None:\n    _ssl_wrap_socket = _ssl_wrap_socket_unsupported\n\nif sys.version_info >= (2, 3):\n    from .iri2uri import iri2uri\nelse:\n\n    def iri2uri(uri):\n        return uri\n\n\ndef has_timeout(timeout):  # python 2.6\n    if hasattr(socket, \"_GLOBAL_DEFAULT_TIMEOUT\"):\n        return timeout is not None and timeout is not socket._GLOBAL_DEFAULT_TIMEOUT\n    return timeout is not None\n\n\n__all__ = [\n    \"Http\",\n    \"Response\",\n    \"ProxyInfo\",\n    \"HttpLib2Error\",\n    \"RedirectMissingLocation\",\n    \"RedirectLimit\",\n    \"FailedToDecompressContent\",\n    \"UnimplementedDigestAuthOptionError\",\n    \"UnimplementedHmacDigestAuthOptionError\",\n    \"debuglevel\",\n    \"ProxiesUnavailableError\",\n]\n\n# The httplib debug level, set to a non-zero value to get debug output\ndebuglevel = 0\n\n# A request will be tried 'RETRIES' times if it fails at the socket/connection level.\nRETRIES = 2\n\n# Python 2.3 support\nif sys.version_info < (2, 4):\n\n    def sorted(seq):\n        seq.sort()\n        return seq\n\n\n# Python 2.3 support\ndef HTTPResponse__getheaders(self):\n    \"\"\"Return list of (header, value) tuples.\"\"\"\n    if self.msg is None:\n        raise httplib.ResponseNotReady()\n    return self.msg.items()\n\n\nif not hasattr(httplib.HTTPResponse, \"getheaders\"):\n    httplib.HTTPResponse.getheaders = HTTPResponse__getheaders\n\n\n# All exceptions raised here derive from HttpLib2Error\nclass HttpLib2Error(Exception):\n    pass\n\n\n# Some exceptions can be caught and optionally\n# be turned back into responses.\nclass HttpLib2ErrorWithResponse(HttpLib2Error):\n    def __init__(self, desc, response, content):\n        self.response = response\n        self.content = content\n        HttpLib2Error.__init__(self, desc)\n\n\nclass RedirectMissingLocation(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass RedirectLimit(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass FailedToDecompressContent(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass UnimplementedDigestAuthOptionError(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass UnimplementedHmacDigestAuthOptionError(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass MalformedHeader(HttpLib2Error):\n    pass\n\n\nclass RelativeURIError(HttpLib2Error):\n    pass\n\n\nclass ServerNotFoundError(HttpLib2Error):\n    pass\n\n\nclass ProxiesUnavailableError(HttpLib2Error):\n    pass\n\n\nclass CertificateValidationUnsupported(HttpLib2Error):\n    pass\n\n\nclass SSLHandshakeError(HttpLib2Error):\n    pass\n\n\nclass NotSupportedOnThisPlatform(HttpLib2Error):\n    pass\n\n\nclass CertificateHostnameMismatch(SSLHandshakeError):\n    def __init__(self, desc, host, cert):\n        HttpLib2Error.__init__(self, desc)\n        self.host = host\n        self.cert = cert\n\n\nclass NotRunningAppEngineEnvironment(HttpLib2Error):\n    pass\n\n\n# Open Items:\n# -----------\n# Proxy support\n\n# Are we removing the cached content too soon on PUT (only delete on 200 Maybe?)\n\n# Pluggable cache storage (supports storing the cache in\n#   flat files by default. We need a plug-in architecture\n#   that can support Berkeley DB and Squid)\n\n# == Known Issues ==\n# Does not handle a resource that uses conneg and Last-Modified but no ETag as a cache validator.\n# Does not handle Cache-Control: max-stale\n# Does not use Age: headers when calculating cache freshness.\n\n# The number of redirections to follow before giving up.\n# Note that only GET redirects are automatically followed.\n# Will also honor 301 requests by saving that info and never\n# requesting that URI again.\nDEFAULT_MAX_REDIRECTS = 5\n\nfrom httplib2 import certs\nCA_CERTS = certs.where()\n\n# Which headers are hop-by-hop headers by default\nHOP_BY_HOP = [\n    \"connection\",\n    \"keep-alive\",\n    \"proxy-authenticate\",\n    \"proxy-authorization\",\n    \"te\",\n    \"trailers\",\n    \"transfer-encoding\",\n    \"upgrade\",\n]\n\n# https://tools.ietf.org/html/rfc7231#section-8.1.3\nSAFE_METHODS = (\"GET\", \"HEAD\")  # TODO add \"OPTIONS\", \"TRACE\"\n\n# To change, assign to `Http().redirect_codes`\nREDIRECT_CODES = frozenset((300, 301, 302, 303, 307, 308))\n\n\ndef _get_end2end_headers(response):\n    hopbyhop = list(HOP_BY_HOP)\n    hopbyhop.extend([x.strip() for x in response.get(\"connection\", \"\").split(\",\")])\n    return [header for header in response.keys() if header not in hopbyhop]\n\n\nURI = re.compile(r\"^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\\?([^#]*))?(#(.*))?\")\n\n\ndef parse_uri(uri):\n    \"\"\"Parses a URI using the regex given in Appendix B of RFC 3986.\n\n        (scheme, authority, path, query, fragment) = parse_uri(uri)\n    \"\"\"\n    groups = URI.match(uri).groups()\n    return (groups[1], groups[3], groups[4], groups[6], groups[8])\n\n\ndef urlnorm(uri):\n    (scheme, authority, path, query, fragment) = parse_uri(uri)\n    if not scheme or not authority:\n        raise RelativeURIError(\"Only absolute URIs are allowed. uri = %s\" % uri)\n    authority = authority.lower()\n    scheme = scheme.lower()\n    if not path:\n        path = \"/\"\n    # Could do syntax based normalization of the URI before\n    # computing the digest. See Section 6.2.2 of Std 66.\n    request_uri = query and \"?\".join([path, query]) or path\n    scheme = scheme.lower()\n    defrag_uri = scheme + \"://\" + authority + request_uri\n    return scheme, authority, request_uri, defrag_uri\n\n\n# Cache filename construction (original borrowed from Venus http://intertwingly.net/code/venus/)\nre_url_scheme = re.compile(r\"^\\w+://\")\nre_unsafe = re.compile(r\"[^\\w\\-_.()=!]+\")\n\n\ndef safename(filename):\n    \"\"\"Return a filename suitable for the cache.\n    Strips dangerous and common characters to create a filename we\n    can use to store the cache in.\n    \"\"\"\n    if isinstance(filename, str):\n        filename_bytes = filename\n        filename = filename.decode(\"utf-8\")\n    else:\n        filename_bytes = filename.encode(\"utf-8\")\n    filemd5 = _md5(filename_bytes).hexdigest()\n    filename = re_url_scheme.sub(\"\", filename)\n    filename = re_unsafe.sub(\"\", filename)\n\n    # limit length of filename (vital for Windows)\n    # https://github.com/httplib2/httplib2/pull/74\n    # C:\\Users\\    <username>    \\AppData\\Local\\Temp\\  <safe_filename>  ,   <md5>\n    #   9 chars + max 104 chars  +     20 chars      +       x       +  1  +  32  = max 259 chars\n    # Thus max safe filename x = 93 chars. Let it be 90 to make a round sum:\n    filename = filename[:90]\n\n    return \",\".join((filename, filemd5))\n\n\nNORMALIZE_SPACE = re.compile(r\"(?:\\r\\n)?[ \\t]+\")\n\n\ndef _normalize_headers(headers):\n    return dict(\n        [\n            (key.lower(), NORMALIZE_SPACE.sub(value, \" \").strip())\n            for (key, value) in headers.iteritems()\n        ]\n    )\n\n\ndef _parse_cache_control(headers):\n    retval = {}\n    if \"cache-control\" in headers:\n        parts = headers[\"cache-control\"].split(\",\")\n        parts_with_args = [\n            tuple([x.strip().lower() for x in part.split(\"=\", 1)])\n            for part in parts\n            if -1 != part.find(\"=\")\n        ]\n        parts_wo_args = [\n            (name.strip().lower(), 1) for name in parts if -1 == name.find(\"=\")\n        ]\n        retval = dict(parts_with_args + parts_wo_args)\n    return retval\n\n\n# Whether to use a strict mode to parse WWW-Authenticate headers\n# Might lead to bad results in case of ill-formed header value,\n# so disabled by default, falling back to relaxed parsing.\n# Set to true to turn on, usefull for testing servers.\nUSE_WWW_AUTH_STRICT_PARSING = 0\n\n# In regex below:\n#    [^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+             matches a \"token\" as defined by HTTP\n#    \"(?:[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?\"    matches a \"quoted-string\" as defined by HTTP, when LWS have already been replaced by a single space\n# Actually, as an auth-param value can be either a token or a quoted-string, they are combined in a single pattern which matches both:\n#    \\\"?((?<=\\\")(?:[^\\0-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?(?=\\\")|(?<!\\\")[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+(?!\\\"))\\\"?\nWWW_AUTH_STRICT = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?(?=\\\")|(?<!\\\")[^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+(?!\\\"))\\\"?)(.*)$\"\n)\nWWW_AUTH_RELAXED = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^ \\t\\r\\n=]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\\\\\\"]|\\\\.)*?(?=\\\")|(?<!\\\")[^ \\t\\r\\n,]+(?!\\\"))\\\"?)(.*)$\"\n)\nUNQUOTE_PAIRS = re.compile(r\"\\\\(.)\")\n\n\ndef _parse_www_authenticate(headers, headername=\"www-authenticate\"):\n    \"\"\"Returns a dictionary of dictionaries, one dict\n    per auth_scheme.\"\"\"\n    retval = {}\n    if headername in headers:\n        try:\n\n            authenticate = headers[headername].strip()\n            www_auth = (\n                USE_WWW_AUTH_STRICT_PARSING and WWW_AUTH_STRICT or WWW_AUTH_RELAXED\n            )\n            while authenticate:\n                # Break off the scheme at the beginning of the line\n                if headername == \"authentication-info\":\n                    (auth_scheme, the_rest) = (\"digest\", authenticate)\n                else:\n                    (auth_scheme, the_rest) = authenticate.split(\" \", 1)\n                # Now loop over all the key value pairs that come after the scheme,\n                # being careful not to roll into the next scheme\n                match = www_auth.search(the_rest)\n                auth_params = {}\n                while match:\n                    if match and len(match.groups()) == 3:\n                        (key, value, the_rest) = match.groups()\n                        auth_params[key.lower()] = UNQUOTE_PAIRS.sub(\n                            r\"\\1\", value\n                        )  # '\\\\'.join([x.replace('\\\\', '') for x in value.split('\\\\\\\\')])\n                    match = www_auth.search(the_rest)\n                retval[auth_scheme.lower()] = auth_params\n                authenticate = the_rest.strip()\n\n        except ValueError:\n            raise MalformedHeader(\"WWW-Authenticate\")\n    return retval\n\n\n# TODO: add current time as _entry_disposition argument to avoid sleep in tests\ndef _entry_disposition(response_headers, request_headers):\n    \"\"\"Determine freshness from the Date, Expires and Cache-Control headers.\n\n    We don't handle the following:\n\n    1. Cache-Control: max-stale\n    2. Age: headers are not used in the calculations.\n\n    Not that this algorithm is simpler than you might think\n    because we are operating as a private (non-shared) cache.\n    This lets us ignore 's-maxage'. We can also ignore\n    'proxy-invalidate' since we aren't a proxy.\n    We will never return a stale document as\n    fresh as a design decision, and thus the non-implementation\n    of 'max-stale'. This also lets us safely ignore 'must-revalidate'\n    since we operate as if every server has sent 'must-revalidate'.\n    Since we are private we get to ignore both 'public' and\n    'private' parameters. We also ignore 'no-transform' since\n    we don't do any transformations.\n    The 'no-store' parameter is handled at a higher level.\n    So the only Cache-Control parameters we look at are:\n\n    no-cache\n    only-if-cached\n    max-age\n    min-fresh\n    \"\"\"\n\n    retval = \"STALE\"\n    cc = _parse_cache_control(request_headers)\n    cc_response = _parse_cache_control(response_headers)\n\n    if (\n        \"pragma\" in request_headers\n        and request_headers[\"pragma\"].lower().find(\"no-cache\") != -1\n    ):\n        retval = \"TRANSPARENT\"\n        if \"cache-control\" not in request_headers:\n            request_headers[\"cache-control\"] = \"no-cache\"\n    elif \"no-cache\" in cc:\n        retval = \"TRANSPARENT\"\n    elif \"no-cache\" in cc_response:\n        retval = \"STALE\"\n    elif \"only-if-cached\" in cc:\n        retval = \"FRESH\"\n    elif \"date\" in response_headers:\n        date = calendar.timegm(email.Utils.parsedate_tz(response_headers[\"date\"]))\n        now = time.time()\n        current_age = max(0, now - date)\n        if \"max-age\" in cc_response:\n            try:\n                freshness_lifetime = int(cc_response[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        elif \"expires\" in response_headers:\n            expires = email.Utils.parsedate_tz(response_headers[\"expires\"])\n            if None == expires:\n                freshness_lifetime = 0\n            else:\n                freshness_lifetime = max(0, calendar.timegm(expires) - date)\n        else:\n            freshness_lifetime = 0\n        if \"max-age\" in cc:\n            try:\n                freshness_lifetime = int(cc[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        if \"min-fresh\" in cc:\n            try:\n                min_fresh = int(cc[\"min-fresh\"])\n            except ValueError:\n                min_fresh = 0\n            current_age += min_fresh\n        if freshness_lifetime > current_age:\n            retval = \"FRESH\"\n    return retval\n\n\ndef _decompressContent(response, new_content):\n    content = new_content\n    try:\n        encoding = response.get(\"content-encoding\", None)\n        if encoding in [\"gzip\", \"deflate\"]:\n            if encoding == \"gzip\":\n                content = gzip.GzipFile(fileobj=StringIO.StringIO(new_content)).read()\n            if encoding == \"deflate\":\n                content = zlib.decompress(content, -zlib.MAX_WBITS)\n            response[\"content-length\"] = str(len(content))\n            # Record the historical presence of the encoding in a way the won't interfere.\n            response[\"-content-encoding\"] = response[\"content-encoding\"]\n            del response[\"content-encoding\"]\n    except (IOError, zlib.error):\n        content = \"\"\n        raise FailedToDecompressContent(\n            _(\"Content purported to be compressed with %s but failed to decompress.\")\n            % response.get(\"content-encoding\"),\n            response,\n            content,\n        )\n    return content\n\n\ndef _updateCache(request_headers, response_headers, content, cache, cachekey):\n    if cachekey:\n        cc = _parse_cache_control(request_headers)\n        cc_response = _parse_cache_control(response_headers)\n        if \"no-store\" in cc or \"no-store\" in cc_response:\n            cache.delete(cachekey)\n        else:\n            info = email.Message.Message()\n            for key, value in response_headers.iteritems():\n                if key not in [\"status\", \"content-encoding\", \"transfer-encoding\"]:\n                    info[key] = value\n\n            # Add annotations to the cache to indicate what headers\n            # are variant for this request.\n            vary = response_headers.get(\"vary\", None)\n            if vary:\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    try:\n                        info[key] = request_headers[header]\n                    except KeyError:\n                        pass\n\n            status = response_headers.status\n            if status == 304:\n                status = 200\n\n            status_header = \"status: %d\\r\\n\" % status\n\n            header_str = info.as_string()\n\n            header_str = re.sub(\"\\r(?!\\n)|(?<!\\r)\\n\", \"\\r\\n\", header_str)\n            text = \"\".join([status_header, header_str, content])\n\n            cache.set(cachekey, text)\n\n\ndef _cnonce():\n    dig = _md5(\n        \"%s:%s\"\n        % (time.ctime(), [\"0123456789\"[random.randrange(0, 9)] for i in range(20)])\n    ).hexdigest()\n    return dig[:16]\n\n\ndef _wsse_username_token(cnonce, iso_now, password):\n    return base64.b64encode(\n        _sha(\"%s%s%s\" % (cnonce, iso_now, password)).digest()\n    ).strip()\n\n\n# For credentials we need two things, first\n# a pool of credential to try (not necesarily tied to BAsic, Digest, etc.)\n# Then we also need a list of URIs that have already demanded authentication\n# That list is tricky since sub-URIs can take the same auth, or the\n# auth scheme may change as you descend the tree.\n# So we also need each Auth instance to be able to tell us\n# how close to the 'top' it is.\n\n\nclass Authentication(object):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        self.path = path\n        self.host = host\n        self.credentials = credentials\n        self.http = http\n\n    def depth(self, request_uri):\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        return request_uri[len(self.path) :].count(\"/\")\n\n    def inscope(self, host, request_uri):\n        # XXX Should we normalize the request_uri?\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        return (host == self.host) and path.startswith(self.path)\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header. Over-ride this in sub-classes.\"\"\"\n        pass\n\n    def response(self, response, content):\n        \"\"\"Gives us a chance to update with new nonces\n        or such returned from the last authorized response.\n        Over-rise this in sub-classes if necessary.\n\n        Return TRUE is the request is to be retried, for\n        example Digest may return stale=true.\n        \"\"\"\n        return False\n\n\nclass BasicAuthentication(Authentication):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = (\n            \"Basic \" + base64.b64encode(\"%s:%s\" % self.credentials).strip()\n        )\n\n\nclass DigestAuthentication(Authentication):\n    \"\"\"Only do qop='auth' and MD5, since that\n    is all Apache currently implements\"\"\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        self.challenge = challenge[\"digest\"]\n        qop = self.challenge.get(\"qop\", \"auth\")\n        self.challenge[\"qop\"] = (\n            (\"auth\" in [x.strip() for x in qop.split()]) and \"auth\" or None\n        )\n        if self.challenge[\"qop\"] is None:\n            raise UnimplementedDigestAuthOptionError(\n                _(\"Unsupported value for qop: %s.\" % qop)\n            )\n        self.challenge[\"algorithm\"] = self.challenge.get(\"algorithm\", \"MD5\").upper()\n        if self.challenge[\"algorithm\"] != \"MD5\":\n            raise UnimplementedDigestAuthOptionError(\n                _(\"Unsupported value for algorithm: %s.\" % self.challenge[\"algorithm\"])\n            )\n        self.A1 = \"\".join(\n            [\n                self.credentials[0],\n                \":\",\n                self.challenge[\"realm\"],\n                \":\",\n                self.credentials[1],\n            ]\n        )\n        self.challenge[\"nc\"] = 1\n\n    def request(self, method, request_uri, headers, content, cnonce=None):\n        \"\"\"Modify the request headers\"\"\"\n        H = lambda x: _md5(x).hexdigest()\n        KD = lambda s, d: H(\"%s:%s\" % (s, d))\n        A2 = \"\".join([method, \":\", request_uri])\n        self.challenge[\"cnonce\"] = cnonce or _cnonce()\n        request_digest = '\"%s\"' % KD(\n            H(self.A1),\n            \"%s:%s:%s:%s:%s\"\n            % (\n                self.challenge[\"nonce\"],\n                \"%08x\" % self.challenge[\"nc\"],\n                self.challenge[\"cnonce\"],\n                self.challenge[\"qop\"],\n                H(A2),\n            ),\n        )\n        headers[\"authorization\"] = (\n            'Digest username=\"%s\", realm=\"%s\", nonce=\"%s\", '\n            'uri=\"%s\", algorithm=%s, response=%s, qop=%s, '\n            'nc=%08x, cnonce=\"%s\"'\n        ) % (\n            self.credentials[0],\n            self.challenge[\"realm\"],\n            self.challenge[\"nonce\"],\n            request_uri,\n            self.challenge[\"algorithm\"],\n            request_digest,\n            self.challenge[\"qop\"],\n            self.challenge[\"nc\"],\n            self.challenge[\"cnonce\"],\n        )\n        if self.challenge.get(\"opaque\"):\n            headers[\"authorization\"] += ', opaque=\"%s\"' % self.challenge[\"opaque\"]\n        self.challenge[\"nc\"] += 1\n\n    def response(self, response, content):\n        if \"authentication-info\" not in response:\n            challenge = _parse_www_authenticate(response, \"www-authenticate\").get(\n                \"digest\", {}\n            )\n            if \"true\" == challenge.get(\"stale\"):\n                self.challenge[\"nonce\"] = challenge[\"nonce\"]\n                self.challenge[\"nc\"] = 1\n                return True\n        else:\n            updated_challenge = _parse_www_authenticate(\n                response, \"authentication-info\"\n            ).get(\"digest\", {})\n\n            if \"nextnonce\" in updated_challenge:\n                self.challenge[\"nonce\"] = updated_challenge[\"nextnonce\"]\n                self.challenge[\"nc\"] = 1\n        return False\n\n\nclass HmacDigestAuthentication(Authentication):\n    \"\"\"Adapted from Robert Sayre's code and DigestAuthentication above.\"\"\"\n\n    __author__ = \"Thomas Broyer (t.broyer@ltgt.net)\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        self.challenge = challenge[\"hmacdigest\"]\n        # TODO: self.challenge['domain']\n        self.challenge[\"reason\"] = self.challenge.get(\"reason\", \"unauthorized\")\n        if self.challenge[\"reason\"] not in [\"unauthorized\", \"integrity\"]:\n            self.challenge[\"reason\"] = \"unauthorized\"\n        self.challenge[\"salt\"] = self.challenge.get(\"salt\", \"\")\n        if not self.challenge.get(\"snonce\"):\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\"The challenge doesn't contain a server nonce, or this one is empty.\")\n            )\n        self.challenge[\"algorithm\"] = self.challenge.get(\"algorithm\", \"HMAC-SHA-1\")\n        if self.challenge[\"algorithm\"] not in [\"HMAC-SHA-1\", \"HMAC-MD5\"]:\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\"Unsupported value for algorithm: %s.\" % self.challenge[\"algorithm\"])\n            )\n        self.challenge[\"pw-algorithm\"] = self.challenge.get(\"pw-algorithm\", \"SHA-1\")\n        if self.challenge[\"pw-algorithm\"] not in [\"SHA-1\", \"MD5\"]:\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\n                    \"Unsupported value for pw-algorithm: %s.\"\n                    % self.challenge[\"pw-algorithm\"]\n                )\n            )\n        if self.challenge[\"algorithm\"] == \"HMAC-MD5\":\n            self.hashmod = _md5\n        else:\n            self.hashmod = _sha\n        if self.challenge[\"pw-algorithm\"] == \"MD5\":\n            self.pwhashmod = _md5\n        else:\n            self.pwhashmod = _sha\n        self.key = \"\".join(\n            [\n                self.credentials[0],\n                \":\",\n                self.pwhashmod.new(\n                    \"\".join([self.credentials[1], self.challenge[\"salt\"]])\n                )\n                .hexdigest()\n                .lower(),\n                \":\",\n                self.challenge[\"realm\"],\n            ]\n        )\n        self.key = self.pwhashmod.new(self.key).hexdigest().lower()\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers\"\"\"\n        keys = _get_end2end_headers(headers)\n        keylist = \"\".join([\"%s \" % k for k in keys])\n        headers_val = \"\".join([headers[k] for k in keys])\n        created = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n        cnonce = _cnonce()\n        request_digest = \"%s:%s:%s:%s:%s\" % (\n            method,\n            request_uri,\n            cnonce,\n            self.challenge[\"snonce\"],\n            headers_val,\n        )\n        request_digest = (\n            hmac.new(self.key, request_digest, self.hashmod).hexdigest().lower()\n        )\n        headers[\"authorization\"] = (\n            'HMACDigest username=\"%s\", realm=\"%s\", snonce=\"%s\",'\n            ' cnonce=\"%s\", uri=\"%s\", created=\"%s\", '\n            'response=\"%s\", headers=\"%s\"'\n        ) % (\n            self.credentials[0],\n            self.challenge[\"realm\"],\n            self.challenge[\"snonce\"],\n            cnonce,\n            request_uri,\n            created,\n            request_digest,\n            keylist,\n        )\n\n    def response(self, response, content):\n        challenge = _parse_www_authenticate(response, \"www-authenticate\").get(\n            \"hmacdigest\", {}\n        )\n        if challenge.get(\"reason\") in [\"integrity\", \"stale\"]:\n            return True\n        return False\n\n\nclass WsseAuthentication(Authentication):\n    \"\"\"This is thinly tested and should not be relied upon.\n    At this time there isn't any third party server to test against.\n    Blogger and TypePad implemented this algorithm at one point\n    but Blogger has since switched to Basic over HTTPS and\n    TypePad has implemented it wrong, by never issuing a 401\n    challenge but instead requiring your client to telepathically know that\n    their endpoint is expecting WSSE profile=\"UsernameToken\".\"\"\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = 'WSSE profile=\"UsernameToken\"'\n        iso_now = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n        cnonce = _cnonce()\n        password_digest = _wsse_username_token(cnonce, iso_now, self.credentials[1])\n        headers[\"X-WSSE\"] = (\n            'UsernameToken Username=\"%s\", PasswordDigest=\"%s\", '\n            'Nonce=\"%s\", Created=\"%s\"'\n        ) % (self.credentials[0], password_digest, cnonce, iso_now)\n\n\nclass GoogleLoginAuthentication(Authentication):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        from urllib import urlencode\n\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        service = challenge[\"googlelogin\"].get(\"service\", \"xapi\")\n        # Bloggger actually returns the service in the challenge\n        # For the rest we guess based on the URI\n        if service == \"xapi\" and request_uri.find(\"calendar\") > 0:\n            service = \"cl\"\n        # No point in guessing Base or Spreadsheet\n        # elif request_uri.find(\"spreadsheets\") > 0:\n        #    service = \"wise\"\n\n        auth = dict(\n            Email=credentials[0],\n            Passwd=credentials[1],\n            service=service,\n            source=headers[\"user-agent\"],\n        )\n        resp, content = self.http.request(\n            \"https://www.google.com/accounts/ClientLogin\",\n            method=\"POST\",\n            body=urlencode(auth),\n            headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n        )\n        lines = content.split(\"\\n\")\n        d = dict([tuple(line.split(\"=\", 1)) for line in lines if line])\n        if resp.status == 403:\n            self.Auth = \"\"\n        else:\n            self.Auth = d[\"Auth\"]\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = \"GoogleLogin Auth=\" + self.Auth\n\n\nAUTH_SCHEME_CLASSES = {\n    \"basic\": BasicAuthentication,\n    \"wsse\": WsseAuthentication,\n    \"digest\": DigestAuthentication,\n    \"hmacdigest\": HmacDigestAuthentication,\n    \"googlelogin\": GoogleLoginAuthentication,\n}\n\nAUTH_SCHEME_ORDER = [\"hmacdigest\", \"googlelogin\", \"digest\", \"wsse\", \"basic\"]\n\n\nclass FileCache(object):\n    \"\"\"Uses a local directory as a store for cached files.\n    Not really safe to use if multiple threads or processes are going to\n    be running on the same cache.\n    \"\"\"\n\n    def __init__(\n        self, cache, safe=safename\n    ):  # use safe=lambda x: md5.new(x).hexdigest() for the old behavior\n        self.cache = cache\n        self.safe = safe\n        if not os.path.exists(cache):\n            os.makedirs(self.cache)\n\n    def get(self, key):\n        retval = None\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        try:\n            f = file(cacheFullPath, \"rb\")\n            retval = f.read()\n            f.close()\n        except IOError:\n            pass\n        return retval\n\n    def set(self, key, value):\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        f = file(cacheFullPath, \"wb\")\n        f.write(value)\n        f.close()\n\n    def delete(self, key):\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        if os.path.exists(cacheFullPath):\n            os.remove(cacheFullPath)\n\n\nclass Credentials(object):\n    def __init__(self):\n        self.credentials = []\n\n    def add(self, name, password, domain=\"\"):\n        self.credentials.append((domain.lower(), name, password))\n\n    def clear(self):\n        self.credentials = []\n\n    def iter(self, domain):\n        for (cdomain, name, password) in self.credentials:\n            if cdomain == \"\" or domain == cdomain:\n                yield (name, password)\n\n\nclass KeyCerts(Credentials):\n    \"\"\"Identical to Credentials except that\n    name/password are mapped to key/cert.\"\"\"\n    def add(self, key, cert, domain, password):\n        self.credentials.append((domain.lower(), key, cert, password))\n\n    def iter(self, domain):\n        for (cdomain, key, cert, password) in self.credentials:\n            if cdomain == \"\" or domain == cdomain:\n                yield (key, cert, password)\n\n\nclass AllHosts(object):\n    pass\n\n\nclass ProxyInfo(object):\n    \"\"\"Collect information required to use a proxy.\"\"\"\n\n    bypass_hosts = ()\n\n    def __init__(\n        self,\n        proxy_type,\n        proxy_host,\n        proxy_port,\n        proxy_rdns=True,\n        proxy_user=None,\n        proxy_pass=None,\n        proxy_headers=None,\n    ):\n        \"\"\"Args:\n\n          proxy_type: The type of proxy server.  This must be set to one of\n          socks.PROXY_TYPE_XXX constants.  For example:  p =\n          ProxyInfo(proxy_type=socks.PROXY_TYPE_HTTP, proxy_host='localhost',\n          proxy_port=8000)\n          proxy_host: The hostname or IP address of the proxy server.\n          proxy_port: The port that the proxy server is running on.\n          proxy_rdns: If True (default), DNS queries will not be performed\n          locally, and instead, handed to the proxy to resolve.  This is useful\n          if the network does not allow resolution of non-local names. In\n          httplib2 0.9 and earlier, this defaulted to False.\n          proxy_user: The username used to authenticate with the proxy server.\n          proxy_pass: The password used to authenticate with the proxy server.\n          proxy_headers: Additional or modified headers for the proxy connect\n          request.\n        \"\"\"\n        self.proxy_type = proxy_type\n        self.proxy_host = proxy_host\n        self.proxy_port = proxy_port\n        self.proxy_rdns = proxy_rdns\n        self.proxy_user = proxy_user\n        self.proxy_pass = proxy_pass\n        self.proxy_headers = proxy_headers\n\n    def astuple(self):\n        return (\n            self.proxy_type,\n            self.proxy_host,\n            self.proxy_port,\n            self.proxy_rdns,\n            self.proxy_user,\n            self.proxy_pass,\n            self.proxy_headers,\n        )\n\n    def isgood(self):\n        return (self.proxy_host != None) and (self.proxy_port != None)\n\n    def applies_to(self, hostname):\n        return not self.bypass_host(hostname)\n\n    def bypass_host(self, hostname):\n        \"\"\"Has this host been excluded from the proxy config\"\"\"\n        if self.bypass_hosts is AllHosts:\n            return True\n\n        hostname = \".\" + hostname.lstrip(\".\")\n        for skip_name in self.bypass_hosts:\n            # *.suffix\n            if skip_name.startswith(\".\") and hostname.endswith(skip_name):\n                return True\n            # exact match\n            if hostname == \".\" + skip_name:\n                return True\n        return False\n\n    def __repr__(self):\n        return (\n            \"<ProxyInfo type={p.proxy_type} \"\n            \"host:port={p.proxy_host}:{p.proxy_port} rdns={p.proxy_rdns}\"\n            + \" user={p.proxy_user} headers={p.proxy_headers}>\"\n        ).format(p=self)\n\n\ndef proxy_info_from_environment(method=\"http\"):\n    \"\"\"Read proxy info from the environment variables.\n    \"\"\"\n    if method not in [\"http\", \"https\"]:\n        return\n\n    env_var = method + \"_proxy\"\n    url = os.environ.get(env_var, os.environ.get(env_var.upper()))\n    if not url:\n        return\n    return proxy_info_from_url(url, method, None)\n\n\ndef proxy_info_from_url(url, method=\"http\", noproxy=None):\n    \"\"\"Construct a ProxyInfo from a URL (such as http_proxy env var)\n    \"\"\"\n    url = urlparse.urlparse(url)\n    username = None\n    password = None\n    port = None\n    if \"@\" in url[1]:\n        ident, host_port = url[1].split(\"@\", 1)\n        if \":\" in ident:\n            username, password = ident.split(\":\", 1)\n        else:\n            password = ident\n    else:\n        host_port = url[1]\n    if \":\" in host_port:\n        host, port = host_port.split(\":\", 1)\n    else:\n        host = host_port\n\n    if port:\n        port = int(port)\n    else:\n        port = dict(https=443, http=80)[method]\n\n    proxy_type = 3  # socks.PROXY_TYPE_HTTP\n    pi = ProxyInfo(\n        proxy_type=proxy_type,\n        proxy_host=host,\n        proxy_port=port,\n        proxy_user=username or None,\n        proxy_pass=password or None,\n        proxy_headers=None,\n    )\n\n    bypass_hosts = []\n    # If not given an explicit noproxy value, respect values in env vars.\n    if noproxy is None:\n        noproxy = os.environ.get(\"no_proxy\", os.environ.get(\"NO_PROXY\", \"\"))\n    # Special case: A single '*' character means all hosts should be bypassed.\n    if noproxy == \"*\":\n        bypass_hosts = AllHosts\n    elif noproxy.strip():\n        bypass_hosts = noproxy.split(\",\")\n        bypass_hosts = filter(bool, bypass_hosts)  # To exclude empty string.\n\n    pi.bypass_hosts = bypass_hosts\n    return pi\n\n\nclass HTTPConnectionWithTimeout(httplib.HTTPConnection):\n    \"\"\"HTTPConnection subclass that supports timeouts\n\n    All timeouts are in seconds. If None is passed for timeout then\n    Python's default timeout for sockets will be used. See for example\n    the docs of socket.setdefaulttimeout():\n    http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n    \"\"\"\n\n    def __init__(self, host, port=None, strict=None, timeout=None, proxy_info=None):\n        httplib.HTTPConnection.__init__(self, host, port, strict)\n        self.timeout = timeout\n        self.proxy_info = proxy_info\n\n    def connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        # Mostly verbatim from httplib.py.\n        if self.proxy_info and socks is None:\n            raise ProxiesUnavailableError(\n                \"Proxy support missing but proxy use was requested!\"\n            )\n        if self.proxy_info and self.proxy_info.isgood():\n            use_proxy = True\n            proxy_type, proxy_host, proxy_port, proxy_rdns, proxy_user, proxy_pass, proxy_headers = (\n                self.proxy_info.astuple()\n            )\n\n            host = proxy_host\n            port = proxy_port\n        else:\n            use_proxy = False\n\n            host = self.host\n            port = self.port\n\n        socket_err = None\n\n        for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n            af, socktype, proto, canonname, sa = res\n            try:\n                if use_proxy:\n                    self.sock = socks.socksocket(af, socktype, proto)\n                    self.sock.setproxy(\n                        proxy_type,\n                        proxy_host,\n                        proxy_port,\n                        proxy_rdns,\n                        proxy_user,\n                        proxy_pass,\n                        proxy_headers,\n                    )\n                else:\n                    self.sock = socket.socket(af, socktype, proto)\n                    self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                # Different from httplib: support timeouts.\n                if has_timeout(self.timeout):\n                    self.sock.settimeout(self.timeout)\n                    # End of difference from httplib.\n                if self.debuglevel > 0:\n                    print(\"connect: (%s, %s) ************\" % (self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: %s ************\"\n                            % str(\n                                (\n                                    proxy_host,\n                                    proxy_port,\n                                    proxy_rdns,\n                                    proxy_user,\n                                    proxy_pass,\n                                    proxy_headers,\n                                )\n                            )\n                        )\n                if use_proxy:\n                    self.sock.connect((self.host, self.port) + sa[2:])\n                else:\n                    self.sock.connect(sa)\n            except socket.error as e:\n                socket_err = e\n                if self.debuglevel > 0:\n                    print(\"connect fail: (%s, %s)\" % (self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: %s\"\n                            % str(\n                                (\n                                    proxy_host,\n                                    proxy_port,\n                                    proxy_rdns,\n                                    proxy_user,\n                                    proxy_pass,\n                                    proxy_headers,\n                                )\n                            )\n                        )\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                continue\n            break\n        if not self.sock:\n            raise socket_err or socket.error(\"getaddrinfo returns an empty list\")\n\n\nclass HTTPSConnectionWithTimeout(httplib.HTTPSConnection):\n    \"\"\"This class allows communication via SSL.\n\n    All timeouts are in seconds. If None is passed for timeout then\n    Python's default timeout for sockets will be used. See for example\n    the docs of socket.setdefaulttimeout():\n    http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        strict=None,\n        timeout=None,\n        proxy_info=None,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        ssl_version=None,\n        key_password=None,\n    ):\n        if key_password:\n            httplib.HTTPSConnection.__init__(self, host, port=port, strict=strict)\n            self._context.load_cert_chain(cert_file, key_file, key_password)\n            self.key_file = key_file\n            self.cert_file = cert_file\n            self.key_password = key_password\n        else:\n            httplib.HTTPSConnection.__init__(\n                self, host, port=port, key_file=key_file, cert_file=cert_file, strict=strict\n            )\n            self.key_password = None\n        self.timeout = timeout\n        self.proxy_info = proxy_info\n        if ca_certs is None:\n            ca_certs = CA_CERTS\n        self.ca_certs = ca_certs\n        self.disable_ssl_certificate_validation = disable_ssl_certificate_validation\n        self.ssl_version = ssl_version\n\n    # The following two methods were adapted from https_wrapper.py, released\n    # with the Google Appengine SDK at\n    # http://googleappengine.googlecode.com/svn-history/r136/trunk/python/google/appengine/tools/https_wrapper.py\n    # under the following license:\n    #\n    # Copyright 2007 Google Inc.\n    #\n    # Licensed under the Apache License, Version 2.0 (the \"License\");\n    # you may not use this file except in compliance with the License.\n    # You may obtain a copy of the License at\n    #\n    #     http://www.apache.org/licenses/LICENSE-2.0\n    #\n    # Unless required by applicable law or agreed to in writing, software\n    # distributed under the License is distributed on an \"AS IS\" BASIS,\n    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    # See the License for the specific language governing permissions and\n    # limitations under the License.\n    #\n\n    def _GetValidHostsForCert(self, cert):\n        \"\"\"Returns a list of valid host globs for an SSL certificate.\n\n        Args:\n          cert: A dictionary representing an SSL certificate.\n        Returns:\n          list: A list of valid host globs.\n        \"\"\"\n        if \"subjectAltName\" in cert:\n            return [x[1] for x in cert[\"subjectAltName\"] if x[0].lower() == \"dns\"]\n        else:\n            return [x[0][1] for x in cert[\"subject\"] if x[0][0].lower() == \"commonname\"]\n\n    def _ValidateCertificateHostname(self, cert, hostname):\n        \"\"\"Validates that a given hostname is valid for an SSL certificate.\n\n        Args:\n          cert: A dictionary representing an SSL certificate.\n          hostname: The hostname to test.\n        Returns:\n          bool: Whether or not the hostname is valid for this certificate.\n        \"\"\"\n        hosts = self._GetValidHostsForCert(cert)\n        for host in hosts:\n            host_re = host.replace(\".\", \"\\.\").replace(\"*\", \"[^.]*\")\n            if re.search(\"^%s$\" % (host_re,), hostname, re.I):\n                return True\n        return False\n\n    def connect(self):\n        \"Connect to a host on a given (SSL) port.\"\n\n        if self.proxy_info and self.proxy_info.isgood():\n            use_proxy = True\n            proxy_type, proxy_host, proxy_port, proxy_rdns, proxy_user, proxy_pass, proxy_headers = (\n                self.proxy_info.astuple()\n            )\n\n            host = proxy_host\n            port = proxy_port\n        else:\n            use_proxy = False\n\n            host = self.host\n            port = self.port\n\n        socket_err = None\n\n        address_info = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\n        for family, socktype, proto, canonname, sockaddr in address_info:\n            try:\n                if use_proxy:\n                    sock = socks.socksocket(family, socktype, proto)\n\n                    sock.setproxy(\n                        proxy_type,\n                        proxy_host,\n                        proxy_port,\n                        proxy_rdns,\n                        proxy_user,\n                        proxy_pass,\n                        proxy_headers,\n                    )\n                else:\n                    sock = socket.socket(family, socktype, proto)\n                    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n\n                if has_timeout(self.timeout):\n                    sock.settimeout(self.timeout)\n\n                if use_proxy:\n                    sock.connect((self.host, self.port) + sockaddr[:2])\n                else:\n                    sock.connect(sockaddr)\n                self.sock = _ssl_wrap_socket(\n                    sock,\n                    self.key_file,\n                    self.cert_file,\n                    self.disable_ssl_certificate_validation,\n                    self.ca_certs,\n                    self.ssl_version,\n                    self.host,\n                    self.key_password,\n                )\n                if self.debuglevel > 0:\n                    print(\"connect: (%s, %s)\" % (self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: %s\"\n                            % str(\n                                (\n                                    proxy_host,\n                                    proxy_port,\n                                    proxy_rdns,\n                                    proxy_user,\n                                    proxy_pass,\n                                    proxy_headers,\n                                )\n                            )\n                        )\n                if not self.disable_ssl_certificate_validation:\n                    cert = self.sock.getpeercert()\n                    hostname = self.host.split(\":\", 0)[0]\n                    if not self._ValidateCertificateHostname(cert, hostname):\n                        raise CertificateHostnameMismatch(\n                            \"Server presented certificate that does not match \"\n                            \"host %s: %s\" % (hostname, cert),\n                            hostname,\n                            cert,\n                        )\n            except (\n                ssl_SSLError,\n                ssl_CertificateError,\n                CertificateHostnameMismatch,\n            ) as e:\n                if sock:\n                    sock.close()\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                # Unfortunately the ssl module doesn't seem to provide any way\n                # to get at more detailed error information, in particular\n                # whether the error is due to certificate validation or\n                # something else (such as SSL protocol mismatch).\n                if getattr(e, \"errno\", None) == ssl.SSL_ERROR_SSL:\n                    raise SSLHandshakeError(e)\n                else:\n                    raise\n            except (socket.timeout, socket.gaierror):\n                raise\n            except socket.error as e:\n                socket_err = e\n                if self.debuglevel > 0:\n                    print(\"connect fail: (%s, %s)\" % (self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: %s\"\n                            % str(\n                                (\n                                    proxy_host,\n                                    proxy_port,\n                                    proxy_rdns,\n                                    proxy_user,\n                                    proxy_pass,\n                                    proxy_headers,\n                                )\n                            )\n                        )\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                continue\n            break\n        if not self.sock:\n            raise socket_err or socket.error(\"getaddrinfo returns an empty list\")\n\n\nSCHEME_TO_CONNECTION = {\n    \"http\": HTTPConnectionWithTimeout,\n    \"https\": HTTPSConnectionWithTimeout,\n}\n\n\ndef _new_fixed_fetch(validate_certificate):\n\n    def fixed_fetch(\n        url,\n        payload=None,\n        method=\"GET\",\n        headers={},\n        allow_truncated=False,\n        follow_redirects=True,\n        deadline=None,\n    ):\n        return fetch(\n            url,\n            payload=payload,\n            method=method,\n            headers=headers,\n            allow_truncated=allow_truncated,\n            follow_redirects=follow_redirects,\n            deadline=deadline,\n            validate_certificate=validate_certificate,\n        )\n\n    return fixed_fetch\n\n\nclass AppEngineHttpConnection(httplib.HTTPConnection):\n    \"\"\"Use httplib on App Engine, but compensate for its weirdness.\n\n    The parameters key_file, cert_file, proxy_info, ca_certs,\n    disable_ssl_certificate_validation, and ssl_version are all dropped on\n    the ground.\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        strict=None,\n        timeout=None,\n        proxy_info=None,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        ssl_version=None,\n    ):\n        httplib.HTTPConnection.__init__(\n            self, host, port=port, strict=strict, timeout=timeout\n        )\n\n\nclass AppEngineHttpsConnection(httplib.HTTPSConnection):\n    \"\"\"Same as AppEngineHttpConnection, but for HTTPS URIs.\n\n    The parameters proxy_info, ca_certs, disable_ssl_certificate_validation,\n    and ssl_version are all dropped on the ground.\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        strict=None,\n        timeout=None,\n        proxy_info=None,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        ssl_version=None,\n        key_password=None,\n    ):\n        if key_password:\n            raise NotSupportedOnThisPlatform(\"Certificate with password is not supported.\")\n        httplib.HTTPSConnection.__init__(\n            self,\n            host,\n            port=port,\n            key_file=key_file,\n            cert_file=cert_file,\n            strict=strict,\n            timeout=timeout,\n        )\n        self._fetch = _new_fixed_fetch(not disable_ssl_certificate_validation)\n\n\n# Use a different connection object for Google App Engine Standard Environment.\ndef is_gae_instance():\n    server_software = os.environ.get('SERVER_SOFTWARE', '')\n    if (server_software.startswith('Google App Engine/') or\n        server_software.startswith('Development/') or\n        server_software.startswith('testutil/')):\n        return True\n    return False\n\n\ntry:\n    if not is_gae_instance():\n        raise NotRunningAppEngineEnvironment()\n\n    from google.appengine.api import apiproxy_stub_map\n    if apiproxy_stub_map.apiproxy.GetStub(\"urlfetch\") is None:\n        raise ImportError\n\n    from google.appengine.api.urlfetch import fetch\n\n    # Update the connection classes to use the Googel App Engine specific ones.\n    SCHEME_TO_CONNECTION = {\n        \"http\": AppEngineHttpConnection,\n        \"https\": AppEngineHttpsConnection,\n    }\nexcept (ImportError, NotRunningAppEngineEnvironment):\n    pass\n\n\nclass Http(object):\n    \"\"\"An HTTP client that handles:\n\n    - all methods\n    - caching\n    - ETags\n    - compression,\n    - HTTPS\n    - Basic\n    - Digest\n    - WSSE\n\n    and more.\n    \"\"\"\n\n    def __init__(\n        self,\n        cache=None,\n        timeout=None,\n        proxy_info=proxy_info_from_environment,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        ssl_version=None,\n    ):\n        \"\"\"If 'cache' is a string then it is used as a directory name for\n        a disk cache. Otherwise it must be an object that supports the\n        same interface as FileCache.\n\n        All timeouts are in seconds. If None is passed for timeout\n        then Python's default timeout for sockets will be used. See\n        for example the docs of socket.setdefaulttimeout():\n        http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n\n        `proxy_info` may be:\n          - a callable that takes the http scheme ('http' or 'https') and\n            returns a ProxyInfo instance per request. By default, uses\n            proxy_nfo_from_environment.\n          - a ProxyInfo instance (static proxy config).\n          - None (proxy disabled).\n\n        ca_certs is the path of a file containing root CA certificates for SSL\n        server certificate validation.  By default, a CA cert file bundled with\n        httplib2 is used.\n\n        If disable_ssl_certificate_validation is true, SSL cert validation will\n        not be performed.\n\n        By default, ssl.PROTOCOL_SSLv23 will be used for the ssl version.\n        \"\"\"\n        self.proxy_info = proxy_info\n        self.ca_certs = ca_certs\n        self.disable_ssl_certificate_validation = disable_ssl_certificate_validation\n        self.ssl_version = ssl_version\n\n        # Map domain name to an httplib connection\n        self.connections = {}\n        # The location of the cache, for now a directory\n        # where cached responses are held.\n        if cache and isinstance(cache, basestring):\n            self.cache = FileCache(cache)\n        else:\n            self.cache = cache\n\n        # Name/password\n        self.credentials = Credentials()\n\n        # Key/cert\n        self.certificates = KeyCerts()\n\n        # authorization objects\n        self.authorizations = []\n\n        # If set to False then no redirects are followed, even safe ones.\n        self.follow_redirects = True\n\n        self.redirect_codes = REDIRECT_CODES\n\n        # Which HTTP methods do we apply optimistic concurrency to, i.e.\n        # which methods get an \"if-match:\" etag header added to them.\n        self.optimistic_concurrency_methods = [\"PUT\", \"PATCH\"]\n\n        self.safe_methods = list(SAFE_METHODS)\n\n        # If 'follow_redirects' is True, and this is set to True then\n        # all redirecs are followed, including unsafe ones.\n        self.follow_all_redirects = False\n\n        self.ignore_etag = False\n\n        self.force_exception_to_status_code = False\n\n        self.timeout = timeout\n\n        # Keep Authorization: headers on a redirect.\n        self.forward_authorization_headers = False\n\n    def close(self):\n        \"\"\"Close persistent connections, clear sensitive data.\n        Not thread-safe, requires external synchronization against concurrent requests.\n        \"\"\"\n        existing, self.connections = self.connections, {}\n        for _, c in existing.iteritems():\n            c.close()\n        self.certificates.clear()\n        self.clear_credentials()\n\n    def __getstate__(self):\n        state_dict = copy.copy(self.__dict__)\n        # In case request is augmented by some foreign object such as\n        # credentials which handle auth\n        if \"request\" in state_dict:\n            del state_dict[\"request\"]\n        if \"connections\" in state_dict:\n            del state_dict[\"connections\"]\n        return state_dict\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.connections = {}\n\n    def _auth_from_challenge(self, host, request_uri, headers, response, content):\n        \"\"\"A generator that creates Authorization objects\n           that can be applied to requests.\n        \"\"\"\n        challenges = _parse_www_authenticate(response, \"www-authenticate\")\n        for cred in self.credentials.iter(host):\n            for scheme in AUTH_SCHEME_ORDER:\n                if scheme in challenges:\n                    yield AUTH_SCHEME_CLASSES[scheme](\n                        cred, host, request_uri, headers, response, content, self\n                    )\n\n    def add_credentials(self, name, password, domain=\"\"):\n        \"\"\"Add a name and password that will be used\n        any time a request requires authentication.\"\"\"\n        self.credentials.add(name, password, domain)\n\n    def add_certificate(self, key, cert, domain, password=None):\n        \"\"\"Add a key and cert that will be used\n        any time a request requires authentication.\"\"\"\n        self.certificates.add(key, cert, domain, password)\n\n    def clear_credentials(self):\n        \"\"\"Remove all the names and passwords\n        that are used for authentication\"\"\"\n        self.credentials.clear()\n        self.authorizations = []\n\n    def _conn_request(self, conn, request_uri, method, body, headers):\n        i = 0\n        seen_bad_status_line = False\n        while i < RETRIES:\n            i += 1\n            try:\n                if hasattr(conn, \"sock\") and conn.sock is None:\n                    conn.connect()\n                conn.request(method, request_uri, body, headers)\n            except socket.timeout:\n                raise\n            except socket.gaierror:\n                conn.close()\n                raise ServerNotFoundError(\"Unable to find the server at %s\" % conn.host)\n            except ssl_SSLError:\n                conn.close()\n                raise\n            except socket.error as e:\n                err = 0\n                if hasattr(e, \"args\"):\n                    err = getattr(e, \"args\")[0]\n                else:\n                    err = e.errno\n                if err == errno.ECONNREFUSED:  # Connection refused\n                    raise\n                if err in (errno.ENETUNREACH, errno.EADDRNOTAVAIL) and i < RETRIES:\n                    continue  # retry on potentially transient socket errors\n            except httplib.HTTPException:\n                # Just because the server closed the connection doesn't apparently mean\n                # that the server didn't send a response.\n                if hasattr(conn, \"sock\") and conn.sock is None:\n                    if i < RETRIES - 1:\n                        conn.close()\n                        conn.connect()\n                        continue\n                    else:\n                        conn.close()\n                        raise\n                if i < RETRIES - 1:\n                    conn.close()\n                    conn.connect()\n                    continue\n            try:\n                response = conn.getresponse()\n            except httplib.BadStatusLine:\n                # If we get a BadStatusLine on the first try then that means\n                # the connection just went stale, so retry regardless of the\n                # number of RETRIES set.\n                if not seen_bad_status_line and i == 1:\n                    i = 0\n                    seen_bad_status_line = True\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    conn.close()\n                    raise\n            except (socket.error, httplib.HTTPException):\n                if i < RETRIES - 1:\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    conn.close()\n                    raise\n            else:\n                content = \"\"\n                if method == \"HEAD\":\n                    conn.close()\n                else:\n                    content = response.read()\n                response = Response(response)\n                if method != \"HEAD\":\n                    content = _decompressContent(response, content)\n            break\n        return (response, content)\n\n    def _request(\n        self,\n        conn,\n        host,\n        absolute_uri,\n        request_uri,\n        method,\n        body,\n        headers,\n        redirections,\n        cachekey,\n    ):\n        \"\"\"Do the actual request using the connection object\n        and also follow one level of redirects if necessary\"\"\"\n\n        auths = [\n            (auth.depth(request_uri), auth)\n            for auth in self.authorizations\n            if auth.inscope(host, request_uri)\n        ]\n        auth = auths and sorted(auths)[0][1] or None\n        if auth:\n            auth.request(method, request_uri, headers, body)\n\n        (response, content) = self._conn_request(\n            conn, request_uri, method, body, headers\n        )\n\n        if auth:\n            if auth.response(response, body):\n                auth.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                response._stale_digest = 1\n\n        if response.status == 401:\n            for authorization in self._auth_from_challenge(\n                host, request_uri, headers, response, content\n            ):\n                authorization.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                if response.status != 401:\n                    self.authorizations.append(authorization)\n                    authorization.response(response, body)\n                    break\n\n        if (\n            self.follow_all_redirects\n            or method in self.safe_methods\n            or response.status in (303, 308)\n        ):\n            if self.follow_redirects and response.status in self.redirect_codes:\n                # Pick out the location header and basically start from the beginning\n                # remembering first to strip the ETag header and decrement our 'depth'\n                if redirections:\n                    if \"location\" not in response and response.status != 300:\n                        raise RedirectMissingLocation(\n                            _(\n                                \"Redirected but the response is missing a Location: header.\"\n                            ),\n                            response,\n                            content,\n                        )\n                    # Fix-up relative redirects (which violate an RFC 2616 MUST)\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        (scheme, authority, path, query, fragment) = parse_uri(location)\n                        if authority == None:\n                            response[\"location\"] = urlparse.urljoin(\n                                absolute_uri, location\n                            )\n                    if response.status == 308 or (response.status == 301 and method in self.safe_methods):\n                        response[\"-x-permanent-redirect-url\"] = response[\"location\"]\n                        if \"content-location\" not in response:\n                            response[\"content-location\"] = absolute_uri\n                        _updateCache(headers, response, content, self.cache, cachekey)\n                    if \"if-none-match\" in headers:\n                        del headers[\"if-none-match\"]\n                    if \"if-modified-since\" in headers:\n                        del headers[\"if-modified-since\"]\n                    if (\n                        \"authorization\" in headers\n                        and not self.forward_authorization_headers\n                    ):\n                        del headers[\"authorization\"]\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        old_response = copy.deepcopy(response)\n                        if \"content-location\" not in old_response:\n                            old_response[\"content-location\"] = absolute_uri\n                        redirect_method = method\n                        if response.status in [302, 303]:\n                            redirect_method = \"GET\"\n                            body = None\n                        (response, content) = self.request(\n                            location,\n                            method=redirect_method,\n                            body=body,\n                            headers=headers,\n                            redirections=redirections - 1,\n                        )\n                        response.previous = old_response\n                else:\n                    raise RedirectLimit(\n                        \"Redirected more times than rediection_limit allows.\",\n                        response,\n                        content,\n                    )\n            elif response.status in [200, 203] and method in self.safe_methods:\n                # Don't cache 206's since we aren't going to handle byte range requests\n                if \"content-location\" not in response:\n                    response[\"content-location\"] = absolute_uri\n                _updateCache(headers, response, content, self.cache, cachekey)\n\n        return (response, content)\n\n    def _normalize_headers(self, headers):\n        return _normalize_headers(headers)\n\n    # Need to catch and rebrand some exceptions\n    # Then need to optionally turn all exceptions into status codes\n    # including all socket.* and httplib.* exceptions.\n\n    def request(\n        self,\n        uri,\n        method=\"GET\",\n        body=None,\n        headers=None,\n        redirections=DEFAULT_MAX_REDIRECTS,\n        connection_type=None,\n    ):\n        \"\"\" Performs a single HTTP request.\n\n        The 'uri' is the URI of the HTTP resource and can begin with either\n        'http' or 'https'. The value of 'uri' must be an absolute URI.\n\n        The 'method' is the HTTP method to perform, such as GET, POST, DELETE,\n        etc. There is no restriction on the methods allowed.\n\n        The 'body' is the entity body to be sent with the request. It is a\n        string object.\n\n        Any extra headers that are to be sent with the request should be\n        provided in the 'headers' dictionary.\n\n        The maximum number of redirect to follow before raising an\n        exception is 'redirections. The default is 5.\n\n        The return value is a tuple of (response, content), the first\n        being and instance of the 'Response' class, the second being\n        a string that contains the response entity body.\n        \"\"\"\n        conn_key = ''\n\n        try:\n            if headers is None:\n                headers = {}\n            else:\n                headers = self._normalize_headers(headers)\n\n            if \"user-agent\" not in headers:\n                headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n\n            uri = iri2uri(uri)\n            # Prevent CWE-75 space injection to manipulate request via part of uri.\n            # Prevent CWE-93 CRLF injection to modify headers via part of uri.\n            uri = uri.replace(\" \", \"%20\").replace(\"\\r\", \"%0D\").replace(\"\\n\", \"%0A\")\n\n            (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n\n            proxy_info = self._get_proxy_info(scheme, authority)\n\n            conn_key = scheme + \":\" + authority\n            conn = self.connections.get(conn_key)\n            if conn is None:\n                if not connection_type:\n                    connection_type = SCHEME_TO_CONNECTION[scheme]\n                certs = list(self.certificates.iter(authority))\n                if scheme == \"https\":\n                    if certs:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            key_file=certs[0][0],\n                            cert_file=certs[0][1],\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                            key_password=certs[0][2],\n                        )\n                    else:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                        )\n                else:\n                    conn = self.connections[conn_key] = connection_type(\n                        authority, timeout=self.timeout, proxy_info=proxy_info\n                    )\n                conn.set_debuglevel(debuglevel)\n\n            if \"range\" not in headers and \"accept-encoding\" not in headers:\n                headers[\"accept-encoding\"] = \"gzip, deflate\"\n\n            info = email.Message.Message()\n            cachekey = None\n            cached_value = None\n            if self.cache:\n                cachekey = defrag_uri.encode(\"utf-8\")\n                cached_value = self.cache.get(cachekey)\n                if cached_value:\n                    # info = email.message_from_string(cached_value)\n                    #\n                    # Need to replace the line above with the kludge below\n                    # to fix the non-existent bug not fixed in this\n                    # bug report: http://mail.python.org/pipermail/python-bugs-list/2005-September/030289.html\n                    try:\n                        info, content = cached_value.split(\"\\r\\n\\r\\n\", 1)\n                        feedparser = email.FeedParser.FeedParser()\n                        feedparser.feed(info)\n                        info = feedparser.close()\n                        feedparser._parse = None\n                    except (IndexError, ValueError):\n                        self.cache.delete(cachekey)\n                        cachekey = None\n                        cached_value = None\n\n            if (\n                method in self.optimistic_concurrency_methods\n                and self.cache\n                and \"etag\" in info\n                and not self.ignore_etag\n                and \"if-match\" not in headers\n            ):\n                # http://www.w3.org/1999/04/Editing/\n                headers[\"if-match\"] = info[\"etag\"]\n\n            # https://tools.ietf.org/html/rfc7234\n            # A cache MUST invalidate the effective Request URI as well as [...] Location and Content-Location\n            # when a non-error status code is received in response to an unsafe request method.\n            if self.cache and cachekey and method not in self.safe_methods:\n                self.cache.delete(cachekey)\n\n            # Check the vary header in the cache to see if this request\n            # matches what varies in the cache.\n            if method in self.safe_methods and \"vary\" in info:\n                vary = info[\"vary\"]\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    value = info[key]\n                    if headers.get(header, None) != value:\n                        cached_value = None\n                        break\n\n            if (\n                self.cache\n                and cached_value\n                and (method in self.safe_methods or info[\"status\"] == \"308\")\n                and \"range\" not in headers\n            ):\n                redirect_method = method\n                if info[\"status\"] not in (\"307\", \"308\"):\n                    redirect_method = \"GET\"\n                if \"-x-permanent-redirect-url\" in info:\n                    # Should cached permanent redirects be counted in our redirection count? For now, yes.\n                    if redirections <= 0:\n                        raise RedirectLimit(\n                            \"Redirected more times than rediection_limit allows.\",\n                            {},\n                            \"\",\n                        )\n                    (response, new_content) = self.request(\n                        info[\"-x-permanent-redirect-url\"],\n                        method=redirect_method,\n                        headers=headers,\n                        redirections=redirections - 1,\n                    )\n                    response.previous = Response(info)\n                    response.previous.fromcache = True\n                else:\n                    # Determine our course of action:\n                    #   Is the cached entry fresh or stale?\n                    #   Has the client requested a non-cached response?\n                    #\n                    # There seems to be three possible answers:\n                    # 1. [FRESH] Return the cache entry w/o doing a GET\n                    # 2. [STALE] Do the GET (but add in cache validators if available)\n                    # 3. [TRANSPARENT] Do a GET w/o any cache validators (Cache-Control: no-cache) on the request\n                    entry_disposition = _entry_disposition(info, headers)\n\n                    if entry_disposition == \"FRESH\":\n                        if not cached_value:\n                            info[\"status\"] = \"504\"\n                            content = \"\"\n                        response = Response(info)\n                        if cached_value:\n                            response.fromcache = True\n                        return (response, content)\n\n                    if entry_disposition == \"STALE\":\n                        if (\n                            \"etag\" in info\n                            and not self.ignore_etag\n                            and not \"if-none-match\" in headers\n                        ):\n                            headers[\"if-none-match\"] = info[\"etag\"]\n                        if \"last-modified\" in info and not \"last-modified\" in headers:\n                            headers[\"if-modified-since\"] = info[\"last-modified\"]\n                    elif entry_disposition == \"TRANSPARENT\":\n                        pass\n\n                    (response, new_content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n\n                if response.status == 304 and method == \"GET\":\n                    # Rewrite the cache entry with the new end-to-end headers\n                    # Take all headers that are in response\n                    # and overwrite their values in info.\n                    # unless they are hop-by-hop, or are listed in the connection header.\n\n                    for key in _get_end2end_headers(response):\n                        info[key] = response[key]\n                    merged_response = Response(info)\n                    if hasattr(response, \"_stale_digest\"):\n                        merged_response._stale_digest = response._stale_digest\n                    _updateCache(\n                        headers, merged_response, content, self.cache, cachekey\n                    )\n                    response = merged_response\n                    response.status = 200\n                    response.fromcache = True\n\n                elif response.status == 200:\n                    content = new_content\n                else:\n                    self.cache.delete(cachekey)\n                    content = new_content\n            else:\n                cc = _parse_cache_control(headers)\n                if \"only-if-cached\" in cc:\n                    info[\"status\"] = \"504\"\n                    response = Response(info)\n                    content = \"\"\n                else:\n                    (response, content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n        except Exception as e:\n            is_timeout = isinstance(e, socket.timeout)\n            if is_timeout:\n                conn = self.connections.pop(conn_key, None)\n                if conn:\n                    conn.close()\n\n            if self.force_exception_to_status_code:\n                if isinstance(e, HttpLib2ErrorWithResponse):\n                    response = e.response\n                    content = e.content\n                    response.status = 500\n                    response.reason = str(e)\n                elif is_timeout:\n                    content = \"Request Timeout\"\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"408\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Request Timeout\"\n                else:\n                    content = str(e)\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"400\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Bad Request\"\n            else:\n                raise\n\n        return (response, content)\n\n    def _get_proxy_info(self, scheme, authority):\n        \"\"\"Return a ProxyInfo instance (or None) based on the scheme\n        and authority.\n        \"\"\"\n        hostname, port = urllib.splitport(authority)\n        proxy_info = self.proxy_info\n        if callable(proxy_info):\n            proxy_info = proxy_info(scheme)\n\n        if hasattr(proxy_info, \"applies_to\") and not proxy_info.applies_to(hostname):\n            proxy_info = None\n        return proxy_info\n\n\nclass Response(dict):\n    \"\"\"An object more like email.Message than httplib.HTTPResponse.\"\"\"\n\n    \"\"\"Is this response from our local cache\"\"\"\n    fromcache = False\n    \"\"\"HTTP protocol version used by server.\n\n    10 for HTTP/1.0, 11 for HTTP/1.1.\n    \"\"\"\n    version = 11\n\n    \"Status code returned by server. \"\n    status = 200\n    \"\"\"Reason phrase returned by server.\"\"\"\n    reason = \"Ok\"\n\n    previous = None\n\n    def __init__(self, info):\n        # info is either an email.Message or\n        # an httplib.HTTPResponse object.\n        if isinstance(info, httplib.HTTPResponse):\n            for key, value in info.getheaders():\n                self[key.lower()] = value\n            self.status = info.status\n            self[\"status\"] = str(self.status)\n            self.reason = info.reason\n            self.version = info.version\n        elif isinstance(info, email.Message.Message):\n            for key, value in info.items():\n                self[key.lower()] = value\n            self.status = int(self[\"status\"])\n        else:\n            for key, value in info.iteritems():\n                self[key.lower()] = value\n            self.status = int(self.get(\"status\", self.status))\n            self.reason = self.get(\"reason\", self.reason)\n\n    def __getattr__(self, name):\n        if name == \"dict\":\n            return self\n        else:\n            raise AttributeError(name)\n", "code_before": "\"\"\"Small, fast HTTP client library for Python.\n\nFeatures persistent connections, cache, and Google App Engine Standard\nEnvironment support.\n\"\"\"\n\nfrom __future__ import print_function\n\n__author__ = \"Joe Gregorio (joe@bitworking.org)\"\n__copyright__ = \"Copyright 2006, Joe Gregorio\"\n__contributors__ = [\n    \"Thomas Broyer (t.broyer@ltgt.net)\",\n    \"James Antill\",\n    \"Xavier Verges Farrero\",\n    \"Jonathan Feinberg\",\n    \"Blair Zajac\",\n    \"Sam Ruby\",\n    \"Louis Nyffenegger\",\n    \"Alex Yu\",\n]\n__license__ = \"MIT\"\n__version__ = '0.17.4'\n\nimport base64\nimport calendar\nimport copy\nimport email\nimport email.FeedParser\nimport email.Message\nimport email.Utils\nimport errno\nimport gzip\nimport httplib\nimport os\nimport random\nimport re\nimport StringIO\nimport sys\nimport time\nimport urllib\nimport urlparse\nimport zlib\n\ntry:\n    from hashlib import sha1 as _sha, md5 as _md5\nexcept ImportError:\n    # prior to Python 2.5, these were separate modules\n    import sha\n    import md5\n\n    _sha = sha.new\n    _md5 = md5.new\nimport hmac\nfrom gettext import gettext as _\nimport socket\n\ntry:\n    from httplib2 import socks\nexcept ImportError:\n    try:\n        import socks\n    except (ImportError, AttributeError):\n        socks = None\n\n# Build the appropriate socket wrapper for ssl\nssl = None\nssl_SSLError = None\nssl_CertificateError = None\ntry:\n    import ssl  # python 2.6\nexcept ImportError:\n    pass\nif ssl is not None:\n    ssl_SSLError = getattr(ssl, \"SSLError\", None)\n    ssl_CertificateError = getattr(ssl, \"CertificateError\", None)\n\n\ndef _ssl_wrap_socket(\n    sock, key_file, cert_file, disable_validation, ca_certs, ssl_version, hostname, key_password\n):\n    if disable_validation:\n        cert_reqs = ssl.CERT_NONE\n    else:\n        cert_reqs = ssl.CERT_REQUIRED\n    if ssl_version is None:\n        ssl_version = ssl.PROTOCOL_SSLv23\n\n    if hasattr(ssl, \"SSLContext\"):  # Python 2.7.9\n        context = ssl.SSLContext(ssl_version)\n        context.verify_mode = cert_reqs\n        context.check_hostname = cert_reqs != ssl.CERT_NONE\n        if cert_file:\n            if key_password:\n                context.load_cert_chain(cert_file, key_file, key_password)\n            else:\n                context.load_cert_chain(cert_file, key_file)\n        if ca_certs:\n            context.load_verify_locations(ca_certs)\n        return context.wrap_socket(sock, server_hostname=hostname)\n    else:\n        if key_password:\n            raise NotSupportedOnThisPlatform(\"Certificate with password is not supported.\")\n        return ssl.wrap_socket(\n            sock,\n            keyfile=key_file,\n            certfile=cert_file,\n            cert_reqs=cert_reqs,\n            ca_certs=ca_certs,\n            ssl_version=ssl_version,\n        )\n\n\ndef _ssl_wrap_socket_unsupported(\n    sock, key_file, cert_file, disable_validation, ca_certs, ssl_version, hostname, key_password\n):\n    if not disable_validation:\n        raise CertificateValidationUnsupported(\n            \"SSL certificate validation is not supported without \"\n            \"the ssl module installed. To avoid this error, install \"\n            \"the ssl module, or explicity disable validation.\"\n        )\n    if key_password:\n        raise NotSupportedOnThisPlatform(\"Certificate with password is not supported.\")\n    ssl_sock = socket.ssl(sock, key_file, cert_file)\n    return httplib.FakeSocket(sock, ssl_sock)\n\n\nif ssl is None:\n    _ssl_wrap_socket = _ssl_wrap_socket_unsupported\n\nif sys.version_info >= (2, 3):\n    from .iri2uri import iri2uri\nelse:\n\n    def iri2uri(uri):\n        return uri\n\n\ndef has_timeout(timeout):  # python 2.6\n    if hasattr(socket, \"_GLOBAL_DEFAULT_TIMEOUT\"):\n        return timeout is not None and timeout is not socket._GLOBAL_DEFAULT_TIMEOUT\n    return timeout is not None\n\n\n__all__ = [\n    \"Http\",\n    \"Response\",\n    \"ProxyInfo\",\n    \"HttpLib2Error\",\n    \"RedirectMissingLocation\",\n    \"RedirectLimit\",\n    \"FailedToDecompressContent\",\n    \"UnimplementedDigestAuthOptionError\",\n    \"UnimplementedHmacDigestAuthOptionError\",\n    \"debuglevel\",\n    \"ProxiesUnavailableError\",\n]\n\n# The httplib debug level, set to a non-zero value to get debug output\ndebuglevel = 0\n\n# A request will be tried 'RETRIES' times if it fails at the socket/connection level.\nRETRIES = 2\n\n# Python 2.3 support\nif sys.version_info < (2, 4):\n\n    def sorted(seq):\n        seq.sort()\n        return seq\n\n\n# Python 2.3 support\ndef HTTPResponse__getheaders(self):\n    \"\"\"Return list of (header, value) tuples.\"\"\"\n    if self.msg is None:\n        raise httplib.ResponseNotReady()\n    return self.msg.items()\n\n\nif not hasattr(httplib.HTTPResponse, \"getheaders\"):\n    httplib.HTTPResponse.getheaders = HTTPResponse__getheaders\n\n\n# All exceptions raised here derive from HttpLib2Error\nclass HttpLib2Error(Exception):\n    pass\n\n\n# Some exceptions can be caught and optionally\n# be turned back into responses.\nclass HttpLib2ErrorWithResponse(HttpLib2Error):\n    def __init__(self, desc, response, content):\n        self.response = response\n        self.content = content\n        HttpLib2Error.__init__(self, desc)\n\n\nclass RedirectMissingLocation(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass RedirectLimit(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass FailedToDecompressContent(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass UnimplementedDigestAuthOptionError(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass UnimplementedHmacDigestAuthOptionError(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass MalformedHeader(HttpLib2Error):\n    pass\n\n\nclass RelativeURIError(HttpLib2Error):\n    pass\n\n\nclass ServerNotFoundError(HttpLib2Error):\n    pass\n\n\nclass ProxiesUnavailableError(HttpLib2Error):\n    pass\n\n\nclass CertificateValidationUnsupported(HttpLib2Error):\n    pass\n\n\nclass SSLHandshakeError(HttpLib2Error):\n    pass\n\n\nclass NotSupportedOnThisPlatform(HttpLib2Error):\n    pass\n\n\nclass CertificateHostnameMismatch(SSLHandshakeError):\n    def __init__(self, desc, host, cert):\n        HttpLib2Error.__init__(self, desc)\n        self.host = host\n        self.cert = cert\n\n\nclass NotRunningAppEngineEnvironment(HttpLib2Error):\n    pass\n\n\n# Open Items:\n# -----------\n# Proxy support\n\n# Are we removing the cached content too soon on PUT (only delete on 200 Maybe?)\n\n# Pluggable cache storage (supports storing the cache in\n#   flat files by default. We need a plug-in architecture\n#   that can support Berkeley DB and Squid)\n\n# == Known Issues ==\n# Does not handle a resource that uses conneg and Last-Modified but no ETag as a cache validator.\n# Does not handle Cache-Control: max-stale\n# Does not use Age: headers when calculating cache freshness.\n\n# The number of redirections to follow before giving up.\n# Note that only GET redirects are automatically followed.\n# Will also honor 301 requests by saving that info and never\n# requesting that URI again.\nDEFAULT_MAX_REDIRECTS = 5\n\nfrom httplib2 import certs\nCA_CERTS = certs.where()\n\n# Which headers are hop-by-hop headers by default\nHOP_BY_HOP = [\n    \"connection\",\n    \"keep-alive\",\n    \"proxy-authenticate\",\n    \"proxy-authorization\",\n    \"te\",\n    \"trailers\",\n    \"transfer-encoding\",\n    \"upgrade\",\n]\n\n# https://tools.ietf.org/html/rfc7231#section-8.1.3\nSAFE_METHODS = (\"GET\", \"HEAD\")  # TODO add \"OPTIONS\", \"TRACE\"\n\n# To change, assign to `Http().redirect_codes`\nREDIRECT_CODES = frozenset((300, 301, 302, 303, 307, 308))\n\n\ndef _get_end2end_headers(response):\n    hopbyhop = list(HOP_BY_HOP)\n    hopbyhop.extend([x.strip() for x in response.get(\"connection\", \"\").split(\",\")])\n    return [header for header in response.keys() if header not in hopbyhop]\n\n\nURI = re.compile(r\"^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\\?([^#]*))?(#(.*))?\")\n\n\ndef parse_uri(uri):\n    \"\"\"Parses a URI using the regex given in Appendix B of RFC 3986.\n\n        (scheme, authority, path, query, fragment) = parse_uri(uri)\n    \"\"\"\n    groups = URI.match(uri).groups()\n    return (groups[1], groups[3], groups[4], groups[6], groups[8])\n\n\ndef urlnorm(uri):\n    (scheme, authority, path, query, fragment) = parse_uri(uri)\n    if not scheme or not authority:\n        raise RelativeURIError(\"Only absolute URIs are allowed. uri = %s\" % uri)\n    authority = authority.lower()\n    scheme = scheme.lower()\n    if not path:\n        path = \"/\"\n    # Could do syntax based normalization of the URI before\n    # computing the digest. See Section 6.2.2 of Std 66.\n    request_uri = query and \"?\".join([path, query]) or path\n    scheme = scheme.lower()\n    defrag_uri = scheme + \"://\" + authority + request_uri\n    return scheme, authority, request_uri, defrag_uri\n\n\n# Cache filename construction (original borrowed from Venus http://intertwingly.net/code/venus/)\nre_url_scheme = re.compile(r\"^\\w+://\")\nre_unsafe = re.compile(r\"[^\\w\\-_.()=!]+\")\n\n\ndef safename(filename):\n    \"\"\"Return a filename suitable for the cache.\n    Strips dangerous and common characters to create a filename we\n    can use to store the cache in.\n    \"\"\"\n    if isinstance(filename, str):\n        filename_bytes = filename\n        filename = filename.decode(\"utf-8\")\n    else:\n        filename_bytes = filename.encode(\"utf-8\")\n    filemd5 = _md5(filename_bytes).hexdigest()\n    filename = re_url_scheme.sub(\"\", filename)\n    filename = re_unsafe.sub(\"\", filename)\n\n    # limit length of filename (vital for Windows)\n    # https://github.com/httplib2/httplib2/pull/74\n    # C:\\Users\\    <username>    \\AppData\\Local\\Temp\\  <safe_filename>  ,   <md5>\n    #   9 chars + max 104 chars  +     20 chars      +       x       +  1  +  32  = max 259 chars\n    # Thus max safe filename x = 93 chars. Let it be 90 to make a round sum:\n    filename = filename[:90]\n\n    return \",\".join((filename, filemd5))\n\n\nNORMALIZE_SPACE = re.compile(r\"(?:\\r\\n)?[ \\t]+\")\n\n\ndef _normalize_headers(headers):\n    return dict(\n        [\n            (key.lower(), NORMALIZE_SPACE.sub(value, \" \").strip())\n            for (key, value) in headers.iteritems()\n        ]\n    )\n\n\ndef _parse_cache_control(headers):\n    retval = {}\n    if \"cache-control\" in headers:\n        parts = headers[\"cache-control\"].split(\",\")\n        parts_with_args = [\n            tuple([x.strip().lower() for x in part.split(\"=\", 1)])\n            for part in parts\n            if -1 != part.find(\"=\")\n        ]\n        parts_wo_args = [\n            (name.strip().lower(), 1) for name in parts if -1 == name.find(\"=\")\n        ]\n        retval = dict(parts_with_args + parts_wo_args)\n    return retval\n\n\n# Whether to use a strict mode to parse WWW-Authenticate headers\n# Might lead to bad results in case of ill-formed header value,\n# so disabled by default, falling back to relaxed parsing.\n# Set to true to turn on, usefull for testing servers.\nUSE_WWW_AUTH_STRICT_PARSING = 0\n\n# In regex below:\n#    [^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+             matches a \"token\" as defined by HTTP\n#    \"(?:[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?\"    matches a \"quoted-string\" as defined by HTTP, when LWS have already been replaced by a single space\n# Actually, as an auth-param value can be either a token or a quoted-string, they are combined in a single pattern which matches both:\n#    \\\"?((?<=\\\")(?:[^\\0-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?(?=\\\")|(?<!\\\")[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+(?!\\\"))\\\"?\nWWW_AUTH_STRICT = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?(?=\\\")|(?<!\\\")[^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+(?!\\\"))\\\"?)(.*)$\"\n)\nWWW_AUTH_RELAXED = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^ \\t\\r\\n=]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\\\\\\"]|\\\\.)*?(?=\\\")|(?<!\\\")[^ \\t\\r\\n,]+(?!\\\"))\\\"?)(.*)$\"\n)\nUNQUOTE_PAIRS = re.compile(r\"\\\\(.)\")\n\n\ndef _parse_www_authenticate(headers, headername=\"www-authenticate\"):\n    \"\"\"Returns a dictionary of dictionaries, one dict\n    per auth_scheme.\"\"\"\n    retval = {}\n    if headername in headers:\n        try:\n\n            authenticate = headers[headername].strip()\n            www_auth = (\n                USE_WWW_AUTH_STRICT_PARSING and WWW_AUTH_STRICT or WWW_AUTH_RELAXED\n            )\n            while authenticate:\n                # Break off the scheme at the beginning of the line\n                if headername == \"authentication-info\":\n                    (auth_scheme, the_rest) = (\"digest\", authenticate)\n                else:\n                    (auth_scheme, the_rest) = authenticate.split(\" \", 1)\n                # Now loop over all the key value pairs that come after the scheme,\n                # being careful not to roll into the next scheme\n                match = www_auth.search(the_rest)\n                auth_params = {}\n                while match:\n                    if match and len(match.groups()) == 3:\n                        (key, value, the_rest) = match.groups()\n                        auth_params[key.lower()] = UNQUOTE_PAIRS.sub(\n                            r\"\\1\", value\n                        )  # '\\\\'.join([x.replace('\\\\', '') for x in value.split('\\\\\\\\')])\n                    match = www_auth.search(the_rest)\n                retval[auth_scheme.lower()] = auth_params\n                authenticate = the_rest.strip()\n\n        except ValueError:\n            raise MalformedHeader(\"WWW-Authenticate\")\n    return retval\n\n\n# TODO: add current time as _entry_disposition argument to avoid sleep in tests\ndef _entry_disposition(response_headers, request_headers):\n    \"\"\"Determine freshness from the Date, Expires and Cache-Control headers.\n\n    We don't handle the following:\n\n    1. Cache-Control: max-stale\n    2. Age: headers are not used in the calculations.\n\n    Not that this algorithm is simpler than you might think\n    because we are operating as a private (non-shared) cache.\n    This lets us ignore 's-maxage'. We can also ignore\n    'proxy-invalidate' since we aren't a proxy.\n    We will never return a stale document as\n    fresh as a design decision, and thus the non-implementation\n    of 'max-stale'. This also lets us safely ignore 'must-revalidate'\n    since we operate as if every server has sent 'must-revalidate'.\n    Since we are private we get to ignore both 'public' and\n    'private' parameters. We also ignore 'no-transform' since\n    we don't do any transformations.\n    The 'no-store' parameter is handled at a higher level.\n    So the only Cache-Control parameters we look at are:\n\n    no-cache\n    only-if-cached\n    max-age\n    min-fresh\n    \"\"\"\n\n    retval = \"STALE\"\n    cc = _parse_cache_control(request_headers)\n    cc_response = _parse_cache_control(response_headers)\n\n    if (\n        \"pragma\" in request_headers\n        and request_headers[\"pragma\"].lower().find(\"no-cache\") != -1\n    ):\n        retval = \"TRANSPARENT\"\n        if \"cache-control\" not in request_headers:\n            request_headers[\"cache-control\"] = \"no-cache\"\n    elif \"no-cache\" in cc:\n        retval = \"TRANSPARENT\"\n    elif \"no-cache\" in cc_response:\n        retval = \"STALE\"\n    elif \"only-if-cached\" in cc:\n        retval = \"FRESH\"\n    elif \"date\" in response_headers:\n        date = calendar.timegm(email.Utils.parsedate_tz(response_headers[\"date\"]))\n        now = time.time()\n        current_age = max(0, now - date)\n        if \"max-age\" in cc_response:\n            try:\n                freshness_lifetime = int(cc_response[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        elif \"expires\" in response_headers:\n            expires = email.Utils.parsedate_tz(response_headers[\"expires\"])\n            if None == expires:\n                freshness_lifetime = 0\n            else:\n                freshness_lifetime = max(0, calendar.timegm(expires) - date)\n        else:\n            freshness_lifetime = 0\n        if \"max-age\" in cc:\n            try:\n                freshness_lifetime = int(cc[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        if \"min-fresh\" in cc:\n            try:\n                min_fresh = int(cc[\"min-fresh\"])\n            except ValueError:\n                min_fresh = 0\n            current_age += min_fresh\n        if freshness_lifetime > current_age:\n            retval = \"FRESH\"\n    return retval\n\n\ndef _decompressContent(response, new_content):\n    content = new_content\n    try:\n        encoding = response.get(\"content-encoding\", None)\n        if encoding in [\"gzip\", \"deflate\"]:\n            if encoding == \"gzip\":\n                content = gzip.GzipFile(fileobj=StringIO.StringIO(new_content)).read()\n            if encoding == \"deflate\":\n                content = zlib.decompress(content, -zlib.MAX_WBITS)\n            response[\"content-length\"] = str(len(content))\n            # Record the historical presence of the encoding in a way the won't interfere.\n            response[\"-content-encoding\"] = response[\"content-encoding\"]\n            del response[\"content-encoding\"]\n    except (IOError, zlib.error):\n        content = \"\"\n        raise FailedToDecompressContent(\n            _(\"Content purported to be compressed with %s but failed to decompress.\")\n            % response.get(\"content-encoding\"),\n            response,\n            content,\n        )\n    return content\n\n\ndef _updateCache(request_headers, response_headers, content, cache, cachekey):\n    if cachekey:\n        cc = _parse_cache_control(request_headers)\n        cc_response = _parse_cache_control(response_headers)\n        if \"no-store\" in cc or \"no-store\" in cc_response:\n            cache.delete(cachekey)\n        else:\n            info = email.Message.Message()\n            for key, value in response_headers.iteritems():\n                if key not in [\"status\", \"content-encoding\", \"transfer-encoding\"]:\n                    info[key] = value\n\n            # Add annotations to the cache to indicate what headers\n            # are variant for this request.\n            vary = response_headers.get(\"vary\", None)\n            if vary:\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    try:\n                        info[key] = request_headers[header]\n                    except KeyError:\n                        pass\n\n            status = response_headers.status\n            if status == 304:\n                status = 200\n\n            status_header = \"status: %d\\r\\n\" % status\n\n            header_str = info.as_string()\n\n            header_str = re.sub(\"\\r(?!\\n)|(?<!\\r)\\n\", \"\\r\\n\", header_str)\n            text = \"\".join([status_header, header_str, content])\n\n            cache.set(cachekey, text)\n\n\ndef _cnonce():\n    dig = _md5(\n        \"%s:%s\"\n        % (time.ctime(), [\"0123456789\"[random.randrange(0, 9)] for i in range(20)])\n    ).hexdigest()\n    return dig[:16]\n\n\ndef _wsse_username_token(cnonce, iso_now, password):\n    return base64.b64encode(\n        _sha(\"%s%s%s\" % (cnonce, iso_now, password)).digest()\n    ).strip()\n\n\n# For credentials we need two things, first\n# a pool of credential to try (not necesarily tied to BAsic, Digest, etc.)\n# Then we also need a list of URIs that have already demanded authentication\n# That list is tricky since sub-URIs can take the same auth, or the\n# auth scheme may change as you descend the tree.\n# So we also need each Auth instance to be able to tell us\n# how close to the 'top' it is.\n\n\nclass Authentication(object):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        self.path = path\n        self.host = host\n        self.credentials = credentials\n        self.http = http\n\n    def depth(self, request_uri):\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        return request_uri[len(self.path) :].count(\"/\")\n\n    def inscope(self, host, request_uri):\n        # XXX Should we normalize the request_uri?\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        return (host == self.host) and path.startswith(self.path)\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header. Over-ride this in sub-classes.\"\"\"\n        pass\n\n    def response(self, response, content):\n        \"\"\"Gives us a chance to update with new nonces\n        or such returned from the last authorized response.\n        Over-rise this in sub-classes if necessary.\n\n        Return TRUE is the request is to be retried, for\n        example Digest may return stale=true.\n        \"\"\"\n        return False\n\n\nclass BasicAuthentication(Authentication):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = (\n            \"Basic \" + base64.b64encode(\"%s:%s\" % self.credentials).strip()\n        )\n\n\nclass DigestAuthentication(Authentication):\n    \"\"\"Only do qop='auth' and MD5, since that\n    is all Apache currently implements\"\"\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        self.challenge = challenge[\"digest\"]\n        qop = self.challenge.get(\"qop\", \"auth\")\n        self.challenge[\"qop\"] = (\n            (\"auth\" in [x.strip() for x in qop.split()]) and \"auth\" or None\n        )\n        if self.challenge[\"qop\"] is None:\n            raise UnimplementedDigestAuthOptionError(\n                _(\"Unsupported value for qop: %s.\" % qop)\n            )\n        self.challenge[\"algorithm\"] = self.challenge.get(\"algorithm\", \"MD5\").upper()\n        if self.challenge[\"algorithm\"] != \"MD5\":\n            raise UnimplementedDigestAuthOptionError(\n                _(\"Unsupported value for algorithm: %s.\" % self.challenge[\"algorithm\"])\n            )\n        self.A1 = \"\".join(\n            [\n                self.credentials[0],\n                \":\",\n                self.challenge[\"realm\"],\n                \":\",\n                self.credentials[1],\n            ]\n        )\n        self.challenge[\"nc\"] = 1\n\n    def request(self, method, request_uri, headers, content, cnonce=None):\n        \"\"\"Modify the request headers\"\"\"\n        H = lambda x: _md5(x).hexdigest()\n        KD = lambda s, d: H(\"%s:%s\" % (s, d))\n        A2 = \"\".join([method, \":\", request_uri])\n        self.challenge[\"cnonce\"] = cnonce or _cnonce()\n        request_digest = '\"%s\"' % KD(\n            H(self.A1),\n            \"%s:%s:%s:%s:%s\"\n            % (\n                self.challenge[\"nonce\"],\n                \"%08x\" % self.challenge[\"nc\"],\n                self.challenge[\"cnonce\"],\n                self.challenge[\"qop\"],\n                H(A2),\n            ),\n        )\n        headers[\"authorization\"] = (\n            'Digest username=\"%s\", realm=\"%s\", nonce=\"%s\", '\n            'uri=\"%s\", algorithm=%s, response=%s, qop=%s, '\n            'nc=%08x, cnonce=\"%s\"'\n        ) % (\n            self.credentials[0],\n            self.challenge[\"realm\"],\n            self.challenge[\"nonce\"],\n            request_uri,\n            self.challenge[\"algorithm\"],\n            request_digest,\n            self.challenge[\"qop\"],\n            self.challenge[\"nc\"],\n            self.challenge[\"cnonce\"],\n        )\n        if self.challenge.get(\"opaque\"):\n            headers[\"authorization\"] += ', opaque=\"%s\"' % self.challenge[\"opaque\"]\n        self.challenge[\"nc\"] += 1\n\n    def response(self, response, content):\n        if \"authentication-info\" not in response:\n            challenge = _parse_www_authenticate(response, \"www-authenticate\").get(\n                \"digest\", {}\n            )\n            if \"true\" == challenge.get(\"stale\"):\n                self.challenge[\"nonce\"] = challenge[\"nonce\"]\n                self.challenge[\"nc\"] = 1\n                return True\n        else:\n            updated_challenge = _parse_www_authenticate(\n                response, \"authentication-info\"\n            ).get(\"digest\", {})\n\n            if \"nextnonce\" in updated_challenge:\n                self.challenge[\"nonce\"] = updated_challenge[\"nextnonce\"]\n                self.challenge[\"nc\"] = 1\n        return False\n\n\nclass HmacDigestAuthentication(Authentication):\n    \"\"\"Adapted from Robert Sayre's code and DigestAuthentication above.\"\"\"\n\n    __author__ = \"Thomas Broyer (t.broyer@ltgt.net)\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        self.challenge = challenge[\"hmacdigest\"]\n        # TODO: self.challenge['domain']\n        self.challenge[\"reason\"] = self.challenge.get(\"reason\", \"unauthorized\")\n        if self.challenge[\"reason\"] not in [\"unauthorized\", \"integrity\"]:\n            self.challenge[\"reason\"] = \"unauthorized\"\n        self.challenge[\"salt\"] = self.challenge.get(\"salt\", \"\")\n        if not self.challenge.get(\"snonce\"):\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\"The challenge doesn't contain a server nonce, or this one is empty.\")\n            )\n        self.challenge[\"algorithm\"] = self.challenge.get(\"algorithm\", \"HMAC-SHA-1\")\n        if self.challenge[\"algorithm\"] not in [\"HMAC-SHA-1\", \"HMAC-MD5\"]:\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\"Unsupported value for algorithm: %s.\" % self.challenge[\"algorithm\"])\n            )\n        self.challenge[\"pw-algorithm\"] = self.challenge.get(\"pw-algorithm\", \"SHA-1\")\n        if self.challenge[\"pw-algorithm\"] not in [\"SHA-1\", \"MD5\"]:\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\n                    \"Unsupported value for pw-algorithm: %s.\"\n                    % self.challenge[\"pw-algorithm\"]\n                )\n            )\n        if self.challenge[\"algorithm\"] == \"HMAC-MD5\":\n            self.hashmod = _md5\n        else:\n            self.hashmod = _sha\n        if self.challenge[\"pw-algorithm\"] == \"MD5\":\n            self.pwhashmod = _md5\n        else:\n            self.pwhashmod = _sha\n        self.key = \"\".join(\n            [\n                self.credentials[0],\n                \":\",\n                self.pwhashmod.new(\n                    \"\".join([self.credentials[1], self.challenge[\"salt\"]])\n                )\n                .hexdigest()\n                .lower(),\n                \":\",\n                self.challenge[\"realm\"],\n            ]\n        )\n        self.key = self.pwhashmod.new(self.key).hexdigest().lower()\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers\"\"\"\n        keys = _get_end2end_headers(headers)\n        keylist = \"\".join([\"%s \" % k for k in keys])\n        headers_val = \"\".join([headers[k] for k in keys])\n        created = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n        cnonce = _cnonce()\n        request_digest = \"%s:%s:%s:%s:%s\" % (\n            method,\n            request_uri,\n            cnonce,\n            self.challenge[\"snonce\"],\n            headers_val,\n        )\n        request_digest = (\n            hmac.new(self.key, request_digest, self.hashmod).hexdigest().lower()\n        )\n        headers[\"authorization\"] = (\n            'HMACDigest username=\"%s\", realm=\"%s\", snonce=\"%s\",'\n            ' cnonce=\"%s\", uri=\"%s\", created=\"%s\", '\n            'response=\"%s\", headers=\"%s\"'\n        ) % (\n            self.credentials[0],\n            self.challenge[\"realm\"],\n            self.challenge[\"snonce\"],\n            cnonce,\n            request_uri,\n            created,\n            request_digest,\n            keylist,\n        )\n\n    def response(self, response, content):\n        challenge = _parse_www_authenticate(response, \"www-authenticate\").get(\n            \"hmacdigest\", {}\n        )\n        if challenge.get(\"reason\") in [\"integrity\", \"stale\"]:\n            return True\n        return False\n\n\nclass WsseAuthentication(Authentication):\n    \"\"\"This is thinly tested and should not be relied upon.\n    At this time there isn't any third party server to test against.\n    Blogger and TypePad implemented this algorithm at one point\n    but Blogger has since switched to Basic over HTTPS and\n    TypePad has implemented it wrong, by never issuing a 401\n    challenge but instead requiring your client to telepathically know that\n    their endpoint is expecting WSSE profile=\"UsernameToken\".\"\"\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = 'WSSE profile=\"UsernameToken\"'\n        iso_now = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n        cnonce = _cnonce()\n        password_digest = _wsse_username_token(cnonce, iso_now, self.credentials[1])\n        headers[\"X-WSSE\"] = (\n            'UsernameToken Username=\"%s\", PasswordDigest=\"%s\", '\n            'Nonce=\"%s\", Created=\"%s\"'\n        ) % (self.credentials[0], password_digest, cnonce, iso_now)\n\n\nclass GoogleLoginAuthentication(Authentication):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        from urllib import urlencode\n\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        service = challenge[\"googlelogin\"].get(\"service\", \"xapi\")\n        # Bloggger actually returns the service in the challenge\n        # For the rest we guess based on the URI\n        if service == \"xapi\" and request_uri.find(\"calendar\") > 0:\n            service = \"cl\"\n        # No point in guessing Base or Spreadsheet\n        # elif request_uri.find(\"spreadsheets\") > 0:\n        #    service = \"wise\"\n\n        auth = dict(\n            Email=credentials[0],\n            Passwd=credentials[1],\n            service=service,\n            source=headers[\"user-agent\"],\n        )\n        resp, content = self.http.request(\n            \"https://www.google.com/accounts/ClientLogin\",\n            method=\"POST\",\n            body=urlencode(auth),\n            headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n        )\n        lines = content.split(\"\\n\")\n        d = dict([tuple(line.split(\"=\", 1)) for line in lines if line])\n        if resp.status == 403:\n            self.Auth = \"\"\n        else:\n            self.Auth = d[\"Auth\"]\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = \"GoogleLogin Auth=\" + self.Auth\n\n\nAUTH_SCHEME_CLASSES = {\n    \"basic\": BasicAuthentication,\n    \"wsse\": WsseAuthentication,\n    \"digest\": DigestAuthentication,\n    \"hmacdigest\": HmacDigestAuthentication,\n    \"googlelogin\": GoogleLoginAuthentication,\n}\n\nAUTH_SCHEME_ORDER = [\"hmacdigest\", \"googlelogin\", \"digest\", \"wsse\", \"basic\"]\n\n\nclass FileCache(object):\n    \"\"\"Uses a local directory as a store for cached files.\n    Not really safe to use if multiple threads or processes are going to\n    be running on the same cache.\n    \"\"\"\n\n    def __init__(\n        self, cache, safe=safename\n    ):  # use safe=lambda x: md5.new(x).hexdigest() for the old behavior\n        self.cache = cache\n        self.safe = safe\n        if not os.path.exists(cache):\n            os.makedirs(self.cache)\n\n    def get(self, key):\n        retval = None\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        try:\n            f = file(cacheFullPath, \"rb\")\n            retval = f.read()\n            f.close()\n        except IOError:\n            pass\n        return retval\n\n    def set(self, key, value):\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        f = file(cacheFullPath, \"wb\")\n        f.write(value)\n        f.close()\n\n    def delete(self, key):\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        if os.path.exists(cacheFullPath):\n            os.remove(cacheFullPath)\n\n\nclass Credentials(object):\n    def __init__(self):\n        self.credentials = []\n\n    def add(self, name, password, domain=\"\"):\n        self.credentials.append((domain.lower(), name, password))\n\n    def clear(self):\n        self.credentials = []\n\n    def iter(self, domain):\n        for (cdomain, name, password) in self.credentials:\n            if cdomain == \"\" or domain == cdomain:\n                yield (name, password)\n\n\nclass KeyCerts(Credentials):\n    \"\"\"Identical to Credentials except that\n    name/password are mapped to key/cert.\"\"\"\n    def add(self, key, cert, domain, password):\n        self.credentials.append((domain.lower(), key, cert, password))\n\n    def iter(self, domain):\n        for (cdomain, key, cert, password) in self.credentials:\n            if cdomain == \"\" or domain == cdomain:\n                yield (key, cert, password)\n\n\nclass AllHosts(object):\n    pass\n\n\nclass ProxyInfo(object):\n    \"\"\"Collect information required to use a proxy.\"\"\"\n\n    bypass_hosts = ()\n\n    def __init__(\n        self,\n        proxy_type,\n        proxy_host,\n        proxy_port,\n        proxy_rdns=True,\n        proxy_user=None,\n        proxy_pass=None,\n        proxy_headers=None,\n    ):\n        \"\"\"Args:\n\n          proxy_type: The type of proxy server.  This must be set to one of\n          socks.PROXY_TYPE_XXX constants.  For example:  p =\n          ProxyInfo(proxy_type=socks.PROXY_TYPE_HTTP, proxy_host='localhost',\n          proxy_port=8000)\n          proxy_host: The hostname or IP address of the proxy server.\n          proxy_port: The port that the proxy server is running on.\n          proxy_rdns: If True (default), DNS queries will not be performed\n          locally, and instead, handed to the proxy to resolve.  This is useful\n          if the network does not allow resolution of non-local names. In\n          httplib2 0.9 and earlier, this defaulted to False.\n          proxy_user: The username used to authenticate with the proxy server.\n          proxy_pass: The password used to authenticate with the proxy server.\n          proxy_headers: Additional or modified headers for the proxy connect\n          request.\n        \"\"\"\n        self.proxy_type = proxy_type\n        self.proxy_host = proxy_host\n        self.proxy_port = proxy_port\n        self.proxy_rdns = proxy_rdns\n        self.proxy_user = proxy_user\n        self.proxy_pass = proxy_pass\n        self.proxy_headers = proxy_headers\n\n    def astuple(self):\n        return (\n            self.proxy_type,\n            self.proxy_host,\n            self.proxy_port,\n            self.proxy_rdns,\n            self.proxy_user,\n            self.proxy_pass,\n            self.proxy_headers,\n        )\n\n    def isgood(self):\n        return (self.proxy_host != None) and (self.proxy_port != None)\n\n    def applies_to(self, hostname):\n        return not self.bypass_host(hostname)\n\n    def bypass_host(self, hostname):\n        \"\"\"Has this host been excluded from the proxy config\"\"\"\n        if self.bypass_hosts is AllHosts:\n            return True\n\n        hostname = \".\" + hostname.lstrip(\".\")\n        for skip_name in self.bypass_hosts:\n            # *.suffix\n            if skip_name.startswith(\".\") and hostname.endswith(skip_name):\n                return True\n            # exact match\n            if hostname == \".\" + skip_name:\n                return True\n        return False\n\n    def __repr__(self):\n        return (\n            \"<ProxyInfo type={p.proxy_type} \"\n            \"host:port={p.proxy_host}:{p.proxy_port} rdns={p.proxy_rdns}\"\n            + \" user={p.proxy_user} headers={p.proxy_headers}>\"\n        ).format(p=self)\n\n\ndef proxy_info_from_environment(method=\"http\"):\n    \"\"\"Read proxy info from the environment variables.\n    \"\"\"\n    if method not in [\"http\", \"https\"]:\n        return\n\n    env_var = method + \"_proxy\"\n    url = os.environ.get(env_var, os.environ.get(env_var.upper()))\n    if not url:\n        return\n    return proxy_info_from_url(url, method, None)\n\n\ndef proxy_info_from_url(url, method=\"http\", noproxy=None):\n    \"\"\"Construct a ProxyInfo from a URL (such as http_proxy env var)\n    \"\"\"\n    url = urlparse.urlparse(url)\n    username = None\n    password = None\n    port = None\n    if \"@\" in url[1]:\n        ident, host_port = url[1].split(\"@\", 1)\n        if \":\" in ident:\n            username, password = ident.split(\":\", 1)\n        else:\n            password = ident\n    else:\n        host_port = url[1]\n    if \":\" in host_port:\n        host, port = host_port.split(\":\", 1)\n    else:\n        host = host_port\n\n    if port:\n        port = int(port)\n    else:\n        port = dict(https=443, http=80)[method]\n\n    proxy_type = 3  # socks.PROXY_TYPE_HTTP\n    pi = ProxyInfo(\n        proxy_type=proxy_type,\n        proxy_host=host,\n        proxy_port=port,\n        proxy_user=username or None,\n        proxy_pass=password or None,\n        proxy_headers=None,\n    )\n\n    bypass_hosts = []\n    # If not given an explicit noproxy value, respect values in env vars.\n    if noproxy is None:\n        noproxy = os.environ.get(\"no_proxy\", os.environ.get(\"NO_PROXY\", \"\"))\n    # Special case: A single '*' character means all hosts should be bypassed.\n    if noproxy == \"*\":\n        bypass_hosts = AllHosts\n    elif noproxy.strip():\n        bypass_hosts = noproxy.split(\",\")\n        bypass_hosts = filter(bool, bypass_hosts)  # To exclude empty string.\n\n    pi.bypass_hosts = bypass_hosts\n    return pi\n\n\nclass HTTPConnectionWithTimeout(httplib.HTTPConnection):\n    \"\"\"HTTPConnection subclass that supports timeouts\n\n    All timeouts are in seconds. If None is passed for timeout then\n    Python's default timeout for sockets will be used. See for example\n    the docs of socket.setdefaulttimeout():\n    http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n    \"\"\"\n\n    def __init__(self, host, port=None, strict=None, timeout=None, proxy_info=None):\n        httplib.HTTPConnection.__init__(self, host, port, strict)\n        self.timeout = timeout\n        self.proxy_info = proxy_info\n\n    def connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        # Mostly verbatim from httplib.py.\n        if self.proxy_info and socks is None:\n            raise ProxiesUnavailableError(\n                \"Proxy support missing but proxy use was requested!\"\n            )\n        if self.proxy_info and self.proxy_info.isgood():\n            use_proxy = True\n            proxy_type, proxy_host, proxy_port, proxy_rdns, proxy_user, proxy_pass, proxy_headers = (\n                self.proxy_info.astuple()\n            )\n\n            host = proxy_host\n            port = proxy_port\n        else:\n            use_proxy = False\n\n            host = self.host\n            port = self.port\n\n        socket_err = None\n\n        for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n            af, socktype, proto, canonname, sa = res\n            try:\n                if use_proxy:\n                    self.sock = socks.socksocket(af, socktype, proto)\n                    self.sock.setproxy(\n                        proxy_type,\n                        proxy_host,\n                        proxy_port,\n                        proxy_rdns,\n                        proxy_user,\n                        proxy_pass,\n                        proxy_headers,\n                    )\n                else:\n                    self.sock = socket.socket(af, socktype, proto)\n                    self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                # Different from httplib: support timeouts.\n                if has_timeout(self.timeout):\n                    self.sock.settimeout(self.timeout)\n                    # End of difference from httplib.\n                if self.debuglevel > 0:\n                    print(\"connect: (%s, %s) ************\" % (self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: %s ************\"\n                            % str(\n                                (\n                                    proxy_host,\n                                    proxy_port,\n                                    proxy_rdns,\n                                    proxy_user,\n                                    proxy_pass,\n                                    proxy_headers,\n                                )\n                            )\n                        )\n                if use_proxy:\n                    self.sock.connect((self.host, self.port) + sa[2:])\n                else:\n                    self.sock.connect(sa)\n            except socket.error as e:\n                socket_err = e\n                if self.debuglevel > 0:\n                    print(\"connect fail: (%s, %s)\" % (self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: %s\"\n                            % str(\n                                (\n                                    proxy_host,\n                                    proxy_port,\n                                    proxy_rdns,\n                                    proxy_user,\n                                    proxy_pass,\n                                    proxy_headers,\n                                )\n                            )\n                        )\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                continue\n            break\n        if not self.sock:\n            raise socket_err or socket.error(\"getaddrinfo returns an empty list\")\n\n\nclass HTTPSConnectionWithTimeout(httplib.HTTPSConnection):\n    \"\"\"This class allows communication via SSL.\n\n    All timeouts are in seconds. If None is passed for timeout then\n    Python's default timeout for sockets will be used. See for example\n    the docs of socket.setdefaulttimeout():\n    http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        strict=None,\n        timeout=None,\n        proxy_info=None,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        ssl_version=None,\n        key_password=None,\n    ):\n        if key_password:\n            httplib.HTTPSConnection.__init__(self, host, port=port, strict=strict)\n            self._context.load_cert_chain(cert_file, key_file, key_password)\n            self.key_file = key_file\n            self.cert_file = cert_file\n            self.key_password = key_password\n        else:\n            httplib.HTTPSConnection.__init__(\n                self, host, port=port, key_file=key_file, cert_file=cert_file, strict=strict\n            )\n            self.key_password = None\n        self.timeout = timeout\n        self.proxy_info = proxy_info\n        if ca_certs is None:\n            ca_certs = CA_CERTS\n        self.ca_certs = ca_certs\n        self.disable_ssl_certificate_validation = disable_ssl_certificate_validation\n        self.ssl_version = ssl_version\n\n    # The following two methods were adapted from https_wrapper.py, released\n    # with the Google Appengine SDK at\n    # http://googleappengine.googlecode.com/svn-history/r136/trunk/python/google/appengine/tools/https_wrapper.py\n    # under the following license:\n    #\n    # Copyright 2007 Google Inc.\n    #\n    # Licensed under the Apache License, Version 2.0 (the \"License\");\n    # you may not use this file except in compliance with the License.\n    # You may obtain a copy of the License at\n    #\n    #     http://www.apache.org/licenses/LICENSE-2.0\n    #\n    # Unless required by applicable law or agreed to in writing, software\n    # distributed under the License is distributed on an \"AS IS\" BASIS,\n    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    # See the License for the specific language governing permissions and\n    # limitations under the License.\n    #\n\n    def _GetValidHostsForCert(self, cert):\n        \"\"\"Returns a list of valid host globs for an SSL certificate.\n\n        Args:\n          cert: A dictionary representing an SSL certificate.\n        Returns:\n          list: A list of valid host globs.\n        \"\"\"\n        if \"subjectAltName\" in cert:\n            return [x[1] for x in cert[\"subjectAltName\"] if x[0].lower() == \"dns\"]\n        else:\n            return [x[0][1] for x in cert[\"subject\"] if x[0][0].lower() == \"commonname\"]\n\n    def _ValidateCertificateHostname(self, cert, hostname):\n        \"\"\"Validates that a given hostname is valid for an SSL certificate.\n\n        Args:\n          cert: A dictionary representing an SSL certificate.\n          hostname: The hostname to test.\n        Returns:\n          bool: Whether or not the hostname is valid for this certificate.\n        \"\"\"\n        hosts = self._GetValidHostsForCert(cert)\n        for host in hosts:\n            host_re = host.replace(\".\", \"\\.\").replace(\"*\", \"[^.]*\")\n            if re.search(\"^%s$\" % (host_re,), hostname, re.I):\n                return True\n        return False\n\n    def connect(self):\n        \"Connect to a host on a given (SSL) port.\"\n\n        if self.proxy_info and self.proxy_info.isgood():\n            use_proxy = True\n            proxy_type, proxy_host, proxy_port, proxy_rdns, proxy_user, proxy_pass, proxy_headers = (\n                self.proxy_info.astuple()\n            )\n\n            host = proxy_host\n            port = proxy_port\n        else:\n            use_proxy = False\n\n            host = self.host\n            port = self.port\n\n        socket_err = None\n\n        address_info = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\n        for family, socktype, proto, canonname, sockaddr in address_info:\n            try:\n                if use_proxy:\n                    sock = socks.socksocket(family, socktype, proto)\n\n                    sock.setproxy(\n                        proxy_type,\n                        proxy_host,\n                        proxy_port,\n                        proxy_rdns,\n                        proxy_user,\n                        proxy_pass,\n                        proxy_headers,\n                    )\n                else:\n                    sock = socket.socket(family, socktype, proto)\n                    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n\n                if has_timeout(self.timeout):\n                    sock.settimeout(self.timeout)\n\n                if use_proxy:\n                    sock.connect((self.host, self.port) + sockaddr[:2])\n                else:\n                    sock.connect(sockaddr)\n                self.sock = _ssl_wrap_socket(\n                    sock,\n                    self.key_file,\n                    self.cert_file,\n                    self.disable_ssl_certificate_validation,\n                    self.ca_certs,\n                    self.ssl_version,\n                    self.host,\n                    self.key_password,\n                )\n                if self.debuglevel > 0:\n                    print(\"connect: (%s, %s)\" % (self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: %s\"\n                            % str(\n                                (\n                                    proxy_host,\n                                    proxy_port,\n                                    proxy_rdns,\n                                    proxy_user,\n                                    proxy_pass,\n                                    proxy_headers,\n                                )\n                            )\n                        )\n                if not self.disable_ssl_certificate_validation:\n                    cert = self.sock.getpeercert()\n                    hostname = self.host.split(\":\", 0)[0]\n                    if not self._ValidateCertificateHostname(cert, hostname):\n                        raise CertificateHostnameMismatch(\n                            \"Server presented certificate that does not match \"\n                            \"host %s: %s\" % (hostname, cert),\n                            hostname,\n                            cert,\n                        )\n            except (\n                ssl_SSLError,\n                ssl_CertificateError,\n                CertificateHostnameMismatch,\n            ) as e:\n                if sock:\n                    sock.close()\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                # Unfortunately the ssl module doesn't seem to provide any way\n                # to get at more detailed error information, in particular\n                # whether the error is due to certificate validation or\n                # something else (such as SSL protocol mismatch).\n                if getattr(e, \"errno\", None) == ssl.SSL_ERROR_SSL:\n                    raise SSLHandshakeError(e)\n                else:\n                    raise\n            except (socket.timeout, socket.gaierror):\n                raise\n            except socket.error as e:\n                socket_err = e\n                if self.debuglevel > 0:\n                    print(\"connect fail: (%s, %s)\" % (self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: %s\"\n                            % str(\n                                (\n                                    proxy_host,\n                                    proxy_port,\n                                    proxy_rdns,\n                                    proxy_user,\n                                    proxy_pass,\n                                    proxy_headers,\n                                )\n                            )\n                        )\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                continue\n            break\n        if not self.sock:\n            raise socket_err or socket.error(\"getaddrinfo returns an empty list\")\n\n\nSCHEME_TO_CONNECTION = {\n    \"http\": HTTPConnectionWithTimeout,\n    \"https\": HTTPSConnectionWithTimeout,\n}\n\n\ndef _new_fixed_fetch(validate_certificate):\n\n    def fixed_fetch(\n        url,\n        payload=None,\n        method=\"GET\",\n        headers={},\n        allow_truncated=False,\n        follow_redirects=True,\n        deadline=None,\n    ):\n        return fetch(\n            url,\n            payload=payload,\n            method=method,\n            headers=headers,\n            allow_truncated=allow_truncated,\n            follow_redirects=follow_redirects,\n            deadline=deadline,\n            validate_certificate=validate_certificate,\n        )\n\n    return fixed_fetch\n\n\nclass AppEngineHttpConnection(httplib.HTTPConnection):\n    \"\"\"Use httplib on App Engine, but compensate for its weirdness.\n\n    The parameters key_file, cert_file, proxy_info, ca_certs,\n    disable_ssl_certificate_validation, and ssl_version are all dropped on\n    the ground.\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        strict=None,\n        timeout=None,\n        proxy_info=None,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        ssl_version=None,\n    ):\n        httplib.HTTPConnection.__init__(\n            self, host, port=port, strict=strict, timeout=timeout\n        )\n\n\nclass AppEngineHttpsConnection(httplib.HTTPSConnection):\n    \"\"\"Same as AppEngineHttpConnection, but for HTTPS URIs.\n\n    The parameters proxy_info, ca_certs, disable_ssl_certificate_validation,\n    and ssl_version are all dropped on the ground.\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        strict=None,\n        timeout=None,\n        proxy_info=None,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        ssl_version=None,\n        key_password=None,\n    ):\n        if key_password:\n            raise NotSupportedOnThisPlatform(\"Certificate with password is not supported.\")\n        httplib.HTTPSConnection.__init__(\n            self,\n            host,\n            port=port,\n            key_file=key_file,\n            cert_file=cert_file,\n            strict=strict,\n            timeout=timeout,\n        )\n        self._fetch = _new_fixed_fetch(not disable_ssl_certificate_validation)\n\n\n# Use a different connection object for Google App Engine Standard Environment.\ndef is_gae_instance():\n    server_software = os.environ.get('SERVER_SOFTWARE', '')\n    if (server_software.startswith('Google App Engine/') or\n        server_software.startswith('Development/') or\n        server_software.startswith('testutil/')):\n        return True\n    return False\n\n\ntry:\n    if not is_gae_instance():\n        raise NotRunningAppEngineEnvironment()\n\n    from google.appengine.api import apiproxy_stub_map\n    if apiproxy_stub_map.apiproxy.GetStub(\"urlfetch\") is None:\n        raise ImportError\n\n    from google.appengine.api.urlfetch import fetch\n\n    # Update the connection classes to use the Googel App Engine specific ones.\n    SCHEME_TO_CONNECTION = {\n        \"http\": AppEngineHttpConnection,\n        \"https\": AppEngineHttpsConnection,\n    }\nexcept (ImportError, NotRunningAppEngineEnvironment):\n    pass\n\n\nclass Http(object):\n    \"\"\"An HTTP client that handles:\n\n    - all methods\n    - caching\n    - ETags\n    - compression,\n    - HTTPS\n    - Basic\n    - Digest\n    - WSSE\n\n    and more.\n    \"\"\"\n\n    def __init__(\n        self,\n        cache=None,\n        timeout=None,\n        proxy_info=proxy_info_from_environment,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        ssl_version=None,\n    ):\n        \"\"\"If 'cache' is a string then it is used as a directory name for\n        a disk cache. Otherwise it must be an object that supports the\n        same interface as FileCache.\n\n        All timeouts are in seconds. If None is passed for timeout\n        then Python's default timeout for sockets will be used. See\n        for example the docs of socket.setdefaulttimeout():\n        http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n\n        `proxy_info` may be:\n          - a callable that takes the http scheme ('http' or 'https') and\n            returns a ProxyInfo instance per request. By default, uses\n            proxy_nfo_from_environment.\n          - a ProxyInfo instance (static proxy config).\n          - None (proxy disabled).\n\n        ca_certs is the path of a file containing root CA certificates for SSL\n        server certificate validation.  By default, a CA cert file bundled with\n        httplib2 is used.\n\n        If disable_ssl_certificate_validation is true, SSL cert validation will\n        not be performed.\n\n        By default, ssl.PROTOCOL_SSLv23 will be used for the ssl version.\n        \"\"\"\n        self.proxy_info = proxy_info\n        self.ca_certs = ca_certs\n        self.disable_ssl_certificate_validation = disable_ssl_certificate_validation\n        self.ssl_version = ssl_version\n\n        # Map domain name to an httplib connection\n        self.connections = {}\n        # The location of the cache, for now a directory\n        # where cached responses are held.\n        if cache and isinstance(cache, basestring):\n            self.cache = FileCache(cache)\n        else:\n            self.cache = cache\n\n        # Name/password\n        self.credentials = Credentials()\n\n        # Key/cert\n        self.certificates = KeyCerts()\n\n        # authorization objects\n        self.authorizations = []\n\n        # If set to False then no redirects are followed, even safe ones.\n        self.follow_redirects = True\n\n        self.redirect_codes = REDIRECT_CODES\n\n        # Which HTTP methods do we apply optimistic concurrency to, i.e.\n        # which methods get an \"if-match:\" etag header added to them.\n        self.optimistic_concurrency_methods = [\"PUT\", \"PATCH\"]\n\n        self.safe_methods = list(SAFE_METHODS)\n\n        # If 'follow_redirects' is True, and this is set to True then\n        # all redirecs are followed, including unsafe ones.\n        self.follow_all_redirects = False\n\n        self.ignore_etag = False\n\n        self.force_exception_to_status_code = False\n\n        self.timeout = timeout\n\n        # Keep Authorization: headers on a redirect.\n        self.forward_authorization_headers = False\n\n    def close(self):\n        \"\"\"Close persistent connections, clear sensitive data.\n        Not thread-safe, requires external synchronization against concurrent requests.\n        \"\"\"\n        existing, self.connections = self.connections, {}\n        for _, c in existing.iteritems():\n            c.close()\n        self.certificates.clear()\n        self.clear_credentials()\n\n    def __getstate__(self):\n        state_dict = copy.copy(self.__dict__)\n        # In case request is augmented by some foreign object such as\n        # credentials which handle auth\n        if \"request\" in state_dict:\n            del state_dict[\"request\"]\n        if \"connections\" in state_dict:\n            del state_dict[\"connections\"]\n        return state_dict\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.connections = {}\n\n    def _auth_from_challenge(self, host, request_uri, headers, response, content):\n        \"\"\"A generator that creates Authorization objects\n           that can be applied to requests.\n        \"\"\"\n        challenges = _parse_www_authenticate(response, \"www-authenticate\")\n        for cred in self.credentials.iter(host):\n            for scheme in AUTH_SCHEME_ORDER:\n                if scheme in challenges:\n                    yield AUTH_SCHEME_CLASSES[scheme](\n                        cred, host, request_uri, headers, response, content, self\n                    )\n\n    def add_credentials(self, name, password, domain=\"\"):\n        \"\"\"Add a name and password that will be used\n        any time a request requires authentication.\"\"\"\n        self.credentials.add(name, password, domain)\n\n    def add_certificate(self, key, cert, domain, password=None):\n        \"\"\"Add a key and cert that will be used\n        any time a request requires authentication.\"\"\"\n        self.certificates.add(key, cert, domain, password)\n\n    def clear_credentials(self):\n        \"\"\"Remove all the names and passwords\n        that are used for authentication\"\"\"\n        self.credentials.clear()\n        self.authorizations = []\n\n    def _conn_request(self, conn, request_uri, method, body, headers):\n        i = 0\n        seen_bad_status_line = False\n        while i < RETRIES:\n            i += 1\n            try:\n                if hasattr(conn, \"sock\") and conn.sock is None:\n                    conn.connect()\n                conn.request(method, request_uri, body, headers)\n            except socket.timeout:\n                raise\n            except socket.gaierror:\n                conn.close()\n                raise ServerNotFoundError(\"Unable to find the server at %s\" % conn.host)\n            except ssl_SSLError:\n                conn.close()\n                raise\n            except socket.error as e:\n                err = 0\n                if hasattr(e, \"args\"):\n                    err = getattr(e, \"args\")[0]\n                else:\n                    err = e.errno\n                if err == errno.ECONNREFUSED:  # Connection refused\n                    raise\n                if err in (errno.ENETUNREACH, errno.EADDRNOTAVAIL) and i < RETRIES:\n                    continue  # retry on potentially transient socket errors\n            except httplib.HTTPException:\n                # Just because the server closed the connection doesn't apparently mean\n                # that the server didn't send a response.\n                if hasattr(conn, \"sock\") and conn.sock is None:\n                    if i < RETRIES - 1:\n                        conn.close()\n                        conn.connect()\n                        continue\n                    else:\n                        conn.close()\n                        raise\n                if i < RETRIES - 1:\n                    conn.close()\n                    conn.connect()\n                    continue\n            try:\n                response = conn.getresponse()\n            except httplib.BadStatusLine:\n                # If we get a BadStatusLine on the first try then that means\n                # the connection just went stale, so retry regardless of the\n                # number of RETRIES set.\n                if not seen_bad_status_line and i == 1:\n                    i = 0\n                    seen_bad_status_line = True\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    conn.close()\n                    raise\n            except (socket.error, httplib.HTTPException):\n                if i < RETRIES - 1:\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    conn.close()\n                    raise\n            else:\n                content = \"\"\n                if method == \"HEAD\":\n                    conn.close()\n                else:\n                    content = response.read()\n                response = Response(response)\n                if method != \"HEAD\":\n                    content = _decompressContent(response, content)\n            break\n        return (response, content)\n\n    def _request(\n        self,\n        conn,\n        host,\n        absolute_uri,\n        request_uri,\n        method,\n        body,\n        headers,\n        redirections,\n        cachekey,\n    ):\n        \"\"\"Do the actual request using the connection object\n        and also follow one level of redirects if necessary\"\"\"\n\n        auths = [\n            (auth.depth(request_uri), auth)\n            for auth in self.authorizations\n            if auth.inscope(host, request_uri)\n        ]\n        auth = auths and sorted(auths)[0][1] or None\n        if auth:\n            auth.request(method, request_uri, headers, body)\n\n        (response, content) = self._conn_request(\n            conn, request_uri, method, body, headers\n        )\n\n        if auth:\n            if auth.response(response, body):\n                auth.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                response._stale_digest = 1\n\n        if response.status == 401:\n            for authorization in self._auth_from_challenge(\n                host, request_uri, headers, response, content\n            ):\n                authorization.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                if response.status != 401:\n                    self.authorizations.append(authorization)\n                    authorization.response(response, body)\n                    break\n\n        if (\n            self.follow_all_redirects\n            or method in self.safe_methods\n            or response.status in (303, 308)\n        ):\n            if self.follow_redirects and response.status in self.redirect_codes:\n                # Pick out the location header and basically start from the beginning\n                # remembering first to strip the ETag header and decrement our 'depth'\n                if redirections:\n                    if \"location\" not in response and response.status != 300:\n                        raise RedirectMissingLocation(\n                            _(\n                                \"Redirected but the response is missing a Location: header.\"\n                            ),\n                            response,\n                            content,\n                        )\n                    # Fix-up relative redirects (which violate an RFC 2616 MUST)\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        (scheme, authority, path, query, fragment) = parse_uri(location)\n                        if authority == None:\n                            response[\"location\"] = urlparse.urljoin(\n                                absolute_uri, location\n                            )\n                    if response.status == 308 or (response.status == 301 and method in self.safe_methods):\n                        response[\"-x-permanent-redirect-url\"] = response[\"location\"]\n                        if \"content-location\" not in response:\n                            response[\"content-location\"] = absolute_uri\n                        _updateCache(headers, response, content, self.cache, cachekey)\n                    if \"if-none-match\" in headers:\n                        del headers[\"if-none-match\"]\n                    if \"if-modified-since\" in headers:\n                        del headers[\"if-modified-since\"]\n                    if (\n                        \"authorization\" in headers\n                        and not self.forward_authorization_headers\n                    ):\n                        del headers[\"authorization\"]\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        old_response = copy.deepcopy(response)\n                        if \"content-location\" not in old_response:\n                            old_response[\"content-location\"] = absolute_uri\n                        redirect_method = method\n                        if response.status in [302, 303]:\n                            redirect_method = \"GET\"\n                            body = None\n                        (response, content) = self.request(\n                            location,\n                            method=redirect_method,\n                            body=body,\n                            headers=headers,\n                            redirections=redirections - 1,\n                        )\n                        response.previous = old_response\n                else:\n                    raise RedirectLimit(\n                        \"Redirected more times than rediection_limit allows.\",\n                        response,\n                        content,\n                    )\n            elif response.status in [200, 203] and method in self.safe_methods:\n                # Don't cache 206's since we aren't going to handle byte range requests\n                if \"content-location\" not in response:\n                    response[\"content-location\"] = absolute_uri\n                _updateCache(headers, response, content, self.cache, cachekey)\n\n        return (response, content)\n\n    def _normalize_headers(self, headers):\n        return _normalize_headers(headers)\n\n    # Need to catch and rebrand some exceptions\n    # Then need to optionally turn all exceptions into status codes\n    # including all socket.* and httplib.* exceptions.\n\n    def request(\n        self,\n        uri,\n        method=\"GET\",\n        body=None,\n        headers=None,\n        redirections=DEFAULT_MAX_REDIRECTS,\n        connection_type=None,\n    ):\n        \"\"\" Performs a single HTTP request.\n\n        The 'uri' is the URI of the HTTP resource and can begin with either\n        'http' or 'https'. The value of 'uri' must be an absolute URI.\n\n        The 'method' is the HTTP method to perform, such as GET, POST, DELETE,\n        etc. There is no restriction on the methods allowed.\n\n        The 'body' is the entity body to be sent with the request. It is a\n        string object.\n\n        Any extra headers that are to be sent with the request should be\n        provided in the 'headers' dictionary.\n\n        The maximum number of redirect to follow before raising an\n        exception is 'redirections. The default is 5.\n\n        The return value is a tuple of (response, content), the first\n        being and instance of the 'Response' class, the second being\n        a string that contains the response entity body.\n        \"\"\"\n        conn_key = ''\n\n        try:\n            if headers is None:\n                headers = {}\n            else:\n                headers = self._normalize_headers(headers)\n\n            if \"user-agent\" not in headers:\n                headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n\n            uri = iri2uri(uri)\n\n            (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n\n            proxy_info = self._get_proxy_info(scheme, authority)\n\n            conn_key = scheme + \":\" + authority\n            conn = self.connections.get(conn_key)\n            if conn is None:\n                if not connection_type:\n                    connection_type = SCHEME_TO_CONNECTION[scheme]\n                certs = list(self.certificates.iter(authority))\n                if scheme == \"https\":\n                    if certs:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            key_file=certs[0][0],\n                            cert_file=certs[0][1],\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                            key_password=certs[0][2],\n                        )\n                    else:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                        )\n                else:\n                    conn = self.connections[conn_key] = connection_type(\n                        authority, timeout=self.timeout, proxy_info=proxy_info\n                    )\n                conn.set_debuglevel(debuglevel)\n\n            if \"range\" not in headers and \"accept-encoding\" not in headers:\n                headers[\"accept-encoding\"] = \"gzip, deflate\"\n\n            info = email.Message.Message()\n            cachekey = None\n            cached_value = None\n            if self.cache:\n                cachekey = defrag_uri.encode(\"utf-8\")\n                cached_value = self.cache.get(cachekey)\n                if cached_value:\n                    # info = email.message_from_string(cached_value)\n                    #\n                    # Need to replace the line above with the kludge below\n                    # to fix the non-existent bug not fixed in this\n                    # bug report: http://mail.python.org/pipermail/python-bugs-list/2005-September/030289.html\n                    try:\n                        info, content = cached_value.split(\"\\r\\n\\r\\n\", 1)\n                        feedparser = email.FeedParser.FeedParser()\n                        feedparser.feed(info)\n                        info = feedparser.close()\n                        feedparser._parse = None\n                    except (IndexError, ValueError):\n                        self.cache.delete(cachekey)\n                        cachekey = None\n                        cached_value = None\n\n            if (\n                method in self.optimistic_concurrency_methods\n                and self.cache\n                and \"etag\" in info\n                and not self.ignore_etag\n                and \"if-match\" not in headers\n            ):\n                # http://www.w3.org/1999/04/Editing/\n                headers[\"if-match\"] = info[\"etag\"]\n\n            # https://tools.ietf.org/html/rfc7234\n            # A cache MUST invalidate the effective Request URI as well as [...] Location and Content-Location\n            # when a non-error status code is received in response to an unsafe request method.\n            if self.cache and cachekey and method not in self.safe_methods:\n                self.cache.delete(cachekey)\n\n            # Check the vary header in the cache to see if this request\n            # matches what varies in the cache.\n            if method in self.safe_methods and \"vary\" in info:\n                vary = info[\"vary\"]\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    value = info[key]\n                    if headers.get(header, None) != value:\n                        cached_value = None\n                        break\n\n            if (\n                self.cache\n                and cached_value\n                and (method in self.safe_methods or info[\"status\"] == \"308\")\n                and \"range\" not in headers\n            ):\n                redirect_method = method\n                if info[\"status\"] not in (\"307\", \"308\"):\n                    redirect_method = \"GET\"\n                if \"-x-permanent-redirect-url\" in info:\n                    # Should cached permanent redirects be counted in our redirection count? For now, yes.\n                    if redirections <= 0:\n                        raise RedirectLimit(\n                            \"Redirected more times than rediection_limit allows.\",\n                            {},\n                            \"\",\n                        )\n                    (response, new_content) = self.request(\n                        info[\"-x-permanent-redirect-url\"],\n                        method=redirect_method,\n                        headers=headers,\n                        redirections=redirections - 1,\n                    )\n                    response.previous = Response(info)\n                    response.previous.fromcache = True\n                else:\n                    # Determine our course of action:\n                    #   Is the cached entry fresh or stale?\n                    #   Has the client requested a non-cached response?\n                    #\n                    # There seems to be three possible answers:\n                    # 1. [FRESH] Return the cache entry w/o doing a GET\n                    # 2. [STALE] Do the GET (but add in cache validators if available)\n                    # 3. [TRANSPARENT] Do a GET w/o any cache validators (Cache-Control: no-cache) on the request\n                    entry_disposition = _entry_disposition(info, headers)\n\n                    if entry_disposition == \"FRESH\":\n                        if not cached_value:\n                            info[\"status\"] = \"504\"\n                            content = \"\"\n                        response = Response(info)\n                        if cached_value:\n                            response.fromcache = True\n                        return (response, content)\n\n                    if entry_disposition == \"STALE\":\n                        if (\n                            \"etag\" in info\n                            and not self.ignore_etag\n                            and not \"if-none-match\" in headers\n                        ):\n                            headers[\"if-none-match\"] = info[\"etag\"]\n                        if \"last-modified\" in info and not \"last-modified\" in headers:\n                            headers[\"if-modified-since\"] = info[\"last-modified\"]\n                    elif entry_disposition == \"TRANSPARENT\":\n                        pass\n\n                    (response, new_content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n\n                if response.status == 304 and method == \"GET\":\n                    # Rewrite the cache entry with the new end-to-end headers\n                    # Take all headers that are in response\n                    # and overwrite their values in info.\n                    # unless they are hop-by-hop, or are listed in the connection header.\n\n                    for key in _get_end2end_headers(response):\n                        info[key] = response[key]\n                    merged_response = Response(info)\n                    if hasattr(response, \"_stale_digest\"):\n                        merged_response._stale_digest = response._stale_digest\n                    _updateCache(\n                        headers, merged_response, content, self.cache, cachekey\n                    )\n                    response = merged_response\n                    response.status = 200\n                    response.fromcache = True\n\n                elif response.status == 200:\n                    content = new_content\n                else:\n                    self.cache.delete(cachekey)\n                    content = new_content\n            else:\n                cc = _parse_cache_control(headers)\n                if \"only-if-cached\" in cc:\n                    info[\"status\"] = \"504\"\n                    response = Response(info)\n                    content = \"\"\n                else:\n                    (response, content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n        except Exception as e:\n            is_timeout = isinstance(e, socket.timeout)\n            if is_timeout:\n                conn = self.connections.pop(conn_key, None)\n                if conn:\n                    conn.close()\n\n            if self.force_exception_to_status_code:\n                if isinstance(e, HttpLib2ErrorWithResponse):\n                    response = e.response\n                    content = e.content\n                    response.status = 500\n                    response.reason = str(e)\n                elif is_timeout:\n                    content = \"Request Timeout\"\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"408\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Request Timeout\"\n                else:\n                    content = str(e)\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"400\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Bad Request\"\n            else:\n                raise\n\n        return (response, content)\n\n    def _get_proxy_info(self, scheme, authority):\n        \"\"\"Return a ProxyInfo instance (or None) based on the scheme\n        and authority.\n        \"\"\"\n        hostname, port = urllib.splitport(authority)\n        proxy_info = self.proxy_info\n        if callable(proxy_info):\n            proxy_info = proxy_info(scheme)\n\n        if hasattr(proxy_info, \"applies_to\") and not proxy_info.applies_to(hostname):\n            proxy_info = None\n        return proxy_info\n\n\nclass Response(dict):\n    \"\"\"An object more like email.Message than httplib.HTTPResponse.\"\"\"\n\n    \"\"\"Is this response from our local cache\"\"\"\n    fromcache = False\n    \"\"\"HTTP protocol version used by server.\n\n    10 for HTTP/1.0, 11 for HTTP/1.1.\n    \"\"\"\n    version = 11\n\n    \"Status code returned by server. \"\n    status = 200\n    \"\"\"Reason phrase returned by server.\"\"\"\n    reason = \"Ok\"\n\n    previous = None\n\n    def __init__(self, info):\n        # info is either an email.Message or\n        # an httplib.HTTPResponse object.\n        if isinstance(info, httplib.HTTPResponse):\n            for key, value in info.getheaders():\n                self[key.lower()] = value\n            self.status = info.status\n            self[\"status\"] = str(self.status)\n            self.reason = info.reason\n            self.version = info.version\n        elif isinstance(info, email.Message.Message):\n            for key, value in info.items():\n                self[key.lower()] = value\n            self.status = int(self[\"status\"])\n        else:\n            for key, value in info.iteritems():\n                self[key.lower()] = value\n            self.status = int(self.get(\"status\", self.status))\n            self.reason = self.get(\"reason\", self.reason)\n\n    def __getattr__(self, name):\n        if name == \"dict\":\n            return self\n        else:\n            raise AttributeError(name)\n", "patch": "@@ -1985,6 +1985,9 @@ def request(\n                 headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n \n             uri = iri2uri(uri)\n+            # Prevent CWE-75 space injection to manipulate request via part of uri.\n+            # Prevent CWE-93 CRLF injection to modify headers via part of uri.\n+            uri = uri.replace(\" \", \"%20\").replace(\"\\r\", \"%0D\").replace(\"\\n\", \"%0A\")\n \n             (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n ", "file_path": "files/2020_8/325", "file_language": "py", "file_name": "python2/httplib2/__init__.py", "outdated_file_modify": 1, "outdated_file_before": 1, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/httplib2/httplib2/raw/a1457cc31f3206cf691d11d2bf34e98865873e9e/python3%2Fhttplib2%2F__init__.py", "code": "# -*- coding: utf-8 -*-\n\"\"\"Small, fast HTTP client library for Python.\"\"\"\n\n__author__ = \"Joe Gregorio (joe@bitworking.org)\"\n__copyright__ = \"Copyright 2006, Joe Gregorio\"\n__contributors__ = [\n    \"Thomas Broyer (t.broyer@ltgt.net)\",\n    \"James Antill\",\n    \"Xavier Verges Farrero\",\n    \"Jonathan Feinberg\",\n    \"Blair Zajac\",\n    \"Sam Ruby\",\n    \"Louis Nyffenegger\",\n    \"Mark Pilgrim\",\n    \"Alex Yu\",\n]\n__license__ = \"MIT\"\n__version__ = '0.17.4'\n\nimport base64\nimport calendar\nimport copy\nimport email\nimport email.feedparser\nfrom email import header\nimport email.message\nimport email.utils\nimport errno\nfrom gettext import gettext as _\nimport gzip\nfrom hashlib import md5 as _md5\nfrom hashlib import sha1 as _sha\nimport hmac\nimport http.client\nimport io\nimport os\nimport random\nimport re\nimport socket\nimport ssl\nimport sys\nimport time\nimport urllib.parse\nimport zlib\n\ntry:\n    import socks\nexcept ImportError:\n    # TODO: remove this fallback and copypasted socksipy module upon py2/3 merge,\n    # idea is to have soft-dependency on any compatible module called socks\n    from . import socks\nfrom .iri2uri import iri2uri\n\n\ndef has_timeout(timeout):\n    if hasattr(socket, \"_GLOBAL_DEFAULT_TIMEOUT\"):\n        return timeout is not None and timeout is not socket._GLOBAL_DEFAULT_TIMEOUT\n    return timeout is not None\n\n\n__all__ = [\n    \"debuglevel\",\n    \"FailedToDecompressContent\",\n    \"Http\",\n    \"HttpLib2Error\",\n    \"ProxyInfo\",\n    \"RedirectLimit\",\n    \"RedirectMissingLocation\",\n    \"Response\",\n    \"RETRIES\",\n    \"UnimplementedDigestAuthOptionError\",\n    \"UnimplementedHmacDigestAuthOptionError\",\n]\n\n# The httplib debug level, set to a non-zero value to get debug output\ndebuglevel = 0\n\n# A request will be tried 'RETRIES' times if it fails at the socket/connection level.\nRETRIES = 2\n\n\n# All exceptions raised here derive from HttpLib2Error\nclass HttpLib2Error(Exception):\n    pass\n\n\n# Some exceptions can be caught and optionally\n# be turned back into responses.\nclass HttpLib2ErrorWithResponse(HttpLib2Error):\n    def __init__(self, desc, response, content):\n        self.response = response\n        self.content = content\n        HttpLib2Error.__init__(self, desc)\n\n\nclass RedirectMissingLocation(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass RedirectLimit(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass FailedToDecompressContent(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass UnimplementedDigestAuthOptionError(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass UnimplementedHmacDigestAuthOptionError(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass MalformedHeader(HttpLib2Error):\n    pass\n\n\nclass RelativeURIError(HttpLib2Error):\n    pass\n\n\nclass ServerNotFoundError(HttpLib2Error):\n    pass\n\n\nclass ProxiesUnavailableError(HttpLib2Error):\n    pass\n\n\n# Open Items:\n# -----------\n\n# Are we removing the cached content too soon on PUT (only delete on 200 Maybe?)\n\n# Pluggable cache storage (supports storing the cache in\n#   flat files by default. We need a plug-in architecture\n#   that can support Berkeley DB and Squid)\n\n# == Known Issues ==\n# Does not handle a resource that uses conneg and Last-Modified but no ETag as a cache validator.\n# Does not handle Cache-Control: max-stale\n# Does not use Age: headers when calculating cache freshness.\n\n# The number of redirections to follow before giving up.\n# Note that only GET redirects are automatically followed.\n# Will also honor 301 requests by saving that info and never\n# requesting that URI again.\nDEFAULT_MAX_REDIRECTS = 5\n\n# Which headers are hop-by-hop headers by default\nHOP_BY_HOP = [\n    \"connection\",\n    \"keep-alive\",\n    \"proxy-authenticate\",\n    \"proxy-authorization\",\n    \"te\",\n    \"trailers\",\n    \"transfer-encoding\",\n    \"upgrade\",\n]\n\n# https://tools.ietf.org/html/rfc7231#section-8.1.3\nSAFE_METHODS = (\"GET\", \"HEAD\", \"OPTIONS\", \"TRACE\")\n\n# To change, assign to `Http().redirect_codes`\nREDIRECT_CODES = frozenset((300, 301, 302, 303, 307, 308))\n\n\nfrom httplib2 import certs\nCA_CERTS = certs.where()\n\n# PROTOCOL_TLS is python 3.5.3+. PROTOCOL_SSLv23 is deprecated.\n# Both PROTOCOL_TLS and PROTOCOL_SSLv23 are equivalent and means:\n# > Selects the highest protocol version that both the client and server support.\n# > Despite the name, this option can select \u201cTLS\u201d protocols as well as \u201cSSL\u201d.\n# source: https://docs.python.org/3.5/library/ssl.html#ssl.PROTOCOL_TLS\nDEFAULT_TLS_VERSION = getattr(ssl, \"PROTOCOL_TLS\", None) or getattr(\n    ssl, \"PROTOCOL_SSLv23\"\n)\n\ndef _build_ssl_context(\n    disable_ssl_certificate_validation, ca_certs, cert_file=None, key_file=None,\n    maximum_version=None, minimum_version=None, key_password=None,\n):\n    if not hasattr(ssl, \"SSLContext\"):\n        raise RuntimeError(\"httplib2 requires Python 3.2+ for ssl.SSLContext\")\n\n    context = ssl.SSLContext(DEFAULT_TLS_VERSION)\n    context.verify_mode = (\n        ssl.CERT_NONE if disable_ssl_certificate_validation else ssl.CERT_REQUIRED\n    )\n\n    # SSLContext.maximum_version and SSLContext.minimum_version are python 3.7+.\n    # source: https://docs.python.org/3/library/ssl.html#ssl.SSLContext.maximum_version\n    if maximum_version is not None:\n        if hasattr(context, \"maximum_version\"):\n            context.maximum_version = getattr(ssl.TLSVersion, maximum_version)\n        else:\n            raise RuntimeError(\"setting tls_maximum_version requires Python 3.7 and OpenSSL 1.1 or newer\")\n    if minimum_version is not None:\n        if hasattr(context, \"minimum_version\"):\n            context.minimum_version = getattr(ssl.TLSVersion, minimum_version)\n        else:\n            raise RuntimeError(\"setting tls_minimum_version requires Python 3.7 and OpenSSL 1.1 or newer\")\n\n    # check_hostname requires python 3.4+\n    # we will perform the equivalent in HTTPSConnectionWithTimeout.connect() by calling ssl.match_hostname\n    # if check_hostname is not supported.\n    if hasattr(context, \"check_hostname\"):\n        context.check_hostname = not disable_ssl_certificate_validation\n\n    context.load_verify_locations(ca_certs)\n\n    if cert_file:\n        context.load_cert_chain(cert_file, key_file, key_password)\n\n    return context\n\n\ndef _get_end2end_headers(response):\n    hopbyhop = list(HOP_BY_HOP)\n    hopbyhop.extend([x.strip() for x in response.get(\"connection\", \"\").split(\",\")])\n    return [header for header in list(response.keys()) if header not in hopbyhop]\n\n\nURI = re.compile(r\"^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\\?([^#]*))?(#(.*))?\")\n\n\ndef parse_uri(uri):\n    \"\"\"Parses a URI using the regex given in Appendix B of RFC 3986.\n\n        (scheme, authority, path, query, fragment) = parse_uri(uri)\n    \"\"\"\n    groups = URI.match(uri).groups()\n    return (groups[1], groups[3], groups[4], groups[6], groups[8])\n\n\ndef urlnorm(uri):\n    (scheme, authority, path, query, fragment) = parse_uri(uri)\n    if not scheme or not authority:\n        raise RelativeURIError(\"Only absolute URIs are allowed. uri = %s\" % uri)\n    authority = authority.lower()\n    scheme = scheme.lower()\n    if not path:\n        path = \"/\"\n    # Could do syntax based normalization of the URI before\n    # computing the digest. See Section 6.2.2 of Std 66.\n    request_uri = query and \"?\".join([path, query]) or path\n    scheme = scheme.lower()\n    defrag_uri = scheme + \"://\" + authority + request_uri\n    return scheme, authority, request_uri, defrag_uri\n\n\n# Cache filename construction (original borrowed from Venus http://intertwingly.net/code/venus/)\nre_url_scheme = re.compile(r\"^\\w+://\")\nre_unsafe = re.compile(r\"[^\\w\\-_.()=!]+\", re.ASCII)\n\n\ndef safename(filename):\n    \"\"\"Return a filename suitable for the cache.\n    Strips dangerous and common characters to create a filename we\n    can use to store the cache in.\n    \"\"\"\n    if isinstance(filename, bytes):\n        filename_bytes = filename\n        filename = filename.decode(\"utf-8\")\n    else:\n        filename_bytes = filename.encode(\"utf-8\")\n    filemd5 = _md5(filename_bytes).hexdigest()\n    filename = re_url_scheme.sub(\"\", filename)\n    filename = re_unsafe.sub(\"\", filename)\n\n    # limit length of filename (vital for Windows)\n    # https://github.com/httplib2/httplib2/pull/74\n    # C:\\Users\\    <username>    \\AppData\\Local\\Temp\\  <safe_filename>  ,   <md5>\n    #   9 chars + max 104 chars  +     20 chars      +       x       +  1  +  32  = max 259 chars\n    # Thus max safe filename x = 93 chars. Let it be 90 to make a round sum:\n    filename = filename[:90]\n\n    return \",\".join((filename, filemd5))\n\n\nNORMALIZE_SPACE = re.compile(r\"(?:\\r\\n)?[ \\t]+\")\n\n\ndef _normalize_headers(headers):\n    return dict(\n        [\n            (\n                _convert_byte_str(key).lower(),\n                NORMALIZE_SPACE.sub(_convert_byte_str(value), \" \").strip(),\n            )\n            for (key, value) in headers.items()\n        ]\n    )\n\n\ndef _convert_byte_str(s):\n    if not isinstance(s, str):\n        return str(s, \"utf-8\")\n    return s\n\n\ndef _parse_cache_control(headers):\n    retval = {}\n    if \"cache-control\" in headers:\n        parts = headers[\"cache-control\"].split(\",\")\n        parts_with_args = [\n            tuple([x.strip().lower() for x in part.split(\"=\", 1)])\n            for part in parts\n            if -1 != part.find(\"=\")\n        ]\n        parts_wo_args = [\n            (name.strip().lower(), 1) for name in parts if -1 == name.find(\"=\")\n        ]\n        retval = dict(parts_with_args + parts_wo_args)\n    return retval\n\n\n# Whether to use a strict mode to parse WWW-Authenticate headers\n# Might lead to bad results in case of ill-formed header value,\n# so disabled by default, falling back to relaxed parsing.\n# Set to true to turn on, useful for testing servers.\nUSE_WWW_AUTH_STRICT_PARSING = 0\n\n# In regex below:\n#    [^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+             matches a \"token\" as defined by HTTP\n#    \"(?:[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?\"    matches a \"quoted-string\" as defined by HTTP, when LWS have already been replaced by a single space\n# Actually, as an auth-param value can be either a token or a quoted-string, they are combined in a single pattern which matches both:\n#    \\\"?((?<=\\\")(?:[^\\0-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?(?=\\\")|(?<!\\\")[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+(?!\\\"))\\\"?\nWWW_AUTH_STRICT = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?(?=\\\")|(?<!\\\")[^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+(?!\\\"))\\\"?)(.*)$\"\n)\nWWW_AUTH_RELAXED = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^ \\t\\r\\n=]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\\\\\\"]|\\\\.)*?(?=\\\")|(?<!\\\")[^ \\t\\r\\n,]+(?!\\\"))\\\"?)(.*)$\"\n)\nUNQUOTE_PAIRS = re.compile(r\"\\\\(.)\")\n\n\ndef _parse_www_authenticate(headers, headername=\"www-authenticate\"):\n    \"\"\"Returns a dictionary of dictionaries, one dict\n    per auth_scheme.\"\"\"\n    retval = {}\n    if headername in headers:\n        try:\n            authenticate = headers[headername].strip()\n            www_auth = (\n                USE_WWW_AUTH_STRICT_PARSING and WWW_AUTH_STRICT or WWW_AUTH_RELAXED\n            )\n            while authenticate:\n                # Break off the scheme at the beginning of the line\n                if headername == \"authentication-info\":\n                    (auth_scheme, the_rest) = (\"digest\", authenticate)\n                else:\n                    (auth_scheme, the_rest) = authenticate.split(\" \", 1)\n                # Now loop over all the key value pairs that come after the scheme,\n                # being careful not to roll into the next scheme\n                match = www_auth.search(the_rest)\n                auth_params = {}\n                while match:\n                    if match and len(match.groups()) == 3:\n                        (key, value, the_rest) = match.groups()\n                        auth_params[key.lower()] = UNQUOTE_PAIRS.sub(\n                            r\"\\1\", value\n                        )  # '\\\\'.join([x.replace('\\\\', '') for x in value.split('\\\\\\\\')])\n                    match = www_auth.search(the_rest)\n                retval[auth_scheme.lower()] = auth_params\n                authenticate = the_rest.strip()\n        except ValueError:\n            raise MalformedHeader(\"WWW-Authenticate\")\n    return retval\n\n\ndef _entry_disposition(response_headers, request_headers):\n    \"\"\"Determine freshness from the Date, Expires and Cache-Control headers.\n\n    We don't handle the following:\n\n    1. Cache-Control: max-stale\n    2. Age: headers are not used in the calculations.\n\n    Not that this algorithm is simpler than you might think\n    because we are operating as a private (non-shared) cache.\n    This lets us ignore 's-maxage'. We can also ignore\n    'proxy-invalidate' since we aren't a proxy.\n    We will never return a stale document as\n    fresh as a design decision, and thus the non-implementation\n    of 'max-stale'. This also lets us safely ignore 'must-revalidate'\n    since we operate as if every server has sent 'must-revalidate'.\n    Since we are private we get to ignore both 'public' and\n    'private' parameters. We also ignore 'no-transform' since\n    we don't do any transformations.\n    The 'no-store' parameter is handled at a higher level.\n    So the only Cache-Control parameters we look at are:\n\n    no-cache\n    only-if-cached\n    max-age\n    min-fresh\n    \"\"\"\n\n    retval = \"STALE\"\n    cc = _parse_cache_control(request_headers)\n    cc_response = _parse_cache_control(response_headers)\n\n    if (\n        \"pragma\" in request_headers\n        and request_headers[\"pragma\"].lower().find(\"no-cache\") != -1\n    ):\n        retval = \"TRANSPARENT\"\n        if \"cache-control\" not in request_headers:\n            request_headers[\"cache-control\"] = \"no-cache\"\n    elif \"no-cache\" in cc:\n        retval = \"TRANSPARENT\"\n    elif \"no-cache\" in cc_response:\n        retval = \"STALE\"\n    elif \"only-if-cached\" in cc:\n        retval = \"FRESH\"\n    elif \"date\" in response_headers:\n        date = calendar.timegm(email.utils.parsedate_tz(response_headers[\"date\"]))\n        now = time.time()\n        current_age = max(0, now - date)\n        if \"max-age\" in cc_response:\n            try:\n                freshness_lifetime = int(cc_response[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        elif \"expires\" in response_headers:\n            expires = email.utils.parsedate_tz(response_headers[\"expires\"])\n            if None == expires:\n                freshness_lifetime = 0\n            else:\n                freshness_lifetime = max(0, calendar.timegm(expires) - date)\n        else:\n            freshness_lifetime = 0\n        if \"max-age\" in cc:\n            try:\n                freshness_lifetime = int(cc[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        if \"min-fresh\" in cc:\n            try:\n                min_fresh = int(cc[\"min-fresh\"])\n            except ValueError:\n                min_fresh = 0\n            current_age += min_fresh\n        if freshness_lifetime > current_age:\n            retval = \"FRESH\"\n    return retval\n\n\ndef _decompressContent(response, new_content):\n    content = new_content\n    try:\n        encoding = response.get(\"content-encoding\", None)\n        if encoding in [\"gzip\", \"deflate\"]:\n            if encoding == \"gzip\":\n                content = gzip.GzipFile(fileobj=io.BytesIO(new_content)).read()\n            if encoding == \"deflate\":\n                content = zlib.decompress(content, -zlib.MAX_WBITS)\n            response[\"content-length\"] = str(len(content))\n            # Record the historical presence of the encoding in a way the won't interfere.\n            response[\"-content-encoding\"] = response[\"content-encoding\"]\n            del response[\"content-encoding\"]\n    except (IOError, zlib.error):\n        content = \"\"\n        raise FailedToDecompressContent(\n            _(\"Content purported to be compressed with %s but failed to decompress.\")\n            % response.get(\"content-encoding\"),\n            response,\n            content,\n        )\n    return content\n\n\ndef _bind_write_headers(msg):\n    def _write_headers(self):\n        # Self refers to the Generator object.\n        for h, v in msg.items():\n            print(\"%s:\" % h, end=\" \", file=self._fp)\n            if isinstance(v, header.Header):\n                print(v.encode(maxlinelen=self._maxheaderlen), file=self._fp)\n            else:\n                # email.Header got lots of smarts, so use it.\n                headers = header.Header(\n                    v, maxlinelen=self._maxheaderlen, charset=\"utf-8\", header_name=h\n                )\n                print(headers.encode(), file=self._fp)\n        # A blank line always separates headers from body.\n        print(file=self._fp)\n\n    return _write_headers\n\n\ndef _updateCache(request_headers, response_headers, content, cache, cachekey):\n    if cachekey:\n        cc = _parse_cache_control(request_headers)\n        cc_response = _parse_cache_control(response_headers)\n        if \"no-store\" in cc or \"no-store\" in cc_response:\n            cache.delete(cachekey)\n        else:\n            info = email.message.Message()\n            for key, value in response_headers.items():\n                if key not in [\"status\", \"content-encoding\", \"transfer-encoding\"]:\n                    info[key] = value\n\n            # Add annotations to the cache to indicate what headers\n            # are variant for this request.\n            vary = response_headers.get(\"vary\", None)\n            if vary:\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    try:\n                        info[key] = request_headers[header]\n                    except KeyError:\n                        pass\n\n            status = response_headers.status\n            if status == 304:\n                status = 200\n\n            status_header = \"status: %d\\r\\n\" % status\n\n            try:\n                header_str = info.as_string()\n            except UnicodeEncodeError:\n                setattr(info, \"_write_headers\", _bind_write_headers(info))\n                header_str = info.as_string()\n\n            header_str = re.sub(\"\\r(?!\\n)|(?<!\\r)\\n\", \"\\r\\n\", header_str)\n            text = b\"\".join(\n                [status_header.encode(\"utf-8\"), header_str.encode(\"utf-8\"), content]\n            )\n\n            cache.set(cachekey, text)\n\n\ndef _cnonce():\n    dig = _md5(\n        (\n            \"%s:%s\"\n            % (time.ctime(), [\"0123456789\"[random.randrange(0, 9)] for i in range(20)])\n        ).encode(\"utf-8\")\n    ).hexdigest()\n    return dig[:16]\n\n\ndef _wsse_username_token(cnonce, iso_now, password):\n    return base64.b64encode(\n        _sha((\"%s%s%s\" % (cnonce, iso_now, password)).encode(\"utf-8\")).digest()\n    ).strip()\n\n\n# For credentials we need two things, first\n# a pool of credential to try (not necesarily tied to BAsic, Digest, etc.)\n# Then we also need a list of URIs that have already demanded authentication\n# That list is tricky since sub-URIs can take the same auth, or the\n# auth scheme may change as you descend the tree.\n# So we also need each Auth instance to be able to tell us\n# how close to the 'top' it is.\n\n\nclass Authentication(object):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        self.path = path\n        self.host = host\n        self.credentials = credentials\n        self.http = http\n\n    def depth(self, request_uri):\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        return request_uri[len(self.path) :].count(\"/\")\n\n    def inscope(self, host, request_uri):\n        # XXX Should we normalize the request_uri?\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        return (host == self.host) and path.startswith(self.path)\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header. Over-rise this in sub-classes.\"\"\"\n        pass\n\n    def response(self, response, content):\n        \"\"\"Gives us a chance to update with new nonces\n        or such returned from the last authorized response.\n        Over-rise this in sub-classes if necessary.\n\n        Return TRUE is the request is to be retried, for\n        example Digest may return stale=true.\n        \"\"\"\n        return False\n\n    def __eq__(self, auth):\n        return False\n\n    def __ne__(self, auth):\n        return True\n\n    def __lt__(self, auth):\n        return True\n\n    def __gt__(self, auth):\n        return False\n\n    def __le__(self, auth):\n        return True\n\n    def __ge__(self, auth):\n        return False\n\n    def __bool__(self):\n        return True\n\n\nclass BasicAuthentication(Authentication):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = \"Basic \" + base64.b64encode(\n            (\"%s:%s\" % self.credentials).encode(\"utf-8\")\n        ).strip().decode(\"utf-8\")\n\n\nclass DigestAuthentication(Authentication):\n    \"\"\"Only do qop='auth' and MD5, since that\n    is all Apache currently implements\"\"\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        self.challenge = challenge[\"digest\"]\n        qop = self.challenge.get(\"qop\", \"auth\")\n        self.challenge[\"qop\"] = (\n            (\"auth\" in [x.strip() for x in qop.split()]) and \"auth\" or None\n        )\n        if self.challenge[\"qop\"] is None:\n            raise UnimplementedDigestAuthOptionError(\n                _(\"Unsupported value for qop: %s.\" % qop)\n            )\n        self.challenge[\"algorithm\"] = self.challenge.get(\"algorithm\", \"MD5\").upper()\n        if self.challenge[\"algorithm\"] != \"MD5\":\n            raise UnimplementedDigestAuthOptionError(\n                _(\"Unsupported value for algorithm: %s.\" % self.challenge[\"algorithm\"])\n            )\n        self.A1 = \"\".join(\n            [\n                self.credentials[0],\n                \":\",\n                self.challenge[\"realm\"],\n                \":\",\n                self.credentials[1],\n            ]\n        )\n        self.challenge[\"nc\"] = 1\n\n    def request(self, method, request_uri, headers, content, cnonce=None):\n        \"\"\"Modify the request headers\"\"\"\n        H = lambda x: _md5(x.encode(\"utf-8\")).hexdigest()\n        KD = lambda s, d: H(\"%s:%s\" % (s, d))\n        A2 = \"\".join([method, \":\", request_uri])\n        self.challenge[\"cnonce\"] = cnonce or _cnonce()\n        request_digest = '\"%s\"' % KD(\n            H(self.A1),\n            \"%s:%s:%s:%s:%s\"\n            % (\n                self.challenge[\"nonce\"],\n                \"%08x\" % self.challenge[\"nc\"],\n                self.challenge[\"cnonce\"],\n                self.challenge[\"qop\"],\n                H(A2),\n            ),\n        )\n        headers[\"authorization\"] = (\n            'Digest username=\"%s\", realm=\"%s\", nonce=\"%s\", '\n            'uri=\"%s\", algorithm=%s, response=%s, qop=%s, '\n            'nc=%08x, cnonce=\"%s\"'\n        ) % (\n            self.credentials[0],\n            self.challenge[\"realm\"],\n            self.challenge[\"nonce\"],\n            request_uri,\n            self.challenge[\"algorithm\"],\n            request_digest,\n            self.challenge[\"qop\"],\n            self.challenge[\"nc\"],\n            self.challenge[\"cnonce\"],\n        )\n        if self.challenge.get(\"opaque\"):\n            headers[\"authorization\"] += ', opaque=\"%s\"' % self.challenge[\"opaque\"]\n        self.challenge[\"nc\"] += 1\n\n    def response(self, response, content):\n        if \"authentication-info\" not in response:\n            challenge = _parse_www_authenticate(response, \"www-authenticate\").get(\n                \"digest\", {}\n            )\n            if \"true\" == challenge.get(\"stale\"):\n                self.challenge[\"nonce\"] = challenge[\"nonce\"]\n                self.challenge[\"nc\"] = 1\n                return True\n        else:\n            updated_challenge = _parse_www_authenticate(\n                response, \"authentication-info\"\n            ).get(\"digest\", {})\n\n            if \"nextnonce\" in updated_challenge:\n                self.challenge[\"nonce\"] = updated_challenge[\"nextnonce\"]\n                self.challenge[\"nc\"] = 1\n        return False\n\n\nclass HmacDigestAuthentication(Authentication):\n    \"\"\"Adapted from Robert Sayre's code and DigestAuthentication above.\"\"\"\n\n    __author__ = \"Thomas Broyer (t.broyer@ltgt.net)\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        self.challenge = challenge[\"hmacdigest\"]\n        # TODO: self.challenge['domain']\n        self.challenge[\"reason\"] = self.challenge.get(\"reason\", \"unauthorized\")\n        if self.challenge[\"reason\"] not in [\"unauthorized\", \"integrity\"]:\n            self.challenge[\"reason\"] = \"unauthorized\"\n        self.challenge[\"salt\"] = self.challenge.get(\"salt\", \"\")\n        if not self.challenge.get(\"snonce\"):\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\"The challenge doesn't contain a server nonce, or this one is empty.\")\n            )\n        self.challenge[\"algorithm\"] = self.challenge.get(\"algorithm\", \"HMAC-SHA-1\")\n        if self.challenge[\"algorithm\"] not in [\"HMAC-SHA-1\", \"HMAC-MD5\"]:\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\"Unsupported value for algorithm: %s.\" % self.challenge[\"algorithm\"])\n            )\n        self.challenge[\"pw-algorithm\"] = self.challenge.get(\"pw-algorithm\", \"SHA-1\")\n        if self.challenge[\"pw-algorithm\"] not in [\"SHA-1\", \"MD5\"]:\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\n                    \"Unsupported value for pw-algorithm: %s.\"\n                    % self.challenge[\"pw-algorithm\"]\n                )\n            )\n        if self.challenge[\"algorithm\"] == \"HMAC-MD5\":\n            self.hashmod = _md5\n        else:\n            self.hashmod = _sha\n        if self.challenge[\"pw-algorithm\"] == \"MD5\":\n            self.pwhashmod = _md5\n        else:\n            self.pwhashmod = _sha\n        self.key = \"\".join(\n            [\n                self.credentials[0],\n                \":\",\n                self.pwhashmod.new(\n                    \"\".join([self.credentials[1], self.challenge[\"salt\"]])\n                )\n                .hexdigest()\n                .lower(),\n                \":\",\n                self.challenge[\"realm\"],\n            ]\n        )\n        self.key = self.pwhashmod.new(self.key).hexdigest().lower()\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers\"\"\"\n        keys = _get_end2end_headers(headers)\n        keylist = \"\".join([\"%s \" % k for k in keys])\n        headers_val = \"\".join([headers[k] for k in keys])\n        created = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n        cnonce = _cnonce()\n        request_digest = \"%s:%s:%s:%s:%s\" % (\n            method,\n            request_uri,\n            cnonce,\n            self.challenge[\"snonce\"],\n            headers_val,\n        )\n        request_digest = (\n            hmac.new(self.key, request_digest, self.hashmod).hexdigest().lower()\n        )\n        headers[\"authorization\"] = (\n            'HMACDigest username=\"%s\", realm=\"%s\", snonce=\"%s\",'\n            ' cnonce=\"%s\", uri=\"%s\", created=\"%s\", '\n            'response=\"%s\", headers=\"%s\"'\n        ) % (\n            self.credentials[0],\n            self.challenge[\"realm\"],\n            self.challenge[\"snonce\"],\n            cnonce,\n            request_uri,\n            created,\n            request_digest,\n            keylist,\n        )\n\n    def response(self, response, content):\n        challenge = _parse_www_authenticate(response, \"www-authenticate\").get(\n            \"hmacdigest\", {}\n        )\n        if challenge.get(\"reason\") in [\"integrity\", \"stale\"]:\n            return True\n        return False\n\n\nclass WsseAuthentication(Authentication):\n    \"\"\"This is thinly tested and should not be relied upon.\n    At this time there isn't any third party server to test against.\n    Blogger and TypePad implemented this algorithm at one point\n    but Blogger has since switched to Basic over HTTPS and\n    TypePad has implemented it wrong, by never issuing a 401\n    challenge but instead requiring your client to telepathically know that\n    their endpoint is expecting WSSE profile=\"UsernameToken\".\"\"\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = 'WSSE profile=\"UsernameToken\"'\n        iso_now = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n        cnonce = _cnonce()\n        password_digest = _wsse_username_token(cnonce, iso_now, self.credentials[1])\n        headers[\"X-WSSE\"] = (\n            'UsernameToken Username=\"%s\", PasswordDigest=\"%s\", '\n            'Nonce=\"%s\", Created=\"%s\"'\n        ) % (self.credentials[0], password_digest, cnonce, iso_now)\n\n\nclass GoogleLoginAuthentication(Authentication):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        from urllib.parse import urlencode\n\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        service = challenge[\"googlelogin\"].get(\"service\", \"xapi\")\n        # Bloggger actually returns the service in the challenge\n        # For the rest we guess based on the URI\n        if service == \"xapi\" and request_uri.find(\"calendar\") > 0:\n            service = \"cl\"\n        # No point in guessing Base or Spreadsheet\n        # elif request_uri.find(\"spreadsheets\") > 0:\n        #    service = \"wise\"\n\n        auth = dict(\n            Email=credentials[0],\n            Passwd=credentials[1],\n            service=service,\n            source=headers[\"user-agent\"],\n        )\n        resp, content = self.http.request(\n            \"https://www.google.com/accounts/ClientLogin\",\n            method=\"POST\",\n            body=urlencode(auth),\n            headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n        )\n        lines = content.split(\"\\n\")\n        d = dict([tuple(line.split(\"=\", 1)) for line in lines if line])\n        if resp.status == 403:\n            self.Auth = \"\"\n        else:\n            self.Auth = d[\"Auth\"]\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = \"GoogleLogin Auth=\" + self.Auth\n\n\nAUTH_SCHEME_CLASSES = {\n    \"basic\": BasicAuthentication,\n    \"wsse\": WsseAuthentication,\n    \"digest\": DigestAuthentication,\n    \"hmacdigest\": HmacDigestAuthentication,\n    \"googlelogin\": GoogleLoginAuthentication,\n}\n\nAUTH_SCHEME_ORDER = [\"hmacdigest\", \"googlelogin\", \"digest\", \"wsse\", \"basic\"]\n\n\nclass FileCache(object):\n    \"\"\"Uses a local directory as a store for cached files.\n    Not really safe to use if multiple threads or processes are going to\n    be running on the same cache.\n    \"\"\"\n\n    def __init__(\n        self, cache, safe=safename\n    ):  # use safe=lambda x: md5.new(x).hexdigest() for the old behavior\n        self.cache = cache\n        self.safe = safe\n        if not os.path.exists(cache):\n            os.makedirs(self.cache)\n\n    def get(self, key):\n        retval = None\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        try:\n            f = open(cacheFullPath, \"rb\")\n            retval = f.read()\n            f.close()\n        except IOError:\n            pass\n        return retval\n\n    def set(self, key, value):\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        f = open(cacheFullPath, \"wb\")\n        f.write(value)\n        f.close()\n\n    def delete(self, key):\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        if os.path.exists(cacheFullPath):\n            os.remove(cacheFullPath)\n\n\nclass Credentials(object):\n    def __init__(self):\n        self.credentials = []\n\n    def add(self, name, password, domain=\"\"):\n        self.credentials.append((domain.lower(), name, password))\n\n    def clear(self):\n        self.credentials = []\n\n    def iter(self, domain):\n        for (cdomain, name, password) in self.credentials:\n            if cdomain == \"\" or domain == cdomain:\n                yield (name, password)\n\n\nclass KeyCerts(Credentials):\n    \"\"\"Identical to Credentials except that\n    name/password are mapped to key/cert.\"\"\"\n    def add(self, key, cert, domain, password):\n        self.credentials.append((domain.lower(), key, cert, password))\n\n    def iter(self, domain):\n        for (cdomain, key, cert, password) in self.credentials:\n            if cdomain == \"\" or domain == cdomain:\n                yield (key, cert, password)\n\n\nclass AllHosts(object):\n    pass\n\n\nclass ProxyInfo(object):\n    \"\"\"Collect information required to use a proxy.\"\"\"\n\n    bypass_hosts = ()\n\n    def __init__(\n        self,\n        proxy_type,\n        proxy_host,\n        proxy_port,\n        proxy_rdns=True,\n        proxy_user=None,\n        proxy_pass=None,\n        proxy_headers=None,\n    ):\n        \"\"\"Args:\n\n          proxy_type: The type of proxy server.  This must be set to one of\n          socks.PROXY_TYPE_XXX constants.  For example:  p =\n          ProxyInfo(proxy_type=socks.PROXY_TYPE_HTTP, proxy_host='localhost',\n          proxy_port=8000)\n          proxy_host: The hostname or IP address of the proxy server.\n          proxy_port: The port that the proxy server is running on.\n          proxy_rdns: If True (default), DNS queries will not be performed\n          locally, and instead, handed to the proxy to resolve.  This is useful\n          if the network does not allow resolution of non-local names. In\n          httplib2 0.9 and earlier, this defaulted to False.\n          proxy_user: The username used to authenticate with the proxy server.\n          proxy_pass: The password used to authenticate with the proxy server.\n          proxy_headers: Additional or modified headers for the proxy connect\n          request.\n        \"\"\"\n        if isinstance(proxy_user, bytes):\n            proxy_user = proxy_user.decode()\n        if isinstance(proxy_pass, bytes):\n            proxy_pass = proxy_pass.decode()\n        self.proxy_type, self.proxy_host, self.proxy_port, self.proxy_rdns, self.proxy_user, self.proxy_pass, self.proxy_headers = (\n            proxy_type,\n            proxy_host,\n            proxy_port,\n            proxy_rdns,\n            proxy_user,\n            proxy_pass,\n            proxy_headers,\n        )\n\n    def astuple(self):\n        return (\n            self.proxy_type,\n            self.proxy_host,\n            self.proxy_port,\n            self.proxy_rdns,\n            self.proxy_user,\n            self.proxy_pass,\n            self.proxy_headers,\n        )\n\n    def isgood(self):\n        return socks and (self.proxy_host != None) and (self.proxy_port != None)\n\n    def applies_to(self, hostname):\n        return not self.bypass_host(hostname)\n\n    def bypass_host(self, hostname):\n        \"\"\"Has this host been excluded from the proxy config\"\"\"\n        if self.bypass_hosts is AllHosts:\n            return True\n\n        hostname = \".\" + hostname.lstrip(\".\")\n        for skip_name in self.bypass_hosts:\n            # *.suffix\n            if skip_name.startswith(\".\") and hostname.endswith(skip_name):\n                return True\n            # exact match\n            if hostname == \".\" + skip_name:\n                return True\n        return False\n\n    def __repr__(self):\n        return (\n            \"<ProxyInfo type={p.proxy_type} \"\n            \"host:port={p.proxy_host}:{p.proxy_port} rdns={p.proxy_rdns}\"\n            + \" user={p.proxy_user} headers={p.proxy_headers}>\"\n        ).format(p=self)\n\n\ndef proxy_info_from_environment(method=\"http\"):\n    \"\"\"Read proxy info from the environment variables.\n    \"\"\"\n    if method not in (\"http\", \"https\"):\n        return\n\n    env_var = method + \"_proxy\"\n    url = os.environ.get(env_var, os.environ.get(env_var.upper()))\n    if not url:\n        return\n    return proxy_info_from_url(url, method, noproxy=None)\n\n\ndef proxy_info_from_url(url, method=\"http\", noproxy=None):\n    \"\"\"Construct a ProxyInfo from a URL (such as http_proxy env var)\n    \"\"\"\n    url = urllib.parse.urlparse(url)\n    username = None\n    password = None\n    port = None\n    if \"@\" in url[1]:\n        ident, host_port = url[1].split(\"@\", 1)\n        if \":\" in ident:\n            username, password = ident.split(\":\", 1)\n        else:\n            password = ident\n    else:\n        host_port = url[1]\n    if \":\" in host_port:\n        host, port = host_port.split(\":\", 1)\n    else:\n        host = host_port\n\n    if port:\n        port = int(port)\n    else:\n        port = dict(https=443, http=80)[method]\n\n    proxy_type = 3  # socks.PROXY_TYPE_HTTP\n    pi = ProxyInfo(\n        proxy_type=proxy_type,\n        proxy_host=host,\n        proxy_port=port,\n        proxy_user=username or None,\n        proxy_pass=password or None,\n        proxy_headers=None,\n    )\n\n    bypass_hosts = []\n    # If not given an explicit noproxy value, respect values in env vars.\n    if noproxy is None:\n        noproxy = os.environ.get(\"no_proxy\", os.environ.get(\"NO_PROXY\", \"\"))\n    # Special case: A single '*' character means all hosts should be bypassed.\n    if noproxy == \"*\":\n        bypass_hosts = AllHosts\n    elif noproxy.strip():\n        bypass_hosts = noproxy.split(\",\")\n        bypass_hosts = tuple(filter(bool, bypass_hosts))  # To exclude empty string.\n\n    pi.bypass_hosts = bypass_hosts\n    return pi\n\n\nclass HTTPConnectionWithTimeout(http.client.HTTPConnection):\n    \"\"\"HTTPConnection subclass that supports timeouts\n\n    HTTPConnection subclass that supports timeouts\n\n    All timeouts are in seconds. If None is passed for timeout then\n    Python's default timeout for sockets will be used. See for example\n    the docs of socket.setdefaulttimeout():\n    http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n    \"\"\"\n\n    def __init__(self, host, port=None, timeout=None, proxy_info=None):\n        http.client.HTTPConnection.__init__(self, host, port=port, timeout=timeout)\n\n        self.proxy_info = proxy_info\n        if proxy_info and not isinstance(proxy_info, ProxyInfo):\n            self.proxy_info = proxy_info(\"http\")\n\n    def connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.proxy_info and socks is None:\n            raise ProxiesUnavailableError(\n                \"Proxy support missing but proxy use was requested!\"\n            )\n        if self.proxy_info and self.proxy_info.isgood() and self.proxy_info.applies_to(self.host):\n            use_proxy = True\n            proxy_type, proxy_host, proxy_port, proxy_rdns, proxy_user, proxy_pass, proxy_headers = (\n                self.proxy_info.astuple()\n            )\n\n            host = proxy_host\n            port = proxy_port\n        else:\n            use_proxy = False\n\n            host = self.host\n            port = self.port\n            proxy_type = None\n\n        socket_err = None\n\n        for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n            af, socktype, proto, canonname, sa = res\n            try:\n                if use_proxy:\n                    self.sock = socks.socksocket(af, socktype, proto)\n                    self.sock.setproxy(\n                        proxy_type,\n                        proxy_host,\n                        proxy_port,\n                        proxy_rdns,\n                        proxy_user,\n                        proxy_pass,\n                    )\n                else:\n                    self.sock = socket.socket(af, socktype, proto)\n                    self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                if has_timeout(self.timeout):\n                    self.sock.settimeout(self.timeout)\n                if self.debuglevel > 0:\n                    print(\n                        \"connect: ({0}, {1}) ************\".format(self.host, self.port)\n                    )\n                    if use_proxy:\n                        print(\n                            \"proxy: {0} ************\".format(\n                                str(\n                                    (\n                                        proxy_host,\n                                        proxy_port,\n                                        proxy_rdns,\n                                        proxy_user,\n                                        proxy_pass,\n                                        proxy_headers,\n                                    )\n                                )\n                            )\n                        )\n\n                self.sock.connect((self.host, self.port) + sa[2:])\n            except socket.error as e:\n                socket_err = e\n                if self.debuglevel > 0:\n                    print(\"connect fail: ({0}, {1})\".format(self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: {0}\".format(\n                                str(\n                                    (\n                                        proxy_host,\n                                        proxy_port,\n                                        proxy_rdns,\n                                        proxy_user,\n                                        proxy_pass,\n                                        proxy_headers,\n                                    )\n                                )\n                            )\n                        )\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                continue\n            break\n        if not self.sock:\n            raise socket_err\n\n\nclass HTTPSConnectionWithTimeout(http.client.HTTPSConnection):\n    \"\"\"This class allows communication via SSL.\n\n    All timeouts are in seconds. If None is passed for timeout then\n    Python's default timeout for sockets will be used. See for example\n    the docs of socket.setdefaulttimeout():\n    http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        timeout=None,\n        proxy_info=None,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        tls_maximum_version=None,\n        tls_minimum_version=None,\n        key_password=None,\n    ):\n\n        self.disable_ssl_certificate_validation = disable_ssl_certificate_validation\n        self.ca_certs = ca_certs if ca_certs else CA_CERTS\n\n        self.proxy_info = proxy_info\n        if proxy_info and not isinstance(proxy_info, ProxyInfo):\n            self.proxy_info = proxy_info(\"https\")\n\n        context = _build_ssl_context(\n            self.disable_ssl_certificate_validation, self.ca_certs, cert_file, key_file,\n            maximum_version=tls_maximum_version, minimum_version=tls_minimum_version,\n            key_password=key_password,\n        )\n        super(HTTPSConnectionWithTimeout, self).__init__(\n            host,\n            port=port,\n            timeout=timeout,\n            context=context,\n        )\n        self.key_file = key_file\n        self.cert_file = cert_file\n        self.key_password = key_password\n\n    def connect(self):\n        \"\"\"Connect to a host on a given (SSL) port.\"\"\"\n        if self.proxy_info and self.proxy_info.isgood() and self.proxy_info.applies_to(self.host):\n            use_proxy = True\n            proxy_type, proxy_host, proxy_port, proxy_rdns, proxy_user, proxy_pass, proxy_headers = (\n                self.proxy_info.astuple()\n            )\n\n            host = proxy_host\n            port = proxy_port\n        else:\n            use_proxy = False\n\n            host = self.host\n            port = self.port\n            proxy_type = None\n            proxy_headers = None\n\n        socket_err = None\n\n        address_info = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\n        for family, socktype, proto, canonname, sockaddr in address_info:\n            try:\n                if use_proxy:\n                    sock = socks.socksocket(family, socktype, proto)\n\n                    sock.setproxy(\n                        proxy_type,\n                        proxy_host,\n                        proxy_port,\n                        proxy_rdns,\n                        proxy_user,\n                        proxy_pass,\n                    )\n                else:\n                    sock = socket.socket(family, socktype, proto)\n                    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                if has_timeout(self.timeout):\n                    sock.settimeout(self.timeout)\n                sock.connect((self.host, self.port))\n\n                self.sock = self._context.wrap_socket(sock, server_hostname=self.host)\n\n                # Python 3.3 compatibility: emulate the check_hostname behavior\n                if (\n                    not hasattr(self._context, \"check_hostname\")\n                    and not self.disable_ssl_certificate_validation\n                ):\n                    try:\n                        ssl.match_hostname(self.sock.getpeercert(), self.host)\n                    except Exception:\n                        self.sock.shutdown(socket.SHUT_RDWR)\n                        self.sock.close()\n                        raise\n\n                if self.debuglevel > 0:\n                    print(\"connect: ({0}, {1})\".format(self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: {0}\".format(\n                                str(\n                                    (\n                                        proxy_host,\n                                        proxy_port,\n                                        proxy_rdns,\n                                        proxy_user,\n                                        proxy_pass,\n                                        proxy_headers,\n                                    )\n                                )\n                            )\n                        )\n            except (ssl.SSLError, ssl.CertificateError) as e:\n                if sock:\n                    sock.close()\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                raise\n            except (socket.timeout, socket.gaierror):\n                raise\n            except socket.error as e:\n                socket_err = e\n                if self.debuglevel > 0:\n                    print(\"connect fail: ({0}, {1})\".format(self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: {0}\".format(\n                                str(\n                                    (\n                                        proxy_host,\n                                        proxy_port,\n                                        proxy_rdns,\n                                        proxy_user,\n                                        proxy_pass,\n                                        proxy_headers,\n                                    )\n                                )\n                            )\n                        )\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                continue\n            break\n        if not self.sock:\n            raise socket_err\n\n\nSCHEME_TO_CONNECTION = {\n    \"http\": HTTPConnectionWithTimeout,\n    \"https\": HTTPSConnectionWithTimeout,\n}\n\n\nclass Http(object):\n    \"\"\"An HTTP client that handles:\n\n    - all methods\n    - caching\n    - ETags\n    - compression,\n    - HTTPS\n    - Basic\n    - Digest\n    - WSSE\n\n    and more.\n    \"\"\"\n\n    def __init__(\n        self,\n        cache=None,\n        timeout=None,\n        proxy_info=proxy_info_from_environment,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        tls_maximum_version=None,\n        tls_minimum_version=None,\n    ):\n        \"\"\"If 'cache' is a string then it is used as a directory name for\n        a disk cache. Otherwise it must be an object that supports the\n        same interface as FileCache.\n\n        All timeouts are in seconds. If None is passed for timeout\n        then Python's default timeout for sockets will be used. See\n        for example the docs of socket.setdefaulttimeout():\n        http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n\n        `proxy_info` may be:\n          - a callable that takes the http scheme ('http' or 'https') and\n            returns a ProxyInfo instance per request. By default, uses\n            proxy_info_from_environment.\n          - a ProxyInfo instance (static proxy config).\n          - None (proxy disabled).\n\n        ca_certs is the path of a file containing root CA certificates for SSL\n        server certificate validation.  By default, a CA cert file bundled with\n        httplib2 is used.\n\n        If disable_ssl_certificate_validation is true, SSL cert validation will\n        not be performed.\n\n        tls_maximum_version / tls_minimum_version require Python 3.7+ /\n        OpenSSL 1.1.0g+. A value of \"TLSv1_3\" requires OpenSSL 1.1.1+.\n\"\"\"\n        self.proxy_info = proxy_info\n        self.ca_certs = ca_certs\n        self.disable_ssl_certificate_validation = disable_ssl_certificate_validation\n        self.tls_maximum_version = tls_maximum_version\n        self.tls_minimum_version = tls_minimum_version\n        # Map domain name to an httplib connection\n        self.connections = {}\n        # The location of the cache, for now a directory\n        # where cached responses are held.\n        if cache and isinstance(cache, str):\n            self.cache = FileCache(cache)\n        else:\n            self.cache = cache\n\n        # Name/password\n        self.credentials = Credentials()\n\n        # Key/cert\n        self.certificates = KeyCerts()\n\n        # authorization objects\n        self.authorizations = []\n\n        # If set to False then no redirects are followed, even safe ones.\n        self.follow_redirects = True\n\n        self.redirect_codes = REDIRECT_CODES\n\n        # Which HTTP methods do we apply optimistic concurrency to, i.e.\n        # which methods get an \"if-match:\" etag header added to them.\n        self.optimistic_concurrency_methods = [\"PUT\", \"PATCH\"]\n\n        self.safe_methods = list(SAFE_METHODS)\n\n        # If 'follow_redirects' is True, and this is set to True then\n        # all redirecs are followed, including unsafe ones.\n        self.follow_all_redirects = False\n\n        self.ignore_etag = False\n\n        self.force_exception_to_status_code = False\n\n        self.timeout = timeout\n\n        # Keep Authorization: headers on a redirect.\n        self.forward_authorization_headers = False\n\n    def close(self):\n        \"\"\"Close persistent connections, clear sensitive data.\n        Not thread-safe, requires external synchronization against concurrent requests.\n        \"\"\"\n        existing, self.connections = self.connections, {}\n        for _, c in existing.items():\n            c.close()\n        self.certificates.clear()\n        self.clear_credentials()\n\n    def __getstate__(self):\n        state_dict = copy.copy(self.__dict__)\n        # In case request is augmented by some foreign object such as\n        # credentials which handle auth\n        if \"request\" in state_dict:\n            del state_dict[\"request\"]\n        if \"connections\" in state_dict:\n            del state_dict[\"connections\"]\n        return state_dict\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.connections = {}\n\n    def _auth_from_challenge(self, host, request_uri, headers, response, content):\n        \"\"\"A generator that creates Authorization objects\n           that can be applied to requests.\n        \"\"\"\n        challenges = _parse_www_authenticate(response, \"www-authenticate\")\n        for cred in self.credentials.iter(host):\n            for scheme in AUTH_SCHEME_ORDER:\n                if scheme in challenges:\n                    yield AUTH_SCHEME_CLASSES[scheme](\n                        cred, host, request_uri, headers, response, content, self\n                    )\n\n    def add_credentials(self, name, password, domain=\"\"):\n        \"\"\"Add a name and password that will be used\n        any time a request requires authentication.\"\"\"\n        self.credentials.add(name, password, domain)\n\n    def add_certificate(self, key, cert, domain, password=None):\n        \"\"\"Add a key and cert that will be used\n        any time a request requires authentication.\"\"\"\n        self.certificates.add(key, cert, domain, password)\n\n    def clear_credentials(self):\n        \"\"\"Remove all the names and passwords\n        that are used for authentication\"\"\"\n        self.credentials.clear()\n        self.authorizations = []\n\n    def _conn_request(self, conn, request_uri, method, body, headers):\n        i = 0\n        seen_bad_status_line = False\n        while i < RETRIES:\n            i += 1\n            try:\n                if conn.sock is None:\n                    conn.connect()\n                conn.request(method, request_uri, body, headers)\n            except socket.timeout:\n                conn.close()\n                raise\n            except socket.gaierror:\n                conn.close()\n                raise ServerNotFoundError(\"Unable to find the server at %s\" % conn.host)\n            except socket.error as e:\n                errno_ = (\n                    e.args[0].errno if isinstance(e.args[0], socket.error) else e.errno\n                )\n                if errno_ in (errno.ENETUNREACH, errno.EADDRNOTAVAIL) and i < RETRIES:\n                    continue  # retry on potentially transient errors\n                raise\n            except http.client.HTTPException:\n                if conn.sock is None:\n                    if i < RETRIES - 1:\n                        conn.close()\n                        conn.connect()\n                        continue\n                    else:\n                        conn.close()\n                        raise\n                if i < RETRIES - 1:\n                    conn.close()\n                    conn.connect()\n                    continue\n                # Just because the server closed the connection doesn't apparently mean\n                # that the server didn't send a response.\n                pass\n            try:\n                response = conn.getresponse()\n            except (http.client.BadStatusLine, http.client.ResponseNotReady):\n                # If we get a BadStatusLine on the first try then that means\n                # the connection just went stale, so retry regardless of the\n                # number of RETRIES set.\n                if not seen_bad_status_line and i == 1:\n                    i = 0\n                    seen_bad_status_line = True\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    conn.close()\n                    raise\n            except socket.timeout:\n                raise\n            except (socket.error, http.client.HTTPException):\n                conn.close()\n                if i == 0:\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    raise\n            else:\n                content = b\"\"\n                if method == \"HEAD\":\n                    conn.close()\n                else:\n                    content = response.read()\n                response = Response(response)\n                if method != \"HEAD\":\n                    content = _decompressContent(response, content)\n\n            break\n        return (response, content)\n\n    def _request(\n        self,\n        conn,\n        host,\n        absolute_uri,\n        request_uri,\n        method,\n        body,\n        headers,\n        redirections,\n        cachekey,\n    ):\n        \"\"\"Do the actual request using the connection object\n        and also follow one level of redirects if necessary\"\"\"\n\n        auths = [\n            (auth.depth(request_uri), auth)\n            for auth in self.authorizations\n            if auth.inscope(host, request_uri)\n        ]\n        auth = auths and sorted(auths)[0][1] or None\n        if auth:\n            auth.request(method, request_uri, headers, body)\n\n        (response, content) = self._conn_request(\n            conn, request_uri, method, body, headers\n        )\n\n        if auth:\n            if auth.response(response, body):\n                auth.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                response._stale_digest = 1\n\n        if response.status == 401:\n            for authorization in self._auth_from_challenge(\n                host, request_uri, headers, response, content\n            ):\n                authorization.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                if response.status != 401:\n                    self.authorizations.append(authorization)\n                    authorization.response(response, body)\n                    break\n\n        if (\n            self.follow_all_redirects\n            or method in self.safe_methods\n            or response.status in (303, 308)\n        ):\n            if self.follow_redirects and response.status in self.redirect_codes:\n                # Pick out the location header and basically start from the beginning\n                # remembering first to strip the ETag header and decrement our 'depth'\n                if redirections:\n                    if \"location\" not in response and response.status != 300:\n                        raise RedirectMissingLocation(\n                            _(\n                                \"Redirected but the response is missing a Location: header.\"\n                            ),\n                            response,\n                            content,\n                        )\n                    # Fix-up relative redirects (which violate an RFC 2616 MUST)\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        (scheme, authority, path, query, fragment) = parse_uri(location)\n                        if authority == None:\n                            response[\"location\"] = urllib.parse.urljoin(\n                                absolute_uri, location\n                            )\n                    if response.status == 308 or (response.status == 301 and (method in self.safe_methods)):\n                        response[\"-x-permanent-redirect-url\"] = response[\"location\"]\n                        if \"content-location\" not in response:\n                            response[\"content-location\"] = absolute_uri\n                        _updateCache(headers, response, content, self.cache, cachekey)\n                    if \"if-none-match\" in headers:\n                        del headers[\"if-none-match\"]\n                    if \"if-modified-since\" in headers:\n                        del headers[\"if-modified-since\"]\n                    if (\n                        \"authorization\" in headers\n                        and not self.forward_authorization_headers\n                    ):\n                        del headers[\"authorization\"]\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        old_response = copy.deepcopy(response)\n                        if \"content-location\" not in old_response:\n                            old_response[\"content-location\"] = absolute_uri\n                        redirect_method = method\n                        if response.status in [302, 303]:\n                            redirect_method = \"GET\"\n                            body = None\n                        (response, content) = self.request(\n                            location,\n                            method=redirect_method,\n                            body=body,\n                            headers=headers,\n                            redirections=redirections - 1,\n                        )\n                        response.previous = old_response\n                else:\n                    raise RedirectLimit(\n                        \"Redirected more times than redirection_limit allows.\",\n                        response,\n                        content,\n                    )\n            elif response.status in [200, 203] and method in self.safe_methods:\n                # Don't cache 206's since we aren't going to handle byte range requests\n                if \"content-location\" not in response:\n                    response[\"content-location\"] = absolute_uri\n                _updateCache(headers, response, content, self.cache, cachekey)\n\n        return (response, content)\n\n    def _normalize_headers(self, headers):\n        return _normalize_headers(headers)\n\n    # Need to catch and rebrand some exceptions\n    # Then need to optionally turn all exceptions into status codes\n    # including all socket.* and httplib.* exceptions.\n\n    def request(\n        self,\n        uri,\n        method=\"GET\",\n        body=None,\n        headers=None,\n        redirections=DEFAULT_MAX_REDIRECTS,\n        connection_type=None,\n    ):\n        \"\"\" Performs a single HTTP request.\nThe 'uri' is the URI of the HTTP resource and can begin\nwith either 'http' or 'https'. The value of 'uri' must be an absolute URI.\n\nThe 'method' is the HTTP method to perform, such as GET, POST, DELETE, etc.\nThere is no restriction on the methods allowed.\n\nThe 'body' is the entity body to be sent with the request. It is a string\nobject.\n\nAny extra headers that are to be sent with the request should be provided in the\n'headers' dictionary.\n\nThe maximum number of redirect to follow before raising an\nexception is 'redirections. The default is 5.\n\nThe return value is a tuple of (response, content), the first\nbeing and instance of the 'Response' class, the second being\na string that contains the response entity body.\n        \"\"\"\n        conn_key = ''\n\n        try:\n            if headers is None:\n                headers = {}\n            else:\n                headers = self._normalize_headers(headers)\n\n            if \"user-agent\" not in headers:\n                headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n\n            uri = iri2uri(uri)\n            # Prevent CWE-75 space injection to manipulate request via part of uri.\n            # Prevent CWE-93 CRLF injection to modify headers via part of uri.\n            uri = uri.replace(\" \", \"%20\").replace(\"\\r\", \"%0D\").replace(\"\\n\", \"%0A\")\n\n            (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n\n            conn_key = scheme + \":\" + authority\n            conn = self.connections.get(conn_key)\n            if conn is None:\n                if not connection_type:\n                    connection_type = SCHEME_TO_CONNECTION[scheme]\n                certs = list(self.certificates.iter(authority))\n                if issubclass(connection_type, HTTPSConnectionWithTimeout):\n                    if certs:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            key_file=certs[0][0],\n                            cert_file=certs[0][1],\n                            timeout=self.timeout,\n                            proxy_info=self.proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            tls_maximum_version=self.tls_maximum_version,\n                            tls_minimum_version=self.tls_minimum_version,\n                            key_password=certs[0][2],\n                        )\n                    else:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            timeout=self.timeout,\n                            proxy_info=self.proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            tls_maximum_version=self.tls_maximum_version,\n                            tls_minimum_version=self.tls_minimum_version,\n                        )\n                else:\n                    conn = self.connections[conn_key] = connection_type(\n                        authority, timeout=self.timeout, proxy_info=self.proxy_info\n                    )\n                conn.set_debuglevel(debuglevel)\n\n            if \"range\" not in headers and \"accept-encoding\" not in headers:\n                headers[\"accept-encoding\"] = \"gzip, deflate\"\n\n            info = email.message.Message()\n            cachekey = None\n            cached_value = None\n            if self.cache:\n                cachekey = defrag_uri\n                cached_value = self.cache.get(cachekey)\n                if cached_value:\n                    try:\n                        info, content = cached_value.split(b\"\\r\\n\\r\\n\", 1)\n                        info = email.message_from_bytes(info)\n                        for k, v in info.items():\n                            if v.startswith(\"=?\") and v.endswith(\"?=\"):\n                                info.replace_header(\n                                    k, str(*email.header.decode_header(v)[0])\n                                )\n                    except (IndexError, ValueError):\n                        self.cache.delete(cachekey)\n                        cachekey = None\n                        cached_value = None\n\n            if (\n                method in self.optimistic_concurrency_methods\n                and self.cache\n                and \"etag\" in info\n                and not self.ignore_etag\n                and \"if-match\" not in headers\n            ):\n                # http://www.w3.org/1999/04/Editing/\n                headers[\"if-match\"] = info[\"etag\"]\n\n            # https://tools.ietf.org/html/rfc7234\n            # A cache MUST invalidate the effective Request URI as well as [...] Location and Content-Location\n            # when a non-error status code is received in response to an unsafe request method.\n            if self.cache and cachekey and method not in self.safe_methods:\n                self.cache.delete(cachekey)\n\n            # Check the vary header in the cache to see if this request\n            # matches what varies in the cache.\n            if method in self.safe_methods and \"vary\" in info:\n                vary = info[\"vary\"]\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    value = info[key]\n                    if headers.get(header, None) != value:\n                        cached_value = None\n                        break\n\n            if (\n                self.cache\n                and cached_value\n                and (method in self.safe_methods or info[\"status\"] == \"308\")\n                and \"range\" not in headers\n            ):\n                redirect_method = method\n                if info[\"status\"] not in (\"307\", \"308\"):\n                    redirect_method = \"GET\"\n                if \"-x-permanent-redirect-url\" in info:\n                    # Should cached permanent redirects be counted in our redirection count? For now, yes.\n                    if redirections <= 0:\n                        raise RedirectLimit(\n                            \"Redirected more times than redirection_limit allows.\",\n                            {},\n                            \"\",\n                        )\n                    (response, new_content) = self.request(\n                        info[\"-x-permanent-redirect-url\"],\n                        method=redirect_method,\n                        headers=headers,\n                        redirections=redirections - 1,\n                    )\n                    response.previous = Response(info)\n                    response.previous.fromcache = True\n                else:\n                    # Determine our course of action:\n                    #   Is the cached entry fresh or stale?\n                    #   Has the client requested a non-cached response?\n                    #\n                    # There seems to be three possible answers:\n                    # 1. [FRESH] Return the cache entry w/o doing a GET\n                    # 2. [STALE] Do the GET (but add in cache validators if available)\n                    # 3. [TRANSPARENT] Do a GET w/o any cache validators (Cache-Control: no-cache) on the request\n                    entry_disposition = _entry_disposition(info, headers)\n\n                    if entry_disposition == \"FRESH\":\n                        if not cached_value:\n                            info[\"status\"] = \"504\"\n                            content = b\"\"\n                        response = Response(info)\n                        if cached_value:\n                            response.fromcache = True\n                        return (response, content)\n\n                    if entry_disposition == \"STALE\":\n                        if (\n                            \"etag\" in info\n                            and not self.ignore_etag\n                            and not \"if-none-match\" in headers\n                        ):\n                            headers[\"if-none-match\"] = info[\"etag\"]\n                        if \"last-modified\" in info and not \"last-modified\" in headers:\n                            headers[\"if-modified-since\"] = info[\"last-modified\"]\n                    elif entry_disposition == \"TRANSPARENT\":\n                        pass\n\n                    (response, new_content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n\n                if response.status == 304 and method == \"GET\":\n                    # Rewrite the cache entry with the new end-to-end headers\n                    # Take all headers that are in response\n                    # and overwrite their values in info.\n                    # unless they are hop-by-hop, or are listed in the connection header.\n\n                    for key in _get_end2end_headers(response):\n                        info[key] = response[key]\n                    merged_response = Response(info)\n                    if hasattr(response, \"_stale_digest\"):\n                        merged_response._stale_digest = response._stale_digest\n                    _updateCache(\n                        headers, merged_response, content, self.cache, cachekey\n                    )\n                    response = merged_response\n                    response.status = 200\n                    response.fromcache = True\n\n                elif response.status == 200:\n                    content = new_content\n                else:\n                    self.cache.delete(cachekey)\n                    content = new_content\n            else:\n                cc = _parse_cache_control(headers)\n                if \"only-if-cached\" in cc:\n                    info[\"status\"] = \"504\"\n                    response = Response(info)\n                    content = b\"\"\n                else:\n                    (response, content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n        except Exception as e:\n            is_timeout = isinstance(e, socket.timeout)\n            if is_timeout:\n                conn = self.connections.pop(conn_key, None)\n                if conn:\n                    conn.close()\n\n            if self.force_exception_to_status_code:\n                if isinstance(e, HttpLib2ErrorWithResponse):\n                    response = e.response\n                    content = e.content\n                    response.status = 500\n                    response.reason = str(e)\n                elif isinstance(e, socket.timeout):\n                    content = b\"Request Timeout\"\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"408\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Request Timeout\"\n                else:\n                    content = str(e).encode(\"utf-8\")\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"400\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Bad Request\"\n            else:\n                raise\n\n        return (response, content)\n\n\nclass Response(dict):\n    \"\"\"An object more like email.message than httplib.HTTPResponse.\"\"\"\n\n    \"\"\"Is this response from our local cache\"\"\"\n    fromcache = False\n    \"\"\"HTTP protocol version used by server.\n\n    10 for HTTP/1.0, 11 for HTTP/1.1.\n    \"\"\"\n    version = 11\n\n    \"Status code returned by server. \"\n    status = 200\n    \"\"\"Reason phrase returned by server.\"\"\"\n    reason = \"Ok\"\n\n    previous = None\n\n    def __init__(self, info):\n        # info is either an email.message or\n        # an httplib.HTTPResponse object.\n        if isinstance(info, http.client.HTTPResponse):\n            for key, value in info.getheaders():\n                key = key.lower()\n                prev = self.get(key)\n                if prev is not None:\n                    value = \", \".join((prev, value))\n                self[key] = value\n            self.status = info.status\n            self[\"status\"] = str(self.status)\n            self.reason = info.reason\n            self.version = info.version\n        elif isinstance(info, email.message.Message):\n            for key, value in list(info.items()):\n                self[key.lower()] = value\n            self.status = int(self[\"status\"])\n        else:\n            for key, value in info.items():\n                self[key.lower()] = value\n            self.status = int(self.get(\"status\", self.status))\n\n    def __getattr__(self, name):\n        if name == \"dict\":\n            return self\n        else:\n            raise AttributeError(name)\n", "code_before": "# -*- coding: utf-8 -*-\n\"\"\"Small, fast HTTP client library for Python.\"\"\"\n\n__author__ = \"Joe Gregorio (joe@bitworking.org)\"\n__copyright__ = \"Copyright 2006, Joe Gregorio\"\n__contributors__ = [\n    \"Thomas Broyer (t.broyer@ltgt.net)\",\n    \"James Antill\",\n    \"Xavier Verges Farrero\",\n    \"Jonathan Feinberg\",\n    \"Blair Zajac\",\n    \"Sam Ruby\",\n    \"Louis Nyffenegger\",\n    \"Mark Pilgrim\",\n    \"Alex Yu\",\n]\n__license__ = \"MIT\"\n__version__ = '0.17.4'\n\nimport base64\nimport calendar\nimport copy\nimport email\nimport email.feedparser\nfrom email import header\nimport email.message\nimport email.utils\nimport errno\nfrom gettext import gettext as _\nimport gzip\nfrom hashlib import md5 as _md5\nfrom hashlib import sha1 as _sha\nimport hmac\nimport http.client\nimport io\nimport os\nimport random\nimport re\nimport socket\nimport ssl\nimport sys\nimport time\nimport urllib.parse\nimport zlib\n\ntry:\n    import socks\nexcept ImportError:\n    # TODO: remove this fallback and copypasted socksipy module upon py2/3 merge,\n    # idea is to have soft-dependency on any compatible module called socks\n    from . import socks\nfrom .iri2uri import iri2uri\n\n\ndef has_timeout(timeout):\n    if hasattr(socket, \"_GLOBAL_DEFAULT_TIMEOUT\"):\n        return timeout is not None and timeout is not socket._GLOBAL_DEFAULT_TIMEOUT\n    return timeout is not None\n\n\n__all__ = [\n    \"debuglevel\",\n    \"FailedToDecompressContent\",\n    \"Http\",\n    \"HttpLib2Error\",\n    \"ProxyInfo\",\n    \"RedirectLimit\",\n    \"RedirectMissingLocation\",\n    \"Response\",\n    \"RETRIES\",\n    \"UnimplementedDigestAuthOptionError\",\n    \"UnimplementedHmacDigestAuthOptionError\",\n]\n\n# The httplib debug level, set to a non-zero value to get debug output\ndebuglevel = 0\n\n# A request will be tried 'RETRIES' times if it fails at the socket/connection level.\nRETRIES = 2\n\n\n# All exceptions raised here derive from HttpLib2Error\nclass HttpLib2Error(Exception):\n    pass\n\n\n# Some exceptions can be caught and optionally\n# be turned back into responses.\nclass HttpLib2ErrorWithResponse(HttpLib2Error):\n    def __init__(self, desc, response, content):\n        self.response = response\n        self.content = content\n        HttpLib2Error.__init__(self, desc)\n\n\nclass RedirectMissingLocation(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass RedirectLimit(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass FailedToDecompressContent(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass UnimplementedDigestAuthOptionError(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass UnimplementedHmacDigestAuthOptionError(HttpLib2ErrorWithResponse):\n    pass\n\n\nclass MalformedHeader(HttpLib2Error):\n    pass\n\n\nclass RelativeURIError(HttpLib2Error):\n    pass\n\n\nclass ServerNotFoundError(HttpLib2Error):\n    pass\n\n\nclass ProxiesUnavailableError(HttpLib2Error):\n    pass\n\n\n# Open Items:\n# -----------\n\n# Are we removing the cached content too soon on PUT (only delete on 200 Maybe?)\n\n# Pluggable cache storage (supports storing the cache in\n#   flat files by default. We need a plug-in architecture\n#   that can support Berkeley DB and Squid)\n\n# == Known Issues ==\n# Does not handle a resource that uses conneg and Last-Modified but no ETag as a cache validator.\n# Does not handle Cache-Control: max-stale\n# Does not use Age: headers when calculating cache freshness.\n\n# The number of redirections to follow before giving up.\n# Note that only GET redirects are automatically followed.\n# Will also honor 301 requests by saving that info and never\n# requesting that URI again.\nDEFAULT_MAX_REDIRECTS = 5\n\n# Which headers are hop-by-hop headers by default\nHOP_BY_HOP = [\n    \"connection\",\n    \"keep-alive\",\n    \"proxy-authenticate\",\n    \"proxy-authorization\",\n    \"te\",\n    \"trailers\",\n    \"transfer-encoding\",\n    \"upgrade\",\n]\n\n# https://tools.ietf.org/html/rfc7231#section-8.1.3\nSAFE_METHODS = (\"GET\", \"HEAD\", \"OPTIONS\", \"TRACE\")\n\n# To change, assign to `Http().redirect_codes`\nREDIRECT_CODES = frozenset((300, 301, 302, 303, 307, 308))\n\n\nfrom httplib2 import certs\nCA_CERTS = certs.where()\n\n# PROTOCOL_TLS is python 3.5.3+. PROTOCOL_SSLv23 is deprecated.\n# Both PROTOCOL_TLS and PROTOCOL_SSLv23 are equivalent and means:\n# > Selects the highest protocol version that both the client and server support.\n# > Despite the name, this option can select \u201cTLS\u201d protocols as well as \u201cSSL\u201d.\n# source: https://docs.python.org/3.5/library/ssl.html#ssl.PROTOCOL_TLS\nDEFAULT_TLS_VERSION = getattr(ssl, \"PROTOCOL_TLS\", None) or getattr(\n    ssl, \"PROTOCOL_SSLv23\"\n)\n\ndef _build_ssl_context(\n    disable_ssl_certificate_validation, ca_certs, cert_file=None, key_file=None,\n    maximum_version=None, minimum_version=None, key_password=None,\n):\n    if not hasattr(ssl, \"SSLContext\"):\n        raise RuntimeError(\"httplib2 requires Python 3.2+ for ssl.SSLContext\")\n\n    context = ssl.SSLContext(DEFAULT_TLS_VERSION)\n    context.verify_mode = (\n        ssl.CERT_NONE if disable_ssl_certificate_validation else ssl.CERT_REQUIRED\n    )\n\n    # SSLContext.maximum_version and SSLContext.minimum_version are python 3.7+.\n    # source: https://docs.python.org/3/library/ssl.html#ssl.SSLContext.maximum_version\n    if maximum_version is not None:\n        if hasattr(context, \"maximum_version\"):\n            context.maximum_version = getattr(ssl.TLSVersion, maximum_version)\n        else:\n            raise RuntimeError(\"setting tls_maximum_version requires Python 3.7 and OpenSSL 1.1 or newer\")\n    if minimum_version is not None:\n        if hasattr(context, \"minimum_version\"):\n            context.minimum_version = getattr(ssl.TLSVersion, minimum_version)\n        else:\n            raise RuntimeError(\"setting tls_minimum_version requires Python 3.7 and OpenSSL 1.1 or newer\")\n\n    # check_hostname requires python 3.4+\n    # we will perform the equivalent in HTTPSConnectionWithTimeout.connect() by calling ssl.match_hostname\n    # if check_hostname is not supported.\n    if hasattr(context, \"check_hostname\"):\n        context.check_hostname = not disable_ssl_certificate_validation\n\n    context.load_verify_locations(ca_certs)\n\n    if cert_file:\n        context.load_cert_chain(cert_file, key_file, key_password)\n\n    return context\n\n\ndef _get_end2end_headers(response):\n    hopbyhop = list(HOP_BY_HOP)\n    hopbyhop.extend([x.strip() for x in response.get(\"connection\", \"\").split(\",\")])\n    return [header for header in list(response.keys()) if header not in hopbyhop]\n\n\nURI = re.compile(r\"^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\\?([^#]*))?(#(.*))?\")\n\n\ndef parse_uri(uri):\n    \"\"\"Parses a URI using the regex given in Appendix B of RFC 3986.\n\n        (scheme, authority, path, query, fragment) = parse_uri(uri)\n    \"\"\"\n    groups = URI.match(uri).groups()\n    return (groups[1], groups[3], groups[4], groups[6], groups[8])\n\n\ndef urlnorm(uri):\n    (scheme, authority, path, query, fragment) = parse_uri(uri)\n    if not scheme or not authority:\n        raise RelativeURIError(\"Only absolute URIs are allowed. uri = %s\" % uri)\n    authority = authority.lower()\n    scheme = scheme.lower()\n    if not path:\n        path = \"/\"\n    # Could do syntax based normalization of the URI before\n    # computing the digest. See Section 6.2.2 of Std 66.\n    request_uri = query and \"?\".join([path, query]) or path\n    scheme = scheme.lower()\n    defrag_uri = scheme + \"://\" + authority + request_uri\n    return scheme, authority, request_uri, defrag_uri\n\n\n# Cache filename construction (original borrowed from Venus http://intertwingly.net/code/venus/)\nre_url_scheme = re.compile(r\"^\\w+://\")\nre_unsafe = re.compile(r\"[^\\w\\-_.()=!]+\", re.ASCII)\n\n\ndef safename(filename):\n    \"\"\"Return a filename suitable for the cache.\n    Strips dangerous and common characters to create a filename we\n    can use to store the cache in.\n    \"\"\"\n    if isinstance(filename, bytes):\n        filename_bytes = filename\n        filename = filename.decode(\"utf-8\")\n    else:\n        filename_bytes = filename.encode(\"utf-8\")\n    filemd5 = _md5(filename_bytes).hexdigest()\n    filename = re_url_scheme.sub(\"\", filename)\n    filename = re_unsafe.sub(\"\", filename)\n\n    # limit length of filename (vital for Windows)\n    # https://github.com/httplib2/httplib2/pull/74\n    # C:\\Users\\    <username>    \\AppData\\Local\\Temp\\  <safe_filename>  ,   <md5>\n    #   9 chars + max 104 chars  +     20 chars      +       x       +  1  +  32  = max 259 chars\n    # Thus max safe filename x = 93 chars. Let it be 90 to make a round sum:\n    filename = filename[:90]\n\n    return \",\".join((filename, filemd5))\n\n\nNORMALIZE_SPACE = re.compile(r\"(?:\\r\\n)?[ \\t]+\")\n\n\ndef _normalize_headers(headers):\n    return dict(\n        [\n            (\n                _convert_byte_str(key).lower(),\n                NORMALIZE_SPACE.sub(_convert_byte_str(value), \" \").strip(),\n            )\n            for (key, value) in headers.items()\n        ]\n    )\n\n\ndef _convert_byte_str(s):\n    if not isinstance(s, str):\n        return str(s, \"utf-8\")\n    return s\n\n\ndef _parse_cache_control(headers):\n    retval = {}\n    if \"cache-control\" in headers:\n        parts = headers[\"cache-control\"].split(\",\")\n        parts_with_args = [\n            tuple([x.strip().lower() for x in part.split(\"=\", 1)])\n            for part in parts\n            if -1 != part.find(\"=\")\n        ]\n        parts_wo_args = [\n            (name.strip().lower(), 1) for name in parts if -1 == name.find(\"=\")\n        ]\n        retval = dict(parts_with_args + parts_wo_args)\n    return retval\n\n\n# Whether to use a strict mode to parse WWW-Authenticate headers\n# Might lead to bad results in case of ill-formed header value,\n# so disabled by default, falling back to relaxed parsing.\n# Set to true to turn on, useful for testing servers.\nUSE_WWW_AUTH_STRICT_PARSING = 0\n\n# In regex below:\n#    [^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+             matches a \"token\" as defined by HTTP\n#    \"(?:[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?\"    matches a \"quoted-string\" as defined by HTTP, when LWS have already been replaced by a single space\n# Actually, as an auth-param value can be either a token or a quoted-string, they are combined in a single pattern which matches both:\n#    \\\"?((?<=\\\")(?:[^\\0-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?(?=\\\")|(?<!\\\")[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+(?!\\\"))\\\"?\nWWW_AUTH_STRICT = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?(?=\\\")|(?<!\\\")[^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+(?!\\\"))\\\"?)(.*)$\"\n)\nWWW_AUTH_RELAXED = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^ \\t\\r\\n=]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\\\\\\"]|\\\\.)*?(?=\\\")|(?<!\\\")[^ \\t\\r\\n,]+(?!\\\"))\\\"?)(.*)$\"\n)\nUNQUOTE_PAIRS = re.compile(r\"\\\\(.)\")\n\n\ndef _parse_www_authenticate(headers, headername=\"www-authenticate\"):\n    \"\"\"Returns a dictionary of dictionaries, one dict\n    per auth_scheme.\"\"\"\n    retval = {}\n    if headername in headers:\n        try:\n            authenticate = headers[headername].strip()\n            www_auth = (\n                USE_WWW_AUTH_STRICT_PARSING and WWW_AUTH_STRICT or WWW_AUTH_RELAXED\n            )\n            while authenticate:\n                # Break off the scheme at the beginning of the line\n                if headername == \"authentication-info\":\n                    (auth_scheme, the_rest) = (\"digest\", authenticate)\n                else:\n                    (auth_scheme, the_rest) = authenticate.split(\" \", 1)\n                # Now loop over all the key value pairs that come after the scheme,\n                # being careful not to roll into the next scheme\n                match = www_auth.search(the_rest)\n                auth_params = {}\n                while match:\n                    if match and len(match.groups()) == 3:\n                        (key, value, the_rest) = match.groups()\n                        auth_params[key.lower()] = UNQUOTE_PAIRS.sub(\n                            r\"\\1\", value\n                        )  # '\\\\'.join([x.replace('\\\\', '') for x in value.split('\\\\\\\\')])\n                    match = www_auth.search(the_rest)\n                retval[auth_scheme.lower()] = auth_params\n                authenticate = the_rest.strip()\n        except ValueError:\n            raise MalformedHeader(\"WWW-Authenticate\")\n    return retval\n\n\ndef _entry_disposition(response_headers, request_headers):\n    \"\"\"Determine freshness from the Date, Expires and Cache-Control headers.\n\n    We don't handle the following:\n\n    1. Cache-Control: max-stale\n    2. Age: headers are not used in the calculations.\n\n    Not that this algorithm is simpler than you might think\n    because we are operating as a private (non-shared) cache.\n    This lets us ignore 's-maxage'. We can also ignore\n    'proxy-invalidate' since we aren't a proxy.\n    We will never return a stale document as\n    fresh as a design decision, and thus the non-implementation\n    of 'max-stale'. This also lets us safely ignore 'must-revalidate'\n    since we operate as if every server has sent 'must-revalidate'.\n    Since we are private we get to ignore both 'public' and\n    'private' parameters. We also ignore 'no-transform' since\n    we don't do any transformations.\n    The 'no-store' parameter is handled at a higher level.\n    So the only Cache-Control parameters we look at are:\n\n    no-cache\n    only-if-cached\n    max-age\n    min-fresh\n    \"\"\"\n\n    retval = \"STALE\"\n    cc = _parse_cache_control(request_headers)\n    cc_response = _parse_cache_control(response_headers)\n\n    if (\n        \"pragma\" in request_headers\n        and request_headers[\"pragma\"].lower().find(\"no-cache\") != -1\n    ):\n        retval = \"TRANSPARENT\"\n        if \"cache-control\" not in request_headers:\n            request_headers[\"cache-control\"] = \"no-cache\"\n    elif \"no-cache\" in cc:\n        retval = \"TRANSPARENT\"\n    elif \"no-cache\" in cc_response:\n        retval = \"STALE\"\n    elif \"only-if-cached\" in cc:\n        retval = \"FRESH\"\n    elif \"date\" in response_headers:\n        date = calendar.timegm(email.utils.parsedate_tz(response_headers[\"date\"]))\n        now = time.time()\n        current_age = max(0, now - date)\n        if \"max-age\" in cc_response:\n            try:\n                freshness_lifetime = int(cc_response[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        elif \"expires\" in response_headers:\n            expires = email.utils.parsedate_tz(response_headers[\"expires\"])\n            if None == expires:\n                freshness_lifetime = 0\n            else:\n                freshness_lifetime = max(0, calendar.timegm(expires) - date)\n        else:\n            freshness_lifetime = 0\n        if \"max-age\" in cc:\n            try:\n                freshness_lifetime = int(cc[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        if \"min-fresh\" in cc:\n            try:\n                min_fresh = int(cc[\"min-fresh\"])\n            except ValueError:\n                min_fresh = 0\n            current_age += min_fresh\n        if freshness_lifetime > current_age:\n            retval = \"FRESH\"\n    return retval\n\n\ndef _decompressContent(response, new_content):\n    content = new_content\n    try:\n        encoding = response.get(\"content-encoding\", None)\n        if encoding in [\"gzip\", \"deflate\"]:\n            if encoding == \"gzip\":\n                content = gzip.GzipFile(fileobj=io.BytesIO(new_content)).read()\n            if encoding == \"deflate\":\n                content = zlib.decompress(content, -zlib.MAX_WBITS)\n            response[\"content-length\"] = str(len(content))\n            # Record the historical presence of the encoding in a way the won't interfere.\n            response[\"-content-encoding\"] = response[\"content-encoding\"]\n            del response[\"content-encoding\"]\n    except (IOError, zlib.error):\n        content = \"\"\n        raise FailedToDecompressContent(\n            _(\"Content purported to be compressed with %s but failed to decompress.\")\n            % response.get(\"content-encoding\"),\n            response,\n            content,\n        )\n    return content\n\n\ndef _bind_write_headers(msg):\n    def _write_headers(self):\n        # Self refers to the Generator object.\n        for h, v in msg.items():\n            print(\"%s:\" % h, end=\" \", file=self._fp)\n            if isinstance(v, header.Header):\n                print(v.encode(maxlinelen=self._maxheaderlen), file=self._fp)\n            else:\n                # email.Header got lots of smarts, so use it.\n                headers = header.Header(\n                    v, maxlinelen=self._maxheaderlen, charset=\"utf-8\", header_name=h\n                )\n                print(headers.encode(), file=self._fp)\n        # A blank line always separates headers from body.\n        print(file=self._fp)\n\n    return _write_headers\n\n\ndef _updateCache(request_headers, response_headers, content, cache, cachekey):\n    if cachekey:\n        cc = _parse_cache_control(request_headers)\n        cc_response = _parse_cache_control(response_headers)\n        if \"no-store\" in cc or \"no-store\" in cc_response:\n            cache.delete(cachekey)\n        else:\n            info = email.message.Message()\n            for key, value in response_headers.items():\n                if key not in [\"status\", \"content-encoding\", \"transfer-encoding\"]:\n                    info[key] = value\n\n            # Add annotations to the cache to indicate what headers\n            # are variant for this request.\n            vary = response_headers.get(\"vary\", None)\n            if vary:\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    try:\n                        info[key] = request_headers[header]\n                    except KeyError:\n                        pass\n\n            status = response_headers.status\n            if status == 304:\n                status = 200\n\n            status_header = \"status: %d\\r\\n\" % status\n\n            try:\n                header_str = info.as_string()\n            except UnicodeEncodeError:\n                setattr(info, \"_write_headers\", _bind_write_headers(info))\n                header_str = info.as_string()\n\n            header_str = re.sub(\"\\r(?!\\n)|(?<!\\r)\\n\", \"\\r\\n\", header_str)\n            text = b\"\".join(\n                [status_header.encode(\"utf-8\"), header_str.encode(\"utf-8\"), content]\n            )\n\n            cache.set(cachekey, text)\n\n\ndef _cnonce():\n    dig = _md5(\n        (\n            \"%s:%s\"\n            % (time.ctime(), [\"0123456789\"[random.randrange(0, 9)] for i in range(20)])\n        ).encode(\"utf-8\")\n    ).hexdigest()\n    return dig[:16]\n\n\ndef _wsse_username_token(cnonce, iso_now, password):\n    return base64.b64encode(\n        _sha((\"%s%s%s\" % (cnonce, iso_now, password)).encode(\"utf-8\")).digest()\n    ).strip()\n\n\n# For credentials we need two things, first\n# a pool of credential to try (not necesarily tied to BAsic, Digest, etc.)\n# Then we also need a list of URIs that have already demanded authentication\n# That list is tricky since sub-URIs can take the same auth, or the\n# auth scheme may change as you descend the tree.\n# So we also need each Auth instance to be able to tell us\n# how close to the 'top' it is.\n\n\nclass Authentication(object):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        self.path = path\n        self.host = host\n        self.credentials = credentials\n        self.http = http\n\n    def depth(self, request_uri):\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        return request_uri[len(self.path) :].count(\"/\")\n\n    def inscope(self, host, request_uri):\n        # XXX Should we normalize the request_uri?\n        (scheme, authority, path, query, fragment) = parse_uri(request_uri)\n        return (host == self.host) and path.startswith(self.path)\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header. Over-rise this in sub-classes.\"\"\"\n        pass\n\n    def response(self, response, content):\n        \"\"\"Gives us a chance to update with new nonces\n        or such returned from the last authorized response.\n        Over-rise this in sub-classes if necessary.\n\n        Return TRUE is the request is to be retried, for\n        example Digest may return stale=true.\n        \"\"\"\n        return False\n\n    def __eq__(self, auth):\n        return False\n\n    def __ne__(self, auth):\n        return True\n\n    def __lt__(self, auth):\n        return True\n\n    def __gt__(self, auth):\n        return False\n\n    def __le__(self, auth):\n        return True\n\n    def __ge__(self, auth):\n        return False\n\n    def __bool__(self):\n        return True\n\n\nclass BasicAuthentication(Authentication):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = \"Basic \" + base64.b64encode(\n            (\"%s:%s\" % self.credentials).encode(\"utf-8\")\n        ).strip().decode(\"utf-8\")\n\n\nclass DigestAuthentication(Authentication):\n    \"\"\"Only do qop='auth' and MD5, since that\n    is all Apache currently implements\"\"\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        self.challenge = challenge[\"digest\"]\n        qop = self.challenge.get(\"qop\", \"auth\")\n        self.challenge[\"qop\"] = (\n            (\"auth\" in [x.strip() for x in qop.split()]) and \"auth\" or None\n        )\n        if self.challenge[\"qop\"] is None:\n            raise UnimplementedDigestAuthOptionError(\n                _(\"Unsupported value for qop: %s.\" % qop)\n            )\n        self.challenge[\"algorithm\"] = self.challenge.get(\"algorithm\", \"MD5\").upper()\n        if self.challenge[\"algorithm\"] != \"MD5\":\n            raise UnimplementedDigestAuthOptionError(\n                _(\"Unsupported value for algorithm: %s.\" % self.challenge[\"algorithm\"])\n            )\n        self.A1 = \"\".join(\n            [\n                self.credentials[0],\n                \":\",\n                self.challenge[\"realm\"],\n                \":\",\n                self.credentials[1],\n            ]\n        )\n        self.challenge[\"nc\"] = 1\n\n    def request(self, method, request_uri, headers, content, cnonce=None):\n        \"\"\"Modify the request headers\"\"\"\n        H = lambda x: _md5(x.encode(\"utf-8\")).hexdigest()\n        KD = lambda s, d: H(\"%s:%s\" % (s, d))\n        A2 = \"\".join([method, \":\", request_uri])\n        self.challenge[\"cnonce\"] = cnonce or _cnonce()\n        request_digest = '\"%s\"' % KD(\n            H(self.A1),\n            \"%s:%s:%s:%s:%s\"\n            % (\n                self.challenge[\"nonce\"],\n                \"%08x\" % self.challenge[\"nc\"],\n                self.challenge[\"cnonce\"],\n                self.challenge[\"qop\"],\n                H(A2),\n            ),\n        )\n        headers[\"authorization\"] = (\n            'Digest username=\"%s\", realm=\"%s\", nonce=\"%s\", '\n            'uri=\"%s\", algorithm=%s, response=%s, qop=%s, '\n            'nc=%08x, cnonce=\"%s\"'\n        ) % (\n            self.credentials[0],\n            self.challenge[\"realm\"],\n            self.challenge[\"nonce\"],\n            request_uri,\n            self.challenge[\"algorithm\"],\n            request_digest,\n            self.challenge[\"qop\"],\n            self.challenge[\"nc\"],\n            self.challenge[\"cnonce\"],\n        )\n        if self.challenge.get(\"opaque\"):\n            headers[\"authorization\"] += ', opaque=\"%s\"' % self.challenge[\"opaque\"]\n        self.challenge[\"nc\"] += 1\n\n    def response(self, response, content):\n        if \"authentication-info\" not in response:\n            challenge = _parse_www_authenticate(response, \"www-authenticate\").get(\n                \"digest\", {}\n            )\n            if \"true\" == challenge.get(\"stale\"):\n                self.challenge[\"nonce\"] = challenge[\"nonce\"]\n                self.challenge[\"nc\"] = 1\n                return True\n        else:\n            updated_challenge = _parse_www_authenticate(\n                response, \"authentication-info\"\n            ).get(\"digest\", {})\n\n            if \"nextnonce\" in updated_challenge:\n                self.challenge[\"nonce\"] = updated_challenge[\"nextnonce\"]\n                self.challenge[\"nc\"] = 1\n        return False\n\n\nclass HmacDigestAuthentication(Authentication):\n    \"\"\"Adapted from Robert Sayre's code and DigestAuthentication above.\"\"\"\n\n    __author__ = \"Thomas Broyer (t.broyer@ltgt.net)\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        self.challenge = challenge[\"hmacdigest\"]\n        # TODO: self.challenge['domain']\n        self.challenge[\"reason\"] = self.challenge.get(\"reason\", \"unauthorized\")\n        if self.challenge[\"reason\"] not in [\"unauthorized\", \"integrity\"]:\n            self.challenge[\"reason\"] = \"unauthorized\"\n        self.challenge[\"salt\"] = self.challenge.get(\"salt\", \"\")\n        if not self.challenge.get(\"snonce\"):\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\"The challenge doesn't contain a server nonce, or this one is empty.\")\n            )\n        self.challenge[\"algorithm\"] = self.challenge.get(\"algorithm\", \"HMAC-SHA-1\")\n        if self.challenge[\"algorithm\"] not in [\"HMAC-SHA-1\", \"HMAC-MD5\"]:\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\"Unsupported value for algorithm: %s.\" % self.challenge[\"algorithm\"])\n            )\n        self.challenge[\"pw-algorithm\"] = self.challenge.get(\"pw-algorithm\", \"SHA-1\")\n        if self.challenge[\"pw-algorithm\"] not in [\"SHA-1\", \"MD5\"]:\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\n                    \"Unsupported value for pw-algorithm: %s.\"\n                    % self.challenge[\"pw-algorithm\"]\n                )\n            )\n        if self.challenge[\"algorithm\"] == \"HMAC-MD5\":\n            self.hashmod = _md5\n        else:\n            self.hashmod = _sha\n        if self.challenge[\"pw-algorithm\"] == \"MD5\":\n            self.pwhashmod = _md5\n        else:\n            self.pwhashmod = _sha\n        self.key = \"\".join(\n            [\n                self.credentials[0],\n                \":\",\n                self.pwhashmod.new(\n                    \"\".join([self.credentials[1], self.challenge[\"salt\"]])\n                )\n                .hexdigest()\n                .lower(),\n                \":\",\n                self.challenge[\"realm\"],\n            ]\n        )\n        self.key = self.pwhashmod.new(self.key).hexdigest().lower()\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers\"\"\"\n        keys = _get_end2end_headers(headers)\n        keylist = \"\".join([\"%s \" % k for k in keys])\n        headers_val = \"\".join([headers[k] for k in keys])\n        created = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n        cnonce = _cnonce()\n        request_digest = \"%s:%s:%s:%s:%s\" % (\n            method,\n            request_uri,\n            cnonce,\n            self.challenge[\"snonce\"],\n            headers_val,\n        )\n        request_digest = (\n            hmac.new(self.key, request_digest, self.hashmod).hexdigest().lower()\n        )\n        headers[\"authorization\"] = (\n            'HMACDigest username=\"%s\", realm=\"%s\", snonce=\"%s\",'\n            ' cnonce=\"%s\", uri=\"%s\", created=\"%s\", '\n            'response=\"%s\", headers=\"%s\"'\n        ) % (\n            self.credentials[0],\n            self.challenge[\"realm\"],\n            self.challenge[\"snonce\"],\n            cnonce,\n            request_uri,\n            created,\n            request_digest,\n            keylist,\n        )\n\n    def response(self, response, content):\n        challenge = _parse_www_authenticate(response, \"www-authenticate\").get(\n            \"hmacdigest\", {}\n        )\n        if challenge.get(\"reason\") in [\"integrity\", \"stale\"]:\n            return True\n        return False\n\n\nclass WsseAuthentication(Authentication):\n    \"\"\"This is thinly tested and should not be relied upon.\n    At this time there isn't any third party server to test against.\n    Blogger and TypePad implemented this algorithm at one point\n    but Blogger has since switched to Basic over HTTPS and\n    TypePad has implemented it wrong, by never issuing a 401\n    challenge but instead requiring your client to telepathically know that\n    their endpoint is expecting WSSE profile=\"UsernameToken\".\"\"\"\n\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = 'WSSE profile=\"UsernameToken\"'\n        iso_now = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n        cnonce = _cnonce()\n        password_digest = _wsse_username_token(cnonce, iso_now, self.credentials[1])\n        headers[\"X-WSSE\"] = (\n            'UsernameToken Username=\"%s\", PasswordDigest=\"%s\", '\n            'Nonce=\"%s\", Created=\"%s\"'\n        ) % (self.credentials[0], password_digest, cnonce, iso_now)\n\n\nclass GoogleLoginAuthentication(Authentication):\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        from urllib.parse import urlencode\n\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        service = challenge[\"googlelogin\"].get(\"service\", \"xapi\")\n        # Bloggger actually returns the service in the challenge\n        # For the rest we guess based on the URI\n        if service == \"xapi\" and request_uri.find(\"calendar\") > 0:\n            service = \"cl\"\n        # No point in guessing Base or Spreadsheet\n        # elif request_uri.find(\"spreadsheets\") > 0:\n        #    service = \"wise\"\n\n        auth = dict(\n            Email=credentials[0],\n            Passwd=credentials[1],\n            service=service,\n            source=headers[\"user-agent\"],\n        )\n        resp, content = self.http.request(\n            \"https://www.google.com/accounts/ClientLogin\",\n            method=\"POST\",\n            body=urlencode(auth),\n            headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n        )\n        lines = content.split(\"\\n\")\n        d = dict([tuple(line.split(\"=\", 1)) for line in lines if line])\n        if resp.status == 403:\n            self.Auth = \"\"\n        else:\n            self.Auth = d[\"Auth\"]\n\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers to add the appropriate\n        Authorization header.\"\"\"\n        headers[\"authorization\"] = \"GoogleLogin Auth=\" + self.Auth\n\n\nAUTH_SCHEME_CLASSES = {\n    \"basic\": BasicAuthentication,\n    \"wsse\": WsseAuthentication,\n    \"digest\": DigestAuthentication,\n    \"hmacdigest\": HmacDigestAuthentication,\n    \"googlelogin\": GoogleLoginAuthentication,\n}\n\nAUTH_SCHEME_ORDER = [\"hmacdigest\", \"googlelogin\", \"digest\", \"wsse\", \"basic\"]\n\n\nclass FileCache(object):\n    \"\"\"Uses a local directory as a store for cached files.\n    Not really safe to use if multiple threads or processes are going to\n    be running on the same cache.\n    \"\"\"\n\n    def __init__(\n        self, cache, safe=safename\n    ):  # use safe=lambda x: md5.new(x).hexdigest() for the old behavior\n        self.cache = cache\n        self.safe = safe\n        if not os.path.exists(cache):\n            os.makedirs(self.cache)\n\n    def get(self, key):\n        retval = None\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        try:\n            f = open(cacheFullPath, \"rb\")\n            retval = f.read()\n            f.close()\n        except IOError:\n            pass\n        return retval\n\n    def set(self, key, value):\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        f = open(cacheFullPath, \"wb\")\n        f.write(value)\n        f.close()\n\n    def delete(self, key):\n        cacheFullPath = os.path.join(self.cache, self.safe(key))\n        if os.path.exists(cacheFullPath):\n            os.remove(cacheFullPath)\n\n\nclass Credentials(object):\n    def __init__(self):\n        self.credentials = []\n\n    def add(self, name, password, domain=\"\"):\n        self.credentials.append((domain.lower(), name, password))\n\n    def clear(self):\n        self.credentials = []\n\n    def iter(self, domain):\n        for (cdomain, name, password) in self.credentials:\n            if cdomain == \"\" or domain == cdomain:\n                yield (name, password)\n\n\nclass KeyCerts(Credentials):\n    \"\"\"Identical to Credentials except that\n    name/password are mapped to key/cert.\"\"\"\n    def add(self, key, cert, domain, password):\n        self.credentials.append((domain.lower(), key, cert, password))\n\n    def iter(self, domain):\n        for (cdomain, key, cert, password) in self.credentials:\n            if cdomain == \"\" or domain == cdomain:\n                yield (key, cert, password)\n\n\nclass AllHosts(object):\n    pass\n\n\nclass ProxyInfo(object):\n    \"\"\"Collect information required to use a proxy.\"\"\"\n\n    bypass_hosts = ()\n\n    def __init__(\n        self,\n        proxy_type,\n        proxy_host,\n        proxy_port,\n        proxy_rdns=True,\n        proxy_user=None,\n        proxy_pass=None,\n        proxy_headers=None,\n    ):\n        \"\"\"Args:\n\n          proxy_type: The type of proxy server.  This must be set to one of\n          socks.PROXY_TYPE_XXX constants.  For example:  p =\n          ProxyInfo(proxy_type=socks.PROXY_TYPE_HTTP, proxy_host='localhost',\n          proxy_port=8000)\n          proxy_host: The hostname or IP address of the proxy server.\n          proxy_port: The port that the proxy server is running on.\n          proxy_rdns: If True (default), DNS queries will not be performed\n          locally, and instead, handed to the proxy to resolve.  This is useful\n          if the network does not allow resolution of non-local names. In\n          httplib2 0.9 and earlier, this defaulted to False.\n          proxy_user: The username used to authenticate with the proxy server.\n          proxy_pass: The password used to authenticate with the proxy server.\n          proxy_headers: Additional or modified headers for the proxy connect\n          request.\n        \"\"\"\n        if isinstance(proxy_user, bytes):\n            proxy_user = proxy_user.decode()\n        if isinstance(proxy_pass, bytes):\n            proxy_pass = proxy_pass.decode()\n        self.proxy_type, self.proxy_host, self.proxy_port, self.proxy_rdns, self.proxy_user, self.proxy_pass, self.proxy_headers = (\n            proxy_type,\n            proxy_host,\n            proxy_port,\n            proxy_rdns,\n            proxy_user,\n            proxy_pass,\n            proxy_headers,\n        )\n\n    def astuple(self):\n        return (\n            self.proxy_type,\n            self.proxy_host,\n            self.proxy_port,\n            self.proxy_rdns,\n            self.proxy_user,\n            self.proxy_pass,\n            self.proxy_headers,\n        )\n\n    def isgood(self):\n        return socks and (self.proxy_host != None) and (self.proxy_port != None)\n\n    def applies_to(self, hostname):\n        return not self.bypass_host(hostname)\n\n    def bypass_host(self, hostname):\n        \"\"\"Has this host been excluded from the proxy config\"\"\"\n        if self.bypass_hosts is AllHosts:\n            return True\n\n        hostname = \".\" + hostname.lstrip(\".\")\n        for skip_name in self.bypass_hosts:\n            # *.suffix\n            if skip_name.startswith(\".\") and hostname.endswith(skip_name):\n                return True\n            # exact match\n            if hostname == \".\" + skip_name:\n                return True\n        return False\n\n    def __repr__(self):\n        return (\n            \"<ProxyInfo type={p.proxy_type} \"\n            \"host:port={p.proxy_host}:{p.proxy_port} rdns={p.proxy_rdns}\"\n            + \" user={p.proxy_user} headers={p.proxy_headers}>\"\n        ).format(p=self)\n\n\ndef proxy_info_from_environment(method=\"http\"):\n    \"\"\"Read proxy info from the environment variables.\n    \"\"\"\n    if method not in (\"http\", \"https\"):\n        return\n\n    env_var = method + \"_proxy\"\n    url = os.environ.get(env_var, os.environ.get(env_var.upper()))\n    if not url:\n        return\n    return proxy_info_from_url(url, method, noproxy=None)\n\n\ndef proxy_info_from_url(url, method=\"http\", noproxy=None):\n    \"\"\"Construct a ProxyInfo from a URL (such as http_proxy env var)\n    \"\"\"\n    url = urllib.parse.urlparse(url)\n    username = None\n    password = None\n    port = None\n    if \"@\" in url[1]:\n        ident, host_port = url[1].split(\"@\", 1)\n        if \":\" in ident:\n            username, password = ident.split(\":\", 1)\n        else:\n            password = ident\n    else:\n        host_port = url[1]\n    if \":\" in host_port:\n        host, port = host_port.split(\":\", 1)\n    else:\n        host = host_port\n\n    if port:\n        port = int(port)\n    else:\n        port = dict(https=443, http=80)[method]\n\n    proxy_type = 3  # socks.PROXY_TYPE_HTTP\n    pi = ProxyInfo(\n        proxy_type=proxy_type,\n        proxy_host=host,\n        proxy_port=port,\n        proxy_user=username or None,\n        proxy_pass=password or None,\n        proxy_headers=None,\n    )\n\n    bypass_hosts = []\n    # If not given an explicit noproxy value, respect values in env vars.\n    if noproxy is None:\n        noproxy = os.environ.get(\"no_proxy\", os.environ.get(\"NO_PROXY\", \"\"))\n    # Special case: A single '*' character means all hosts should be bypassed.\n    if noproxy == \"*\":\n        bypass_hosts = AllHosts\n    elif noproxy.strip():\n        bypass_hosts = noproxy.split(\",\")\n        bypass_hosts = tuple(filter(bool, bypass_hosts))  # To exclude empty string.\n\n    pi.bypass_hosts = bypass_hosts\n    return pi\n\n\nclass HTTPConnectionWithTimeout(http.client.HTTPConnection):\n    \"\"\"HTTPConnection subclass that supports timeouts\n\n    HTTPConnection subclass that supports timeouts\n\n    All timeouts are in seconds. If None is passed for timeout then\n    Python's default timeout for sockets will be used. See for example\n    the docs of socket.setdefaulttimeout():\n    http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n    \"\"\"\n\n    def __init__(self, host, port=None, timeout=None, proxy_info=None):\n        http.client.HTTPConnection.__init__(self, host, port=port, timeout=timeout)\n\n        self.proxy_info = proxy_info\n        if proxy_info and not isinstance(proxy_info, ProxyInfo):\n            self.proxy_info = proxy_info(\"http\")\n\n    def connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.proxy_info and socks is None:\n            raise ProxiesUnavailableError(\n                \"Proxy support missing but proxy use was requested!\"\n            )\n        if self.proxy_info and self.proxy_info.isgood() and self.proxy_info.applies_to(self.host):\n            use_proxy = True\n            proxy_type, proxy_host, proxy_port, proxy_rdns, proxy_user, proxy_pass, proxy_headers = (\n                self.proxy_info.astuple()\n            )\n\n            host = proxy_host\n            port = proxy_port\n        else:\n            use_proxy = False\n\n            host = self.host\n            port = self.port\n            proxy_type = None\n\n        socket_err = None\n\n        for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n            af, socktype, proto, canonname, sa = res\n            try:\n                if use_proxy:\n                    self.sock = socks.socksocket(af, socktype, proto)\n                    self.sock.setproxy(\n                        proxy_type,\n                        proxy_host,\n                        proxy_port,\n                        proxy_rdns,\n                        proxy_user,\n                        proxy_pass,\n                    )\n                else:\n                    self.sock = socket.socket(af, socktype, proto)\n                    self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                if has_timeout(self.timeout):\n                    self.sock.settimeout(self.timeout)\n                if self.debuglevel > 0:\n                    print(\n                        \"connect: ({0}, {1}) ************\".format(self.host, self.port)\n                    )\n                    if use_proxy:\n                        print(\n                            \"proxy: {0} ************\".format(\n                                str(\n                                    (\n                                        proxy_host,\n                                        proxy_port,\n                                        proxy_rdns,\n                                        proxy_user,\n                                        proxy_pass,\n                                        proxy_headers,\n                                    )\n                                )\n                            )\n                        )\n\n                self.sock.connect((self.host, self.port) + sa[2:])\n            except socket.error as e:\n                socket_err = e\n                if self.debuglevel > 0:\n                    print(\"connect fail: ({0}, {1})\".format(self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: {0}\".format(\n                                str(\n                                    (\n                                        proxy_host,\n                                        proxy_port,\n                                        proxy_rdns,\n                                        proxy_user,\n                                        proxy_pass,\n                                        proxy_headers,\n                                    )\n                                )\n                            )\n                        )\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                continue\n            break\n        if not self.sock:\n            raise socket_err\n\n\nclass HTTPSConnectionWithTimeout(http.client.HTTPSConnection):\n    \"\"\"This class allows communication via SSL.\n\n    All timeouts are in seconds. If None is passed for timeout then\n    Python's default timeout for sockets will be used. See for example\n    the docs of socket.setdefaulttimeout():\n    http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        timeout=None,\n        proxy_info=None,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        tls_maximum_version=None,\n        tls_minimum_version=None,\n        key_password=None,\n    ):\n\n        self.disable_ssl_certificate_validation = disable_ssl_certificate_validation\n        self.ca_certs = ca_certs if ca_certs else CA_CERTS\n\n        self.proxy_info = proxy_info\n        if proxy_info and not isinstance(proxy_info, ProxyInfo):\n            self.proxy_info = proxy_info(\"https\")\n\n        context = _build_ssl_context(\n            self.disable_ssl_certificate_validation, self.ca_certs, cert_file, key_file,\n            maximum_version=tls_maximum_version, minimum_version=tls_minimum_version,\n            key_password=key_password,\n        )\n        super(HTTPSConnectionWithTimeout, self).__init__(\n            host,\n            port=port,\n            timeout=timeout,\n            context=context,\n        )\n        self.key_file = key_file\n        self.cert_file = cert_file\n        self.key_password = key_password\n\n    def connect(self):\n        \"\"\"Connect to a host on a given (SSL) port.\"\"\"\n        if self.proxy_info and self.proxy_info.isgood() and self.proxy_info.applies_to(self.host):\n            use_proxy = True\n            proxy_type, proxy_host, proxy_port, proxy_rdns, proxy_user, proxy_pass, proxy_headers = (\n                self.proxy_info.astuple()\n            )\n\n            host = proxy_host\n            port = proxy_port\n        else:\n            use_proxy = False\n\n            host = self.host\n            port = self.port\n            proxy_type = None\n            proxy_headers = None\n\n        socket_err = None\n\n        address_info = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\n        for family, socktype, proto, canonname, sockaddr in address_info:\n            try:\n                if use_proxy:\n                    sock = socks.socksocket(family, socktype, proto)\n\n                    sock.setproxy(\n                        proxy_type,\n                        proxy_host,\n                        proxy_port,\n                        proxy_rdns,\n                        proxy_user,\n                        proxy_pass,\n                    )\n                else:\n                    sock = socket.socket(family, socktype, proto)\n                    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                if has_timeout(self.timeout):\n                    sock.settimeout(self.timeout)\n                sock.connect((self.host, self.port))\n\n                self.sock = self._context.wrap_socket(sock, server_hostname=self.host)\n\n                # Python 3.3 compatibility: emulate the check_hostname behavior\n                if (\n                    not hasattr(self._context, \"check_hostname\")\n                    and not self.disable_ssl_certificate_validation\n                ):\n                    try:\n                        ssl.match_hostname(self.sock.getpeercert(), self.host)\n                    except Exception:\n                        self.sock.shutdown(socket.SHUT_RDWR)\n                        self.sock.close()\n                        raise\n\n                if self.debuglevel > 0:\n                    print(\"connect: ({0}, {1})\".format(self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: {0}\".format(\n                                str(\n                                    (\n                                        proxy_host,\n                                        proxy_port,\n                                        proxy_rdns,\n                                        proxy_user,\n                                        proxy_pass,\n                                        proxy_headers,\n                                    )\n                                )\n                            )\n                        )\n            except (ssl.SSLError, ssl.CertificateError) as e:\n                if sock:\n                    sock.close()\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                raise\n            except (socket.timeout, socket.gaierror):\n                raise\n            except socket.error as e:\n                socket_err = e\n                if self.debuglevel > 0:\n                    print(\"connect fail: ({0}, {1})\".format(self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: {0}\".format(\n                                str(\n                                    (\n                                        proxy_host,\n                                        proxy_port,\n                                        proxy_rdns,\n                                        proxy_user,\n                                        proxy_pass,\n                                        proxy_headers,\n                                    )\n                                )\n                            )\n                        )\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                continue\n            break\n        if not self.sock:\n            raise socket_err\n\n\nSCHEME_TO_CONNECTION = {\n    \"http\": HTTPConnectionWithTimeout,\n    \"https\": HTTPSConnectionWithTimeout,\n}\n\n\nclass Http(object):\n    \"\"\"An HTTP client that handles:\n\n    - all methods\n    - caching\n    - ETags\n    - compression,\n    - HTTPS\n    - Basic\n    - Digest\n    - WSSE\n\n    and more.\n    \"\"\"\n\n    def __init__(\n        self,\n        cache=None,\n        timeout=None,\n        proxy_info=proxy_info_from_environment,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        tls_maximum_version=None,\n        tls_minimum_version=None,\n    ):\n        \"\"\"If 'cache' is a string then it is used as a directory name for\n        a disk cache. Otherwise it must be an object that supports the\n        same interface as FileCache.\n\n        All timeouts are in seconds. If None is passed for timeout\n        then Python's default timeout for sockets will be used. See\n        for example the docs of socket.setdefaulttimeout():\n        http://docs.python.org/library/socket.html#socket.setdefaulttimeout\n\n        `proxy_info` may be:\n          - a callable that takes the http scheme ('http' or 'https') and\n            returns a ProxyInfo instance per request. By default, uses\n            proxy_info_from_environment.\n          - a ProxyInfo instance (static proxy config).\n          - None (proxy disabled).\n\n        ca_certs is the path of a file containing root CA certificates for SSL\n        server certificate validation.  By default, a CA cert file bundled with\n        httplib2 is used.\n\n        If disable_ssl_certificate_validation is true, SSL cert validation will\n        not be performed.\n\n        tls_maximum_version / tls_minimum_version require Python 3.7+ /\n        OpenSSL 1.1.0g+. A value of \"TLSv1_3\" requires OpenSSL 1.1.1+.\n\"\"\"\n        self.proxy_info = proxy_info\n        self.ca_certs = ca_certs\n        self.disable_ssl_certificate_validation = disable_ssl_certificate_validation\n        self.tls_maximum_version = tls_maximum_version\n        self.tls_minimum_version = tls_minimum_version\n        # Map domain name to an httplib connection\n        self.connections = {}\n        # The location of the cache, for now a directory\n        # where cached responses are held.\n        if cache and isinstance(cache, str):\n            self.cache = FileCache(cache)\n        else:\n            self.cache = cache\n\n        # Name/password\n        self.credentials = Credentials()\n\n        # Key/cert\n        self.certificates = KeyCerts()\n\n        # authorization objects\n        self.authorizations = []\n\n        # If set to False then no redirects are followed, even safe ones.\n        self.follow_redirects = True\n\n        self.redirect_codes = REDIRECT_CODES\n\n        # Which HTTP methods do we apply optimistic concurrency to, i.e.\n        # which methods get an \"if-match:\" etag header added to them.\n        self.optimistic_concurrency_methods = [\"PUT\", \"PATCH\"]\n\n        self.safe_methods = list(SAFE_METHODS)\n\n        # If 'follow_redirects' is True, and this is set to True then\n        # all redirecs are followed, including unsafe ones.\n        self.follow_all_redirects = False\n\n        self.ignore_etag = False\n\n        self.force_exception_to_status_code = False\n\n        self.timeout = timeout\n\n        # Keep Authorization: headers on a redirect.\n        self.forward_authorization_headers = False\n\n    def close(self):\n        \"\"\"Close persistent connections, clear sensitive data.\n        Not thread-safe, requires external synchronization against concurrent requests.\n        \"\"\"\n        existing, self.connections = self.connections, {}\n        for _, c in existing.items():\n            c.close()\n        self.certificates.clear()\n        self.clear_credentials()\n\n    def __getstate__(self):\n        state_dict = copy.copy(self.__dict__)\n        # In case request is augmented by some foreign object such as\n        # credentials which handle auth\n        if \"request\" in state_dict:\n            del state_dict[\"request\"]\n        if \"connections\" in state_dict:\n            del state_dict[\"connections\"]\n        return state_dict\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.connections = {}\n\n    def _auth_from_challenge(self, host, request_uri, headers, response, content):\n        \"\"\"A generator that creates Authorization objects\n           that can be applied to requests.\n        \"\"\"\n        challenges = _parse_www_authenticate(response, \"www-authenticate\")\n        for cred in self.credentials.iter(host):\n            for scheme in AUTH_SCHEME_ORDER:\n                if scheme in challenges:\n                    yield AUTH_SCHEME_CLASSES[scheme](\n                        cred, host, request_uri, headers, response, content, self\n                    )\n\n    def add_credentials(self, name, password, domain=\"\"):\n        \"\"\"Add a name and password that will be used\n        any time a request requires authentication.\"\"\"\n        self.credentials.add(name, password, domain)\n\n    def add_certificate(self, key, cert, domain, password=None):\n        \"\"\"Add a key and cert that will be used\n        any time a request requires authentication.\"\"\"\n        self.certificates.add(key, cert, domain, password)\n\n    def clear_credentials(self):\n        \"\"\"Remove all the names and passwords\n        that are used for authentication\"\"\"\n        self.credentials.clear()\n        self.authorizations = []\n\n    def _conn_request(self, conn, request_uri, method, body, headers):\n        i = 0\n        seen_bad_status_line = False\n        while i < RETRIES:\n            i += 1\n            try:\n                if conn.sock is None:\n                    conn.connect()\n                conn.request(method, request_uri, body, headers)\n            except socket.timeout:\n                conn.close()\n                raise\n            except socket.gaierror:\n                conn.close()\n                raise ServerNotFoundError(\"Unable to find the server at %s\" % conn.host)\n            except socket.error as e:\n                errno_ = (\n                    e.args[0].errno if isinstance(e.args[0], socket.error) else e.errno\n                )\n                if errno_ in (errno.ENETUNREACH, errno.EADDRNOTAVAIL) and i < RETRIES:\n                    continue  # retry on potentially transient errors\n                raise\n            except http.client.HTTPException:\n                if conn.sock is None:\n                    if i < RETRIES - 1:\n                        conn.close()\n                        conn.connect()\n                        continue\n                    else:\n                        conn.close()\n                        raise\n                if i < RETRIES - 1:\n                    conn.close()\n                    conn.connect()\n                    continue\n                # Just because the server closed the connection doesn't apparently mean\n                # that the server didn't send a response.\n                pass\n            try:\n                response = conn.getresponse()\n            except (http.client.BadStatusLine, http.client.ResponseNotReady):\n                # If we get a BadStatusLine on the first try then that means\n                # the connection just went stale, so retry regardless of the\n                # number of RETRIES set.\n                if not seen_bad_status_line and i == 1:\n                    i = 0\n                    seen_bad_status_line = True\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    conn.close()\n                    raise\n            except socket.timeout:\n                raise\n            except (socket.error, http.client.HTTPException):\n                conn.close()\n                if i == 0:\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    raise\n            else:\n                content = b\"\"\n                if method == \"HEAD\":\n                    conn.close()\n                else:\n                    content = response.read()\n                response = Response(response)\n                if method != \"HEAD\":\n                    content = _decompressContent(response, content)\n\n            break\n        return (response, content)\n\n    def _request(\n        self,\n        conn,\n        host,\n        absolute_uri,\n        request_uri,\n        method,\n        body,\n        headers,\n        redirections,\n        cachekey,\n    ):\n        \"\"\"Do the actual request using the connection object\n        and also follow one level of redirects if necessary\"\"\"\n\n        auths = [\n            (auth.depth(request_uri), auth)\n            for auth in self.authorizations\n            if auth.inscope(host, request_uri)\n        ]\n        auth = auths and sorted(auths)[0][1] or None\n        if auth:\n            auth.request(method, request_uri, headers, body)\n\n        (response, content) = self._conn_request(\n            conn, request_uri, method, body, headers\n        )\n\n        if auth:\n            if auth.response(response, body):\n                auth.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                response._stale_digest = 1\n\n        if response.status == 401:\n            for authorization in self._auth_from_challenge(\n                host, request_uri, headers, response, content\n            ):\n                authorization.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                if response.status != 401:\n                    self.authorizations.append(authorization)\n                    authorization.response(response, body)\n                    break\n\n        if (\n            self.follow_all_redirects\n            or method in self.safe_methods\n            or response.status in (303, 308)\n        ):\n            if self.follow_redirects and response.status in self.redirect_codes:\n                # Pick out the location header and basically start from the beginning\n                # remembering first to strip the ETag header and decrement our 'depth'\n                if redirections:\n                    if \"location\" not in response and response.status != 300:\n                        raise RedirectMissingLocation(\n                            _(\n                                \"Redirected but the response is missing a Location: header.\"\n                            ),\n                            response,\n                            content,\n                        )\n                    # Fix-up relative redirects (which violate an RFC 2616 MUST)\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        (scheme, authority, path, query, fragment) = parse_uri(location)\n                        if authority == None:\n                            response[\"location\"] = urllib.parse.urljoin(\n                                absolute_uri, location\n                            )\n                    if response.status == 308 or (response.status == 301 and (method in self.safe_methods)):\n                        response[\"-x-permanent-redirect-url\"] = response[\"location\"]\n                        if \"content-location\" not in response:\n                            response[\"content-location\"] = absolute_uri\n                        _updateCache(headers, response, content, self.cache, cachekey)\n                    if \"if-none-match\" in headers:\n                        del headers[\"if-none-match\"]\n                    if \"if-modified-since\" in headers:\n                        del headers[\"if-modified-since\"]\n                    if (\n                        \"authorization\" in headers\n                        and not self.forward_authorization_headers\n                    ):\n                        del headers[\"authorization\"]\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        old_response = copy.deepcopy(response)\n                        if \"content-location\" not in old_response:\n                            old_response[\"content-location\"] = absolute_uri\n                        redirect_method = method\n                        if response.status in [302, 303]:\n                            redirect_method = \"GET\"\n                            body = None\n                        (response, content) = self.request(\n                            location,\n                            method=redirect_method,\n                            body=body,\n                            headers=headers,\n                            redirections=redirections - 1,\n                        )\n                        response.previous = old_response\n                else:\n                    raise RedirectLimit(\n                        \"Redirected more times than redirection_limit allows.\",\n                        response,\n                        content,\n                    )\n            elif response.status in [200, 203] and method in self.safe_methods:\n                # Don't cache 206's since we aren't going to handle byte range requests\n                if \"content-location\" not in response:\n                    response[\"content-location\"] = absolute_uri\n                _updateCache(headers, response, content, self.cache, cachekey)\n\n        return (response, content)\n\n    def _normalize_headers(self, headers):\n        return _normalize_headers(headers)\n\n    # Need to catch and rebrand some exceptions\n    # Then need to optionally turn all exceptions into status codes\n    # including all socket.* and httplib.* exceptions.\n\n    def request(\n        self,\n        uri,\n        method=\"GET\",\n        body=None,\n        headers=None,\n        redirections=DEFAULT_MAX_REDIRECTS,\n        connection_type=None,\n    ):\n        \"\"\" Performs a single HTTP request.\nThe 'uri' is the URI of the HTTP resource and can begin\nwith either 'http' or 'https'. The value of 'uri' must be an absolute URI.\n\nThe 'method' is the HTTP method to perform, such as GET, POST, DELETE, etc.\nThere is no restriction on the methods allowed.\n\nThe 'body' is the entity body to be sent with the request. It is a string\nobject.\n\nAny extra headers that are to be sent with the request should be provided in the\n'headers' dictionary.\n\nThe maximum number of redirect to follow before raising an\nexception is 'redirections. The default is 5.\n\nThe return value is a tuple of (response, content), the first\nbeing and instance of the 'Response' class, the second being\na string that contains the response entity body.\n        \"\"\"\n        conn_key = ''\n\n        try:\n            if headers is None:\n                headers = {}\n            else:\n                headers = self._normalize_headers(headers)\n\n            if \"user-agent\" not in headers:\n                headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n\n            uri = iri2uri(uri)\n\n            (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n\n            conn_key = scheme + \":\" + authority\n            conn = self.connections.get(conn_key)\n            if conn is None:\n                if not connection_type:\n                    connection_type = SCHEME_TO_CONNECTION[scheme]\n                certs = list(self.certificates.iter(authority))\n                if issubclass(connection_type, HTTPSConnectionWithTimeout):\n                    if certs:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            key_file=certs[0][0],\n                            cert_file=certs[0][1],\n                            timeout=self.timeout,\n                            proxy_info=self.proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            tls_maximum_version=self.tls_maximum_version,\n                            tls_minimum_version=self.tls_minimum_version,\n                            key_password=certs[0][2],\n                        )\n                    else:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            timeout=self.timeout,\n                            proxy_info=self.proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            tls_maximum_version=self.tls_maximum_version,\n                            tls_minimum_version=self.tls_minimum_version,\n                        )\n                else:\n                    conn = self.connections[conn_key] = connection_type(\n                        authority, timeout=self.timeout, proxy_info=self.proxy_info\n                    )\n                conn.set_debuglevel(debuglevel)\n\n            if \"range\" not in headers and \"accept-encoding\" not in headers:\n                headers[\"accept-encoding\"] = \"gzip, deflate\"\n\n            info = email.message.Message()\n            cachekey = None\n            cached_value = None\n            if self.cache:\n                cachekey = defrag_uri\n                cached_value = self.cache.get(cachekey)\n                if cached_value:\n                    try:\n                        info, content = cached_value.split(b\"\\r\\n\\r\\n\", 1)\n                        info = email.message_from_bytes(info)\n                        for k, v in info.items():\n                            if v.startswith(\"=?\") and v.endswith(\"?=\"):\n                                info.replace_header(\n                                    k, str(*email.header.decode_header(v)[0])\n                                )\n                    except (IndexError, ValueError):\n                        self.cache.delete(cachekey)\n                        cachekey = None\n                        cached_value = None\n\n            if (\n                method in self.optimistic_concurrency_methods\n                and self.cache\n                and \"etag\" in info\n                and not self.ignore_etag\n                and \"if-match\" not in headers\n            ):\n                # http://www.w3.org/1999/04/Editing/\n                headers[\"if-match\"] = info[\"etag\"]\n\n            # https://tools.ietf.org/html/rfc7234\n            # A cache MUST invalidate the effective Request URI as well as [...] Location and Content-Location\n            # when a non-error status code is received in response to an unsafe request method.\n            if self.cache and cachekey and method not in self.safe_methods:\n                self.cache.delete(cachekey)\n\n            # Check the vary header in the cache to see if this request\n            # matches what varies in the cache.\n            if method in self.safe_methods and \"vary\" in info:\n                vary = info[\"vary\"]\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    value = info[key]\n                    if headers.get(header, None) != value:\n                        cached_value = None\n                        break\n\n            if (\n                self.cache\n                and cached_value\n                and (method in self.safe_methods or info[\"status\"] == \"308\")\n                and \"range\" not in headers\n            ):\n                redirect_method = method\n                if info[\"status\"] not in (\"307\", \"308\"):\n                    redirect_method = \"GET\"\n                if \"-x-permanent-redirect-url\" in info:\n                    # Should cached permanent redirects be counted in our redirection count? For now, yes.\n                    if redirections <= 0:\n                        raise RedirectLimit(\n                            \"Redirected more times than redirection_limit allows.\",\n                            {},\n                            \"\",\n                        )\n                    (response, new_content) = self.request(\n                        info[\"-x-permanent-redirect-url\"],\n                        method=redirect_method,\n                        headers=headers,\n                        redirections=redirections - 1,\n                    )\n                    response.previous = Response(info)\n                    response.previous.fromcache = True\n                else:\n                    # Determine our course of action:\n                    #   Is the cached entry fresh or stale?\n                    #   Has the client requested a non-cached response?\n                    #\n                    # There seems to be three possible answers:\n                    # 1. [FRESH] Return the cache entry w/o doing a GET\n                    # 2. [STALE] Do the GET (but add in cache validators if available)\n                    # 3. [TRANSPARENT] Do a GET w/o any cache validators (Cache-Control: no-cache) on the request\n                    entry_disposition = _entry_disposition(info, headers)\n\n                    if entry_disposition == \"FRESH\":\n                        if not cached_value:\n                            info[\"status\"] = \"504\"\n                            content = b\"\"\n                        response = Response(info)\n                        if cached_value:\n                            response.fromcache = True\n                        return (response, content)\n\n                    if entry_disposition == \"STALE\":\n                        if (\n                            \"etag\" in info\n                            and not self.ignore_etag\n                            and not \"if-none-match\" in headers\n                        ):\n                            headers[\"if-none-match\"] = info[\"etag\"]\n                        if \"last-modified\" in info and not \"last-modified\" in headers:\n                            headers[\"if-modified-since\"] = info[\"last-modified\"]\n                    elif entry_disposition == \"TRANSPARENT\":\n                        pass\n\n                    (response, new_content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n\n                if response.status == 304 and method == \"GET\":\n                    # Rewrite the cache entry with the new end-to-end headers\n                    # Take all headers that are in response\n                    # and overwrite their values in info.\n                    # unless they are hop-by-hop, or are listed in the connection header.\n\n                    for key in _get_end2end_headers(response):\n                        info[key] = response[key]\n                    merged_response = Response(info)\n                    if hasattr(response, \"_stale_digest\"):\n                        merged_response._stale_digest = response._stale_digest\n                    _updateCache(\n                        headers, merged_response, content, self.cache, cachekey\n                    )\n                    response = merged_response\n                    response.status = 200\n                    response.fromcache = True\n\n                elif response.status == 200:\n                    content = new_content\n                else:\n                    self.cache.delete(cachekey)\n                    content = new_content\n            else:\n                cc = _parse_cache_control(headers)\n                if \"only-if-cached\" in cc:\n                    info[\"status\"] = \"504\"\n                    response = Response(info)\n                    content = b\"\"\n                else:\n                    (response, content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n        except Exception as e:\n            is_timeout = isinstance(e, socket.timeout)\n            if is_timeout:\n                conn = self.connections.pop(conn_key, None)\n                if conn:\n                    conn.close()\n\n            if self.force_exception_to_status_code:\n                if isinstance(e, HttpLib2ErrorWithResponse):\n                    response = e.response\n                    content = e.content\n                    response.status = 500\n                    response.reason = str(e)\n                elif isinstance(e, socket.timeout):\n                    content = b\"Request Timeout\"\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"408\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Request Timeout\"\n                else:\n                    content = str(e).encode(\"utf-8\")\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"400\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Bad Request\"\n            else:\n                raise\n\n        return (response, content)\n\n\nclass Response(dict):\n    \"\"\"An object more like email.message than httplib.HTTPResponse.\"\"\"\n\n    \"\"\"Is this response from our local cache\"\"\"\n    fromcache = False\n    \"\"\"HTTP protocol version used by server.\n\n    10 for HTTP/1.0, 11 for HTTP/1.1.\n    \"\"\"\n    version = 11\n\n    \"Status code returned by server. \"\n    status = 200\n    \"\"\"Reason phrase returned by server.\"\"\"\n    reason = \"Ok\"\n\n    previous = None\n\n    def __init__(self, info):\n        # info is either an email.message or\n        # an httplib.HTTPResponse object.\n        if isinstance(info, http.client.HTTPResponse):\n            for key, value in info.getheaders():\n                key = key.lower()\n                prev = self.get(key)\n                if prev is not None:\n                    value = \", \".join((prev, value))\n                self[key] = value\n            self.status = info.status\n            self[\"status\"] = str(self.status)\n            self.reason = info.reason\n            self.version = info.version\n        elif isinstance(info, email.message.Message):\n            for key, value in list(info.items()):\n                self[key.lower()] = value\n            self.status = int(self[\"status\"])\n        else:\n            for key, value in info.items():\n                self[key.lower()] = value\n            self.status = int(self.get(\"status\", self.status))\n\n    def __getattr__(self, name):\n        if name == \"dict\":\n            return self\n        else:\n            raise AttributeError(name)\n", "patch": "@@ -1790,6 +1790,9 @@ def request(\n                 headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n \n             uri = iri2uri(uri)\n+            # Prevent CWE-75 space injection to manipulate request via part of uri.\n+            # Prevent CWE-93 CRLF injection to modify headers via part of uri.\n+            uri = uri.replace(\" \", \"%20\").replace(\"\\r\", \"%0D\").replace(\"\\n\", \"%0A\")\n \n             (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n ", "file_path": "files/2020_8/326", "file_language": "py", "file_name": "python3/httplib2/__init__.py", "outdated_file_modify": 1, "outdated_file_before": 1, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/httplib2/httplib2/raw/a1457cc31f3206cf691d11d2bf34e98865873e9e/tests%2F__init__.py", "code": "from __future__ import print_function\n\nimport base64\nimport contextlib\nimport copy\nimport email.utils\nimport functools\nimport gzip\nimport hashlib\nimport httplib2\nimport os\nimport random\nimport re\nimport shutil\nimport six\nimport socket\nimport ssl\nimport struct\nimport sys\nimport threading\nimport time\nimport traceback\nimport zlib\nfrom six.moves import http_client, queue\n\n\nDUMMY_URL = \"http://127.0.0.1:1\"\nDUMMY_HTTPS_URL = \"https://127.0.0.1:2\"\n\ntls_dir = os.path.join(os.path.dirname(__file__), \"tls\")\nCA_CERTS = os.path.join(tls_dir, \"ca.pem\")\nCA_UNUSED_CERTS = os.path.join(tls_dir, \"ca_unused.pem\")\nCLIENT_PEM = os.path.join(tls_dir, \"client.pem\")\nCLIENT_ENCRYPTED_PEM = os.path.join(tls_dir, \"client_encrypted.pem\")\nSERVER_PEM = os.path.join(tls_dir, \"server.pem\")\nSERVER_CHAIN = os.path.join(tls_dir, \"server_chain.pem\")\n\n\n@contextlib.contextmanager\ndef assert_raises(exc_type):\n    def _name(t):\n        return getattr(t, \"__name__\", None) or str(t)\n\n    if not isinstance(exc_type, tuple):\n        exc_type = (exc_type,)\n    names = \", \".join(map(_name, exc_type))\n\n    try:\n        yield\n    except exc_type:\n        pass\n    else:\n        assert False, \"Expected exception(s) {0}\".format(names)\n\n\nclass BufferedReader(object):\n    \"\"\"io.BufferedReader with \\r\\n support\n    \"\"\"\n\n    def __init__(self, sock):\n        self._buf = b\"\"\n        self._end = False\n        self._newline = b\"\\r\\n\"\n        self._sock = sock\n        if isinstance(sock, bytes):\n            self._sock = None\n            self._buf = sock\n\n    def _fill(self, target=1, more=None, untilend=False):\n        if more:\n            target = len(self._buf) + more\n        while untilend or (len(self._buf) < target):\n            # crutch to enable HttpRequest.from_bytes\n            if self._sock is None:\n                chunk = b\"\"\n            else:\n                chunk = self._sock.recv(8 << 10)\n            # print(\"!!! recv\", chunk)\n            if not chunk:\n                self._end = True\n                if untilend:\n                    return\n                else:\n                    raise EOFError\n            self._buf += chunk\n\n    def peek(self, size):\n        self._fill(target=size)\n        return self._buf[:size]\n\n    def read(self, size):\n        self._fill(target=size)\n        chunk, self._buf = self._buf[:size], self._buf[size:]\n        return chunk\n\n    def readall(self):\n        self._fill(untilend=True)\n        chunk, self._buf = self._buf, b\"\"\n        return chunk\n\n    def readline(self):\n        while True:\n            i = self._buf.find(self._newline)\n            if i >= 0:\n                break\n            self._fill(more=1)\n        inext = i + len(self._newline)\n        line, self._buf = self._buf[:inext], self._buf[inext:]\n        return line\n\n\ndef parse_http_message(kind, buf):\n    if buf._end:\n        return None\n    try:\n        start_line = buf.readline()\n    except EOFError:\n        return None\n    msg = kind()\n    msg.raw = start_line\n    if kind is HttpRequest:\n        assert re.match(\n            br\".+ HTTP/\\d\\.\\d\\r\\n$\", start_line\n        ), \"Start line does not look like HTTP request: \" + repr(start_line)\n        msg.method, msg.uri, msg.proto = start_line.rstrip().decode().split(\" \", 2)\n        assert msg.proto.startswith(\"HTTP/\"), repr(start_line)\n    elif kind is HttpResponse:\n        assert re.match(\n            br\"^HTTP/\\d\\.\\d \\d+ .+\\r\\n$\", start_line\n        ), \"Start line does not look like HTTP response: \" + repr(start_line)\n        msg.proto, msg.status, msg.reason = start_line.rstrip().decode().split(\" \", 2)\n        msg.status = int(msg.status)\n        assert msg.proto.startswith(\"HTTP/\"), repr(start_line)\n    else:\n        raise Exception(\"Use HttpRequest or HttpResponse .from_{bytes,buffered}\")\n    msg.version = msg.proto[5:]\n\n    while True:\n        line = buf.readline()\n        msg.raw += line\n        line = line.rstrip()\n        if not line:\n            break\n        t = line.decode().split(\":\", 1)\n        msg.headers[t[0].lower()] = t[1].lstrip()\n\n    content_length_string = msg.headers.get(\"content-length\", \"\")\n    if content_length_string.isdigit():\n        content_length = int(content_length_string)\n        msg.body = msg.body_raw = buf.read(content_length)\n    elif msg.headers.get(\"transfer-encoding\") == \"chunked\":\n        raise NotImplemented\n    elif msg.version == \"1.0\":\n        msg.body = msg.body_raw = buf.readall()\n    else:\n        msg.body = msg.body_raw = b\"\"\n\n    msg.raw += msg.body_raw\n    return msg\n\n\nclass HttpMessage(object):\n    def __init__(self):\n        self.headers = {}\n\n    @classmethod\n    def from_bytes(cls, bs):\n        buf = BufferedReader(bs)\n        return parse_http_message(cls, buf)\n\n    @classmethod\n    def from_buffered(cls, buf):\n        return parse_http_message(cls, buf)\n\n    def __repr__(self):\n        return \"{} {}\".format(self.__class__, repr(vars(self)))\n\n\nclass HttpRequest(HttpMessage):\n    pass\n\n\nclass HttpResponse(HttpMessage):\n    pass\n\n\nclass MockResponse(six.BytesIO):\n    def __init__(self, body, **kwargs):\n        six.BytesIO.__init__(self, body)\n        self.headers = kwargs\n\n    def items(self):\n        return self.headers.items()\n\n    def iteritems(self):\n        return six.iteritems(self.headers)\n\n\nclass MockHTTPConnection(object):\n    \"\"\"This class is just a mock of httplib.HTTPConnection used for testing\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        strict=None,\n        timeout=None,\n        proxy_info=None,\n    ):\n        self.host = host\n        self.port = port\n        self.timeout = timeout\n        self.log = \"\"\n        self.sock = None\n\n    def set_debuglevel(self, level):\n        pass\n\n    def connect(self):\n        \"Connect to a host on a given port.\"\n        pass\n\n    def close(self):\n        pass\n\n    def request(self, method, request_uri, body, headers):\n        pass\n\n    def getresponse(self):\n        return MockResponse(b\"the body\", status=\"200\")\n\n\nclass MockHTTPBadStatusConnection(object):\n    \"\"\"Mock of httplib.HTTPConnection that raises BadStatusLine.\n    \"\"\"\n\n    num_calls = 0\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        strict=None,\n        timeout=None,\n        proxy_info=None,\n    ):\n        self.host = host\n        self.port = port\n        self.timeout = timeout\n        self.log = \"\"\n        self.sock = None\n        MockHTTPBadStatusConnection.num_calls = 0\n\n    def set_debuglevel(self, level):\n        pass\n\n    def connect(self):\n        pass\n\n    def close(self):\n        pass\n\n    def request(self, method, request_uri, body, headers):\n        pass\n\n    def getresponse(self):\n        MockHTTPBadStatusConnection.num_calls += 1\n        raise http_client.BadStatusLine(\"\")\n\n\n@contextlib.contextmanager\ndef server_socket(fun, request_count=1, timeout=5, scheme=\"\", tls=None):\n    \"\"\"Base socket server for tests.\n    Likely you want to use server_request or other higher level helpers.\n    All arguments except fun can be passed to other server_* helpers.\n\n    :param fun: fun(client_sock, tick) called after successful accept().\n    :param request_count: test succeeds after exactly this number of requests, triggered by tick(request)\n    :param timeout: seconds.\n    :param scheme: affects yielded value\n        \"\" - build normal http/https URI.\n        string - build normal URI using supplied scheme.\n        None - yield (addr, port) tuple.\n    :param tls:\n        None (default) - plain HTTP.\n        True - HTTPS with reasonable defaults. Likely you want httplib2.Http(ca_certs=tests.CA_CERTS)\n        string - path to custom server cert+key PEM file.\n        callable - function(context, listener, skip_errors) -> ssl_wrapped_listener\n    \"\"\"\n    gresult = [None]\n    gcounter = [0]\n    tls_skip_errors = [\n        \"TLSV1_ALERT_UNKNOWN_CA\",\n    ]\n\n    def tick(request):\n        gcounter[0] += 1\n        keep = True\n        keep &= gcounter[0] < request_count\n        if request is not None:\n            keep &= request.headers.get(\"connection\", \"\").lower() != \"close\"\n        return keep\n\n    def server_socket_thread(srv):\n        try:\n            while gcounter[0] < request_count:\n                try:\n                    client, _ = srv.accept()\n                except ssl.SSLError as e:\n                    if e.reason in tls_skip_errors:\n                        return\n                    raise\n\n                try:\n                    client.settimeout(timeout)\n                    fun(client, tick)\n                finally:\n                    try:\n                        client.shutdown(socket.SHUT_RDWR)\n                    except (IOError, socket.error):\n                        pass\n                    # FIXME: client.close() introduces connection reset by peer\n                    # at least in other/connection_close test\n                    # should not be a problem since socket would close upon garbage collection\n            if gcounter[0] > request_count:\n                gresult[0] = Exception(\n                    \"Request count expected={0} actual={1}\".format(\n                        request_count, gcounter[0]\n                    )\n                )\n        except Exception as e:\n            # traceback.print_exc caused IOError: concurrent operation on sys.stderr.close() under setup.py test\n            print(traceback.format_exc(), file=sys.stderr)\n            gresult[0] = e\n\n    bind_hostname = \"localhost\"\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.bind((bind_hostname, 0))\n    try:\n        server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    except socket.error as ex:\n        print(\"non critical error on SO_REUSEADDR\", ex)\n    server.listen(10)\n    server.settimeout(timeout)\n    server_port = server.getsockname()[1]\n    if tls is True:\n        tls = SERVER_CHAIN\n    if tls:\n        context = ssl_context()\n        if callable(tls):\n            context.load_cert_chain(SERVER_CHAIN)\n            server = tls(context, server, tls_skip_errors)\n        else:\n            context.load_cert_chain(tls)\n            server = context.wrap_socket(server, server_side=True)\n    if scheme == \"\":\n        scheme = \"https\" if tls else \"http\"\n\n    t = threading.Thread(target=server_socket_thread, args=(server,))\n    t.daemon = True\n    t.start()\n    if scheme is None:\n        yield (bind_hostname, server_port)\n    else:\n        yield u\"{scheme}://{host}:{port}/\".format(scheme=scheme, host=bind_hostname, port=server_port)\n    server.close()\n    t.join()\n    if gresult[0] is not None:\n        raise gresult[0]\n\n\ndef server_yield(fun, **kwargs):\n    q = queue.Queue(1)\n    g = fun(q.get)\n\n    def server_yield_socket_handler(sock, tick):\n        buf = BufferedReader(sock)\n        i = 0\n        while True:\n            request = HttpRequest.from_buffered(buf)\n            if request is None:\n                break\n            i += 1\n            request.client_sock = sock\n            request.number = i\n            q.put(request)\n            response = six.next(g)\n            sock.sendall(response)\n            request.client_sock = None\n            if not tick(request):\n                break\n\n    return server_socket(server_yield_socket_handler, **kwargs)\n\n\ndef server_request(request_handler, **kwargs):\n    def server_request_socket_handler(sock, tick):\n        buf = BufferedReader(sock)\n        i = 0\n        while True:\n            request = HttpRequest.from_buffered(buf)\n            if request is None:\n                break\n            # print(\"--- debug request\\n\" + request.raw.decode(\"ascii\", \"replace\"))\n            i += 1\n            request.client_sock = sock\n            request.number = i\n            response = request_handler(request=request)\n            # print(\"--- debug response\\n\" + response.decode(\"ascii\", \"replace\"))\n            sock.sendall(response)\n            request.client_sock = None\n            if not tick(request):\n                break\n\n    return server_socket(server_request_socket_handler, **kwargs)\n\n\ndef server_const_bytes(response_content, **kwargs):\n    return server_request(lambda request: response_content, **kwargs)\n\n\n_http_kwargs = (\n    \"proto\",\n    \"status\",\n    \"headers\",\n    \"body\",\n    \"add_content_length\",\n    \"add_date\",\n    \"add_etag\",\n    \"undefined_body_length\",\n)\n\n\ndef http_response_bytes(\n    proto=\"HTTP/1.1\",\n    status=\"200 OK\",\n    headers=None,\n    body=b\"\",\n    add_content_length=True,\n    add_date=False,\n    add_etag=False,\n    undefined_body_length=False,\n    **kwargs\n):\n    if undefined_body_length:\n        add_content_length = False\n    if headers is None:\n        headers = {}\n    if add_content_length:\n        headers.setdefault(\"content-length\", str(len(body)))\n    if add_date:\n        headers.setdefault(\"date\", email.utils.formatdate())\n    if add_etag:\n        headers.setdefault(\"etag\", '\"{0}\"'.format(hashlib.md5(body).hexdigest()))\n    header_string = \"\".join(\"{0}: {1}\\r\\n\".format(k, v) for k, v in headers.items())\n    if (\n        not undefined_body_length\n        and proto != \"HTTP/1.0\"\n        and \"content-length\" not in headers\n    ):\n        raise Exception(\n            \"httplib2.tests.http_response_bytes: client could not figure response body length\"\n        )\n    if str(status).isdigit():\n        status = \"{} {}\".format(status, http_client.responses[status])\n    response = (\n        \"{proto} {status}\\r\\n{headers}\\r\\n\".format(\n            proto=proto, status=status, headers=header_string\n        ).encode()\n        + body\n    )\n    return response\n\n\ndef make_http_reflect(**kwargs):\n    assert \"body\" not in kwargs, \"make_http_reflect will overwrite response \" \"body\"\n\n    def fun(request):\n        kw = copy.deepcopy(kwargs)\n        kw[\"body\"] = request.raw\n        response = http_response_bytes(**kw)\n        return response\n\n    return fun\n\n\ndef server_route(routes, **kwargs):\n    response_404 = http_response_bytes(status=\"404 Not Found\")\n    response_wildcard = routes.get(\"\")\n\n    def handler(request):\n        target = routes.get(request.uri, response_wildcard) or response_404\n        if callable(target):\n            response = target(request=request)\n        else:\n            response = target\n        return response\n\n    return server_request(handler, **kwargs)\n\n\ndef server_const_http(**kwargs):\n    response_kwargs = {k: kwargs.pop(k) for k in dict(kwargs) if k in _http_kwargs}\n    response = http_response_bytes(**response_kwargs)\n    return server_const_bytes(response, **kwargs)\n\n\ndef server_list_http(responses, **kwargs):\n    i = iter(responses)\n\n    def handler(request):\n        return next(i)\n\n    kwargs.setdefault(\"request_count\", len(responses))\n    return server_request(handler, **kwargs)\n\n\ndef server_reflect(**kwargs):\n    response_kwargs = {k: kwargs.pop(k) for k in dict(kwargs) if k in _http_kwargs}\n    http_handler = make_http_reflect(**response_kwargs)\n    return server_request(http_handler, **kwargs)\n\n\ndef http_parse_auth(s):\n    \"\"\"https://tools.ietf.org/html/rfc7235#section-2.1\n    \"\"\"\n    scheme, rest = s.split(\" \", 1)\n    result = {}\n    while True:\n        m = httplib2.WWW_AUTH_RELAXED.search(rest)\n        if not m:\n            break\n        if len(m.groups()) == 3:\n            key, value, rest = m.groups()\n            result[key.lower()] = httplib2.UNQUOTE_PAIRS.sub(r\"\\1\", value)\n    return result\n\n\ndef store_request_response(out):\n    def wrapper(fun):\n        @functools.wraps(fun)\n        def wrapped(request, *a, **kw):\n            response_bytes = fun(request, *a, **kw)\n            if out is not None:\n                response = HttpResponse.from_bytes(response_bytes)\n                out.append((request, response))\n            return response_bytes\n\n        return wrapped\n\n    return wrapper\n\n\ndef http_reflect_with_auth(\n    allow_scheme, allow_credentials, out_renew_nonce=None, out_requests=None\n):\n    \"\"\"allow_scheme - 'basic', 'digest', etc allow_credentials - sequence of ('name', 'password') out_renew_nonce - None | [function]\n\n        Way to return nonce renew function to caller.\n        Kind of `out` parameter in some programming languages.\n        Allows to keep same signature for all handler builder functions.\n    out_requests - None | []\n        If set to list, every parsed request will be appended here.\n    \"\"\"\n    glastnc = [None]\n    gnextnonce = [None]\n    gserver_nonce = [gen_digest_nonce(salt=b\"n\")]\n    realm = \"httplib2 test\"\n    server_opaque = gen_digest_nonce(salt=b\"o\")\n\n    def renew_nonce():\n        if gnextnonce[0]:\n            assert False, (\n                \"previous nextnonce was not used, probably bug in \" \"test code\"\n            )\n        gnextnonce[0] = gen_digest_nonce()\n        return gserver_nonce[0], gnextnonce[0]\n\n    if out_renew_nonce:\n        out_renew_nonce[0] = renew_nonce\n\n    def deny(**kwargs):\n        nonce_stale = kwargs.pop(\"nonce_stale\", False)\n        if nonce_stale:\n            kwargs.setdefault(\"body\", b\"nonce stale\")\n        if allow_scheme == \"basic\":\n            authenticate = 'basic realm=\"{realm}\"'.format(realm=realm)\n        elif allow_scheme == \"digest\":\n            authenticate = (\n                'digest realm=\"{realm}\", qop=\"auth\"'\n                + ', nonce=\"{nonce}\", opaque=\"{opaque}\"'\n                + (\", stale=true\" if nonce_stale else \"\")\n            ).format(realm=realm, nonce=gserver_nonce[0], opaque=server_opaque)\n        else:\n            raise Exception(\"unknown allow_scheme={0}\".format(allow_scheme))\n        deny_headers = {\"www-authenticate\": authenticate}\n        kwargs.setdefault(\"status\", 401)\n        # supplied headers may overwrite generated ones\n        deny_headers.update(kwargs.get(\"headers\", {}))\n        kwargs[\"headers\"] = deny_headers\n        kwargs.setdefault(\"body\", b\"HTTP authorization required\")\n        return http_response_bytes(**kwargs)\n\n    @store_request_response(out_requests)\n    def http_reflect_with_auth_handler(request):\n        auth_header = request.headers.get(\"authorization\", \"\")\n        if not auth_header:\n            return deny()\n        if \" \" not in auth_header:\n            return http_response_bytes(\n                status=400, body=b\"authorization header syntax error\"\n            )\n        scheme, data = auth_header.split(\" \", 1)\n        scheme = scheme.lower()\n        if scheme != allow_scheme:\n            return deny(body=b\"must use different auth scheme\")\n        if scheme == \"basic\":\n            decoded = base64.b64decode(data).decode()\n            username, password = decoded.split(\":\", 1)\n            if (username, password) in allow_credentials:\n                return make_http_reflect()(request)\n            else:\n                return deny(body=b\"supplied credentials are not allowed\")\n        elif scheme == \"digest\":\n            server_nonce_old = gserver_nonce[0]\n            nextnonce = gnextnonce[0]\n            if nextnonce:\n                # server decided to change nonce, in this case, guided by caller test code\n                gserver_nonce[0] = nextnonce\n                gnextnonce[0] = None\n            server_nonce_current = gserver_nonce[0]\n            auth_info = http_parse_auth(data)\n            client_cnonce = auth_info.get(\"cnonce\", \"\")\n            client_nc = auth_info.get(\"nc\", \"\")\n            client_nonce = auth_info.get(\"nonce\", \"\")\n            client_opaque = auth_info.get(\"opaque\", \"\")\n            client_qop = auth_info.get(\"qop\", \"auth\").strip('\"')\n\n            # TODO: auth_info.get('algorithm', 'md5')\n            hasher = hashlib.md5\n\n            # TODO: client_qop auth-int\n            ha2 = hasher(\":\".join((request.method, request.uri)).encode()).hexdigest()\n\n            if client_nonce != server_nonce_current:\n                if client_nonce == server_nonce_old:\n                    return deny(nonce_stale=True)\n                return deny(body=b\"invalid nonce\")\n            if not client_nc:\n                return deny(body=b\"auth-info nc missing\")\n            if client_opaque != server_opaque:\n                return deny(\n                    body=\"auth-info opaque mismatch expected={} actual={}\".format(\n                        server_opaque, client_opaque\n                    ).encode()\n                )\n            for allow_username, allow_password in allow_credentials:\n                ha1 = hasher(\n                    \":\".join((allow_username, realm, allow_password)).encode()\n                ).hexdigest()\n                allow_response = hasher(\n                    \":\".join(\n                        (ha1, client_nonce, client_nc, client_cnonce, client_qop, ha2)\n                    ).encode()\n                ).hexdigest()\n                rspauth_ha2 = hasher(\":{}\".format(request.uri).encode()).hexdigest()\n                rspauth = hasher(\n                    \":\".join(\n                        (\n                            ha1,\n                            client_nonce,\n                            client_nc,\n                            client_cnonce,\n                            client_qop,\n                            rspauth_ha2,\n                        )\n                    ).encode()\n                ).hexdigest()\n                if auth_info.get(\"response\", \"\") == allow_response:\n                    # TODO: fix or remove doubtful comment\n                    # do we need to save nc only on success?\n                    glastnc[0] = client_nc\n                    allow_headers = {\n                        \"authentication-info\": \" \".join(\n                            (\n                                'nextnonce=\"{}\"'.format(nextnonce) if nextnonce else \"\",\n                                \"qop={}\".format(client_qop),\n                                'rspauth=\"{}\"'.format(rspauth),\n                                'cnonce=\"{}\"'.format(client_cnonce),\n                                \"nc={}\".format(client_nc),\n                            )\n                        ).strip()\n                    }\n                    return make_http_reflect(headers=allow_headers)(request)\n            return deny(body=b\"supplied credentials are not allowed\")\n        else:\n            return http_response_bytes(\n                status=400,\n                body=\"unknown authorization scheme={0}\".format(scheme).encode(),\n            )\n\n    return http_reflect_with_auth_handler\n\n\ndef get_cache_path():\n    default = \"./_httplib2_test_cache\"\n    path = os.environ.get(\"httplib2_test_cache_path\") or default\n    if os.path.exists(path):\n        shutil.rmtree(path)\n    return path\n\n\ndef gen_digest_nonce(salt=b\"\"):\n    t = struct.pack(\">Q\", int(time.time() * 1e9))\n    return base64.b64encode(t + b\":\" + hashlib.sha1(t + salt).digest()).decode()\n\n\ndef gen_password():\n    length = random.randint(8, 64)\n    return \"\".join(six.unichr(random.randint(0, 127)) for _ in range(length))\n\n\ndef gzip_compress(bs):\n    # gzipobj = zlib.compressobj(9, zlib.DEFLATED, zlib.MAX_WBITS | 16)\n    # result = gzipobj.compress(text) + gzipobj.flush()\n    buf = six.BytesIO()\n    gf = gzip.GzipFile(fileobj=buf, mode=\"wb\", compresslevel=6)\n    gf.write(bs)\n    gf.close()\n    return buf.getvalue()\n\n\ndef gzip_decompress(bs):\n    return zlib.decompress(bs, zlib.MAX_WBITS | 16)\n\n\ndef deflate_compress(bs):\n    do = zlib.compressobj(9, zlib.DEFLATED, -zlib.MAX_WBITS)\n    return do.compress(bs) + do.flush()\n\n\ndef deflate_decompress(bs):\n    return zlib.decompress(bs, -zlib.MAX_WBITS)\n\n\ndef ssl_context(protocol=None):\n    \"\"\"Workaround for old SSLContext() required protocol argument.\n    \"\"\"\n    if sys.version_info < (3, 5, 3):\n        return ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n    return ssl.SSLContext()\n", "code_before": "from __future__ import print_function\n\nimport base64\nimport contextlib\nimport copy\nimport email.utils\nimport functools\nimport gzip\nimport hashlib\nimport httplib2\nimport os\nimport random\nimport re\nimport shutil\nimport six\nimport socket\nimport ssl\nimport struct\nimport sys\nimport threading\nimport time\nimport traceback\nimport zlib\nfrom six.moves import http_client, queue\n\n\nDUMMY_URL = \"http://127.0.0.1:1\"\nDUMMY_HTTPS_URL = \"https://127.0.0.1:2\"\n\ntls_dir = os.path.join(os.path.dirname(__file__), \"tls\")\nCA_CERTS = os.path.join(tls_dir, \"ca.pem\")\nCA_UNUSED_CERTS = os.path.join(tls_dir, \"ca_unused.pem\")\nCLIENT_PEM = os.path.join(tls_dir, \"client.pem\")\nCLIENT_ENCRYPTED_PEM = os.path.join(tls_dir, \"client_encrypted.pem\")\nSERVER_PEM = os.path.join(tls_dir, \"server.pem\")\nSERVER_CHAIN = os.path.join(tls_dir, \"server_chain.pem\")\n\n\n@contextlib.contextmanager\ndef assert_raises(exc_type):\n    def _name(t):\n        return getattr(t, \"__name__\", None) or str(t)\n\n    if not isinstance(exc_type, tuple):\n        exc_type = (exc_type,)\n    names = \", \".join(map(_name, exc_type))\n\n    try:\n        yield\n    except exc_type:\n        pass\n    else:\n        assert False, \"Expected exception(s) {0}\".format(names)\n\n\nclass BufferedReader(object):\n    \"\"\"io.BufferedReader with \\r\\n support\n    \"\"\"\n\n    def __init__(self, sock):\n        self._buf = b\"\"\n        self._end = False\n        self._newline = b\"\\r\\n\"\n        self._sock = sock\n        if isinstance(sock, bytes):\n            self._sock = None\n            self._buf = sock\n\n    def _fill(self, target=1, more=None, untilend=False):\n        if more:\n            target = len(self._buf) + more\n        while untilend or (len(self._buf) < target):\n            # crutch to enable HttpRequest.from_bytes\n            if self._sock is None:\n                chunk = b\"\"\n            else:\n                chunk = self._sock.recv(8 << 10)\n            # print('!!! recv', chunk)\n            if not chunk:\n                self._end = True\n                if untilend:\n                    return\n                else:\n                    raise EOFError\n            self._buf += chunk\n\n    def peek(self, size):\n        self._fill(target=size)\n        return self._buf[:size]\n\n    def read(self, size):\n        self._fill(target=size)\n        chunk, self._buf = self._buf[:size], self._buf[size:]\n        return chunk\n\n    def readall(self):\n        self._fill(untilend=True)\n        chunk, self._buf = self._buf, b\"\"\n        return chunk\n\n    def readline(self):\n        while True:\n            i = self._buf.find(self._newline)\n            if i >= 0:\n                break\n            self._fill(more=1)\n        inext = i + len(self._newline)\n        line, self._buf = self._buf[:inext], self._buf[inext:]\n        return line\n\n\ndef parse_http_message(kind, buf):\n    if buf._end:\n        return None\n    try:\n        start_line = buf.readline()\n    except EOFError:\n        return None\n    msg = kind()\n    msg.raw = start_line\n    if kind is HttpRequest:\n        assert re.match(\n            br\".+ HTTP/\\d\\.\\d\\r\\n$\", start_line\n        ), \"Start line does not look like HTTP request: \" + repr(start_line)\n        msg.method, msg.uri, msg.proto = start_line.rstrip().decode().split(\" \", 2)\n        assert msg.proto.startswith(\"HTTP/\"), repr(start_line)\n    elif kind is HttpResponse:\n        assert re.match(\n            br\"^HTTP/\\d\\.\\d \\d+ .+\\r\\n$\", start_line\n        ), \"Start line does not look like HTTP response: \" + repr(start_line)\n        msg.proto, msg.status, msg.reason = start_line.rstrip().decode().split(\" \", 2)\n        msg.status = int(msg.status)\n        assert msg.proto.startswith(\"HTTP/\"), repr(start_line)\n    else:\n        raise Exception(\"Use HttpRequest or HttpResponse .from_{bytes,buffered}\")\n    msg.version = msg.proto[5:]\n\n    while True:\n        line = buf.readline()\n        msg.raw += line\n        line = line.rstrip()\n        if not line:\n            break\n        t = line.decode().split(\":\", 1)\n        msg.headers[t[0].lower()] = t[1].lstrip()\n\n    content_length_string = msg.headers.get(\"content-length\", \"\")\n    if content_length_string.isdigit():\n        content_length = int(content_length_string)\n        msg.body = msg.body_raw = buf.read(content_length)\n    elif msg.headers.get(\"transfer-encoding\") == \"chunked\":\n        raise NotImplemented\n    elif msg.version == \"1.0\":\n        msg.body = msg.body_raw = buf.readall()\n    else:\n        msg.body = msg.body_raw = b\"\"\n\n    msg.raw += msg.body_raw\n    return msg\n\n\nclass HttpMessage(object):\n    def __init__(self):\n        self.headers = {}\n\n    @classmethod\n    def from_bytes(cls, bs):\n        buf = BufferedReader(bs)\n        return parse_http_message(cls, buf)\n\n    @classmethod\n    def from_buffered(cls, buf):\n        return parse_http_message(cls, buf)\n\n    def __repr__(self):\n        return \"{} {}\".format(self.__class__, repr(vars(self)))\n\n\nclass HttpRequest(HttpMessage):\n    pass\n\n\nclass HttpResponse(HttpMessage):\n    pass\n\n\nclass MockResponse(six.BytesIO):\n    def __init__(self, body, **kwargs):\n        six.BytesIO.__init__(self, body)\n        self.headers = kwargs\n\n    def items(self):\n        return self.headers.items()\n\n    def iteritems(self):\n        return six.iteritems(self.headers)\n\n\nclass MockHTTPConnection(object):\n    \"\"\"This class is just a mock of httplib.HTTPConnection used for testing\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        strict=None,\n        timeout=None,\n        proxy_info=None,\n    ):\n        self.host = host\n        self.port = port\n        self.timeout = timeout\n        self.log = \"\"\n        self.sock = None\n\n    def set_debuglevel(self, level):\n        pass\n\n    def connect(self):\n        \"Connect to a host on a given port.\"\n        pass\n\n    def close(self):\n        pass\n\n    def request(self, method, request_uri, body, headers):\n        pass\n\n    def getresponse(self):\n        return MockResponse(b\"the body\", status=\"200\")\n\n\nclass MockHTTPBadStatusConnection(object):\n    \"\"\"Mock of httplib.HTTPConnection that raises BadStatusLine.\n    \"\"\"\n\n    num_calls = 0\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        strict=None,\n        timeout=None,\n        proxy_info=None,\n    ):\n        self.host = host\n        self.port = port\n        self.timeout = timeout\n        self.log = \"\"\n        self.sock = None\n        MockHTTPBadStatusConnection.num_calls = 0\n\n    def set_debuglevel(self, level):\n        pass\n\n    def connect(self):\n        pass\n\n    def close(self):\n        pass\n\n    def request(self, method, request_uri, body, headers):\n        pass\n\n    def getresponse(self):\n        MockHTTPBadStatusConnection.num_calls += 1\n        raise http_client.BadStatusLine(\"\")\n\n\n@contextlib.contextmanager\ndef server_socket(fun, request_count=1, timeout=5, scheme=\"\", tls=None):\n    \"\"\"Base socket server for tests.\n    Likely you want to use server_request or other higher level helpers.\n    All arguments except fun can be passed to other server_* helpers.\n\n    :param fun: fun(client_sock, tick) called after successful accept().\n    :param request_count: test succeeds after exactly this number of requests, triggered by tick(request)\n    :param timeout: seconds.\n    :param scheme: affects yielded value\n        \"\" - build normal http/https URI.\n        string - build normal URI using supplied scheme.\n        None - yield (addr, port) tuple.\n    :param tls:\n        None (default) - plain HTTP.\n        True - HTTPS with reasonable defaults. Likely you want httplib2.Http(ca_certs=tests.CA_CERTS)\n        string - path to custom server cert+key PEM file.\n        callable - function(context, listener, skip_errors) -> ssl_wrapped_listener\n    \"\"\"\n    gresult = [None]\n    gcounter = [0]\n    tls_skip_errors = [\n        \"TLSV1_ALERT_UNKNOWN_CA\",\n    ]\n\n    def tick(request):\n        gcounter[0] += 1\n        keep = True\n        keep &= gcounter[0] < request_count\n        if request is not None:\n            keep &= request.headers.get(\"connection\", \"\").lower() != \"close\"\n        return keep\n\n    def server_socket_thread(srv):\n        try:\n            while gcounter[0] < request_count:\n                try:\n                    client, _ = srv.accept()\n                except ssl.SSLError as e:\n                    if e.reason in tls_skip_errors:\n                        return\n                    raise\n\n                try:\n                    client.settimeout(timeout)\n                    fun(client, tick)\n                finally:\n                    try:\n                        client.shutdown(socket.SHUT_RDWR)\n                    except (IOError, socket.error):\n                        pass\n                    # FIXME: client.close() introduces connection reset by peer\n                    # at least in other/connection_close test\n                    # should not be a problem since socket would close upon garbage collection\n            if gcounter[0] > request_count:\n                gresult[0] = Exception(\n                    \"Request count expected={0} actual={1}\".format(\n                        request_count, gcounter[0]\n                    )\n                )\n        except Exception as e:\n            # traceback.print_exc caused IOError: concurrent operation on sys.stderr.close() under setup.py test\n            print(traceback.format_exc(), file=sys.stderr)\n            gresult[0] = e\n\n    bind_hostname = \"localhost\"\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.bind((bind_hostname, 0))\n    try:\n        server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    except socket.error as ex:\n        print(\"non critical error on SO_REUSEADDR\", ex)\n    server.listen(10)\n    server.settimeout(timeout)\n    server_port = server.getsockname()[1]\n    if tls is True:\n        tls = SERVER_CHAIN\n    if tls:\n        context = ssl_context()\n        if callable(tls):\n            context.load_cert_chain(SERVER_CHAIN)\n            server = tls(context, server, tls_skip_errors)\n        else:\n            context.load_cert_chain(tls)\n            server = context.wrap_socket(server, server_side=True)\n    if scheme == \"\":\n        scheme = \"https\" if tls else \"http\"\n\n    t = threading.Thread(target=server_socket_thread, args=(server,))\n    t.daemon = True\n    t.start()\n    if scheme is None:\n        yield (bind_hostname, server_port)\n    else:\n        yield u\"{scheme}://{host}:{port}/\".format(scheme=scheme, host=bind_hostname, port=server_port)\n    server.close()\n    t.join()\n    if gresult[0] is not None:\n        raise gresult[0]\n\n\ndef server_yield(fun, **kwargs):\n    q = queue.Queue(1)\n    g = fun(q.get)\n\n    def server_yield_socket_handler(sock, tick):\n        buf = BufferedReader(sock)\n        i = 0\n        while True:\n            request = HttpRequest.from_buffered(buf)\n            if request is None:\n                break\n            i += 1\n            request.client_sock = sock\n            request.number = i\n            q.put(request)\n            response = six.next(g)\n            sock.sendall(response)\n            request.client_sock = None\n            if not tick(request):\n                break\n\n    return server_socket(server_yield_socket_handler, **kwargs)\n\n\ndef server_request(request_handler, **kwargs):\n    def server_request_socket_handler(sock, tick):\n        buf = BufferedReader(sock)\n        i = 0\n        while True:\n            request = HttpRequest.from_buffered(buf)\n            if request is None:\n                break\n            # print(\"--- debug request\\n\" + request.raw.decode(\"ascii\", \"replace\"))\n            i += 1\n            request.client_sock = sock\n            request.number = i\n            response = request_handler(request=request)\n            # print(\"--- debug response\\n\" + response.decode(\"ascii\", \"replace\"))\n            sock.sendall(response)\n            request.client_sock = None\n            if not tick(request):\n                break\n\n    return server_socket(server_request_socket_handler, **kwargs)\n\n\ndef server_const_bytes(response_content, **kwargs):\n    return server_request(lambda request: response_content, **kwargs)\n\n\n_http_kwargs = (\n    \"proto\",\n    \"status\",\n    \"headers\",\n    \"body\",\n    \"add_content_length\",\n    \"add_date\",\n    \"add_etag\",\n    \"undefined_body_length\",\n)\n\n\ndef http_response_bytes(\n    proto=\"HTTP/1.1\",\n    status=\"200 OK\",\n    headers=None,\n    body=b\"\",\n    add_content_length=True,\n    add_date=False,\n    add_etag=False,\n    undefined_body_length=False,\n    **kwargs\n):\n    if undefined_body_length:\n        add_content_length = False\n    if headers is None:\n        headers = {}\n    if add_content_length:\n        headers.setdefault(\"content-length\", str(len(body)))\n    if add_date:\n        headers.setdefault(\"date\", email.utils.formatdate())\n    if add_etag:\n        headers.setdefault(\"etag\", '\"{0}\"'.format(hashlib.md5(body).hexdigest()))\n    header_string = \"\".join(\"{0}: {1}\\r\\n\".format(k, v) for k, v in headers.items())\n    if (\n        not undefined_body_length\n        and proto != \"HTTP/1.0\"\n        and \"content-length\" not in headers\n    ):\n        raise Exception(\n            \"httplib2.tests.http_response_bytes: client could not figure response body length\"\n        )\n    if str(status).isdigit():\n        status = \"{} {}\".format(status, http_client.responses[status])\n    response = (\n        \"{proto} {status}\\r\\n{headers}\\r\\n\".format(\n            proto=proto, status=status, headers=header_string\n        ).encode()\n        + body\n    )\n    return response\n\n\ndef make_http_reflect(**kwargs):\n    assert \"body\" not in kwargs, \"make_http_reflect will overwrite response \" \"body\"\n\n    def fun(request):\n        kw = copy.deepcopy(kwargs)\n        kw[\"body\"] = request.raw\n        response = http_response_bytes(**kw)\n        return response\n\n    return fun\n\n\ndef server_route(routes, **kwargs):\n    response_404 = http_response_bytes(status=\"404 Not Found\")\n    response_wildcard = routes.get(\"\")\n\n    def handler(request):\n        target = routes.get(request.uri, response_wildcard) or response_404\n        if callable(target):\n            response = target(request=request)\n        else:\n            response = target\n        return response\n\n    return server_request(handler, **kwargs)\n\n\ndef server_const_http(**kwargs):\n    response_kwargs = {k: kwargs.pop(k) for k in dict(kwargs) if k in _http_kwargs}\n    response = http_response_bytes(**response_kwargs)\n    return server_const_bytes(response, **kwargs)\n\n\ndef server_list_http(responses, **kwargs):\n    i = iter(responses)\n\n    def handler(request):\n        return next(i)\n\n    kwargs.setdefault(\"request_count\", len(responses))\n    return server_request(handler, **kwargs)\n\n\ndef server_reflect(**kwargs):\n    response_kwargs = {k: kwargs.pop(k) for k in dict(kwargs) if k in _http_kwargs}\n    http_handler = make_http_reflect(**response_kwargs)\n    return server_request(http_handler, **kwargs)\n\n\ndef http_parse_auth(s):\n    \"\"\"https://tools.ietf.org/html/rfc7235#section-2.1\n    \"\"\"\n    scheme, rest = s.split(\" \", 1)\n    result = {}\n    while True:\n        m = httplib2.WWW_AUTH_RELAXED.search(rest)\n        if not m:\n            break\n        if len(m.groups()) == 3:\n            key, value, rest = m.groups()\n            result[key.lower()] = httplib2.UNQUOTE_PAIRS.sub(r\"\\1\", value)\n    return result\n\n\ndef store_request_response(out):\n    def wrapper(fun):\n        @functools.wraps(fun)\n        def wrapped(request, *a, **kw):\n            response_bytes = fun(request, *a, **kw)\n            if out is not None:\n                response = HttpResponse.from_bytes(response_bytes)\n                out.append((request, response))\n            return response_bytes\n\n        return wrapped\n\n    return wrapper\n\n\ndef http_reflect_with_auth(\n    allow_scheme, allow_credentials, out_renew_nonce=None, out_requests=None\n):\n    \"\"\"allow_scheme - 'basic', 'digest', etc allow_credentials - sequence of ('name', 'password') out_renew_nonce - None | [function]\n\n        Way to return nonce renew function to caller.\n        Kind of `out` parameter in some programming languages.\n        Allows to keep same signature for all handler builder functions.\n    out_requests - None | []\n        If set to list, every parsed request will be appended here.\n    \"\"\"\n    glastnc = [None]\n    gnextnonce = [None]\n    gserver_nonce = [gen_digest_nonce(salt=b\"n\")]\n    realm = \"httplib2 test\"\n    server_opaque = gen_digest_nonce(salt=b\"o\")\n\n    def renew_nonce():\n        if gnextnonce[0]:\n            assert False, (\n                \"previous nextnonce was not used, probably bug in \" \"test code\"\n            )\n        gnextnonce[0] = gen_digest_nonce()\n        return gserver_nonce[0], gnextnonce[0]\n\n    if out_renew_nonce:\n        out_renew_nonce[0] = renew_nonce\n\n    def deny(**kwargs):\n        nonce_stale = kwargs.pop(\"nonce_stale\", False)\n        if nonce_stale:\n            kwargs.setdefault(\"body\", b\"nonce stale\")\n        if allow_scheme == \"basic\":\n            authenticate = 'basic realm=\"{realm}\"'.format(realm=realm)\n        elif allow_scheme == \"digest\":\n            authenticate = (\n                'digest realm=\"{realm}\", qop=\"auth\"'\n                + ', nonce=\"{nonce}\", opaque=\"{opaque}\"'\n                + (\", stale=true\" if nonce_stale else \"\")\n            ).format(realm=realm, nonce=gserver_nonce[0], opaque=server_opaque)\n        else:\n            raise Exception(\"unknown allow_scheme={0}\".format(allow_scheme))\n        deny_headers = {\"www-authenticate\": authenticate}\n        kwargs.setdefault(\"status\", 401)\n        # supplied headers may overwrite generated ones\n        deny_headers.update(kwargs.get(\"headers\", {}))\n        kwargs[\"headers\"] = deny_headers\n        kwargs.setdefault(\"body\", b\"HTTP authorization required\")\n        return http_response_bytes(**kwargs)\n\n    @store_request_response(out_requests)\n    def http_reflect_with_auth_handler(request):\n        auth_header = request.headers.get(\"authorization\", \"\")\n        if not auth_header:\n            return deny()\n        if \" \" not in auth_header:\n            return http_response_bytes(\n                status=400, body=b\"authorization header syntax error\"\n            )\n        scheme, data = auth_header.split(\" \", 1)\n        scheme = scheme.lower()\n        if scheme != allow_scheme:\n            return deny(body=b\"must use different auth scheme\")\n        if scheme == \"basic\":\n            decoded = base64.b64decode(data).decode()\n            username, password = decoded.split(\":\", 1)\n            if (username, password) in allow_credentials:\n                return make_http_reflect()(request)\n            else:\n                return deny(body=b\"supplied credentials are not allowed\")\n        elif scheme == \"digest\":\n            server_nonce_old = gserver_nonce[0]\n            nextnonce = gnextnonce[0]\n            if nextnonce:\n                # server decided to change nonce, in this case, guided by caller test code\n                gserver_nonce[0] = nextnonce\n                gnextnonce[0] = None\n            server_nonce_current = gserver_nonce[0]\n            auth_info = http_parse_auth(data)\n            client_cnonce = auth_info.get(\"cnonce\", \"\")\n            client_nc = auth_info.get(\"nc\", \"\")\n            client_nonce = auth_info.get(\"nonce\", \"\")\n            client_opaque = auth_info.get(\"opaque\", \"\")\n            client_qop = auth_info.get(\"qop\", \"auth\").strip('\"')\n\n            # TODO: auth_info.get('algorithm', 'md5')\n            hasher = hashlib.md5\n\n            # TODO: client_qop auth-int\n            ha2 = hasher(\":\".join((request.method, request.uri)).encode()).hexdigest()\n\n            if client_nonce != server_nonce_current:\n                if client_nonce == server_nonce_old:\n                    return deny(nonce_stale=True)\n                return deny(body=b\"invalid nonce\")\n            if not client_nc:\n                return deny(body=b\"auth-info nc missing\")\n            if client_opaque != server_opaque:\n                return deny(\n                    body=\"auth-info opaque mismatch expected={} actual={}\".format(\n                        server_opaque, client_opaque\n                    ).encode()\n                )\n            for allow_username, allow_password in allow_credentials:\n                ha1 = hasher(\n                    \":\".join((allow_username, realm, allow_password)).encode()\n                ).hexdigest()\n                allow_response = hasher(\n                    \":\".join(\n                        (ha1, client_nonce, client_nc, client_cnonce, client_qop, ha2)\n                    ).encode()\n                ).hexdigest()\n                rspauth_ha2 = hasher(\":{}\".format(request.uri).encode()).hexdigest()\n                rspauth = hasher(\n                    \":\".join(\n                        (\n                            ha1,\n                            client_nonce,\n                            client_nc,\n                            client_cnonce,\n                            client_qop,\n                            rspauth_ha2,\n                        )\n                    ).encode()\n                ).hexdigest()\n                if auth_info.get(\"response\", \"\") == allow_response:\n                    # TODO: fix or remove doubtful comment\n                    # do we need to save nc only on success?\n                    glastnc[0] = client_nc\n                    allow_headers = {\n                        \"authentication-info\": \" \".join(\n                            (\n                                'nextnonce=\"{}\"'.format(nextnonce) if nextnonce else \"\",\n                                \"qop={}\".format(client_qop),\n                                'rspauth=\"{}\"'.format(rspauth),\n                                'cnonce=\"{}\"'.format(client_cnonce),\n                                \"nc={}\".format(client_nc),\n                            )\n                        ).strip()\n                    }\n                    return make_http_reflect(headers=allow_headers)(request)\n            return deny(body=b\"supplied credentials are not allowed\")\n        else:\n            return http_response_bytes(\n                status=400,\n                body=\"unknown authorization scheme={0}\".format(scheme).encode(),\n            )\n\n    return http_reflect_with_auth_handler\n\n\ndef get_cache_path():\n    default = \"./_httplib2_test_cache\"\n    path = os.environ.get(\"httplib2_test_cache_path\") or default\n    if os.path.exists(path):\n        shutil.rmtree(path)\n    return path\n\n\ndef gen_digest_nonce(salt=b\"\"):\n    t = struct.pack(\">Q\", int(time.time() * 1e9))\n    return base64.b64encode(t + b\":\" + hashlib.sha1(t + salt).digest()).decode()\n\n\ndef gen_password():\n    length = random.randint(8, 64)\n    return \"\".join(six.unichr(random.randint(0, 127)) for _ in range(length))\n\n\ndef gzip_compress(bs):\n    # gzipobj = zlib.compressobj(9, zlib.DEFLATED, zlib.MAX_WBITS | 16)\n    # result = gzipobj.compress(text) + gzipobj.flush()\n    buf = six.BytesIO()\n    gf = gzip.GzipFile(fileobj=buf, mode=\"wb\", compresslevel=6)\n    gf.write(bs)\n    gf.close()\n    return buf.getvalue()\n\n\ndef gzip_decompress(bs):\n    return zlib.decompress(bs, zlib.MAX_WBITS | 16)\n\n\ndef deflate_compress(bs):\n    do = zlib.compressobj(9, zlib.DEFLATED, -zlib.MAX_WBITS)\n    return do.compress(bs) + do.flush()\n\n\ndef deflate_decompress(bs):\n    return zlib.decompress(bs, -zlib.MAX_WBITS)\n\n\ndef ssl_context(protocol=None):\n    \"\"\"Workaround for old SSLContext() required protocol argument.\n    \"\"\"\n    if sys.version_info < (3, 5, 3):\n        return ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n    return ssl.SSLContext()\n", "patch": "@@ -75,7 +75,7 @@ def _fill(self, target=1, more=None, untilend=False):\n                 chunk = b\"\"\n             else:\n                 chunk = self._sock.recv(8 << 10)\n-            # print('!!! recv', chunk)\n+            # print(\"!!! recv\", chunk)\n             if not chunk:\n                 self._end = True\n                 if untilend:", "file_path": "files/2020_8/327", "file_language": "py", "file_name": "tests/__init__.py", "outdated_file_modify": 1, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/httplib2/httplib2/raw/a1457cc31f3206cf691d11d2bf34e98865873e9e/tests%2Ftest_http.py", "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport email.utils\nimport errno\nimport httplib2\nimport mock\nimport os\nimport pytest\nfrom six.moves import http_client, urllib\nimport socket\nimport tests\n\n\ndef _raise_connection_refused_exception(*args, **kwargs):\n    raise socket.error(errno.ECONNREFUSED, \"Connection refused.\")\n\n\ndef test_connection_type():\n    http = httplib2.Http()\n    http.force_exception_to_status_code = False\n    response, content = http.request(\n        tests.DUMMY_URL, connection_type=tests.MockHTTPConnection\n    )\n    assert response[\"content-location\"] == tests.DUMMY_URL\n    assert content == b\"the body\"\n\n\ndef test_bad_status_line_retry():\n    http = httplib2.Http()\n    old_retries = httplib2.RETRIES\n    httplib2.RETRIES = 1\n    http.force_exception_to_status_code = False\n    try:\n        response, content = http.request(\n            tests.DUMMY_URL, connection_type=tests.MockHTTPBadStatusConnection\n        )\n    except http_client.BadStatusLine:\n        assert tests.MockHTTPBadStatusConnection.num_calls == 2\n    httplib2.RETRIES = old_retries\n\n\ndef test_unknown_server():\n    http = httplib2.Http()\n    http.force_exception_to_status_code = False\n    with tests.assert_raises(httplib2.ServerNotFoundError):\n        with mock.patch(\"socket.socket.connect\", side_effect=socket.gaierror):\n            http.request(\"http://no-such-hostname./\")\n\n    # Now test with exceptions turned off\n    http.force_exception_to_status_code = True\n    response, content = http.request(\"http://no-such-hostname./\")\n    assert response[\"content-type\"] == \"text/plain\"\n    assert content.startswith(b\"Unable to find\")\n    assert response.status == 400\n\n\n@pytest.mark.skipif(\n    os.environ.get(\"TRAVIS_PYTHON_VERSION\") in (\"2.7\", \"pypy\"),\n    reason=\"Fails on Travis py27/pypy, works elsewhere. \"\n    \"See https://travis-ci.org/httplib2/httplib2/jobs/408769880.\",\n)\n@mock.patch(\"socket.socket.connect\", spec=True)\ndef test_connection_refused_raises_exception(mock_socket_connect):\n    mock_socket_connect.side_effect = _raise_connection_refused_exception\n    http = httplib2.Http()\n    http.force_exception_to_status_code = False\n    with tests.assert_raises(socket.error):\n        http.request(tests.DUMMY_URL)\n\n\n@pytest.mark.skipif(\n    os.environ.get(\"TRAVIS_PYTHON_VERSION\") in (\"2.7\", \"pypy\"),\n    reason=\"Fails on Travis py27/pypy, works elsewhere. \"\n    \"See https://travis-ci.org/httplib2/httplib2/jobs/408769880.\",\n)\n@mock.patch(\"socket.socket.connect\", spec=True)\ndef test_connection_refused_returns_response(mock_socket_connect):\n    mock_socket_connect.side_effect = _raise_connection_refused_exception\n    http = httplib2.Http()\n    http.force_exception_to_status_code = True\n    response, content = http.request(tests.DUMMY_URL)\n    content = content.lower()\n    assert response[\"content-type\"] == \"text/plain\"\n    assert (\n        b\"connection refused\" in content\n        or b\"actively refused\" in content\n        or b\"socket is not connected\" in content\n    )\n    assert response.status == 400\n\n\ndef test_get_iri():\n    http = httplib2.Http()\n    query = u\"?a=\\N{CYRILLIC CAPITAL LETTER DJE}\"\n    with tests.server_reflect() as uri:\n        response, content = http.request(uri + query, \"GET\")\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.uri == \"/?a=%D0%82\"\n\n\ndef test_get_is_default_method():\n    # Test that GET is the default method\n    http = httplib2.Http()\n    with tests.server_reflect() as uri:\n        response, content = http.request(uri)\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.method == \"GET\"\n\n\ndef test_different_methods():\n    # Test that all methods can be used\n    http = httplib2.Http()\n    methods = [\"GET\", \"PUT\", \"DELETE\", \"POST\", \"unknown\"]\n    with tests.server_reflect(request_count=len(methods)) as uri:\n        for method in methods:\n            response, content = http.request(uri, method, body=b\" \")\n            assert response.status == 200\n            reflected = tests.HttpRequest.from_bytes(content)\n            assert reflected.method == method\n\n\ndef test_head_read():\n    # Test that we don't try to read the response of a HEAD request\n    # since httplib blocks response.read() for HEAD requests.\n    http = httplib2.Http()\n    respond_with = b\"HTTP/1.0 200 OK\\r\\ncontent-length: \" b\"14\\r\\n\\r\\nnon-empty-body\"\n    with tests.server_const_bytes(respond_with) as uri:\n        response, content = http.request(uri, \"HEAD\")\n    assert response.status == 200\n    assert content == b\"\"\n\n\ndef test_get_no_cache():\n    # Test that can do a GET w/o the cache turned on.\n    http = httplib2.Http()\n    with tests.server_const_http() as uri:\n        response, content = http.request(uri, \"GET\")\n    assert response.status == 200\n    assert response.previous is None\n\n\ndef test_user_agent():\n    # Test that we provide a default user-agent\n    http = httplib2.Http()\n    with tests.server_reflect() as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.headers.get(\"user-agent\", \"\").startswith(\"Python-httplib2/\")\n\n\ndef test_user_agent_non_default():\n    # Test that the default user-agent can be over-ridden\n    http = httplib2.Http()\n    with tests.server_reflect() as uri:\n        response, content = http.request(uri, \"GET\", headers={\"User-Agent\": \"fred/1.0\"})\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.headers.get(\"user-agent\") == \"fred/1.0\"\n\n\ndef test_get_300_with_location():\n    # Test the we automatically follow 300 redirects if a Location: header is provided\n    http = httplib2.Http()\n    final_content = b\"This is the final destination.\\n\"\n    routes = {\n        \"/final\": tests.http_response_bytes(body=final_content),\n        \"\": tests.http_response_bytes(\n            status=\"300 Multiple Choices\", headers={\"location\": \"/final\"}\n        ),\n    }\n    with tests.server_route(routes, request_count=2) as uri:\n        response, content = http.request(uri, \"GET\")\n    assert response.status == 200\n    assert content == final_content\n    assert response.previous.status == 300\n    assert not response.previous.fromcache\n\n    # Confirm that the intermediate 300 is not cached\n    with tests.server_route(routes, request_count=2) as uri:\n        response, content = http.request(uri, \"GET\")\n    assert response.status == 200\n    assert content == final_content\n    assert response.previous.status == 300\n    assert not response.previous.fromcache\n\n\ndef test_get_300_with_location_noredirect():\n    # Test the we automatically follow 300 redirects if a Location: header is provided\n    http = httplib2.Http()\n    http.follow_redirects = False\n    response = tests.http_response_bytes(\n        status=\"300 Multiple Choices\",\n        headers={\"location\": \"/final\"},\n        body=b\"redirect body\",\n    )\n    with tests.server_const_bytes(response) as uri:\n        response, content = http.request(uri, \"GET\")\n    assert response.status == 300\n\n\ndef test_get_300_without_location():\n    # Not giving a Location: header in a 300 response is acceptable\n    # In which case we just return the 300 response\n    http = httplib2.Http()\n    with tests.server_const_http(\n        status=\"300 Multiple Choices\", body=b\"redirect body\"\n    ) as uri:\n        response, content = http.request(uri, \"GET\")\n    assert response.status == 300\n    assert response.previous is None\n    assert content == b\"redirect body\"\n\n\ndef test_get_301():\n    # Test that we automatically follow 301 redirects\n    # and that we cache the 301 response\n    http = httplib2.Http(cache=tests.get_cache_path())\n    destination = \"\"\n    routes = {\n        \"/final\": tests.http_response_bytes(body=b\"This is the final destination.\\n\"),\n        \"\": tests.http_response_bytes(\n            status=\"301 Now where did I leave that URL\",\n            headers={\"location\": \"/final\"},\n            body=b\"redirect body\",\n        ),\n    }\n    with tests.server_route(routes, request_count=3) as uri:\n        destination = urllib.parse.urljoin(uri, \"/final\")\n        response1, content1 = http.request(uri, \"GET\")\n        response2, content2 = http.request(uri, \"GET\")\n    assert response1.status == 200\n    assert \"content-location\" in response2\n    assert response1[\"content-location\"] == destination\n    assert content1 == b\"This is the final destination.\\n\"\n    assert response1.previous.status == 301\n    assert not response1.previous.fromcache\n\n    assert response2.status == 200\n    assert response2[\"content-location\"] == destination\n    assert content2 == b\"This is the final destination.\\n\"\n    assert response2.previous.status == 301\n    assert response2.previous.fromcache\n\n\n@pytest.mark.skip(\n    not os.environ.get(\"httplib2_test_still_run_skipped\")\n    and os.environ.get(\"TRAVIS_PYTHON_VERSION\") in (\"2.7\", \"pypy\"),\n    reason=\"FIXME: timeout on Travis py27 and pypy, works elsewhere\",\n)\ndef test_head_301():\n    # Test that we automatically follow 301 redirects\n    http = httplib2.Http()\n    destination = \"\"\n    routes = {\n        \"/final\": tests.http_response_bytes(body=b\"This is the final destination.\\n\"),\n        \"\": tests.http_response_bytes(\n            status=\"301 Now where did I leave that URL\",\n            headers={\"location\": \"/final\"},\n            body=b\"redirect body\",\n        ),\n    }\n    with tests.server_route(routes, request_count=2) as uri:\n        destination = urllib.parse.urljoin(uri, \"/final\")\n        response, content = http.request(uri, \"HEAD\")\n    assert response.status == 200\n    assert response[\"content-location\"] == destination\n    assert response.previous.status == 301\n    assert not response.previous.fromcache\n\n\n@pytest.mark.xfail(\n    reason=(\n        \"FIXME: 301 cache works only with follow_redirects, should work \" \"regardless\"\n    )\n)\ndef test_get_301_no_redirect():\n    # Test that we cache the 301 response\n    http = httplib2.Http(cache=tests.get_cache_path(), timeout=0.5)\n    http.follow_redirects = False\n    response = tests.http_response_bytes(\n        status=\"301 Now where did I leave that URL\",\n        headers={\"location\": \"/final\", \"cache-control\": \"max-age=300\"},\n        body=b\"redirect body\",\n        add_date=True,\n    )\n    with tests.server_const_bytes(response) as uri:\n        response, _ = http.request(uri, \"GET\")\n        assert response.status == 301\n        assert not response.fromcache\n        response, _ = http.request(uri, \"GET\")\n        assert response.status == 301\n        assert response.fromcache\n\n\ndef test_get_302():\n    # Test that we automatically follow 302 redirects\n    # and that we DO NOT cache the 302 response\n    http = httplib2.Http(cache=tests.get_cache_path())\n    second_url, final_url = \"\", \"\"\n    routes = {\n        \"/final\": tests.http_response_bytes(body=b\"This is the final destination.\\n\"),\n        \"/second\": tests.http_response_bytes(\n            status=\"302 Found\", headers={\"location\": \"/final\"}, body=b\"second redirect\"\n        ),\n        \"\": tests.http_response_bytes(\n            status=\"302 Found\", headers={\"location\": \"/second\"}, body=b\"redirect body\"\n        ),\n    }\n    with tests.server_route(routes, request_count=7) as uri:\n        second_url = urllib.parse.urljoin(uri, \"/second\")\n        final_url = urllib.parse.urljoin(uri, \"/final\")\n        response1, content1 = http.request(second_url, \"GET\")\n        response2, content2 = http.request(second_url, \"GET\")\n        response3, content3 = http.request(uri, \"GET\")\n    assert response1.status == 200\n    assert response1[\"content-location\"] == final_url\n    assert content1 == b\"This is the final destination.\\n\"\n    assert response1.previous.status == 302\n    assert not response1.previous.fromcache\n\n    assert response2.status == 200\n    # FIXME:\n    # assert response2.fromcache\n    assert response2[\"content-location\"] == final_url\n    assert content2 == b\"This is the final destination.\\n\"\n    assert response2.previous.status == 302\n    assert not response2.previous.fromcache\n    assert response2.previous[\"content-location\"] == second_url\n\n    assert response3.status == 200\n    # FIXME:\n    # assert response3.fromcache\n    assert content3 == b\"This is the final destination.\\n\"\n    assert response3.previous.status == 302\n    assert not response3.previous.fromcache\n\n\ndef test_get_302_redirection_limit():\n    # Test that we can set a lower redirection limit\n    # and that we raise an exception when we exceed\n    # that limit.\n    http = httplib2.Http()\n    http.force_exception_to_status_code = False\n    routes = {\n        \"/second\": tests.http_response_bytes(\n            status=\"302 Found\", headers={\"location\": \"/final\"}, body=b\"second redirect\"\n        ),\n        \"\": tests.http_response_bytes(\n            status=\"302 Found\", headers={\"location\": \"/second\"}, body=b\"redirect body\"\n        ),\n    }\n    with tests.server_route(routes, request_count=4) as uri:\n        try:\n            http.request(uri, \"GET\", redirections=1)\n            assert False, \"This should not happen\"\n        except httplib2.RedirectLimit:\n            pass\n        except Exception:\n            assert False, \"Threw wrong kind of exception \"\n\n        # Re-run the test with out the exceptions\n        http.force_exception_to_status_code = True\n        response, content = http.request(uri, \"GET\", redirections=1)\n\n    assert response.status == 500\n    assert response.reason.startswith(\"Redirected more\")\n    assert response[\"status\"] == \"302\"\n    assert content == b\"second redirect\"\n    assert response.previous is not None\n\n\ndef test_get_302_no_location():\n    # Test that we throw an exception when we get\n    # a 302 with no Location: header.\n    http = httplib2.Http()\n    http.force_exception_to_status_code = False\n    with tests.server_const_http(status=\"302 Found\", request_count=2) as uri:\n        try:\n            http.request(uri, \"GET\")\n            assert False, \"Should never reach here\"\n        except httplib2.RedirectMissingLocation:\n            pass\n        except Exception:\n            assert False, \"Threw wrong kind of exception \"\n\n        # Re-run the test with out the exceptions\n        http.force_exception_to_status_code = True\n        response, content = http.request(uri, \"GET\")\n\n    assert response.status == 500\n    assert response.reason.startswith(\"Redirected but\")\n    assert \"302\" == response[\"status\"]\n    assert content == b\"\"\n\n\n@pytest.mark.skip(\n    not os.environ.get(\"httplib2_test_still_run_skipped\")\n    and os.environ.get(\"TRAVIS_PYTHON_VERSION\") in (\"2.7\", \"pypy\"),\n    reason=\"FIXME: timeout on Travis py27 and pypy, works elsewhere\",\n)\ndef test_303():\n    # Do a follow-up GET on a Location: header\n    # returned from a POST that gave a 303.\n    http = httplib2.Http()\n    routes = {\n        \"/final\": tests.make_http_reflect(),\n        \"\": tests.make_http_reflect(\n            status=\"303 See Other\", headers={\"location\": \"/final\"}\n        ),\n    }\n    with tests.server_route(routes, request_count=2) as uri:\n        response, content = http.request(uri, \"POST\", \" \")\n    assert response.status == 200\n    reflected = tests.HttpRequest.from_bytes(content)\n    assert reflected.uri == \"/final\"\n    assert response.previous.status == 303\n\n    # Skip follow-up GET\n    http = httplib2.Http()\n    http.follow_redirects = False\n    with tests.server_route(routes, request_count=1) as uri:\n        response, content = http.request(uri, \"POST\", \" \")\n    assert response.status == 303\n\n    # All methods can be used\n    http = httplib2.Http()\n    cases = \"DELETE GET HEAD POST PUT EVEN_NEW_ONES\".split(\" \")\n    with tests.server_route(routes, request_count=len(cases) * 2) as uri:\n        for method in cases:\n            response, content = http.request(uri, method, body=b\"q q\")\n            assert response.status == 200\n            reflected = tests.HttpRequest.from_bytes(content)\n            assert reflected.method == \"GET\"\n\n\ndef test_etag_used():\n    # Test that we use ETags properly to validate our cache\n    cache_path = tests.get_cache_path()\n    http = httplib2.Http(cache=cache_path)\n    response_kwargs = dict(\n        add_date=True,\n        add_etag=True,\n        body=b\"something\",\n        headers={\"cache-control\": \"public,max-age=300\"},\n    )\n\n    def handler(request):\n        if request.headers.get(\"range\"):\n            return tests.http_response_bytes(status=206, **response_kwargs)\n        return tests.http_response_bytes(**response_kwargs)\n\n    with tests.server_request(handler, request_count=2) as uri:\n        response, _ = http.request(uri, \"GET\", headers={\"accept-encoding\": \"identity\"})\n        assert response[\"etag\"] == '\"437b930db84b8079c2dd804a71936b5f\"'\n\n        http.request(uri, \"GET\", headers={\"accept-encoding\": \"identity\"})\n        response, _ = http.request(\n            uri,\n            \"GET\",\n            headers={\"accept-encoding\": \"identity\", \"cache-control\": \"must-revalidate\"},\n        )\n        assert response.status == 200\n        assert response.fromcache\n\n        # TODO: API to read cache item, at least internal to tests\n        cache_file_name = os.path.join(\n            cache_path, httplib2.safename(httplib2.urlnorm(uri)[-1])\n        )\n        with open(cache_file_name, \"r\") as f:\n            status_line = f.readline()\n        assert status_line.startswith(\"status:\")\n\n        response, content = http.request(\n            uri, \"HEAD\", headers={\"accept-encoding\": \"identity\"}\n        )\n        assert response.status == 200\n        assert response.fromcache\n\n        response, content = http.request(\n            uri, \"GET\", headers={\"accept-encoding\": \"identity\", \"range\": \"bytes=0-0\"}\n        )\n        assert response.status == 206\n        assert not response.fromcache\n\n\ndef test_etag_ignore():\n    # Test that we can forcibly ignore ETags\n    http = httplib2.Http(cache=tests.get_cache_path())\n    response_kwargs = dict(add_date=True, add_etag=True)\n    with tests.server_reflect(request_count=3, **response_kwargs) as uri:\n        response, content = http.request(\n            uri, \"GET\", headers={\"accept-encoding\": \"identity\"}\n        )\n        assert response.status == 200\n        assert response[\"etag\"] != \"\"\n\n        response, content = http.request(\n            uri,\n            \"GET\",\n            headers={\"accept-encoding\": \"identity\", \"cache-control\": \"max-age=0\"},\n        )\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.headers.get(\"if-none-match\")\n\n        http.ignore_etag = True\n        response, content = http.request(\n            uri,\n            \"GET\",\n            headers={\"accept-encoding\": \"identity\", \"cache-control\": \"max-age=0\"},\n        )\n        assert not response.fromcache\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert not reflected.headers.get(\"if-none-match\")\n\n\ndef test_etag_override():\n    # Test that we can forcibly ignore ETags\n    http = httplib2.Http(cache=tests.get_cache_path())\n    response_kwargs = dict(add_date=True, add_etag=True)\n    with tests.server_reflect(request_count=3, **response_kwargs) as uri:\n        response, _ = http.request(uri, \"GET\", headers={\"accept-encoding\": \"identity\"})\n        assert response.status == 200\n        assert response[\"etag\"] != \"\"\n\n        response, content = http.request(\n            uri,\n            \"GET\",\n            headers={\"accept-encoding\": \"identity\", \"cache-control\": \"max-age=0\"},\n        )\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.headers.get(\"if-none-match\")\n        assert reflected.headers.get(\"if-none-match\") != \"fred\"\n\n        response, content = http.request(\n            uri,\n            \"GET\",\n            headers={\n                \"accept-encoding\": \"identity\",\n                \"cache-control\": \"max-age=0\",\n                \"if-none-match\": \"fred\",\n            },\n        )\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.headers.get(\"if-none-match\") == \"fred\"\n\n\n@pytest.mark.skip(reason=\"was commented in legacy code\")\ndef test_get_304_end_to_end():\n    pass\n    # Test that end to end headers get overwritten in the cache\n    # uri = urllib.parse.urljoin(base, \"304/end2end.cgi\")\n    # response, content = http.request(uri, 'GET')\n    # assertNotEqual(response['etag'], \"\")\n    # old_date = response['date']\n    # time.sleep(2)\n\n    # response, content = http.request(uri, 'GET', headers = {'Cache-Control': 'max-age=0'})\n    # # The response should be from the cache, but the Date: header should be updated.\n    # new_date = response['date']\n    # assert new_date != old_date\n    # assert response.status == 200\n    # assert response.fromcache == True\n\n\ndef test_get_304_last_modified():\n    # Test that we can still handle a 304\n    # by only using the last-modified cache validator.\n    http = httplib2.Http(cache=tests.get_cache_path())\n    date = email.utils.formatdate()\n\n    def handler(read):\n        read()\n        yield tests.http_response_bytes(\n            status=200, body=b\"something\", headers={\"date\": date, \"last-modified\": date}\n        )\n\n        request2 = read()\n        assert request2.headers[\"if-modified-since\"] == date\n        yield tests.http_response_bytes(status=304)\n\n    with tests.server_yield(handler, request_count=2) as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.get(\"last-modified\") == date\n\n        response, content = http.request(uri, \"GET\")\n        assert response.status == 200\n        assert response.fromcache\n\n\ndef test_get_307():\n    # Test that we do follow 307 redirects but\n    # do not cache the 307\n    http = httplib2.Http(cache=tests.get_cache_path(), timeout=1)\n    r307 = tests.http_response_bytes(status=307, headers={\"location\": \"/final\"})\n    r200 = tests.http_response_bytes(\n        status=200,\n        add_date=True,\n        body=b\"final content\\n\",\n        headers={\"cache-control\": \"max-age=300\"},\n    )\n\n    with tests.server_list_http([r307, r200, r307]) as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.previous.status == 307\n        assert not response.previous.fromcache\n        assert response.status == 200\n        assert not response.fromcache\n        assert content == b\"final content\\n\"\n\n        response, content = http.request(uri, \"GET\")\n        assert response.previous.status == 307\n        assert not response.previous.fromcache\n        assert response.status == 200\n        assert response.fromcache\n        assert content == b\"final content\\n\"\n\n\ndef test_post_307():\n    # 307: follow with same method\n    http = httplib2.Http(cache=tests.get_cache_path(), timeout=1)\n    http.follow_all_redirects = True\n    r307 = tests.http_response_bytes(status=307, headers={\"location\": \"/final\"})\n    r200 = tests.http_response_bytes(status=200, body=b\"final content\\n\")\n\n    with tests.server_list_http([r307, r200, r307, r200]) as uri:\n        response, content = http.request(uri, \"POST\")\n        assert response.previous.status == 307\n        assert not response.previous.fromcache\n        assert response.status == 200\n        assert not response.fromcache\n        assert content == b\"final content\\n\"\n\n        response, content = http.request(uri, \"POST\")\n        assert response.previous.status == 307\n        assert not response.previous.fromcache\n        assert response.status == 200\n        assert not response.fromcache\n        assert content == b\"final content\\n\"\n\n\ndef test_change_308():\n    # 308: follow with same method, cache redirect\n    http = httplib2.Http(cache=tests.get_cache_path(), timeout=1)\n    routes = {\n        \"/final\": tests.make_http_reflect(),\n        \"\": tests.http_response_bytes(\n            status=\"308 Permanent Redirect\",\n            add_date=True,\n            headers={\"cache-control\": \"max-age=300\", \"location\": \"/final\"},\n        ),\n    }\n\n    with tests.server_route(routes, request_count=3) as uri:\n        response, content = http.request(uri, \"CHANGE\", body=b\"hello308\")\n        assert response.previous.status == 308\n        assert not response.previous.fromcache\n        assert response.status == 200\n        assert not response.fromcache\n        assert content.startswith(b\"CHANGE /final HTTP\")\n\n        response, content = http.request(uri, \"CHANGE\")\n        assert response.previous.status == 308\n        assert response.previous.fromcache\n        assert response.status == 200\n        assert not response.fromcache\n        assert content.startswith(b\"CHANGE /final HTTP\")\n\n\ndef test_get_410():\n    # Test that we pass 410's through\n    http = httplib2.Http()\n    with tests.server_const_http(status=410) as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.status == 410\n\n\ndef test_get_duplicate_headers():\n    # Test that duplicate headers get concatenated via ','\n    http = httplib2.Http()\n    response = b\"\"\"HTTP/1.0 200 OK\\r\\n\\\nLink: link1\\r\\n\\\nContent-Length: 7\\r\\n\\\nLink: link2\\r\\n\\r\\n\\\ncontent\"\"\"\n    with tests.server_const_bytes(response) as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.status == 200\n        assert content == b\"content\"\n        assert response[\"link\"], \"link1, link2\"\n\n\ndef test_custom_redirect_codes():\n    http = httplib2.Http()\n    http.redirect_codes = set([300])\n    with tests.server_const_http(status=301, request_count=1) as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.status == 301\n        assert response.previous is None\n\n\ndef test_cwe93_inject_crlf():\n    # https://cwe.mitre.org/data/definitions/93.html\n    # GET /?q= HTTP/1.1      <- injected \"HTTP/1.1\" from attacker\n    # injected: attack\n    # ignore-http: HTTP/1.1  <- nominal \"HTTP/1.1\" from library\n    # Host: localhost:57285\n    http = httplib2.Http()\n    with tests.server_reflect() as uri:\n        danger_url = urllib.parse.urljoin(\n            uri, \"?q= HTTP/1.1\\r\\ninjected: attack\\r\\nignore-http:\"\n        )\n        response, content = http.request(danger_url, \"GET\")\n        assert response.status == 200\n        req = tests.HttpRequest.from_bytes(content)\n        assert req.headers.get(\"injected\") is None\n\n\ndef test_inject_space():\n    # Injecting space into request line is precursor to CWE-93 and possibly other injections\n    http = httplib2.Http()\n    with tests.server_reflect() as uri:\n        # \"\\r\\nignore-http:\" suffix is nuance for current server implementation\n        # please only pay attention to space after \"?q=\"\n        danger_url = urllib.parse.urljoin(uri, \"?q= HTTP/1.1\\r\\nignore-http:\")\n        response, content = http.request(danger_url, \"GET\")\n        assert response.status == 200\n        req = tests.HttpRequest.from_bytes(content)\n        assert req.uri == \"/?q=%20HTTP/1.1%0D%0Aignore-http:\"\n", "code_before": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport email.utils\nimport errno\nimport httplib2\nimport mock\nimport os\nimport pytest\nfrom six.moves import http_client, urllib\nimport socket\nimport tests\n\n\ndef _raise_connection_refused_exception(*args, **kwargs):\n    raise socket.error(errno.ECONNREFUSED, \"Connection refused.\")\n\n\ndef test_connection_type():\n    http = httplib2.Http()\n    http.force_exception_to_status_code = False\n    response, content = http.request(\n        tests.DUMMY_URL, connection_type=tests.MockHTTPConnection\n    )\n    assert response[\"content-location\"] == tests.DUMMY_URL\n    assert content == b\"the body\"\n\n\ndef test_bad_status_line_retry():\n    http = httplib2.Http()\n    old_retries = httplib2.RETRIES\n    httplib2.RETRIES = 1\n    http.force_exception_to_status_code = False\n    try:\n        response, content = http.request(\n            tests.DUMMY_URL, connection_type=tests.MockHTTPBadStatusConnection\n        )\n    except http_client.BadStatusLine:\n        assert tests.MockHTTPBadStatusConnection.num_calls == 2\n    httplib2.RETRIES = old_retries\n\n\ndef test_unknown_server():\n    http = httplib2.Http()\n    http.force_exception_to_status_code = False\n    with tests.assert_raises(httplib2.ServerNotFoundError):\n        with mock.patch(\"socket.socket.connect\", side_effect=socket.gaierror):\n            http.request(\"http://no-such-hostname./\")\n\n    # Now test with exceptions turned off\n    http.force_exception_to_status_code = True\n    response, content = http.request(\"http://no-such-hostname./\")\n    assert response[\"content-type\"] == \"text/plain\"\n    assert content.startswith(b\"Unable to find\")\n    assert response.status == 400\n\n\n@pytest.mark.skipif(\n    os.environ.get(\"TRAVIS_PYTHON_VERSION\") in (\"2.7\", \"pypy\"),\n    reason=\"Fails on Travis py27/pypy, works elsewhere. \"\n    \"See https://travis-ci.org/httplib2/httplib2/jobs/408769880.\",\n)\n@mock.patch(\"socket.socket.connect\", spec=True)\ndef test_connection_refused_raises_exception(mock_socket_connect):\n    mock_socket_connect.side_effect = _raise_connection_refused_exception\n    http = httplib2.Http()\n    http.force_exception_to_status_code = False\n    with tests.assert_raises(socket.error):\n        http.request(tests.DUMMY_URL)\n\n\n@pytest.mark.skipif(\n    os.environ.get(\"TRAVIS_PYTHON_VERSION\") in (\"2.7\", \"pypy\"),\n    reason=\"Fails on Travis py27/pypy, works elsewhere. \"\n    \"See https://travis-ci.org/httplib2/httplib2/jobs/408769880.\",\n)\n@mock.patch(\"socket.socket.connect\", spec=True)\ndef test_connection_refused_returns_response(mock_socket_connect):\n    mock_socket_connect.side_effect = _raise_connection_refused_exception\n    http = httplib2.Http()\n    http.force_exception_to_status_code = True\n    response, content = http.request(tests.DUMMY_URL)\n    content = content.lower()\n    assert response[\"content-type\"] == \"text/plain\"\n    assert (\n        b\"connection refused\" in content\n        or b\"actively refused\" in content\n        or b\"socket is not connected\" in content\n    )\n    assert response.status == 400\n\n\ndef test_get_iri():\n    http = httplib2.Http()\n    query = u\"?a=\\N{CYRILLIC CAPITAL LETTER DJE}\"\n    with tests.server_reflect() as uri:\n        response, content = http.request(uri + query, \"GET\")\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.uri == \"/?a=%D0%82\"\n\n\ndef test_get_is_default_method():\n    # Test that GET is the default method\n    http = httplib2.Http()\n    with tests.server_reflect() as uri:\n        response, content = http.request(uri)\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.method == \"GET\"\n\n\ndef test_different_methods():\n    # Test that all methods can be used\n    http = httplib2.Http()\n    methods = [\"GET\", \"PUT\", \"DELETE\", \"POST\", \"unknown\"]\n    with tests.server_reflect(request_count=len(methods)) as uri:\n        for method in methods:\n            response, content = http.request(uri, method, body=b\" \")\n            assert response.status == 200\n            reflected = tests.HttpRequest.from_bytes(content)\n            assert reflected.method == method\n\n\ndef test_head_read():\n    # Test that we don't try to read the response of a HEAD request\n    # since httplib blocks response.read() for HEAD requests.\n    http = httplib2.Http()\n    respond_with = b\"HTTP/1.0 200 OK\\r\\ncontent-length: \" b\"14\\r\\n\\r\\nnon-empty-body\"\n    with tests.server_const_bytes(respond_with) as uri:\n        response, content = http.request(uri, \"HEAD\")\n    assert response.status == 200\n    assert content == b\"\"\n\n\ndef test_get_no_cache():\n    # Test that can do a GET w/o the cache turned on.\n    http = httplib2.Http()\n    with tests.server_const_http() as uri:\n        response, content = http.request(uri, \"GET\")\n    assert response.status == 200\n    assert response.previous is None\n\n\ndef test_user_agent():\n    # Test that we provide a default user-agent\n    http = httplib2.Http()\n    with tests.server_reflect() as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.headers.get(\"user-agent\", \"\").startswith(\"Python-httplib2/\")\n\n\ndef test_user_agent_non_default():\n    # Test that the default user-agent can be over-ridden\n    http = httplib2.Http()\n    with tests.server_reflect() as uri:\n        response, content = http.request(uri, \"GET\", headers={\"User-Agent\": \"fred/1.0\"})\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.headers.get(\"user-agent\") == \"fred/1.0\"\n\n\ndef test_get_300_with_location():\n    # Test the we automatically follow 300 redirects if a Location: header is provided\n    http = httplib2.Http()\n    final_content = b\"This is the final destination.\\n\"\n    routes = {\n        \"/final\": tests.http_response_bytes(body=final_content),\n        \"\": tests.http_response_bytes(\n            status=\"300 Multiple Choices\", headers={\"location\": \"/final\"}\n        ),\n    }\n    with tests.server_route(routes, request_count=2) as uri:\n        response, content = http.request(uri, \"GET\")\n    assert response.status == 200\n    assert content == final_content\n    assert response.previous.status == 300\n    assert not response.previous.fromcache\n\n    # Confirm that the intermediate 300 is not cached\n    with tests.server_route(routes, request_count=2) as uri:\n        response, content = http.request(uri, \"GET\")\n    assert response.status == 200\n    assert content == final_content\n    assert response.previous.status == 300\n    assert not response.previous.fromcache\n\n\ndef test_get_300_with_location_noredirect():\n    # Test the we automatically follow 300 redirects if a Location: header is provided\n    http = httplib2.Http()\n    http.follow_redirects = False\n    response = tests.http_response_bytes(\n        status=\"300 Multiple Choices\",\n        headers={\"location\": \"/final\"},\n        body=b\"redirect body\",\n    )\n    with tests.server_const_bytes(response) as uri:\n        response, content = http.request(uri, \"GET\")\n    assert response.status == 300\n\n\ndef test_get_300_without_location():\n    # Not giving a Location: header in a 300 response is acceptable\n    # In which case we just return the 300 response\n    http = httplib2.Http()\n    with tests.server_const_http(\n        status=\"300 Multiple Choices\", body=b\"redirect body\"\n    ) as uri:\n        response, content = http.request(uri, \"GET\")\n    assert response.status == 300\n    assert response.previous is None\n    assert content == b\"redirect body\"\n\n\ndef test_get_301():\n    # Test that we automatically follow 301 redirects\n    # and that we cache the 301 response\n    http = httplib2.Http(cache=tests.get_cache_path())\n    destination = \"\"\n    routes = {\n        \"/final\": tests.http_response_bytes(body=b\"This is the final destination.\\n\"),\n        \"\": tests.http_response_bytes(\n            status=\"301 Now where did I leave that URL\",\n            headers={\"location\": \"/final\"},\n            body=b\"redirect body\",\n        ),\n    }\n    with tests.server_route(routes, request_count=3) as uri:\n        destination = urllib.parse.urljoin(uri, \"/final\")\n        response1, content1 = http.request(uri, \"GET\")\n        response2, content2 = http.request(uri, \"GET\")\n    assert response1.status == 200\n    assert \"content-location\" in response2\n    assert response1[\"content-location\"] == destination\n    assert content1 == b\"This is the final destination.\\n\"\n    assert response1.previous.status == 301\n    assert not response1.previous.fromcache\n\n    assert response2.status == 200\n    assert response2[\"content-location\"] == destination\n    assert content2 == b\"This is the final destination.\\n\"\n    assert response2.previous.status == 301\n    assert response2.previous.fromcache\n\n\n@pytest.mark.skip(\n    not os.environ.get(\"httplib2_test_still_run_skipped\")\n    and os.environ.get(\"TRAVIS_PYTHON_VERSION\") in (\"2.7\", \"pypy\"),\n    reason=\"FIXME: timeout on Travis py27 and pypy, works elsewhere\",\n)\ndef test_head_301():\n    # Test that we automatically follow 301 redirects\n    http = httplib2.Http()\n    destination = \"\"\n    routes = {\n        \"/final\": tests.http_response_bytes(body=b\"This is the final destination.\\n\"),\n        \"\": tests.http_response_bytes(\n            status=\"301 Now where did I leave that URL\",\n            headers={\"location\": \"/final\"},\n            body=b\"redirect body\",\n        ),\n    }\n    with tests.server_route(routes, request_count=2) as uri:\n        destination = urllib.parse.urljoin(uri, \"/final\")\n        response, content = http.request(uri, \"HEAD\")\n    assert response.status == 200\n    assert response[\"content-location\"] == destination\n    assert response.previous.status == 301\n    assert not response.previous.fromcache\n\n\n@pytest.mark.xfail(\n    reason=(\n        \"FIXME: 301 cache works only with follow_redirects, should work \" \"regardless\"\n    )\n)\ndef test_get_301_no_redirect():\n    # Test that we cache the 301 response\n    http = httplib2.Http(cache=tests.get_cache_path(), timeout=0.5)\n    http.follow_redirects = False\n    response = tests.http_response_bytes(\n        status=\"301 Now where did I leave that URL\",\n        headers={\"location\": \"/final\", \"cache-control\": \"max-age=300\"},\n        body=b\"redirect body\",\n        add_date=True,\n    )\n    with tests.server_const_bytes(response) as uri:\n        response, _ = http.request(uri, \"GET\")\n        assert response.status == 301\n        assert not response.fromcache\n        response, _ = http.request(uri, \"GET\")\n        assert response.status == 301\n        assert response.fromcache\n\n\ndef test_get_302():\n    # Test that we automatically follow 302 redirects\n    # and that we DO NOT cache the 302 response\n    http = httplib2.Http(cache=tests.get_cache_path())\n    second_url, final_url = \"\", \"\"\n    routes = {\n        \"/final\": tests.http_response_bytes(body=b\"This is the final destination.\\n\"),\n        \"/second\": tests.http_response_bytes(\n            status=\"302 Found\", headers={\"location\": \"/final\"}, body=b\"second redirect\"\n        ),\n        \"\": tests.http_response_bytes(\n            status=\"302 Found\", headers={\"location\": \"/second\"}, body=b\"redirect body\"\n        ),\n    }\n    with tests.server_route(routes, request_count=7) as uri:\n        second_url = urllib.parse.urljoin(uri, \"/second\")\n        final_url = urllib.parse.urljoin(uri, \"/final\")\n        response1, content1 = http.request(second_url, \"GET\")\n        response2, content2 = http.request(second_url, \"GET\")\n        response3, content3 = http.request(uri, \"GET\")\n    assert response1.status == 200\n    assert response1[\"content-location\"] == final_url\n    assert content1 == b\"This is the final destination.\\n\"\n    assert response1.previous.status == 302\n    assert not response1.previous.fromcache\n\n    assert response2.status == 200\n    # FIXME:\n    # assert response2.fromcache\n    assert response2[\"content-location\"] == final_url\n    assert content2 == b\"This is the final destination.\\n\"\n    assert response2.previous.status == 302\n    assert not response2.previous.fromcache\n    assert response2.previous[\"content-location\"] == second_url\n\n    assert response3.status == 200\n    # FIXME:\n    # assert response3.fromcache\n    assert content3 == b\"This is the final destination.\\n\"\n    assert response3.previous.status == 302\n    assert not response3.previous.fromcache\n\n\ndef test_get_302_redirection_limit():\n    # Test that we can set a lower redirection limit\n    # and that we raise an exception when we exceed\n    # that limit.\n    http = httplib2.Http()\n    http.force_exception_to_status_code = False\n    routes = {\n        \"/second\": tests.http_response_bytes(\n            status=\"302 Found\", headers={\"location\": \"/final\"}, body=b\"second redirect\"\n        ),\n        \"\": tests.http_response_bytes(\n            status=\"302 Found\", headers={\"location\": \"/second\"}, body=b\"redirect body\"\n        ),\n    }\n    with tests.server_route(routes, request_count=4) as uri:\n        try:\n            http.request(uri, \"GET\", redirections=1)\n            assert False, \"This should not happen\"\n        except httplib2.RedirectLimit:\n            pass\n        except Exception:\n            assert False, \"Threw wrong kind of exception \"\n\n        # Re-run the test with out the exceptions\n        http.force_exception_to_status_code = True\n        response, content = http.request(uri, \"GET\", redirections=1)\n\n    assert response.status == 500\n    assert response.reason.startswith(\"Redirected more\")\n    assert response[\"status\"] == \"302\"\n    assert content == b\"second redirect\"\n    assert response.previous is not None\n\n\ndef test_get_302_no_location():\n    # Test that we throw an exception when we get\n    # a 302 with no Location: header.\n    http = httplib2.Http()\n    http.force_exception_to_status_code = False\n    with tests.server_const_http(status=\"302 Found\", request_count=2) as uri:\n        try:\n            http.request(uri, \"GET\")\n            assert False, \"Should never reach here\"\n        except httplib2.RedirectMissingLocation:\n            pass\n        except Exception:\n            assert False, \"Threw wrong kind of exception \"\n\n        # Re-run the test with out the exceptions\n        http.force_exception_to_status_code = True\n        response, content = http.request(uri, \"GET\")\n\n    assert response.status == 500\n    assert response.reason.startswith(\"Redirected but\")\n    assert \"302\" == response[\"status\"]\n    assert content == b\"\"\n\n\n@pytest.mark.skip(\n    not os.environ.get(\"httplib2_test_still_run_skipped\")\n    and os.environ.get(\"TRAVIS_PYTHON_VERSION\") in (\"2.7\", \"pypy\"),\n    reason=\"FIXME: timeout on Travis py27 and pypy, works elsewhere\",\n)\ndef test_303():\n    # Do a follow-up GET on a Location: header\n    # returned from a POST that gave a 303.\n    http = httplib2.Http()\n    routes = {\n        \"/final\": tests.make_http_reflect(),\n        \"\": tests.make_http_reflect(\n            status=\"303 See Other\", headers={\"location\": \"/final\"}\n        ),\n    }\n    with tests.server_route(routes, request_count=2) as uri:\n        response, content = http.request(uri, \"POST\", \" \")\n    assert response.status == 200\n    reflected = tests.HttpRequest.from_bytes(content)\n    assert reflected.uri == \"/final\"\n    assert response.previous.status == 303\n\n    # Skip follow-up GET\n    http = httplib2.Http()\n    http.follow_redirects = False\n    with tests.server_route(routes, request_count=1) as uri:\n        response, content = http.request(uri, \"POST\", \" \")\n    assert response.status == 303\n\n    # All methods can be used\n    http = httplib2.Http()\n    cases = \"DELETE GET HEAD POST PUT EVEN_NEW_ONES\".split(\" \")\n    with tests.server_route(routes, request_count=len(cases) * 2) as uri:\n        for method in cases:\n            response, content = http.request(uri, method, body=b\"q q\")\n            assert response.status == 200\n            reflected = tests.HttpRequest.from_bytes(content)\n            assert reflected.method == \"GET\"\n\n\ndef test_etag_used():\n    # Test that we use ETags properly to validate our cache\n    cache_path = tests.get_cache_path()\n    http = httplib2.Http(cache=cache_path)\n    response_kwargs = dict(\n        add_date=True,\n        add_etag=True,\n        body=b\"something\",\n        headers={\"cache-control\": \"public,max-age=300\"},\n    )\n\n    def handler(request):\n        if request.headers.get(\"range\"):\n            return tests.http_response_bytes(status=206, **response_kwargs)\n        return tests.http_response_bytes(**response_kwargs)\n\n    with tests.server_request(handler, request_count=2) as uri:\n        response, _ = http.request(uri, \"GET\", headers={\"accept-encoding\": \"identity\"})\n        assert response[\"etag\"] == '\"437b930db84b8079c2dd804a71936b5f\"'\n\n        http.request(uri, \"GET\", headers={\"accept-encoding\": \"identity\"})\n        response, _ = http.request(\n            uri,\n            \"GET\",\n            headers={\"accept-encoding\": \"identity\", \"cache-control\": \"must-revalidate\"},\n        )\n        assert response.status == 200\n        assert response.fromcache\n\n        # TODO: API to read cache item, at least internal to tests\n        cache_file_name = os.path.join(\n            cache_path, httplib2.safename(httplib2.urlnorm(uri)[-1])\n        )\n        with open(cache_file_name, \"r\") as f:\n            status_line = f.readline()\n        assert status_line.startswith(\"status:\")\n\n        response, content = http.request(\n            uri, \"HEAD\", headers={\"accept-encoding\": \"identity\"}\n        )\n        assert response.status == 200\n        assert response.fromcache\n\n        response, content = http.request(\n            uri, \"GET\", headers={\"accept-encoding\": \"identity\", \"range\": \"bytes=0-0\"}\n        )\n        assert response.status == 206\n        assert not response.fromcache\n\n\ndef test_etag_ignore():\n    # Test that we can forcibly ignore ETags\n    http = httplib2.Http(cache=tests.get_cache_path())\n    response_kwargs = dict(add_date=True, add_etag=True)\n    with tests.server_reflect(request_count=3, **response_kwargs) as uri:\n        response, content = http.request(\n            uri, \"GET\", headers={\"accept-encoding\": \"identity\"}\n        )\n        assert response.status == 200\n        assert response[\"etag\"] != \"\"\n\n        response, content = http.request(\n            uri,\n            \"GET\",\n            headers={\"accept-encoding\": \"identity\", \"cache-control\": \"max-age=0\"},\n        )\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.headers.get(\"if-none-match\")\n\n        http.ignore_etag = True\n        response, content = http.request(\n            uri,\n            \"GET\",\n            headers={\"accept-encoding\": \"identity\", \"cache-control\": \"max-age=0\"},\n        )\n        assert not response.fromcache\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert not reflected.headers.get(\"if-none-match\")\n\n\ndef test_etag_override():\n    # Test that we can forcibly ignore ETags\n    http = httplib2.Http(cache=tests.get_cache_path())\n    response_kwargs = dict(add_date=True, add_etag=True)\n    with tests.server_reflect(request_count=3, **response_kwargs) as uri:\n        response, _ = http.request(uri, \"GET\", headers={\"accept-encoding\": \"identity\"})\n        assert response.status == 200\n        assert response[\"etag\"] != \"\"\n\n        response, content = http.request(\n            uri,\n            \"GET\",\n            headers={\"accept-encoding\": \"identity\", \"cache-control\": \"max-age=0\"},\n        )\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.headers.get(\"if-none-match\")\n        assert reflected.headers.get(\"if-none-match\") != \"fred\"\n\n        response, content = http.request(\n            uri,\n            \"GET\",\n            headers={\n                \"accept-encoding\": \"identity\",\n                \"cache-control\": \"max-age=0\",\n                \"if-none-match\": \"fred\",\n            },\n        )\n        assert response.status == 200\n        reflected = tests.HttpRequest.from_bytes(content)\n        assert reflected.headers.get(\"if-none-match\") == \"fred\"\n\n\n@pytest.mark.skip(reason=\"was commented in legacy code\")\ndef test_get_304_end_to_end():\n    pass\n    # Test that end to end headers get overwritten in the cache\n    # uri = urllib.parse.urljoin(base, \"304/end2end.cgi\")\n    # response, content = http.request(uri, 'GET')\n    # assertNotEqual(response['etag'], \"\")\n    # old_date = response['date']\n    # time.sleep(2)\n\n    # response, content = http.request(uri, 'GET', headers = {'Cache-Control': 'max-age=0'})\n    # # The response should be from the cache, but the Date: header should be updated.\n    # new_date = response['date']\n    # assert new_date != old_date\n    # assert response.status == 200\n    # assert response.fromcache == True\n\n\ndef test_get_304_last_modified():\n    # Test that we can still handle a 304\n    # by only using the last-modified cache validator.\n    http = httplib2.Http(cache=tests.get_cache_path())\n    date = email.utils.formatdate()\n\n    def handler(read):\n        read()\n        yield tests.http_response_bytes(\n            status=200, body=b\"something\", headers={\"date\": date, \"last-modified\": date}\n        )\n\n        request2 = read()\n        assert request2.headers[\"if-modified-since\"] == date\n        yield tests.http_response_bytes(status=304)\n\n    with tests.server_yield(handler, request_count=2) as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.get(\"last-modified\") == date\n\n        response, content = http.request(uri, \"GET\")\n        assert response.status == 200\n        assert response.fromcache\n\n\ndef test_get_307():\n    # Test that we do follow 307 redirects but\n    # do not cache the 307\n    http = httplib2.Http(cache=tests.get_cache_path(), timeout=1)\n    r307 = tests.http_response_bytes(status=307, headers={\"location\": \"/final\"})\n    r200 = tests.http_response_bytes(\n        status=200,\n        add_date=True,\n        body=b\"final content\\n\",\n        headers={\"cache-control\": \"max-age=300\"},\n    )\n\n    with tests.server_list_http([r307, r200, r307]) as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.previous.status == 307\n        assert not response.previous.fromcache\n        assert response.status == 200\n        assert not response.fromcache\n        assert content == b\"final content\\n\"\n\n        response, content = http.request(uri, \"GET\")\n        assert response.previous.status == 307\n        assert not response.previous.fromcache\n        assert response.status == 200\n        assert response.fromcache\n        assert content == b\"final content\\n\"\n\n\ndef test_post_307():\n    # 307: follow with same method\n    http = httplib2.Http(cache=tests.get_cache_path(), timeout=1)\n    http.follow_all_redirects = True\n    r307 = tests.http_response_bytes(status=307, headers={\"location\": \"/final\"})\n    r200 = tests.http_response_bytes(status=200, body=b\"final content\\n\")\n\n    with tests.server_list_http([r307, r200, r307, r200]) as uri:\n        response, content = http.request(uri, \"POST\")\n        assert response.previous.status == 307\n        assert not response.previous.fromcache\n        assert response.status == 200\n        assert not response.fromcache\n        assert content == b\"final content\\n\"\n\n        response, content = http.request(uri, \"POST\")\n        assert response.previous.status == 307\n        assert not response.previous.fromcache\n        assert response.status == 200\n        assert not response.fromcache\n        assert content == b\"final content\\n\"\n\n\ndef test_change_308():\n    # 308: follow with same method, cache redirect\n    http = httplib2.Http(cache=tests.get_cache_path(), timeout=1)\n    routes = {\n        \"/final\": tests.make_http_reflect(),\n        \"\": tests.http_response_bytes(\n            status=\"308 Permanent Redirect\",\n            add_date=True,\n            headers={\"cache-control\": \"max-age=300\", \"location\": \"/final\"},\n        ),\n    }\n\n    with tests.server_route(routes, request_count=3) as uri:\n        response, content = http.request(uri, \"CHANGE\", body=b\"hello308\")\n        assert response.previous.status == 308\n        assert not response.previous.fromcache\n        assert response.status == 200\n        assert not response.fromcache\n        assert content.startswith(b\"CHANGE /final HTTP\")\n\n        response, content = http.request(uri, \"CHANGE\")\n        assert response.previous.status == 308\n        assert response.previous.fromcache\n        assert response.status == 200\n        assert not response.fromcache\n        assert content.startswith(b\"CHANGE /final HTTP\")\n\n\ndef test_get_410():\n    # Test that we pass 410's through\n    http = httplib2.Http()\n    with tests.server_const_http(status=410) as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.status == 410\n\n\ndef test_get_duplicate_headers():\n    # Test that duplicate headers get concatenated via ','\n    http = httplib2.Http()\n    response = b\"\"\"HTTP/1.0 200 OK\\r\\n\\\nLink: link1\\r\\n\\\nContent-Length: 7\\r\\n\\\nLink: link2\\r\\n\\r\\n\\\ncontent\"\"\"\n    with tests.server_const_bytes(response) as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.status == 200\n        assert content == b\"content\"\n        assert response[\"link\"], \"link1, link2\"\n\n\ndef test_custom_redirect_codes():\n    http = httplib2.Http()\n    http.redirect_codes = set([300])\n    with tests.server_const_http(status=301, request_count=1) as uri:\n        response, content = http.request(uri, \"GET\")\n        assert response.status == 301\n        assert response.previous is None\n", "patch": "@@ -703,3 +703,33 @@ def test_custom_redirect_codes():\n         response, content = http.request(uri, \"GET\")\n         assert response.status == 301\n         assert response.previous is None\n+\n+\n+def test_cwe93_inject_crlf():\n+    # https://cwe.mitre.org/data/definitions/93.html\n+    # GET /?q= HTTP/1.1      <- injected \"HTTP/1.1\" from attacker\n+    # injected: attack\n+    # ignore-http: HTTP/1.1  <- nominal \"HTTP/1.1\" from library\n+    # Host: localhost:57285\n+    http = httplib2.Http()\n+    with tests.server_reflect() as uri:\n+        danger_url = urllib.parse.urljoin(\n+            uri, \"?q= HTTP/1.1\\r\\ninjected: attack\\r\\nignore-http:\"\n+        )\n+        response, content = http.request(danger_url, \"GET\")\n+        assert response.status == 200\n+        req = tests.HttpRequest.from_bytes(content)\n+        assert req.headers.get(\"injected\") is None\n+\n+\n+def test_inject_space():\n+    # Injecting space into request line is precursor to CWE-93 and possibly other injections\n+    http = httplib2.Http()\n+    with tests.server_reflect() as uri:\n+        # \"\\r\\nignore-http:\" suffix is nuance for current server implementation\n+        # please only pay attention to space after \"?q=\"\n+        danger_url = urllib.parse.urljoin(uri, \"?q= HTTP/1.1\\r\\nignore-http:\")\n+        response, content = http.request(danger_url, \"GET\")\n+        assert response.status == 200\n+        req = tests.HttpRequest.from_bytes(content)\n+        assert req.uri == \"/?q=%20HTTP/1.1%0D%0Aignore-http:\"", "file_path": "files/2020_8/328", "file_language": "py", "file_name": "tests/test_http.py", "outdated_file_modify": 1, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}], "outdated": 1, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
