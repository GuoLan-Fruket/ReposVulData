{"index": 10283, "cve_id": "CVE-2015-10062", "cwe_id": ["CWE-74"], "cve_language": "Python", "cve_description": "A vulnerability, which was classified as problematic, was found in galaxy-data-resource up to 14.10.0. This affects an unknown part of the component Command Line Template. The manipulation leads to injection. Upgrading to version 14.10.1 is able to address this issue. The patch is named 50d65f45d3f5be5d1fbff2e45ac5cec075f07d42. It is recommended to upgrade the affected component. The associated identifier of this vulnerability is VDB-218451.", "cvss": "9.8", "publish_date": "January 17, 2023", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "NONE", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "50d65f45d3f5be5d1fbff2e45ac5cec075f07d42", "commit_message": "Fix a critical security vulnerability where unsanitized user-modifiable values could be included in a command line template.", "commit_date": "2015-01-13T15:27:49Z", "project": "blankenberg/galaxy-data-resource", "url": "https://api.github.com/repos/blankenberg/galaxy-data-resource/commits/50d65f45d3f5be5d1fbff2e45ac5cec075f07d42", "html_url": "https://github.com/blankenberg/galaxy-data-resource/commit/50d65f45d3f5be5d1fbff2e45ac5cec075f07d42", "windows_before": [{"commit_id": "2d910de0902e8849f7dba95a64e4711ee38aeaf4", "commit_date": "Tue Jan 13 10:27:47 2015 -0500", "commit_message": "Merge head created for security fix on latest_2014.08.11", "files_name": ["4e28a8c13bf1109f23d40aad7d5798fe5e9780c1 - Tue Jan 13 10:27:46 2015 -0500 : Update tag latest_2014.08.11 for changeset 8150024c0e6f", ".hgtags"]}, {"commit_id": "703cbb071037b6c23b882eefb8d1fab80d85246f", "commit_date": "Tue Jan 13 10:27:43 2015 -0500", "commit_message": "Fix a critical security vulnerability where unsanitized user-modifiable values could be included in a command line template.", "files_name": ["lib/galaxy/datatypes/metadata.py", "lib/galaxy/tools/evaluation.py", "lib/galaxy/tools/wrappers.py", "lib/galaxy/util/__init__.py", "lib/galaxy/util/dbkeys.py", "lib/galaxy/util/object_wrapper.py"]}, {"commit_id": "423e4f136760e2deb2c9d894e4c76ad1343dc5e6", "commit_date": "Tue Jan 13 10:27:40 2015 -0500", "commit_message": "Merge head created for security fix on latest_2014.06.02", "files_name": ["de482b5daf2198c2a1fe2e941048d5ac553a94f0 - Tue Jan 13 10:27:39 2015 -0500 : Update tag latest_2014.06.02 for changeset 4145417a6e1c", ".hgtags"]}, {"commit_id": "1d8d57d3b2884171481136483602c3533f4575b5", "commit_date": "Tue Jan 13 10:27:36 2015 -0500", "commit_message": "Fix a critical security vulnerability where unsanitized user-modifiable values could be included in a command line template.", "files_name": ["lib/galaxy/datatypes/metadata.py", "lib/galaxy/tools/evaluation.py", "lib/galaxy/tools/wrappers.py", "lib/galaxy/util/__init__.py", "lib/galaxy/util/dbkeys.py", "lib/galaxy/util/object_wrapper.py"]}, {"commit_id": "da9de74dee42b837b26d8e84eef002c38c48acb5", "commit_date": "Tue Jan 13 10:27:32 2015 -0500", "commit_message": "Merge head created for security fix on latest_2014.04.14", "files_name": ["92dc9fa09e3f69672955b8b55fe4feaf0d7f918b - Tue Jan 13 10:27:31 2015 -0500 : Update tag latest_2014.04.14 for changeset 8f9dcac03369", ".hgtags"]}, {"commit_id": "4ab1216fbe674d2a5f8b6a4dabe6e01b6b220d1a", "commit_date": "Tue Jan 13 10:27:27 2015 -0500", "commit_message": "Fix a critical security vulnerability where unsanitized user-modifiable values could be included in a command line template.", "files_name": ["lib/galaxy/datatypes/metadata.py", "lib/galaxy/tools/actions/__init__.py", "lib/galaxy/tools/evaluation.py", "lib/galaxy/tools/wrappers.py", "lib/galaxy/util/__init__.py", "lib/galaxy/util/object_wrapper.py"]}, {"commit_id": "b0ed6d79a6003c89372e186aaebdb27c0417f0bc", "commit_date": "Tue Jan 13 10:27:23 2015 -0500", "commit_message": "Merge head created for security fix on latest_2014.02.10", "files_name": ["0562409fab695f12b9de5cf2a46fc712e2aa469d - Tue Jan 13 10:27:22 2015 -0500 : Update tag latest_2014.02.10 for changeset 0c000cc2f9c0", ".hgtags"]}, {"commit_id": "c01623f344560681e953277a45896f8e8bf1fb23", "commit_date": "Tue Jan 13 10:27:19 2015 -0500", "commit_message": "Fix a critical security vulnerability where unsanitized user-modifiable values could be included in a command line template.", "files_name": ["lib/galaxy/datatypes/metadata.py", "lib/galaxy/tools/actions/__init__.py", "lib/galaxy/tools/evaluation.py", "lib/galaxy/tools/wrappers.py", "lib/galaxy/util/__init__.py", "lib/galaxy/util/object_wrapper.py"]}, {"commit_id": "6e5910b54c92d63091a16118e0e7a880d47c0811", "commit_date": "Tue Jan 13 10:27:15 2015 -0500", "commit_message": "Merge head created for security fix on latest_2013.11.04", "files_name": ["833e1d9682804866a3e2608f9d853557b29c64ff - Tue Jan 13 10:27:13 2015 -0500 : Update tag latest_2013.11.04 for changeset 7d5aa19a166c", ".hgtags"]}, {"commit_id": "48d9a82f9b4b9b217c33a9411e8011467196fb17", "commit_date": "Tue Jan 13 10:27:09 2015 -0500", "commit_message": "Fix a critical security vulnerability where unsanitized user-modifiable values could be included in a command line template.", "files_name": ["lib/galaxy/datatypes/metadata.py", "lib/galaxy/tools/__init__.py", "lib/galaxy/tools/actions/__init__.py", "lib/galaxy/util/__init__.py", "lib/galaxy/util/object_wrapper.py"]}, {"commit_id": "c1339e0c5d8fab092086289c65bfa98e97391003", "commit_date": "Tue Jan 13 10:27:03 2015 -0500", "commit_message": "Merge head created for security fix on latest_2013.08.12", "files_name": ["95b55a3dae9924fc28ab3755fbd552daf2f4a940 - Tue Jan 13 10:27:02 2015 -0500 : Update tag latest_2013.08.12 for changeset cee903b8b3ee", ".hgtags"]}, {"commit_id": "a32fca777ec5b64a0e7a3eab3abde56afdddd1ff", "commit_date": "Tue Jan 13 10:26:56 2015 -0500", "commit_message": "Fix a critical security vulnerability where unsanitized user-modifiable values could be included in a command line template.", "files_name": ["lib/galaxy/datatypes/metadata.py", "lib/galaxy/tools/__init__.py", "lib/galaxy/tools/actions/__init__.py", "lib/galaxy/util/__init__.py", "lib/galaxy/util/object_wrapper.py"]}, {"commit_id": "3480c8093576533ccd3a0b741147b14839da1f3f", "commit_date": "Tue Jan 13 10:26:46 2015 -0500", "commit_message": "Merge head created for security fix on latest_2013.06.03", "files_name": ["898b85d9c5f98183b950a428583ff96bb2300371 - Tue Jan 13 10:26:45 2015 -0500 : Update tag latest_2013.06.03 for changeset 19e56e66b0b3", ".hgtags"]}, {"commit_id": "bb3a003d95f45eaf0a60b3b7ed2381635030a2d0", "commit_date": "Tue Jan 13 10:26:40 2015 -0500", "commit_message": "Fix a critical security vulnerability where unsanitized user-modifiable values could be included in a command line template.", "files_name": ["lib/galaxy/datatypes/metadata.py", "lib/galaxy/tools/__init__.py", "lib/galaxy/tools/actions/__init__.py", "lib/galaxy/util/__init__.py", "lib/galaxy/util/object_wrapper.py"]}, {"commit_id": "aa5b28554a76b4dc80e6bb982233757e0d23d4cd", "commit_date": "Tue Jan 13 10:26:33 2015 -0500", "commit_message": "Merge head created for security fix on latest_2013.04.01", "files_name": ["76926d410f77406270c5a6b3fcfc139a06e3aac0 - Tue Jan 13 10:26:32 2015 -0500 : Update tag latest_2013.04.01 for changeset dec9431d66b8", ".hgtags"]}, {"commit_id": "e2a5756a4df96fe5807a58101c2d33d79cce02f4", "commit_date": "Tue Jan 13 10:26:27 2015 -0500", "commit_message": "Fix a critical security vulnerability where unsanitized user-modifiable values could be included in a command line template.", "files_name": ["lib/galaxy/datatypes/metadata.py", "lib/galaxy/tools/__init__.py", "lib/galaxy/tools/actions/__init__.py", "lib/galaxy/util/__init__.py", "lib/galaxy/util/object_wrapper.py"]}, {"commit_id": "9a279fa5dd9f98864a31bf4f16d3d2d841b3e4b2", "commit_date": "Tue Jan 13 10:26:20 2015 -0500", "commit_message": "Merge head created for security fix on latest_2013.02.08", "files_name": ["6ed3e9d34ec8ed0d355da3f7c8e9a97ec89cb213 - Tue Jan 13 10:26:18 2015 -0500 : Update tag latest_2013.02.08 for changeset b986c184be88", ".hgtags"]}, {"commit_id": "f5282fa0eeb25bfe8dded7733b2575751b46a8cc", "commit_date": "Tue Jan 13 10:26:12 2015 -0500", "commit_message": "Fix a critical security vulnerability where unsanitized user-modifiable values could be included in a command line template.", "files_name": ["lib/galaxy/datatypes/metadata.py", "lib/galaxy/tools/__init__.py", "lib/galaxy/tools/actions/__init__.py", "lib/galaxy/util/__init__.py", "lib/galaxy/util/object_wrapper.py"]}, {"commit_id": "a8b5ff96dcdedf9c7b07a2337999c1fd9e8baddb", "commit_date": "Tue Jan 13 10:26:04 2015 -0500", "commit_message": "Merge head created for security fix on latest_2013.01.13", "files_name": ["aa32f8b751b9ad0bc84751ab7897f6ecfbef43d7 - Tue Jan 13 10:26:03 2015 -0500 : Update tag latest_2013.01.13 for changeset 9c323aad4ffd", ".hgtags"]}, {"commit_id": "581977f112d86c9b01d951aa75ed98696ab064d8", "commit_date": "Tue Jan 13 10:25:57 2015 -0500", "commit_message": "Fix a critical security vulnerability where unsanitized user-modifiable values could be included in a command line template.", "files_name": ["lib/galaxy/datatypes/metadata.py", "lib/galaxy/tools/__init__.py", "lib/galaxy/tools/actions/__init__.py", "lib/galaxy/util/__init__.py", "lib/galaxy/util/object_wrapper.py"]}, {"commit_id": "b307fb76fec285da93b55ee7020aa5c98d3fa32e", "commit_date": "Fri Jan 9 14:48:56 2015 -0500", "commit_message": "Add gemini.sqlite datatype.", "files_name": ["config/datatypes_conf.xml.sample", "lib/galaxy/datatypes/binary.py", "tools/data_source/upload.py"]}, {"commit_id": "30446643362c2dbec849d803e69ff2101fcbe479", "commit_date": "Tue Jan 6 09:00:16 2015 -0500", "commit_message": "Update tag latest_2014.10.06 for changeset ff6e36d7a238", "files_name": [".hgtags"]}, {"commit_id": "65ef16fc015d78ae4006483df405a802a6a12c2e", "commit_date": "Mon Jan 5 22:53:57 2015 +0100", "commit_message": "Merged in jmchilton/galaxy-central-fork-1/stable (pull request #620)", "files_name": ["fc04d37c8e1b10b5d6492ff535bf48dd7de009b4 - Mon Jan 5 09:00:18 2015 -0500 : Update tag latest_2014.10.06 for changeset 793d9cd5f9de", ".hgtags"]}, {"commit_id": "bcb4c3b6dd64afd62afbd9b30959475fbd0f26d1", "commit_date": "Mon Jan 5 08:56:20 2015 -0500", "commit_message": "Merge stable.", "files_name": ["f01ae9f7b3b9d4f4a10b7799ae5ac235f2e1b126 - Sat Dec 27 17:30:59 2014 -0500 : Fixes for over escaping in c2bed0a.", "lib/galaxy/web/base/controllers/admin.py", "lib/galaxy/webapps/galaxy/controllers/admin.py", "lib/galaxy/webapps/galaxy/controllers/admin_toolshed.py", "lib/galaxy/webapps/tool_shed/controllers/admin.py", "lib/galaxy/webapps/tool_shed/controllers/repository.py", "lib/galaxy/webapps/tool_shed/controllers/repository_review.py", "lib/galaxy/webapps/tool_shed/controllers/upload.py", "lib/tool_shed/util/repository_util.py", "lib/tool_shed/util/web_util.py"]}, {"commit_id": "1f96fa9b33ddd67589913a893f9da8bb700850f9", "commit_date": "Sat Dec 27 17:30:59 2014 -0500", "commit_message": "Fixes for over escaping in c2bed0a.", "files_name": ["lib/galaxy/web/base/controllers/admin.py", "lib/galaxy/webapps/galaxy/controllers/admin.py", "lib/galaxy/webapps/galaxy/controllers/admin_toolshed.py", "lib/galaxy/webapps/tool_shed/controllers/admin.py", "lib/galaxy/webapps/tool_shed/controllers/repository.py", "lib/galaxy/webapps/tool_shed/controllers/repository_review.py", "lib/galaxy/webapps/tool_shed/controllers/upload.py", "lib/tool_shed/util/repository_util.py", "lib/tool_shed/util/web_util.py"]}, {"commit_id": "981954e1077c82ff459c3748517a5ea1c72b7407", "commit_date": "Mon Dec 22 14:35:08 2014 -0500", "commit_message": "UsesHistoryMixin, get_history: allow anon users to get their current history; update browser tests; remove unused selenium tests", "files_name": ["lib/galaxy/web/base/controller.py", "test/casperjs/anon-history-tests.js", "test/casperjs/api-anon-history-permission-tests.js", "test/casperjs/api-anon-history-tests.js", "test/casperjs/api-configuration-tests.js", "test/casperjs/api-hda-tests.js", "test/casperjs/api-history-permission-tests.js", "test/casperjs/api-history-tests.js", "test/casperjs/api-tool-tests.js", "test/casperjs/api-user-tests.js", "test/casperjs/api-visualizations-tests.js", "test/casperjs/api-workflow-tests.js", "test/casperjs/hda-state-tests.js", "test/casperjs/history-options-tests.js", "test/casperjs/history-panel-tests.js", "test/casperjs/modules/api.js", "test/casperjs/modules/historyoptions.js", "test/casperjs/upload-tests.js", "test/selenium/root/Login.html", "test/selenium/root/Suite.html", "test/selenium/root/UserRegistration.html", "test/selenium/visualization/Trackster.html", "test/selenium/workflow/CreateSimpleWorkflow.html", "test/selenium/workflow/CreateUser.html", "test/selenium/workflow/CreateWorkflowWithGroupingAndRuntimeParameters.html", "test/selenium/workflow/RunWorkflowWithGroupingAndRuntimeParameters.html", "test/selenium/workflow/Suite.html", "test/selenium/workflow/VerifyRunSimpleWorkflow.html"]}, {"commit_id": "269e24397db4378ba298892213a016f25e669fe5", "commit_date": "Thu Dec 18 09:47:08 2014 -0500", "commit_message": "Don't choke on tool versions switches with significant parameter changes. Just regenerate the tool state from the supplied parameters instead - seems to still perserve parameters on the tool form that are common between the versions because they are coming in throuh kwd.", "files_name": ["lib/galaxy/webapps/galaxy/controllers/tool_runner.py"]}, {"commit_id": "195eab61fbdfacb9bc1bf1132c83ac7e3d4a5ac8", "commit_date": "Wed Dec 17 10:57:44 2014 -0500", "commit_message": "Fix a74d320.", "files_name": ["lib/galaxy/tools/test.py"]}, {"commit_id": "c4d29ad1b64881701dfc100c8f73ff532fa2e5d0", "commit_date": "Wed Dec 17 09:38:49 2014 -0500", "commit_message": "Fix gz/zip upload of files to test framework.", "files_name": ["lib/galaxy/tools/test.py", "test/functional/tools/gzipped_inputs.xml", "test/functional/tools/samples_tool_conf.xml"]}, {"commit_id": "c1f421da52ad37cfced2b72f35d4bb35ab4fcc17", "commit_date": "Wed Dec 17 09:00:15 2014 -0500", "commit_message": "Update tag latest_2014.10.06 for changeset 5834b1066462", "files_name": [".hgtags"]}], "windows_after": [{"commit_id": "07436f42609b7f47ce62d4d5ab31814b61dac560", "commit_date": "Tue Jan 13 10:27:50 2015 -0500", "commit_message": "Update tag latest_2014.10.06 for changeset c437b28348a9", "files_name": [".hgtags"]}, {"commit_id": "f7884919c0c3620c314bf0edac370d77cab736fb", "commit_date": "Tue Jan 13 10:27:51 2015 -0500", "commit_message": "Merge head created for security fix on latest_2014.10.06", "files_name": ["43a0cd67577fefe1af0b7938d878d1fa98cb2e0c - Tue Jan 13 10:33:41 2015 -0500 : Close next-stable branch for release_2015.01.13", "lib/galaxy/datatypes/metadata.py", "lib/galaxy/tools/evaluation.py", "lib/galaxy/tools/wrappers.py", "lib/galaxy/util/__init__.py", "lib/galaxy/util/dbkeys.py", "lib/galaxy/util/object_wrapper.py"]}, {"commit_id": "c38fd1f011f7247a73fc71b40081e04a1e54d415", "commit_date": "Tue Jan 13 10:33:50 2015 -0500", "commit_message": "Merge next-stable to stable for release_2015.01.13", "files_name": ["eae1a0972427a25e12e4f949bbb28370e5328fa1 - Tue Jan 13 10:36:58 2015 -0500 : Added tag release_2015.01.13 for changeset 2e8dd2949dd3", ".hgtags"]}, {"commit_id": "7358a456bbb97e0bc5c71774bf84f21b12c1ae89", "commit_date": "Tue Jan 13 10:37:28 2015 -0500", "commit_message": "Added tag latest_2015.01.13 for changeset 2e8dd2949dd3", "files_name": [".hgtags"]}, {"commit_id": "871640b7543326d4a976471e99aa64c1713fe0e7", "commit_date": "Wed Jan 14 15:02:23 2015 -0500", "commit_message": "Make DatasetListWrapper and DatasetCollectionWrapper subclasses of ToolParameterValueWrapper.", "files_name": ["lib/galaxy/tools/wrappers.py"]}, {"commit_id": "80c4c1a26090a1293b32d47b02f33b0600198812", "commit_date": "Wed Jan 14 15:59:46 2015 -0500", "commit_message": "Upload: Fix space_to_tab option", "files_name": ["client/galaxy/scripts/mvc/upload/upload-view.js", "static/scripts/mvc/upload/upload-view.js", "static/scripts/packed/mvc/upload/upload-view.js"]}, {"commit_id": "68c06232ef21163ef6ec1109bd3277bc4e0f9f1f", "commit_date": "Wed Jan 14 17:07:37 2015 -0500", "commit_message": "Make DatasetListWrapper and DatasetCollectionWrapper subclasses of ToolParameterValueWrapper.", "files_name": ["lib/galaxy/tools/wrappers.py"]}, {"commit_id": "00fcac07875240266cf0c1eb9ae514701a92ecb1", "commit_date": "Wed Jan 14 17:07:41 2015 -0500", "commit_message": "Update tag latest_2014.06.02 for changeset 9bce3f426863", "files_name": [".hgtags"]}, {"commit_id": "a381565ff6fb884dd039ae94c2554da44c7b48e4", "commit_date": "Wed Jan 14 17:07:42 2015 -0500", "commit_message": "Merge head created for backport bug fix on latest_2014.06.02", "files_name": ["136239b1b6982162728571c43f500b3d67f3b897 - Wed Jan 14 17:07:46 2015 -0500 : Make DatasetListWrapper and DatasetCollectionWrapper subclasses of ToolParameterValueWrapper.", "lib/galaxy/tools/wrappers.py"]}, {"commit_id": "7a95a1eb6faeaebda359c7eda84df0718ed353cd", "commit_date": "Wed Jan 14 17:07:50 2015 -0500", "commit_message": "Update tag latest_2014.08.11 for changeset f3fc4602e22b", "files_name": [".hgtags"]}, {"commit_id": "cb9dbc81baf3df9ce3bd1a8c9d6c604664479c4b", "commit_date": "Wed Jan 14 17:07:51 2015 -0500", "commit_message": "Merge head created for backport bug fix on latest_2014.08.11", "files_name": ["7f62891da4f5b6df9e642050b796686e6773554c - Wed Jan 14 17:07:54 2015 -0500 : Make DatasetListWrapper and DatasetCollectionWrapper subclasses of ToolParameterValueWrapper.", "lib/galaxy/tools/wrappers.py"]}, {"commit_id": "1927fd9fff37a077ba7b03d9f64013cff4e67d5f", "commit_date": "Wed Jan 14 17:07:56 2015 -0500", "commit_message": "Update tag latest_2014.10.06 for changeset 9bd6f8b5b815", "files_name": [".hgtags"]}, {"commit_id": "b025470849babe79285501d843ce0ebca963707b", "commit_date": "Wed Jan 14 17:07:57 2015 -0500", "commit_message": "Merge head created for backport bug fix on latest_2014.10.06", "files_name": ["b52817d28b02e6d04e07010a35f33353015e522f - Thu Jan 15 07:53:44 2015 -0500 : Merged in guerler/guerler-galaxy-central/stable (pull request #632)", "4623342a3a74b54cf26dbeee52c8ea2d206843ff - Thu Jan 15 11:59:20 2015 -0500 : Bump bioblend to 0.5.2.  The egg no longer depends on mock, which is sort of a bandaid fix for the startup eggs dependency bug not fetching it correctly.  TODO: followup on that bug.", "eggs.ini"]}, {"commit_id": "a9a1b78946e3882ec10e8e932b9d511362b2a734", "commit_date": "Thu Jan 15 12:00:23 2015 -0500", "commit_message": "Update tag latest_2015.01.13 for changeset 3b559c4b9399", "files_name": [".hgtags"]}, {"commit_id": "3f1d9b0286db048aac5f28689cdfeb6d982d12e1", "commit_date": "Thu Jan 15 13:20:34 2015 -0500", "commit_message": "Skip extra wrapping around template-style macros. Cheetah wants these to be strings.", "files_name": ["lib/galaxy/tools/evaluation.py"]}, {"commit_id": "4cd434bee973385573a29860eb97370e96af3a08", "commit_date": "Thu Jan 15 16:01:03 2015 -0500", "commit_message": "Bugfix: Skip extra wrapping around template-style macros.", "files_name": ["lib/galaxy/tools/__init__.py"]}, {"commit_id": "e462c6bcc734213a4be5bd0638af041f5241c159", "commit_date": "Thu Jan 15 16:01:05 2015 -0500", "commit_message": "Update tag latest_2013.06.03 for changeset aae74ee09e46", "files_name": [".hgtags"]}, {"commit_id": "2ed61bbe65c73d824ad6ac47f711ad7547c1f5ba", "commit_date": "Thu Jan 15 16:01:11 2015 -0500", "commit_message": "Merge head created for backport bug fix on latest_2013.06.03", "files_name": ["b954ccaa67eab6126002e59be88bc5a6cf67438c - Thu Jan 15 16:01:13 2015 -0500 : Bugfix: Skip extra wrapping around template-style macros.", "lib/galaxy/tools/__init__.py"]}, {"commit_id": "647178e174d64d946d1b42650be4057581baf2e0", "commit_date": "Thu Jan 15 16:01:15 2015 -0500", "commit_message": "Update tag latest_2013.08.12 for changeset db967a25c5db", "files_name": [".hgtags"]}, {"commit_id": "834eafbdd17a618aab4bbc6f4f58184c85006cbb", "commit_date": "Thu Jan 15 16:01:17 2015 -0500", "commit_message": "Merge head created for backport bug fix on latest_2013.08.12", "files_name": ["71ebf96cba8483a91c197241b161fa61a4fe7fe8 - Thu Jan 15 16:01:18 2015 -0500 : Bugfix: Skip extra wrapping around template-style macros.", "lib/galaxy/tools/__init__.py"]}, {"commit_id": "d194b20a6e840d68f1ca8f5a05b9b5f77332a5ac", "commit_date": "Thu Jan 15 16:01:20 2015 -0500", "commit_message": "Update tag latest_2013.11.04 for changeset 52a18b44474f", "files_name": [".hgtags"]}, {"commit_id": "327d77852b076015fd3c176bb9a54d73295405d7", "commit_date": "Thu Jan 15 16:01:22 2015 -0500", "commit_message": "Merge head created for backport bug fix on latest_2013.11.04", "files_name": ["baf4b18556d6273005abbdf7021441f185160665 - Thu Jan 15 16:01:23 2015 -0500 : Bugfix: Skip extra wrapping around template-style macros.", "lib/galaxy/tools/evaluation.py"]}, {"commit_id": "3b40397068eb183209dffe5578584dc7f6e1d0fc", "commit_date": "Thu Jan 15 16:01:25 2015 -0500", "commit_message": "Update tag latest_2014.02.10 for changeset 746db2bf4da0", "files_name": [".hgtags"]}, {"commit_id": "9e3da977a8d7af646dec5231bec496014e3ad2c5", "commit_date": "Thu Jan 15 16:01:25 2015 -0500", "commit_message": "Merge head created for backport bug fix on latest_2014.02.10", "files_name": ["ca34200e50b838436d47a44ab33e2bb12adf6718 - Thu Jan 15 16:01:27 2015 -0500 : Bugfix: Skip extra wrapping around template-style macros.", "lib/galaxy/tools/evaluation.py"]}, {"commit_id": "ba7880683bdb6bac0fb868a3328665731dd71327", "commit_date": "Thu Jan 15 16:01:28 2015 -0500", "commit_message": "Update tag latest_2014.04.14 for changeset b2c0570f52e1", "files_name": [".hgtags"]}, {"commit_id": "29beb00aa7b78fefb2debb264f7d1dfd02a02d00", "commit_date": "Thu Jan 15 16:01:29 2015 -0500", "commit_message": "Merge head created for backport bug fix on latest_2014.04.14", "files_name": ["cf64a220d2417dd104366d867d212ca37f256b74 - Thu Jan 15 16:01:30 2015 -0500 : Bugfix: Skip extra wrapping around template-style macros.", "lib/galaxy/tools/evaluation.py"]}, {"commit_id": "9dcee0218ba2620b2e865ea056469c4a93c78eb2", "commit_date": "Thu Jan 15 16:01:31 2015 -0500", "commit_message": "Update tag latest_2014.06.02 for changeset c52dc4c72b77", "files_name": [".hgtags"]}, {"commit_id": "bf4381a5818a2d022de1910ccd646f468af116fb", "commit_date": "Thu Jan 15 16:01:32 2015 -0500", "commit_message": "Merge head created for backport bug fix on latest_2014.06.02", "files_name": ["6304c816c9c5bd1bd60bedae42aa70b98bfeb1cf - Thu Jan 15 16:01:33 2015 -0500 : Bugfix: Skip extra wrapping around template-style macros.", "lib/galaxy/tools/evaluation.py"]}, {"commit_id": "88d4c63f8ffe00fdd7994455d6bacde96e542ef1", "commit_date": "Thu Jan 15 16:01:34 2015 -0500", "commit_message": "Update tag latest_2014.08.11 for changeset 6d6d7f8b3217", "files_name": [".hgtags"]}, {"commit_id": "cbfab74060025639a98054ca9eec29d2edcf7cea", "commit_date": "Thu Jan 15 16:01:34 2015 -0500", "commit_message": "Merge head created for backport bug fix on latest_2014.08.11", "files_name": ["ed0e764320d3fd55439d0a50151f4fe5f8a89f45 - Thu Jan 15 16:01:35 2015 -0500 : Bugfix: Skip extra wrapping around template-style macros.", "lib/galaxy/tools/evaluation.py"]}, {"commit_id": "bc713eb6eed04177efc5465a669f668469da801d", "commit_date": "Thu Jan 15 16:01:36 2015 -0500", "commit_message": "Update tag latest_2014.10.06 for changeset 782fa60fc654", "files_name": [".hgtags"]}, {"commit_id": "873ce9e0d5ae28f270b841c3ce9d941665e266d4", "commit_date": "Thu Jan 15 16:01:37 2015 -0500", "commit_message": "Merge head created for backport bug fix on latest_2014.10.06", "files_name": ["f5f4049dead495bcebe61a3ddf029d48d43001bf - Thu Jan 15 16:09:24 2015 -0500 : Fix for LibraryDatasetToolParameter", "lib/galaxy/tools/parameters/basic.py"]}, {"commit_id": "8adba8c9d3f05e1255d0e141608bd1ef8c729fcf", "commit_date": "Fri Jan 16 09:00:18 2015 -0500", "commit_message": "Update tag latest_2015.01.13 for changeset f7235d0ad14f", "files_name": [".hgtags"]}, {"commit_id": "e1dd5b515d108fb93a4be37627944256a89d0a9b", "commit_date": "Sun Jan 18 07:39:43 2015 -0600", "commit_message": "Merged in guerler/guerler-galaxy-central/stable (pull request #639)", "files_name": ["27f66b3b848ea2871f38dca4c278a1b8afc84125 - Mon Jan 19 09:00:26 2015 -0500 : Update tag latest_2015.01.13 for changeset 0cb6cec4ee57", ".hgtags"]}, {"commit_id": "fe273faa4338786342cad5eca9e9ba81572d3dd6", "commit_date": "Mon Jan 19 15:43:13 2015 -0500", "commit_message": "galaxy_config.root defaulted from '' to '/' somewhere along the way breaking these links", "files_name": ["client/galaxy/scripts/galaxy.masthead.js", "static/scripts/galaxy.masthead.js", "static/scripts/packed/galaxy.masthead.js"]}, {"commit_id": "1a028337dc7afebd452931eb1aed5422546cddd2", "commit_date": "Mon Jan 19 15:45:50 2015 -0500", "commit_message": "Merged in dannon/galaxy-central/stable (pull request #635)", "files_name": ["071400aeeb4e9664c342d588d07ad3a38cd76a59 - Tue Jan 20 09:00:22 2015 -0500 : Update tag latest_2015.01.13 for changeset f1cd1bec8272", ".hgtags"]}, {"commit_id": "447a48c043c71d80587b538b51ccf32e811a4b49", "commit_date": "Tue Jan 20 10:12:35 2015 -0500", "commit_message": "add ?use_panels=True to the login and register links in WF/VIZ", "files_name": ["client/galaxy/scripts/galaxy.masthead.js", "static/scripts/galaxy.masthead.js", "static/scripts/packed/galaxy.masthead.js"]}, {"commit_id": "beec87f2326937cfbf3543a5238d3a412f7da91f", "commit_date": "Tue Jan 20 12:34:26 2015 -0500", "commit_message": "Merged in martenson/galaxy-central-stablefix/stable (pull request #641)", "files_name": ["e5db731083176b801f499a1d55fe7f171c4ce954 - Wed Jan 21 09:00:27 2015 -0500 : Update tag latest_2015.01.13 for changeset 507edf82ad1d", ".hgtags"]}, {"commit_id": "c446be0d6df79525582bb4fa0838feaa18a00c0a", "commit_date": "Wed Jan 21 11:33:40 2015 -0500", "commit_message": "Fix to allow anon users to view pages with histories", "files_name": ["lib/galaxy/web/base/controller.py", "lib/galaxy/webapps/galaxy/controllers/page.py"]}, {"commit_id": "df6a7d8b816c9419a28f685a916b27cbb23cf095", "commit_date": "Wed Jan 21 11:46:38 2015 -0500", "commit_message": "Merged in carlfeberhard/carlfeberhard-galaxy-central-stable/stable (pull request #642)", "files_name": ["21409d676cf88c08cbd1981bc80f7a80833af025 - Wed Jan 21 16:00:24 2015 -0500 : Update tag latest_2015.01.13 for changeset 4039bfd5584a", ".hgtags"]}, {"commit_id": "43b2f4ffa1032b3b52cba5c574a177ddf993624d", "commit_date": "Tue Jan 27 14:32:16 2015 -0500", "commit_message": "Remove paragraph tags from history/rename feedback messages", "files_name": ["lib/galaxy/webapps/galaxy/controllers/history.py"]}, {"commit_id": "9bed3041f563b823ab2c55d7c478ee17345e1e19", "commit_date": "Wed Jan 28 09:00:26 2015 -0500", "commit_message": "Update tag latest_2015.01.13 for changeset 1b96d3a4ff28", "files_name": [".hgtags"]}, {"commit_id": "2f558e3919a90b9faa269a4211cbedd4cc937572", "commit_date": "Wed Jan 28 10:38:22 2015 -0500", "commit_message": "Allow BAM's set_meta() to use samtools 1 to generate the index, if the samtools found on $PATH is samtools 1.", "files_name": ["lib/galaxy/datatypes/binary.py"]}, {"commit_id": "fffdaf115450e3d9181cd547e6ba39aecd9557ca", "commit_date": "Wed Jan 28 11:58:23 2015 -0500", "commit_message": "Use a symlink to the dataset in the same directory as the MetadataTempFile as the input to samtools index so there's no clobber risk. Thanks Dan.", "files_name": ["lib/galaxy/datatypes/binary.py"]}, {"commit_id": "9252d11d269773ac039d709393d5ffd92c145464", "commit_date": "Wed Jan 28 13:25:11 2015 -0500", "commit_message": "Use subprocess.call() rather than Popen(), thanks Nicola.", "files_name": ["lib/galaxy/datatypes/binary.py"]}, {"commit_id": "93d377f9d4abac6215676b3a0bbd1cf6fec1a69b", "commit_date": "Thu Jan 29 09:00:24 2015 -0500", "commit_message": "Update tag latest_2015.01.13 for changeset a481d17c0448", "files_name": [".hgtags"]}, {"commit_id": "9e18e1ab7269547dd9f12dcd7e713e47df27f5d8", "commit_date": "Thu Jan 29 11:23:35 2015 -0500", "commit_message": "Set time to epoch instead of throwing server error if repository_metadata.time_last_tested is unset.", "files_name": ["lib/galaxy/webapps/tool_shed/controllers/repository.py"]}, {"commit_id": "9c538ee4eb0941c58dd2939fd4c5d8fc93b6fc16", "commit_date": "Fri Jan 30 13:00:22 2015 -0500", "commit_message": "Update tag latest_2015.01.13 for changeset c5e7535b4d22", "files_name": [".hgtags"]}, {"commit_id": "c874b2306aa69acef36d5b10a53749f267413ad8", "commit_date": "Mon Feb 2 09:10:43 2015 -0500", "commit_message": "Merged in jmchilton/galaxy-central-backport-2/stable (pull request #638)", "files_name": ["57ae0139bb71cd4601253ea9b646092a379e294b - Mon Feb 2 14:00:23 2015 -0500 : Update tag latest_2015.01.13 for changeset fd75aaee91cf", ".hgtags"]}, {"commit_id": "48f77dc742acf01ddbafafcc4634e69378f1f020", "commit_date": "Mon Feb 2 16:21:00 2015 -0500", "commit_message": "Fix for preventing non-admins from running data managers via the api.", "files_name": ["lib/galaxy/tools/__init__.py", "lib/galaxy/tools/actions/__init__.py", "lib/galaxy/webapps/galaxy/api/tools.py", "lib/galaxy/webapps/galaxy/controllers/tool_runner.py"]}, {"commit_id": "39baead5f34344ee282221f5b421fe92b4560cc9", "commit_date": "Mon Feb 2 16:30:49 2015 -0500", "commit_message": "Merged in dan/galaxy-central-prs/stable (pull request #657)", "files_name": ["accd6f0810fed2a68b923c3bd98c8122e24721e6 - Tue Feb 3 09:00:27 2015 -0500 : Update tag latest_2015.01.13 for changeset 5e4060f5ac7a", ".hgtags"]}, {"commit_id": "51153c3d66bdc628048f51c55b26278d6be198f1", "commit_date": "Thu Feb 5 12:14:01 2015 -0500", "commit_message": "Workflow scheduling delay fix. There were problems if all three of these conditions were met - 1) workflow from GUI, 2) workflow evaluation delayed, and 3) a delayed step was connected to a input dataset. This fixes these workflows.", "files_name": ["lib/galaxy/model/__init__.py", "lib/galaxy/workflow/modules.py", "lib/galaxy/workflow/run_request.py"]}], "parents": [{"commit_id_before": "65ef16fc015d78ae4006483df405a802a6a12c2e", "url_before": "https://api.github.com/repos/blankenberg/galaxy-data-resource/commits/65ef16fc015d78ae4006483df405a802a6a12c2e", "html_url_before": "https://github.com/blankenberg/galaxy-data-resource/commit/65ef16fc015d78ae4006483df405a802a6a12c2e"}], "details": [{"raw_url": "https://github.com/blankenberg/galaxy-data-resource/raw/50d65f45d3f5be5d1fbff2e45ac5cec075f07d42/lib%2Fgalaxy%2Fdatatypes%2Fmetadata.py", "code": "\"\"\"\nGalaxy Metadata\n\n\"\"\"\n\nimport copy\nimport cPickle\nimport json\nimport os\nimport shutil\nimport sys\nimport tempfile\nimport weakref\n\nfrom os.path import abspath\n\nfrom galaxy import eggs\neggs.require( \"SQLAlchemy >= 0.4\" )\nfrom sqlalchemy.orm import object_session\n\nimport galaxy.model\nfrom galaxy.util import listify\nfrom galaxy.util.object_wrapper import sanitize_lists_to_string\nfrom galaxy.util import stringify_dictionary_keys\nfrom galaxy.util import string_as_bool\nfrom galaxy.util import in_directory\nfrom galaxy.util.odict import odict\nfrom galaxy.web import form_builder\n\nimport logging\nlog = logging.getLogger(__name__)\n\nSTATEMENTS = \"__galaxy_statements__\" #this is the name of the property in a Datatype class where new metadata spec element Statements are stored\n\nclass Statement( object ):\n    \"\"\"\n    This class inserts its target into a list in the surrounding\n    class.  the data.Data class has a metaclass which executes these\n    statements.  This is how we shove the metadata element spec into\n    the class.\n    \"\"\"\n    def __init__( self, target ):\n        self.target = target\n\n    def __call__( self, *args, **kwargs ):\n        class_locals = sys._getframe( 1 ).f_locals #get the locals dictionary of the frame object one down in the call stack (i.e. the Datatype class calling MetadataElement)\n        statements = class_locals.setdefault( STATEMENTS, [] ) #get and set '__galaxy_statments__' to an empty list if not in locals dict\n        statements.append( ( self, args, kwargs ) ) #add Statement containing info to populate a MetadataElementSpec\n\n    @classmethod\n    def process( cls, element ):\n        for statement, args, kwargs in getattr( element, STATEMENTS, [] ):\n            statement.target( element, *args, **kwargs ) #statement.target is MetadataElementSpec, element is a Datatype class\n\n\nclass MetadataCollection( object ):\n    \"\"\"\n    MetadataCollection is not a collection at all, but rather a proxy\n    to the real metadata which is stored as a Dictionary. This class\n    handles processing the metadata elements when they are set and\n    retrieved, returning default values in cases when metadata is not set.\n    \"\"\"\n    def __init__(self, parent ):\n        self.parent = parent\n        #initialize dict if needed\n        if self.parent._metadata is None:\n            self.parent._metadata = {}\n\n    def get_parent( self ):\n        if \"_parent\" in self.__dict__:\n            return self.__dict__[\"_parent\"]()\n        return None\n\n    def set_parent( self, parent ):\n        self.__dict__[\"_parent\"] = weakref.ref( parent ) # use weakref to prevent a circular reference interfering with garbage collection: hda/lda (parent) <--> MetadataCollection (self) ; needs to be hashable, so cannot use proxy.\n    parent = property( get_parent, set_parent )\n\n    @property\n    def spec( self ):\n        return self.parent.datatype.metadata_spec\n\n    def __iter__( self ):\n        return self.parent._metadata.__iter__()\n\n    def get( self, key, default=None ):\n        try:\n            return self.__getattr__( key ) or default\n        except:\n            return default\n\n    def items(self):\n        return iter( [ ( k, self.get( k ) ) for k in self.spec.iterkeys() ] )\n\n    def __str__(self):\n        return dict( self.items() ).__str__()\n\n    def __nonzero__( self ):\n        return bool( self.parent._metadata )\n\n    def __getattr__( self, name ):\n        if name in self.spec:\n            if name in self.parent._metadata:\n                return self.spec[name].wrap( self.parent._metadata[name], object_session( self.parent ) )\n            return self.spec[name].wrap( self.spec[name].default, object_session( self.parent ) )\n        if name in self.parent._metadata:\n            return self.parent._metadata[name]\n\n    def __setattr__( self, name, value ):\n        if name == \"parent\":\n            return self.set_parent( value )\n        else:\n            if name in self.spec:\n                self.parent._metadata[name] = self.spec[name].unwrap( value )\n            else:\n                self.parent._metadata[name] = value\n\n    def element_is_set( self, name ):\n        return bool( self.parent._metadata.get( name, False ) )\n\n    def get_html_by_name( self, name, **kwd ):\n        if name in self.spec:\n            rval = self.spec[name].param.get_html( value=getattr( self, name ), context=self, **kwd )\n            if rval is None:\n                return self.spec[name].no_value\n            return rval\n\n    def make_dict_copy( self, to_copy ):\n        \"\"\"Makes a deep copy of input iterable to_copy according to self.spec\"\"\"\n        rval = {}\n        for key, value in to_copy.items():\n            if key in self.spec:\n                rval[key] = self.spec[key].param.make_copy( value, target_context=self, source_context=to_copy )\n        return rval\n\n    def from_JSON_dict( self, filename=None, path_rewriter=None, json_dict=None ):\n        dataset = self.parent\n        if filename is not None:\n            log.debug( 'loading metadata from file for: %s %s' % ( dataset.__class__.__name__, dataset.id ) )\n            JSONified_dict = json.load( open( filename ) )\n        elif json_dict is not None:\n            log.debug( 'loading metadata from dict for: %s %s' % ( dataset.__class__.__name__, dataset.id ) )\n            if isinstance( json_dict, basestring ):\n                JSONified_dict = json.loads( json_dict )\n            elif isinstance( json_dict, dict ):\n                JSONified_dict = json_dict\n            else:\n                raise ValueError( \"json_dict must be either a dictionary or a string, got %s.\"  % ( type( json_dict ) ) )\n        else:\n            raise ValueError( \"You must provide either a filename or a json_dict\" )\n        for name, spec in self.spec.items():\n            if name in JSONified_dict:\n                from_ext_kwds = {}\n                external_value = JSONified_dict[ name ]\n                param = spec.param\n                if isinstance( param, FileParameter ):\n                    from_ext_kwds[ 'path_rewriter' ] = path_rewriter\n                dataset._metadata[ name ] = param.from_external_value( external_value, dataset, **from_ext_kwds )\n            elif name in dataset._metadata:\n                #if the metadata value is not found in our externally set metadata but it has a value in the 'old'\n                #metadata associated with our dataset, we'll delete it from our dataset's metadata dict\n                del dataset._metadata[ name ]\n\n    def to_JSON_dict( self, filename=None ):\n        #galaxy.model.customtypes.json_encoder.encode()\n        meta_dict = {}\n        dataset_meta_dict = self.parent._metadata\n        for name, spec in self.spec.items():\n            if name in dataset_meta_dict:\n                meta_dict[ name ] = spec.param.to_external_value( dataset_meta_dict[ name ] )\n        if filename is None:\n            return json.dumps( meta_dict )\n        json.dump( meta_dict, open( filename, 'wb+' ) )\n\n    def __getstate__( self ):\n        return None #cannot pickle a weakref item (self._parent), when data._metadata_collection is None, it will be recreated on demand\n\n\nclass MetadataSpecCollection( odict ):\n    \"\"\"\n    A simple extension of dict which allows cleaner access to items\n    and allows the values to be iterated over directly as if it were a\n    list.  append() is also implemented for simplicity and does not\n    \"append\".\n    \"\"\"\n    def __init__( self, dict = None ):\n        odict.__init__( self, dict = None )\n\n    def append( self, item ):\n        self[item.name] = item\n\n    def iter( self ):\n        return self.itervalues()\n\n    def __getattr__( self, name ):\n        return self.get( name )\n\n    def __repr__( self ):\n        # force elements to draw with __str__ for sphinx-apidoc\n        return ', '.join([ item.__str__() for item in self.iter() ])\n\n\nclass MetadataParameter( object ):\n    def __init__( self, spec ):\n        self.spec = spec\n\n    def get_html_field( self, value=None, context=None, other_values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n        return form_builder.TextField( self.spec.name, value=value )\n\n    def get_html( self, value, context=None, other_values=None, **kwd ):\n        \"\"\"\n        The \"context\" is simply the metadata collection/bunch holding\n        this piece of metadata. This is passed in to allow for\n        metadata to validate against each other (note: this could turn\n        into a huge, recursive mess if not done with care). For\n        example, a column assignment should validate against the\n        number of columns in the dataset.\n        \"\"\"\n        context = context or {}\n        other_values = other_values or {}\n\n        if self.spec.get(\"readonly\"):\n            return value\n        if self.spec.get(\"optional\"):\n            checked = False\n            if value: checked = \"true\"\n            checkbox = form_builder.CheckboxField( \"is_\" + self.spec.name, checked=checked )\n            return checkbox.get_html() + self.get_html_field( value=value, context=context, other_values=other_values, **kwd ).get_html()\n        else:\n            return self.get_html_field( value=value, context=context, other_values=other_values, **kwd ).get_html()\n\n    def to_string( self, value ):\n        return str( value )\n\n    def to_safe_string( self, value ):\n        return sanitize_lists_to_string( self.to_string( value ) )\n\n    def make_copy( self, value, target_context = None, source_context = None ):\n        return copy.deepcopy( value )\n\n    @classmethod\n    def marshal ( cls, value ):\n        \"\"\"\n        This method should/can be overridden to convert the incoming\n        value to whatever type it is supposed to be.\n        \"\"\"\n        return value\n\n    def validate( self, value ):\n        \"\"\"\n        Throw an exception if the value is invalid.\n        \"\"\"\n        pass\n\n    def unwrap( self, form_value ):\n        \"\"\"\n        Turns a value into its storable form.\n        \"\"\"\n        value = self.marshal( form_value )\n        self.validate( value )\n        return value\n\n    def wrap( self, value, session ):\n        \"\"\"\n        Turns a value into its usable form.\n        \"\"\"\n        return value\n\n    def from_external_value( self, value, parent ):\n        \"\"\"\n        Turns a value read from an external dict into its value to be pushed directly into the metadata dict.\n        \"\"\"\n        return value\n\n    def to_external_value( self, value ):\n        \"\"\"\n        Turns a value read from a metadata into its value to be pushed directly into the external dict.\n        \"\"\"\n        return value\n\n\nclass MetadataElementSpec( object ):\n    \"\"\"\n    Defines a metadata element and adds it to the metadata_spec (which\n    is a MetadataSpecCollection) of datatype.\n    \"\"\"\n    def __init__( self, datatype,\n                  name=None, desc=None, param=MetadataParameter, default=None, no_value = None,\n                  visible=True, set_in_upload = False, **kwargs ):\n        self.name = name\n        self.desc = desc or name\n        self.default = default\n        self.no_value = no_value\n        self.visible = visible\n        self.set_in_upload = set_in_upload\n        # Catch-all, allows for extra attributes to be set\n        self.__dict__.update(kwargs)\n        # set up param last, as it uses values set above\n        self.param = param( self )\n        # add spec element to the spec\n        datatype.metadata_spec.append( self )\n\n    def get( self, name, default=None ):\n        return self.__dict__.get(name, default)\n\n    def wrap( self, value, session ):\n        \"\"\"\n        Turns a stored value into its usable form.\n        \"\"\"\n        return self.param.wrap( value, session )\n\n    def unwrap( self, value ):\n        \"\"\"\n        Turns an incoming value into its storable form.\n        \"\"\"\n        return self.param.unwrap( value )\n\n    def __str__( self ):\n        #TODO??: assuming param is the class of this MetadataElementSpec - add the plain class name for that\n        spec_dict = dict( param_class=self.param.__class__.__name__ )\n        spec_dict.update( self.__dict__ )\n        return ( \"{name} ({param_class}): {desc}, defaults to '{default}'\".format( **spec_dict ) )\n\n# create a statement class that, when called,\n#   will add a new MetadataElementSpec to a class's metadata_spec\nMetadataElement = Statement( MetadataElementSpec )\n\n\n\"\"\"\nMetadataParameter sub-classes.\n\"\"\"\n\nclass SelectParameter( MetadataParameter ):\n    def __init__( self, spec ):\n        MetadataParameter.__init__( self, spec )\n        self.values = self.spec.get( \"values\" )\n        self.multiple = string_as_bool( self.spec.get( \"multiple\" ) )\n\n    def to_string( self, value ):\n        if value in [ None, [] ]:\n            return str( self.spec.no_value )\n        if not isinstance( value, list ):\n            value = [value]\n        return \",\".join( map( str, value ) )\n\n    def get_html_field( self, value=None, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        field = form_builder.SelectField( self.spec.name, multiple=self.multiple, display=self.spec.get(\"display\") )\n        if self.values:\n            value_list = self.values\n        elif values:\n            value_list = values\n        elif value:\n            value_list = [ ( v, v ) for v in listify( value )]\n        else:\n            value_list = []\n        for val, label in value_list:\n            try:\n                if ( self.multiple and val in value ) or ( not self.multiple and val == value ):\n                    field.add_option( label, val, selected=True )\n                else:\n                    field.add_option( label, val, selected=False )\n            except TypeError:\n                field.add_option( val, label, selected=False )\n        return field\n\n    def get_html( self, value, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        if self.spec.get(\"readonly\"):\n            if value in [ None, [] ]:\n                return str( self.spec.no_value )\n            return \", \".join( map( str, value ) )\n        return MetadataParameter.get_html( self, value, context=context, other_values=other_values, values=values, **kwd )\n\n    def wrap( self, value, session ):\n        value = self.marshal( value ) #do we really need this (wasteful)? - yes because we are not sure that all existing selects have been stored previously as lists. Also this will handle the case where defaults/no_values are specified and are single non-list values.\n        if self.multiple:\n            return value\n        elif value:\n            return value[0] #single select, only return the first value\n        return None\n\n    @classmethod\n    def marshal( cls, value ):\n        # Store select as list, even if single item\n        if value is None: return []\n        if not isinstance( value, list ): return [value]\n        return value\n\n\nclass DBKeyParameter( SelectParameter ):\n\n    def get_html_field( self, value=None, context=None, other_values=None, values=None, **kwd):\n        context = context or {}\n        other_values = other_values or {}\n        try:\n            values = kwd['trans'].app.genome_builds.get_genome_build_names( kwd['trans'] )\n        except KeyError:\n            pass\n        return super(DBKeyParameter, self).get_html_field( value, context, other_values, values, **kwd)\n\n    def get_html( self, value=None, context=None, other_values=None, values=None, **kwd):\n        context = context or {}\n        other_values = other_values or {}\n        try:\n            values = kwd['trans'].app.genome_builds.get_genome_build_names( kwd['trans'] )\n        except KeyError:\n            pass\n        return super(DBKeyParameter, self).get_html( value, context, other_values, values, **kwd)\n\n\nclass RangeParameter( SelectParameter ):\n\n    def __init__( self, spec ):\n        SelectParameter.__init__( self, spec )\n        # The spec must be set with min and max values\n        self.min = spec.get( \"min\" ) or 1\n        self.max = spec.get( \"max\" ) or 1\n        self.step = self.spec.get( \"step\" ) or 1\n\n    def get_html_field( self, value=None, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        if values is None:\n            values = zip( range( self.min, self.max, self.step ), range( self.min, self.max, self.step ))\n        return SelectParameter.get_html_field( self, value=value, context=context, other_values=other_values, values=values, **kwd )\n\n    def get_html( self, value, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        if values is None:\n            values = zip( range( self.min, self.max, self.step ), range( self.min, self.max, self.step ))\n        return SelectParameter.get_html( self, value, context=context, other_values=other_values, values=values, **kwd )\n\n    @classmethod\n    def marshal( cls, value ):\n        value = SelectParameter.marshal( value )\n        values = [ int(x) for x in value ]\n        return values\n\n\nclass ColumnParameter( RangeParameter ):\n\n    def get_html_field( self, value=None, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        if values is None and context:\n            column_range = range( 1, ( context.columns or 0 ) + 1, 1 )\n            values = zip( column_range, column_range )\n        return RangeParameter.get_html_field( self, value=value, context=context, other_values=other_values, values=values, **kwd )\n\n    def get_html( self, value, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        if values is None and context:\n            column_range = range( 1, ( context.columns or 0 ) + 1, 1 )\n            values = zip( column_range, column_range )\n        return RangeParameter.get_html( self, value, context=context, other_values=other_values, values=values, **kwd )\n\n\nclass ColumnTypesParameter( MetadataParameter ):\n\n    def to_string( self, value ):\n        return \",\".join( map( str, value ) )\n\n\nclass ListParameter( MetadataParameter ):\n\n    def to_string( self, value ):\n        return \",\".join( [str(x) for x in value] )\n\n\nclass DictParameter( MetadataParameter ):\n\n    def to_string( self, value ):\n        return  json.dumps( value )\n\n    def to_safe_string( self, value ):\n        # We do not sanitize json dicts\n        return json.safe_dumps( value )\n\n\nclass PythonObjectParameter( MetadataParameter ):\n\n    def to_string( self, value ):\n        if not value:\n            return self.spec._to_string( self.spec.no_value )\n        return self.spec._to_string( value )\n\n    def get_html_field( self, value=None, context=None, other_values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n        return form_builder.TextField( self.spec.name, value=self._to_string( value ) )\n\n    def get_html( self, value=None, context=None, other_values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n        return str( self )\n\n    @classmethod\n    def marshal( cls, value ):\n        return value\n\n\nclass FileParameter( MetadataParameter ):\n\n    def to_string( self, value ):\n        if not value:\n            return str( self.spec.no_value )\n        return value.file_name\n\n    def to_safe_string( self, value ):\n        # We do not sanitize file names\n        return self.to_string( value )\n\n    def get_html_field( self, value=None, context=None, other_values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n        return form_builder.TextField( self.spec.name, value=str( value.id ) )\n\n    def get_html( self, value=None, context=None, other_values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n        return \"<div>No display available for Metadata Files</div>\"\n\n    def wrap( self, value, session ):\n        if value is None:\n            return None\n        if isinstance( value, galaxy.model.MetadataFile ) or isinstance( value, MetadataTempFile ):\n            return value\n        mf = session.query( galaxy.model.MetadataFile ).get( value )\n        return mf\n\n    def make_copy( self, value, target_context, source_context ):\n        value = self.wrap( value, object_session( target_context.parent ) )\n        if value:\n            new_value = galaxy.model.MetadataFile( dataset = target_context.parent, name = self.spec.name )\n            object_session( target_context.parent ).add( new_value )\n            object_session( target_context.parent ).flush()\n            shutil.copy( value.file_name, new_value.file_name )\n            return self.unwrap( new_value )\n        return None\n\n    @classmethod\n    def marshal( cls, value ):\n        if isinstance( value, galaxy.model.MetadataFile ):\n            value = value.id\n        return value\n\n    def from_external_value( self, value, parent, path_rewriter=None ):\n        \"\"\"\n        Turns a value read from a external dict into its value to be pushed directly into the metadata dict.\n        \"\"\"\n        if MetadataTempFile.is_JSONified_value( value ):\n            value = MetadataTempFile.from_JSON( value )\n        if isinstance( value, MetadataTempFile ):\n            mf = parent.metadata.get( self.spec.name, None)\n            if mf is None:\n                mf = self.new_file( dataset = parent, **value.kwds )\n            # Ensure the metadata file gets updated with content\n            file_name = value.file_name\n            if path_rewriter:\n                # Job may have run with a different (non-local) tmp/working\n                # directory. Correct.\n                file_name = path_rewriter( file_name )\n            parent.dataset.object_store.update_from_file( mf, file_name=file_name, extra_dir='_metadata_files', extra_dir_at_root=True, alt_name=os.path.basename(mf.file_name) )\n            os.unlink( file_name )\n            value = mf.id\n        return value\n\n    def to_external_value( self, value ):\n        \"\"\"\n        Turns a value read from a metadata into its value to be pushed directly into the external dict.\n        \"\"\"\n        if isinstance( value, galaxy.model.MetadataFile ):\n            value = value.id\n        elif isinstance( value, MetadataTempFile ):\n            value = MetadataTempFile.to_JSON( value )\n        return value\n\n    def new_file( self, dataset = None, **kwds ):\n        if object_session( dataset ):\n            mf = galaxy.model.MetadataFile( name = self.spec.name, dataset = dataset, **kwds )\n            object_session( dataset ).add( mf )\n            object_session( dataset ).flush() #flush to assign id\n            return mf\n        else:\n            #we need to make a tmp file that is accessable to the head node,\n            #we will be copying its contents into the MetadataFile objects filename after restoring from JSON\n            #we do not include 'dataset' in the kwds passed, as from_JSON_value() will handle this for us\n            return MetadataTempFile( **kwds )\n\n\n#This class is used when a database file connection is not available\nclass MetadataTempFile( object ):\n    tmp_dir = 'database/tmp' #this should be overwritten as necessary in calling scripts\n\n    def __init__( self, **kwds ):\n        self.kwds = kwds\n        self._filename = None\n\n    @property\n    def file_name( self ):\n        if self._filename is None:\n            #we need to create a tmp file, accessable across all nodes/heads, save the name, and return it\n            self._filename = abspath( tempfile.NamedTemporaryFile( dir = self.tmp_dir, prefix = \"metadata_temp_file_\" ).name )\n            open( self._filename, 'wb+' ) #create an empty file, so it can't be reused using tempfile\n        return self._filename\n\n    def to_JSON( self ):\n        return { '__class__':self.__class__.__name__, 'filename':self.file_name, 'kwds':self.kwds }\n\n    @classmethod\n    def from_JSON( cls, json_dict ):\n        #need to ensure our keywords are not unicode\n        rval = cls( **stringify_dictionary_keys( json_dict['kwds'] ) )\n        rval._filename = json_dict['filename']\n        return rval\n\n    @classmethod\n    def is_JSONified_value( cls, value ):\n        return ( isinstance( value, dict ) and value.get( '__class__', None ) == cls.__name__ )\n\n    @classmethod\n    def cleanup_from_JSON_dict_filename( cls, filename ):\n        try:\n            for key, value in json.load( open( filename ) ).items():\n                if cls.is_JSONified_value( value ):\n                    value = cls.from_JSON( value )\n                if isinstance( value, cls ) and os.path.exists( value.file_name ):\n                    log.debug( 'Cleaning up abandoned MetadataTempFile file: %s' % value.file_name )\n                    os.unlink( value.file_name )\n        except Exception, e:\n            log.debug( 'Failed to cleanup MetadataTempFile temp files from %s: %s' % ( filename, e ) )\n\n\n#Class with methods allowing set_meta() to be called externally to the Galaxy head\nclass JobExternalOutputMetadataWrapper( object ):\n    #this class allows access to external metadata filenames for all outputs associated with a job\n    #We will use JSON as the medium of exchange of information, except for the DatasetInstance object which will use pickle (in the future this could be JSONified as well)\n\n    def __init__( self, job ):\n        self.job_id = job.id\n\n    def get_output_filenames_by_dataset( self, dataset, sa_session ):\n        if isinstance( dataset, galaxy.model.HistoryDatasetAssociation ):\n            return sa_session.query( galaxy.model.JobExternalOutputMetadata ) \\\n                             .filter_by( job_id = self.job_id, history_dataset_association_id = dataset.id ) \\\n                             .first() #there should only be one or None\n        elif isinstance( dataset, galaxy.model.LibraryDatasetDatasetAssociation ):\n            return sa_session.query( galaxy.model.JobExternalOutputMetadata ) \\\n                             .filter_by( job_id = self.job_id, library_dataset_dataset_association_id = dataset.id ) \\\n                             .first() #there should only be one or None\n        return None\n\n    def get_dataset_metadata_key( self, dataset ):\n        # Set meta can be called on library items and history items,\n        # need to make different keys for them, since ids can overlap\n        return \"%s_%d\" % ( dataset.__class__.__name__, dataset.id )\n\n    def setup_external_metadata( self, datasets, sa_session, exec_dir=None, tmp_dir=None, dataset_files_path=None,\n                                 output_fnames=None, config_root=None, config_file=None, datatypes_config=None, job_metadata=None, compute_tmp_dir=None, kwds=None ):\n        kwds = kwds or {}\n        if tmp_dir is None:\n            tmp_dir = MetadataTempFile.tmp_dir\n\n        # path is calculated for Galaxy, may be different on compute - rewrite\n        # for the compute server.\n        def metadata_path_on_compute(path):\n            compute_path = path\n            if compute_tmp_dir and tmp_dir and in_directory(path, tmp_dir):\n                path_relative = os.path.relpath(path, tmp_dir)\n                compute_path = os.path.join(compute_tmp_dir, path_relative)\n            return compute_path\n\n        #fill in metadata_files_dict and return the command with args required to set metadata\n        def __metadata_files_list_to_cmd_line( metadata_files ):\n            def __get_filename_override():\n                if output_fnames:\n                    for dataset_path in output_fnames:\n                        if dataset_path.false_path and dataset_path.real_path == metadata_files.dataset.file_name:\n                            return dataset_path.false_path\n                return \"\"\n            line = \"%s,%s,%s,%s,%s,%s\" % (\n                metadata_path_on_compute(metadata_files.filename_in),\n                metadata_path_on_compute(metadata_files.filename_kwds),\n                metadata_path_on_compute(metadata_files.filename_out),\n                metadata_path_on_compute(metadata_files.filename_results_code),\n                __get_filename_override(),\n                metadata_path_on_compute(metadata_files.filename_override_metadata),\n            )\n            return line\n        if not isinstance( datasets, list ):\n            datasets = [ datasets ]\n        if exec_dir is None:\n            exec_dir = os.path.abspath( os.getcwd() )\n        if dataset_files_path is None:\n            dataset_files_path = galaxy.model.Dataset.file_path\n        if config_root is None:\n            config_root = os.path.abspath( os.getcwd() )\n        if datatypes_config is None:\n            raise Exception( 'In setup_external_metadata, the received datatypes_config is None.' )\n            datatypes_config = 'datatypes_conf.xml'\n        metadata_files_list = []\n        for dataset in datasets:\n            key = self.get_dataset_metadata_key( dataset )\n            #future note:\n            #wonkiness in job execution causes build command line to be called more than once\n            #when setting metadata externally, via 'auto-detect' button in edit attributes, etc.,\n            #we don't want to overwrite (losing the ability to cleanup) our existing dataset keys and files,\n            #so we will only populate the dictionary once\n            metadata_files = self.get_output_filenames_by_dataset( dataset, sa_session )\n            if not metadata_files:\n                metadata_files = galaxy.model.JobExternalOutputMetadata( dataset = dataset)\n                metadata_files.job_id = self.job_id\n                #we are using tempfile to create unique filenames, tempfile always returns an absolute path\n                #we will use pathnames relative to the galaxy root, to accommodate instances where the galaxy root\n                #is located differently, i.e. on a cluster node with a different filesystem structure\n\n                #file to store existing dataset\n                metadata_files.filename_in = abspath( tempfile.NamedTemporaryFile( dir = tmp_dir, prefix = \"metadata_in_%s_\" % key ).name )\n\n                #FIXME: HACK\n                #sqlalchemy introduced 'expire_on_commit' flag for sessionmaker at version 0.5x\n                #This may be causing the dataset attribute of the dataset_association object to no-longer be loaded into memory when needed for pickling.\n                #For now, we'll simply 'touch' dataset_association.dataset to force it back into memory.\n                dataset.dataset #force dataset_association.dataset to be loaded before pickling\n                #A better fix could be setting 'expire_on_commit=False' on the session, or modifying where commits occur, or ?\n\n                cPickle.dump( dataset, open( metadata_files.filename_in, 'wb+' ) )\n                #file to store metadata results of set_meta()\n                metadata_files.filename_out = abspath( tempfile.NamedTemporaryFile( dir = tmp_dir, prefix = \"metadata_out_%s_\" % key ).name )\n                open( metadata_files.filename_out, 'wb+' ) # create the file on disk, so it cannot be reused by tempfile (unlikely, but possible)\n                #file to store a 'return code' indicating the results of the set_meta() call\n                #results code is like (True/False - if setting metadata was successful/failed , exception or string of reason of success/failure )\n                metadata_files.filename_results_code = abspath( tempfile.NamedTemporaryFile( dir = tmp_dir, prefix = \"metadata_results_%s_\" % key ).name )\n                json.dump( ( False, 'External set_meta() not called' ), open( metadata_files.filename_results_code, 'wb+' ) ) # create the file on disk, so it cannot be reused by tempfile (unlikely, but possible)\n                #file to store kwds passed to set_meta()\n                metadata_files.filename_kwds = abspath( tempfile.NamedTemporaryFile( dir = tmp_dir, prefix = \"metadata_kwds_%s_\" % key ).name )\n                json.dump( kwds, open( metadata_files.filename_kwds, 'wb+' ), ensure_ascii=True )\n                #existing metadata file parameters need to be overridden with cluster-writable file locations\n                metadata_files.filename_override_metadata = abspath( tempfile.NamedTemporaryFile( dir = tmp_dir, prefix = \"metadata_override_%s_\" % key ).name )\n                open( metadata_files.filename_override_metadata, 'wb+' ) # create the file on disk, so it cannot be reused by tempfile (unlikely, but possible)\n                override_metadata = []\n                for meta_key, spec_value in dataset.metadata.spec.iteritems():\n                    if isinstance( spec_value.param, FileParameter ) and dataset.metadata.get( meta_key, None ) is not None:\n                        metadata_temp = MetadataTempFile()\n                        shutil.copy( dataset.metadata.get( meta_key, None ).file_name, metadata_temp.file_name )\n                        override_metadata.append( ( meta_key, metadata_temp.to_JSON() ) )\n                json.dump( override_metadata, open( metadata_files.filename_override_metadata, 'wb+' ) )\n                #add to session and flush\n                sa_session.add( metadata_files )\n                sa_session.flush()\n            metadata_files_list.append( metadata_files )\n        #return command required to build\n        return \"%s %s %s %s %s %s %s %s\" % ( os.path.join( exec_dir, 'set_metadata.sh' ), dataset_files_path, compute_tmp_dir or tmp_dir, config_root, config_file, datatypes_config, job_metadata, \" \".join( map( __metadata_files_list_to_cmd_line, metadata_files_list ) ) )\n\n    def external_metadata_set_successfully( self, dataset, sa_session ):\n        metadata_files = self.get_output_filenames_by_dataset( dataset, sa_session )\n        if not metadata_files:\n            return False # this file doesn't exist\n        rval, rstring = json.load( open( metadata_files.filename_results_code ) )\n        if not rval:\n            log.debug( 'setting metadata externally failed for %s %s: %s' % ( dataset.__class__.__name__, dataset.id, rstring ) )\n        return rval\n\n    def cleanup_external_metadata( self, sa_session ):\n        log.debug( 'Cleaning up external metadata files' )\n        for metadata_files in sa_session.query( galaxy.model.Job ).get( self.job_id ).external_output_metadata:\n            #we need to confirm that any MetadataTempFile files were removed, if not we need to remove them\n            #can occur if the job was stopped before completion, but a MetadataTempFile is used in the set_meta\n            MetadataTempFile.cleanup_from_JSON_dict_filename( metadata_files.filename_out )\n            dataset_key = self.get_dataset_metadata_key( metadata_files.dataset )\n            for key, fname in [ ( 'filename_in', metadata_files.filename_in ), ( 'filename_out', metadata_files.filename_out ), ( 'filename_results_code', metadata_files.filename_results_code ), ( 'filename_kwds', metadata_files.filename_kwds ), ( 'filename_override_metadata', metadata_files.filename_override_metadata ) ]:\n                try:\n                    os.remove( fname )\n                except Exception, e:\n                    log.debug( 'Failed to cleanup external metadata file (%s) for %s: %s' % ( key, dataset_key, e ) )\n\n    def set_job_runner_external_pid( self, pid, sa_session ):\n        for metadata_files in sa_session.query( galaxy.model.Job ).get( self.job_id ).external_output_metadata:\n            metadata_files.job_runner_external_pid = pid\n            sa_session.add( metadata_files )\n            sa_session.flush()\n", "code_before": "\"\"\"\nGalaxy Metadata\n\n\"\"\"\n\nimport copy\nimport cPickle\nimport json\nimport os\nimport shutil\nimport sys\nimport tempfile\nimport weakref\n\nfrom os.path import abspath\n\nfrom galaxy import eggs\neggs.require( \"SQLAlchemy >= 0.4\" )\nfrom sqlalchemy.orm import object_session\n\nimport galaxy.model\nfrom galaxy.util import listify\nfrom galaxy.util import stringify_dictionary_keys\nfrom galaxy.util import string_as_bool\nfrom galaxy.util import in_directory\nfrom galaxy.util.odict import odict\nfrom galaxy.web import form_builder\n\nimport logging\nlog = logging.getLogger(__name__)\n\nSTATEMENTS = \"__galaxy_statements__\" #this is the name of the property in a Datatype class where new metadata spec element Statements are stored\n\nclass Statement( object ):\n    \"\"\"\n    This class inserts its target into a list in the surrounding\n    class.  the data.Data class has a metaclass which executes these\n    statements.  This is how we shove the metadata element spec into\n    the class.\n    \"\"\"\n    def __init__( self, target ):\n        self.target = target\n\n    def __call__( self, *args, **kwargs ):\n        class_locals = sys._getframe( 1 ).f_locals #get the locals dictionary of the frame object one down in the call stack (i.e. the Datatype class calling MetadataElement)\n        statements = class_locals.setdefault( STATEMENTS, [] ) #get and set '__galaxy_statments__' to an empty list if not in locals dict\n        statements.append( ( self, args, kwargs ) ) #add Statement containing info to populate a MetadataElementSpec\n\n    @classmethod\n    def process( cls, element ):\n        for statement, args, kwargs in getattr( element, STATEMENTS, [] ):\n            statement.target( element, *args, **kwargs ) #statement.target is MetadataElementSpec, element is a Datatype class\n\n\nclass MetadataCollection( object ):\n    \"\"\"\n    MetadataCollection is not a collection at all, but rather a proxy\n    to the real metadata which is stored as a Dictionary. This class\n    handles processing the metadata elements when they are set and\n    retrieved, returning default values in cases when metadata is not set.\n    \"\"\"\n    def __init__(self, parent ):\n        self.parent = parent\n        #initialize dict if needed\n        if self.parent._metadata is None:\n            self.parent._metadata = {}\n\n    def get_parent( self ):\n        if \"_parent\" in self.__dict__:\n            return self.__dict__[\"_parent\"]()\n        return None\n\n    def set_parent( self, parent ):\n        self.__dict__[\"_parent\"] = weakref.ref( parent ) # use weakref to prevent a circular reference interfering with garbage collection: hda/lda (parent) <--> MetadataCollection (self) ; needs to be hashable, so cannot use proxy.\n    parent = property( get_parent, set_parent )\n\n    @property\n    def spec( self ):\n        return self.parent.datatype.metadata_spec\n\n    def __iter__( self ):\n        return self.parent._metadata.__iter__()\n\n    def get( self, key, default=None ):\n        try:\n            return self.__getattr__( key ) or default\n        except:\n            return default\n\n    def items(self):\n        return iter( [ ( k, self.get( k ) ) for k in self.spec.iterkeys() ] )\n\n    def __str__(self):\n        return dict( self.items() ).__str__()\n\n    def __nonzero__( self ):\n        return bool( self.parent._metadata )\n\n    def __getattr__( self, name ):\n        if name in self.spec:\n            if name in self.parent._metadata:\n                return self.spec[name].wrap( self.parent._metadata[name], object_session( self.parent ) )\n            return self.spec[name].wrap( self.spec[name].default, object_session( self.parent ) )\n        if name in self.parent._metadata:\n            return self.parent._metadata[name]\n\n    def __setattr__( self, name, value ):\n        if name == \"parent\":\n            return self.set_parent( value )\n        else:\n            if name in self.spec:\n                self.parent._metadata[name] = self.spec[name].unwrap( value )\n            else:\n                self.parent._metadata[name] = value\n\n    def element_is_set( self, name ):\n        return bool( self.parent._metadata.get( name, False ) )\n\n    def get_html_by_name( self, name, **kwd ):\n        if name in self.spec:\n            rval = self.spec[name].param.get_html( value=getattr( self, name ), context=self, **kwd )\n            if rval is None:\n                return self.spec[name].no_value\n            return rval\n\n    def make_dict_copy( self, to_copy ):\n        \"\"\"Makes a deep copy of input iterable to_copy according to self.spec\"\"\"\n        rval = {}\n        for key, value in to_copy.items():\n            if key in self.spec:\n                rval[key] = self.spec[key].param.make_copy( value, target_context=self, source_context=to_copy )\n        return rval\n\n    def from_JSON_dict( self, filename=None, path_rewriter=None, json_dict=None ):\n        dataset = self.parent\n        if filename is not None:\n            log.debug( 'loading metadata from file for: %s %s' % ( dataset.__class__.__name__, dataset.id ) )\n            JSONified_dict = json.load( open( filename ) )\n        elif json_dict is not None:\n            log.debug( 'loading metadata from dict for: %s %s' % ( dataset.__class__.__name__, dataset.id ) )\n            if isinstance( json_dict, basestring ):\n                JSONified_dict = json.loads( json_dict )\n            elif isinstance( json_dict, dict ):\n                JSONified_dict = json_dict\n            else:\n                raise ValueError( \"json_dict must be either a dictionary or a string, got %s.\"  % ( type( json_dict ) ) )\n        else:\n            raise ValueError( \"You must provide either a filename or a json_dict\" )\n        for name, spec in self.spec.items():\n            if name in JSONified_dict:\n                from_ext_kwds = {}\n                external_value = JSONified_dict[ name ]\n                param = spec.param\n                if isinstance( param, FileParameter ):\n                    from_ext_kwds[ 'path_rewriter' ] = path_rewriter\n                dataset._metadata[ name ] = param.from_external_value( external_value, dataset, **from_ext_kwds )\n            elif name in dataset._metadata:\n                #if the metadata value is not found in our externally set metadata but it has a value in the 'old'\n                #metadata associated with our dataset, we'll delete it from our dataset's metadata dict\n                del dataset._metadata[ name ]\n\n    def to_JSON_dict( self, filename=None ):\n        #galaxy.model.customtypes.json_encoder.encode()\n        meta_dict = {}\n        dataset_meta_dict = self.parent._metadata\n        for name, spec in self.spec.items():\n            if name in dataset_meta_dict:\n                meta_dict[ name ] = spec.param.to_external_value( dataset_meta_dict[ name ] )\n        if filename is None:\n            return json.dumps( meta_dict )\n        json.dump( meta_dict, open( filename, 'wb+' ) )\n\n    def __getstate__( self ):\n        return None #cannot pickle a weakref item (self._parent), when data._metadata_collection is None, it will be recreated on demand\n\n\nclass MetadataSpecCollection( odict ):\n    \"\"\"\n    A simple extension of dict which allows cleaner access to items\n    and allows the values to be iterated over directly as if it were a\n    list.  append() is also implemented for simplicity and does not\n    \"append\".\n    \"\"\"\n    def __init__( self, dict = None ):\n        odict.__init__( self, dict = None )\n\n    def append( self, item ):\n        self[item.name] = item\n\n    def iter( self ):\n        return self.itervalues()\n\n    def __getattr__( self, name ):\n        return self.get( name )\n\n    def __repr__( self ):\n        # force elements to draw with __str__ for sphinx-apidoc\n        return ', '.join([ item.__str__() for item in self.iter() ])\n\n\nclass MetadataParameter( object ):\n    def __init__( self, spec ):\n        self.spec = spec\n\n    def get_html_field( self, value=None, context=None, other_values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n        return form_builder.TextField( self.spec.name, value=value )\n\n    def get_html( self, value, context=None, other_values=None, **kwd ):\n        \"\"\"\n        The \"context\" is simply the metadata collection/bunch holding\n        this piece of metadata. This is passed in to allow for\n        metadata to validate against each other (note: this could turn\n        into a huge, recursive mess if not done with care). For\n        example, a column assignment should validate against the\n        number of columns in the dataset.\n        \"\"\"\n        context = context or {}\n        other_values = other_values or {}\n\n        if self.spec.get(\"readonly\"):\n            return value\n        if self.spec.get(\"optional\"):\n            checked = False\n            if value: checked = \"true\"\n            checkbox = form_builder.CheckboxField( \"is_\" + self.spec.name, checked=checked )\n            return checkbox.get_html() + self.get_html_field( value=value, context=context, other_values=other_values, **kwd ).get_html()\n        else:\n            return self.get_html_field( value=value, context=context, other_values=other_values, **kwd ).get_html()\n\n    def to_string( self, value ):\n        return str( value )\n\n    def make_copy( self, value, target_context = None, source_context = None ):\n        return copy.deepcopy( value )\n\n    @classmethod\n    def marshal ( cls, value ):\n        \"\"\"\n        This method should/can be overridden to convert the incoming\n        value to whatever type it is supposed to be.\n        \"\"\"\n        return value\n\n    def validate( self, value ):\n        \"\"\"\n        Throw an exception if the value is invalid.\n        \"\"\"\n        pass\n\n    def unwrap( self, form_value ):\n        \"\"\"\n        Turns a value into its storable form.\n        \"\"\"\n        value = self.marshal( form_value )\n        self.validate( value )\n        return value\n\n    def wrap( self, value, session ):\n        \"\"\"\n        Turns a value into its usable form.\n        \"\"\"\n        return value\n\n    def from_external_value( self, value, parent ):\n        \"\"\"\n        Turns a value read from an external dict into its value to be pushed directly into the metadata dict.\n        \"\"\"\n        return value\n\n    def to_external_value( self, value ):\n        \"\"\"\n        Turns a value read from a metadata into its value to be pushed directly into the external dict.\n        \"\"\"\n        return value\n\n\nclass MetadataElementSpec( object ):\n    \"\"\"\n    Defines a metadata element and adds it to the metadata_spec (which\n    is a MetadataSpecCollection) of datatype.\n    \"\"\"\n    def __init__( self, datatype,\n                  name=None, desc=None, param=MetadataParameter, default=None, no_value = None,\n                  visible=True, set_in_upload = False, **kwargs ):\n        self.name = name\n        self.desc = desc or name\n        self.default = default\n        self.no_value = no_value\n        self.visible = visible\n        self.set_in_upload = set_in_upload\n        # Catch-all, allows for extra attributes to be set\n        self.__dict__.update(kwargs)\n        # set up param last, as it uses values set above\n        self.param = param( self )\n        # add spec element to the spec\n        datatype.metadata_spec.append( self )\n\n    def get( self, name, default=None ):\n        return self.__dict__.get(name, default)\n\n    def wrap( self, value, session ):\n        \"\"\"\n        Turns a stored value into its usable form.\n        \"\"\"\n        return self.param.wrap( value, session )\n\n    def unwrap( self, value ):\n        \"\"\"\n        Turns an incoming value into its storable form.\n        \"\"\"\n        return self.param.unwrap( value )\n\n    def __str__( self ):\n        #TODO??: assuming param is the class of this MetadataElementSpec - add the plain class name for that\n        spec_dict = dict( param_class=self.param.__class__.__name__ )\n        spec_dict.update( self.__dict__ )\n        return ( \"{name} ({param_class}): {desc}, defaults to '{default}'\".format( **spec_dict ) )\n\n# create a statement class that, when called,\n#   will add a new MetadataElementSpec to a class's metadata_spec\nMetadataElement = Statement( MetadataElementSpec )\n\n\n\"\"\"\nMetadataParameter sub-classes.\n\"\"\"\n\nclass SelectParameter( MetadataParameter ):\n    def __init__( self, spec ):\n        MetadataParameter.__init__( self, spec )\n        self.values = self.spec.get( \"values\" )\n        self.multiple = string_as_bool( self.spec.get( \"multiple\" ) )\n\n    def to_string( self, value ):\n        if value in [ None, [] ]:\n            return str( self.spec.no_value )\n        if not isinstance( value, list ):\n            value = [value]\n        return \",\".join( map( str, value ) )\n\n    def get_html_field( self, value=None, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        field = form_builder.SelectField( self.spec.name, multiple=self.multiple, display=self.spec.get(\"display\") )\n        if self.values:\n            value_list = self.values\n        elif values:\n            value_list = values\n        elif value:\n            value_list = [ ( v, v ) for v in listify( value )]\n        else:\n            value_list = []\n        for val, label in value_list:\n            try:\n                if ( self.multiple and val in value ) or ( not self.multiple and val == value ):\n                    field.add_option( label, val, selected=True )\n                else:\n                    field.add_option( label, val, selected=False )\n            except TypeError:\n                field.add_option( val, label, selected=False )\n        return field\n\n    def get_html( self, value, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        if self.spec.get(\"readonly\"):\n            if value in [ None, [] ]:\n                return str( self.spec.no_value )\n            return \", \".join( map( str, value ) )\n        return MetadataParameter.get_html( self, value, context=context, other_values=other_values, values=values, **kwd )\n\n    def wrap( self, value, session ):\n        value = self.marshal( value ) #do we really need this (wasteful)? - yes because we are not sure that all existing selects have been stored previously as lists. Also this will handle the case where defaults/no_values are specified and are single non-list values.\n        if self.multiple:\n            return value\n        elif value:\n            return value[0] #single select, only return the first value\n        return None\n\n    @classmethod\n    def marshal( cls, value ):\n        # Store select as list, even if single item\n        if value is None: return []\n        if not isinstance( value, list ): return [value]\n        return value\n\n\nclass DBKeyParameter( SelectParameter ):\n\n    def get_html_field( self, value=None, context=None, other_values=None, values=None, **kwd):\n        context = context or {}\n        other_values = other_values or {}\n        try:\n            values = kwd['trans'].app.genome_builds.get_genome_build_names( kwd['trans'] )\n        except KeyError:\n            pass\n        return super(DBKeyParameter, self).get_html_field( value, context, other_values, values, **kwd)\n\n    def get_html( self, value=None, context=None, other_values=None, values=None, **kwd):\n        context = context or {}\n        other_values = other_values or {}\n        try:\n            values = kwd['trans'].app.genome_builds.get_genome_build_names( kwd['trans'] )\n        except KeyError:\n            pass\n        return super(DBKeyParameter, self).get_html( value, context, other_values, values, **kwd)\n\n\nclass RangeParameter( SelectParameter ):\n\n    def __init__( self, spec ):\n        SelectParameter.__init__( self, spec )\n        # The spec must be set with min and max values\n        self.min = spec.get( \"min\" ) or 1\n        self.max = spec.get( \"max\" ) or 1\n        self.step = self.spec.get( \"step\" ) or 1\n\n    def get_html_field( self, value=None, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        if values is None:\n            values = zip( range( self.min, self.max, self.step ), range( self.min, self.max, self.step ))\n        return SelectParameter.get_html_field( self, value=value, context=context, other_values=other_values, values=values, **kwd )\n\n    def get_html( self, value, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        if values is None:\n            values = zip( range( self.min, self.max, self.step ), range( self.min, self.max, self.step ))\n        return SelectParameter.get_html( self, value, context=context, other_values=other_values, values=values, **kwd )\n\n    @classmethod\n    def marshal( cls, value ):\n        value = SelectParameter.marshal( value )\n        values = [ int(x) for x in value ]\n        return values\n\n\nclass ColumnParameter( RangeParameter ):\n\n    def get_html_field( self, value=None, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        if values is None and context:\n            column_range = range( 1, ( context.columns or 0 ) + 1, 1 )\n            values = zip( column_range, column_range )\n        return RangeParameter.get_html_field( self, value=value, context=context, other_values=other_values, values=values, **kwd )\n\n    def get_html( self, value, context=None, other_values=None, values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n\n        if values is None and context:\n            column_range = range( 1, ( context.columns or 0 ) + 1, 1 )\n            values = zip( column_range, column_range )\n        return RangeParameter.get_html( self, value, context=context, other_values=other_values, values=values, **kwd )\n\n\nclass ColumnTypesParameter( MetadataParameter ):\n\n    def to_string( self, value ):\n        return \",\".join( map( str, value ) )\n\n\nclass ListParameter( MetadataParameter ):\n\n    def to_string( self, value ):\n        return \",\".join( [str(x) for x in value] )\n\n\nclass DictParameter( MetadataParameter ):\n\n    def to_string( self, value ):\n        return  json.dumps( value )\n\n\nclass PythonObjectParameter( MetadataParameter ):\n\n    def to_string( self, value ):\n        if not value:\n            return self.spec._to_string( self.spec.no_value )\n        return self.spec._to_string( value )\n\n    def get_html_field( self, value=None, context=None, other_values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n        return form_builder.TextField( self.spec.name, value=self._to_string( value ) )\n\n    def get_html( self, value=None, context=None, other_values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n        return str( self )\n\n    @classmethod\n    def marshal( cls, value ):\n        return value\n\n\nclass FileParameter( MetadataParameter ):\n\n    def to_string( self, value ):\n        if not value:\n            return str( self.spec.no_value )\n        return value.file_name\n\n    def get_html_field( self, value=None, context=None, other_values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n        return form_builder.TextField( self.spec.name, value=str( value.id ) )\n\n    def get_html( self, value=None, context=None, other_values=None, **kwd ):\n        context = context or {}\n        other_values = other_values or {}\n        return \"<div>No display available for Metadata Files</div>\"\n\n    def wrap( self, value, session ):\n        if value is None:\n            return None\n        if isinstance( value, galaxy.model.MetadataFile ) or isinstance( value, MetadataTempFile ):\n            return value\n        mf = session.query( galaxy.model.MetadataFile ).get( value )\n        return mf\n\n    def make_copy( self, value, target_context, source_context ):\n        value = self.wrap( value, object_session( target_context.parent ) )\n        if value:\n            new_value = galaxy.model.MetadataFile( dataset = target_context.parent, name = self.spec.name )\n            object_session( target_context.parent ).add( new_value )\n            object_session( target_context.parent ).flush()\n            shutil.copy( value.file_name, new_value.file_name )\n            return self.unwrap( new_value )\n        return None\n\n    @classmethod\n    def marshal( cls, value ):\n        if isinstance( value, galaxy.model.MetadataFile ):\n            value = value.id\n        return value\n\n    def from_external_value( self, value, parent, path_rewriter=None ):\n        \"\"\"\n        Turns a value read from a external dict into its value to be pushed directly into the metadata dict.\n        \"\"\"\n        if MetadataTempFile.is_JSONified_value( value ):\n            value = MetadataTempFile.from_JSON( value )\n        if isinstance( value, MetadataTempFile ):\n            mf = parent.metadata.get( self.spec.name, None)\n            if mf is None:\n                mf = self.new_file( dataset = parent, **value.kwds )\n            # Ensure the metadata file gets updated with content\n            file_name = value.file_name\n            if path_rewriter:\n                # Job may have run with a different (non-local) tmp/working\n                # directory. Correct.\n                file_name = path_rewriter( file_name )\n            parent.dataset.object_store.update_from_file( mf, file_name=file_name, extra_dir='_metadata_files', extra_dir_at_root=True, alt_name=os.path.basename(mf.file_name) )\n            os.unlink( file_name )\n            value = mf.id\n        return value\n\n    def to_external_value( self, value ):\n        \"\"\"\n        Turns a value read from a metadata into its value to be pushed directly into the external dict.\n        \"\"\"\n        if isinstance( value, galaxy.model.MetadataFile ):\n            value = value.id\n        elif isinstance( value, MetadataTempFile ):\n            value = MetadataTempFile.to_JSON( value )\n        return value\n\n    def new_file( self, dataset = None, **kwds ):\n        if object_session( dataset ):\n            mf = galaxy.model.MetadataFile( name = self.spec.name, dataset = dataset, **kwds )\n            object_session( dataset ).add( mf )\n            object_session( dataset ).flush() #flush to assign id\n            return mf\n        else:\n            #we need to make a tmp file that is accessable to the head node,\n            #we will be copying its contents into the MetadataFile objects filename after restoring from JSON\n            #we do not include 'dataset' in the kwds passed, as from_JSON_value() will handle this for us\n            return MetadataTempFile( **kwds )\n\n\n#This class is used when a database file connection is not available\nclass MetadataTempFile( object ):\n    tmp_dir = 'database/tmp' #this should be overwritten as necessary in calling scripts\n\n    def __init__( self, **kwds ):\n        self.kwds = kwds\n        self._filename = None\n\n    @property\n    def file_name( self ):\n        if self._filename is None:\n            #we need to create a tmp file, accessable across all nodes/heads, save the name, and return it\n            self._filename = abspath( tempfile.NamedTemporaryFile( dir = self.tmp_dir, prefix = \"metadata_temp_file_\" ).name )\n            open( self._filename, 'wb+' ) #create an empty file, so it can't be reused using tempfile\n        return self._filename\n\n    def to_JSON( self ):\n        return { '__class__':self.__class__.__name__, 'filename':self.file_name, 'kwds':self.kwds }\n\n    @classmethod\n    def from_JSON( cls, json_dict ):\n        #need to ensure our keywords are not unicode\n        rval = cls( **stringify_dictionary_keys( json_dict['kwds'] ) )\n        rval._filename = json_dict['filename']\n        return rval\n\n    @classmethod\n    def is_JSONified_value( cls, value ):\n        return ( isinstance( value, dict ) and value.get( '__class__', None ) == cls.__name__ )\n\n    @classmethod\n    def cleanup_from_JSON_dict_filename( cls, filename ):\n        try:\n            for key, value in json.load( open( filename ) ).items():\n                if cls.is_JSONified_value( value ):\n                    value = cls.from_JSON( value )\n                if isinstance( value, cls ) and os.path.exists( value.file_name ):\n                    log.debug( 'Cleaning up abandoned MetadataTempFile file: %s' % value.file_name )\n                    os.unlink( value.file_name )\n        except Exception, e:\n            log.debug( 'Failed to cleanup MetadataTempFile temp files from %s: %s' % ( filename, e ) )\n\n\n#Class with methods allowing set_meta() to be called externally to the Galaxy head\nclass JobExternalOutputMetadataWrapper( object ):\n    #this class allows access to external metadata filenames for all outputs associated with a job\n    #We will use JSON as the medium of exchange of information, except for the DatasetInstance object which will use pickle (in the future this could be JSONified as well)\n\n    def __init__( self, job ):\n        self.job_id = job.id\n\n    def get_output_filenames_by_dataset( self, dataset, sa_session ):\n        if isinstance( dataset, galaxy.model.HistoryDatasetAssociation ):\n            return sa_session.query( galaxy.model.JobExternalOutputMetadata ) \\\n                             .filter_by( job_id = self.job_id, history_dataset_association_id = dataset.id ) \\\n                             .first() #there should only be one or None\n        elif isinstance( dataset, galaxy.model.LibraryDatasetDatasetAssociation ):\n            return sa_session.query( galaxy.model.JobExternalOutputMetadata ) \\\n                             .filter_by( job_id = self.job_id, library_dataset_dataset_association_id = dataset.id ) \\\n                             .first() #there should only be one or None\n        return None\n\n    def get_dataset_metadata_key( self, dataset ):\n        # Set meta can be called on library items and history items,\n        # need to make different keys for them, since ids can overlap\n        return \"%s_%d\" % ( dataset.__class__.__name__, dataset.id )\n\n    def setup_external_metadata( self, datasets, sa_session, exec_dir=None, tmp_dir=None, dataset_files_path=None,\n                                 output_fnames=None, config_root=None, config_file=None, datatypes_config=None, job_metadata=None, compute_tmp_dir=None, kwds=None ):\n        kwds = kwds or {}\n        if tmp_dir is None:\n            tmp_dir = MetadataTempFile.tmp_dir\n\n        # path is calculated for Galaxy, may be different on compute - rewrite\n        # for the compute server.\n        def metadata_path_on_compute(path):\n            compute_path = path\n            if compute_tmp_dir and tmp_dir and in_directory(path, tmp_dir):\n                path_relative = os.path.relpath(path, tmp_dir)\n                compute_path = os.path.join(compute_tmp_dir, path_relative)\n            return compute_path\n\n        #fill in metadata_files_dict and return the command with args required to set metadata\n        def __metadata_files_list_to_cmd_line( metadata_files ):\n            def __get_filename_override():\n                if output_fnames:\n                    for dataset_path in output_fnames:\n                        if dataset_path.false_path and dataset_path.real_path == metadata_files.dataset.file_name:\n                            return dataset_path.false_path\n                return \"\"\n            line = \"%s,%s,%s,%s,%s,%s\" % (\n                metadata_path_on_compute(metadata_files.filename_in),\n                metadata_path_on_compute(metadata_files.filename_kwds),\n                metadata_path_on_compute(metadata_files.filename_out),\n                metadata_path_on_compute(metadata_files.filename_results_code),\n                __get_filename_override(),\n                metadata_path_on_compute(metadata_files.filename_override_metadata),\n            )\n            return line\n        if not isinstance( datasets, list ):\n            datasets = [ datasets ]\n        if exec_dir is None:\n            exec_dir = os.path.abspath( os.getcwd() )\n        if dataset_files_path is None:\n            dataset_files_path = galaxy.model.Dataset.file_path\n        if config_root is None:\n            config_root = os.path.abspath( os.getcwd() )\n        if datatypes_config is None:\n            raise Exception( 'In setup_external_metadata, the received datatypes_config is None.' )\n            datatypes_config = 'datatypes_conf.xml'\n        metadata_files_list = []\n        for dataset in datasets:\n            key = self.get_dataset_metadata_key( dataset )\n            #future note:\n            #wonkiness in job execution causes build command line to be called more than once\n            #when setting metadata externally, via 'auto-detect' button in edit attributes, etc.,\n            #we don't want to overwrite (losing the ability to cleanup) our existing dataset keys and files,\n            #so we will only populate the dictionary once\n            metadata_files = self.get_output_filenames_by_dataset( dataset, sa_session )\n            if not metadata_files:\n                metadata_files = galaxy.model.JobExternalOutputMetadata( dataset = dataset)\n                metadata_files.job_id = self.job_id\n                #we are using tempfile to create unique filenames, tempfile always returns an absolute path\n                #we will use pathnames relative to the galaxy root, to accommodate instances where the galaxy root\n                #is located differently, i.e. on a cluster node with a different filesystem structure\n\n                #file to store existing dataset\n                metadata_files.filename_in = abspath( tempfile.NamedTemporaryFile( dir = tmp_dir, prefix = \"metadata_in_%s_\" % key ).name )\n\n                #FIXME: HACK\n                #sqlalchemy introduced 'expire_on_commit' flag for sessionmaker at version 0.5x\n                #This may be causing the dataset attribute of the dataset_association object to no-longer be loaded into memory when needed for pickling.\n                #For now, we'll simply 'touch' dataset_association.dataset to force it back into memory.\n                dataset.dataset #force dataset_association.dataset to be loaded before pickling\n                #A better fix could be setting 'expire_on_commit=False' on the session, or modifying where commits occur, or ?\n\n                cPickle.dump( dataset, open( metadata_files.filename_in, 'wb+' ) )\n                #file to store metadata results of set_meta()\n                metadata_files.filename_out = abspath( tempfile.NamedTemporaryFile( dir = tmp_dir, prefix = \"metadata_out_%s_\" % key ).name )\n                open( metadata_files.filename_out, 'wb+' ) # create the file on disk, so it cannot be reused by tempfile (unlikely, but possible)\n                #file to store a 'return code' indicating the results of the set_meta() call\n                #results code is like (True/False - if setting metadata was successful/failed , exception or string of reason of success/failure )\n                metadata_files.filename_results_code = abspath( tempfile.NamedTemporaryFile( dir = tmp_dir, prefix = \"metadata_results_%s_\" % key ).name )\n                json.dump( ( False, 'External set_meta() not called' ), open( metadata_files.filename_results_code, 'wb+' ) ) # create the file on disk, so it cannot be reused by tempfile (unlikely, but possible)\n                #file to store kwds passed to set_meta()\n                metadata_files.filename_kwds = abspath( tempfile.NamedTemporaryFile( dir = tmp_dir, prefix = \"metadata_kwds_%s_\" % key ).name )\n                json.dump( kwds, open( metadata_files.filename_kwds, 'wb+' ), ensure_ascii=True )\n                #existing metadata file parameters need to be overridden with cluster-writable file locations\n                metadata_files.filename_override_metadata = abspath( tempfile.NamedTemporaryFile( dir = tmp_dir, prefix = \"metadata_override_%s_\" % key ).name )\n                open( metadata_files.filename_override_metadata, 'wb+' ) # create the file on disk, so it cannot be reused by tempfile (unlikely, but possible)\n                override_metadata = []\n                for meta_key, spec_value in dataset.metadata.spec.iteritems():\n                    if isinstance( spec_value.param, FileParameter ) and dataset.metadata.get( meta_key, None ) is not None:\n                        metadata_temp = MetadataTempFile()\n                        shutil.copy( dataset.metadata.get( meta_key, None ).file_name, metadata_temp.file_name )\n                        override_metadata.append( ( meta_key, metadata_temp.to_JSON() ) )\n                json.dump( override_metadata, open( metadata_files.filename_override_metadata, 'wb+' ) )\n                #add to session and flush\n                sa_session.add( metadata_files )\n                sa_session.flush()\n            metadata_files_list.append( metadata_files )\n        #return command required to build\n        return \"%s %s %s %s %s %s %s %s\" % ( os.path.join( exec_dir, 'set_metadata.sh' ), dataset_files_path, compute_tmp_dir or tmp_dir, config_root, config_file, datatypes_config, job_metadata, \" \".join( map( __metadata_files_list_to_cmd_line, metadata_files_list ) ) )\n\n    def external_metadata_set_successfully( self, dataset, sa_session ):\n        metadata_files = self.get_output_filenames_by_dataset( dataset, sa_session )\n        if not metadata_files:\n            return False # this file doesn't exist\n        rval, rstring = json.load( open( metadata_files.filename_results_code ) )\n        if not rval:\n            log.debug( 'setting metadata externally failed for %s %s: %s' % ( dataset.__class__.__name__, dataset.id, rstring ) )\n        return rval\n\n    def cleanup_external_metadata( self, sa_session ):\n        log.debug( 'Cleaning up external metadata files' )\n        for metadata_files in sa_session.query( galaxy.model.Job ).get( self.job_id ).external_output_metadata:\n            #we need to confirm that any MetadataTempFile files were removed, if not we need to remove them\n            #can occur if the job was stopped before completion, but a MetadataTempFile is used in the set_meta\n            MetadataTempFile.cleanup_from_JSON_dict_filename( metadata_files.filename_out )\n            dataset_key = self.get_dataset_metadata_key( metadata_files.dataset )\n            for key, fname in [ ( 'filename_in', metadata_files.filename_in ), ( 'filename_out', metadata_files.filename_out ), ( 'filename_results_code', metadata_files.filename_results_code ), ( 'filename_kwds', metadata_files.filename_kwds ), ( 'filename_override_metadata', metadata_files.filename_override_metadata ) ]:\n                try:\n                    os.remove( fname )\n                except Exception, e:\n                    log.debug( 'Failed to cleanup external metadata file (%s) for %s: %s' % ( key, dataset_key, e ) )\n\n    def set_job_runner_external_pid( self, pid, sa_session ):\n        for metadata_files in sa_session.query( galaxy.model.Job ).get( self.job_id ).external_output_metadata:\n            metadata_files.job_runner_external_pid = pid\n            sa_session.add( metadata_files )\n            sa_session.flush()\n", "patch": "@@ -20,6 +20,7 @@\n \n import galaxy.model\n from galaxy.util import listify\n+from galaxy.util.object_wrapper import sanitize_lists_to_string\n from galaxy.util import stringify_dictionary_keys\n from galaxy.util import string_as_bool\n from galaxy.util import in_directory\n@@ -232,6 +233,9 @@ def get_html( self, value, context=None, other_values=None, **kwd ):\n     def to_string( self, value ):\n         return str( value )\n \n+    def to_safe_string( self, value ):\n+        return sanitize_lists_to_string( self.to_string( value ) )\n+\n     def make_copy( self, value, target_context = None, source_context = None ):\n         return copy.deepcopy( value )\n \n@@ -480,6 +484,10 @@ class DictParameter( MetadataParameter ):\n     def to_string( self, value ):\n         return  json.dumps( value )\n \n+    def to_safe_string( self, value ):\n+        # We do not sanitize json dicts\n+        return json.safe_dumps( value )\n+\n \n class PythonObjectParameter( MetadataParameter ):\n \n@@ -510,6 +518,10 @@ def to_string( self, value ):\n             return str( self.spec.no_value )\n         return value.file_name\n \n+    def to_safe_string( self, value ):\n+        # We do not sanitize file names\n+        return self.to_string( value )\n+\n     def get_html_field( self, value=None, context=None, other_values=None, **kwd ):\n         context = context or {}\n         other_values = other_values or {}", "file_path": "files/2023_1/989", "file_language": "py", "file_name": "lib/galaxy/datatypes/metadata.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/blankenberg/galaxy-data-resource/raw/50d65f45d3f5be5d1fbff2e45ac5cec075f07d42/lib%2Fgalaxy%2Ftools%2Fevaluation.py", "code": "import os\nimport tempfile\n\nfrom galaxy import model\nfrom galaxy.util.object_wrapper import wrap_with_safe_string\nfrom galaxy.util.bunch import Bunch\nfrom galaxy.util.none_like import NoneDataset\nfrom galaxy.util.template import fill_template\nfrom galaxy.tools.wrappers import (\n    ToolParameterValueWrapper,\n    DatasetFilenameWrapper,\n    DatasetListWrapper,\n    DatasetCollectionWrapper,\n    LibraryDatasetValueWrapper,\n    SelectToolParameterWrapper,\n    InputValueWrapper,\n    RawObjectWrapper\n)\nfrom galaxy.tools.parameters.basic import (\n    DataToolParameter,\n    DataCollectionToolParameter,\n    LibraryDatasetToolParameter,\n    SelectToolParameter,\n)\nfrom galaxy.tools.parameters.grouping import Conditional, Repeat\nfrom galaxy.jobs.datasets import dataset_path_rewrites\n\nimport logging\nlog = logging.getLogger( __name__ )\n\n\nclass ToolEvaluator( object ):\n    \"\"\" An abstraction linking together a tool and a job runtime to evaluate\n    tool inputs in an isolated, testable manner.\n    \"\"\"\n\n    def __init__( self, app, tool, job, local_working_directory ):\n        self.app = app\n        self.job = job\n        self.tool = tool\n        self.local_working_directory = local_working_directory\n\n    def set_compute_environment( self, compute_environment, get_special=None ):\n        \"\"\"\n        Setup the compute environment and established the outline of the param_dict\n        for evaluating command and config cheetah templates.\n        \"\"\"\n        self.compute_environment = compute_environment\n        self.unstructured_path_rewriter = compute_environment.unstructured_path_rewriter()\n\n        job = self.job\n        incoming = dict( [ ( p.name, p.value ) for p in job.parameters ] )\n        incoming = self.tool.params_from_strings( incoming, self.app )\n        # Do any validation that could not be done at job creation\n        self.tool.handle_unvalidated_param_values( incoming, self.app )\n        # Restore input / output data lists\n        inp_data = dict( [ ( da.name, da.dataset ) for da in job.input_datasets ] )\n        out_data = dict( [ ( da.name, da.dataset ) for da in job.output_datasets ] )\n        inp_data.update( [ ( da.name, da.dataset ) for da in job.input_library_datasets ] )\n        out_data.update( [ ( da.name, da.dataset ) for da in job.output_library_datasets ] )\n\n        if get_special:\n\n            # Set up output dataset association for export history jobs. Because job\n            # uses a Dataset rather than an HDA or LDA, it's necessary to set up a\n            # fake dataset association that provides the needed attributes for\n            # preparing a job.\n            class FakeDatasetAssociation ( object ):\n                def __init__( self, dataset=None ):\n                    self.dataset = dataset\n                    self.file_name = dataset.file_name\n                    self.metadata = dict()\n                    self.children = []\n\n            special = get_special()\n            if special:\n                out_data[ \"output_file\" ] = FakeDatasetAssociation( dataset=special.dataset )\n\n        # These can be passed on the command line if wanted as $__user_*__\n        incoming.update( model.User.user_template_environment( job.history and job.history.user ) )\n\n        # Build params, done before hook so hook can use\n        param_dict = self.build_param_dict(\n            incoming,\n            inp_data,\n            out_data,\n            output_paths=compute_environment.output_paths(),\n            job_working_directory=compute_environment.working_directory(),\n            input_paths=compute_environment.input_paths()\n        )\n\n        # Certain tools require tasks to be completed prior to job execution\n        # ( this used to be performed in the \"exec_before_job\" hook, but hooks are deprecated ).\n        self.tool.exec_before_job( self.app, inp_data, out_data, param_dict )\n        # Run the before queue (\"exec_before_job\") hook\n        self.tool.call_hook( 'exec_before_job', self.app, inp_data=inp_data,\n                             out_data=out_data, tool=self.tool, param_dict=incoming)\n\n        self.param_dict = param_dict\n\n    def build_param_dict( self, incoming, input_datasets, output_datasets, output_paths, job_working_directory, input_paths=[] ):\n        \"\"\"\n        Build the dictionary of parameters for substituting into the command\n        line. Each value is wrapped in a `InputValueWrapper`, which allows\n        all the attributes of the value to be used in the template, *but*\n        when the __str__ method is called it actually calls the\n        `to_param_dict_string` method of the associated input.\n        \"\"\"\n        param_dict = dict()\n        param_dict.update(self.tool.template_macro_params)\n        # All parameters go into the param_dict\n        param_dict.update( incoming )\n\n        input_dataset_paths = dataset_path_rewrites( input_paths )\n        self.__populate_wrappers(param_dict, input_dataset_paths)\n        self.__populate_input_dataset_wrappers(param_dict, input_datasets, input_dataset_paths)\n        self.__populate_output_dataset_wrappers(param_dict, output_datasets, output_paths, job_working_directory)\n        self.__populate_unstructured_path_rewrites(param_dict)\n        # Call param dict sanitizer, before non-job params are added, as we don't want to sanitize filenames.\n        self.__sanitize_param_dict( param_dict )\n        # Parameters added after this line are not sanitized\n        self.__populate_non_job_params(param_dict)\n\n        # Return the dictionary of parameters\n        return param_dict\n\n    def __walk_inputs(self, inputs, input_values, func):\n\n        def do_walk( inputs, input_values ):\n            \"\"\"\n            Wraps parameters as neccesary.\n            \"\"\"\n            for input in inputs.itervalues():\n                if isinstance( input, Repeat ):\n                    for d in input_values[ input.name ]:\n                        do_walk( input.inputs, d )\n                elif isinstance( input, Conditional ):\n                    values = input_values[ input.name ]\n                    current = values[\"__current_case__\"]\n                    do_walk( input.cases[current].inputs, values )\n                else:\n                    func( input_values, input )\n\n        do_walk( inputs, input_values )\n\n    def __populate_wrappers(self, param_dict, input_dataset_paths):\n\n        def wrap_input( input_values, input ):\n            if isinstance( input, DataToolParameter ) and input.multiple:\n                dataset_instances = input_values[ input.name ]\n                if isinstance( dataset_instances, model.HistoryDatasetCollectionAssociation ):\n                    dataset_instances = dataset_instances.collection.dataset_instances[:]\n                input_values[ input.name ] = \\\n                    DatasetListWrapper( dataset_instances,\n                                        dataset_paths=input_dataset_paths,\n                                        datatypes_registry=self.app.datatypes_registry,\n                                        tool=self.tool,\n                                        name=input.name )\n            elif isinstance( input, DataToolParameter ):\n                ## FIXME: We're populating param_dict with conversions when\n                ##        wrapping values, this should happen as a separate\n                ##        step before wrapping (or call this wrapping step\n                ##        something more generic) (but iterating this same\n                ##        list twice would be wasteful)\n                # Add explicit conversions by name to current parent\n                for conversion_name, conversion_extensions, conversion_datatypes in input.conversions:\n                    # If we are at building cmdline step, then converters\n                    # have already executed\n                    conv_ext, converted_dataset = input_values[ input.name ].find_conversion_destination( conversion_datatypes )\n                    # When dealing with optional inputs, we'll provide a\n                    # valid extension to be used for None converted dataset\n                    if not conv_ext:\n                        conv_ext = conversion_extensions[0]\n                    # input_values[ input.name ] is None when optional\n                    # dataset, 'conversion' of optional dataset should\n                    # create wrapper around NoneDataset for converter output\n                    if input_values[ input.name ] and not converted_dataset:\n                        # Input that converter is based from has a value,\n                        # but converted dataset does not exist\n                        raise Exception( 'A path for explicit datatype conversion has not been found: %s --/--> %s'\n                            % ( input_values[ input.name ].extension, conversion_extensions ) )\n                    else:\n                        # Trick wrapper into using target conv ext (when\n                        # None) without actually being a tool parameter\n                        input_values[ conversion_name ] = \\\n                            DatasetFilenameWrapper( converted_dataset,\n                                                    datatypes_registry=self.app.datatypes_registry,\n                                                    tool=Bunch( conversion_name=Bunch( extensions=conv_ext ) ),\n                                                    name=conversion_name )\n                # Wrap actual input dataset\n                dataset = input_values[ input.name ]\n                wrapper_kwds = dict(\n                    datatypes_registry=self.app.datatypes_registry,\n                    tool=self,\n                    name=input.name\n                )\n                if dataset:\n                    #A None dataset does not have a filename\n                    real_path = dataset.file_name\n                    if real_path in input_dataset_paths:\n                        wrapper_kwds[ \"dataset_path\" ] = input_dataset_paths[ real_path ]\n                input_values[ input.name ] = \\\n                    DatasetFilenameWrapper( dataset, **wrapper_kwds )\n            elif isinstance( input, DataCollectionToolParameter ):\n                dataset_collection = input_values[ input.name ]\n                wrapper_kwds = dict(\n                    datatypes_registry=self.app.datatypes_registry,\n                    dataset_paths=input_dataset_paths,\n                    tool=self,\n                    name=input.name\n                )\n                wrapper = DatasetCollectionWrapper(\n                    dataset_collection,\n                    **wrapper_kwds\n                )\n                input_values[ input.name ] = wrapper\n            elif isinstance( input, SelectToolParameter ):\n                input_values[ input.name ] = SelectToolParameterWrapper(\n                    input, input_values[ input.name ], self.app, other_values=param_dict, path_rewriter=self.unstructured_path_rewriter )\n            elif isinstance( input, LibraryDatasetToolParameter ):\n                # TODO: Handle input rewrites in here? How to test LibraryDatasetToolParameters?\n                input_values[ input.name ] = LibraryDatasetValueWrapper(\n                    input, input_values[ input.name ], param_dict )\n            else:\n                input_values[ input.name ] = InputValueWrapper(\n                    input, input_values[ input.name ], param_dict )\n\n        # HACK: only wrap if check_values is not false, this deals with external\n        #       tools where the inputs don't even get passed through. These\n        #       tools (e.g. UCSC) should really be handled in a special way.\n        if self.tool.check_values:\n            self.__walk_inputs( self.tool.inputs, param_dict, wrap_input )\n\n    def __populate_input_dataset_wrappers(self, param_dict, input_datasets, input_dataset_paths):\n        # TODO: Update this method for dataset collections? Need to test. -John.\n\n        ## FIXME: when self.check_values==True, input datasets are being wrapped\n        ##        twice (above and below, creating 2 separate\n        ##        DatasetFilenameWrapper objects - first is overwritten by\n        ##        second), is this necessary? - if we get rid of this way to\n        ##        access children, can we stop this redundancy, or is there\n        ##        another reason for this?\n        ## - Only necessary when self.check_values is False (==external dataset\n        ##   tool?: can this be abstracted out as part of being a datasouce tool?)\n        ## - But we still want (ALWAYS) to wrap input datasets (this should be\n        ##   checked to prevent overhead of creating a new object?)\n        # Additionally, datasets go in the param dict. We wrap them such that\n        # if the bare variable name is used it returns the filename (for\n        # backwards compatibility). We also add any child datasets to the\n        # the param dict encoded as:\n        #   \"_CHILD___{dataset_name}___{child_designation}\",\n        # but this should be considered DEPRECATED, instead use:\n        #   $dataset.get_child( 'name' ).filename\n        for name, data in input_datasets.items():\n            param_dict_value = param_dict.get(name, None)\n            if not isinstance(param_dict_value, (DatasetFilenameWrapper, DatasetListWrapper)):\n                wrapper_kwds = dict(\n                    datatypes_registry=self.app.datatypes_registry,\n                    tool=self,\n                    name=name,\n                )\n                if data:\n                    real_path = data.file_name\n                    if real_path in input_dataset_paths:\n                        dataset_path = input_dataset_paths[ real_path ]\n                        wrapper_kwds[ 'dataset_path' ] = dataset_path\n                param_dict[name] = DatasetFilenameWrapper( data, **wrapper_kwds )\n            if data:\n                for child in data.children:\n                    param_dict[ \"_CHILD___%s___%s\" % ( name, child.designation ) ] = DatasetFilenameWrapper( child )\n\n    def __populate_output_dataset_wrappers(self, param_dict, output_datasets, output_paths, job_working_directory):\n        output_dataset_paths = dataset_path_rewrites( output_paths )\n        for name, hda in output_datasets.items():\n            # Write outputs to the working directory (for security purposes)\n            # if desired.\n            real_path = hda.file_name\n            if real_path in output_dataset_paths:\n                dataset_path = output_dataset_paths[ real_path ]\n                param_dict[name] = DatasetFilenameWrapper( hda, dataset_path=dataset_path )\n                try:\n                    open( dataset_path.false_path, 'w' ).close()\n                except EnvironmentError:\n                    pass  # May well not exist - e.g. LWR.\n            else:\n                param_dict[name] = DatasetFilenameWrapper( hda )\n            # Provide access to a path to store additional files\n            # TODO: path munging for cluster/dataset server relocatability\n            param_dict[name].files_path = os.path.abspath(os.path.join( job_working_directory, \"dataset_%s_files\" % (hda.dataset.id) ))\n            for child in hda.children:\n                param_dict[ \"_CHILD___%s___%s\" % ( name, child.designation ) ] = DatasetFilenameWrapper( child )\n        for out_name, output in self.tool.outputs.iteritems():\n            if out_name not in param_dict and output.filters:\n                # Assume the reason we lack this output is because a filter\n                # failed to pass; for tool writing convienence, provide a\n                # NoneDataset\n                param_dict[ out_name ] = NoneDataset( datatypes_registry=self.app.datatypes_registry, ext=output.format )\n\n    def __populate_non_job_params(self, param_dict):\n        # -- Add useful attributes/functions for use in creating command line.\n\n        # Function for querying a data table.\n        def get_data_table_entry(table_name, query_attr, query_val, return_attr):\n            \"\"\"\n            Queries and returns an entry in a data table.\n            \"\"\"\n\n            if table_name in self.app.tool_data_tables:\n                return self.app.tool_data_tables[ table_name ].get_entry( query_attr, query_val, return_attr )\n\n        param_dict['__get_data_table_entry__'] = get_data_table_entry\n\n        # We add access to app here, this allows access to app.config, etc\n        param_dict['__app__'] = RawObjectWrapper( self.app )\n        # More convienent access to app.config.new_file_path; we don't need to\n        # wrap a string, but this method of generating additional datasets\n        # should be considered DEPRECATED\n        # TODO: path munging for cluster/dataset server relocatability\n        param_dict['__new_file_path__'] = os.path.abspath(self.compute_environment.new_file_path())\n        # The following points to location (xxx.loc) files which are pointers\n        # to locally cached data\n        param_dict['__tool_data_path__'] = param_dict['GALAXY_DATA_INDEX_DIR'] = self.app.config.tool_data_path\n        # For the upload tool, we need to know the root directory and the\n        # datatypes conf path, so we can load the datatypes registry\n        param_dict['__root_dir__'] = param_dict['GALAXY_ROOT_DIR'] = os.path.abspath( self.app.config.root )\n        param_dict['__datatypes_config__'] = param_dict['GALAXY_DATATYPES_CONF_FILE'] = self.app.datatypes_registry.integrated_datatypes_configs\n        param_dict['__admin_users__'] = self.app.config.admin_users\n        param_dict['__user__'] = RawObjectWrapper( param_dict.get( '__user__', None ) )\n\n    def __populate_unstructured_path_rewrites(self, param_dict):\n\n        def rewrite_unstructured_paths( input_values, input ):\n            if isinstance( input, SelectToolParameter ):\n                input_values[ input.name ] = SelectToolParameterWrapper(\n                    input, input_values[ input.name ], self.app, other_values=param_dict, path_rewriter=self.unstructured_path_rewriter )\n\n        if not self.tool.check_values and self.unstructured_path_rewriter:\n            # The tools weren't \"wrapped\" yet, but need to be in order to get\n            #the paths rewritten.\n            self.__walk_inputs( self.tool.inputs, param_dict, rewrite_unstructured_paths )\n\n    def __sanitize_param_dict( self, param_dict ):\n        \"\"\"\n        Sanitize all values that will be substituted on the command line, with the exception of ToolParameterValueWrappers,\n        which already have their own specific sanitization rules and also exclude special-cased named values.\n        We will only examine the first level for values to skip; the wrapping function will recurse as necessary.\n        \n        Note: this method follows the style of the similar populate calls, in that param_dict is modified in-place.\n        \"\"\"\n        # chromInfo is a filename, do not sanitize it.\n        skip = [ 'chromInfo' ]\n        if not self.tool or not self.tool.options or self.tool.options.sanitize:\n            for key, value in param_dict.items():\n                if key not in skip:\n                    # Remove key so that new wrapped object will occupy key slot\n                    del param_dict[key]\n                    # And replace with new wrapped key\n                    param_dict[ wrap_with_safe_string( key, no_wrap_classes=ToolParameterValueWrapper ) ] = wrap_with_safe_string( value, no_wrap_classes=ToolParameterValueWrapper )\n\n    def build( self ):\n        \"\"\"\n        Build runtime description of job to execute, evaluate command and\n        config templates corresponding to this tool with these inputs on this\n        compute environment.\n        \"\"\"\n        self.extra_filenames = []\n        self.command_line = None\n\n        self.__build_config_files( )\n        self.__build_param_file( )\n        self.__build_command_line( )\n\n        return self.command_line, self.extra_filenames\n\n    def __build_command_line( self ):\n        \"\"\"\n        Build command line to invoke this tool given a populated param_dict\n        \"\"\"\n        command = self.tool.command\n        param_dict = self.param_dict\n        interpreter = self.tool.interpreter\n        command_line = None\n        if not command:\n            return\n        try:\n            # Substituting parameters into the command\n            command_line = fill_template( command, context=param_dict )\n            # Remove newlines from command line, and any leading/trailing white space\n            command_line = command_line.replace( \"\\n\", \" \" ).replace( \"\\r\", \" \" ).strip()\n        except Exception:\n            # Modify exception message to be more clear\n            #e.args = ( 'Error substituting into command line. Params: %r, Command: %s' % ( param_dict, self.command ), )\n            raise\n        if interpreter:\n            # TODO: path munging for cluster/dataset server relocatability\n            executable = command_line.split()[0]\n            tool_dir = os.path.abspath( self.tool.tool_dir )\n            abs_executable = os.path.join( tool_dir, executable )\n            command_line = command_line.replace(executable, abs_executable, 1)\n            command_line = interpreter + \" \" + command_line\n        self.command_line = command_line\n\n    def __build_config_files( self ):\n        \"\"\"\n        Build temporary file for file based parameter transfer if needed\n        \"\"\"\n        param_dict = self.param_dict\n        config_filenames = []\n        for name, filename, template_text in self.tool.config_files:\n            # If a particular filename was forced by the config use it\n            directory = self.local_working_directory\n            if filename is not None:\n                config_filename = os.path.join( directory, filename )\n            else:\n                fd, config_filename = tempfile.mkstemp( dir=directory )\n                os.close( fd )\n            f = open( config_filename, \"wt\" )\n            f.write( fill_template( template_text, context=param_dict ) )\n            f.close()\n            # For running jobs as the actual user, ensure the config file is globally readable\n            os.chmod( config_filename, 0644 )\n            self.__register_extra_file( name, config_filename )\n            config_filenames.append( config_filename )\n        return config_filenames\n\n    def __build_param_file( self ):\n        \"\"\"\n        Build temporary file for file based parameter transfer if needed\n        \"\"\"\n        param_dict = self.param_dict\n        directory = self.local_working_directory\n        command = self.tool.command\n        if command and \"$param_file\" in command:\n            fd, param_filename = tempfile.mkstemp( dir=directory )\n            os.close( fd )\n            f = open( param_filename, \"wt\" )\n            for key, value in param_dict.items():\n                # parameters can be strings or lists of strings, coerce to list\n                if type(value) != type([]):\n                    value = [ value ]\n                for elem in value:\n                    f.write( '%s=%s\\n' % (key, elem) )\n            f.close()\n            self.__register_extra_file( 'param_file', param_filename )\n            return param_filename\n        else:\n            return None\n\n    def __register_extra_file( self, name, local_config_path ):\n        \"\"\"\n        Takes in the local path to a config file and registers the (potentially\n        remote) ultimate path of the config file with the parameter dict.\n        \"\"\"\n        self.extra_filenames.append( local_config_path )\n        config_basename = os.path.basename( local_config_path )\n        compute_config_path = self.__join_for_compute(self.compute_environment.config_directory(), config_basename)\n        self.param_dict[ name ] = compute_config_path\n\n    def __join_for_compute( self, *args ):\n        \"\"\"\n        os.path.join but with compute_environment.sep for cross-platform\n        compat.\n        \"\"\"\n        return self.compute_environment.sep().join( args )\n", "code_before": "import os\nimport tempfile\n\nfrom galaxy import model\nfrom galaxy.util.bunch import Bunch\nfrom galaxy.util.none_like import NoneDataset\nfrom galaxy.util.template import fill_template\nfrom galaxy.tools.wrappers import (\n    DatasetFilenameWrapper,\n    DatasetListWrapper,\n    DatasetCollectionWrapper,\n    LibraryDatasetValueWrapper,\n    SelectToolParameterWrapper,\n    InputValueWrapper,\n    RawObjectWrapper\n)\nfrom galaxy.tools.parameters.basic import (\n    DataToolParameter,\n    DataCollectionToolParameter,\n    LibraryDatasetToolParameter,\n    SelectToolParameter,\n)\nfrom galaxy.tools.parameters.grouping import Conditional, Repeat\nfrom galaxy.jobs.datasets import dataset_path_rewrites\n\nimport logging\nlog = logging.getLogger( __name__ )\n\n\nclass ToolEvaluator( object ):\n    \"\"\" An abstraction linking together a tool and a job runtime to evaluate\n    tool inputs in an isolated, testable manner.\n    \"\"\"\n\n    def __init__( self, app, tool, job, local_working_directory ):\n        self.app = app\n        self.job = job\n        self.tool = tool\n        self.local_working_directory = local_working_directory\n\n    def set_compute_environment( self, compute_environment, get_special=None ):\n        \"\"\"\n        Setup the compute environment and established the outline of the param_dict\n        for evaluating command and config cheetah templates.\n        \"\"\"\n        self.compute_environment = compute_environment\n        self.unstructured_path_rewriter = compute_environment.unstructured_path_rewriter()\n\n        job = self.job\n        incoming = dict( [ ( p.name, p.value ) for p in job.parameters ] )\n        incoming = self.tool.params_from_strings( incoming, self.app )\n        # Do any validation that could not be done at job creation\n        self.tool.handle_unvalidated_param_values( incoming, self.app )\n        # Restore input / output data lists\n        inp_data = dict( [ ( da.name, da.dataset ) for da in job.input_datasets ] )\n        out_data = dict( [ ( da.name, da.dataset ) for da in job.output_datasets ] )\n        inp_data.update( [ ( da.name, da.dataset ) for da in job.input_library_datasets ] )\n        out_data.update( [ ( da.name, da.dataset ) for da in job.output_library_datasets ] )\n\n        if get_special:\n\n            # Set up output dataset association for export history jobs. Because job\n            # uses a Dataset rather than an HDA or LDA, it's necessary to set up a\n            # fake dataset association that provides the needed attributes for\n            # preparing a job.\n            class FakeDatasetAssociation ( object ):\n                def __init__( self, dataset=None ):\n                    self.dataset = dataset\n                    self.file_name = dataset.file_name\n                    self.metadata = dict()\n                    self.children = []\n\n            special = get_special()\n            if special:\n                out_data[ \"output_file\" ] = FakeDatasetAssociation( dataset=special.dataset )\n\n        # These can be passed on the command line if wanted as $__user_*__\n        incoming.update( model.User.user_template_environment( job.history and job.history.user ) )\n\n        # Build params, done before hook so hook can use\n        param_dict = self.build_param_dict(\n            incoming,\n            inp_data,\n            out_data,\n            output_paths=compute_environment.output_paths(),\n            job_working_directory=compute_environment.working_directory(),\n            input_paths=compute_environment.input_paths()\n        )\n\n        # Certain tools require tasks to be completed prior to job execution\n        # ( this used to be performed in the \"exec_before_job\" hook, but hooks are deprecated ).\n        self.tool.exec_before_job( self.app, inp_data, out_data, param_dict )\n        # Run the before queue (\"exec_before_job\") hook\n        self.tool.call_hook( 'exec_before_job', self.app, inp_data=inp_data,\n                             out_data=out_data, tool=self.tool, param_dict=incoming)\n\n        self.param_dict = param_dict\n\n    def build_param_dict( self, incoming, input_datasets, output_datasets, output_paths, job_working_directory, input_paths=[] ):\n        \"\"\"\n        Build the dictionary of parameters for substituting into the command\n        line. Each value is wrapped in a `InputValueWrapper`, which allows\n        all the attributes of the value to be used in the template, *but*\n        when the __str__ method is called it actually calls the\n        `to_param_dict_string` method of the associated input.\n        \"\"\"\n        param_dict = dict()\n        param_dict.update(self.tool.template_macro_params)\n        # All parameters go into the param_dict\n        param_dict.update( incoming )\n\n        input_dataset_paths = dataset_path_rewrites( input_paths )\n        self.__populate_wrappers(param_dict, input_dataset_paths)\n        self.__populate_input_dataset_wrappers(param_dict, input_datasets, input_dataset_paths)\n        self.__populate_output_dataset_wrappers(param_dict, output_datasets, output_paths, job_working_directory)\n        self.__populate_unstructured_path_rewrites(param_dict)\n        self.__populate_non_job_params(param_dict)\n\n        # Return the dictionary of parameters\n        return param_dict\n\n    def __walk_inputs(self, inputs, input_values, func):\n\n        def do_walk( inputs, input_values ):\n            \"\"\"\n            Wraps parameters as neccesary.\n            \"\"\"\n            for input in inputs.itervalues():\n                if isinstance( input, Repeat ):\n                    for d in input_values[ input.name ]:\n                        do_walk( input.inputs, d )\n                elif isinstance( input, Conditional ):\n                    values = input_values[ input.name ]\n                    current = values[\"__current_case__\"]\n                    do_walk( input.cases[current].inputs, values )\n                else:\n                    func( input_values, input )\n\n        do_walk( inputs, input_values )\n\n    def __populate_wrappers(self, param_dict, input_dataset_paths):\n\n        def wrap_input( input_values, input ):\n            if isinstance( input, DataToolParameter ) and input.multiple:\n                dataset_instances = input_values[ input.name ]\n                if isinstance( dataset_instances, model.HistoryDatasetCollectionAssociation ):\n                    dataset_instances = dataset_instances.collection.dataset_instances[:]\n                input_values[ input.name ] = \\\n                    DatasetListWrapper( dataset_instances,\n                                        dataset_paths=input_dataset_paths,\n                                        datatypes_registry=self.app.datatypes_registry,\n                                        tool=self.tool,\n                                        name=input.name )\n            elif isinstance( input, DataToolParameter ):\n                ## FIXME: We're populating param_dict with conversions when\n                ##        wrapping values, this should happen as a separate\n                ##        step before wrapping (or call this wrapping step\n                ##        something more generic) (but iterating this same\n                ##        list twice would be wasteful)\n                # Add explicit conversions by name to current parent\n                for conversion_name, conversion_extensions, conversion_datatypes in input.conversions:\n                    # If we are at building cmdline step, then converters\n                    # have already executed\n                    conv_ext, converted_dataset = input_values[ input.name ].find_conversion_destination( conversion_datatypes )\n                    # When dealing with optional inputs, we'll provide a\n                    # valid extension to be used for None converted dataset\n                    if not conv_ext:\n                        conv_ext = conversion_extensions[0]\n                    # input_values[ input.name ] is None when optional\n                    # dataset, 'conversion' of optional dataset should\n                    # create wrapper around NoneDataset for converter output\n                    if input_values[ input.name ] and not converted_dataset:\n                        # Input that converter is based from has a value,\n                        # but converted dataset does not exist\n                        raise Exception( 'A path for explicit datatype conversion has not been found: %s --/--> %s'\n                            % ( input_values[ input.name ].extension, conversion_extensions ) )\n                    else:\n                        # Trick wrapper into using target conv ext (when\n                        # None) without actually being a tool parameter\n                        input_values[ conversion_name ] = \\\n                            DatasetFilenameWrapper( converted_dataset,\n                                                    datatypes_registry=self.app.datatypes_registry,\n                                                    tool=Bunch( conversion_name=Bunch( extensions=conv_ext ) ),\n                                                    name=conversion_name )\n                # Wrap actual input dataset\n                dataset = input_values[ input.name ]\n                wrapper_kwds = dict(\n                    datatypes_registry=self.app.datatypes_registry,\n                    tool=self,\n                    name=input.name\n                )\n                if dataset:\n                    #A None dataset does not have a filename\n                    real_path = dataset.file_name\n                    if real_path in input_dataset_paths:\n                        wrapper_kwds[ \"dataset_path\" ] = input_dataset_paths[ real_path ]\n                input_values[ input.name ] = \\\n                    DatasetFilenameWrapper( dataset, **wrapper_kwds )\n            elif isinstance( input, DataCollectionToolParameter ):\n                dataset_collection = input_values[ input.name ]\n                wrapper_kwds = dict(\n                    datatypes_registry=self.app.datatypes_registry,\n                    dataset_paths=input_dataset_paths,\n                    tool=self,\n                    name=input.name\n                )\n                wrapper = DatasetCollectionWrapper(\n                    dataset_collection,\n                    **wrapper_kwds\n                )\n                input_values[ input.name ] = wrapper\n            elif isinstance( input, SelectToolParameter ):\n                input_values[ input.name ] = SelectToolParameterWrapper(\n                    input, input_values[ input.name ], self.app, other_values=param_dict, path_rewriter=self.unstructured_path_rewriter )\n            elif isinstance( input, LibraryDatasetToolParameter ):\n                # TODO: Handle input rewrites in here? How to test LibraryDatasetToolParameters?\n                input_values[ input.name ] = LibraryDatasetValueWrapper(\n                    input, input_values[ input.name ], param_dict )\n            else:\n                input_values[ input.name ] = InputValueWrapper(\n                    input, input_values[ input.name ], param_dict )\n\n        # HACK: only wrap if check_values is not false, this deals with external\n        #       tools where the inputs don't even get passed through. These\n        #       tools (e.g. UCSC) should really be handled in a special way.\n        if self.tool.check_values:\n            self.__walk_inputs( self.tool.inputs, param_dict, wrap_input )\n\n    def __populate_input_dataset_wrappers(self, param_dict, input_datasets, input_dataset_paths):\n        # TODO: Update this method for dataset collections? Need to test. -John.\n\n        ## FIXME: when self.check_values==True, input datasets are being wrapped\n        ##        twice (above and below, creating 2 separate\n        ##        DatasetFilenameWrapper objects - first is overwritten by\n        ##        second), is this necessary? - if we get rid of this way to\n        ##        access children, can we stop this redundancy, or is there\n        ##        another reason for this?\n        ## - Only necessary when self.check_values is False (==external dataset\n        ##   tool?: can this be abstracted out as part of being a datasouce tool?)\n        ## - But we still want (ALWAYS) to wrap input datasets (this should be\n        ##   checked to prevent overhead of creating a new object?)\n        # Additionally, datasets go in the param dict. We wrap them such that\n        # if the bare variable name is used it returns the filename (for\n        # backwards compatibility). We also add any child datasets to the\n        # the param dict encoded as:\n        #   \"_CHILD___{dataset_name}___{child_designation}\",\n        # but this should be considered DEPRECATED, instead use:\n        #   $dataset.get_child( 'name' ).filename\n        for name, data in input_datasets.items():\n            param_dict_value = param_dict.get(name, None)\n            if not isinstance(param_dict_value, (DatasetFilenameWrapper, DatasetListWrapper)):\n                wrapper_kwds = dict(\n                    datatypes_registry=self.app.datatypes_registry,\n                    tool=self,\n                    name=name,\n                )\n                if data:\n                    real_path = data.file_name\n                    if real_path in input_dataset_paths:\n                        dataset_path = input_dataset_paths[ real_path ]\n                        wrapper_kwds[ 'dataset_path' ] = dataset_path\n                param_dict[name] = DatasetFilenameWrapper( data, **wrapper_kwds )\n            if data:\n                for child in data.children:\n                    param_dict[ \"_CHILD___%s___%s\" % ( name, child.designation ) ] = DatasetFilenameWrapper( child )\n\n    def __populate_output_dataset_wrappers(self, param_dict, output_datasets, output_paths, job_working_directory):\n        output_dataset_paths = dataset_path_rewrites( output_paths )\n        for name, hda in output_datasets.items():\n            # Write outputs to the working directory (for security purposes)\n            # if desired.\n            real_path = hda.file_name\n            if real_path in output_dataset_paths:\n                dataset_path = output_dataset_paths[ real_path ]\n                param_dict[name] = DatasetFilenameWrapper( hda, dataset_path=dataset_path )\n                try:\n                    open( dataset_path.false_path, 'w' ).close()\n                except EnvironmentError:\n                    pass  # May well not exist - e.g. LWR.\n            else:\n                param_dict[name] = DatasetFilenameWrapper( hda )\n            # Provide access to a path to store additional files\n            # TODO: path munging for cluster/dataset server relocatability\n            param_dict[name].files_path = os.path.abspath(os.path.join( job_working_directory, \"dataset_%s_files\" % (hda.dataset.id) ))\n            for child in hda.children:\n                param_dict[ \"_CHILD___%s___%s\" % ( name, child.designation ) ] = DatasetFilenameWrapper( child )\n        for out_name, output in self.tool.outputs.iteritems():\n            if out_name not in param_dict and output.filters:\n                # Assume the reason we lack this output is because a filter\n                # failed to pass; for tool writing convienence, provide a\n                # NoneDataset\n                param_dict[ out_name ] = NoneDataset( datatypes_registry=self.app.datatypes_registry, ext=output.format )\n\n    def __populate_non_job_params(self, param_dict):\n        # -- Add useful attributes/functions for use in creating command line.\n\n        # Function for querying a data table.\n        def get_data_table_entry(table_name, query_attr, query_val, return_attr):\n            \"\"\"\n            Queries and returns an entry in a data table.\n            \"\"\"\n\n            if table_name in self.app.tool_data_tables:\n                return self.app.tool_data_tables[ table_name ].get_entry( query_attr, query_val, return_attr )\n\n        param_dict['__get_data_table_entry__'] = get_data_table_entry\n\n        # We add access to app here, this allows access to app.config, etc\n        param_dict['__app__'] = RawObjectWrapper( self.app )\n        # More convienent access to app.config.new_file_path; we don't need to\n        # wrap a string, but this method of generating additional datasets\n        # should be considered DEPRECATED\n        # TODO: path munging for cluster/dataset server relocatability\n        param_dict['__new_file_path__'] = os.path.abspath(self.compute_environment.new_file_path())\n        # The following points to location (xxx.loc) files which are pointers\n        # to locally cached data\n        param_dict['__tool_data_path__'] = param_dict['GALAXY_DATA_INDEX_DIR'] = self.app.config.tool_data_path\n        # For the upload tool, we need to know the root directory and the\n        # datatypes conf path, so we can load the datatypes registry\n        param_dict['__root_dir__'] = param_dict['GALAXY_ROOT_DIR'] = os.path.abspath( self.app.config.root )\n        param_dict['__datatypes_config__'] = param_dict['GALAXY_DATATYPES_CONF_FILE'] = self.app.datatypes_registry.integrated_datatypes_configs\n        param_dict['__admin_users__'] = self.app.config.admin_users\n        param_dict['__user__'] = RawObjectWrapper( param_dict.get( '__user__', None ) )\n\n    def __populate_unstructured_path_rewrites(self, param_dict):\n\n        def rewrite_unstructured_paths( input_values, input ):\n            if isinstance( input, SelectToolParameter ):\n                input_values[ input.name ] = SelectToolParameterWrapper(\n                    input, input_values[ input.name ], self.app, other_values=param_dict, path_rewriter=self.unstructured_path_rewriter )\n\n        if not self.tool.check_values and self.unstructured_path_rewriter:\n            # The tools weren't \"wrapped\" yet, but need to be in order to get\n            #the paths rewritten.\n            self.__walk_inputs( self.tool.inputs, param_dict, rewrite_unstructured_paths )\n\n    def build( self ):\n        \"\"\"\n        Build runtime description of job to execute, evaluate command and\n        config templates corresponding to this tool with these inputs on this\n        compute environment.\n        \"\"\"\n        self.extra_filenames = []\n        self.command_line = None\n\n        self.__build_config_files( )\n        self.__build_param_file( )\n        self.__build_command_line( )\n\n        return self.command_line, self.extra_filenames\n\n    def __build_command_line( self ):\n        \"\"\"\n        Build command line to invoke this tool given a populated param_dict\n        \"\"\"\n        command = self.tool.command\n        param_dict = self.param_dict\n        interpreter = self.tool.interpreter\n        command_line = None\n        if not command:\n            return\n        try:\n            # Substituting parameters into the command\n            command_line = fill_template( command, context=param_dict )\n            # Remove newlines from command line, and any leading/trailing white space\n            command_line = command_line.replace( \"\\n\", \" \" ).replace( \"\\r\", \" \" ).strip()\n        except Exception:\n            # Modify exception message to be more clear\n            #e.args = ( 'Error substituting into command line. Params: %r, Command: %s' % ( param_dict, self.command ), )\n            raise\n        if interpreter:\n            # TODO: path munging for cluster/dataset server relocatability\n            executable = command_line.split()[0]\n            tool_dir = os.path.abspath( self.tool.tool_dir )\n            abs_executable = os.path.join( tool_dir, executable )\n            command_line = command_line.replace(executable, abs_executable, 1)\n            command_line = interpreter + \" \" + command_line\n        self.command_line = command_line\n\n    def __build_config_files( self ):\n        \"\"\"\n        Build temporary file for file based parameter transfer if needed\n        \"\"\"\n        param_dict = self.param_dict\n        config_filenames = []\n        for name, filename, template_text in self.tool.config_files:\n            # If a particular filename was forced by the config use it\n            directory = self.local_working_directory\n            if filename is not None:\n                config_filename = os.path.join( directory, filename )\n            else:\n                fd, config_filename = tempfile.mkstemp( dir=directory )\n                os.close( fd )\n            f = open( config_filename, \"wt\" )\n            f.write( fill_template( template_text, context=param_dict ) )\n            f.close()\n            # For running jobs as the actual user, ensure the config file is globally readable\n            os.chmod( config_filename, 0644 )\n            self.__register_extra_file( name, config_filename )\n            config_filenames.append( config_filename )\n        return config_filenames\n\n    def __build_param_file( self ):\n        \"\"\"\n        Build temporary file for file based parameter transfer if needed\n        \"\"\"\n        param_dict = self.param_dict\n        directory = self.local_working_directory\n        command = self.tool.command\n        if command and \"$param_file\" in command:\n            fd, param_filename = tempfile.mkstemp( dir=directory )\n            os.close( fd )\n            f = open( param_filename, \"wt\" )\n            for key, value in param_dict.items():\n                # parameters can be strings or lists of strings, coerce to list\n                if type(value) != type([]):\n                    value = [ value ]\n                for elem in value:\n                    f.write( '%s=%s\\n' % (key, elem) )\n            f.close()\n            self.__register_extra_file( 'param_file', param_filename )\n            return param_filename\n        else:\n            return None\n\n    def __register_extra_file( self, name, local_config_path ):\n        \"\"\"\n        Takes in the local path to a config file and registers the (potentially\n        remote) ultimate path of the config file with the parameter dict.\n        \"\"\"\n        self.extra_filenames.append( local_config_path )\n        config_basename = os.path.basename( local_config_path )\n        compute_config_path = self.__join_for_compute(self.compute_environment.config_directory(), config_basename)\n        self.param_dict[ name ] = compute_config_path\n\n    def __join_for_compute( self, *args ):\n        \"\"\"\n        os.path.join but with compute_environment.sep for cross-platform\n        compat.\n        \"\"\"\n        return self.compute_environment.sep().join( args )\n", "patch": "@@ -2,10 +2,12 @@\n import tempfile\n \n from galaxy import model\n+from galaxy.util.object_wrapper import wrap_with_safe_string\n from galaxy.util.bunch import Bunch\n from galaxy.util.none_like import NoneDataset\n from galaxy.util.template import fill_template\n from galaxy.tools.wrappers import (\n+    ToolParameterValueWrapper,\n     DatasetFilenameWrapper,\n     DatasetListWrapper,\n     DatasetCollectionWrapper,\n@@ -114,6 +116,9 @@ def build_param_dict( self, incoming, input_datasets, output_datasets, output_pa\n         self.__populate_input_dataset_wrappers(param_dict, input_datasets, input_dataset_paths)\n         self.__populate_output_dataset_wrappers(param_dict, output_datasets, output_paths, job_working_directory)\n         self.__populate_unstructured_path_rewrites(param_dict)\n+        # Call param dict sanitizer, before non-job params are added, as we don't want to sanitize filenames.\n+        self.__sanitize_param_dict( param_dict )\n+        # Parameters added after this line are not sanitized\n         self.__populate_non_job_params(param_dict)\n \n         # Return the dictionary of parameters\n@@ -334,6 +339,24 @@ def rewrite_unstructured_paths( input_values, input ):\n             #the paths rewritten.\n             self.__walk_inputs( self.tool.inputs, param_dict, rewrite_unstructured_paths )\n \n+    def __sanitize_param_dict( self, param_dict ):\n+        \"\"\"\n+        Sanitize all values that will be substituted on the command line, with the exception of ToolParameterValueWrappers,\n+        which already have their own specific sanitization rules and also exclude special-cased named values.\n+        We will only examine the first level for values to skip; the wrapping function will recurse as necessary.\n+        \n+        Note: this method follows the style of the similar populate calls, in that param_dict is modified in-place.\n+        \"\"\"\n+        # chromInfo is a filename, do not sanitize it.\n+        skip = [ 'chromInfo' ]\n+        if not self.tool or not self.tool.options or self.tool.options.sanitize:\n+            for key, value in param_dict.items():\n+                if key not in skip:\n+                    # Remove key so that new wrapped object will occupy key slot\n+                    del param_dict[key]\n+                    # And replace with new wrapped key\n+                    param_dict[ wrap_with_safe_string( key, no_wrap_classes=ToolParameterValueWrapper ) ] = wrap_with_safe_string( value, no_wrap_classes=ToolParameterValueWrapper )\n+\n     def build( self ):\n         \"\"\"\n         Build runtime description of job to execute, evaluate command and", "file_path": "files/2023_1/990", "file_language": "py", "file_name": "lib/galaxy/tools/evaluation.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/blankenberg/galaxy-data-resource/raw/50d65f45d3f5be5d1fbff2e45ac5cec075f07d42/lib%2Fgalaxy%2Ftools%2Fwrappers.py", "code": "import pipes\nfrom galaxy import exceptions\nfrom galaxy.util.none_like import NoneDataset\nfrom galaxy.util import odict\nfrom galaxy.util.object_wrapper import wrap_with_safe_string\n\nfrom logging import getLogger\nlog = getLogger( __name__ )\n\n# Fields in .log files corresponding to paths, must have one of the following\n# field names and all such fields are assumed to be paths. This is to allow\n# remote ComputeEnvironments (such as one used by LWR) determine what values to\n# rewrite or transfer...\nPATH_ATTRIBUTES = [ \"path\" ]\n# ... by default though - don't rewrite anything (if no ComputeEnviornment\n# defined or ComputeEnvironment doesn't supply a rewriter).\nDEFAULT_PATH_REWRITER = lambda x: x\n\n\nclass ToolParameterValueWrapper( object ):\n    \"\"\"\n    Base class for object that Wraps a Tool Parameter and Value.\n    \"\"\"\n\n    def __nonzero__( self ):\n        return bool( self.value )\n\n    def get_display_text( self, quote=True ):\n        \"\"\"\n        Returns a string containing the value that would be displayed to the user in the tool interface.\n        When quote is True (default), the string is escaped for e.g. command-line usage.\n        \"\"\"\n        rval = self.input.value_to_display_text( self.value, self.input.tool.app ) or ''\n        if quote:\n            return pipes.quote( rval ) or \"''\"  # pipes.quote in Python < 2.7 returns an empty string instead of the expected quoted empty string\n        return rval\n\n\nclass RawObjectWrapper( ToolParameterValueWrapper ):\n    \"\"\"\n    Wraps an object so that __str__ returns module_name:class_name.\n    \"\"\"\n    def __init__( self, obj ):\n        self.obj = obj\n\n    def __nonzero__( self ):\n        return bool( self.obj )  # FIXME: would it be safe/backwards compatible to rename .obj to .value, so that we can just inherit this method?\n\n    def __str__( self ):\n        try:\n            return \"%s:%s\" % (self.obj.__module__, self.obj.__class__.__name__)\n        except:\n            #Most likely None, which lacks __module__.\n            return str( self.obj )\n\n    def __getattr__( self, key ):\n        return getattr( self.obj, key )\n\n\nclass LibraryDatasetValueWrapper( ToolParameterValueWrapper ):\n    \"\"\"\n    Wraps an input so that __str__ gives the \"param_dict\" representation.\n    \"\"\"\n    def __init__( self, input, value, other_values={} ):\n        self.input = input\n        self.value = value\n        self._other_values = other_values\n        self.counter = 0\n\n    def __str__( self ):\n        return self.value\n\n    def __iter__( self ):\n        return self\n\n    def next( self ):\n        if self.counter >= len(self.value):\n            raise StopIteration\n        self.counter += 1\n        return self.value[ self.counter - 1 ]\n\n    def __getattr__( self, key ):\n        return getattr( self.value, key )\n\n\nclass InputValueWrapper( ToolParameterValueWrapper ):\n    \"\"\"\n    Wraps an input so that __str__ gives the \"param_dict\" representation.\n    \"\"\"\n    def __init__( self, input, value, other_values={} ):\n        self.input = input\n        self.value = value\n        self._other_values = other_values\n\n    def __str__( self ):\n        return self.input.to_param_dict_string( self.value, self._other_values )\n\n    def __getattr__( self, key ):\n        return getattr( self.value, key )\n\n\nclass SelectToolParameterWrapper( ToolParameterValueWrapper ):\n    \"\"\"\n    Wraps a SelectTooParameter so that __str__ returns the selected value, but all other\n    attributes are accessible.\n    \"\"\"\n\n    class SelectToolParameterFieldWrapper:\n        \"\"\"\n        Provide access to any field by name or index for this particular value.\n        Only applicable for dynamic_options selects, which have more than simple 'options' defined (name, value, selected).\n        \"\"\"\n        def __init__( self, input, value, other_values, path_rewriter ):\n            self._input = input\n            self._value = value\n            self._other_values = other_values\n            self._fields = {}\n            self._path_rewriter = path_rewriter\n\n        def __getattr__( self, name ):\n            if name not in self._fields:\n                self._fields[ name ] = self._input.options.get_field_by_name_for_value( name, self._value, None, self._other_values )\n            values = map( str, self._fields[ name ] )\n            if name in PATH_ATTRIBUTES:\n                # If we infer this is a path, rewrite it if needed.\n                values = map( self._path_rewriter, values )\n            return self._input.separator.join( values )\n\n    def __init__( self, input, value, app, other_values={}, path_rewriter=None ):\n        self.input = input\n        self.value = value\n        self.input.value_label = input.value_to_display_text( value, app )\n        self._other_values = other_values\n        self._path_rewriter = path_rewriter or DEFAULT_PATH_REWRITER\n        self.fields = self.SelectToolParameterFieldWrapper( input, value, other_values, self._path_rewriter )\n\n    def __str__( self ):\n        # Assuming value is never a path - otherwise would need to pass\n        # along following argument value_map=self._path_rewriter.\n        return self.input.to_param_dict_string( self.value, other_values=self._other_values )\n\n    def __getattr__( self, key ):\n        return getattr( self.input, key )\n\n\nclass DatasetFilenameWrapper( ToolParameterValueWrapper ):\n    \"\"\"\n    Wraps a dataset so that __str__ returns the filename, but all other\n    attributes are accessible.\n    \"\"\"\n\n    class MetadataWrapper:\n        \"\"\"\n        Wraps a Metadata Collection to return MetadataParameters wrapped\n        according to the metadata spec. Methods implemented to match behavior\n        of a Metadata Collection.\n        \"\"\"\n        def __init__( self, metadata ):\n            self.metadata = metadata\n\n        def __getattr__( self, name ):\n            rval = self.metadata.get( name, None )\n            if name in self.metadata.spec:\n                if rval is None:\n                    rval = self.metadata.spec[name].no_value\n                rval = self.metadata.spec[ name ].param.to_safe_string( rval )\n                # Store this value, so we don't need to recalculate if needed\n                # again\n                setattr( self, name, rval )\n            else:\n                #escape string value of non-defined metadata value\n                rval = wrap_with_safe_string( rval )\n            return rval\n\n        def __nonzero__( self ):\n            return self.metadata.__nonzero__()\n\n        def __iter__( self ):\n            return self.metadata.__iter__()\n\n        def get( self, key, default=None ):\n            try:\n                return getattr( self, key )\n            except:\n                return default\n\n        def items( self ):\n            return iter( [ ( k, self.get( k ) ) for k, v in self.metadata.items() ] )\n\n    def __init__( self, dataset, datatypes_registry=None, tool=None, name=None, dataset_path=None ):\n        if not dataset:\n            try:\n                # TODO: allow this to work when working with grouping\n                ext = tool.inputs[name].extensions[0]\n            except:\n                ext = 'data'\n            self.dataset = wrap_with_safe_string( NoneDataset( datatypes_registry=datatypes_registry, ext=ext ), no_wrap_classes=ToolParameterValueWrapper )\n        else:\n            # Tool wrappers should not normally be accessing .dataset directly, \n            # so we will wrap it and keep the original around for file paths\n            # Should we name this .value to maintain consistency with most other ToolParameterValueWrapper?\n            self.unsanitized = dataset\n            self.dataset = wrap_with_safe_string( dataset, no_wrap_classes=ToolParameterValueWrapper )\n            self.metadata = self.MetadataWrapper( dataset.metadata )\n        self.datatypes_registry = datatypes_registry\n        self.false_path = getattr( dataset_path, \"false_path\", None )\n        self.false_extra_files_path = getattr( dataset_path, \"false_extra_files_path\", None )\n\n    @property\n    def is_collection( self ):\n        return False\n\n    def is_of_type( self, *exts ):\n        datatypes = [ self.datatypes_registry.get_datatype_by_extension( e ) for e in exts ]\n        return self.dataset.datatype.matches_any( datatypes )\n\n    def __str__( self ):\n        if self.false_path is not None:\n            return self.false_path\n        else:\n            return self.unsanitized.file_name\n\n    def __getattr__( self, key ):\n        if self.false_path is not None and key == 'file_name':\n            # Path to dataset was rewritten for this job.\n            return self.false_path\n        elif self.false_extra_files_path is not None and key == 'extra_files_path':\n            # Path to extra files was rewritten for this job.\n            return self.false_extra_files_path\n        elif key == 'extra_files_path':\n            try:\n                # Assume it is an output and that this wrapper\n                # will be set with correct \"files_path\" for this\n                # job.\n                return self.files_path\n            except AttributeError:\n                # Otherwise, we have an input - delegate to model and\n                # object store to find the static location of this\n                # directory.\n                try:\n                    return self.unsanitized.extra_files_path\n                except exceptions.ObjectNotFound:\n                    # NestedObjectstore raises an error here\n                    # instead of just returning a non-existent\n                    # path like DiskObjectStore.\n                    raise\n        else:\n            return getattr( self.dataset, key )\n\n    def __nonzero__( self ):\n        return bool( self.dataset )\n\n\nclass HasDatasets:\n\n    def _dataset_wrapper( self, dataset, dataset_paths, **kwargs ):\n        wrapper_kwds = kwargs.copy()\n        if dataset:\n            real_path = dataset.file_name\n            if real_path in dataset_paths:\n                wrapper_kwds[ \"dataset_path\" ] = dataset_paths[ real_path ]\n        return DatasetFilenameWrapper( dataset, **wrapper_kwds )\n\n\nclass DatasetListWrapper( list, HasDatasets ):\n    \"\"\"\n    \"\"\"\n    def __init__( self, datasets, dataset_paths=[], **kwargs ):\n        if not isinstance(datasets, list):\n            datasets = [datasets]\n\n        def to_wrapper( dataset ):\n            return self._dataset_wrapper( dataset, dataset_paths, **kwargs )\n\n        list.__init__( self, map( to_wrapper, datasets ) )\n    def __str__( self ):\n        return ','.join( map( str, self ) )\n\n\nclass DatasetCollectionWrapper( object, HasDatasets ):\n\n    def __init__( self, has_collection, dataset_paths=[], **kwargs ):\n        super(DatasetCollectionWrapper, self).__init__()\n\n        if has_collection is None:\n            self.__input_supplied = False\n            return\n        else:\n            self.__input_supplied = True\n\n        if hasattr( has_collection, \"name\" ):\n            # It is a HistoryDatasetCollectionAssociation\n            collection = has_collection.collection\n            self.name = has_collection.name\n        else:\n            # It is a DatasetCollectionElement instance referencing another collection\n            collection = has_collection.child_collection\n            self.name = has_collection.element_identifier\n\n        elements = collection.elements\n        element_instances = odict.odict()\n\n        element_instance_list = []\n        for dataset_collection_element in elements:\n            element_object = dataset_collection_element.element_object\n            element_identifier = dataset_collection_element.element_identifier\n\n            if dataset_collection_element.is_collection:\n                element_wrapper = DatasetCollectionWrapper( dataset_collection_element, dataset_paths, **kwargs )\n            else:\n                element_wrapper = self._dataset_wrapper( element_object, dataset_paths, **kwargs)\n\n            element_instances[element_identifier] = element_wrapper\n            element_instance_list.append( element_wrapper )\n\n        self.__element_instances = element_instances\n        self.__element_instance_list = element_instance_list\n\n    def keys( self ):\n        if not self.__input_supplied:\n            return []\n        return self.__element_instances.keys()\n\n    @property\n    def is_collection( self ):\n        return True\n\n    @property\n    def is_input_supplied( self ):\n        return self.__input_supplied\n\n    def __getitem__( self, key ):\n        if not self.__input_supplied:\n            return None\n        if isinstance( key, int ):\n            return self.__element_instance_list[ key ]\n        else:\n            return self.__element_instances[ key ]\n\n    def __getattr__( self, key ):\n        if not self.__input_supplied:\n            return None\n        return self.__element_instances[ key ]\n\n    def __iter__( self ):\n        if not self.__input_supplied:\n            return [].__iter__()\n        return self.__element_instance_list.__iter__()\n\n    def __nonzero__( self ):\n        # Fail `#if $param` checks in cheetah is optional input\n        # not specified or if resulting collection is empty.\n        return self.__input_supplied and bool( self.__element_instance_list )\n", "code_before": "import pipes\nfrom galaxy import exceptions\nfrom galaxy.util.none_like import NoneDataset\nfrom galaxy.util import odict\n\nfrom logging import getLogger\nlog = getLogger( __name__ )\n\n# Fields in .log files corresponding to paths, must have one of the following\n# field names and all such fields are assumed to be paths. This is to allow\n# remote ComputeEnvironments (such as one used by LWR) determine what values to\n# rewrite or transfer...\nPATH_ATTRIBUTES = [ \"path\" ]\n# ... by default though - don't rewrite anything (if no ComputeEnviornment\n# defined or ComputeEnvironment doesn't supply a rewriter).\nDEFAULT_PATH_REWRITER = lambda x: x\n\n\nclass ToolParameterValueWrapper( object ):\n    \"\"\"\n    Base class for object that Wraps a Tool Parameter and Value.\n    \"\"\"\n\n    def __nonzero__( self ):\n        return bool( self.value )\n\n    def get_display_text( self, quote=True ):\n        \"\"\"\n        Returns a string containing the value that would be displayed to the user in the tool interface.\n        When quote is True (default), the string is escaped for e.g. command-line usage.\n        \"\"\"\n        rval = self.input.value_to_display_text( self.value, self.input.tool.app ) or ''\n        if quote:\n            return pipes.quote( rval ) or \"''\"  # pipes.quote in Python < 2.7 returns an empty string instead of the expected quoted empty string\n        return rval\n\n\nclass RawObjectWrapper( ToolParameterValueWrapper ):\n    \"\"\"\n    Wraps an object so that __str__ returns module_name:class_name.\n    \"\"\"\n    def __init__( self, obj ):\n        self.obj = obj\n\n    def __nonzero__( self ):\n        return bool( self.obj )  # FIXME: would it be safe/backwards compatible to rename .obj to .value, so that we can just inherit this method?\n\n    def __str__( self ):\n        try:\n            return \"%s:%s\" % (self.obj.__module__, self.obj.__class__.__name__)\n        except:\n            #Most likely None, which lacks __module__.\n            return str( self.obj )\n\n    def __getattr__( self, key ):\n        return getattr( self.obj, key )\n\n\nclass LibraryDatasetValueWrapper( ToolParameterValueWrapper ):\n    \"\"\"\n    Wraps an input so that __str__ gives the \"param_dict\" representation.\n    \"\"\"\n    def __init__( self, input, value, other_values={} ):\n        self.input = input\n        self.value = value\n        self._other_values = other_values\n        self.counter = 0\n\n    def __str__( self ):\n        return self.value\n\n    def __iter__( self ):\n        return self\n\n    def next( self ):\n        if self.counter >= len(self.value):\n            raise StopIteration\n        self.counter += 1\n        return self.value[ self.counter - 1 ]\n\n    def __getattr__( self, key ):\n        return getattr( self.value, key )\n\n\nclass InputValueWrapper( ToolParameterValueWrapper ):\n    \"\"\"\n    Wraps an input so that __str__ gives the \"param_dict\" representation.\n    \"\"\"\n    def __init__( self, input, value, other_values={} ):\n        self.input = input\n        self.value = value\n        self._other_values = other_values\n\n    def __str__( self ):\n        return self.input.to_param_dict_string( self.value, self._other_values )\n\n    def __getattr__( self, key ):\n        return getattr( self.value, key )\n\n\nclass SelectToolParameterWrapper( ToolParameterValueWrapper ):\n    \"\"\"\n    Wraps a SelectTooParameter so that __str__ returns the selected value, but all other\n    attributes are accessible.\n    \"\"\"\n\n    class SelectToolParameterFieldWrapper:\n        \"\"\"\n        Provide access to any field by name or index for this particular value.\n        Only applicable for dynamic_options selects, which have more than simple 'options' defined (name, value, selected).\n        \"\"\"\n        def __init__( self, input, value, other_values, path_rewriter ):\n            self._input = input\n            self._value = value\n            self._other_values = other_values\n            self._fields = {}\n            self._path_rewriter = path_rewriter\n\n        def __getattr__( self, name ):\n            if name not in self._fields:\n                self._fields[ name ] = self._input.options.get_field_by_name_for_value( name, self._value, None, self._other_values )\n            values = map( str, self._fields[ name ] )\n            if name in PATH_ATTRIBUTES:\n                # If we infer this is a path, rewrite it if needed.\n                values = map( self._path_rewriter, values )\n            return self._input.separator.join( values )\n\n    def __init__( self, input, value, app, other_values={}, path_rewriter=None ):\n        self.input = input\n        self.value = value\n        self.input.value_label = input.value_to_display_text( value, app )\n        self._other_values = other_values\n        self._path_rewriter = path_rewriter or DEFAULT_PATH_REWRITER\n        self.fields = self.SelectToolParameterFieldWrapper( input, value, other_values, self._path_rewriter )\n\n    def __str__( self ):\n        # Assuming value is never a path - otherwise would need to pass\n        # along following argument value_map=self._path_rewriter.\n        return self.input.to_param_dict_string( self.value, other_values=self._other_values )\n\n    def __getattr__( self, key ):\n        return getattr( self.input, key )\n\n\nclass DatasetFilenameWrapper( ToolParameterValueWrapper ):\n    \"\"\"\n    Wraps a dataset so that __str__ returns the filename, but all other\n    attributes are accessible.\n    \"\"\"\n\n    class MetadataWrapper:\n        \"\"\"\n        Wraps a Metadata Collection to return MetadataParameters wrapped\n        according to the metadata spec. Methods implemented to match behavior\n        of a Metadata Collection.\n        \"\"\"\n        def __init__( self, metadata ):\n            self.metadata = metadata\n\n        def __getattr__( self, name ):\n            rval = self.metadata.get( name, None )\n            if name in self.metadata.spec:\n                if rval is None:\n                    rval = self.metadata.spec[name].no_value\n                rval = self.metadata.spec[name].param.to_string( rval )\n                # Store this value, so we don't need to recalculate if needed\n                # again\n                setattr( self, name, rval )\n            return rval\n\n        def __nonzero__( self ):\n            return self.metadata.__nonzero__()\n\n        def __iter__( self ):\n            return self.metadata.__iter__()\n\n        def get( self, key, default=None ):\n            try:\n                return getattr( self, key )\n            except:\n                return default\n\n        def items( self ):\n            return iter( [ ( k, self.get( k ) ) for k, v in self.metadata.items() ] )\n\n    def __init__( self, dataset, datatypes_registry=None, tool=None, name=None, dataset_path=None ):\n        if not dataset:\n            try:\n                # TODO: allow this to work when working with grouping\n                ext = tool.inputs[name].extensions[0]\n            except:\n                ext = 'data'\n            self.dataset = NoneDataset( datatypes_registry=datatypes_registry, ext=ext )\n        else:\n            self.dataset = dataset\n            self.metadata = self.MetadataWrapper( dataset.metadata )\n        self.datatypes_registry = datatypes_registry\n        self.false_path = getattr( dataset_path, \"false_path\", None )\n        self.false_extra_files_path = getattr( dataset_path, \"false_extra_files_path\", None )\n\n    @property\n    def is_collection( self ):\n        return False\n\n    def is_of_type( self, *exts ):\n        datatypes = [ self.datatypes_registry.get_datatype_by_extension( e ) for e in exts ]\n        return self.dataset.datatype.matches_any( datatypes )\n\n    def __str__( self ):\n        if self.false_path is not None:\n            return self.false_path\n        else:\n            return self.dataset.file_name\n\n    def __getattr__( self, key ):\n        if self.false_path is not None and key == 'file_name':\n            # Path to dataset was rewritten for this job.\n            return self.false_path\n        elif self.false_extra_files_path is not None and key == 'extra_files_path':\n            # Path to extra files was rewritten for this job.\n            return self.false_extra_files_path\n        elif key == 'extra_files_path':\n            try:\n                # Assume it is an output and that this wrapper\n                # will be set with correct \"files_path\" for this\n                # job.\n                return self.files_path\n            except AttributeError:\n                # Otherwise, we have an input - delegate to model and\n                # object store to find the static location of this\n                # directory.\n                try:\n                    return self.dataset.extra_files_path\n                except exceptions.ObjectNotFound:\n                    # NestedObjectstore raises an error here\n                    # instead of just returning a non-existent\n                    # path like DiskObjectStore.\n                    raise\n        else:\n            return getattr( self.dataset, key )\n\n    def __nonzero__( self ):\n        return bool( self.dataset )\n\n\nclass HasDatasets:\n\n    def _dataset_wrapper( self, dataset, dataset_paths, **kwargs ):\n        wrapper_kwds = kwargs.copy()\n        if dataset:\n            real_path = dataset.file_name\n            if real_path in dataset_paths:\n                wrapper_kwds[ \"dataset_path\" ] = dataset_paths[ real_path ]\n        return DatasetFilenameWrapper( dataset, **wrapper_kwds )\n\n\nclass DatasetListWrapper( list, HasDatasets ):\n    \"\"\"\n    \"\"\"\n    def __init__( self, datasets, dataset_paths=[], **kwargs ):\n        if not isinstance(datasets, list):\n            datasets = [datasets]\n\n        def to_wrapper( dataset ):\n            return self._dataset_wrapper( dataset, dataset_paths, **kwargs )\n\n        list.__init__( self, map( to_wrapper, datasets ) )\n    def __str__( self ):\n        return ','.join( map( str, self ) )\n\n\nclass DatasetCollectionWrapper( object, HasDatasets ):\n\n    def __init__( self, has_collection, dataset_paths=[], **kwargs ):\n        super(DatasetCollectionWrapper, self).__init__()\n\n        if has_collection is None:\n            self.__input_supplied = False\n            return\n        else:\n            self.__input_supplied = True\n\n        if hasattr( has_collection, \"name\" ):\n            # It is a HistoryDatasetCollectionAssociation\n            collection = has_collection.collection\n            self.name = has_collection.name\n        else:\n            # It is a DatasetCollectionElement instance referencing another collection\n            collection = has_collection.child_collection\n            self.name = has_collection.element_identifier\n\n        elements = collection.elements\n        element_instances = odict.odict()\n\n        element_instance_list = []\n        for dataset_collection_element in elements:\n            element_object = dataset_collection_element.element_object\n            element_identifier = dataset_collection_element.element_identifier\n\n            if dataset_collection_element.is_collection:\n                element_wrapper = DatasetCollectionWrapper( dataset_collection_element, dataset_paths, **kwargs )\n            else:\n                element_wrapper = self._dataset_wrapper( element_object, dataset_paths, **kwargs)\n\n            element_instances[element_identifier] = element_wrapper\n            element_instance_list.append( element_wrapper )\n\n        self.__element_instances = element_instances\n        self.__element_instance_list = element_instance_list\n\n    def keys( self ):\n        if not self.__input_supplied:\n            return []\n        return self.__element_instances.keys()\n\n    @property\n    def is_collection( self ):\n        return True\n\n    @property\n    def is_input_supplied( self ):\n        return self.__input_supplied\n\n    def __getitem__( self, key ):\n        if not self.__input_supplied:\n            return None\n        if isinstance( key, int ):\n            return self.__element_instance_list[ key ]\n        else:\n            return self.__element_instances[ key ]\n\n    def __getattr__( self, key ):\n        if not self.__input_supplied:\n            return None\n        return self.__element_instances[ key ]\n\n    def __iter__( self ):\n        if not self.__input_supplied:\n            return [].__iter__()\n        return self.__element_instance_list.__iter__()\n\n    def __nonzero__( self ):\n        # Fail `#if $param` checks in cheetah is optional input\n        # not specified or if resulting collection is empty.\n        return self.__input_supplied and bool( self.__element_instance_list )\n", "patch": "@@ -2,6 +2,7 @@\n from galaxy import exceptions\n from galaxy.util.none_like import NoneDataset\n from galaxy.util import odict\n+from galaxy.util.object_wrapper import wrap_with_safe_string\n \n from logging import getLogger\n log = getLogger( __name__ )\n@@ -162,10 +163,13 @@ def __getattr__( self, name ):\n             if name in self.metadata.spec:\n                 if rval is None:\n                     rval = self.metadata.spec[name].no_value\n-                rval = self.metadata.spec[name].param.to_string( rval )\n+                rval = self.metadata.spec[ name ].param.to_safe_string( rval )\n                 # Store this value, so we don't need to recalculate if needed\n                 # again\n                 setattr( self, name, rval )\n+            else:\n+                #escape string value of non-defined metadata value\n+                rval = wrap_with_safe_string( rval )\n             return rval\n \n         def __nonzero__( self ):\n@@ -190,9 +194,13 @@ def __init__( self, dataset, datatypes_registry=None, tool=None, name=None, data\n                 ext = tool.inputs[name].extensions[0]\n             except:\n                 ext = 'data'\n-            self.dataset = NoneDataset( datatypes_registry=datatypes_registry, ext=ext )\n+            self.dataset = wrap_with_safe_string( NoneDataset( datatypes_registry=datatypes_registry, ext=ext ), no_wrap_classes=ToolParameterValueWrapper )\n         else:\n-            self.dataset = dataset\n+            # Tool wrappers should not normally be accessing .dataset directly, \n+            # so we will wrap it and keep the original around for file paths\n+            # Should we name this .value to maintain consistency with most other ToolParameterValueWrapper?\n+            self.unsanitized = dataset\n+            self.dataset = wrap_with_safe_string( dataset, no_wrap_classes=ToolParameterValueWrapper )\n             self.metadata = self.MetadataWrapper( dataset.metadata )\n         self.datatypes_registry = datatypes_registry\n         self.false_path = getattr( dataset_path, \"false_path\", None )\n@@ -210,7 +218,7 @@ def __str__( self ):\n         if self.false_path is not None:\n             return self.false_path\n         else:\n-            return self.dataset.file_name\n+            return self.unsanitized.file_name\n \n     def __getattr__( self, key ):\n         if self.false_path is not None and key == 'file_name':\n@@ -230,7 +238,7 @@ def __getattr__( self, key ):\n                 # object store to find the static location of this\n                 # directory.\n                 try:\n-                    return self.dataset.extra_files_path\n+                    return self.unsanitized.extra_files_path\n                 except exceptions.ObjectNotFound:\n                     # NestedObjectstore raises an error here\n                     # instead of just returning a non-existent", "file_path": "files/2023_1/991", "file_language": "py", "file_name": "lib/galaxy/tools/wrappers.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/blankenberg/galaxy-data-resource/raw/50d65f45d3f5be5d1fbff2e45ac5cec075f07d42/lib%2Fgalaxy%2Futil%2F__init__.py", "code": "# -*- coding: utf-8 -*-\n\"\"\"\nUtility functions used systemwide.\n\n\"\"\"\n\nfrom __future__ import absolute_import\n\nimport binascii\nimport collections\nimport errno\nimport grp\nimport logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nimport smtplib\nimport stat\nimport string\nimport sys\nimport tempfile\nimport threading\n\nfrom galaxy.util import json\n\nfrom email.MIMEText import MIMEText\n\nfrom os.path import relpath\nfrom hashlib import md5\nfrom itertools import izip\n\nfrom urlparse import urlparse\n\nfrom galaxy import eggs\n\neggs.require( 'docutils' )\nimport docutils.core\nimport docutils.writers.html4css1\n\neggs.require( 'elementtree' )\nfrom elementtree import ElementTree, ElementInclude\n\neggs.require( \"wchartype\" )\nimport wchartype\n\nfrom .inflection import Inflector, English\ninflector = Inflector(English)\n\nlog = logging.getLogger(__name__)\n_lock = threading.RLock()\n\nCHUNK_SIZE = 65536  # 64k\n\nDATABASE_MAX_STRING_SIZE = 32768\nDATABASE_MAX_STRING_SIZE_PRETTY = '32K'\n\ngzip_magic = '\\037\\213'\nbz2_magic = 'BZh'\nDEFAULT_ENCODING = os.environ.get('GALAXY_DEFAULT_ENCODING', 'utf-8')\nNULL_CHAR = '\\000'\nBINARY_CHARS = [ NULL_CHAR ]\n\n\ndef is_multi_byte( chars ):\n    for char in chars:\n        try:\n            char = unicode( char )\n        except UnicodeDecodeError:\n            # Probably binary\n            return False\n        if ( wchartype.is_asian( char ) or wchartype.is_full_width( char ) or\n             wchartype.is_kanji( char ) or wchartype.is_hiragana( char ) or\n             wchartype.is_katakana( char ) or wchartype.is_half_katakana( char )\n             or wchartype.is_hangul( char ) or wchartype.is_full_digit( char )\n             or wchartype.is_full_letter( char )):\n            return True\n    return False\n\n\ndef is_binary( value, binary_chars=None ):\n    \"\"\"\n    File is binary if it contains a null-byte by default (e.g. behavior of grep, etc.).\n    This may fail for utf-16 files, but so would ASCII encoding.\n    >>> is_binary( string.printable )\n    False\n    >>> is_binary( '\\\\xce\\\\x94' )\n    False\n    >>> is_binary( '\\\\000' )\n    True\n    \"\"\"\n    if binary_chars is None:\n        binary_chars = BINARY_CHARS\n    for binary_char in binary_chars:\n        if binary_char in value:\n            return True\n    return False\n\n\ndef is_uuid( value ):\n    \"\"\"\n    This method returns True if value is a UUID, otherwise False.\n    >>> is_uuid( \"123e4567-e89b-12d3-a456-426655440000\" )\n    True\n    >>> is_uuid( \"0x3242340298902834\" )\n    False\n    \"\"\"\n    uuid_re = re.compile( \"[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\" )\n    if re.match( uuid_re, str( value ) ):\n        return True\n    else:\n        return False\n\n\ndef get_charset_from_http_headers( headers, default=None ):\n    rval = headers.get('content-type', None )\n    if rval and 'charset=' in rval:\n        rval = rval.split('charset=')[-1].split(';')[0].strip()\n        if rval:\n            return rval\n    return default\n\n\ndef synchronized(func):\n    \"\"\"This wrapper will serialize access to 'func' to a single thread. Use it as a decorator.\"\"\"\n    def caller(*params, **kparams):\n        _lock.acquire(True)  # Wait\n        try:\n            return func(*params, **kparams)\n        finally:\n            _lock.release()\n    return caller\n\n\ndef file_iter(fname, sep=None):\n    \"\"\"\n    This generator iterates over a file and yields its lines\n    splitted via the C{sep} parameter. Skips empty lines and lines starting with\n    the C{#} character.\n\n    >>> lines = [ line for line in file_iter(__file__) ]\n    >>> len(lines) !=  0\n    True\n    \"\"\"\n    for line in file(fname):\n        if line and line[0] != '#':\n            yield line.split(sep)\n\n\ndef file_reader( fp, chunk_size=CHUNK_SIZE ):\n    \"\"\"This generator yields the open fileobject in chunks (default 64k). Closes the file at the end\"\"\"\n    while 1:\n        data = fp.read(chunk_size)\n        if not data:\n            break\n        yield data\n    fp.close()\n\n\ndef unique_id(KEY_SIZE=128):\n    \"\"\"\n    Generates an unique id\n\n    >>> ids = [ unique_id() for i in range(1000) ]\n    >>> len(set(ids))\n    1000\n    \"\"\"\n    return md5(str( random.getrandbits( KEY_SIZE ) )).hexdigest()\n\n\ndef parse_xml(fname):\n    \"\"\"Returns a parsed xml tree\"\"\"\n    tree = ElementTree.parse(fname)\n    root = tree.getroot()\n    ElementInclude.include(root)\n    return tree\n\n\ndef parse_xml_string(xml_string):\n    tree = ElementTree.fromstring(xml_string)\n    return tree\n\n\ndef xml_to_string( elem, pretty=False ):\n    \"\"\"Returns a string from an xml tree\"\"\"\n    if pretty:\n        elem = pretty_print_xml( elem )\n    try:\n        return ElementTree.tostring( elem )\n    except TypeError, e:\n        # we assume this is a comment\n        if hasattr( elem, 'text' ):\n            return \"<!-- %s -->\\n\" % ( elem.text )\n        else:\n            raise e\n\n\ndef xml_element_compare( elem1, elem2 ):\n    if not isinstance( elem1, dict ):\n        elem1 = xml_element_to_dict( elem1 )\n    if not isinstance( elem2, dict ):\n        elem2 = xml_element_to_dict( elem2 )\n    return elem1 == elem2\n\n\ndef xml_element_list_compare( elem_list1, elem_list2 ):\n    return [ xml_element_to_dict( elem ) for elem in elem_list1  ] == [ xml_element_to_dict( elem ) for elem in elem_list2  ]\n\n\ndef xml_element_to_dict( elem ):\n    rval = {}\n    if elem.attrib:\n        rval[ elem.tag ] = {}\n    else:\n        rval[ elem.tag ] = None\n\n    sub_elems = list( elem )\n    if sub_elems:\n        sub_elem_dict = dict()\n        for sub_sub_elem_dict in map( xml_element_to_dict, sub_elems ):\n            for key, value in sub_sub_elem_dict.iteritems():\n                if key not in sub_elem_dict:\n                    sub_elem_dict[ key ] = []\n                sub_elem_dict[ key ].append( value )\n        for key, value in sub_elem_dict.iteritems():\n            if len( value ) == 1:\n                rval[ elem.tag ][ key ] = value[0]\n            else:\n                rval[ elem.tag ][ key ] = value\n    if elem.attrib:\n        for key, value in elem.attrib.iteritems():\n            rval[ elem.tag ][ \"@%s\" % key ] = value\n\n    if elem.text:\n        text = elem.text.strip()\n        if text and sub_elems or elem.attrib:\n            rval[ elem.tag ][ '#text' ] = text\n        else:\n            rval[ elem.tag ] = text\n\n    return rval\n\n\ndef pretty_print_xml( elem, level=0 ):\n    pad = '    '\n    i = \"\\n\" + level * pad\n    if len( elem ):\n        if not elem.text or not elem.text.strip():\n            elem.text = i + pad + pad\n        if not elem.tail or not elem.tail.strip():\n            elem.tail = i\n        for e in elem:\n            pretty_print_xml( e, level + 1 )\n        if not elem.tail or not elem.tail.strip():\n            elem.tail = i\n    else:\n        if level and ( not elem.tail or not elem.tail.strip() ):\n            elem.tail = i + pad\n    return elem\n\n\ndef get_file_size( value, default=None ):\n    try:\n        # try built-in\n        return os.path.getsize( value )\n    except:\n        try:\n            # try built-in one name attribute\n            return os.path.getsize( value.name )\n        except:\n            try:\n                # try tell() of end of object\n                offset = value.tell()\n                value.seek( 0, 2 )\n                rval = value.tell()\n                value.seek( offset )\n                return rval\n            except:\n                # return default value\n                return default\n\n\ndef shrink_stream_by_size( value, size, join_by=\"..\", left_larger=True, beginning_on_size_error=False, end_on_size_error=False ):\n    rval = ''\n    if get_file_size( value ) > size:\n        start = value.tell()\n        len_join_by = len( join_by )\n        min_size = len_join_by + 2\n        if size < min_size:\n            if beginning_on_size_error:\n                rval = value.read( size )\n                value.seek( start )\n                return rval\n            elif end_on_size_error:\n                value.seek( -size, 2 )\n                rval = value.read( size )\n                value.seek( start )\n                return rval\n            raise ValueError( 'With the provided join_by value (%s), the minimum size value is %i.' % ( join_by, min_size ) )\n        left_index = right_index = int( ( size - len_join_by ) / 2 )\n        if left_index + right_index + len_join_by < size:\n            if left_larger:\n                left_index += 1\n            else:\n                right_index += 1\n        rval = value.read( left_index ) + join_by\n        value.seek( -right_index, 2 )\n        rval += value.read( right_index )\n    else:\n        while True:\n            data = value.read( CHUNK_SIZE )\n            if not data:\n                break\n            rval += data\n    return rval\n\n\ndef shrink_string_by_size( value, size, join_by=\"..\", left_larger=True, beginning_on_size_error=False, end_on_size_error=False ):\n    if len( value ) > size:\n        len_join_by = len( join_by )\n        min_size = len_join_by + 2\n        if size < min_size:\n            if beginning_on_size_error:\n                return value[:size]\n            elif end_on_size_error:\n                return value[-size:]\n            raise ValueError( 'With the provided join_by value (%s), the minimum size value is %i.' % ( join_by, min_size ) )\n        left_index = right_index = int( ( size - len_join_by ) / 2 )\n        if left_index + right_index + len_join_by < size:\n            if left_larger:\n                left_index += 1\n            else:\n                right_index += 1\n        value = \"%s%s%s\" % ( value[:left_index], join_by, value[-right_index:] )\n    return value\n\n\ndef pretty_print_json(json_data, is_json_string=False):\n    if is_json_string:\n        json_data = json.loads(json_data)\n    return json.dumps(json_data, sort_keys=True, indent=4)\n\n# characters that are valid\nvalid_chars = set(string.letters + string.digits + \" -=_.()/+*^,:?!\")\n\n# characters that are allowed but need to be escaped\nmapped_chars = { '>': '__gt__',\n                 '<': '__lt__',\n                 \"'\": '__sq__',\n                 '\"': '__dq__',\n                 '[': '__ob__',\n                 ']': '__cb__',\n                 '{': '__oc__',\n                 '}': '__cc__',\n                 '@': '__at__',\n                 '\\n': '__cn__',\n                 '\\r': '__cr__',\n                 '\\t': '__tc__',\n                 '#': '__pd__'}\n\n\ndef restore_text( text, character_map=mapped_chars ):\n    \"\"\"Restores sanitized text\"\"\"\n    if not text:\n        return text\n    for key, value in character_map.items():\n        text = text.replace(value, key)\n    return text\n\n\ndef sanitize_text( text, valid_characters=valid_chars, character_map=mapped_chars, invalid_character='X' ):\n    \"\"\"\n    Restricts the characters that are allowed in text; accepts both strings\n    and lists of strings; non-string entities will be cast to strings.\n    \"\"\"\n    if isinstance( text, list ):\n        return map( lambda x: sanitize_text( x, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character ), text )\n    if not isinstance( text, basestring ):\n        text = smart_str( text )\n    return _sanitize_text_helper( text, valid_characters=valid_characters, character_map=character_map )\n\ndef _sanitize_text_helper( text, valid_characters=valid_chars, character_map=mapped_chars, invalid_character='X' ):\n    \"\"\"Restricts the characters that are allowed in a string\"\"\"\n\n    out = []\n    for c in text:\n        if c in valid_characters:\n            out.append(c)\n        elif c in character_map:\n            out.append( character_map[c] )\n        else:\n            out.append( invalid_character )  # makes debugging easier\n    return ''.join(out)\n\n\ndef sanitize_lists_to_string( values, valid_characters=valid_chars, character_map=mapped_chars, invalid_character='X'  ):\n    if isinstance( values, list ):\n        rval = []\n        for value in values:\n            rval.append( sanitize_lists_to_string( value, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character ) )\n        values = \",\".join( rval )\n    else:\n        values = sanitize_text( values, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character )\n    return values\n\n\ndef sanitize_param( value, valid_characters=valid_chars, character_map=mapped_chars, invalid_character='X' ):\n    \"\"\"Clean incoming parameters (strings or lists)\"\"\"\n    if isinstance( value, basestring ):\n        return sanitize_text( value, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character )\n    elif isinstance( value, list ):\n        return map( lambda x: sanitize_text( x, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character ), value )\n    else:\n        raise Exception('Unknown parameter type (%s)' % ( type( value ) ))\n\nvalid_filename_chars = set( string.ascii_letters + string.digits + '_.' )\ninvalid_filenames = [ '', '.', '..' ]\n\n\ndef sanitize_for_filename( text, default=None ):\n    \"\"\"\n    Restricts the characters that are allowed in a filename portion; Returns default value or a unique id string if result is not a valid name.\n    Method is overly aggressive to minimize possible complications, but a maximum length is not considered.\n    \"\"\"\n    out = []\n    for c in text:\n        if c in valid_filename_chars:\n            out.append( c )\n        else:\n            out.append( '_' )\n    out = ''.join( out )\n    if out in invalid_filenames:\n        if default is None:\n            return sanitize_for_filename( str( unique_id() ) )\n        return default\n    return out\n\n\ndef ready_name_for_url( raw_name ):\n    \"\"\" General method to convert a string (i.e. object name) to a URL-ready\n    slug.\n\n    >>> ready_name_for_url( \"My Cool Object\" )\n    'My-Cool-Object'\n    >>> ready_name_for_url( \"!My Cool Object!\" )\n    'My-Cool-Object'\n    >>> ready_name_for_url( \"Hello\u20a9\u25ce\u0491\u029f\u217e\" )\n    'Hello'\n    \"\"\"\n\n    # Replace whitespace with '-'\n    slug_base = re.sub( \"\\s+\", \"-\", raw_name )\n    # Remove all non-alphanumeric characters.\n    slug_base = re.sub( \"[^a-zA-Z0-9\\-]\", \"\", slug_base )\n    # Remove trailing '-'.\n    if slug_base.endswith('-'):\n        slug_base = slug_base[:-1]\n    return slug_base\n\n\ndef in_directory( file, directory, local_path_module=os.path ):\n    \"\"\"\n    Return true, if the common prefix of both is equal to directory\n    e.g. /a/b/c/d.rst and directory is /a/b, the common prefix is /a/b\n    \"\"\"\n\n    # Make both absolute.\n    directory = local_path_module.abspath(directory)\n    file = local_path_module.abspath(file)\n    return local_path_module.commonprefix([file, directory]) == directory\n\n\ndef merge_sorted_iterables( operator, *iterables ):\n    \"\"\"\n\n    >>> operator = lambda x: x\n    >>> list( merge_sorted_iterables( operator, [1,2,3], [4,5] ) )\n    [1, 2, 3, 4, 5]\n    >>> list( merge_sorted_iterables( operator, [4, 5], [1,2,3] ) )\n    [1, 2, 3, 4, 5]\n    >>> list( merge_sorted_iterables( operator, [1, 4, 5], [2], [3] ) )\n    [1, 2, 3, 4, 5]\n    \"\"\"\n    first_iterable = iterables[ 0 ]\n    if len( iterables ) == 1:\n        for el in first_iterable:\n            yield el\n    else:\n        for el in __merge_two_sorted_iterables(\n            operator,\n            iter( first_iterable ),\n            merge_sorted_iterables( operator, *iterables[ 1: ] )\n        ):\n            yield el\n\n\ndef __merge_two_sorted_iterables( operator, iterable1, iterable2 ):\n    unset = object()\n    continue_merge = True\n    next_1 = unset\n    next_2 = unset\n    while continue_merge:\n        try:\n            if next_1 is unset:\n                next_1 = next( iterable1 )\n            if next_2 is unset:\n                next_2 = next( iterable2 )\n            if operator( next_2 ) < operator( next_1 ):\n                yield next_2\n                next_2 = unset\n            else:\n                yield next_1\n                next_1 = unset\n        except StopIteration:\n            continue_merge = False\n    if next_1 is not unset:\n        yield next_1\n    if next_2 is not unset:\n        yield next_2\n    for el in iterable1:\n        yield el\n    for el in iterable2:\n        yield el\n\n\nclass Params( object ):\n    \"\"\"\n    Stores and 'sanitizes' parameters. Alphanumeric characters and the\n    non-alphanumeric ones that are deemed safe are let to pass through (see L{valid_chars}).\n    Some non-safe characters are escaped to safe forms for example C{>} becomes C{__lt__}\n    (see L{mapped_chars}). All other characters are replaced with C{X}.\n\n    Operates on string or list values only (HTTP parameters).\n\n    >>> values = { 'status':'on', 'symbols':[  'alpha', '<>', '$rm&#!' ]  }\n    >>> par = Params(values)\n    >>> par.status\n    'on'\n    >>> par.value == None      # missing attributes return None\n    True\n    >>> par.get('price', 0)\n    0\n    >>> par.symbols            # replaces unknown symbols with X\n    ['alpha', '__lt____gt__', 'XrmX__pd__!']\n    >>> par.flatten()          # flattening to a list\n    [('status', 'on'), ('symbols', 'alpha'), ('symbols', '__lt____gt__'), ('symbols', 'XrmX__pd__!')]\n    \"\"\"\n\n    # is NEVER_SANITIZE required now that sanitizing for tool parameters can be controlled on a per parameter basis and occurs via InputValueWrappers?\n    NEVER_SANITIZE = ['file_data', 'url_paste', 'URL', 'filesystem_paths']\n\n    def __init__( self, params, sanitize=True ):\n        if sanitize:\n            for key, value in params.items():\n                if key not in self.NEVER_SANITIZE and True not in [ key.endswith( \"|%s\" % nonsanitize_parameter ) for nonsanitize_parameter in self.NEVER_SANITIZE ]:  # sanitize check both ungrouped and grouped parameters by name. Anything relying on NEVER_SANITIZE should be changed to not require this and NEVER_SANITIZE should be removed.\n                    self.__dict__[ key ] = sanitize_param( value )\n                else:\n                    self.__dict__[ key ] = value\n        else:\n            self.__dict__.update(params)\n\n    def flatten(self):\n        \"\"\"\n        Creates a tuple list from a dict with a tuple/value pair for every value that is a list\n        \"\"\"\n        flat = []\n        for key, value in self.__dict__.items():\n            if isinstance(value, list):\n                for v in value:\n                    flat.append( (key, v) )\n            else:\n                flat.append( (key, value) )\n        return flat\n\n    def __getattr__(self, name):\n        \"\"\"This is here to ensure that we get None for non existing parameters\"\"\"\n        return None\n\n    def get(self, key, default):\n        return self.__dict__.get(key, default)\n\n    def __str__(self):\n        return '%s' % self.__dict__\n\n    def __len__(self):\n        return len(self.__dict__)\n\n    def __iter__(self):\n        return iter(self.__dict__)\n\n    def update(self, values):\n        self.__dict__.update(values)\n\n\ndef rst_to_html( s ):\n    \"\"\"Convert a blob of reStructuredText to HTML\"\"\"\n    log = logging.getLogger( \"docutils\" )\n\n    class FakeStream( object ):\n        def write( self, str ):\n            if len( str ) > 0 and not str.isspace():\n                log.warn( str )\n    return unicodify( docutils.core.publish_string( s,\n                      writer=docutils.writers.html4css1.Writer(),\n                      settings_overrides={ \"embed_stylesheet\": False, \"template\": os.path.join(os.path.dirname(__file__), \"docutils_template.txt\"), \"warning_stream\": FakeStream() } ) )\n\n\ndef xml_text(root, name=None):\n    \"\"\"Returns the text inside an element\"\"\"\n    if name is not None:\n        # Try attribute first\n        val = root.get(name)\n        if val:\n            return val\n        # Then try as element\n        elem = root.find(name)\n    else:\n        elem = root\n    if elem is not None and elem.text:\n        text = ''.join(elem.text.splitlines())\n        return text.strip()\n    # No luck, return empty string\n    return ''\n\n# asbool implementation pulled from PasteDeploy\ntruthy = frozenset(['true', 'yes', 'on', 'y', 't', '1'])\nfalsy = frozenset(['false', 'no', 'off', 'n', 'f', '0'])\n\n\ndef asbool(obj):\n    if isinstance(obj, basestring):\n        obj = obj.strip().lower()\n        if obj in truthy:\n            return True\n        elif obj in falsy:\n            return False\n        else:\n            raise ValueError(\"String is not true/false: %r\" % obj)\n    return bool(obj)\n\n\ndef string_as_bool( string ):\n    if str( string ).lower() in ( 'true', 'yes', 'on' ):\n        return True\n    else:\n        return False\n\n\ndef string_as_bool_or_none( string ):\n    \"\"\"\n    Returns True, None or False based on the argument:\n        True if passed True, 'True', 'Yes', or 'On'\n        None if passed None or 'None'\n        False otherwise\n\n    Note: string comparison is case-insensitive so lowecase versions of those\n    function equivalently.\n    \"\"\"\n    string = str( string ).lower()\n    if string in ( 'true', 'yes', 'on' ):\n        return True\n    elif string == 'none':\n        return None\n    else:\n        return False\n\n\ndef listify( item, do_strip=False ):\n    \"\"\"\n    Make a single item a single item list, or return a list if passed a\n    list.  Passing a None returns an empty list.\n    \"\"\"\n    if not item:\n        return []\n    elif isinstance( item, list ):\n        return item\n    elif isinstance( item, basestring ) and item.count( ',' ):\n        if do_strip:\n            return [token.strip() for token in item.split( ',' )]\n        else:\n            return item.split( ',' )\n    else:\n        return [ item ]\n\n\ndef commaify(amount):\n    orig = amount\n    new = re.sub(\"^(-?\\d+)(\\d{3})\", '\\g<1>,\\g<2>', amount)\n    if orig == new:\n        return new\n    else:\n        return commaify(new)\n\n\ndef roundify(amount, sfs=2):\n    \"\"\"\n    Take a number in string form and truncate to 'sfs' significant figures.\n    \"\"\"\n    if len(amount) <= sfs:\n        return amount\n    else:\n        return amount[0:sfs] + '0'*(len(amount) - sfs)\n\n\ndef unicodify( value, encoding=DEFAULT_ENCODING, error='replace', default=None ):\n    \"\"\"\n    Returns a unicode string or None\n    \"\"\"\n\n    if isinstance( value, unicode ):\n        return value\n    try:\n        return unicode( str( value ), encoding, error )\n    except:\n        return default\n\n\ndef smart_str(s, encoding='utf-8', strings_only=False, errors='strict'):\n    \"\"\"\n    Returns a bytestring version of 's', encoded as specified in 'encoding'.\n\n    If strings_only is True, don't convert (some) non-string-like objects.\n\n    Adapted from an older, simpler version of django.utils.encoding.smart_str.\n    \"\"\"\n    if strings_only and isinstance(s, (type(None), int)):\n        return s\n    if not isinstance(s, basestring):\n        try:\n            return str(s)\n        except UnicodeEncodeError:\n            return unicode(s).encode(encoding, errors)\n    elif isinstance(s, unicode):\n        return s.encode(encoding, errors)\n    elif s and encoding != 'utf-8':\n        return s.decode('utf-8', errors).encode(encoding, errors)\n    else:\n        return s\n\n\ndef object_to_string( obj ):\n    return binascii.hexlify( obj )\n\n\ndef string_to_object( s ):\n    return binascii.unhexlify( s )\n\n\nclass ParamsWithSpecs( collections.defaultdict ):\n    \"\"\"\n    \"\"\"\n\n    def __init__( self, specs=None, params=None ):\n        self.specs = specs or dict()\n        self.params = params or dict()\n        for name, value in self.params.items():\n            if name not in self.specs:\n                self._param_unknown_error( name )\n            if 'map' in self.specs[ name ]:\n                try:\n                    self.params[ name ] = self.specs[ name ][ 'map' ]( value )\n                except Exception:\n                    self._param_map_error( name, value )\n            if 'valid' in self.specs[ name ]:\n                if not self.specs[ name ][ 'valid' ]( value ):\n                    self._param_vaildation_error( name, value )\n\n        self.update( self.params )\n\n    def __missing__( self, name ):\n        return self.specs[ name ][ 'default' ]\n\n    def __getattr__( self, name ):\n        return self[ name ]\n\n    def _param_unknown_error( self, name ):\n        raise NotImplementedError()\n\n    def _param_map_error( self, name, value ):\n        raise NotImplementedError()\n\n    def _param_vaildation_error( self, name, value ):\n        raise NotImplementedError()\n\n\ndef compare_urls( url1, url2, compare_scheme=True, compare_hostname=True, compare_path=True ):\n    url1 = urlparse( url1 )\n    url2 = urlparse( url2 )\n    if compare_scheme and url1.scheme and url2.scheme and url1.scheme != url2.scheme:\n        return False\n    if compare_hostname and url1.hostname and url2.hostname and url1.hostname != url2.hostname:\n        return False\n    if compare_path and url1.path and url2.path and url1.path != url2.path:\n        return False\n    return True\n\n\ndef read_dbnames(filename):\n    \"\"\" Read build names from file \"\"\"\n    class DBNames( list ):\n        default_value = \"?\"\n        default_name = \"unspecified (?)\"\n    db_names = DBNames()\n    try:\n        ucsc_builds = {}\n        man_builds = []  # assume these are integers\n        name_to_db_base = {}\n        if filename is None:\n            # Should only be happening with the galaxy.tools.parameters.basic:GenomeBuildParameter docstring unit test\n            filename = os.path.join( 'tool-data', 'shared', 'ucsc', 'builds.txt.sample' )\n        for line in open(filename):\n            try:\n                if line[0:1] == \"#\":\n                    continue\n                fields = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").split(\"\\t\")\n                # Special case of unspecified build is at top of list\n                if fields[0] == \"?\":\n                    db_names.insert(0, (fields[0], fields[1]))\n                    continue\n                try:  # manual build (i.e. microbes)\n                    int(fields[0])\n                    man_builds.append((fields[1], fields[0]))\n                except:  # UCSC build\n                    db_base = fields[0].rstrip('0123456789')\n                    if db_base not in ucsc_builds:\n                        ucsc_builds[db_base] = []\n                        name_to_db_base[fields[1]] = db_base\n                    # we want to sort within a species numerically by revision number\n                    build_rev = re.compile(r'\\d+$')\n                    try:\n                        build_rev = int(build_rev.findall(fields[0])[0])\n                    except:\n                        build_rev = 0\n                    ucsc_builds[db_base].append((build_rev, fields[0], fields[1]))\n            except:\n                continue\n        sort_names = name_to_db_base.keys()\n        sort_names.sort()\n        for name in sort_names:\n            db_base = name_to_db_base[name]\n            ucsc_builds[db_base].sort()\n            ucsc_builds[db_base].reverse()\n            ucsc_builds[db_base] = [(build, name) for _, build, name in ucsc_builds[db_base]]\n            db_names = DBNames( db_names + ucsc_builds[db_base] )\n        if len( db_names ) > 1 and len( man_builds ) > 0:\n            db_names.append( ( db_names.default_value, '----- Additional Species Are Below -----' ) )\n        man_builds.sort()\n        man_builds = [(build, name) for name, build in man_builds]\n        db_names = DBNames( db_names + man_builds )\n    except Exception, e:\n        print \"ERROR: Unable to read builds file:\", e\n    if len(db_names) < 1:\n        db_names = DBNames( [( db_names.default_value,  db_names.default_name )] )\n    return db_names\n\n\ndef read_build_sites( filename, check_builds=True ):\n    \"\"\" read db names to ucsc mappings from file, this file should probably be merged with the one above \"\"\"\n    build_sites = []\n    try:\n        for line in open(filename):\n            try:\n                if line[0:1] == \"#\":\n                    continue\n                fields = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").split(\"\\t\")\n                site_name = fields[0]\n                site = fields[1]\n                if check_builds:\n                    site_builds = fields[2].split(\",\")\n                    site_dict = {'name': site_name, 'url': site, 'builds': site_builds}\n                else:\n                    site_dict = {'name': site_name, 'url': site}\n                build_sites.append( site_dict )\n            except:\n                continue\n    except:\n        print \"ERROR: Unable to read builds for site file %s\" % filename\n    return build_sites\n\n\ndef relativize_symlinks( path, start=None, followlinks=False):\n    for root, dirs, files in os.walk( path, followlinks=followlinks ):\n        rel_start = None\n        for file_name in files:\n            symlink_file_name = os.path.join( root, file_name )\n            if os.path.islink( symlink_file_name ):\n                symlink_target = os.readlink( symlink_file_name )\n                if rel_start is None:\n                    if start is None:\n                        rel_start = root\n                    else:\n                        rel_start = start\n                rel_path = relpath( symlink_target, rel_start )\n                os.remove( symlink_file_name )\n                os.symlink( rel_path, symlink_file_name )\n\n\ndef stringify_dictionary_keys( in_dict ):\n    # returns a new dictionary\n    # changes unicode keys into strings, only works on top level (does not recurse)\n    # unicode keys are not valid for expansion into keyword arguments on method calls\n    out_dict = {}\n    for key, value in in_dict.iteritems():\n        out_dict[ str( key ) ] = value\n    return out_dict\n\n\ndef recursively_stringify_dictionary_keys( d ):\n    if isinstance(d, dict):\n        return dict([(k.encode( DEFAULT_ENCODING ), recursively_stringify_dictionary_keys(v)) for k, v in d.iteritems()])\n    elif isinstance(d, list):\n        return [recursively_stringify_dictionary_keys(x) for x in d]\n    else:\n        return d\n\n\ndef mkstemp_ln( src, prefix='mkstemp_ln_' ):\n    \"\"\"\n    From tempfile._mkstemp_inner, generate a hard link in the same dir with a\n    random name.  Created so we can persist the underlying file of a\n    NamedTemporaryFile upon its closure.\n    \"\"\"\n    dir = os.path.dirname(src)\n    names = tempfile._get_candidate_names()\n    for seq in xrange(tempfile.TMP_MAX):\n        name = names.next()\n        file = os.path.join(dir, prefix + name)\n        try:\n            os.link( src, file )\n            return (os.path.abspath(file))\n        except OSError, e:\n            if e.errno == errno.EEXIST:\n                continue  # try again\n            raise\n    raise IOError(errno.EEXIST, \"No usable temporary file name found\")\n\n\ndef umask_fix_perms( path, umask, unmasked_perms, gid=None ):\n    \"\"\"\n    umask-friendly permissions fixing\n    \"\"\"\n    perms = unmasked_perms & ~umask\n    try:\n        st = os.stat( path )\n    except OSError, e:\n        log.exception( 'Unable to set permissions or group on %s' % path )\n        return\n    # fix modes\n    if stat.S_IMODE( st.st_mode ) != perms:\n        try:\n            os.chmod( path, perms )\n        except Exception, e:\n            log.warning( 'Unable to honor umask (%s) for %s, tried to set: %s but mode remains %s, error was: %s' % ( oct( umask ),\n                                                                                                                      path,\n                                                                                                                      oct( perms ),\n                                                                                                                      oct( stat.S_IMODE( st.st_mode ) ),\n                                                                                                                      e ) )\n    # fix group\n    if gid is not None and st.st_gid != gid:\n        try:\n            os.chown( path, -1, gid )\n        except Exception, e:\n            try:\n                desired_group = grp.getgrgid( gid )\n                current_group = grp.getgrgid( st.st_gid )\n            except:\n                desired_group = gid\n                current_group = st.st_gid\n            log.warning( 'Unable to honor primary group (%s) for %s, group remains %s, error was: %s' % ( desired_group,\n                                                                                                          path,\n                                                                                                          current_group,\n                                                                                                          e ) )\n\n\ndef docstring_trim(docstring):\n    \"\"\"Trimming python doc strings. Taken from: http://www.python.org/dev/peps/pep-0257/\"\"\"\n    if not docstring:\n        return ''\n    # Convert tabs to spaces (following the normal Python rules)\n    # and split into a list of lines:\n    lines = docstring.expandtabs().splitlines()\n    # Determine minimum indentation (first line doesn't count):\n    indent = sys.maxint\n    for line in lines[1:]:\n        stripped = line.lstrip()\n        if stripped:\n            indent = min(indent, len(line) - len(stripped))\n    # Remove indentation (first line is special):\n    trimmed = [lines[0].strip()]\n    if indent < sys.maxint:\n        for line in lines[1:]:\n            trimmed.append(line[indent:].rstrip())\n    # Strip off trailing and leading blank lines:\n    while trimmed and not trimmed[-1]:\n        trimmed.pop()\n    while trimmed and not trimmed[0]:\n        trimmed.pop(0)\n    # Return a single string:\n    return '\\n'.join(trimmed)\n\n\ndef nice_size(size):\n    \"\"\"\n    Returns a readably formatted string with the size\n\n    >>> nice_size(100)\n    '100 bytes'\n    >>> nice_size(10000)\n    '9.8 KB'\n    >>> nice_size(1000000)\n    '976.6 KB'\n    >>> nice_size(100000000)\n    '95.4 MB'\n    \"\"\"\n    words = [ 'bytes', 'KB', 'MB', 'GB', 'TB' ]\n    try:\n        size = float( size )\n    except:\n        return '??? bytes'\n    for ind, word in enumerate(words):\n        step = 1024 ** (ind + 1)\n        if step > size:\n            size = size / float(1024 ** ind)\n            if word == 'bytes':  # No decimals for bytes\n                return \"%d bytes\" % size\n            return \"%.1f %s\" % (size, word)\n    return '??? bytes'\n\n\ndef size_to_bytes( size ):\n    \"\"\"\n    Returns a number of bytes if given a reasonably formatted string with the size\n    \"\"\"\n    # Assume input in bytes if we can convert directly to an int\n    try:\n        return int( size )\n    except:\n        pass\n    # Otherwise it must have non-numeric characters\n    size_re = re.compile( '([\\d\\.]+)\\s*([tgmk]b?|b|bytes?)$' )\n    size_match = re.match( size_re, size.lower() )\n    assert size_match is not None\n    size = float( size_match.group(1) )\n    multiple = size_match.group(2)\n    if multiple.startswith( 't' ):\n        return int( size * 1024**4 )\n    elif multiple.startswith( 'g' ):\n        return int( size * 1024**3 )\n    elif multiple.startswith( 'm' ):\n        return int( size * 1024**2 )\n    elif multiple.startswith( 'k' ):\n        return int( size * 1024 )\n    elif multiple.startswith( 'b' ):\n        return int( size )\n\n\ndef send_mail( frm, to, subject, body, config ):\n    \"\"\"\n    Sends an email.\n    \"\"\"\n    to = listify( to )\n    msg = MIMEText(  body.encode( 'ascii', 'replace' ) )\n    msg[ 'To' ] = ', '.join( to )\n    msg[ 'From' ] = frm\n    msg[ 'Subject' ] = subject\n    if config.smtp_server is None:\n        log.error( \"Mail is not configured for this Galaxy instance.\" )\n        log.info( msg )\n        return\n    smtp_ssl = asbool( getattr(config, 'smtp_ssl', False ) )\n    if smtp_ssl:\n        s = smtplib.SMTP_SSL()\n    else:\n        s = smtplib.SMTP()\n    s.connect( config.smtp_server )\n    if not smtp_ssl:\n        try:\n            s.starttls()\n            log.debug( 'Initiated SSL/TLS connection to SMTP server: %s' % config.smtp_server )\n        except RuntimeError, e:\n            log.warning( 'SSL/TLS support is not available to your Python interpreter: %s' % e )\n        except smtplib.SMTPHeloError, e:\n            log.error( \"The server didn't reply properly to the HELO greeting: %s\" % e )\n            s.close()\n            raise\n        except smtplib.SMTPException, e:\n            log.warning( 'The server does not support the STARTTLS extension: %s' % e )\n    if config.smtp_username and config.smtp_password:\n        try:\n            s.login( config.smtp_username, config.smtp_password )\n        except smtplib.SMTPHeloError, e:\n            log.error( \"The server didn't reply properly to the HELO greeting: %s\" % e )\n            s.close()\n            raise\n        except smtplib.SMTPAuthenticationError, e:\n            log.error( \"The server didn't accept the username/password combination: %s\" % e )\n            s.close()\n            raise\n        except smtplib.SMTPException, e:\n            log.error( \"No suitable authentication method was found: %s\" % e )\n            s.close()\n            raise\n    s.sendmail( frm, to, msg.as_string() )\n    s.quit()\n\n\ndef force_symlink( source, link_name ):\n    try:\n        os.symlink( source, link_name )\n    except OSError, e:\n        if e.errno == errno.EEXIST:\n            os.remove( link_name )\n            os.symlink( source, link_name )\n        else:\n            raise e\n\n\ndef move_merge( source, target ):\n    # when using shutil and moving a directory, if the target exists,\n    # then the directory is placed inside of it\n    # if the target doesn't exist, then the target is made into the directory\n    # this makes it so that the target is always the target, and if it exists,\n    # the source contents are moved into the target\n    if os.path.isdir( source ) and os.path.exists( target ) and os.path.isdir( target ):\n        for name in os.listdir( source ):\n            move_merge( os.path.join( source, name ), os.path.join( target, name ) )\n    else:\n        return shutil.move( source, target )\n\n\ndef safe_str_cmp(a, b):\n    if len(a) != len(b):\n        return False\n    rv = 0\n    for x, y in izip(a, b):\n        rv |= ord(x) ^ ord(y)\n    return rv == 0\n\ngalaxy_root_path = os.path.join(__path__[0], \"..\", \"..\", \"..\")\n\n\ndef galaxy_directory():\n    return os.path.abspath(galaxy_root_path)\n\nif __name__ == '__main__':\n    import doctest\n    doctest.testmod(sys.modules[__name__], verbose=False)\n", "code_before": "# -*- coding: utf-8 -*-\n\"\"\"\nUtility functions used systemwide.\n\n\"\"\"\n\nfrom __future__ import absolute_import\n\nimport binascii\nimport collections\nimport errno\nimport grp\nimport logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nimport smtplib\nimport stat\nimport string\nimport sys\nimport tempfile\nimport threading\n\nfrom galaxy.util import json\n\nfrom email.MIMEText import MIMEText\n\nfrom os.path import relpath\nfrom hashlib import md5\nfrom itertools import izip\n\nfrom urlparse import urlparse\n\nfrom galaxy import eggs\n\neggs.require( 'docutils' )\nimport docutils.core\nimport docutils.writers.html4css1\n\neggs.require( 'elementtree' )\nfrom elementtree import ElementTree, ElementInclude\n\neggs.require( \"wchartype\" )\nimport wchartype\n\nfrom .inflection import Inflector, English\ninflector = Inflector(English)\n\nlog = logging.getLogger(__name__)\n_lock = threading.RLock()\n\nCHUNK_SIZE = 65536  # 64k\n\nDATABASE_MAX_STRING_SIZE = 32768\nDATABASE_MAX_STRING_SIZE_PRETTY = '32K'\n\ngzip_magic = '\\037\\213'\nbz2_magic = 'BZh'\nDEFAULT_ENCODING = os.environ.get('GALAXY_DEFAULT_ENCODING', 'utf-8')\nNULL_CHAR = '\\000'\nBINARY_CHARS = [ NULL_CHAR ]\n\n\ndef is_multi_byte( chars ):\n    for char in chars:\n        try:\n            char = unicode( char )\n        except UnicodeDecodeError:\n            # Probably binary\n            return False\n        if ( wchartype.is_asian( char ) or wchartype.is_full_width( char ) or\n             wchartype.is_kanji( char ) or wchartype.is_hiragana( char ) or\n             wchartype.is_katakana( char ) or wchartype.is_half_katakana( char )\n             or wchartype.is_hangul( char ) or wchartype.is_full_digit( char )\n             or wchartype.is_full_letter( char )):\n            return True\n    return False\n\n\ndef is_binary( value, binary_chars=None ):\n    \"\"\"\n    File is binary if it contains a null-byte by default (e.g. behavior of grep, etc.).\n    This may fail for utf-16 files, but so would ASCII encoding.\n    >>> is_binary( string.printable )\n    False\n    >>> is_binary( '\\\\xce\\\\x94' )\n    False\n    >>> is_binary( '\\\\000' )\n    True\n    \"\"\"\n    if binary_chars is None:\n        binary_chars = BINARY_CHARS\n    for binary_char in binary_chars:\n        if binary_char in value:\n            return True\n    return False\n\n\ndef is_uuid( value ):\n    \"\"\"\n    This method returns True if value is a UUID, otherwise False.\n    >>> is_uuid( \"123e4567-e89b-12d3-a456-426655440000\" )\n    True\n    >>> is_uuid( \"0x3242340298902834\" )\n    False\n    \"\"\"\n    uuid_re = re.compile( \"[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\" )\n    if re.match( uuid_re, str( value ) ):\n        return True\n    else:\n        return False\n\n\ndef get_charset_from_http_headers( headers, default=None ):\n    rval = headers.get('content-type', None )\n    if rval and 'charset=' in rval:\n        rval = rval.split('charset=')[-1].split(';')[0].strip()\n        if rval:\n            return rval\n    return default\n\n\ndef synchronized(func):\n    \"\"\"This wrapper will serialize access to 'func' to a single thread. Use it as a decorator.\"\"\"\n    def caller(*params, **kparams):\n        _lock.acquire(True)  # Wait\n        try:\n            return func(*params, **kparams)\n        finally:\n            _lock.release()\n    return caller\n\n\ndef file_iter(fname, sep=None):\n    \"\"\"\n    This generator iterates over a file and yields its lines\n    splitted via the C{sep} parameter. Skips empty lines and lines starting with\n    the C{#} character.\n\n    >>> lines = [ line for line in file_iter(__file__) ]\n    >>> len(lines) !=  0\n    True\n    \"\"\"\n    for line in file(fname):\n        if line and line[0] != '#':\n            yield line.split(sep)\n\n\ndef file_reader( fp, chunk_size=CHUNK_SIZE ):\n    \"\"\"This generator yields the open fileobject in chunks (default 64k). Closes the file at the end\"\"\"\n    while 1:\n        data = fp.read(chunk_size)\n        if not data:\n            break\n        yield data\n    fp.close()\n\n\ndef unique_id(KEY_SIZE=128):\n    \"\"\"\n    Generates an unique id\n\n    >>> ids = [ unique_id() for i in range(1000) ]\n    >>> len(set(ids))\n    1000\n    \"\"\"\n    return md5(str( random.getrandbits( KEY_SIZE ) )).hexdigest()\n\n\ndef parse_xml(fname):\n    \"\"\"Returns a parsed xml tree\"\"\"\n    tree = ElementTree.parse(fname)\n    root = tree.getroot()\n    ElementInclude.include(root)\n    return tree\n\n\ndef parse_xml_string(xml_string):\n    tree = ElementTree.fromstring(xml_string)\n    return tree\n\n\ndef xml_to_string( elem, pretty=False ):\n    \"\"\"Returns a string from an xml tree\"\"\"\n    if pretty:\n        elem = pretty_print_xml( elem )\n    try:\n        return ElementTree.tostring( elem )\n    except TypeError, e:\n        # we assume this is a comment\n        if hasattr( elem, 'text' ):\n            return \"<!-- %s -->\\n\" % ( elem.text )\n        else:\n            raise e\n\n\ndef xml_element_compare( elem1, elem2 ):\n    if not isinstance( elem1, dict ):\n        elem1 = xml_element_to_dict( elem1 )\n    if not isinstance( elem2, dict ):\n        elem2 = xml_element_to_dict( elem2 )\n    return elem1 == elem2\n\n\ndef xml_element_list_compare( elem_list1, elem_list2 ):\n    return [ xml_element_to_dict( elem ) for elem in elem_list1  ] == [ xml_element_to_dict( elem ) for elem in elem_list2  ]\n\n\ndef xml_element_to_dict( elem ):\n    rval = {}\n    if elem.attrib:\n        rval[ elem.tag ] = {}\n    else:\n        rval[ elem.tag ] = None\n\n    sub_elems = list( elem )\n    if sub_elems:\n        sub_elem_dict = dict()\n        for sub_sub_elem_dict in map( xml_element_to_dict, sub_elems ):\n            for key, value in sub_sub_elem_dict.iteritems():\n                if key not in sub_elem_dict:\n                    sub_elem_dict[ key ] = []\n                sub_elem_dict[ key ].append( value )\n        for key, value in sub_elem_dict.iteritems():\n            if len( value ) == 1:\n                rval[ elem.tag ][ key ] = value[0]\n            else:\n                rval[ elem.tag ][ key ] = value\n    if elem.attrib:\n        for key, value in elem.attrib.iteritems():\n            rval[ elem.tag ][ \"@%s\" % key ] = value\n\n    if elem.text:\n        text = elem.text.strip()\n        if text and sub_elems or elem.attrib:\n            rval[ elem.tag ][ '#text' ] = text\n        else:\n            rval[ elem.tag ] = text\n\n    return rval\n\n\ndef pretty_print_xml( elem, level=0 ):\n    pad = '    '\n    i = \"\\n\" + level * pad\n    if len( elem ):\n        if not elem.text or not elem.text.strip():\n            elem.text = i + pad + pad\n        if not elem.tail or not elem.tail.strip():\n            elem.tail = i\n        for e in elem:\n            pretty_print_xml( e, level + 1 )\n        if not elem.tail or not elem.tail.strip():\n            elem.tail = i\n    else:\n        if level and ( not elem.tail or not elem.tail.strip() ):\n            elem.tail = i + pad\n    return elem\n\n\ndef get_file_size( value, default=None ):\n    try:\n        # try built-in\n        return os.path.getsize( value )\n    except:\n        try:\n            # try built-in one name attribute\n            return os.path.getsize( value.name )\n        except:\n            try:\n                # try tell() of end of object\n                offset = value.tell()\n                value.seek( 0, 2 )\n                rval = value.tell()\n                value.seek( offset )\n                return rval\n            except:\n                # return default value\n                return default\n\n\ndef shrink_stream_by_size( value, size, join_by=\"..\", left_larger=True, beginning_on_size_error=False, end_on_size_error=False ):\n    rval = ''\n    if get_file_size( value ) > size:\n        start = value.tell()\n        len_join_by = len( join_by )\n        min_size = len_join_by + 2\n        if size < min_size:\n            if beginning_on_size_error:\n                rval = value.read( size )\n                value.seek( start )\n                return rval\n            elif end_on_size_error:\n                value.seek( -size, 2 )\n                rval = value.read( size )\n                value.seek( start )\n                return rval\n            raise ValueError( 'With the provided join_by value (%s), the minimum size value is %i.' % ( join_by, min_size ) )\n        left_index = right_index = int( ( size - len_join_by ) / 2 )\n        if left_index + right_index + len_join_by < size:\n            if left_larger:\n                left_index += 1\n            else:\n                right_index += 1\n        rval = value.read( left_index ) + join_by\n        value.seek( -right_index, 2 )\n        rval += value.read( right_index )\n    else:\n        while True:\n            data = value.read( CHUNK_SIZE )\n            if not data:\n                break\n            rval += data\n    return rval\n\n\ndef shrink_string_by_size( value, size, join_by=\"..\", left_larger=True, beginning_on_size_error=False, end_on_size_error=False ):\n    if len( value ) > size:\n        len_join_by = len( join_by )\n        min_size = len_join_by + 2\n        if size < min_size:\n            if beginning_on_size_error:\n                return value[:size]\n            elif end_on_size_error:\n                return value[-size:]\n            raise ValueError( 'With the provided join_by value (%s), the minimum size value is %i.' % ( join_by, min_size ) )\n        left_index = right_index = int( ( size - len_join_by ) / 2 )\n        if left_index + right_index + len_join_by < size:\n            if left_larger:\n                left_index += 1\n            else:\n                right_index += 1\n        value = \"%s%s%s\" % ( value[:left_index], join_by, value[-right_index:] )\n    return value\n\n\ndef pretty_print_json(json_data, is_json_string=False):\n    if is_json_string:\n        json_data = json.loads(json_data)\n    return json.dumps(json_data, sort_keys=True, indent=4)\n\n# characters that are valid\nvalid_chars = set(string.letters + string.digits + \" -=_.()/+*^,:?!\")\n\n# characters that are allowed but need to be escaped\nmapped_chars = { '>': '__gt__',\n                 '<': '__lt__',\n                 \"'\": '__sq__',\n                 '\"': '__dq__',\n                 '[': '__ob__',\n                 ']': '__cb__',\n                 '{': '__oc__',\n                 '}': '__cc__',\n                 '@': '__at__',\n                 '\\n': '__cn__',\n                 '\\r': '__cr__',\n                 '\\t': '__tc__',\n                 '#': '__pd__'}\n\n\ndef restore_text(text):\n    \"\"\"Restores sanitized text\"\"\"\n    if not text:\n        return text\n    for key, value in mapped_chars.items():\n        text = text.replace(value, key)\n    return text\n\n\ndef sanitize_text(text):\n    \"\"\"\n    Restricts the characters that are allowed in text; accepts both strings\n    and lists of strings.\n    \"\"\"\n    if isinstance( text, basestring ):\n        return _sanitize_text_helper(text)\n    elif isinstance( text, list ):\n        return [ _sanitize_text_helper(t) for t in text ]\n\n\ndef _sanitize_text_helper(text):\n    \"\"\"Restricts the characters that are allowed in a string\"\"\"\n\n    out = []\n    for c in text:\n        if c in valid_chars:\n            out.append(c)\n        elif c in mapped_chars:\n            out.append(mapped_chars[c])\n        else:\n            out.append('X')  # makes debugging easier\n    return ''.join(out)\n\n\ndef sanitize_param(value):\n    \"\"\"Clean incoming parameters (strings or lists)\"\"\"\n    if isinstance( value, basestring ):\n        return sanitize_text(value)\n    elif isinstance( value, list ):\n        return map(sanitize_text, value)\n    else:\n        raise Exception('Unknown parameter type (%s)' % ( type( value ) ))\n\nvalid_filename_chars = set( string.ascii_letters + string.digits + '_.' )\ninvalid_filenames = [ '', '.', '..' ]\n\n\ndef sanitize_for_filename( text, default=None ):\n    \"\"\"\n    Restricts the characters that are allowed in a filename portion; Returns default value or a unique id string if result is not a valid name.\n    Method is overly aggressive to minimize possible complications, but a maximum length is not considered.\n    \"\"\"\n    out = []\n    for c in text:\n        if c in valid_filename_chars:\n            out.append( c )\n        else:\n            out.append( '_' )\n    out = ''.join( out )\n    if out in invalid_filenames:\n        if default is None:\n            return sanitize_for_filename( str( unique_id() ) )\n        return default\n    return out\n\n\ndef ready_name_for_url( raw_name ):\n    \"\"\" General method to convert a string (i.e. object name) to a URL-ready\n    slug.\n\n    >>> ready_name_for_url( \"My Cool Object\" )\n    'My-Cool-Object'\n    >>> ready_name_for_url( \"!My Cool Object!\" )\n    'My-Cool-Object'\n    >>> ready_name_for_url( \"Hello\u20a9\u25ce\u0491\u029f\u217e\" )\n    'Hello'\n    \"\"\"\n\n    # Replace whitespace with '-'\n    slug_base = re.sub( \"\\s+\", \"-\", raw_name )\n    # Remove all non-alphanumeric characters.\n    slug_base = re.sub( \"[^a-zA-Z0-9\\-]\", \"\", slug_base )\n    # Remove trailing '-'.\n    if slug_base.endswith('-'):\n        slug_base = slug_base[:-1]\n    return slug_base\n\n\ndef in_directory( file, directory, local_path_module=os.path ):\n    \"\"\"\n    Return true, if the common prefix of both is equal to directory\n    e.g. /a/b/c/d.rst and directory is /a/b, the common prefix is /a/b\n    \"\"\"\n\n    # Make both absolute.\n    directory = local_path_module.abspath(directory)\n    file = local_path_module.abspath(file)\n    return local_path_module.commonprefix([file, directory]) == directory\n\n\ndef merge_sorted_iterables( operator, *iterables ):\n    \"\"\"\n\n    >>> operator = lambda x: x\n    >>> list( merge_sorted_iterables( operator, [1,2,3], [4,5] ) )\n    [1, 2, 3, 4, 5]\n    >>> list( merge_sorted_iterables( operator, [4, 5], [1,2,3] ) )\n    [1, 2, 3, 4, 5]\n    >>> list( merge_sorted_iterables( operator, [1, 4, 5], [2], [3] ) )\n    [1, 2, 3, 4, 5]\n    \"\"\"\n    first_iterable = iterables[ 0 ]\n    if len( iterables ) == 1:\n        for el in first_iterable:\n            yield el\n    else:\n        for el in __merge_two_sorted_iterables(\n            operator,\n            iter( first_iterable ),\n            merge_sorted_iterables( operator, *iterables[ 1: ] )\n        ):\n            yield el\n\n\ndef __merge_two_sorted_iterables( operator, iterable1, iterable2 ):\n    unset = object()\n    continue_merge = True\n    next_1 = unset\n    next_2 = unset\n    while continue_merge:\n        try:\n            if next_1 is unset:\n                next_1 = next( iterable1 )\n            if next_2 is unset:\n                next_2 = next( iterable2 )\n            if operator( next_2 ) < operator( next_1 ):\n                yield next_2\n                next_2 = unset\n            else:\n                yield next_1\n                next_1 = unset\n        except StopIteration:\n            continue_merge = False\n    if next_1 is not unset:\n        yield next_1\n    if next_2 is not unset:\n        yield next_2\n    for el in iterable1:\n        yield el\n    for el in iterable2:\n        yield el\n\n\nclass Params( object ):\n    \"\"\"\n    Stores and 'sanitizes' parameters. Alphanumeric characters and the\n    non-alphanumeric ones that are deemed safe are let to pass through (see L{valid_chars}).\n    Some non-safe characters are escaped to safe forms for example C{>} becomes C{__lt__}\n    (see L{mapped_chars}). All other characters are replaced with C{X}.\n\n    Operates on string or list values only (HTTP parameters).\n\n    >>> values = { 'status':'on', 'symbols':[  'alpha', '<>', '$rm&#!' ]  }\n    >>> par = Params(values)\n    >>> par.status\n    'on'\n    >>> par.value == None      # missing attributes return None\n    True\n    >>> par.get('price', 0)\n    0\n    >>> par.symbols            # replaces unknown symbols with X\n    ['alpha', '__lt____gt__', 'XrmX__pd__!']\n    >>> par.flatten()          # flattening to a list\n    [('status', 'on'), ('symbols', 'alpha'), ('symbols', '__lt____gt__'), ('symbols', 'XrmX__pd__!')]\n    \"\"\"\n\n    # is NEVER_SANITIZE required now that sanitizing for tool parameters can be controlled on a per parameter basis and occurs via InputValueWrappers?\n    NEVER_SANITIZE = ['file_data', 'url_paste', 'URL', 'filesystem_paths']\n\n    def __init__( self, params, sanitize=True ):\n        if sanitize:\n            for key, value in params.items():\n                if key not in self.NEVER_SANITIZE and True not in [ key.endswith( \"|%s\" % nonsanitize_parameter ) for nonsanitize_parameter in self.NEVER_SANITIZE ]:  # sanitize check both ungrouped and grouped parameters by name. Anything relying on NEVER_SANITIZE should be changed to not require this and NEVER_SANITIZE should be removed.\n                    self.__dict__[ key ] = sanitize_param( value )\n                else:\n                    self.__dict__[ key ] = value\n        else:\n            self.__dict__.update(params)\n\n    def flatten(self):\n        \"\"\"\n        Creates a tuple list from a dict with a tuple/value pair for every value that is a list\n        \"\"\"\n        flat = []\n        for key, value in self.__dict__.items():\n            if isinstance(value, list):\n                for v in value:\n                    flat.append( (key, v) )\n            else:\n                flat.append( (key, value) )\n        return flat\n\n    def __getattr__(self, name):\n        \"\"\"This is here to ensure that we get None for non existing parameters\"\"\"\n        return None\n\n    def get(self, key, default):\n        return self.__dict__.get(key, default)\n\n    def __str__(self):\n        return '%s' % self.__dict__\n\n    def __len__(self):\n        return len(self.__dict__)\n\n    def __iter__(self):\n        return iter(self.__dict__)\n\n    def update(self, values):\n        self.__dict__.update(values)\n\n\ndef rst_to_html( s ):\n    \"\"\"Convert a blob of reStructuredText to HTML\"\"\"\n    log = logging.getLogger( \"docutils\" )\n\n    class FakeStream( object ):\n        def write( self, str ):\n            if len( str ) > 0 and not str.isspace():\n                log.warn( str )\n    return unicodify( docutils.core.publish_string( s,\n                      writer=docutils.writers.html4css1.Writer(),\n                      settings_overrides={ \"embed_stylesheet\": False, \"template\": os.path.join(os.path.dirname(__file__), \"docutils_template.txt\"), \"warning_stream\": FakeStream() } ) )\n\n\ndef xml_text(root, name=None):\n    \"\"\"Returns the text inside an element\"\"\"\n    if name is not None:\n        # Try attribute first\n        val = root.get(name)\n        if val:\n            return val\n        # Then try as element\n        elem = root.find(name)\n    else:\n        elem = root\n    if elem is not None and elem.text:\n        text = ''.join(elem.text.splitlines())\n        return text.strip()\n    # No luck, return empty string\n    return ''\n\n# asbool implementation pulled from PasteDeploy\ntruthy = frozenset(['true', 'yes', 'on', 'y', 't', '1'])\nfalsy = frozenset(['false', 'no', 'off', 'n', 'f', '0'])\n\n\ndef asbool(obj):\n    if isinstance(obj, basestring):\n        obj = obj.strip().lower()\n        if obj in truthy:\n            return True\n        elif obj in falsy:\n            return False\n        else:\n            raise ValueError(\"String is not true/false: %r\" % obj)\n    return bool(obj)\n\n\ndef string_as_bool( string ):\n    if str( string ).lower() in ( 'true', 'yes', 'on' ):\n        return True\n    else:\n        return False\n\n\ndef string_as_bool_or_none( string ):\n    \"\"\"\n    Returns True, None or False based on the argument:\n        True if passed True, 'True', 'Yes', or 'On'\n        None if passed None or 'None'\n        False otherwise\n\n    Note: string comparison is case-insensitive so lowecase versions of those\n    function equivalently.\n    \"\"\"\n    string = str( string ).lower()\n    if string in ( 'true', 'yes', 'on' ):\n        return True\n    elif string == 'none':\n        return None\n    else:\n        return False\n\n\ndef listify( item, do_strip=False ):\n    \"\"\"\n    Make a single item a single item list, or return a list if passed a\n    list.  Passing a None returns an empty list.\n    \"\"\"\n    if not item:\n        return []\n    elif isinstance( item, list ):\n        return item\n    elif isinstance( item, basestring ) and item.count( ',' ):\n        if do_strip:\n            return [token.strip() for token in item.split( ',' )]\n        else:\n            return item.split( ',' )\n    else:\n        return [ item ]\n\n\ndef commaify(amount):\n    orig = amount\n    new = re.sub(\"^(-?\\d+)(\\d{3})\", '\\g<1>,\\g<2>', amount)\n    if orig == new:\n        return new\n    else:\n        return commaify(new)\n\n\ndef roundify(amount, sfs=2):\n    \"\"\"\n    Take a number in string form and truncate to 'sfs' significant figures.\n    \"\"\"\n    if len(amount) <= sfs:\n        return amount\n    else:\n        return amount[0:sfs] + '0'*(len(amount) - sfs)\n\n\ndef unicodify( value, encoding=DEFAULT_ENCODING, error='replace', default=None ):\n    \"\"\"\n    Returns a unicode string or None\n    \"\"\"\n\n    if isinstance( value, unicode ):\n        return value\n    try:\n        return unicode( str( value ), encoding, error )\n    except:\n        return default\n\n\ndef smart_str(s, encoding='utf-8', strings_only=False, errors='strict'):\n    \"\"\"\n    Returns a bytestring version of 's', encoded as specified in 'encoding'.\n\n    If strings_only is True, don't convert (some) non-string-like objects.\n\n    Adapted from an older, simpler version of django.utils.encoding.smart_str.\n    \"\"\"\n    if strings_only and isinstance(s, (type(None), int)):\n        return s\n    if not isinstance(s, basestring):\n        try:\n            return str(s)\n        except UnicodeEncodeError:\n            return unicode(s).encode(encoding, errors)\n    elif isinstance(s, unicode):\n        return s.encode(encoding, errors)\n    elif s and encoding != 'utf-8':\n        return s.decode('utf-8', errors).encode(encoding, errors)\n    else:\n        return s\n\n\ndef object_to_string( obj ):\n    return binascii.hexlify( obj )\n\n\ndef string_to_object( s ):\n    return binascii.unhexlify( s )\n\n\nclass ParamsWithSpecs( collections.defaultdict ):\n    \"\"\"\n    \"\"\"\n\n    def __init__( self, specs=None, params=None ):\n        self.specs = specs or dict()\n        self.params = params or dict()\n        for name, value in self.params.items():\n            if name not in self.specs:\n                self._param_unknown_error( name )\n            if 'map' in self.specs[ name ]:\n                try:\n                    self.params[ name ] = self.specs[ name ][ 'map' ]( value )\n                except Exception:\n                    self._param_map_error( name, value )\n            if 'valid' in self.specs[ name ]:\n                if not self.specs[ name ][ 'valid' ]( value ):\n                    self._param_vaildation_error( name, value )\n\n        self.update( self.params )\n\n    def __missing__( self, name ):\n        return self.specs[ name ][ 'default' ]\n\n    def __getattr__( self, name ):\n        return self[ name ]\n\n    def _param_unknown_error( self, name ):\n        raise NotImplementedError()\n\n    def _param_map_error( self, name, value ):\n        raise NotImplementedError()\n\n    def _param_vaildation_error( self, name, value ):\n        raise NotImplementedError()\n\n\ndef compare_urls( url1, url2, compare_scheme=True, compare_hostname=True, compare_path=True ):\n    url1 = urlparse( url1 )\n    url2 = urlparse( url2 )\n    if compare_scheme and url1.scheme and url2.scheme and url1.scheme != url2.scheme:\n        return False\n    if compare_hostname and url1.hostname and url2.hostname and url1.hostname != url2.hostname:\n        return False\n    if compare_path and url1.path and url2.path and url1.path != url2.path:\n        return False\n    return True\n\n\ndef read_dbnames(filename):\n    \"\"\" Read build names from file \"\"\"\n    class DBNames( list ):\n        default_value = \"?\"\n        default_name = \"unspecified (?)\"\n    db_names = DBNames()\n    try:\n        ucsc_builds = {}\n        man_builds = []  # assume these are integers\n        name_to_db_base = {}\n        if filename is None:\n            # Should only be happening with the galaxy.tools.parameters.basic:GenomeBuildParameter docstring unit test\n            filename = os.path.join( 'tool-data', 'shared', 'ucsc', 'builds.txt.sample' )\n        for line in open(filename):\n            try:\n                if line[0:1] == \"#\":\n                    continue\n                fields = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").split(\"\\t\")\n                # Special case of unspecified build is at top of list\n                if fields[0] == \"?\":\n                    db_names.insert(0, (fields[0], fields[1]))\n                    continue\n                try:  # manual build (i.e. microbes)\n                    int(fields[0])\n                    man_builds.append((fields[1], fields[0]))\n                except:  # UCSC build\n                    db_base = fields[0].rstrip('0123456789')\n                    if db_base not in ucsc_builds:\n                        ucsc_builds[db_base] = []\n                        name_to_db_base[fields[1]] = db_base\n                    # we want to sort within a species numerically by revision number\n                    build_rev = re.compile(r'\\d+$')\n                    try:\n                        build_rev = int(build_rev.findall(fields[0])[0])\n                    except:\n                        build_rev = 0\n                    ucsc_builds[db_base].append((build_rev, fields[0], fields[1]))\n            except:\n                continue\n        sort_names = name_to_db_base.keys()\n        sort_names.sort()\n        for name in sort_names:\n            db_base = name_to_db_base[name]\n            ucsc_builds[db_base].sort()\n            ucsc_builds[db_base].reverse()\n            ucsc_builds[db_base] = [(build, name) for _, build, name in ucsc_builds[db_base]]\n            db_names = DBNames( db_names + ucsc_builds[db_base] )\n        if len( db_names ) > 1 and len( man_builds ) > 0:\n            db_names.append( ( db_names.default_value, '----- Additional Species Are Below -----' ) )\n        man_builds.sort()\n        man_builds = [(build, name) for name, build in man_builds]\n        db_names = DBNames( db_names + man_builds )\n    except Exception, e:\n        print \"ERROR: Unable to read builds file:\", e\n    if len(db_names) < 1:\n        db_names = DBNames( [( db_names.default_value,  db_names.default_name )] )\n    return db_names\n\n\ndef read_build_sites( filename, check_builds=True ):\n    \"\"\" read db names to ucsc mappings from file, this file should probably be merged with the one above \"\"\"\n    build_sites = []\n    try:\n        for line in open(filename):\n            try:\n                if line[0:1] == \"#\":\n                    continue\n                fields = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").split(\"\\t\")\n                site_name = fields[0]\n                site = fields[1]\n                if check_builds:\n                    site_builds = fields[2].split(\",\")\n                    site_dict = {'name': site_name, 'url': site, 'builds': site_builds}\n                else:\n                    site_dict = {'name': site_name, 'url': site}\n                build_sites.append( site_dict )\n            except:\n                continue\n    except:\n        print \"ERROR: Unable to read builds for site file %s\" % filename\n    return build_sites\n\n\ndef relativize_symlinks( path, start=None, followlinks=False):\n    for root, dirs, files in os.walk( path, followlinks=followlinks ):\n        rel_start = None\n        for file_name in files:\n            symlink_file_name = os.path.join( root, file_name )\n            if os.path.islink( symlink_file_name ):\n                symlink_target = os.readlink( symlink_file_name )\n                if rel_start is None:\n                    if start is None:\n                        rel_start = root\n                    else:\n                        rel_start = start\n                rel_path = relpath( symlink_target, rel_start )\n                os.remove( symlink_file_name )\n                os.symlink( rel_path, symlink_file_name )\n\n\ndef stringify_dictionary_keys( in_dict ):\n    # returns a new dictionary\n    # changes unicode keys into strings, only works on top level (does not recurse)\n    # unicode keys are not valid for expansion into keyword arguments on method calls\n    out_dict = {}\n    for key, value in in_dict.iteritems():\n        out_dict[ str( key ) ] = value\n    return out_dict\n\n\ndef recursively_stringify_dictionary_keys( d ):\n    if isinstance(d, dict):\n        return dict([(k.encode( DEFAULT_ENCODING ), recursively_stringify_dictionary_keys(v)) for k, v in d.iteritems()])\n    elif isinstance(d, list):\n        return [recursively_stringify_dictionary_keys(x) for x in d]\n    else:\n        return d\n\n\ndef mkstemp_ln( src, prefix='mkstemp_ln_' ):\n    \"\"\"\n    From tempfile._mkstemp_inner, generate a hard link in the same dir with a\n    random name.  Created so we can persist the underlying file of a\n    NamedTemporaryFile upon its closure.\n    \"\"\"\n    dir = os.path.dirname(src)\n    names = tempfile._get_candidate_names()\n    for seq in xrange(tempfile.TMP_MAX):\n        name = names.next()\n        file = os.path.join(dir, prefix + name)\n        try:\n            os.link( src, file )\n            return (os.path.abspath(file))\n        except OSError, e:\n            if e.errno == errno.EEXIST:\n                continue  # try again\n            raise\n    raise IOError(errno.EEXIST, \"No usable temporary file name found\")\n\n\ndef umask_fix_perms( path, umask, unmasked_perms, gid=None ):\n    \"\"\"\n    umask-friendly permissions fixing\n    \"\"\"\n    perms = unmasked_perms & ~umask\n    try:\n        st = os.stat( path )\n    except OSError, e:\n        log.exception( 'Unable to set permissions or group on %s' % path )\n        return\n    # fix modes\n    if stat.S_IMODE( st.st_mode ) != perms:\n        try:\n            os.chmod( path, perms )\n        except Exception, e:\n            log.warning( 'Unable to honor umask (%s) for %s, tried to set: %s but mode remains %s, error was: %s' % ( oct( umask ),\n                                                                                                                      path,\n                                                                                                                      oct( perms ),\n                                                                                                                      oct( stat.S_IMODE( st.st_mode ) ),\n                                                                                                                      e ) )\n    # fix group\n    if gid is not None and st.st_gid != gid:\n        try:\n            os.chown( path, -1, gid )\n        except Exception, e:\n            try:\n                desired_group = grp.getgrgid( gid )\n                current_group = grp.getgrgid( st.st_gid )\n            except:\n                desired_group = gid\n                current_group = st.st_gid\n            log.warning( 'Unable to honor primary group (%s) for %s, group remains %s, error was: %s' % ( desired_group,\n                                                                                                          path,\n                                                                                                          current_group,\n                                                                                                          e ) )\n\n\ndef docstring_trim(docstring):\n    \"\"\"Trimming python doc strings. Taken from: http://www.python.org/dev/peps/pep-0257/\"\"\"\n    if not docstring:\n        return ''\n    # Convert tabs to spaces (following the normal Python rules)\n    # and split into a list of lines:\n    lines = docstring.expandtabs().splitlines()\n    # Determine minimum indentation (first line doesn't count):\n    indent = sys.maxint\n    for line in lines[1:]:\n        stripped = line.lstrip()\n        if stripped:\n            indent = min(indent, len(line) - len(stripped))\n    # Remove indentation (first line is special):\n    trimmed = [lines[0].strip()]\n    if indent < sys.maxint:\n        for line in lines[1:]:\n            trimmed.append(line[indent:].rstrip())\n    # Strip off trailing and leading blank lines:\n    while trimmed and not trimmed[-1]:\n        trimmed.pop()\n    while trimmed and not trimmed[0]:\n        trimmed.pop(0)\n    # Return a single string:\n    return '\\n'.join(trimmed)\n\n\ndef nice_size(size):\n    \"\"\"\n    Returns a readably formatted string with the size\n\n    >>> nice_size(100)\n    '100 bytes'\n    >>> nice_size(10000)\n    '9.8 KB'\n    >>> nice_size(1000000)\n    '976.6 KB'\n    >>> nice_size(100000000)\n    '95.4 MB'\n    \"\"\"\n    words = [ 'bytes', 'KB', 'MB', 'GB', 'TB' ]\n    try:\n        size = float( size )\n    except:\n        return '??? bytes'\n    for ind, word in enumerate(words):\n        step = 1024 ** (ind + 1)\n        if step > size:\n            size = size / float(1024 ** ind)\n            if word == 'bytes':  # No decimals for bytes\n                return \"%d bytes\" % size\n            return \"%.1f %s\" % (size, word)\n    return '??? bytes'\n\n\ndef size_to_bytes( size ):\n    \"\"\"\n    Returns a number of bytes if given a reasonably formatted string with the size\n    \"\"\"\n    # Assume input in bytes if we can convert directly to an int\n    try:\n        return int( size )\n    except:\n        pass\n    # Otherwise it must have non-numeric characters\n    size_re = re.compile( '([\\d\\.]+)\\s*([tgmk]b?|b|bytes?)$' )\n    size_match = re.match( size_re, size.lower() )\n    assert size_match is not None\n    size = float( size_match.group(1) )\n    multiple = size_match.group(2)\n    if multiple.startswith( 't' ):\n        return int( size * 1024**4 )\n    elif multiple.startswith( 'g' ):\n        return int( size * 1024**3 )\n    elif multiple.startswith( 'm' ):\n        return int( size * 1024**2 )\n    elif multiple.startswith( 'k' ):\n        return int( size * 1024 )\n    elif multiple.startswith( 'b' ):\n        return int( size )\n\n\ndef send_mail( frm, to, subject, body, config ):\n    \"\"\"\n    Sends an email.\n    \"\"\"\n    to = listify( to )\n    msg = MIMEText(  body.encode( 'ascii', 'replace' ) )\n    msg[ 'To' ] = ', '.join( to )\n    msg[ 'From' ] = frm\n    msg[ 'Subject' ] = subject\n    if config.smtp_server is None:\n        log.error( \"Mail is not configured for this Galaxy instance.\" )\n        log.info( msg )\n        return\n    smtp_ssl = asbool( getattr(config, 'smtp_ssl', False ) )\n    if smtp_ssl:\n        s = smtplib.SMTP_SSL()\n    else:\n        s = smtplib.SMTP()\n    s.connect( config.smtp_server )\n    if not smtp_ssl:\n        try:\n            s.starttls()\n            log.debug( 'Initiated SSL/TLS connection to SMTP server: %s' % config.smtp_server )\n        except RuntimeError, e:\n            log.warning( 'SSL/TLS support is not available to your Python interpreter: %s' % e )\n        except smtplib.SMTPHeloError, e:\n            log.error( \"The server didn't reply properly to the HELO greeting: %s\" % e )\n            s.close()\n            raise\n        except smtplib.SMTPException, e:\n            log.warning( 'The server does not support the STARTTLS extension: %s' % e )\n    if config.smtp_username and config.smtp_password:\n        try:\n            s.login( config.smtp_username, config.smtp_password )\n        except smtplib.SMTPHeloError, e:\n            log.error( \"The server didn't reply properly to the HELO greeting: %s\" % e )\n            s.close()\n            raise\n        except smtplib.SMTPAuthenticationError, e:\n            log.error( \"The server didn't accept the username/password combination: %s\" % e )\n            s.close()\n            raise\n        except smtplib.SMTPException, e:\n            log.error( \"No suitable authentication method was found: %s\" % e )\n            s.close()\n            raise\n    s.sendmail( frm, to, msg.as_string() )\n    s.quit()\n\n\ndef force_symlink( source, link_name ):\n    try:\n        os.symlink( source, link_name )\n    except OSError, e:\n        if e.errno == errno.EEXIST:\n            os.remove( link_name )\n            os.symlink( source, link_name )\n        else:\n            raise e\n\n\ndef move_merge( source, target ):\n    # when using shutil and moving a directory, if the target exists,\n    # then the directory is placed inside of it\n    # if the target doesn't exist, then the target is made into the directory\n    # this makes it so that the target is always the target, and if it exists,\n    # the source contents are moved into the target\n    if os.path.isdir( source ) and os.path.exists( target ) and os.path.isdir( target ):\n        for name in os.listdir( source ):\n            move_merge( os.path.join( source, name ), os.path.join( target, name ) )\n    else:\n        return shutil.move( source, target )\n\n\ndef safe_str_cmp(a, b):\n    if len(a) != len(b):\n        return False\n    rv = 0\n    for x, y in izip(a, b):\n        rv |= ord(x) ^ ord(y)\n    return rv == 0\n\ngalaxy_root_path = os.path.join(__path__[0], \"..\", \"..\", \"..\")\n\n\ndef galaxy_directory():\n    return os.path.abspath(galaxy_root_path)\n\nif __name__ == '__main__':\n    import doctest\n    doctest.testmod(sys.modules[__name__], verbose=False)\n", "patch": "@@ -360,46 +360,57 @@ def pretty_print_json(json_data, is_json_string=False):\n                  '#': '__pd__'}\n \n \n-def restore_text(text):\n+def restore_text( text, character_map=mapped_chars ):\n     \"\"\"Restores sanitized text\"\"\"\n     if not text:\n         return text\n-    for key, value in mapped_chars.items():\n+    for key, value in character_map.items():\n         text = text.replace(value, key)\n     return text\n \n \n-def sanitize_text(text):\n+def sanitize_text( text, valid_characters=valid_chars, character_map=mapped_chars, invalid_character='X' ):\n     \"\"\"\n     Restricts the characters that are allowed in text; accepts both strings\n-    and lists of strings.\n+    and lists of strings; non-string entities will be cast to strings.\n     \"\"\"\n-    if isinstance( text, basestring ):\n-        return _sanitize_text_helper(text)\n-    elif isinstance( text, list ):\n-        return [ _sanitize_text_helper(t) for t in text ]\n+    if isinstance( text, list ):\n+        return map( lambda x: sanitize_text( x, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character ), text )\n+    if not isinstance( text, basestring ):\n+        text = smart_str( text )\n+    return _sanitize_text_helper( text, valid_characters=valid_characters, character_map=character_map )\n \n-\n-def _sanitize_text_helper(text):\n+def _sanitize_text_helper( text, valid_characters=valid_chars, character_map=mapped_chars, invalid_character='X' ):\n     \"\"\"Restricts the characters that are allowed in a string\"\"\"\n \n     out = []\n     for c in text:\n-        if c in valid_chars:\n+        if c in valid_characters:\n             out.append(c)\n-        elif c in mapped_chars:\n-            out.append(mapped_chars[c])\n+        elif c in character_map:\n+            out.append( character_map[c] )\n         else:\n-            out.append('X')  # makes debugging easier\n+            out.append( invalid_character )  # makes debugging easier\n     return ''.join(out)\n \n \n-def sanitize_param(value):\n+def sanitize_lists_to_string( values, valid_characters=valid_chars, character_map=mapped_chars, invalid_character='X'  ):\n+    if isinstance( values, list ):\n+        rval = []\n+        for value in values:\n+            rval.append( sanitize_lists_to_string( value, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character ) )\n+        values = \",\".join( rval )\n+    else:\n+        values = sanitize_text( values, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character )\n+    return values\n+\n+\n+def sanitize_param( value, valid_characters=valid_chars, character_map=mapped_chars, invalid_character='X' ):\n     \"\"\"Clean incoming parameters (strings or lists)\"\"\"\n     if isinstance( value, basestring ):\n-        return sanitize_text(value)\n+        return sanitize_text( value, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character )\n     elif isinstance( value, list ):\n-        return map(sanitize_text, value)\n+        return map( lambda x: sanitize_text( x, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character ), value )\n     else:\n         raise Exception('Unknown parameter type (%s)' % ( type( value ) ))\n ", "file_path": "files/2023_1/992", "file_language": "py", "file_name": "lib/galaxy/util/__init__.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/blankenberg/galaxy-data-resource/raw/50d65f45d3f5be5d1fbff2e45ac5cec075f07d42/lib%2Fgalaxy%2Futil%2Fdbkeys.py", "code": "\"\"\"\nFunctionality for dealing with dbkeys.\n\"\"\"\n#dbkeys read from disk using builds.txt\nfrom galaxy.util import read_dbnames\nfrom galaxy.util.json import loads\nfrom galaxy.util.object_wrapper import sanitize_lists_to_string\nimport os.path\n\n\nclass GenomeBuilds( object ):\n    default_value = \"?\"\n    default_name = \"unspecified (?)\"\n\n    def __init__( self, app, data_table_name=\"__dbkeys__\", load_old_style=True ):\n        self._app = app\n        self._data_table_name = data_table_name\n        self._static_chrom_info_path = app.config.len_file_path\n        # A dbkey can be listed multiple times, but with different names, so we can't use dictionaries for lookups\n        if load_old_style:\n            self._static_dbkeys = list( read_dbnames( app.config.builds_file_path ) )\n        else:\n            self._static_dbkeys = []\n\n    def get_genome_build_names( self, trans=None ):\n        # FIXME: how to deal with key duplicates?\n        rval = []\n        # load user custom genome builds\n        if trans is not None:\n            if trans.history:\n                # This is a little bit Odd. We are adding every .len file in the current history to dbkey list,\n                # but this is previous behavior from trans.db_names, so we'll continue to do it.\n                # It does allow one-off, history specific dbkeys to be created by a user. But we are not filtering,\n                # so a len file will be listed twice (as the build name and again as dataset name), \n                # if custom dbkey creation/conversion occurred within the current history.\n                datasets = trans.sa_session.query( self._app.model.HistoryDatasetAssociation ) \\\n                                          .filter_by( deleted=False, history_id=trans.history.id, extension=\"len\" )\n                for dataset in datasets:\n                    rval.append( ( dataset.dbkey, \"%s (%s) [History]\" % ( dataset.name, dataset.dbkey ) ) )\n            user = trans.user\n            if user and 'dbkeys' in user.preferences:\n                user_keys = loads( user.preferences['dbkeys'] )\n                for key, chrom_dict in user_keys.iteritems():\n                    rval.append( ( key, \"%s (%s) [Custom]\" % ( chrom_dict['name'], key ) ) )\n        # Load old builds.txt static keys\n        rval.extend( self._static_dbkeys )\n        #load dbkeys from dbkey data table\n        dbkey_table = self._app.tool_data_tables.get( self._data_table_name, None )\n        if dbkey_table is not None:\n            for field_dict in dbkey_table.get_named_fields_list():\n                rval.append( ( field_dict[ 'value' ], field_dict[ 'name' ] ) )\n        return rval\n\n    def get_chrom_info( self, dbkey, trans=None, custom_build_hack_get_len_from_fasta_conversion=True ):\n        # FIXME: flag to turn off custom_build_hack_get_len_from_fasta_conversion should not be required \n        chrom_info = None\n        db_dataset = None\n        # Collect chromInfo from custom builds\n        if trans:\n            db_dataset = trans.db_dataset_for( dbkey )\n            if db_dataset:\n                chrom_info = db_dataset.file_name\n            else:\n                # Do Custom Build handling\n                if trans.user and ( 'dbkeys' in trans.user.preferences ) and ( dbkey in loads( trans.user.preferences[ 'dbkeys' ] ) ):\n                    custom_build_dict = loads( trans.user.preferences[ 'dbkeys' ] )[ dbkey ]\n                    # HACK: the attempt to get chrom_info below will trigger the\n                    # fasta-to-len converter if the dataset is not available or,\n                    # which will in turn create a recursive loop when\n                    # running the fasta-to-len tool. So, use a hack in the second\n                    # condition below to avoid getting chrom_info when running the\n                    # fasta-to-len converter.\n                    if 'fasta' in custom_build_dict and custom_build_hack_get_len_from_fasta_conversion:\n                        # Build is defined by fasta; get len file, which is obtained from converting fasta.\n                        build_fasta_dataset = trans.sa_session.query( trans.app.model.HistoryDatasetAssociation ).get( custom_build_dict[ 'fasta' ] )\n                        chrom_info = build_fasta_dataset.get_converted_dataset( trans, 'len' ).file_name\n                    elif 'len' in custom_build_dict:\n                        # Build is defined by len file, so use it.\n                        chrom_info = trans.sa_session.query( trans.app.model.HistoryDatasetAssociation ).get( custom_build_dict[ 'len' ] ).file_name\n        # Check Data table\n        if not chrom_info:\n            dbkey_table = self._app.tool_data_tables.get( self._data_table_name, None )\n            if dbkey_table is not None:\n                chrom_info = dbkey_table.get_entry( 'value', dbkey, 'len_path', default=None )\n        # use configured server len path\n        if not chrom_info:\n            # Default to built-in build.\n            # Since we are using an unverified dbkey, we will sanitize the dbkey before use\n            chrom_info = os.path.join( self._static_chrom_info_path, \"%s.len\" % sanitize_lists_to_string( dbkey ) )\n        chrom_info = os.path.abspath( chrom_info )\n        return ( chrom_info, db_dataset )\n", "code_before": "\"\"\"\nFunctionality for dealing with dbkeys.\n\"\"\"\n#dbkeys read from disk using builds.txt\nfrom galaxy.util import read_dbnames\nfrom galaxy.util.json import loads\nimport os.path\n\n\nclass GenomeBuilds( object ):\n    default_value = \"?\"\n    default_name = \"unspecified (?)\"\n\n    def __init__( self, app, data_table_name=\"__dbkeys__\", load_old_style=True ):\n        self._app = app\n        self._data_table_name = data_table_name\n        self._static_chrom_info_path = app.config.len_file_path\n        # A dbkey can be listed multiple times, but with different names, so we can't use dictionaries for lookups\n        if load_old_style:\n            self._static_dbkeys = list( read_dbnames( app.config.builds_file_path ) )\n        else:\n            self._static_dbkeys = []\n\n    def get_genome_build_names( self, trans=None ):\n        # FIXME: how to deal with key duplicates?\n        rval = []\n        # load user custom genome builds\n        if trans is not None:\n            if trans.history:\n                # This is a little bit Odd. We are adding every .len file in the current history to dbkey list,\n                # but this is previous behavior from trans.db_names, so we'll continue to do it.\n                # It does allow one-off, history specific dbkeys to be created by a user. But we are not filtering,\n                # so a len file will be listed twice (as the build name and again as dataset name), \n                # if custom dbkey creation/conversion occurred within the current history.\n                datasets = trans.sa_session.query( self._app.model.HistoryDatasetAssociation ) \\\n                                          .filter_by( deleted=False, history_id=trans.history.id, extension=\"len\" )\n                for dataset in datasets:\n                    rval.append( ( dataset.dbkey, \"%s (%s) [History]\" % ( dataset.name, dataset.dbkey ) ) )\n            user = trans.user\n            if user and 'dbkeys' in user.preferences:\n                user_keys = loads( user.preferences['dbkeys'] )\n                for key, chrom_dict in user_keys.iteritems():\n                    rval.append( ( key, \"%s (%s) [Custom]\" % ( chrom_dict['name'], key ) ) )\n        # Load old builds.txt static keys\n        rval.extend( self._static_dbkeys )\n        #load dbkeys from dbkey data table\n        dbkey_table = self._app.tool_data_tables.get( self._data_table_name, None )\n        if dbkey_table is not None:\n            for field_dict in dbkey_table.get_named_fields_list():\n                rval.append( ( field_dict[ 'value' ], field_dict[ 'name' ] ) )\n        return rval\n\n    def get_chrom_info( self, dbkey, trans=None, custom_build_hack_get_len_from_fasta_conversion=True ):\n        # FIXME: flag to turn off custom_build_hack_get_len_from_fasta_conversion should not be required \n        chrom_info = None\n        db_dataset = None\n        # Collect chromInfo from custom builds\n        if trans:\n            db_dataset = trans.db_dataset_for( dbkey )\n            if db_dataset:\n                chrom_info = db_dataset.file_name\n            else:\n                # Do Custom Build handling\n                if trans.user and ( 'dbkeys' in trans.user.preferences ) and ( dbkey in loads( trans.user.preferences[ 'dbkeys' ] ) ):\n                    custom_build_dict = loads( trans.user.preferences[ 'dbkeys' ] )[ dbkey ]\n                    # HACK: the attempt to get chrom_info below will trigger the\n                    # fasta-to-len converter if the dataset is not available or,\n                    # which will in turn create a recursive loop when\n                    # running the fasta-to-len tool. So, use a hack in the second\n                    # condition below to avoid getting chrom_info when running the\n                    # fasta-to-len converter.\n                    if 'fasta' in custom_build_dict and custom_build_hack_get_len_from_fasta_conversion:\n                        # Build is defined by fasta; get len file, which is obtained from converting fasta.\n                        build_fasta_dataset = trans.sa_session.query( trans.app.model.HistoryDatasetAssociation ).get( custom_build_dict[ 'fasta' ] )\n                        chrom_info = build_fasta_dataset.get_converted_dataset( trans, 'len' ).file_name\n                    elif 'len' in custom_build_dict:\n                        # Build is defined by len file, so use it.\n                        chrom_info = trans.sa_session.query( trans.app.model.HistoryDatasetAssociation ).get( custom_build_dict[ 'len' ] ).file_name\n        # Check Data table\n        if not chrom_info:\n            dbkey_table = self._app.tool_data_tables.get( self._data_table_name, None )\n            if dbkey_table is not None:\n                chrom_info = dbkey_table.get_entry( 'value', dbkey, 'len_path', default=None )\n        # use configured server len path\n        if not chrom_info:\n            # Default to built-in build.\n            chrom_info = os.path.join( self._static_chrom_info_path, \"%s.len\" % dbkey )\n        chrom_info = os.path.abspath( chrom_info )\n        return ( chrom_info, db_dataset )\n", "patch": "@@ -4,6 +4,7 @@\n #dbkeys read from disk using builds.txt\n from galaxy.util import read_dbnames\n from galaxy.util.json import loads\n+from galaxy.util.object_wrapper import sanitize_lists_to_string\n import os.path\n \n \n@@ -84,6 +85,7 @@ def get_chrom_info( self, dbkey, trans=None, custom_build_hack_get_len_from_fast\n         # use configured server len path\n         if not chrom_info:\n             # Default to built-in build.\n-            chrom_info = os.path.join( self._static_chrom_info_path, \"%s.len\" % dbkey )\n+            # Since we are using an unverified dbkey, we will sanitize the dbkey before use\n+            chrom_info = os.path.join( self._static_chrom_info_path, \"%s.len\" % sanitize_lists_to_string( dbkey ) )\n         chrom_info = os.path.abspath( chrom_info )\n         return ( chrom_info, db_dataset )", "file_path": "files/2023_1/993", "file_language": "py", "file_name": "lib/galaxy/util/dbkeys.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/blankenberg/galaxy-data-resource/raw/50d65f45d3f5be5d1fbff2e45ac5cec075f07d42/lib%2Fgalaxy%2Futil%2Fobject_wrapper.py", "code": "\"\"\"\nClasses for wrapping Objects and Sanitizing string output.\n\"\"\"\n\nimport inspect\nimport copy_reg\nimport logging\nimport string\nfrom numbers import Number\nfrom types import ( NoneType, NotImplementedType, EllipsisType, FunctionType, MethodType, GeneratorType, CodeType,\n                    BuiltinFunctionType, BuiltinMethodType, ModuleType, XRangeType, SliceType, TracebackType, FrameType,\n                    BufferType, DictProxyType, GetSetDescriptorType, MemberDescriptorType )\nfrom UserDict import UserDict\n\nfrom galaxy.util import sanitize_lists_to_string as _sanitize_lists_to_string\n\nlog = logging.getLogger( __name__ )\n\n# Define different behaviors for different types, see also: https://docs.python.org/2/library/types.html\n\n# Known Callable types\n__CALLABLE_TYPES__ = ( FunctionType, MethodType, GeneratorType, CodeType, BuiltinFunctionType, BuiltinMethodType, )\n\n# Always wrap these types without attempting to subclass\n__WRAP_NO_SUBCLASS__ =  ( ModuleType, XRangeType, SliceType, BufferType, TracebackType, FrameType, DictProxyType,\n                          GetSetDescriptorType, MemberDescriptorType ) + __CALLABLE_TYPES__\n\n# Don't wrap or sanitize.\n__DONT_SANITIZE_TYPES__ = ( Number, bool, NoneType, NotImplementedType, EllipsisType, bytearray, )\n\n# Don't wrap, but do sanitize.\n__DONT_WRAP_TYPES__ = tuple() #( basestring, ) so that we can get the unsanitized string, we will now wrap basestring instances\n\n# Wrap contents, but not the container\n__WRAP_SEQUENCES__ = ( tuple, list, )\n__WRAP_SETS__ = ( set, frozenset, )\n__WRAP_MAPPINGS__ = ( dict, UserDict, )\n\n\n# Define the set of characters that are not sanitized, and define a set of mappings for those that are.\n# characters that are valid\nVALID_CHARACTERS = set( string.letters + string.digits + \" -=_.()/+*^,:?!@\" )\n\n# characters that are allowed but need to be escaped\nCHARACTER_MAP = { '>': '__gt__',\n                 '<': '__lt__',\n                 \"'\": '__sq__',\n                 '\"': '__dq__',\n                 '[': '__ob__',\n                 ']': '__cb__',\n                 '{': '__oc__',\n                 '}': '__cc__',\n                 '\\n': '__cn__',\n                 '\\r': '__cr__',\n                 '\\t': '__tc__',\n                 '#': '__pd__'}\n\nINVALID_CHARACTER = \"X\"\n\ndef sanitize_lists_to_string( values, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP, invalid_character=INVALID_CHARACTER  ):\n    return _sanitize_lists_to_string( values, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character  )\n\n\ndef wrap_with_safe_string( value, no_wrap_classes = None ):\n    \"\"\"\n    Recursively wrap values that should be wrapped.\n    \"\"\"\n\n    def __do_wrap( value ):\n        if isinstance( value, SafeStringWrapper ):\n            # Only ever wrap one-layer\n            return value\n        if callable( value ):\n            safe_class = CallableSafeStringWrapper\n        else:\n            safe_class = SafeStringWrapper\n        if isinstance( value, no_wrap_classes ):\n            return value\n        if isinstance( value, __DONT_WRAP_TYPES__ ):\n            return sanitize_lists_to_string( value, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP )\n        if isinstance( value, __WRAP_NO_SUBCLASS__ ):\n            return safe_class( value, safe_string_wrapper_function = __do_wrap )\n        for this_type in __WRAP_SEQUENCES__ + __WRAP_SETS__:\n            if isinstance( value, this_type ):\n                return this_type( map( __do_wrap, value ) )\n        for this_type in __WRAP_MAPPINGS__:\n            if isinstance( value, this_type ):\n                # Wrap both key and value\n                return this_type( map( lambda x: ( __do_wrap( x[0] ), __do_wrap( x[1] ) ), value.items() ) )\n        # Create a dynamic class that joins SafeStringWrapper with the object being wrapped.\n        # This allows e.g. isinstance to continue to work.\n        try:\n            wrapped_class_name = value.__name__\n            wrapped_class = value\n        except:\n            wrapped_class_name = value.__class__.__name__\n            wrapped_class = value.__class__\n        value_mod = inspect.getmodule( value )\n        if value_mod:\n            wrapped_class_name = \"%s.%s\" % ( value_mod.__name__, wrapped_class_name )\n        wrapped_class_name = \"SafeStringWrapper(%s:%s)\" % ( wrapped_class_name, \",\".join( sorted( map( str, no_wrap_classes ) ) ) )\n        do_wrap_func_name = \"__do_wrap_%s\" % ( wrapped_class_name )\n        do_wrap_func = __do_wrap\n        global_dict = globals()\n        if wrapped_class_name in global_dict:\n            # Check to see if we have created a wrapper for this class yet, if so, reuse\n            wrapped_class = global_dict.get( wrapped_class_name )\n            do_wrap_func = global_dict.get( do_wrap_func_name, __do_wrap )\n        else:\n            try:\n                wrapped_class = type( wrapped_class_name, ( safe_class, wrapped_class, ), {} )\n            except TypeError, e:\n                 # Fail-safe for when a class cannot be dynamically subclassed.\n                 log.warning( \"Unable to create dynamic subclass for %s, %s: %s\", type( value), value, e )\n                 wrapped_class = type( wrapped_class_name, ( safe_class, ), {} )\n            if wrapped_class not in ( SafeStringWrapper, CallableSafeStringWrapper ):\n                # Save this wrapper for reuse and pickling/copying\n                global_dict[ wrapped_class_name ] = wrapped_class\n                do_wrap_func.__name__ = do_wrap_func_name\n                global_dict[ do_wrap_func_name ] = do_wrap_func\n                def pickle_safe_object( safe_object ):\n                    return ( wrapped_class, ( safe_object.unsanitized, do_wrap_func, ) )\n                # Set pickle and copy properties\n                copy_reg.pickle( wrapped_class, pickle_safe_object, do_wrap_func )\n        return wrapped_class( value, safe_string_wrapper_function = do_wrap_func )\n    # Determine classes not to wrap\n    if no_wrap_classes:\n        if not isinstance( no_wrap_classes, ( tuple, list ) ):\n            no_wrap_classes = [ no_wrap_classes ]\n        no_wrap_classes = list( no_wrap_classes ) + list( __DONT_SANITIZE_TYPES__ ) + [ SafeStringWrapper ]\n    else:\n        no_wrap_classes = list( __DONT_SANITIZE_TYPES__ ) + [ SafeStringWrapper ]\n    no_wrap_classes = tuple( set( sorted( no_wrap_classes, key=str ) ) )\n    return __do_wrap( value )\n\n\n# N.B. refer to e.g. https://docs.python.org/2/reference/datamodel.html for information on Python's Data Model.\n\n\nclass SafeStringWrapper( object ):\n    \"\"\"\n    Class that wraps and sanitizes any provided value's attributes \n    that will attempt to be cast into a string.\n    \n    Attempts to mimic behavior of original class, including operands.\n    \n    To ensure proper handling of e.g. subclass checks, the *wrap_with_safe_string()*\n    method should be used.\n    \n    This wrapping occurs in a recursive/parasitic fashion, as all called attributes of \n    the originally wrapped object will also be wrapped and sanitized, unless the attribute\n    is of a type found in __DONT_SANITIZE_TYPES__ + __DONT_WRAP_TYPES__, where e.g. ~(strings\n    will still be sanitized, but not wrapped), and e.g. integers will have neither.\n    \"\"\"\n    __UNSANITIZED_ATTRIBUTE_NAME__ = 'unsanitized' \n    __NO_WRAP_NAMES__ = [ '__safe_string_wrapper_function__', __UNSANITIZED_ATTRIBUTE_NAME__]\n\n\n    def __new__( cls, *arg, **kwd ):\n        # We need to define a __new__ since, we are subclassing from e.g. immutable str, which internally sets data \n        # that will be used when other + this (this + other is handled by __add__)\n        safe_string_wrapper_function = kwd.get( 'safe_string_wrapper_function', None) or wrap_with_safe_string\n        try:\n            return super( SafeStringWrapper, cls ).__new__( cls, sanitize_lists_to_string( arg[0], valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP ) )\n        except Exception, e:\n            log.warning( \"Could not provide an argument to %s.__new__: %s; will try without arguments.\", cls, e )\n            return super( SafeStringWrapper, cls ).__new__( cls )\n\n    def __init__( self, value, safe_string_wrapper_function = wrap_with_safe_string ):\n        self.unsanitized = value\n        self.__safe_string_wrapper_function__ = safe_string_wrapper_function\n\n    def __str__( self ):\n        return sanitize_lists_to_string( self.unsanitized, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP )\n\n    def __repr__( self ):\n        return \"%s object at %x on: %s\" % ( sanitize_lists_to_string( self.__class__.__name__, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP ), id( self ), sanitize_lists_to_string( repr( self.unsanitized ), valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP ) )\n\n    def __lt__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.unsanitized < other\n\n    def __le__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.unsanitized <= other\n\n    def __eq__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.unsanitized == other\n\n    def __ne__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.unsanitized != other\n\n    def __gt__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.unsanitized > other\n\n    def __ge__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.unsanitized >= other\n\n    def __lt__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.unsanitized < other\n\n    def __cmp__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return cmp( self.unsanitized, other )\n\n    # Do not implement __rcmp__, python 2.2 < 2.6\n\n    def __hash__( self ):\n        return hash( self.unsanitized )\n\n    def __nonzero__( self ):\n        return bool( self.unsanitized )\n\n    # Do not implement __unicode__, we will rely on __str__\n\n    def __getattr__( self, name ):\n        if name in SafeStringWrapper.__NO_WRAP_NAMES__:\n            #FIXME: is this ever reached?\n            return object.__getattr__( self, name )\n        return self.__safe_string_wrapper_function__( getattr( self.unsanitized, name ) )\n\n    def __setattr__( self, name, value ):\n        if name in SafeStringWrapper.__NO_WRAP_NAMES__:\n            return object.__setattr__( self, name, value )\n        return setattr( self.unsanitized, name, value )\n\n    def __delattr__( self, name ):\n        if name in SafeStringWrapper.__NO_WRAP_NAMES__:\n            return object.__delattr__( self, name )\n        return delattr( self.unsanitized, name )\n\n    def __getattribute__( self, name ):\n        if name in SafeStringWrapper.__NO_WRAP_NAMES__:\n            return object.__getattribute__( self, name )\n        return self.__safe_string_wrapper_function__( getattr( object.__getattribute__( self, 'unsanitized' ), name ) )\n\n    # Skip Descriptors\n\n    # Skip __slots__\n    \n    # Don't need __metaclass__, we'll use the helper function to handle with subclassing for e.g. isinstance()\n\n    # Revisit:\n    # __instancecheck__\n    # __subclasscheck__\n    # We are using a helper class to create dynamic subclasses to handle class checks\n\n    # We address __call__ as needed based upon unsanitized, through the use of a CallableSafeStringWrapper class\n\n    def __len__( self ):\n        original_value = self.unsanitized\n        while isinstance( original_value, SafeStringWrapper ):\n            original_value = self.unsanitized\n        return len( self.unsanitized )\n\n    def __getitem__( self, key ):\n        return self.__safe_string_wrapper_function__( self.unsanitized[ key ] )\n\n    def __setitem__( self, key, value ):\n        while isinstance( value, SafeStringWrapper ):\n            value = value.unsanitized\n        self.unsanitized[ key ] = value\n\n    def __delitem__( self, key ):\n        del self.unsanitized[ key ]\n\n    def __iter__( self ):\n        return iter( map( self.__safe_string_wrapper_function__, iter( self.unsanitized ) ) )\n\n    # Do not implement __reversed__\n\n    def __contains__( self, item ):\n        # FIXME: Do we need to consider if item is/isn't or does/doesn't contain SafeStringWrapper?\n        # When considering e.g. nested lists/dicts/etc, this gets complicated\n        while isinstance( item, SafeStringWrapper ):\n            item = item.unsanitized\n        return item in self.unsanitized\n\n    # Not sure that we need these slice methods, but will provide anyway\n    def __getslice__( self, i, j ):\n        return self.__safe_string_wrapper_function__( self.unsanitized[ i:j ] )\n\n    def __setslice__( self, i, j, value ):\n        self.unsanitized[ i:j ] = value\n\n    def __delslice__( self, i, j ):\n        del self.unsanitized[ i:j ]\n\n    def __add__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized + other )\n\n    def __sub__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized - other )\n\n    def __mul__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized * other )\n\n    def __floordiv__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized // other )\n\n    def __mod__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized % other )\n\n    def __divmod__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( divmod( self.unsanitized, other ) )\n\n    def __pow__( self, *other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( pow( self.unsanitized, *other ) )\n\n    def __lshift__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized << other )\n\n    def __rshift__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized >> other )\n\n    def __and__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized & other )\n\n    def __xor__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized ^ other )\n\n    def __or__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized | other )\n\n    def __div__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized / other )\n\n    def __truediv__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( self.unsanitized / other )\n\n    # The only reflected operand that we will define is __rpow__, due to coercion rules complications as per docs\n    def __rpow__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return self.__safe_string_wrapper_function__( pow( other, self.unsanitized ) )\n\n    # Do not implement in-place operands\n\n    def __neg__( self ):\n        return __safe_string_wrapper_function__( -self.unsanitized )\n\n    def __pos__( self ):\n        return __safe_string_wrapper_function__( +self.unsanitized )\n\n    def __abs__( self ):\n        return __safe_string_wrapper_function__( abs( self.unsanitized ) )\n\n    def __invert__( self ):\n        return __safe_string_wrapper_function__( ~self.unsanitized )\n\n    def __complex__( self ):\n        return __safe_string_wrapper_function__( complex( self.unsanitized ) )\n\n    def __int__( self ):\n        return int( self.unsanitized )\n\n    def __float__( self ):\n        return float( self.unsanitized )\n\n    def __oct__( self ):\n        return oct( self.unsanitized )\n\n    def __hex__( self ):\n        return hex( self.unsanitized )\n\n    def __index__( self ):\n        return self.unsanitized.index()\n\n    def __coerce__( self, other ):\n        while isinstance( other, SafeStringWrapper ):\n            other = other.unsanitized\n        return coerce( self.unsanitized, other )\n\n    def __enter__( self ):\n        return self.unsanitized.__enter__()\n\n    def __exit__( self, *args ):\n        return self.unsanitized.__exit__( *args )\n\nclass CallableSafeStringWrapper( SafeStringWrapper ):\n    \n    def __call__( self, *args, **kwds ):\n        return self.__safe_string_wrapper_function__( self.unsanitized( *args, **kwds ) )\n\n\n# Enable pickling/deepcopy\ndef pickle_SafeStringWrapper( safe_object ):\n    args = ( safe_object.unsanitized, )\n    cls = SafeStringWrapper\n    if isinstance( safe_object, CallableSafeStringWrapper ):\n        cls = CallableSafeStringWrapper\n    return ( cls, args )\ncopy_reg.pickle( SafeStringWrapper, pickle_SafeStringWrapper, wrap_with_safe_string )\ncopy_reg.pickle( CallableSafeStringWrapper, pickle_SafeStringWrapper, wrap_with_safe_string )\n\n", "code_before": "", "patch": "@@ -0,0 +1,436 @@\n+\"\"\"\n+Classes for wrapping Objects and Sanitizing string output.\n+\"\"\"\n+\n+import inspect\n+import copy_reg\n+import logging\n+import string\n+from numbers import Number\n+from types import ( NoneType, NotImplementedType, EllipsisType, FunctionType, MethodType, GeneratorType, CodeType,\n+                    BuiltinFunctionType, BuiltinMethodType, ModuleType, XRangeType, SliceType, TracebackType, FrameType,\n+                    BufferType, DictProxyType, GetSetDescriptorType, MemberDescriptorType )\n+from UserDict import UserDict\n+\n+from galaxy.util import sanitize_lists_to_string as _sanitize_lists_to_string\n+\n+log = logging.getLogger( __name__ )\n+\n+# Define different behaviors for different types, see also: https://docs.python.org/2/library/types.html\n+\n+# Known Callable types\n+__CALLABLE_TYPES__ = ( FunctionType, MethodType, GeneratorType, CodeType, BuiltinFunctionType, BuiltinMethodType, )\n+\n+# Always wrap these types without attempting to subclass\n+__WRAP_NO_SUBCLASS__ =  ( ModuleType, XRangeType, SliceType, BufferType, TracebackType, FrameType, DictProxyType,\n+                          GetSetDescriptorType, MemberDescriptorType ) + __CALLABLE_TYPES__\n+\n+# Don't wrap or sanitize.\n+__DONT_SANITIZE_TYPES__ = ( Number, bool, NoneType, NotImplementedType, EllipsisType, bytearray, )\n+\n+# Don't wrap, but do sanitize.\n+__DONT_WRAP_TYPES__ = tuple() #( basestring, ) so that we can get the unsanitized string, we will now wrap basestring instances\n+\n+# Wrap contents, but not the container\n+__WRAP_SEQUENCES__ = ( tuple, list, )\n+__WRAP_SETS__ = ( set, frozenset, )\n+__WRAP_MAPPINGS__ = ( dict, UserDict, )\n+\n+\n+# Define the set of characters that are not sanitized, and define a set of mappings for those that are.\n+# characters that are valid\n+VALID_CHARACTERS = set( string.letters + string.digits + \" -=_.()/+*^,:?!@\" )\n+\n+# characters that are allowed but need to be escaped\n+CHARACTER_MAP = { '>': '__gt__',\n+                 '<': '__lt__',\n+                 \"'\": '__sq__',\n+                 '\"': '__dq__',\n+                 '[': '__ob__',\n+                 ']': '__cb__',\n+                 '{': '__oc__',\n+                 '}': '__cc__',\n+                 '\\n': '__cn__',\n+                 '\\r': '__cr__',\n+                 '\\t': '__tc__',\n+                 '#': '__pd__'}\n+\n+INVALID_CHARACTER = \"X\"\n+\n+def sanitize_lists_to_string( values, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP, invalid_character=INVALID_CHARACTER  ):\n+    return _sanitize_lists_to_string( values, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character  )\n+\n+\n+def wrap_with_safe_string( value, no_wrap_classes = None ):\n+    \"\"\"\n+    Recursively wrap values that should be wrapped.\n+    \"\"\"\n+\n+    def __do_wrap( value ):\n+        if isinstance( value, SafeStringWrapper ):\n+            # Only ever wrap one-layer\n+            return value\n+        if callable( value ):\n+            safe_class = CallableSafeStringWrapper\n+        else:\n+            safe_class = SafeStringWrapper\n+        if isinstance( value, no_wrap_classes ):\n+            return value\n+        if isinstance( value, __DONT_WRAP_TYPES__ ):\n+            return sanitize_lists_to_string( value, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP )\n+        if isinstance( value, __WRAP_NO_SUBCLASS__ ):\n+            return safe_class( value, safe_string_wrapper_function = __do_wrap )\n+        for this_type in __WRAP_SEQUENCES__ + __WRAP_SETS__:\n+            if isinstance( value, this_type ):\n+                return this_type( map( __do_wrap, value ) )\n+        for this_type in __WRAP_MAPPINGS__:\n+            if isinstance( value, this_type ):\n+                # Wrap both key and value\n+                return this_type( map( lambda x: ( __do_wrap( x[0] ), __do_wrap( x[1] ) ), value.items() ) )\n+        # Create a dynamic class that joins SafeStringWrapper with the object being wrapped.\n+        # This allows e.g. isinstance to continue to work.\n+        try:\n+            wrapped_class_name = value.__name__\n+            wrapped_class = value\n+        except:\n+            wrapped_class_name = value.__class__.__name__\n+            wrapped_class = value.__class__\n+        value_mod = inspect.getmodule( value )\n+        if value_mod:\n+            wrapped_class_name = \"%s.%s\" % ( value_mod.__name__, wrapped_class_name )\n+        wrapped_class_name = \"SafeStringWrapper(%s:%s)\" % ( wrapped_class_name, \",\".join( sorted( map( str, no_wrap_classes ) ) ) )\n+        do_wrap_func_name = \"__do_wrap_%s\" % ( wrapped_class_name )\n+        do_wrap_func = __do_wrap\n+        global_dict = globals()\n+        if wrapped_class_name in global_dict:\n+            # Check to see if we have created a wrapper for this class yet, if so, reuse\n+            wrapped_class = global_dict.get( wrapped_class_name )\n+            do_wrap_func = global_dict.get( do_wrap_func_name, __do_wrap )\n+        else:\n+            try:\n+                wrapped_class = type( wrapped_class_name, ( safe_class, wrapped_class, ), {} )\n+            except TypeError, e:\n+                 # Fail-safe for when a class cannot be dynamically subclassed.\n+                 log.warning( \"Unable to create dynamic subclass for %s, %s: %s\", type( value), value, e )\n+                 wrapped_class = type( wrapped_class_name, ( safe_class, ), {} )\n+            if wrapped_class not in ( SafeStringWrapper, CallableSafeStringWrapper ):\n+                # Save this wrapper for reuse and pickling/copying\n+                global_dict[ wrapped_class_name ] = wrapped_class\n+                do_wrap_func.__name__ = do_wrap_func_name\n+                global_dict[ do_wrap_func_name ] = do_wrap_func\n+                def pickle_safe_object( safe_object ):\n+                    return ( wrapped_class, ( safe_object.unsanitized, do_wrap_func, ) )\n+                # Set pickle and copy properties\n+                copy_reg.pickle( wrapped_class, pickle_safe_object, do_wrap_func )\n+        return wrapped_class( value, safe_string_wrapper_function = do_wrap_func )\n+    # Determine classes not to wrap\n+    if no_wrap_classes:\n+        if not isinstance( no_wrap_classes, ( tuple, list ) ):\n+            no_wrap_classes = [ no_wrap_classes ]\n+        no_wrap_classes = list( no_wrap_classes ) + list( __DONT_SANITIZE_TYPES__ ) + [ SafeStringWrapper ]\n+    else:\n+        no_wrap_classes = list( __DONT_SANITIZE_TYPES__ ) + [ SafeStringWrapper ]\n+    no_wrap_classes = tuple( set( sorted( no_wrap_classes, key=str ) ) )\n+    return __do_wrap( value )\n+\n+\n+# N.B. refer to e.g. https://docs.python.org/2/reference/datamodel.html for information on Python's Data Model.\n+\n+\n+class SafeStringWrapper( object ):\n+    \"\"\"\n+    Class that wraps and sanitizes any provided value's attributes \n+    that will attempt to be cast into a string.\n+    \n+    Attempts to mimic behavior of original class, including operands.\n+    \n+    To ensure proper handling of e.g. subclass checks, the *wrap_with_safe_string()*\n+    method should be used.\n+    \n+    This wrapping occurs in a recursive/parasitic fashion, as all called attributes of \n+    the originally wrapped object will also be wrapped and sanitized, unless the attribute\n+    is of a type found in __DONT_SANITIZE_TYPES__ + __DONT_WRAP_TYPES__, where e.g. ~(strings\n+    will still be sanitized, but not wrapped), and e.g. integers will have neither.\n+    \"\"\"\n+    __UNSANITIZED_ATTRIBUTE_NAME__ = 'unsanitized' \n+    __NO_WRAP_NAMES__ = [ '__safe_string_wrapper_function__', __UNSANITIZED_ATTRIBUTE_NAME__]\n+\n+\n+    def __new__( cls, *arg, **kwd ):\n+        # We need to define a __new__ since, we are subclassing from e.g. immutable str, which internally sets data \n+        # that will be used when other + this (this + other is handled by __add__)\n+        safe_string_wrapper_function = kwd.get( 'safe_string_wrapper_function', None) or wrap_with_safe_string\n+        try:\n+            return super( SafeStringWrapper, cls ).__new__( cls, sanitize_lists_to_string( arg[0], valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP ) )\n+        except Exception, e:\n+            log.warning( \"Could not provide an argument to %s.__new__: %s; will try without arguments.\", cls, e )\n+            return super( SafeStringWrapper, cls ).__new__( cls )\n+\n+    def __init__( self, value, safe_string_wrapper_function = wrap_with_safe_string ):\n+        self.unsanitized = value\n+        self.__safe_string_wrapper_function__ = safe_string_wrapper_function\n+\n+    def __str__( self ):\n+        return sanitize_lists_to_string( self.unsanitized, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP )\n+\n+    def __repr__( self ):\n+        return \"%s object at %x on: %s\" % ( sanitize_lists_to_string( self.__class__.__name__, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP ), id( self ), sanitize_lists_to_string( repr( self.unsanitized ), valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP ) )\n+\n+    def __lt__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.unsanitized < other\n+\n+    def __le__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.unsanitized <= other\n+\n+    def __eq__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.unsanitized == other\n+\n+    def __ne__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.unsanitized != other\n+\n+    def __gt__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.unsanitized > other\n+\n+    def __ge__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.unsanitized >= other\n+\n+    def __lt__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.unsanitized < other\n+\n+    def __cmp__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return cmp( self.unsanitized, other )\n+\n+    # Do not implement __rcmp__, python 2.2 < 2.6\n+\n+    def __hash__( self ):\n+        return hash( self.unsanitized )\n+\n+    def __nonzero__( self ):\n+        return bool( self.unsanitized )\n+\n+    # Do not implement __unicode__, we will rely on __str__\n+\n+    def __getattr__( self, name ):\n+        if name in SafeStringWrapper.__NO_WRAP_NAMES__:\n+            #FIXME: is this ever reached?\n+            return object.__getattr__( self, name )\n+        return self.__safe_string_wrapper_function__( getattr( self.unsanitized, name ) )\n+\n+    def __setattr__( self, name, value ):\n+        if name in SafeStringWrapper.__NO_WRAP_NAMES__:\n+            return object.__setattr__( self, name, value )\n+        return setattr( self.unsanitized, name, value )\n+\n+    def __delattr__( self, name ):\n+        if name in SafeStringWrapper.__NO_WRAP_NAMES__:\n+            return object.__delattr__( self, name )\n+        return delattr( self.unsanitized, name )\n+\n+    def __getattribute__( self, name ):\n+        if name in SafeStringWrapper.__NO_WRAP_NAMES__:\n+            return object.__getattribute__( self, name )\n+        return self.__safe_string_wrapper_function__( getattr( object.__getattribute__( self, 'unsanitized' ), name ) )\n+\n+    # Skip Descriptors\n+\n+    # Skip __slots__\n+    \n+    # Don't need __metaclass__, we'll use the helper function to handle with subclassing for e.g. isinstance()\n+\n+    # Revisit:\n+    # __instancecheck__\n+    # __subclasscheck__\n+    # We are using a helper class to create dynamic subclasses to handle class checks\n+\n+    # We address __call__ as needed based upon unsanitized, through the use of a CallableSafeStringWrapper class\n+\n+    def __len__( self ):\n+        original_value = self.unsanitized\n+        while isinstance( original_value, SafeStringWrapper ):\n+            original_value = self.unsanitized\n+        return len( self.unsanitized )\n+\n+    def __getitem__( self, key ):\n+        return self.__safe_string_wrapper_function__( self.unsanitized[ key ] )\n+\n+    def __setitem__( self, key, value ):\n+        while isinstance( value, SafeStringWrapper ):\n+            value = value.unsanitized\n+        self.unsanitized[ key ] = value\n+\n+    def __delitem__( self, key ):\n+        del self.unsanitized[ key ]\n+\n+    def __iter__( self ):\n+        return iter( map( self.__safe_string_wrapper_function__, iter( self.unsanitized ) ) )\n+\n+    # Do not implement __reversed__\n+\n+    def __contains__( self, item ):\n+        # FIXME: Do we need to consider if item is/isn't or does/doesn't contain SafeStringWrapper?\n+        # When considering e.g. nested lists/dicts/etc, this gets complicated\n+        while isinstance( item, SafeStringWrapper ):\n+            item = item.unsanitized\n+        return item in self.unsanitized\n+\n+    # Not sure that we need these slice methods, but will provide anyway\n+    def __getslice__( self, i, j ):\n+        return self.__safe_string_wrapper_function__( self.unsanitized[ i:j ] )\n+\n+    def __setslice__( self, i, j, value ):\n+        self.unsanitized[ i:j ] = value\n+\n+    def __delslice__( self, i, j ):\n+        del self.unsanitized[ i:j ]\n+\n+    def __add__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized + other )\n+\n+    def __sub__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized - other )\n+\n+    def __mul__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized * other )\n+\n+    def __floordiv__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized // other )\n+\n+    def __mod__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized % other )\n+\n+    def __divmod__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( divmod( self.unsanitized, other ) )\n+\n+    def __pow__( self, *other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( pow( self.unsanitized, *other ) )\n+\n+    def __lshift__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized << other )\n+\n+    def __rshift__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized >> other )\n+\n+    def __and__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized & other )\n+\n+    def __xor__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized ^ other )\n+\n+    def __or__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized | other )\n+\n+    def __div__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized / other )\n+\n+    def __truediv__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( self.unsanitized / other )\n+\n+    # The only reflected operand that we will define is __rpow__, due to coercion rules complications as per docs\n+    def __rpow__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return self.__safe_string_wrapper_function__( pow( other, self.unsanitized ) )\n+\n+    # Do not implement in-place operands\n+\n+    def __neg__( self ):\n+        return __safe_string_wrapper_function__( -self.unsanitized )\n+\n+    def __pos__( self ):\n+        return __safe_string_wrapper_function__( +self.unsanitized )\n+\n+    def __abs__( self ):\n+        return __safe_string_wrapper_function__( abs( self.unsanitized ) )\n+\n+    def __invert__( self ):\n+        return __safe_string_wrapper_function__( ~self.unsanitized )\n+\n+    def __complex__( self ):\n+        return __safe_string_wrapper_function__( complex( self.unsanitized ) )\n+\n+    def __int__( self ):\n+        return int( self.unsanitized )\n+\n+    def __float__( self ):\n+        return float( self.unsanitized )\n+\n+    def __oct__( self ):\n+        return oct( self.unsanitized )\n+\n+    def __hex__( self ):\n+        return hex( self.unsanitized )\n+\n+    def __index__( self ):\n+        return self.unsanitized.index()\n+\n+    def __coerce__( self, other ):\n+        while isinstance( other, SafeStringWrapper ):\n+            other = other.unsanitized\n+        return coerce( self.unsanitized, other )\n+\n+    def __enter__( self ):\n+        return self.unsanitized.__enter__()\n+\n+    def __exit__( self, *args ):\n+        return self.unsanitized.__exit__( *args )\n+\n+class CallableSafeStringWrapper( SafeStringWrapper ):\n+    \n+    def __call__( self, *args, **kwds ):\n+        return self.__safe_string_wrapper_function__( self.unsanitized( *args, **kwds ) )\n+\n+\n+# Enable pickling/deepcopy\n+def pickle_SafeStringWrapper( safe_object ):\n+    args = ( safe_object.unsanitized, )\n+    cls = SafeStringWrapper\n+    if isinstance( safe_object, CallableSafeStringWrapper ):\n+        cls = CallableSafeStringWrapper\n+    return ( cls, args )\n+copy_reg.pickle( SafeStringWrapper, pickle_SafeStringWrapper, wrap_with_safe_string )\n+copy_reg.pickle( CallableSafeStringWrapper, pickle_SafeStringWrapper, wrap_with_safe_string )\n+", "file_path": "files/2023_1/994", "file_language": "py", "file_name": "lib/galaxy/util/object_wrapper.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
