{"index": 11993, "cve_id": "CVE-2023-35934", "cwe_id": ["CWE-200", "CWE-601"], "cve_language": "Python", "cve_description": "yt-dlp is a command-line program to download videos from video sites. During file downloads, yt-dlp or the external downloaders that yt-dlp employs may leak cookies on HTTP redirects to a different host, or leak them when the host for download fragments differs from their parent manifest's host. This vulnerable behavior is present in yt-dlp prior to 2023.07.06 and nightly 2023.07.06.185519. All native and external downloaders are affected, except for `curl` and `httpie` (version 3.1.0 or later).\n\nAt the file download stage, all cookies are passed by yt-dlp to the file downloader as a `Cookie` header, thereby losing their scope. This also occurs in yt-dlp's info JSON output, which may be used by external tools. As a result, the downloader or external tool may indiscriminately send cookies with requests to domains or paths for which the cookies are not scoped.\n\nyt-dlp version 2023.07.06 and nightly 2023.07.06.185519 fix this issue by removing the `Cookie` header upon HTTP redirects; having native downloaders calculate the `Cookie` header from the cookiejar, utilizing external downloaders' built-in support for cookies instead of passing them as header arguments, disabling HTTP redirectiong if the external downloader does not have proper cookie support, processing cookies passed as HTTP headers to limit their scope, and having a separate field for cookies in the info dict storing more information about scoping\n\nSome workarounds are available for those who are unable to upgrade. Avoid using cookies and user authentication methods. While extractors may set custom cookies, these usually do not contain sensitive information. Alternatively, avoid using `--load-info-json`. Or, if authentication is a must: verify the integrity of download links from unknown sources in browser (including redirects) before passing them to yt-dlp; use `curl` as external downloader, since it is not impacted; and/or avoid fragmented formats such as HLS/m3u8, DASH/mpd and ISM.", "cvss": "8.2", "publish_date": "July 6, 2023", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "REQUIRED", "S": "CHANGED", "C": "HIGH", "I": "LOW", "A": "NONE", "commit_id": "f8b4bcc0a791274223723488bfbfc23ea3276641", "commit_message": "[core] Prevent `Cookie` leaks on HTTP redirect\n\nRef: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-v8mc-9377-rwjj\n\nAuthored by: coletdjnz", "commit_date": "2023-07-06T17:44:39Z", "project": "yt-dlp/yt-dlp", "url": "https://api.github.com/repos/yt-dlp/yt-dlp/commits/f8b4bcc0a791274223723488bfbfc23ea3276641", "html_url": "https://github.com/yt-dlp/yt-dlp/commit/f8b4bcc0a791274223723488bfbfc23ea3276641", "windows_before": [{"commit_id": "3121512228487c9c690d3d39bfd2579addf96e07", "commit_date": "Thu Jul 6 21:51:04 2023 +0530", "commit_message": "[core] Change how `Cookie` headers are handled", "files_name": ["test/test_YoutubeDL.py", "yt_dlp/YoutubeDL.py", "yt_dlp/downloader/common.py"]}, {"commit_id": "1ceb657bdd254ad961489e5060f2ccc7d556b729", "commit_date": "Wed Jul 5 15:16:28 2023 -0500", "commit_message": "[fd/external] Scope cookies", "files_name": ["test/test_downloader_external.py", "yt_dlp/cookies.py", "yt_dlp/downloader/external.py"]}, {"commit_id": "ad8902f616ad2541f9b9626738f1393fad89a64c", "commit_date": "Thu Jul 6 19:35:49 2023 +0530", "commit_message": "[ie/vidlii] Handle relative URLs", "files_name": ["yt_dlp/extractor/vidlii.py"]}, {"commit_id": "94ed638a437fc766699d440e978982e24ce6a30a", "commit_date": "Fri Jun 23 18:16:07 2023 +0530", "commit_message": "[ie/youtube] Avoid false DRM detection (#7396)", "files_name": ["yt_dlp/extractor/youtube.py"]}, {"commit_id": "bc344cd456380999c1ee74554dfd432a38f32ec7", "commit_date": "Thu Jul 6 18:39:50 2023 +0530", "commit_message": "[core] Allow extractors to mark formats as potentially DRM (#7396)", "files_name": ["yt_dlp/YoutubeDL.py", "yt_dlp/downloader/hls.py", "yt_dlp/extractor/common.py"]}, {"commit_id": "906c0bdcd8974340d619e99ccd613c163eb0d0c2", "commit_date": "Thu Jul 6 18:17:42 2023 +0530", "commit_message": "[formats] Fix best fallback for storyboards", "files_name": ["yt_dlp/YoutubeDL.py"]}, {"commit_id": "337734d4a8a6500bc65434843db346b5cbd05e81", "commit_date": "Thu Jul 6 20:09:42 2023 +0530", "commit_message": "[cleanup] Misc", "files_name": ["devscripts/make_changelog.py", "setup.cfg", "yt_dlp/YoutubeDL.py", "yt_dlp/downloader/common.py", "yt_dlp/downloader/fragment.py", "yt_dlp/extractor/adobepass.py", "yt_dlp/extractor/iqiyi.py", "yt_dlp/extractor/vshare.py", "yt_dlp/extractor/youtube.py", "yt_dlp/utils/__init__.py"]}, {"commit_id": "fa44802809d189fca0f4782263d48d6533384503", "commit_date": "Thu Jul 6 17:34:51 2023 +0530", "commit_message": "[devscripts/make_changelog] Skip reverted commits", "files_name": ["devscripts/make_changelog.py"]}, {"commit_id": "47bcd437247152e0af5b3ebc5592db7bb66855c2", "commit_date": "Thu Jul 6 18:08:44 2023 +0530", "commit_message": "[outtmpl] Pad `playlist_index` etc even when with internal formatting", "files_name": ["test/test_YoutubeDL.py", "yt_dlp/YoutubeDL.py"]}, {"commit_id": "662ef1e910b72e57957f06589925b2332ba52821", "commit_date": "Tue Jul 4 18:46:32 2023 +0530", "commit_message": "[downloader/http] Avoid infinite loop when no data is received", "files_name": ["yt_dlp/downloader/http.py"]}, {"commit_id": "6355b5f1e1e8e7f4ef866d71d51e03baf0e82f17", "commit_date": "Thu Jul 6 16:51:46 2023 +0200", "commit_message": "[misc] Add CodeQL workflow (#7497)", "files_name": [".github/workflows/codeql.yml"]}, {"commit_id": "90db9a3c00ca80492c6a58c542e4cbf4c2710866", "commit_date": "Fri Jul 7 01:32:41 2023 +1200", "commit_message": "[extractor/youtube:stories] Remove (#7459)", "files_name": ["yt_dlp/extractor/_extractors.py", "yt_dlp/extractor/youtube.py"]}, {"commit_id": "49296437a8e5fa91dacb5446e51ab588474c85d3", "commit_date": "Wed Jul 5 11:27:36 2023 -0500", "commit_message": "[extractor/twitter] Fix unauthenticated extraction (#7476)", "files_name": ["README.md", "yt_dlp/extractor/twitter.py"]}, {"commit_id": "1cffd621cb371f1563563cfb2fe37d137e8a7bee", "commit_date": "Tue Jul 4 22:05:52 2023 -0500", "commit_message": "[extractor/twitter:spaces] Fix extraction (#7512)", "files_name": ["yt_dlp/extractor/twitter.py"]}, {"commit_id": "3b7f5300c577fef40464d46d4e4037a69d51fe82", "commit_date": "Wed Jul 5 09:17:13 2023 +0700", "commit_message": "[extractor/googledrive] Fix source format extraction (#7395)", "files_name": ["yt_dlp/extractor/googledrive.py"]}, {"commit_id": "4dc4d8473c085900edc841c87c20041233d25b1f", "commit_date": "Mon Jul 3 10:47:10 2023 +0000", "commit_message": "[extractor/youtube] Ignore incomplete data for comment threads by default (#7475)", "files_name": ["yt_dlp/extractor/youtube.py"]}, {"commit_id": "8776349ef6b1f644584a92dfa00a05208a48edc4", "commit_date": "Sun Jul 2 15:31:00 2023 -0400", "commit_message": "[extractor/vk] VKPlay, VKPlayLive: Add extractors (#7358)", "files_name": ["yt_dlp/extractor/_extractors.py", "yt_dlp/extractor/vk.py"]}, {"commit_id": "af1fd12f675220df6793fc019dff320bc76e8080", "commit_date": "Sat Jul 1 03:27:07 2023 +0900", "commit_message": "[extractor/stacommu] Add extractors (#7432)", "files_name": ["README.md", "yt_dlp/extractor/_extractors.py", "yt_dlp/extractor/stacommu.py", "yt_dlp/extractor/wrestleuniverse.py"]}, {"commit_id": "fcbc9ed760be6e3455bbadfaf277b4504b06f068", "commit_date": "Thu Jun 29 23:26:27 2023 +0000", "commit_message": "[extractor/youtube:tab] Support shorts-only playlists (#7425)", "files_name": ["yt_dlp/extractor/youtube.py"]}, {"commit_id": "a2be9781fbf4d7e4db245c277ca2ecc41cf3a7b2", "commit_date": "Tue Jun 27 16:50:02 2023 -0500", "commit_message": "[extractor/Douyin] Fix extraction from webpage", "files_name": ["yt_dlp/extractor/tiktok.py"]}, {"commit_id": "8f05fbae2a79ce0713077ccc68b354e63216bf20", "commit_date": "Tue Jun 27 16:16:57 2023 -0500", "commit_message": "[extractor/abc] Fix extraction (#7434)", "files_name": ["yt_dlp/extractor/abc.py"]}, {"commit_id": "5b4b92769afcc398475e481bfa839f1158902fe9", "commit_date": "Wed Jun 28 01:58:23 2023 +0530", "commit_message": "[extractor/crunchyroll:music] Fix `_VALID_URL` (#7439)", "files_name": ["yt_dlp/extractor/crunchyroll.py"]}, {"commit_id": "91302ed349f34dc26cc1d661bb45a4b71f4417f7", "commit_date": "Mon Jun 26 16:19:49 2023 +0530", "commit_message": "[utils] clean_podcast_url: Handle protocol in redirect URL", "files_name": ["yt_dlp/utils/_utils.py"]}, {"commit_id": "f393bbe724b1fc6c7f754a5da507e807b2b40ad2", "commit_date": "Mon Jun 26 16:14:20 2023 +0530", "commit_message": "[extractor/sbs] Python 3.7 compat", "files_name": ["yt_dlp/extractor/sbs.py"]}, {"commit_id": "8a8af356e3bba98a7f7d333aff0777d5d92130c8", "commit_date": "Mon Jun 26 16:13:31 2023 +0530", "commit_message": "[downloader/aria2c] Add `--no-conf`", "files_name": ["yt_dlp/downloader/external.py"]}, {"commit_id": "d949c10c45bfc359bdacd52e6a180169b8128958", "commit_date": "Mon Jun 26 07:25:47 2023 +0530", "commit_message": "[extractor/youtube] Process `post_live` over 2 hours", "files_name": ["yt_dlp/extractor/youtube.py"]}, {"commit_id": "ef8509c300ea50da86aea447eb214d3d6f6db6bb", "commit_date": "Sun Jun 25 17:04:42 2023 -0500", "commit_message": "[extractor/kick] Fix `_VALID_URL`", "files_name": ["yt_dlp/extractor/kick.py"]}, {"commit_id": "5e16cf92eb496b7c1541a6b1d727cb87542984db", "commit_date": "Sun Jun 25 16:22:38 2023 -0400", "commit_message": "[extractor/AdultSwim] Extract subtitles from m3u8 (#7421)", "files_name": ["yt_dlp/extractor/adultswim.py"]}, {"commit_id": "f0a1ff118145b6449982ba401f9a9f656ecd8062", "commit_date": "Sun Jun 25 13:13:28 2023 -0500", "commit_message": "[extractor/qdance] Add extractor (#7420)", "files_name": ["yt_dlp/extractor/_extractors.py", "yt_dlp/extractor/qdance.py"]}, {"commit_id": "58786a10f212bd63f9ad1d0b4d9e4d31c3b385e2", "commit_date": "Sun Jun 25 20:10:00 2023 +0530", "commit_message": "[extractor/youtube] Add extractor-arg `formats`", "files_name": ["README.md", "yt_dlp/extractor/youtube.py"]}, {"commit_id": "e59e20744eb32ce4b6ea0dece7c673be8376a710", "commit_date": "Thu Jun 22 23:22:14 2023 +0530", "commit_message": "Bugfix for b4e0d75848e9447cee2cd3646ce54d4744a7ff56", "files_name": ["yt_dlp/utils/_utils.py"]}, {"commit_id": "89bed013741a776506f60380b7fd89d27d0710b4", "commit_date": "Fri Jun 23 01:08:42 2023 +0700", "commit_message": "[extractor/youtube] Fix comments' `is_favorited` (#7390)", "files_name": ["yt_dlp/extractor/youtube.py"]}, {"commit_id": "de4cf77ec1a13f020e6afe4ed04248c6b19fccb6", "commit_date": "Thu Jun 22 08:09:31 2023 +0000", "commit_message": "Release 2023.06.22", "files_name": [".github/ISSUE_TEMPLATE/1_broken_site.yml", ".github/ISSUE_TEMPLATE/2_site_support_request.yml", ".github/ISSUE_TEMPLATE/3_site_feature_request.yml", ".github/ISSUE_TEMPLATE/4_bug_report.yml", ".github/ISSUE_TEMPLATE/5_feature_request.yml", ".github/ISSUE_TEMPLATE/6_question.yml", "CONTRIBUTORS", "Changelog.md", "supportedsites.md", "yt_dlp/version.py"]}, {"commit_id": "812cdfa06c33a40e73a8e04b3e6f42c084666a43", "commit_date": "Thu Jun 22 10:02:38 2023 +0530", "commit_message": "[cleanup] Misc", "files_name": ["README.md", "devscripts/changelog_override.json", "devscripts/make_changelog.py", "yt_dlp/extractor/testurl.py", "yt_dlp/utils/_utils.py"]}, {"commit_id": "cd810afe2ac5567c822b7424800fc470ef2d0045", "commit_date": "Thu Jun 22 13:23:31 2023 +0530", "commit_message": "[extractor/youtube] Improve nsig function name extraction", "files_name": ["test/test_youtube_signature.py", "yt_dlp/extractor/youtube.py"]}, {"commit_id": "b4e0d75848e9447cee2cd3646ce54d4744a7ff56", "commit_date": "Thu Jun 22 04:54:39 2023 +0530", "commit_message": "Improve `--download-sections`", "files_name": ["README.md", "yt_dlp/YoutubeDL.py", "yt_dlp/__init__.py", "yt_dlp/options.py", "yt_dlp/utils/_utils.py"]}, {"commit_id": "71dc18fa29263a1ff0472c23d81bfc8dd4422d48", "commit_date": "Thu Jun 22 10:27:54 2023 +0300", "commit_message": "[extractor/youtube] Improve description parsing performance (#7315)", "files_name": ["yt_dlp/extractor/youtube.py"]}, {"commit_id": "98cb1eda7a4cf67c96078980dbd63e6c06ad7f7c", "commit_date": "Thu Jun 22 00:24:52 2023 -0500", "commit_message": "[extractor/rheinmaintv] Add extractor (#7311)", "files_name": ["yt_dlp/extractor/_extractors.py", "yt_dlp/extractor/rheinmaintv.py"]}, {"commit_id": "774aa09dd6aa61ced9ec818d1f67e53414d22762", "commit_date": "Thu Jun 22 00:16:39 2023 -0500", "commit_message": "[extractor/dplay] GlobalCyclingNetworkPlus: Add extractor (#7360)", "files_name": ["yt_dlp/extractor/_extractors.py", "yt_dlp/extractor/dplay.py"]}, {"commit_id": "f2ff0f6f1914b82d4a51681a72cc0828115dcb4a", "commit_date": "Wed Jun 21 20:00:54 2023 -0400", "commit_message": "[extractor/motherless] Add gallery support, fix groups (#7211)", "files_name": ["yt_dlp/extractor/_extractors.py", "yt_dlp/extractor/motherless.py"]}, {"commit_id": "5fd8367496b42c7b900b896a0d5460561a2859de", "commit_date": "Thu Jun 22 02:57:00 2023 +0530", "commit_message": "[extractor] Support multiple `_VALID_URL`s (#5812)", "files_name": ["devscripts/lazy_load_template.py", "yt_dlp/extractor/common.py"]}, {"commit_id": "0dff8e4d1e6e9fb938f4256ea9af7d81f42fd54f", "commit_date": "Thu Jun 22 01:37:55 2023 +0530", "commit_message": "Indicate `filesize` approximated from `tbr` better", "files_name": ["yt_dlp/YoutubeDL.py"]}, {"commit_id": "1e75d97db21152acc764b30a688e516f04b8a142", "commit_date": "Thu Jun 22 00:20:04 2023 +0530", "commit_message": "[extractor/youtube] Add `ios` to default clients used", "files_name": ["README.md", "yt_dlp/extractor/youtube.py"]}, {"commit_id": "81ca451480051d7ce1a31c017e005358345a9149", "commit_date": "Thu Jun 22 00:15:22 2023 +0530", "commit_message": "[extractor/youtube] Workaround 403 for android formats", "files_name": ["yt_dlp/extractor/youtube.py"]}, {"commit_id": "a4486bfc1dc7057efca9dd3fe70d7fa25c56f700", "commit_date": "Wed Jun 21 12:35:14 2023 +0530", "commit_message": "Revert \"[misc] Add automatic duplicate issue detection\"", "files_name": [".github/workflows/potential-duplicates.yml"]}, {"commit_id": "3f756c8c4095b942cf49788eb0862ceaf57847f2", "commit_date": "Wed Jun 21 10:29:34 2023 +0200", "commit_message": "[extractor/nebula] Fix extractor (#7156)", "files_name": ["yt_dlp/extractor/nebula.py"]}, {"commit_id": "7f9c6a63b16e145495479e9f666f5b9e2ee69e2f", "commit_date": "Wed Jun 21 03:24:24 2023 -0500", "commit_message": "[cleanup] Misc", "files_name": ["README.md", "yt_dlp/extractor/twitch.py"]}, {"commit_id": "db22142f6f817ff673d417b4b78e8db497bf8ab3", "commit_date": "Wed Jun 21 03:17:07 2023 -0400", "commit_message": "[extractor/dropout] Fix season extraction (#7304)", "files_name": ["yt_dlp/extractor/dropout.py"]}, {"commit_id": "d7cd97e8d8d42b500fea9abb2aa4ac9b0f98b2ad", "commit_date": "Wed Jun 21 12:12:15 2023 +0530", "commit_message": "Fix bug in db3ad8a67661d7b234a6954d9c6a4a9b1749f5eb", "files_name": ["README.md", "yt_dlp/extractor/common.py"]}], "windows_after": [{"commit_id": "3121512228487c9c690d3d39bfd2579addf96e07", "commit_date": "Thu Jul 6 21:51:04 2023 +0530", "commit_message": "[core] Change how `Cookie` headers are handled", "files_name": ["test/test_YoutubeDL.py", "yt_dlp/YoutubeDL.py", "yt_dlp/downloader/common.py"]}, {"commit_id": "b532a3481046e1eabb6232ee8196fb696c356ff6", "commit_date": "Thu Jul 6 19:18:35 2023 +0530", "commit_message": "[docs] Minor fixes", "files_name": ["Changelog.md", "README.md", "devscripts/changelog_override.json"]}, {"commit_id": "cc0619f62d6da52689797483e96b29290b0c0873", "commit_date": "Thu Jul 6 18:57:59 2023 +0000", "commit_message": "Release 2023.07.06", "files_name": [".github/ISSUE_TEMPLATE/1_broken_site.yml", ".github/ISSUE_TEMPLATE/2_site_support_request.yml", ".github/ISSUE_TEMPLATE/3_site_feature_request.yml", ".github/ISSUE_TEMPLATE/4_bug_report.yml", ".github/ISSUE_TEMPLATE/5_feature_request.yml", ".github/ISSUE_TEMPLATE/6_question.yml", "CONTRIBUTORS", "Changelog.md", "supportedsites.md", "yt_dlp/version.py"]}, {"commit_id": "b03fa7834579a01cc5fba48c0e73488a16683d48", "commit_date": "Thu Jul 6 02:00:23 2023 +0530", "commit_message": "Revert 49296437a8e5fa91dacb5446e51ab588474c85d3", "files_name": ["README.md", "yt_dlp/extractor/twitter.py"]}, {"commit_id": "92315c03774cfabb3a921884326beb4b981f786b", "commit_date": "Thu Jul 6 14:39:51 2023 -0500", "commit_message": "[extractor/twitter] Fix GraphQL and legacy API (#7516)", "files_name": ["yt_dlp/extractor/twitter.py"]}, {"commit_id": "bdd0b75e3f41ff35440eda6d395008beef19ef2f", "commit_date": "Sun Jul 9 06:26:03 2023 +0800", "commit_message": "[ie/BiliBiliBangumi] Fix extractors (#7337)", "files_name": ["yt_dlp/extractor/_extractors.py", "yt_dlp/extractor/bilibili.py"]}, {"commit_id": "325191d0c9bf3fe257b8a7c2eb95080f44f6ddfc", "commit_date": "Mon Jul 10 15:15:47 2023 +0200", "commit_message": "[ie/vrt] Update token signing key (#7519)", "files_name": ["yt_dlp/extractor/vrt.py"]}, {"commit_id": "2af4eeb77246b8183aae75a0a8d19f18c08115b2", "commit_date": "Tue Jul 11 05:00:38 2023 +0400", "commit_message": "[utils] `clean_podcast_url`: Handle more trackers (#7556)", "files_name": ["test/test_utils.py", "yt_dlp/utils/_utils.py"]}, {"commit_id": "2cfe221fbbe46faa3f46552c08d947a51f424903", "commit_date": "Thu Jul 13 20:17:05 2023 +0600", "commit_message": "[ie/streamanity] Remove (#7571)", "files_name": ["yt_dlp/extractor/_extractors.py", "yt_dlp/extractor/streamanity.py"]}, {"commit_id": "8a4cd12c8f8e93292e3e95200b9d17a3af39624c", "commit_date": "Thu Jul 13 16:39:21 2023 -0400", "commit_message": "[pp/EmbedThumbnail] Support `m4v` (#7583)", "files_name": ["yt_dlp/postprocessor/embedthumbnail.py"]}, {"commit_id": "1bcb9fe8715b1f288efc322be3de409ee0597080", "commit_date": "Fri Jul 14 20:09:02 2023 +0200", "commit_message": "[ie/piapro] Support `/content` URL (#7592)", "files_name": ["yt_dlp/extractor/piapro.py"]}, {"commit_id": "1ba6fe9db5f660d5538588315c23ad6cf0371c5f", "commit_date": "Sat Jul 15 15:20:24 2023 +1200", "commit_message": "[ie/youtube:tab] Detect looping feeds (#6621)", "files_name": ["yt_dlp/extractor/youtube.py"]}, {"commit_id": "1b392f905d20ef1f1b300b180f867d43c9ce49b8", "commit_date": "Sat Jul 15 11:41:08 2023 +0530", "commit_message": "[utils] Add temporary shim for logging", "files_name": ["test/test_downloader_http.py", "yt_dlp/cookies.py", "yt_dlp/utils/_utils.py"]}, {"commit_id": "c365dba8430ee33abda85d31f95128605bf240eb", "commit_date": "Sat Jul 15 14:30:08 2023 +0530", "commit_message": "[networking] Add module (#2861)", "files_name": ["Makefile", "devscripts/make_changelog.py", "test/test_networking.py", "test/test_utils.py", "yt_dlp/YoutubeDL.py", "yt_dlp/networking/__init__.py", "yt_dlp/networking/_helper.py", "yt_dlp/networking/_urllib.py", "yt_dlp/networking/exceptions.py", "yt_dlp/utils/__init__.py", "yt_dlp/utils/_deprecated.py", "yt_dlp/utils/_utils.py", "yt_dlp/utils/networking.py"]}, {"commit_id": "227bf1a33be7b89cd7d44ad046844c4ccba104f4", "commit_date": "Sat Jul 15 15:55:23 2023 +0530", "commit_message": "[networking] Rewrite architecture (#2861)", "files_name": ["test/test_download.py", "test/test_networking.py", "test/test_networking_utils.py", "test/test_utils.py", "yt_dlp/YoutubeDL.py", "yt_dlp/compat/__init__.py", "yt_dlp/downloader/http.py", "yt_dlp/extractor/common.py", "yt_dlp/networking/__init__.py", "yt_dlp/networking/_helper.py", "yt_dlp/networking/_urllib.py", "yt_dlp/networking/common.py", "yt_dlp/networking/exceptions.py", "yt_dlp/utils/_deprecated.py", "yt_dlp/utils/_utils.py", "yt_dlp/utils/networking.py"]}, {"commit_id": "3d2623a898196640f7cc0fc8b70118ff19e6925d", "commit_date": "Sun Jul 9 13:23:02 2023 +0530", "commit_message": "[compat, networking] Deprecate old functions (#2861)", "files_name": ["test/test_download.py", "test/test_networking.py", "test/test_networking_utils.py", "yt_dlp/YoutubeDL.py", "yt_dlp/__init__.py", "yt_dlp/compat/_deprecated.py", "yt_dlp/compat/_legacy.py", "yt_dlp/downloader/external.py", "yt_dlp/downloader/f4m.py", "yt_dlp/downloader/fragment.py", "yt_dlp/downloader/hls.py", "yt_dlp/downloader/http.py", "yt_dlp/downloader/ism.py", "yt_dlp/downloader/niconico.py", "yt_dlp/downloader/youtube_live_chat.py", "yt_dlp/extractor/abematv.py", "yt_dlp/extractor/adn.py", "yt_dlp/extractor/adobepass.py", "yt_dlp/extractor/ant1newsgr.py", "yt_dlp/extractor/archiveorg.py", "yt_dlp/extractor/atresplayer.py", "yt_dlp/extractor/bbc.py", "yt_dlp/extractor/bilibili.py", "yt_dlp/extractor/bitchute.py", "yt_dlp/extractor/bravotv.py", "yt_dlp/extractor/brightcove.py", "yt_dlp/extractor/canalplus.py", "yt_dlp/extractor/cbsnews.py", "yt_dlp/extractor/ceskatelevize.py", "yt_dlp/extractor/cinetecamilano.py", "yt_dlp/extractor/ciscowebex.py", "yt_dlp/extractor/common.py", "yt_dlp/extractor/crackle.py", "yt_dlp/extractor/crunchyroll.py", "yt_dlp/extractor/cultureunplugged.py", "yt_dlp/extractor/dacast.py", "yt_dlp/extractor/dailymotion.py", "yt_dlp/extractor/discovery.py", "yt_dlp/extractor/dplay.py", "yt_dlp/extractor/eagleplatform.py", "yt_dlp/extractor/eitb.py", "yt_dlp/extractor/eporner.py", "yt_dlp/extractor/facebook.py", "yt_dlp/extractor/fc2.py", "yt_dlp/extractor/filmon.py", "yt_dlp/extractor/fox.py", "yt_dlp/extractor/foxsports.py", "yt_dlp/extractor/fujitv.py", "yt_dlp/extractor/funimation.py", "yt_dlp/extractor/gdcvault.py", "yt_dlp/extractor/generic.py", "yt_dlp/extractor/globo.py", "yt_dlp/extractor/googledrive.py", "yt_dlp/extractor/hketv.py", "yt_dlp/extractor/hotnewhiphop.py", "yt_dlp/extractor/hotstar.py", "yt_dlp/extractor/hrti.py", "yt_dlp/extractor/ign.py", "yt_dlp/extractor/imggaming.py", "yt_dlp/extractor/instagram.py", "yt_dlp/extractor/iprima.py", "yt_dlp/extractor/kakao.py", "yt_dlp/extractor/kick.py", "yt_dlp/extractor/kuwo.py", "yt_dlp/extractor/la7.py", "yt_dlp/extractor/lbry.py", "yt_dlp/extractor/lecturio.py", "yt_dlp/extractor/lego.py", "yt_dlp/extractor/limelight.py", "yt_dlp/extractor/linuxacademy.py", "yt_dlp/extractor/mediasite.py", "yt_dlp/extractor/megatvcom.py", "yt_dlp/extractor/mgtv.py", "yt_dlp/extractor/minds.py", "yt_dlp/extractor/miomio.py", "yt_dlp/extractor/mtv.py", "yt_dlp/extractor/nbc.py", "yt_dlp/extractor/nebula.py", "yt_dlp/extractor/neteasemusic.py", "yt_dlp/extractor/niconico.py", "yt_dlp/extractor/njpwworld.py", "yt_dlp/extractor/nosvideo.py", "yt_dlp/extractor/nowness.py", "yt_dlp/extractor/nrk.py", "yt_dlp/extractor/odkmedia.py", "yt_dlp/extractor/odnoklassniki.py", "yt_dlp/extractor/orf.py", "yt_dlp/extractor/owncloud.py", "yt_dlp/extractor/packtpub.py", "yt_dlp/extractor/patreon.py", "yt_dlp/extractor/peloton.py", "yt_dlp/extractor/piapro.py", "yt_dlp/extractor/pladform.py", "yt_dlp/extractor/platzi.py", "yt_dlp/extractor/playplustv.py", "yt_dlp/extractor/pornhub.py", "yt_dlp/extractor/puhutv.py", "yt_dlp/extractor/radiko.py", "yt_dlp/extractor/radiocanada.py", "yt_dlp/extractor/rcs.py", "yt_dlp/extractor/rcti.py", "yt_dlp/extractor/recurbate.py", "yt_dlp/extractor/redbulltv.py", "yt_dlp/extractor/redgifs.py"]}], "parents": [{"commit_id_before": "1ceb657bdd254ad961489e5060f2ccc7d556b729", "url_before": "https://api.github.com/repos/yt-dlp/yt-dlp/commits/1ceb657bdd254ad961489e5060f2ccc7d556b729", "html_url_before": "https://github.com/yt-dlp/yt-dlp/commit/1ceb657bdd254ad961489e5060f2ccc7d556b729"}], "details": [{"raw_url": "https://github.com/yt-dlp/yt-dlp/raw/f8b4bcc0a791274223723488bfbfc23ea3276641/test%2Ftest_http.py", "code": "#!/usr/bin/env python3\n\n# Allow direct execution\nimport os\nimport sys\nimport unittest\n\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport gzip\nimport http.cookiejar\nimport http.server\nimport io\nimport pathlib\nimport ssl\nimport tempfile\nimport threading\nimport urllib.error\nimport urllib.request\nimport zlib\n\nfrom test.helper import http_server_port\nfrom yt_dlp import YoutubeDL\nfrom yt_dlp.dependencies import brotli\nfrom yt_dlp.utils import sanitized_Request, urlencode_postdata\n\nfrom .helper import FakeYDL\n\nTEST_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\nclass HTTPTestRequestHandler(http.server.BaseHTTPRequestHandler):\n    protocol_version = 'HTTP/1.1'\n\n    def log_message(self, format, *args):\n        pass\n\n    def _headers(self):\n        payload = str(self.headers).encode('utf-8')\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.send_header('Content-Length', str(len(payload)))\n        self.end_headers()\n        self.wfile.write(payload)\n\n    def _redirect(self):\n        self.send_response(int(self.path[len('/redirect_'):]))\n        self.send_header('Location', '/method')\n        self.send_header('Content-Length', '0')\n        self.end_headers()\n\n    def _method(self, method, payload=None):\n        self.send_response(200)\n        self.send_header('Content-Length', str(len(payload or '')))\n        self.send_header('Method', method)\n        self.end_headers()\n        if payload:\n            self.wfile.write(payload)\n\n    def _status(self, status):\n        payload = f'<html>{status} NOT FOUND</html>'.encode()\n        self.send_response(int(status))\n        self.send_header('Content-Type', 'text/html; charset=utf-8')\n        self.send_header('Content-Length', str(len(payload)))\n        self.end_headers()\n        self.wfile.write(payload)\n\n    def _read_data(self):\n        if 'Content-Length' in self.headers:\n            return self.rfile.read(int(self.headers['Content-Length']))\n\n    def do_POST(self):\n        data = self._read_data()\n        if self.path.startswith('/redirect_'):\n            self._redirect()\n        elif self.path.startswith('/method'):\n            self._method('POST', data)\n        elif self.path.startswith('/headers'):\n            self._headers()\n        else:\n            self._status(404)\n\n    def do_HEAD(self):\n        if self.path.startswith('/redirect_'):\n            self._redirect()\n        elif self.path.startswith('/method'):\n            self._method('HEAD')\n        else:\n            self._status(404)\n\n    def do_PUT(self):\n        data = self._read_data()\n        if self.path.startswith('/redirect_'):\n            self._redirect()\n        elif self.path.startswith('/method'):\n            self._method('PUT', data)\n        else:\n            self._status(404)\n\n    def do_GET(self):\n        if self.path == '/video.html':\n            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/html; charset=utf-8')\n            self.send_header('Content-Length', str(len(payload)))  # required for persistent connections\n            self.end_headers()\n            self.wfile.write(payload)\n        elif self.path == '/vid.mp4':\n            payload = b'\\x00\\x00\\x00\\x00\\x20\\x66\\x74[video]'\n            self.send_response(200)\n            self.send_header('Content-Type', 'video/mp4')\n            self.send_header('Content-Length', str(len(payload)))\n            self.end_headers()\n            self.wfile.write(payload)\n        elif self.path == '/%E4%B8%AD%E6%96%87.html':\n            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/html; charset=utf-8')\n            self.send_header('Content-Length', str(len(payload)))\n            self.end_headers()\n            self.wfile.write(payload)\n        elif self.path == '/%c7%9f':\n            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/html; charset=utf-8')\n            self.send_header('Content-Length', str(len(payload)))\n            self.end_headers()\n            self.wfile.write(payload)\n        elif self.path.startswith('/redirect_'):\n            self._redirect()\n        elif self.path.startswith('/method'):\n            self._method('GET')\n        elif self.path.startswith('/headers'):\n            self._headers()\n        elif self.path.startswith('/308-to-headers'):\n            self.send_response(308)\n            self.send_header('Location', '/headers')\n            self.send_header('Content-Length', '0')\n            self.end_headers()\n        elif self.path == '/trailing_garbage':\n            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/html; charset=utf-8')\n            self.send_header('Content-Encoding', 'gzip')\n            buf = io.BytesIO()\n            with gzip.GzipFile(fileobj=buf, mode='wb') as f:\n                f.write(payload)\n            compressed = buf.getvalue() + b'trailing garbage'\n            self.send_header('Content-Length', str(len(compressed)))\n            self.end_headers()\n            self.wfile.write(compressed)\n        elif self.path == '/302-non-ascii-redirect':\n            new_url = f'http://127.0.0.1:{http_server_port(self.server)}/\u4e2d\u6587.html'\n            self.send_response(301)\n            self.send_header('Location', new_url)\n            self.send_header('Content-Length', '0')\n            self.end_headers()\n        elif self.path == '/content-encoding':\n            encodings = self.headers.get('ytdl-encoding', '')\n            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n            for encoding in filter(None, (e.strip() for e in encodings.split(','))):\n                if encoding == 'br' and brotli:\n                    payload = brotli.compress(payload)\n                elif encoding == 'gzip':\n                    buf = io.BytesIO()\n                    with gzip.GzipFile(fileobj=buf, mode='wb') as f:\n                        f.write(payload)\n                    payload = buf.getvalue()\n                elif encoding == 'deflate':\n                    payload = zlib.compress(payload)\n                elif encoding == 'unsupported':\n                    payload = b'raw'\n                    break\n                else:\n                    self._status(415)\n                    return\n            self.send_response(200)\n            self.send_header('Content-Encoding', encodings)\n            self.send_header('Content-Length', str(len(payload)))\n            self.end_headers()\n            self.wfile.write(payload)\n\n        else:\n            self._status(404)\n\n    def send_header(self, keyword, value):\n        \"\"\"\n        Forcibly allow HTTP server to send non percent-encoded non-ASCII characters in headers.\n        This is against what is defined in RFC 3986, however we need to test we support this\n        since some sites incorrectly do this.\n        \"\"\"\n        if keyword.lower() == 'connection':\n            return super().send_header(keyword, value)\n\n        if not hasattr(self, '_headers_buffer'):\n            self._headers_buffer = []\n\n        self._headers_buffer.append(f'{keyword}: {value}\\r\\n'.encode())\n\n\nclass FakeLogger:\n    def debug(self, msg):\n        pass\n\n    def warning(self, msg):\n        pass\n\n    def error(self, msg):\n        pass\n\n\nclass TestHTTP(unittest.TestCase):\n    def setUp(self):\n        # HTTP server\n        self.http_httpd = http.server.ThreadingHTTPServer(\n            ('127.0.0.1', 0), HTTPTestRequestHandler)\n        self.http_port = http_server_port(self.http_httpd)\n        self.http_server_thread = threading.Thread(target=self.http_httpd.serve_forever)\n        # FIXME: we should probably stop the http server thread after each test\n        # See: https://github.com/yt-dlp/yt-dlp/pull/7094#discussion_r1199746041\n        self.http_server_thread.daemon = True\n        self.http_server_thread.start()\n\n        # HTTPS server\n        certfn = os.path.join(TEST_DIR, 'testcert.pem')\n        self.https_httpd = http.server.ThreadingHTTPServer(\n            ('127.0.0.1', 0), HTTPTestRequestHandler)\n        sslctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n        sslctx.load_cert_chain(certfn, None)\n        self.https_httpd.socket = sslctx.wrap_socket(self.https_httpd.socket, server_side=True)\n        self.https_port = http_server_port(self.https_httpd)\n        self.https_server_thread = threading.Thread(target=self.https_httpd.serve_forever)\n        self.https_server_thread.daemon = True\n        self.https_server_thread.start()\n\n    def test_nocheckcertificate(self):\n        with FakeYDL({'logger': FakeLogger()}) as ydl:\n            with self.assertRaises(urllib.error.URLError):\n                ydl.urlopen(sanitized_Request(f'https://127.0.0.1:{self.https_port}/headers'))\n\n        with FakeYDL({'logger': FakeLogger(), 'nocheckcertificate': True}) as ydl:\n            r = ydl.urlopen(sanitized_Request(f'https://127.0.0.1:{self.https_port}/headers'))\n            self.assertEqual(r.status, 200)\n            r.close()\n\n    def test_percent_encode(self):\n        with FakeYDL() as ydl:\n            # Unicode characters should be encoded with uppercase percent-encoding\n            res = ydl.urlopen(sanitized_Request(f'http://127.0.0.1:{self.http_port}/\u4e2d\u6587.html'))\n            self.assertEqual(res.status, 200)\n            res.close()\n            # don't normalize existing percent encodings\n            res = ydl.urlopen(sanitized_Request(f'http://127.0.0.1:{self.http_port}/%c7%9f'))\n            self.assertEqual(res.status, 200)\n            res.close()\n\n    def test_unicode_path_redirection(self):\n        with FakeYDL() as ydl:\n            r = ydl.urlopen(sanitized_Request(f'http://127.0.0.1:{self.http_port}/302-non-ascii-redirect'))\n            self.assertEqual(r.url, f'http://127.0.0.1:{self.http_port}/%E4%B8%AD%E6%96%87.html')\n            r.close()\n\n    def test_redirect(self):\n        with FakeYDL() as ydl:\n            def do_req(redirect_status, method):\n                data = b'testdata' if method in ('POST', 'PUT') else None\n                res = ydl.urlopen(sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/redirect_{redirect_status}', method=method, data=data))\n                return res.read().decode('utf-8'), res.headers.get('method', '')\n\n            # A 303 must either use GET or HEAD for subsequent request\n            self.assertEqual(do_req(303, 'POST'), ('', 'GET'))\n            self.assertEqual(do_req(303, 'HEAD'), ('', 'HEAD'))\n\n            self.assertEqual(do_req(303, 'PUT'), ('', 'GET'))\n\n            # 301 and 302 turn POST only into a GET\n            # XXX: we should also test if the Content-Type and Content-Length headers are removed\n            self.assertEqual(do_req(301, 'POST'), ('', 'GET'))\n            self.assertEqual(do_req(301, 'HEAD'), ('', 'HEAD'))\n            self.assertEqual(do_req(302, 'POST'), ('', 'GET'))\n            self.assertEqual(do_req(302, 'HEAD'), ('', 'HEAD'))\n\n            self.assertEqual(do_req(301, 'PUT'), ('testdata', 'PUT'))\n            self.assertEqual(do_req(302, 'PUT'), ('testdata', 'PUT'))\n\n            # 307 and 308 should not change method\n            for m in ('POST', 'PUT'):\n                self.assertEqual(do_req(307, m), ('testdata', m))\n                self.assertEqual(do_req(308, m), ('testdata', m))\n\n            self.assertEqual(do_req(307, 'HEAD'), ('', 'HEAD'))\n            self.assertEqual(do_req(308, 'HEAD'), ('', 'HEAD'))\n\n            # These should not redirect and instead raise an HTTPError\n            for code in (300, 304, 305, 306):\n                with self.assertRaises(urllib.error.HTTPError):\n                    do_req(code, 'GET')\n\n    def test_content_type(self):\n        # https://github.com/yt-dlp/yt-dlp/commit/379a4f161d4ad3e40932dcf5aca6e6fb9715ab28\n        with FakeYDL({'nocheckcertificate': True}) as ydl:\n            # method should be auto-detected as POST\n            r = sanitized_Request(f'https://localhost:{self.https_port}/headers', data=urlencode_postdata({'test': 'test'}))\n\n            headers = ydl.urlopen(r).read().decode('utf-8')\n            self.assertIn('Content-Type: application/x-www-form-urlencoded', headers)\n\n            # test http\n            r = sanitized_Request(f'http://localhost:{self.http_port}/headers', data=urlencode_postdata({'test': 'test'}))\n            headers = ydl.urlopen(r).read().decode('utf-8')\n            self.assertIn('Content-Type: application/x-www-form-urlencoded', headers)\n\n    def test_cookiejar(self):\n        with FakeYDL() as ydl:\n            ydl.cookiejar.set_cookie(http.cookiejar.Cookie(\n                0, 'test', 'ytdlp', None, False, '127.0.0.1', True,\n                False, '/headers', True, False, None, False, None, None, {}))\n            data = ydl.urlopen(sanitized_Request(f'http://127.0.0.1:{self.http_port}/headers')).read()\n            self.assertIn(b'Cookie: test=ytdlp', data)\n\n    def test_passed_cookie_header(self):\n        # We should accept a Cookie header being passed as in normal headers and handle it appropriately.\n        with FakeYDL() as ydl:\n            # Specified Cookie header should be used\n            res = ydl.urlopen(\n                sanitized_Request(f'http://127.0.0.1:{self.http_port}/headers',\n                                  headers={'Cookie': 'test=test'})).read().decode('utf-8')\n            self.assertIn('Cookie: test=test', res)\n\n            # Specified Cookie header should be removed on any redirect\n            res = ydl.urlopen(\n                sanitized_Request(f'http://127.0.0.1:{self.http_port}/308-to-headers', headers={'Cookie': 'test=test'})).read().decode('utf-8')\n            self.assertNotIn('Cookie: test=test', res)\n\n            # Specified Cookie header should override global cookiejar for that request\n            ydl.cookiejar.set_cookie(http.cookiejar.Cookie(\n                version=0, name='test', value='ytdlp', port=None, port_specified=False,\n                domain='127.0.0.1', domain_specified=True, domain_initial_dot=False, path='/',\n                path_specified=True, secure=False, expires=None, discard=False, comment=None,\n                comment_url=None, rest={}))\n\n            data = ydl.urlopen(sanitized_Request(f'http://127.0.0.1:{self.http_port}/headers', headers={'Cookie': 'test=test'})).read()\n            self.assertNotIn(b'Cookie: test=ytdlp', data)\n            self.assertIn(b'Cookie: test=test', data)\n\n    def test_no_compression_compat_header(self):\n        with FakeYDL() as ydl:\n            data = ydl.urlopen(\n                sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/headers',\n                    headers={'Youtubedl-no-compression': True})).read()\n            self.assertIn(b'Accept-Encoding: identity', data)\n            self.assertNotIn(b'youtubedl-no-compression', data.lower())\n\n    def test_gzip_trailing_garbage(self):\n        # https://github.com/ytdl-org/youtube-dl/commit/aa3e950764337ef9800c936f4de89b31c00dfcf5\n        # https://github.com/ytdl-org/youtube-dl/commit/6f2ec15cee79d35dba065677cad9da7491ec6e6f\n        with FakeYDL() as ydl:\n            data = ydl.urlopen(sanitized_Request(f'http://localhost:{self.http_port}/trailing_garbage')).read().decode('utf-8')\n            self.assertEqual(data, '<html><video src=\"/vid.mp4\" /></html>')\n\n    @unittest.skipUnless(brotli, 'brotli support is not installed')\n    def test_brotli(self):\n        with FakeYDL() as ydl:\n            res = ydl.urlopen(\n                sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/content-encoding',\n                    headers={'ytdl-encoding': 'br'}))\n            self.assertEqual(res.headers.get('Content-Encoding'), 'br')\n            self.assertEqual(res.read(), b'<html><video src=\"/vid.mp4\" /></html>')\n\n    def test_deflate(self):\n        with FakeYDL() as ydl:\n            res = ydl.urlopen(\n                sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/content-encoding',\n                    headers={'ytdl-encoding': 'deflate'}))\n            self.assertEqual(res.headers.get('Content-Encoding'), 'deflate')\n            self.assertEqual(res.read(), b'<html><video src=\"/vid.mp4\" /></html>')\n\n    def test_gzip(self):\n        with FakeYDL() as ydl:\n            res = ydl.urlopen(\n                sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/content-encoding',\n                    headers={'ytdl-encoding': 'gzip'}))\n            self.assertEqual(res.headers.get('Content-Encoding'), 'gzip')\n            self.assertEqual(res.read(), b'<html><video src=\"/vid.mp4\" /></html>')\n\n    def test_multiple_encodings(self):\n        # https://www.rfc-editor.org/rfc/rfc9110.html#section-8.4\n        with FakeYDL() as ydl:\n            for pair in ('gzip,deflate', 'deflate, gzip', 'gzip, gzip', 'deflate, deflate'):\n                res = ydl.urlopen(\n                    sanitized_Request(\n                        f'http://127.0.0.1:{self.http_port}/content-encoding',\n                        headers={'ytdl-encoding': pair}))\n                self.assertEqual(res.headers.get('Content-Encoding'), pair)\n                self.assertEqual(res.read(), b'<html><video src=\"/vid.mp4\" /></html>')\n\n    def test_unsupported_encoding(self):\n        # it should return the raw content\n        with FakeYDL() as ydl:\n            res = ydl.urlopen(\n                sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/content-encoding',\n                    headers={'ytdl-encoding': 'unsupported'}))\n            self.assertEqual(res.headers.get('Content-Encoding'), 'unsupported')\n            self.assertEqual(res.read(), b'raw')\n\n\nclass TestClientCert(unittest.TestCase):\n    def setUp(self):\n        certfn = os.path.join(TEST_DIR, 'testcert.pem')\n        self.certdir = os.path.join(TEST_DIR, 'testdata', 'certificate')\n        cacertfn = os.path.join(self.certdir, 'ca.crt')\n        self.httpd = http.server.HTTPServer(('127.0.0.1', 0), HTTPTestRequestHandler)\n        sslctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n        sslctx.verify_mode = ssl.CERT_REQUIRED\n        sslctx.load_verify_locations(cafile=cacertfn)\n        sslctx.load_cert_chain(certfn, None)\n        self.httpd.socket = sslctx.wrap_socket(self.httpd.socket, server_side=True)\n        self.port = http_server_port(self.httpd)\n        self.server_thread = threading.Thread(target=self.httpd.serve_forever)\n        self.server_thread.daemon = True\n        self.server_thread.start()\n\n    def _run_test(self, **params):\n        ydl = YoutubeDL({\n            'logger': FakeLogger(),\n            # Disable client-side validation of unacceptable self-signed testcert.pem\n            # The test is of a check on the server side, so unaffected\n            'nocheckcertificate': True,\n            **params,\n        })\n        r = ydl.extract_info(f'https://127.0.0.1:{self.port}/video.html')\n        self.assertEqual(r['url'], f'https://127.0.0.1:{self.port}/vid.mp4')\n\n    def test_certificate_combined_nopass(self):\n        self._run_test(client_certificate=os.path.join(self.certdir, 'clientwithkey.crt'))\n\n    def test_certificate_nocombined_nopass(self):\n        self._run_test(client_certificate=os.path.join(self.certdir, 'client.crt'),\n                       client_certificate_key=os.path.join(self.certdir, 'client.key'))\n\n    def test_certificate_combined_pass(self):\n        self._run_test(client_certificate=os.path.join(self.certdir, 'clientwithencryptedkey.crt'),\n                       client_certificate_password='foobar')\n\n    def test_certificate_nocombined_pass(self):\n        self._run_test(client_certificate=os.path.join(self.certdir, 'client.crt'),\n                       client_certificate_key=os.path.join(self.certdir, 'clientencrypted.key'),\n                       client_certificate_password='foobar')\n\n\ndef _build_proxy_handler(name):\n    class HTTPTestRequestHandler(http.server.BaseHTTPRequestHandler):\n        proxy_name = name\n\n        def log_message(self, format, *args):\n            pass\n\n        def do_GET(self):\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/plain; charset=utf-8')\n            self.end_headers()\n            self.wfile.write(f'{self.proxy_name}: {self.path}'.encode())\n    return HTTPTestRequestHandler\n\n\nclass TestProxy(unittest.TestCase):\n    def setUp(self):\n        self.proxy = http.server.HTTPServer(\n            ('127.0.0.1', 0), _build_proxy_handler('normal'))\n        self.port = http_server_port(self.proxy)\n        self.proxy_thread = threading.Thread(target=self.proxy.serve_forever)\n        self.proxy_thread.daemon = True\n        self.proxy_thread.start()\n\n        self.geo_proxy = http.server.HTTPServer(\n            ('127.0.0.1', 0), _build_proxy_handler('geo'))\n        self.geo_port = http_server_port(self.geo_proxy)\n        self.geo_proxy_thread = threading.Thread(target=self.geo_proxy.serve_forever)\n        self.geo_proxy_thread.daemon = True\n        self.geo_proxy_thread.start()\n\n    def test_proxy(self):\n        geo_proxy = f'127.0.0.1:{self.geo_port}'\n        ydl = YoutubeDL({\n            'proxy': f'127.0.0.1:{self.port}',\n            'geo_verification_proxy': geo_proxy,\n        })\n        url = 'http://foo.com/bar'\n        response = ydl.urlopen(url).read().decode()\n        self.assertEqual(response, f'normal: {url}')\n\n        req = urllib.request.Request(url)\n        req.add_header('Ytdl-request-proxy', geo_proxy)\n        response = ydl.urlopen(req).read().decode()\n        self.assertEqual(response, f'geo: {url}')\n\n    def test_proxy_with_idn(self):\n        ydl = YoutubeDL({\n            'proxy': f'127.0.0.1:{self.port}',\n        })\n        url = 'http://\u4e2d\u6587.tw/'\n        response = ydl.urlopen(url).read().decode()\n        # b'xn--fiq228c' is '\u4e2d\u6587'.encode('idna')\n        self.assertEqual(response, 'normal: http://xn--fiq228c.tw/')\n\n\nclass TestFileURL(unittest.TestCase):\n    # See https://github.com/ytdl-org/youtube-dl/issues/8227\n    def test_file_urls(self):\n        tf = tempfile.NamedTemporaryFile(delete=False)\n        tf.write(b'foobar')\n        tf.close()\n        url = pathlib.Path(tf.name).as_uri()\n        with FakeYDL() as ydl:\n            self.assertRaisesRegex(\n                urllib.error.URLError, 'file:// URLs are explicitly disabled in yt-dlp for security reasons', ydl.urlopen, url)\n        with FakeYDL({'enable_file_urls': True}) as ydl:\n            res = ydl.urlopen(url)\n            self.assertEqual(res.read(), b'foobar')\n            res.close()\n        os.unlink(tf.name)\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "code_before": "#!/usr/bin/env python3\n\n# Allow direct execution\nimport os\nimport sys\nimport unittest\n\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport gzip\nimport http.cookiejar\nimport http.server\nimport io\nimport pathlib\nimport ssl\nimport tempfile\nimport threading\nimport urllib.error\nimport urllib.request\nimport zlib\n\nfrom test.helper import http_server_port\nfrom yt_dlp import YoutubeDL\nfrom yt_dlp.dependencies import brotli\nfrom yt_dlp.utils import sanitized_Request, urlencode_postdata\n\nfrom .helper import FakeYDL\n\nTEST_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\nclass HTTPTestRequestHandler(http.server.BaseHTTPRequestHandler):\n    protocol_version = 'HTTP/1.1'\n\n    def log_message(self, format, *args):\n        pass\n\n    def _headers(self):\n        payload = str(self.headers).encode('utf-8')\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.send_header('Content-Length', str(len(payload)))\n        self.end_headers()\n        self.wfile.write(payload)\n\n    def _redirect(self):\n        self.send_response(int(self.path[len('/redirect_'):]))\n        self.send_header('Location', '/method')\n        self.send_header('Content-Length', '0')\n        self.end_headers()\n\n    def _method(self, method, payload=None):\n        self.send_response(200)\n        self.send_header('Content-Length', str(len(payload or '')))\n        self.send_header('Method', method)\n        self.end_headers()\n        if payload:\n            self.wfile.write(payload)\n\n    def _status(self, status):\n        payload = f'<html>{status} NOT FOUND</html>'.encode()\n        self.send_response(int(status))\n        self.send_header('Content-Type', 'text/html; charset=utf-8')\n        self.send_header('Content-Length', str(len(payload)))\n        self.end_headers()\n        self.wfile.write(payload)\n\n    def _read_data(self):\n        if 'Content-Length' in self.headers:\n            return self.rfile.read(int(self.headers['Content-Length']))\n\n    def do_POST(self):\n        data = self._read_data()\n        if self.path.startswith('/redirect_'):\n            self._redirect()\n        elif self.path.startswith('/method'):\n            self._method('POST', data)\n        elif self.path.startswith('/headers'):\n            self._headers()\n        else:\n            self._status(404)\n\n    def do_HEAD(self):\n        if self.path.startswith('/redirect_'):\n            self._redirect()\n        elif self.path.startswith('/method'):\n            self._method('HEAD')\n        else:\n            self._status(404)\n\n    def do_PUT(self):\n        data = self._read_data()\n        if self.path.startswith('/redirect_'):\n            self._redirect()\n        elif self.path.startswith('/method'):\n            self._method('PUT', data)\n        else:\n            self._status(404)\n\n    def do_GET(self):\n        if self.path == '/video.html':\n            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/html; charset=utf-8')\n            self.send_header('Content-Length', str(len(payload)))  # required for persistent connections\n            self.end_headers()\n            self.wfile.write(payload)\n        elif self.path == '/vid.mp4':\n            payload = b'\\x00\\x00\\x00\\x00\\x20\\x66\\x74[video]'\n            self.send_response(200)\n            self.send_header('Content-Type', 'video/mp4')\n            self.send_header('Content-Length', str(len(payload)))\n            self.end_headers()\n            self.wfile.write(payload)\n        elif self.path == '/%E4%B8%AD%E6%96%87.html':\n            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/html; charset=utf-8')\n            self.send_header('Content-Length', str(len(payload)))\n            self.end_headers()\n            self.wfile.write(payload)\n        elif self.path == '/%c7%9f':\n            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/html; charset=utf-8')\n            self.send_header('Content-Length', str(len(payload)))\n            self.end_headers()\n            self.wfile.write(payload)\n        elif self.path.startswith('/redirect_'):\n            self._redirect()\n        elif self.path.startswith('/method'):\n            self._method('GET')\n        elif self.path.startswith('/headers'):\n            self._headers()\n        elif self.path == '/trailing_garbage':\n            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/html; charset=utf-8')\n            self.send_header('Content-Encoding', 'gzip')\n            buf = io.BytesIO()\n            with gzip.GzipFile(fileobj=buf, mode='wb') as f:\n                f.write(payload)\n            compressed = buf.getvalue() + b'trailing garbage'\n            self.send_header('Content-Length', str(len(compressed)))\n            self.end_headers()\n            self.wfile.write(compressed)\n        elif self.path == '/302-non-ascii-redirect':\n            new_url = f'http://127.0.0.1:{http_server_port(self.server)}/\u4e2d\u6587.html'\n            self.send_response(301)\n            self.send_header('Location', new_url)\n            self.send_header('Content-Length', '0')\n            self.end_headers()\n        elif self.path == '/content-encoding':\n            encodings = self.headers.get('ytdl-encoding', '')\n            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n            for encoding in filter(None, (e.strip() for e in encodings.split(','))):\n                if encoding == 'br' and brotli:\n                    payload = brotli.compress(payload)\n                elif encoding == 'gzip':\n                    buf = io.BytesIO()\n                    with gzip.GzipFile(fileobj=buf, mode='wb') as f:\n                        f.write(payload)\n                    payload = buf.getvalue()\n                elif encoding == 'deflate':\n                    payload = zlib.compress(payload)\n                elif encoding == 'unsupported':\n                    payload = b'raw'\n                    break\n                else:\n                    self._status(415)\n                    return\n            self.send_response(200)\n            self.send_header('Content-Encoding', encodings)\n            self.send_header('Content-Length', str(len(payload)))\n            self.end_headers()\n            self.wfile.write(payload)\n\n        else:\n            self._status(404)\n\n    def send_header(self, keyword, value):\n        \"\"\"\n        Forcibly allow HTTP server to send non percent-encoded non-ASCII characters in headers.\n        This is against what is defined in RFC 3986, however we need to test we support this\n        since some sites incorrectly do this.\n        \"\"\"\n        if keyword.lower() == 'connection':\n            return super().send_header(keyword, value)\n\n        if not hasattr(self, '_headers_buffer'):\n            self._headers_buffer = []\n\n        self._headers_buffer.append(f'{keyword}: {value}\\r\\n'.encode())\n\n\nclass FakeLogger:\n    def debug(self, msg):\n        pass\n\n    def warning(self, msg):\n        pass\n\n    def error(self, msg):\n        pass\n\n\nclass TestHTTP(unittest.TestCase):\n    def setUp(self):\n        # HTTP server\n        self.http_httpd = http.server.ThreadingHTTPServer(\n            ('127.0.0.1', 0), HTTPTestRequestHandler)\n        self.http_port = http_server_port(self.http_httpd)\n        self.http_server_thread = threading.Thread(target=self.http_httpd.serve_forever)\n        # FIXME: we should probably stop the http server thread after each test\n        # See: https://github.com/yt-dlp/yt-dlp/pull/7094#discussion_r1199746041\n        self.http_server_thread.daemon = True\n        self.http_server_thread.start()\n\n        # HTTPS server\n        certfn = os.path.join(TEST_DIR, 'testcert.pem')\n        self.https_httpd = http.server.ThreadingHTTPServer(\n            ('127.0.0.1', 0), HTTPTestRequestHandler)\n        sslctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n        sslctx.load_cert_chain(certfn, None)\n        self.https_httpd.socket = sslctx.wrap_socket(self.https_httpd.socket, server_side=True)\n        self.https_port = http_server_port(self.https_httpd)\n        self.https_server_thread = threading.Thread(target=self.https_httpd.serve_forever)\n        self.https_server_thread.daemon = True\n        self.https_server_thread.start()\n\n    def test_nocheckcertificate(self):\n        with FakeYDL({'logger': FakeLogger()}) as ydl:\n            with self.assertRaises(urllib.error.URLError):\n                ydl.urlopen(sanitized_Request(f'https://127.0.0.1:{self.https_port}/headers'))\n\n        with FakeYDL({'logger': FakeLogger(), 'nocheckcertificate': True}) as ydl:\n            r = ydl.urlopen(sanitized_Request(f'https://127.0.0.1:{self.https_port}/headers'))\n            self.assertEqual(r.status, 200)\n            r.close()\n\n    def test_percent_encode(self):\n        with FakeYDL() as ydl:\n            # Unicode characters should be encoded with uppercase percent-encoding\n            res = ydl.urlopen(sanitized_Request(f'http://127.0.0.1:{self.http_port}/\u4e2d\u6587.html'))\n            self.assertEqual(res.status, 200)\n            res.close()\n            # don't normalize existing percent encodings\n            res = ydl.urlopen(sanitized_Request(f'http://127.0.0.1:{self.http_port}/%c7%9f'))\n            self.assertEqual(res.status, 200)\n            res.close()\n\n    def test_unicode_path_redirection(self):\n        with FakeYDL() as ydl:\n            r = ydl.urlopen(sanitized_Request(f'http://127.0.0.1:{self.http_port}/302-non-ascii-redirect'))\n            self.assertEqual(r.url, f'http://127.0.0.1:{self.http_port}/%E4%B8%AD%E6%96%87.html')\n            r.close()\n\n    def test_redirect(self):\n        with FakeYDL() as ydl:\n            def do_req(redirect_status, method):\n                data = b'testdata' if method in ('POST', 'PUT') else None\n                res = ydl.urlopen(sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/redirect_{redirect_status}', method=method, data=data))\n                return res.read().decode('utf-8'), res.headers.get('method', '')\n\n            # A 303 must either use GET or HEAD for subsequent request\n            self.assertEqual(do_req(303, 'POST'), ('', 'GET'))\n            self.assertEqual(do_req(303, 'HEAD'), ('', 'HEAD'))\n\n            self.assertEqual(do_req(303, 'PUT'), ('', 'GET'))\n\n            # 301 and 302 turn POST only into a GET\n            self.assertEqual(do_req(301, 'POST'), ('', 'GET'))\n            self.assertEqual(do_req(301, 'HEAD'), ('', 'HEAD'))\n            self.assertEqual(do_req(302, 'POST'), ('', 'GET'))\n            self.assertEqual(do_req(302, 'HEAD'), ('', 'HEAD'))\n\n            self.assertEqual(do_req(301, 'PUT'), ('testdata', 'PUT'))\n            self.assertEqual(do_req(302, 'PUT'), ('testdata', 'PUT'))\n\n            # 307 and 308 should not change method\n            for m in ('POST', 'PUT'):\n                self.assertEqual(do_req(307, m), ('testdata', m))\n                self.assertEqual(do_req(308, m), ('testdata', m))\n\n            self.assertEqual(do_req(307, 'HEAD'), ('', 'HEAD'))\n            self.assertEqual(do_req(308, 'HEAD'), ('', 'HEAD'))\n\n            # These should not redirect and instead raise an HTTPError\n            for code in (300, 304, 305, 306):\n                with self.assertRaises(urllib.error.HTTPError):\n                    do_req(code, 'GET')\n\n    def test_content_type(self):\n        # https://github.com/yt-dlp/yt-dlp/commit/379a4f161d4ad3e40932dcf5aca6e6fb9715ab28\n        with FakeYDL({'nocheckcertificate': True}) as ydl:\n            # method should be auto-detected as POST\n            r = sanitized_Request(f'https://localhost:{self.https_port}/headers', data=urlencode_postdata({'test': 'test'}))\n\n            headers = ydl.urlopen(r).read().decode('utf-8')\n            self.assertIn('Content-Type: application/x-www-form-urlencoded', headers)\n\n            # test http\n            r = sanitized_Request(f'http://localhost:{self.http_port}/headers', data=urlencode_postdata({'test': 'test'}))\n            headers = ydl.urlopen(r).read().decode('utf-8')\n            self.assertIn('Content-Type: application/x-www-form-urlencoded', headers)\n\n    def test_cookiejar(self):\n        with FakeYDL() as ydl:\n            ydl.cookiejar.set_cookie(http.cookiejar.Cookie(\n                0, 'test', 'ytdlp', None, False, '127.0.0.1', True,\n                False, '/headers', True, False, None, False, None, None, {}))\n            data = ydl.urlopen(sanitized_Request(f'http://127.0.0.1:{self.http_port}/headers')).read()\n            self.assertIn(b'Cookie: test=ytdlp', data)\n\n    def test_no_compression_compat_header(self):\n        with FakeYDL() as ydl:\n            data = ydl.urlopen(\n                sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/headers',\n                    headers={'Youtubedl-no-compression': True})).read()\n            self.assertIn(b'Accept-Encoding: identity', data)\n            self.assertNotIn(b'youtubedl-no-compression', data.lower())\n\n    def test_gzip_trailing_garbage(self):\n        # https://github.com/ytdl-org/youtube-dl/commit/aa3e950764337ef9800c936f4de89b31c00dfcf5\n        # https://github.com/ytdl-org/youtube-dl/commit/6f2ec15cee79d35dba065677cad9da7491ec6e6f\n        with FakeYDL() as ydl:\n            data = ydl.urlopen(sanitized_Request(f'http://localhost:{self.http_port}/trailing_garbage')).read().decode('utf-8')\n            self.assertEqual(data, '<html><video src=\"/vid.mp4\" /></html>')\n\n    @unittest.skipUnless(brotli, 'brotli support is not installed')\n    def test_brotli(self):\n        with FakeYDL() as ydl:\n            res = ydl.urlopen(\n                sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/content-encoding',\n                    headers={'ytdl-encoding': 'br'}))\n            self.assertEqual(res.headers.get('Content-Encoding'), 'br')\n            self.assertEqual(res.read(), b'<html><video src=\"/vid.mp4\" /></html>')\n\n    def test_deflate(self):\n        with FakeYDL() as ydl:\n            res = ydl.urlopen(\n                sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/content-encoding',\n                    headers={'ytdl-encoding': 'deflate'}))\n            self.assertEqual(res.headers.get('Content-Encoding'), 'deflate')\n            self.assertEqual(res.read(), b'<html><video src=\"/vid.mp4\" /></html>')\n\n    def test_gzip(self):\n        with FakeYDL() as ydl:\n            res = ydl.urlopen(\n                sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/content-encoding',\n                    headers={'ytdl-encoding': 'gzip'}))\n            self.assertEqual(res.headers.get('Content-Encoding'), 'gzip')\n            self.assertEqual(res.read(), b'<html><video src=\"/vid.mp4\" /></html>')\n\n    def test_multiple_encodings(self):\n        # https://www.rfc-editor.org/rfc/rfc9110.html#section-8.4\n        with FakeYDL() as ydl:\n            for pair in ('gzip,deflate', 'deflate, gzip', 'gzip, gzip', 'deflate, deflate'):\n                res = ydl.urlopen(\n                    sanitized_Request(\n                        f'http://127.0.0.1:{self.http_port}/content-encoding',\n                        headers={'ytdl-encoding': pair}))\n                self.assertEqual(res.headers.get('Content-Encoding'), pair)\n                self.assertEqual(res.read(), b'<html><video src=\"/vid.mp4\" /></html>')\n\n    def test_unsupported_encoding(self):\n        # it should return the raw content\n        with FakeYDL() as ydl:\n            res = ydl.urlopen(\n                sanitized_Request(\n                    f'http://127.0.0.1:{self.http_port}/content-encoding',\n                    headers={'ytdl-encoding': 'unsupported'}))\n            self.assertEqual(res.headers.get('Content-Encoding'), 'unsupported')\n            self.assertEqual(res.read(), b'raw')\n\n\nclass TestClientCert(unittest.TestCase):\n    def setUp(self):\n        certfn = os.path.join(TEST_DIR, 'testcert.pem')\n        self.certdir = os.path.join(TEST_DIR, 'testdata', 'certificate')\n        cacertfn = os.path.join(self.certdir, 'ca.crt')\n        self.httpd = http.server.HTTPServer(('127.0.0.1', 0), HTTPTestRequestHandler)\n        sslctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n        sslctx.verify_mode = ssl.CERT_REQUIRED\n        sslctx.load_verify_locations(cafile=cacertfn)\n        sslctx.load_cert_chain(certfn, None)\n        self.httpd.socket = sslctx.wrap_socket(self.httpd.socket, server_side=True)\n        self.port = http_server_port(self.httpd)\n        self.server_thread = threading.Thread(target=self.httpd.serve_forever)\n        self.server_thread.daemon = True\n        self.server_thread.start()\n\n    def _run_test(self, **params):\n        ydl = YoutubeDL({\n            'logger': FakeLogger(),\n            # Disable client-side validation of unacceptable self-signed testcert.pem\n            # The test is of a check on the server side, so unaffected\n            'nocheckcertificate': True,\n            **params,\n        })\n        r = ydl.extract_info(f'https://127.0.0.1:{self.port}/video.html')\n        self.assertEqual(r['url'], f'https://127.0.0.1:{self.port}/vid.mp4')\n\n    def test_certificate_combined_nopass(self):\n        self._run_test(client_certificate=os.path.join(self.certdir, 'clientwithkey.crt'))\n\n    def test_certificate_nocombined_nopass(self):\n        self._run_test(client_certificate=os.path.join(self.certdir, 'client.crt'),\n                       client_certificate_key=os.path.join(self.certdir, 'client.key'))\n\n    def test_certificate_combined_pass(self):\n        self._run_test(client_certificate=os.path.join(self.certdir, 'clientwithencryptedkey.crt'),\n                       client_certificate_password='foobar')\n\n    def test_certificate_nocombined_pass(self):\n        self._run_test(client_certificate=os.path.join(self.certdir, 'client.crt'),\n                       client_certificate_key=os.path.join(self.certdir, 'clientencrypted.key'),\n                       client_certificate_password='foobar')\n\n\ndef _build_proxy_handler(name):\n    class HTTPTestRequestHandler(http.server.BaseHTTPRequestHandler):\n        proxy_name = name\n\n        def log_message(self, format, *args):\n            pass\n\n        def do_GET(self):\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/plain; charset=utf-8')\n            self.end_headers()\n            self.wfile.write(f'{self.proxy_name}: {self.path}'.encode())\n    return HTTPTestRequestHandler\n\n\nclass TestProxy(unittest.TestCase):\n    def setUp(self):\n        self.proxy = http.server.HTTPServer(\n            ('127.0.0.1', 0), _build_proxy_handler('normal'))\n        self.port = http_server_port(self.proxy)\n        self.proxy_thread = threading.Thread(target=self.proxy.serve_forever)\n        self.proxy_thread.daemon = True\n        self.proxy_thread.start()\n\n        self.geo_proxy = http.server.HTTPServer(\n            ('127.0.0.1', 0), _build_proxy_handler('geo'))\n        self.geo_port = http_server_port(self.geo_proxy)\n        self.geo_proxy_thread = threading.Thread(target=self.geo_proxy.serve_forever)\n        self.geo_proxy_thread.daemon = True\n        self.geo_proxy_thread.start()\n\n    def test_proxy(self):\n        geo_proxy = f'127.0.0.1:{self.geo_port}'\n        ydl = YoutubeDL({\n            'proxy': f'127.0.0.1:{self.port}',\n            'geo_verification_proxy': geo_proxy,\n        })\n        url = 'http://foo.com/bar'\n        response = ydl.urlopen(url).read().decode()\n        self.assertEqual(response, f'normal: {url}')\n\n        req = urllib.request.Request(url)\n        req.add_header('Ytdl-request-proxy', geo_proxy)\n        response = ydl.urlopen(req).read().decode()\n        self.assertEqual(response, f'geo: {url}')\n\n    def test_proxy_with_idn(self):\n        ydl = YoutubeDL({\n            'proxy': f'127.0.0.1:{self.port}',\n        })\n        url = 'http://\u4e2d\u6587.tw/'\n        response = ydl.urlopen(url).read().decode()\n        # b'xn--fiq228c' is '\u4e2d\u6587'.encode('idna')\n        self.assertEqual(response, 'normal: http://xn--fiq228c.tw/')\n\n\nclass TestFileURL(unittest.TestCase):\n    # See https://github.com/ytdl-org/youtube-dl/issues/8227\n    def test_file_urls(self):\n        tf = tempfile.NamedTemporaryFile(delete=False)\n        tf.write(b'foobar')\n        tf.close()\n        url = pathlib.Path(tf.name).as_uri()\n        with FakeYDL() as ydl:\n            self.assertRaisesRegex(\n                urllib.error.URLError, 'file:// URLs are explicitly disabled in yt-dlp for security reasons', ydl.urlopen, url)\n        with FakeYDL({'enable_file_urls': True}) as ydl:\n            res = ydl.urlopen(url)\n            self.assertEqual(res.read(), b'foobar')\n            res.close()\n        os.unlink(tf.name)\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "patch": "@@ -132,6 +132,11 @@ def do_GET(self):\n             self._method('GET')\n         elif self.path.startswith('/headers'):\n             self._headers()\n+        elif self.path.startswith('/308-to-headers'):\n+            self.send_response(308)\n+            self.send_header('Location', '/headers')\n+            self.send_header('Content-Length', '0')\n+            self.end_headers()\n         elif self.path == '/trailing_garbage':\n             payload = b'<html><video src=\"/vid.mp4\" /></html>'\n             self.send_response(200)\n@@ -270,6 +275,7 @@ def do_req(redirect_status, method):\n             self.assertEqual(do_req(303, 'PUT'), ('', 'GET'))\n \n             # 301 and 302 turn POST only into a GET\n+            # XXX: we should also test if the Content-Type and Content-Length headers are removed\n             self.assertEqual(do_req(301, 'POST'), ('', 'GET'))\n             self.assertEqual(do_req(301, 'HEAD'), ('', 'HEAD'))\n             self.assertEqual(do_req(302, 'POST'), ('', 'GET'))\n@@ -313,6 +319,31 @@ def test_cookiejar(self):\n             data = ydl.urlopen(sanitized_Request(f'http://127.0.0.1:{self.http_port}/headers')).read()\n             self.assertIn(b'Cookie: test=ytdlp', data)\n \n+    def test_passed_cookie_header(self):\n+        # We should accept a Cookie header being passed as in normal headers and handle it appropriately.\n+        with FakeYDL() as ydl:\n+            # Specified Cookie header should be used\n+            res = ydl.urlopen(\n+                sanitized_Request(f'http://127.0.0.1:{self.http_port}/headers',\n+                                  headers={'Cookie': 'test=test'})).read().decode('utf-8')\n+            self.assertIn('Cookie: test=test', res)\n+\n+            # Specified Cookie header should be removed on any redirect\n+            res = ydl.urlopen(\n+                sanitized_Request(f'http://127.0.0.1:{self.http_port}/308-to-headers', headers={'Cookie': 'test=test'})).read().decode('utf-8')\n+            self.assertNotIn('Cookie: test=test', res)\n+\n+            # Specified Cookie header should override global cookiejar for that request\n+            ydl.cookiejar.set_cookie(http.cookiejar.Cookie(\n+                version=0, name='test', value='ytdlp', port=None, port_specified=False,\n+                domain='127.0.0.1', domain_specified=True, domain_initial_dot=False, path='/',\n+                path_specified=True, secure=False, expires=None, discard=False, comment=None,\n+                comment_url=None, rest={}))\n+\n+            data = ydl.urlopen(sanitized_Request(f'http://127.0.0.1:{self.http_port}/headers', headers={'Cookie': 'test=test'})).read()\n+            self.assertNotIn(b'Cookie: test=ytdlp', data)\n+            self.assertIn(b'Cookie: test=test', data)\n+\n     def test_no_compression_compat_header(self):\n         with FakeYDL() as ydl:\n             data = ydl.urlopen(", "file_path": "files/2023_7/629", "file_language": "py", "file_name": "test/test_http.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/yt-dlp/yt-dlp/raw/f8b4bcc0a791274223723488bfbfc23ea3276641/yt_dlp%2Futils%2F_utils.py", "code": "import asyncio\nimport atexit\nimport base64\nimport binascii\nimport calendar\nimport codecs\nimport collections\nimport collections.abc\nimport contextlib\nimport datetime\nimport email.header\nimport email.utils\nimport errno\nimport gzip\nimport hashlib\nimport hmac\nimport html.entities\nimport html.parser\nimport http.client\nimport http.cookiejar\nimport inspect\nimport io\nimport itertools\nimport json\nimport locale\nimport math\nimport mimetypes\nimport netrc\nimport operator\nimport os\nimport platform\nimport random\nimport re\nimport shlex\nimport socket\nimport ssl\nimport struct\nimport subprocess\nimport sys\nimport tempfile\nimport time\nimport traceback\nimport types\nimport unicodedata\nimport urllib.error\nimport urllib.parse\nimport urllib.request\nimport xml.etree.ElementTree\nimport zlib\n\nfrom . import traversal\n\nfrom ..compat import functools  # isort: split\nfrom ..compat import (\n    compat_etree_fromstring,\n    compat_expanduser,\n    compat_HTMLParseError,\n    compat_os_name,\n    compat_shlex_quote,\n)\nfrom ..dependencies import brotli, certifi, websockets, xattr\nfrom ..socks import ProxyType, sockssocket\n\n__name__ = __name__.rsplit('.', 1)[0]  # Pretend to be the parent module\n\n# This is not clearly defined otherwise\ncompiled_regex_type = type(re.compile(''))\n\n\ndef random_user_agent():\n    _USER_AGENT_TPL = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/%s Safari/537.36'\n    _CHROME_VERSIONS = (\n        '90.0.4430.212',\n        '90.0.4430.24',\n        '90.0.4430.70',\n        '90.0.4430.72',\n        '90.0.4430.85',\n        '90.0.4430.93',\n        '91.0.4472.101',\n        '91.0.4472.106',\n        '91.0.4472.114',\n        '91.0.4472.124',\n        '91.0.4472.164',\n        '91.0.4472.19',\n        '91.0.4472.77',\n        '92.0.4515.107',\n        '92.0.4515.115',\n        '92.0.4515.131',\n        '92.0.4515.159',\n        '92.0.4515.43',\n        '93.0.4556.0',\n        '93.0.4577.15',\n        '93.0.4577.63',\n        '93.0.4577.82',\n        '94.0.4606.41',\n        '94.0.4606.54',\n        '94.0.4606.61',\n        '94.0.4606.71',\n        '94.0.4606.81',\n        '94.0.4606.85',\n        '95.0.4638.17',\n        '95.0.4638.50',\n        '95.0.4638.54',\n        '95.0.4638.69',\n        '95.0.4638.74',\n        '96.0.4664.18',\n        '96.0.4664.45',\n        '96.0.4664.55',\n        '96.0.4664.93',\n        '97.0.4692.20',\n    )\n    return _USER_AGENT_TPL % random.choice(_CHROME_VERSIONS)\n\n\nSUPPORTED_ENCODINGS = [\n    'gzip', 'deflate'\n]\nif brotli:\n    SUPPORTED_ENCODINGS.append('br')\n\nstd_headers = {\n    'User-Agent': random_user_agent(),\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en-us,en;q=0.5',\n    'Sec-Fetch-Mode': 'navigate',\n}\n\n\nUSER_AGENTS = {\n    'Safari': 'Mozilla/5.0 (X11; Linux x86_64; rv:10.0) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27',\n}\n\n\nclass NO_DEFAULT:\n    pass\n\n\ndef IDENTITY(x):\n    return x\n\n\nENGLISH_MONTH_NAMES = [\n    'January', 'February', 'March', 'April', 'May', 'June',\n    'July', 'August', 'September', 'October', 'November', 'December']\n\nMONTH_NAMES = {\n    'en': ENGLISH_MONTH_NAMES,\n    'fr': [\n        'janvier', 'f\u00e9vrier', 'mars', 'avril', 'mai', 'juin',\n        'juillet', 'ao\u00fbt', 'septembre', 'octobre', 'novembre', 'd\u00e9cembre'],\n    # these follow the genitive grammatical case (dope\u0142niacz)\n    # some websites might be using nominative, which will require another month list\n    # https://en.wikibooks.org/wiki/Polish/Noun_cases\n    'pl': ['stycznia', 'lutego', 'marca', 'kwietnia', 'maja', 'czerwca',\n           'lipca', 'sierpnia', 'wrze\u015bnia', 'pa\u017adziernika', 'listopada', 'grudnia'],\n}\n\n# From https://github.com/python/cpython/blob/3.11/Lib/email/_parseaddr.py#L36-L42\nTIMEZONE_NAMES = {\n    'UT': 0, 'UTC': 0, 'GMT': 0, 'Z': 0,\n    'AST': -4, 'ADT': -3,  # Atlantic (used in Canada)\n    'EST': -5, 'EDT': -4,  # Eastern\n    'CST': -6, 'CDT': -5,  # Central\n    'MST': -7, 'MDT': -6,  # Mountain\n    'PST': -8, 'PDT': -7   # Pacific\n}\n\n# needed for sanitizing filenames in restricted mode\nACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',\n                        itertools.chain('AAAAAA', ['AE'], 'CEEEEIIIIDNOOOOOOO', ['OE'], 'UUUUUY', ['TH', 'ss'],\n                                        'aaaaaa', ['ae'], 'ceeeeiiiionooooooo', ['oe'], 'uuuuuy', ['th'], 'y')))\n\nDATE_FORMATS = (\n    '%d %B %Y',\n    '%d %b %Y',\n    '%B %d %Y',\n    '%B %dst %Y',\n    '%B %dnd %Y',\n    '%B %drd %Y',\n    '%B %dth %Y',\n    '%b %d %Y',\n    '%b %dst %Y',\n    '%b %dnd %Y',\n    '%b %drd %Y',\n    '%b %dth %Y',\n    '%b %dst %Y %I:%M',\n    '%b %dnd %Y %I:%M',\n    '%b %drd %Y %I:%M',\n    '%b %dth %Y %I:%M',\n    '%Y %m %d',\n    '%Y-%m-%d',\n    '%Y.%m.%d.',\n    '%Y/%m/%d',\n    '%Y/%m/%d %H:%M',\n    '%Y/%m/%d %H:%M:%S',\n    '%Y%m%d%H%M',\n    '%Y%m%d%H%M%S',\n    '%Y%m%d',\n    '%Y-%m-%d %H:%M',\n    '%Y-%m-%d %H:%M:%S',\n    '%Y-%m-%d %H:%M:%S.%f',\n    '%Y-%m-%d %H:%M:%S:%f',\n    '%d.%m.%Y %H:%M',\n    '%d.%m.%Y %H.%M',\n    '%Y-%m-%dT%H:%M:%SZ',\n    '%Y-%m-%dT%H:%M:%S.%fZ',\n    '%Y-%m-%dT%H:%M:%S.%f0Z',\n    '%Y-%m-%dT%H:%M:%S',\n    '%Y-%m-%dT%H:%M:%S.%f',\n    '%Y-%m-%dT%H:%M',\n    '%b %d %Y at %H:%M',\n    '%b %d %Y at %H:%M:%S',\n    '%B %d %Y at %H:%M',\n    '%B %d %Y at %H:%M:%S',\n    '%H:%M %d-%b-%Y',\n)\n\nDATE_FORMATS_DAY_FIRST = list(DATE_FORMATS)\nDATE_FORMATS_DAY_FIRST.extend([\n    '%d-%m-%Y',\n    '%d.%m.%Y',\n    '%d.%m.%y',\n    '%d/%m/%Y',\n    '%d/%m/%y',\n    '%d/%m/%Y %H:%M:%S',\n    '%d-%m-%Y %H:%M',\n    '%H:%M %d/%m/%Y',\n])\n\nDATE_FORMATS_MONTH_FIRST = list(DATE_FORMATS)\nDATE_FORMATS_MONTH_FIRST.extend([\n    '%m-%d-%Y',\n    '%m.%d.%Y',\n    '%m/%d/%Y',\n    '%m/%d/%y',\n    '%m/%d/%Y %H:%M:%S',\n])\n\nPACKED_CODES_RE = r\"}\\('(.+)',(\\d+),(\\d+),'([^']+)'\\.split\\('\\|'\\)\"\nJSON_LD_RE = r'(?is)<script[^>]+type=([\"\\']?)application/ld\\+json\\1[^>]*>\\s*(?P<json_ld>{.+?}|\\[.+?\\])\\s*</script>'\n\nNUMBER_RE = r'\\d+(?:\\.\\d+)?'\n\n\n@functools.cache\ndef preferredencoding():\n    \"\"\"Get preferred encoding.\n\n    Returns the best encoding scheme for the system, based on\n    locale.getpreferredencoding() and some further tweaks.\n    \"\"\"\n    try:\n        pref = locale.getpreferredencoding()\n        'TEST'.encode(pref)\n    except Exception:\n        pref = 'UTF-8'\n\n    return pref\n\n\ndef write_json_file(obj, fn):\n    \"\"\" Encode obj as JSON and write it to fn, atomically if possible \"\"\"\n\n    tf = tempfile.NamedTemporaryFile(\n        prefix=f'{os.path.basename(fn)}.', dir=os.path.dirname(fn),\n        suffix='.tmp', delete=False, mode='w', encoding='utf-8')\n\n    try:\n        with tf:\n            json.dump(obj, tf, ensure_ascii=False)\n        if sys.platform == 'win32':\n            # Need to remove existing file on Windows, else os.rename raises\n            # WindowsError or FileExistsError.\n            with contextlib.suppress(OSError):\n                os.unlink(fn)\n        with contextlib.suppress(OSError):\n            mask = os.umask(0)\n            os.umask(mask)\n            os.chmod(tf.name, 0o666 & ~mask)\n        os.rename(tf.name, fn)\n    except Exception:\n        with contextlib.suppress(OSError):\n            os.remove(tf.name)\n        raise\n\n\ndef find_xpath_attr(node, xpath, key, val=None):\n    \"\"\" Find the xpath xpath[@key=val] \"\"\"\n    assert re.match(r'^[a-zA-Z_-]+$', key)\n    expr = xpath + ('[@%s]' % key if val is None else f\"[@{key}='{val}']\")\n    return node.find(expr)\n\n# On python2.6 the xml.etree.ElementTree.Element methods don't support\n# the namespace parameter\n\n\ndef xpath_with_ns(path, ns_map):\n    components = [c.split(':') for c in path.split('/')]\n    replaced = []\n    for c in components:\n        if len(c) == 1:\n            replaced.append(c[0])\n        else:\n            ns, tag = c\n            replaced.append('{%s}%s' % (ns_map[ns], tag))\n    return '/'.join(replaced)\n\n\ndef xpath_element(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    def _find_xpath(xpath):\n        return node.find(xpath)\n\n    if isinstance(xpath, str):\n        n = _find_xpath(xpath)\n    else:\n        for xp in xpath:\n            n = _find_xpath(xp)\n            if n is not None:\n                break\n\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element %s' % name)\n        else:\n            return None\n    return n\n\n\ndef xpath_text(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    n = xpath_element(node, xpath, name, fatal=fatal, default=default)\n    if n is None or n == default:\n        return n\n    if n.text is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element\\'s text %s' % name)\n        else:\n            return None\n    return n.text\n\n\ndef xpath_attr(node, xpath, key, name=None, fatal=False, default=NO_DEFAULT):\n    n = find_xpath_attr(node, xpath, key)\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = f'{xpath}[@{key}]' if name is None else name\n            raise ExtractorError('Could not find XML attribute %s' % name)\n        else:\n            return None\n    return n.attrib[key]\n\n\ndef get_element_by_id(id, html, **kwargs):\n    \"\"\"Return the content of the tag with the specified ID in the passed HTML document\"\"\"\n    return get_element_by_attribute('id', id, html, **kwargs)\n\n\ndef get_element_html_by_id(id, html, **kwargs):\n    \"\"\"Return the html of the tag with the specified ID in the passed HTML document\"\"\"\n    return get_element_html_by_attribute('id', id, html, **kwargs)\n\n\ndef get_element_by_class(class_name, html):\n    \"\"\"Return the content of the first tag with the specified class in the passed HTML document\"\"\"\n    retval = get_elements_by_class(class_name, html)\n    return retval[0] if retval else None\n\n\ndef get_element_html_by_class(class_name, html):\n    \"\"\"Return the html of the first tag with the specified class in the passed HTML document\"\"\"\n    retval = get_elements_html_by_class(class_name, html)\n    return retval[0] if retval else None\n\n\ndef get_element_by_attribute(attribute, value, html, **kwargs):\n    retval = get_elements_by_attribute(attribute, value, html, **kwargs)\n    return retval[0] if retval else None\n\n\ndef get_element_html_by_attribute(attribute, value, html, **kargs):\n    retval = get_elements_html_by_attribute(attribute, value, html, **kargs)\n    return retval[0] if retval else None\n\n\ndef get_elements_by_class(class_name, html, **kargs):\n    \"\"\"Return the content of all tags with the specified class in the passed HTML document as a list\"\"\"\n    return get_elements_by_attribute(\n        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),\n        html, escape_value=False)\n\n\ndef get_elements_html_by_class(class_name, html):\n    \"\"\"Return the html of all tags with the specified class in the passed HTML document as a list\"\"\"\n    return get_elements_html_by_attribute(\n        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),\n        html, escape_value=False)\n\n\ndef get_elements_by_attribute(*args, **kwargs):\n    \"\"\"Return the content of the tag with the specified attribute in the passed HTML document\"\"\"\n    return [content for content, _ in get_elements_text_and_html_by_attribute(*args, **kwargs)]\n\n\ndef get_elements_html_by_attribute(*args, **kwargs):\n    \"\"\"Return the html of the tag with the specified attribute in the passed HTML document\"\"\"\n    return [whole for _, whole in get_elements_text_and_html_by_attribute(*args, **kwargs)]\n\n\ndef get_elements_text_and_html_by_attribute(attribute, value, html, *, tag=r'[\\w:.-]+', escape_value=True):\n    \"\"\"\n    Return the text (content) and the html (whole) of the tag with the specified\n    attribute in the passed HTML document\n    \"\"\"\n    if not value:\n        return\n\n    quote = '' if re.match(r'''[\\s\"'`=<>]''', value) else '?'\n\n    value = re.escape(value) if escape_value else value\n\n    partial_element_re = rf'''(?x)\n        <(?P<tag>{tag})\n         (?:\\s(?:[^>\"']|\"[^\"]*\"|'[^']*')*)?\n         \\s{re.escape(attribute)}\\s*=\\s*(?P<_q>['\"]{quote})(?-x:{value})(?P=_q)\n        '''\n\n    for m in re.finditer(partial_element_re, html):\n        content, whole = get_element_text_and_html_by_tag(m.group('tag'), html[m.start():])\n\n        yield (\n            unescapeHTML(re.sub(r'^(?P<q>[\"\\'])(?P<content>.*)(?P=q)$', r'\\g<content>', content, flags=re.DOTALL)),\n            whole\n        )\n\n\nclass HTMLBreakOnClosingTagParser(html.parser.HTMLParser):\n    \"\"\"\n    HTML parser which raises HTMLBreakOnClosingTagException upon reaching the\n    closing tag for the first opening tag it has encountered, and can be used\n    as a context manager\n    \"\"\"\n\n    class HTMLBreakOnClosingTagException(Exception):\n        pass\n\n    def __init__(self):\n        self.tagstack = collections.deque()\n        html.parser.HTMLParser.__init__(self)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *_):\n        self.close()\n\n    def close(self):\n        # handle_endtag does not return upon raising HTMLBreakOnClosingTagException,\n        # so data remains buffered; we no longer have any interest in it, thus\n        # override this method to discard it\n        pass\n\n    def handle_starttag(self, tag, _):\n        self.tagstack.append(tag)\n\n    def handle_endtag(self, tag):\n        if not self.tagstack:\n            raise compat_HTMLParseError('no tags in the stack')\n        while self.tagstack:\n            inner_tag = self.tagstack.pop()\n            if inner_tag == tag:\n                break\n        else:\n            raise compat_HTMLParseError(f'matching opening tag for closing {tag} tag not found')\n        if not self.tagstack:\n            raise self.HTMLBreakOnClosingTagException()\n\n\n# XXX: This should be far less strict\ndef get_element_text_and_html_by_tag(tag, html):\n    \"\"\"\n    For the first element with the specified tag in the passed HTML document\n    return its' content (text) and the whole element (html)\n    \"\"\"\n    def find_or_raise(haystack, needle, exc):\n        try:\n            return haystack.index(needle)\n        except ValueError:\n            raise exc\n    closing_tag = f'</{tag}>'\n    whole_start = find_or_raise(\n        html, f'<{tag}', compat_HTMLParseError(f'opening {tag} tag not found'))\n    content_start = find_or_raise(\n        html[whole_start:], '>', compat_HTMLParseError(f'malformed opening {tag} tag'))\n    content_start += whole_start + 1\n    with HTMLBreakOnClosingTagParser() as parser:\n        parser.feed(html[whole_start:content_start])\n        if not parser.tagstack or parser.tagstack[0] != tag:\n            raise compat_HTMLParseError(f'parser did not match opening {tag} tag')\n        offset = content_start\n        while offset < len(html):\n            next_closing_tag_start = find_or_raise(\n                html[offset:], closing_tag,\n                compat_HTMLParseError(f'closing {tag} tag not found'))\n            next_closing_tag_end = next_closing_tag_start + len(closing_tag)\n            try:\n                parser.feed(html[offset:offset + next_closing_tag_end])\n                offset += next_closing_tag_end\n            except HTMLBreakOnClosingTagParser.HTMLBreakOnClosingTagException:\n                return html[content_start:offset + next_closing_tag_start], \\\n                    html[whole_start:offset + next_closing_tag_end]\n        raise compat_HTMLParseError('unexpected end of html')\n\n\nclass HTMLAttributeParser(html.parser.HTMLParser):\n    \"\"\"Trivial HTML parser to gather the attributes for a single element\"\"\"\n\n    def __init__(self):\n        self.attrs = {}\n        html.parser.HTMLParser.__init__(self)\n\n    def handle_starttag(self, tag, attrs):\n        self.attrs = dict(attrs)\n        raise compat_HTMLParseError('done')\n\n\nclass HTMLListAttrsParser(html.parser.HTMLParser):\n    \"\"\"HTML parser to gather the attributes for the elements of a list\"\"\"\n\n    def __init__(self):\n        html.parser.HTMLParser.__init__(self)\n        self.items = []\n        self._level = 0\n\n    def handle_starttag(self, tag, attrs):\n        if tag == 'li' and self._level == 0:\n            self.items.append(dict(attrs))\n        self._level += 1\n\n    def handle_endtag(self, tag):\n        self._level -= 1\n\n\ndef extract_attributes(html_element):\n    \"\"\"Given a string for an HTML element such as\n    <el\n         a=\"foo\" B=\"bar\" c=\"&98;az\" d=boz\n         empty= noval entity=\"&amp;\"\n         sq='\"' dq=\"'\"\n    >\n    Decode and return a dictionary of attributes.\n    {\n        'a': 'foo', 'b': 'bar', c: 'baz', d: 'boz',\n        'empty': '', 'noval': None, 'entity': '&',\n        'sq': '\"', 'dq': '\\''\n    }.\n    \"\"\"\n    parser = HTMLAttributeParser()\n    with contextlib.suppress(compat_HTMLParseError):\n        parser.feed(html_element)\n        parser.close()\n    return parser.attrs\n\n\ndef parse_list(webpage):\n    \"\"\"Given a string for an series of HTML <li> elements,\n    return a dictionary of their attributes\"\"\"\n    parser = HTMLListAttrsParser()\n    parser.feed(webpage)\n    parser.close()\n    return parser.items\n\n\ndef clean_html(html):\n    \"\"\"Clean an HTML snippet into a readable string\"\"\"\n\n    if html is None:  # Convenience for sanitizing descriptions etc.\n        return html\n\n    html = re.sub(r'\\s+', ' ', html)\n    html = re.sub(r'(?u)\\s?<\\s?br\\s?/?\\s?>\\s?', '\\n', html)\n    html = re.sub(r'(?u)<\\s?/\\s?p\\s?>\\s?<\\s?p[^>]*>', '\\n', html)\n    # Strip html tags\n    html = re.sub('<.*?>', '', html)\n    # Replace html entities\n    html = unescapeHTML(html)\n    return html.strip()\n\n\nclass LenientJSONDecoder(json.JSONDecoder):\n    # TODO: Write tests\n    def __init__(self, *args, transform_source=None, ignore_extra=False, close_objects=0, **kwargs):\n        self.transform_source, self.ignore_extra = transform_source, ignore_extra\n        self._close_attempts = 2 * close_objects\n        super().__init__(*args, **kwargs)\n\n    @staticmethod\n    def _close_object(err):\n        doc = err.doc[:err.pos]\n        # We need to add comma first to get the correct error message\n        if err.msg.startswith('Expecting \\',\\''):\n            return doc + ','\n        elif not doc.endswith(','):\n            return\n\n        if err.msg.startswith('Expecting property name'):\n            return doc[:-1] + '}'\n        elif err.msg.startswith('Expecting value'):\n            return doc[:-1] + ']'\n\n    def decode(self, s):\n        if self.transform_source:\n            s = self.transform_source(s)\n        for attempt in range(self._close_attempts + 1):\n            try:\n                if self.ignore_extra:\n                    return self.raw_decode(s.lstrip())[0]\n                return super().decode(s)\n            except json.JSONDecodeError as e:\n                if e.pos is None:\n                    raise\n                elif attempt < self._close_attempts:\n                    s = self._close_object(e)\n                    if s is not None:\n                        continue\n                raise type(e)(f'{e.msg} in {s[e.pos-10:e.pos+10]!r}', s, e.pos)\n        assert False, 'Too many attempts to decode JSON'\n\n\ndef sanitize_open(filename, open_mode):\n    \"\"\"Try to open the given filename, and slightly tweak it if this fails.\n\n    Attempts to open the given filename. If this fails, it tries to change\n    the filename slightly, step by step, until it's either able to open it\n    or it fails and raises a final exception, like the standard open()\n    function.\n\n    It returns the tuple (stream, definitive_file_name).\n    \"\"\"\n    if filename == '-':\n        if sys.platform == 'win32':\n            import msvcrt\n\n            # stdout may be any IO stream, e.g. when using contextlib.redirect_stdout\n            with contextlib.suppress(io.UnsupportedOperation):\n                msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n        return (sys.stdout.buffer if hasattr(sys.stdout, 'buffer') else sys.stdout, filename)\n\n    for attempt in range(2):\n        try:\n            try:\n                if sys.platform == 'win32':\n                    # FIXME: An exclusive lock also locks the file from being read.\n                    # Since windows locks are mandatory, don't lock the file on windows (for now).\n                    # Ref: https://github.com/yt-dlp/yt-dlp/issues/3124\n                    raise LockingUnsupportedError()\n                stream = locked_file(filename, open_mode, block=False).__enter__()\n            except OSError:\n                stream = open(filename, open_mode)\n            return stream, filename\n        except OSError as err:\n            if attempt or err.errno in (errno.EACCES,):\n                raise\n            old_filename, filename = filename, sanitize_path(filename)\n            if old_filename == filename:\n                raise\n\n\ndef timeconvert(timestr):\n    \"\"\"Convert RFC 2822 defined time string into system timestamp\"\"\"\n    timestamp = None\n    timetuple = email.utils.parsedate_tz(timestr)\n    if timetuple is not None:\n        timestamp = email.utils.mktime_tz(timetuple)\n    return timestamp\n\n\ndef sanitize_filename(s, restricted=False, is_id=NO_DEFAULT):\n    \"\"\"Sanitizes a string so it could be used as part of a filename.\n    @param restricted   Use a stricter subset of allowed characters\n    @param is_id        Whether this is an ID that should be kept unchanged if possible.\n                        If unset, yt-dlp's new sanitization rules are in effect\n    \"\"\"\n    if s == '':\n        return ''\n\n    def replace_insane(char):\n        if restricted and char in ACCENT_CHARS:\n            return ACCENT_CHARS[char]\n        elif not restricted and char == '\\n':\n            return '\\0 '\n        elif is_id is NO_DEFAULT and not restricted and char in '\"*:<>?|/\\\\':\n            # Replace with their full-width unicode counterparts\n            return {'/': '\\u29F8', '\\\\': '\\u29f9'}.get(char, chr(ord(char) + 0xfee0))\n        elif char == '?' or ord(char) < 32 or ord(char) == 127:\n            return ''\n        elif char == '\"':\n            return '' if restricted else '\\''\n        elif char == ':':\n            return '\\0_\\0-' if restricted else '\\0 \\0-'\n        elif char in '\\\\/|*<>':\n            return '\\0_'\n        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace() or ord(char) > 127):\n            return '\\0_'\n        return char\n\n    # Replace look-alike Unicode glyphs\n    if restricted and (is_id is NO_DEFAULT or not is_id):\n        s = unicodedata.normalize('NFKC', s)\n    s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s)  # Handle timestamps\n    result = ''.join(map(replace_insane, s))\n    if is_id is NO_DEFAULT:\n        result = re.sub(r'(\\0.)(?:(?=\\1)..)+', r'\\1', result)  # Remove repeated substitute chars\n        STRIP_RE = r'(?:\\0.|[ _-])*'\n        result = re.sub(f'^\\0.{STRIP_RE}|{STRIP_RE}\\0.$', '', result)  # Remove substitute chars from start/end\n    result = result.replace('\\0', '') or '_'\n\n    if not is_id:\n        while '__' in result:\n            result = result.replace('__', '_')\n        result = result.strip('_')\n        # Common case of \"Foreign band name - English song title\"\n        if restricted and result.startswith('-_'):\n            result = result[2:]\n        if result.startswith('-'):\n            result = '_' + result[len('-'):]\n        result = result.lstrip('.')\n        if not result:\n            result = '_'\n    return result\n\n\ndef sanitize_path(s, force=False):\n    \"\"\"Sanitizes and normalizes path on Windows\"\"\"\n    if sys.platform == 'win32':\n        force = False\n        drive_or_unc, _ = os.path.splitdrive(s)\n    elif force:\n        drive_or_unc = ''\n    else:\n        return s\n\n    norm_path = os.path.normpath(remove_start(s, drive_or_unc)).split(os.path.sep)\n    if drive_or_unc:\n        norm_path.pop(0)\n    sanitized_path = [\n        path_part if path_part in ['.', '..'] else re.sub(r'(?:[/<>:\"\\|\\\\?\\*]|[\\s.]$)', '#', path_part)\n        for path_part in norm_path]\n    if drive_or_unc:\n        sanitized_path.insert(0, drive_or_unc + os.path.sep)\n    elif force and s and s[0] == os.path.sep:\n        sanitized_path.insert(0, os.path.sep)\n    return os.path.join(*sanitized_path)\n\n\ndef sanitize_url(url, *, scheme='http'):\n    # Prepend protocol-less URLs with `http:` scheme in order to mitigate\n    # the number of unwanted failures due to missing protocol\n    if url is None:\n        return\n    elif url.startswith('//'):\n        return f'{scheme}:{url}'\n    # Fix some common typos seen so far\n    COMMON_TYPOS = (\n        # https://github.com/ytdl-org/youtube-dl/issues/15649\n        (r'^httpss://', r'https://'),\n        # https://bx1.be/lives/direct-tv/\n        (r'^rmtp([es]?)://', r'rtmp\\1://'),\n    )\n    for mistake, fixup in COMMON_TYPOS:\n        if re.match(mistake, url):\n            return re.sub(mistake, fixup, url)\n    return url\n\n\ndef extract_basic_auth(url):\n    parts = urllib.parse.urlsplit(url)\n    if parts.username is None:\n        return url, None\n    url = urllib.parse.urlunsplit(parts._replace(netloc=(\n        parts.hostname if parts.port is None\n        else '%s:%d' % (parts.hostname, parts.port))))\n    auth_payload = base64.b64encode(\n        ('%s:%s' % (parts.username, parts.password or '')).encode())\n    return url, f'Basic {auth_payload.decode()}'\n\n\ndef sanitized_Request(url, *args, **kwargs):\n    url, auth_header = extract_basic_auth(escape_url(sanitize_url(url)))\n    if auth_header is not None:\n        headers = args[1] if len(args) >= 2 else kwargs.setdefault('headers', {})\n        headers['Authorization'] = auth_header\n    return urllib.request.Request(url, *args, **kwargs)\n\n\ndef expand_path(s):\n    \"\"\"Expand shell variables and ~\"\"\"\n    return os.path.expandvars(compat_expanduser(s))\n\n\ndef orderedSet(iterable, *, lazy=False):\n    \"\"\"Remove all duplicates from the input iterable\"\"\"\n    def _iter():\n        seen = []  # Do not use set since the items can be unhashable\n        for x in iterable:\n            if x not in seen:\n                seen.append(x)\n                yield x\n\n    return _iter() if lazy else list(_iter())\n\n\ndef _htmlentity_transform(entity_with_semicolon):\n    \"\"\"Transforms an HTML entity to a character.\"\"\"\n    entity = entity_with_semicolon[:-1]\n\n    # Known non-numeric HTML entity\n    if entity in html.entities.name2codepoint:\n        return chr(html.entities.name2codepoint[entity])\n\n    # TODO: HTML5 allows entities without a semicolon.\n    # E.g. '&Eacuteric' should be decoded as '\u00c9ric'.\n    if entity_with_semicolon in html.entities.html5:\n        return html.entities.html5[entity_with_semicolon]\n\n    mobj = re.match(r'#(x[0-9a-fA-F]+|[0-9]+)', entity)\n    if mobj is not None:\n        numstr = mobj.group(1)\n        if numstr.startswith('x'):\n            base = 16\n            numstr = '0%s' % numstr\n        else:\n            base = 10\n        # See https://github.com/ytdl-org/youtube-dl/issues/7518\n        with contextlib.suppress(ValueError):\n            return chr(int(numstr, base))\n\n    # Unknown entity in name, return its literal representation\n    return '&%s;' % entity\n\n\ndef unescapeHTML(s):\n    if s is None:\n        return None\n    assert isinstance(s, str)\n\n    return re.sub(\n        r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)\n\n\ndef escapeHTML(text):\n    return (\n        text\n        .replace('&', '&amp;')\n        .replace('<', '&lt;')\n        .replace('>', '&gt;')\n        .replace('\"', '&quot;')\n        .replace(\"'\", '&#39;')\n    )\n\n\nclass netrc_from_content(netrc.netrc):\n    def __init__(self, content):\n        self.hosts, self.macros = {}, {}\n        with io.StringIO(content) as stream:\n            self._parse('-', stream, False)\n\n\nclass Popen(subprocess.Popen):\n    if sys.platform == 'win32':\n        _startupinfo = subprocess.STARTUPINFO()\n        _startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n    else:\n        _startupinfo = None\n\n    @staticmethod\n    def _fix_pyinstaller_ld_path(env):\n        \"\"\"Restore LD_LIBRARY_PATH when using PyInstaller\n            Ref: https://github.com/pyinstaller/pyinstaller/blob/develop/doc/runtime-information.rst#ld_library_path--libpath-considerations\n                 https://github.com/yt-dlp/yt-dlp/issues/4573\n        \"\"\"\n        if not hasattr(sys, '_MEIPASS'):\n            return\n\n        def _fix(key):\n            orig = env.get(f'{key}_ORIG')\n            if orig is None:\n                env.pop(key, None)\n            else:\n                env[key] = orig\n\n        _fix('LD_LIBRARY_PATH')  # Linux\n        _fix('DYLD_LIBRARY_PATH')  # macOS\n\n    def __init__(self, *args, env=None, text=False, **kwargs):\n        if env is None:\n            env = os.environ.copy()\n        self._fix_pyinstaller_ld_path(env)\n\n        self.__text_mode = kwargs.get('encoding') or kwargs.get('errors') or text or kwargs.get('universal_newlines')\n        if text is True:\n            kwargs['universal_newlines'] = True  # For 3.6 compatibility\n            kwargs.setdefault('encoding', 'utf-8')\n            kwargs.setdefault('errors', 'replace')\n        super().__init__(*args, env=env, **kwargs, startupinfo=self._startupinfo)\n\n    def communicate_or_kill(self, *args, **kwargs):\n        try:\n            return self.communicate(*args, **kwargs)\n        except BaseException:  # Including KeyboardInterrupt\n            self.kill(timeout=None)\n            raise\n\n    def kill(self, *, timeout=0):\n        super().kill()\n        if timeout != 0:\n            self.wait(timeout=timeout)\n\n    @classmethod\n    def run(cls, *args, timeout=None, **kwargs):\n        with cls(*args, **kwargs) as proc:\n            default = '' if proc.__text_mode else b''\n            stdout, stderr = proc.communicate_or_kill(timeout=timeout)\n            return stdout or default, stderr or default, proc.returncode\n\n\ndef encodeArgument(s):\n    # Legacy code that uses byte strings\n    # Uncomment the following line after fixing all post processors\n    # assert isinstance(s, str), 'Internal error: %r should be of type %r, is %r' % (s, str, type(s))\n    return s if isinstance(s, str) else s.decode('ascii')\n\n\n_timetuple = collections.namedtuple('Time', ('hours', 'minutes', 'seconds', 'milliseconds'))\n\n\ndef timetuple_from_msec(msec):\n    secs, msec = divmod(msec, 1000)\n    mins, secs = divmod(secs, 60)\n    hrs, mins = divmod(mins, 60)\n    return _timetuple(hrs, mins, secs, msec)\n\n\ndef formatSeconds(secs, delim=':', msec=False):\n    time = timetuple_from_msec(secs * 1000)\n    if time.hours:\n        ret = '%d%s%02d%s%02d' % (time.hours, delim, time.minutes, delim, time.seconds)\n    elif time.minutes:\n        ret = '%d%s%02d' % (time.minutes, delim, time.seconds)\n    else:\n        ret = '%d' % time.seconds\n    return '%s.%03d' % (ret, time.milliseconds) if msec else ret\n\n\ndef _ssl_load_windows_store_certs(ssl_context, storename):\n    # Code adapted from _load_windows_store_certs in https://github.com/python/cpython/blob/main/Lib/ssl.py\n    try:\n        certs = [cert for cert, encoding, trust in ssl.enum_certificates(storename)\n                 if encoding == 'x509_asn' and (\n                     trust is True or ssl.Purpose.SERVER_AUTH.oid in trust)]\n    except PermissionError:\n        return\n    for cert in certs:\n        with contextlib.suppress(ssl.SSLError):\n            ssl_context.load_verify_locations(cadata=cert)\n\n\ndef make_HTTPS_handler(params, **kwargs):\n    opts_check_certificate = not params.get('nocheckcertificate')\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n    context.check_hostname = opts_check_certificate\n    if params.get('legacyserverconnect'):\n        context.options |= 4  # SSL_OP_LEGACY_SERVER_CONNECT\n        # Allow use of weaker ciphers in Python 3.10+. See https://bugs.python.org/issue43998\n        context.set_ciphers('DEFAULT')\n    elif (\n        sys.version_info < (3, 10)\n        and ssl.OPENSSL_VERSION_INFO >= (1, 1, 1)\n        and not ssl.OPENSSL_VERSION.startswith('LibreSSL')\n    ):\n        # Backport the default SSL ciphers and minimum TLS version settings from Python 3.10 [1].\n        # This is to ensure consistent behavior across Python versions, and help avoid fingerprinting\n        # in some situations [2][3].\n        # Python 3.10 only supports OpenSSL 1.1.1+ [4]. Because this change is likely\n        # untested on older versions, we only apply this to OpenSSL 1.1.1+ to be safe.\n        # LibreSSL is excluded until further investigation due to cipher support issues [5][6].\n        # 1. https://github.com/python/cpython/commit/e983252b516edb15d4338b0a47631b59ef1e2536\n        # 2. https://github.com/yt-dlp/yt-dlp/issues/4627\n        # 3. https://github.com/yt-dlp/yt-dlp/pull/5294\n        # 4. https://peps.python.org/pep-0644/\n        # 5. https://peps.python.org/pep-0644/#libressl-support\n        # 6. https://github.com/yt-dlp/yt-dlp/commit/5b9f253fa0aee996cf1ed30185d4b502e00609c4#commitcomment-89054368\n        context.set_ciphers('@SECLEVEL=2:ECDH+AESGCM:ECDH+CHACHA20:ECDH+AES:DHE+AES:!aNULL:!eNULL:!aDSS:!SHA1:!AESCCM')\n        context.minimum_version = ssl.TLSVersion.TLSv1_2\n\n    context.verify_mode = ssl.CERT_REQUIRED if opts_check_certificate else ssl.CERT_NONE\n    if opts_check_certificate:\n        if certifi and 'no-certifi' not in params.get('compat_opts', []):\n            context.load_verify_locations(cafile=certifi.where())\n        else:\n            try:\n                context.load_default_certs()\n                # Work around the issue in load_default_certs when there are bad certificates. See:\n                # https://github.com/yt-dlp/yt-dlp/issues/1060,\n                # https://bugs.python.org/issue35665, https://bugs.python.org/issue45312\n            except ssl.SSLError:\n                # enum_certificates is not present in mingw python. See https://github.com/yt-dlp/yt-dlp/issues/1151\n                if sys.platform == 'win32' and hasattr(ssl, 'enum_certificates'):\n                    for storename in ('CA', 'ROOT'):\n                        _ssl_load_windows_store_certs(context, storename)\n                context.set_default_verify_paths()\n\n    client_certfile = params.get('client_certificate')\n    if client_certfile:\n        try:\n            context.load_cert_chain(\n                client_certfile, keyfile=params.get('client_certificate_key'),\n                password=params.get('client_certificate_password'))\n        except ssl.SSLError:\n            raise YoutubeDLError('Unable to load client certificate')\n\n    # Some servers may reject requests if ALPN extension is not sent. See:\n    # https://github.com/python/cpython/issues/85140\n    # https://github.com/yt-dlp/yt-dlp/issues/3878\n    with contextlib.suppress(NotImplementedError):\n        context.set_alpn_protocols(['http/1.1'])\n\n    return YoutubeDLHTTPSHandler(params, context=context, **kwargs)\n\n\ndef bug_reports_message(before=';'):\n    from ..update import REPOSITORY\n\n    msg = (f'please report this issue on  https://github.com/{REPOSITORY}/issues?q= , '\n           'filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U')\n\n    before = before.rstrip()\n    if not before or before.endswith(('.', '!', '?')):\n        msg = msg[0].title() + msg[1:]\n\n    return (before + ' ' if before else '') + msg\n\n\nclass YoutubeDLError(Exception):\n    \"\"\"Base exception for YoutubeDL errors.\"\"\"\n    msg = None\n\n    def __init__(self, msg=None):\n        if msg is not None:\n            self.msg = msg\n        elif self.msg is None:\n            self.msg = type(self).__name__\n        super().__init__(self.msg)\n\n\nnetwork_exceptions = [urllib.error.URLError, http.client.HTTPException, socket.error]\nif hasattr(ssl, 'CertificateError'):\n    network_exceptions.append(ssl.CertificateError)\nnetwork_exceptions = tuple(network_exceptions)\n\n\nclass ExtractorError(YoutubeDLError):\n    \"\"\"Error during info extraction.\"\"\"\n\n    def __init__(self, msg, tb=None, expected=False, cause=None, video_id=None, ie=None):\n        \"\"\" tb, if given, is the original traceback (so that it can be printed out).\n        If expected is set, this is a normal error message and most likely not a bug in yt-dlp.\n        \"\"\"\n        if sys.exc_info()[0] in network_exceptions:\n            expected = True\n\n        self.orig_msg = str(msg)\n        self.traceback = tb\n        self.expected = expected\n        self.cause = cause\n        self.video_id = video_id\n        self.ie = ie\n        self.exc_info = sys.exc_info()  # preserve original exception\n        if isinstance(self.exc_info[1], ExtractorError):\n            self.exc_info = self.exc_info[1].exc_info\n        super().__init__(self.__msg)\n\n    @property\n    def __msg(self):\n        return ''.join((\n            format_field(self.ie, None, '[%s] '),\n            format_field(self.video_id, None, '%s: '),\n            self.orig_msg,\n            format_field(self.cause, None, ' (caused by %r)'),\n            '' if self.expected else bug_reports_message()))\n\n    def format_traceback(self):\n        return join_nonempty(\n            self.traceback and ''.join(traceback.format_tb(self.traceback)),\n            self.cause and ''.join(traceback.format_exception(None, self.cause, self.cause.__traceback__)[1:]),\n            delim='\\n') or None\n\n    def __setattr__(self, name, value):\n        super().__setattr__(name, value)\n        if getattr(self, 'msg', None) and name not in ('msg', 'args'):\n            self.msg = self.__msg or type(self).__name__\n            self.args = (self.msg, )  # Cannot be property\n\n\nclass UnsupportedError(ExtractorError):\n    def __init__(self, url):\n        super().__init__(\n            'Unsupported URL: %s' % url, expected=True)\n        self.url = url\n\n\nclass RegexNotFoundError(ExtractorError):\n    \"\"\"Error when a regex didn't match\"\"\"\n    pass\n\n\nclass GeoRestrictedError(ExtractorError):\n    \"\"\"Geographic restriction Error exception.\n\n    This exception may be thrown when a video is not available from your\n    geographic location due to geographic restrictions imposed by a website.\n    \"\"\"\n\n    def __init__(self, msg, countries=None, **kwargs):\n        kwargs['expected'] = True\n        super().__init__(msg, **kwargs)\n        self.countries = countries\n\n\nclass UserNotLive(ExtractorError):\n    \"\"\"Error when a channel/user is not live\"\"\"\n\n    def __init__(self, msg=None, **kwargs):\n        kwargs['expected'] = True\n        super().__init__(msg or 'The channel is not currently live', **kwargs)\n\n\nclass DownloadError(YoutubeDLError):\n    \"\"\"Download Error exception.\n\n    This exception may be thrown by FileDownloader objects if they are not\n    configured to continue on errors. They will contain the appropriate\n    error message.\n    \"\"\"\n\n    def __init__(self, msg, exc_info=None):\n        \"\"\" exc_info, if given, is the original exception that caused the trouble (as returned by sys.exc_info()). \"\"\"\n        super().__init__(msg)\n        self.exc_info = exc_info\n\n\nclass EntryNotInPlaylist(YoutubeDLError):\n    \"\"\"Entry not in playlist exception.\n\n    This exception will be thrown by YoutubeDL when a requested entry\n    is not found in the playlist info_dict\n    \"\"\"\n    msg = 'Entry not found in info'\n\n\nclass SameFileError(YoutubeDLError):\n    \"\"\"Same File exception.\n\n    This exception will be thrown by FileDownloader objects if they detect\n    multiple files would have to be downloaded to the same file on disk.\n    \"\"\"\n    msg = 'Fixed output name but more than one file to download'\n\n    def __init__(self, filename=None):\n        if filename is not None:\n            self.msg += f': {filename}'\n        super().__init__(self.msg)\n\n\nclass PostProcessingError(YoutubeDLError):\n    \"\"\"Post Processing exception.\n\n    This exception may be raised by PostProcessor's .run() method to\n    indicate an error in the postprocessing task.\n    \"\"\"\n\n\nclass DownloadCancelled(YoutubeDLError):\n    \"\"\" Exception raised when the download queue should be interrupted \"\"\"\n    msg = 'The download was cancelled'\n\n\nclass ExistingVideoReached(DownloadCancelled):\n    \"\"\" --break-on-existing triggered \"\"\"\n    msg = 'Encountered a video that is already in the archive, stopping due to --break-on-existing'\n\n\nclass RejectedVideoReached(DownloadCancelled):\n    \"\"\" --break-match-filter triggered \"\"\"\n    msg = 'Encountered a video that did not match filter, stopping due to --break-match-filter'\n\n\nclass MaxDownloadsReached(DownloadCancelled):\n    \"\"\" --max-downloads limit has been reached. \"\"\"\n    msg = 'Maximum number of downloads reached, stopping due to --max-downloads'\n\n\nclass ReExtractInfo(YoutubeDLError):\n    \"\"\" Video info needs to be re-extracted. \"\"\"\n\n    def __init__(self, msg, expected=False):\n        super().__init__(msg)\n        self.expected = expected\n\n\nclass ThrottledDownload(ReExtractInfo):\n    \"\"\" Download speed below --throttled-rate. \"\"\"\n    msg = 'The download speed is below throttle limit'\n\n    def __init__(self):\n        super().__init__(self.msg, expected=False)\n\n\nclass UnavailableVideoError(YoutubeDLError):\n    \"\"\"Unavailable Format exception.\n\n    This exception will be thrown when a video is requested\n    in a format that is not available for that video.\n    \"\"\"\n    msg = 'Unable to download video'\n\n    def __init__(self, err=None):\n        if err is not None:\n            self.msg += f': {err}'\n        super().__init__(self.msg)\n\n\nclass ContentTooShortError(YoutubeDLError):\n    \"\"\"Content Too Short exception.\n\n    This exception may be raised by FileDownloader objects when a file they\n    download is too small for what the server announced first, indicating\n    the connection was probably interrupted.\n    \"\"\"\n\n    def __init__(self, downloaded, expected):\n        super().__init__(f'Downloaded {downloaded} bytes, expected {expected} bytes')\n        # Both in bytes\n        self.downloaded = downloaded\n        self.expected = expected\n\n\nclass XAttrMetadataError(YoutubeDLError):\n    def __init__(self, code=None, msg='Unknown error'):\n        super().__init__(msg)\n        self.code = code\n        self.msg = msg\n\n        # Parsing code and msg\n        if (self.code in (errno.ENOSPC, errno.EDQUOT)\n                or 'No space left' in self.msg or 'Disk quota exceeded' in self.msg):\n            self.reason = 'NO_SPACE'\n        elif self.code == errno.E2BIG or 'Argument list too long' in self.msg:\n            self.reason = 'VALUE_TOO_LONG'\n        else:\n            self.reason = 'NOT_SUPPORTED'\n\n\nclass XAttrUnavailableError(YoutubeDLError):\n    pass\n\n\ndef _create_http_connection(ydl_handler, http_class, is_https, *args, **kwargs):\n    hc = http_class(*args, **kwargs)\n    source_address = ydl_handler._params.get('source_address')\n\n    if source_address is not None:\n        # This is to workaround _create_connection() from socket where it will try all\n        # address data from getaddrinfo() including IPv6. This filters the result from\n        # getaddrinfo() based on the source_address value.\n        # This is based on the cpython socket.create_connection() function.\n        # https://github.com/python/cpython/blob/master/Lib/socket.py#L691\n        def _create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None):\n            host, port = address\n            err = None\n            addrs = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\n            af = socket.AF_INET if '.' in source_address[0] else socket.AF_INET6\n            ip_addrs = [addr for addr in addrs if addr[0] == af]\n            if addrs and not ip_addrs:\n                ip_version = 'v4' if af == socket.AF_INET else 'v6'\n                raise OSError(\n                    \"No remote IP%s addresses available for connect, can't use '%s' as source address\"\n                    % (ip_version, source_address[0]))\n            for res in ip_addrs:\n                af, socktype, proto, canonname, sa = res\n                sock = None\n                try:\n                    sock = socket.socket(af, socktype, proto)\n                    if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:\n                        sock.settimeout(timeout)\n                    sock.bind(source_address)\n                    sock.connect(sa)\n                    err = None  # Explicitly break reference cycle\n                    return sock\n                except OSError as _:\n                    err = _\n                    if sock is not None:\n                        sock.close()\n            if err is not None:\n                raise err\n            else:\n                raise OSError('getaddrinfo returns an empty list')\n        if hasattr(hc, '_create_connection'):\n            hc._create_connection = _create_connection\n        hc.source_address = (source_address, 0)\n\n    return hc\n\n\nclass YoutubeDLHandler(urllib.request.HTTPHandler):\n    \"\"\"Handler for HTTP requests and responses.\n\n    This class, when installed with an OpenerDirector, automatically adds\n    the standard headers to every HTTP request and handles gzipped, deflated and\n    brotli responses from web servers.\n\n    Part of this code was copied from:\n\n    http://techknack.net/python-urllib2-handlers/\n\n    Andrew Rowls, the author of that code, agreed to release it to the\n    public domain.\n    \"\"\"\n\n    def __init__(self, params, *args, **kwargs):\n        urllib.request.HTTPHandler.__init__(self, *args, **kwargs)\n        self._params = params\n\n    def http_open(self, req):\n        conn_class = http.client.HTTPConnection\n\n        socks_proxy = req.headers.get('Ytdl-socks-proxy')\n        if socks_proxy:\n            conn_class = make_socks_conn_class(conn_class, socks_proxy)\n            del req.headers['Ytdl-socks-proxy']\n\n        return self.do_open(functools.partial(\n            _create_http_connection, self, conn_class, False),\n            req)\n\n    @staticmethod\n    def deflate(data):\n        if not data:\n            return data\n        try:\n            return zlib.decompress(data, -zlib.MAX_WBITS)\n        except zlib.error:\n            return zlib.decompress(data)\n\n    @staticmethod\n    def brotli(data):\n        if not data:\n            return data\n        return brotli.decompress(data)\n\n    @staticmethod\n    def gz(data):\n        gz = gzip.GzipFile(fileobj=io.BytesIO(data), mode='rb')\n        try:\n            return gz.read()\n        except OSError as original_oserror:\n            # There may be junk add the end of the file\n            # See http://stackoverflow.com/q/4928560/35070 for details\n            for i in range(1, 1024):\n                try:\n                    gz = gzip.GzipFile(fileobj=io.BytesIO(data[:-i]), mode='rb')\n                    return gz.read()\n                except OSError:\n                    continue\n            else:\n                raise original_oserror\n\n    def http_request(self, req):\n        # According to RFC 3986, URLs can not contain non-ASCII characters, however this is not\n        # always respected by websites, some tend to give out URLs with non percent-encoded\n        # non-ASCII characters (see telemb.py, ard.py [#3412])\n        # urllib chokes on URLs with non-ASCII characters (see http://bugs.python.org/issue3991)\n        # To work around aforementioned issue we will replace request's original URL with\n        # percent-encoded one\n        # Since redirects are also affected (e.g. http://www.southpark.de/alle-episoden/s18e09)\n        # the code of this workaround has been moved here from YoutubeDL.urlopen()\n        url = req.get_full_url()\n        url_escaped = escape_url(url)\n\n        # Substitute URL if any change after escaping\n        if url != url_escaped:\n            req = update_Request(req, url=url_escaped)\n\n        for h, v in self._params.get('http_headers', std_headers).items():\n            # Capitalize is needed because of Python bug 2275: http://bugs.python.org/issue2275\n            # The dict keys are capitalized because of this bug by urllib\n            if h.capitalize() not in req.headers:\n                req.add_header(h, v)\n\n        if 'Youtubedl-no-compression' in req.headers:  # deprecated\n            req.headers.pop('Youtubedl-no-compression', None)\n            req.add_header('Accept-encoding', 'identity')\n\n        if 'Accept-encoding' not in req.headers:\n            req.add_header('Accept-encoding', ', '.join(SUPPORTED_ENCODINGS))\n\n        return super().do_request_(req)\n\n    def http_response(self, req, resp):\n        old_resp = resp\n\n        # Content-Encoding header lists the encodings in order that they were applied [1].\n        # To decompress, we simply do the reverse.\n        # [1]: https://datatracker.ietf.org/doc/html/rfc9110#name-content-encoding\n        decoded_response = None\n        for encoding in (e.strip() for e in reversed(resp.headers.get('Content-encoding', '').split(','))):\n            if encoding == 'gzip':\n                decoded_response = self.gz(decoded_response or resp.read())\n            elif encoding == 'deflate':\n                decoded_response = self.deflate(decoded_response or resp.read())\n            elif encoding == 'br' and brotli:\n                decoded_response = self.brotli(decoded_response or resp.read())\n\n        if decoded_response is not None:\n            resp = urllib.request.addinfourl(io.BytesIO(decoded_response), old_resp.headers, old_resp.url, old_resp.code)\n            resp.msg = old_resp.msg\n        # Percent-encode redirect URL of Location HTTP header to satisfy RFC 3986 (see\n        # https://github.com/ytdl-org/youtube-dl/issues/6457).\n        if 300 <= resp.code < 400:\n            location = resp.headers.get('Location')\n            if location:\n                # As of RFC 2616 default charset is iso-8859-1 that is respected by python 3\n                location = location.encode('iso-8859-1').decode()\n                location_escaped = escape_url(location)\n                if location != location_escaped:\n                    del resp.headers['Location']\n                    resp.headers['Location'] = location_escaped\n        return resp\n\n    https_request = http_request\n    https_response = http_response\n\n\ndef make_socks_conn_class(base_class, socks_proxy):\n    assert issubclass(base_class, (\n        http.client.HTTPConnection, http.client.HTTPSConnection))\n\n    url_components = urllib.parse.urlparse(socks_proxy)\n    if url_components.scheme.lower() == 'socks5':\n        socks_type = ProxyType.SOCKS5\n    elif url_components.scheme.lower() in ('socks', 'socks4'):\n        socks_type = ProxyType.SOCKS4\n    elif url_components.scheme.lower() == 'socks4a':\n        socks_type = ProxyType.SOCKS4A\n\n    def unquote_if_non_empty(s):\n        if not s:\n            return s\n        return urllib.parse.unquote_plus(s)\n\n    proxy_args = (\n        socks_type,\n        url_components.hostname, url_components.port or 1080,\n        True,  # Remote DNS\n        unquote_if_non_empty(url_components.username),\n        unquote_if_non_empty(url_components.password),\n    )\n\n    class SocksConnection(base_class):\n        def connect(self):\n            self.sock = sockssocket()\n            self.sock.setproxy(*proxy_args)\n            if isinstance(self.timeout, (int, float)):\n                self.sock.settimeout(self.timeout)\n            self.sock.connect((self.host, self.port))\n\n            if isinstance(self, http.client.HTTPSConnection):\n                if hasattr(self, '_context'):  # Python > 2.6\n                    self.sock = self._context.wrap_socket(\n                        self.sock, server_hostname=self.host)\n                else:\n                    self.sock = ssl.wrap_socket(self.sock)\n\n    return SocksConnection\n\n\nclass YoutubeDLHTTPSHandler(urllib.request.HTTPSHandler):\n    def __init__(self, params, https_conn_class=None, *args, **kwargs):\n        urllib.request.HTTPSHandler.__init__(self, *args, **kwargs)\n        self._https_conn_class = https_conn_class or http.client.HTTPSConnection\n        self._params = params\n\n    def https_open(self, req):\n        kwargs = {}\n        conn_class = self._https_conn_class\n\n        if hasattr(self, '_context'):  # python > 2.6\n            kwargs['context'] = self._context\n        if hasattr(self, '_check_hostname'):  # python 3.x\n            kwargs['check_hostname'] = self._check_hostname\n\n        socks_proxy = req.headers.get('Ytdl-socks-proxy')\n        if socks_proxy:\n            conn_class = make_socks_conn_class(conn_class, socks_proxy)\n            del req.headers['Ytdl-socks-proxy']\n\n        try:\n            return self.do_open(\n                functools.partial(_create_http_connection, self, conn_class, True), req, **kwargs)\n        except urllib.error.URLError as e:\n            if (isinstance(e.reason, ssl.SSLError)\n                    and getattr(e.reason, 'reason', None) == 'SSLV3_ALERT_HANDSHAKE_FAILURE'):\n                raise YoutubeDLError('SSLV3_ALERT_HANDSHAKE_FAILURE: Try using --legacy-server-connect')\n            raise\n\n\ndef is_path_like(f):\n    return isinstance(f, (str, bytes, os.PathLike))\n\n\nclass YoutubeDLCookieProcessor(urllib.request.HTTPCookieProcessor):\n    def __init__(self, cookiejar=None):\n        urllib.request.HTTPCookieProcessor.__init__(self, cookiejar)\n\n    def http_response(self, request, response):\n        return urllib.request.HTTPCookieProcessor.http_response(self, request, response)\n\n    https_request = urllib.request.HTTPCookieProcessor.http_request\n    https_response = http_response\n\n\nclass YoutubeDLRedirectHandler(urllib.request.HTTPRedirectHandler):\n    \"\"\"YoutubeDL redirect handler\n\n    The code is based on HTTPRedirectHandler implementation from CPython [1].\n\n    This redirect handler fixes and improves the logic to better align with RFC7261\n     and what browsers tend to do [2][3]\n\n    1. https://github.com/python/cpython/blob/master/Lib/urllib/request.py\n    2. https://datatracker.ietf.org/doc/html/rfc7231\n    3. https://github.com/python/cpython/issues/91306\n    \"\"\"\n\n    http_error_301 = http_error_303 = http_error_307 = http_error_308 = urllib.request.HTTPRedirectHandler.http_error_302\n\n    def redirect_request(self, req, fp, code, msg, headers, newurl):\n        if code not in (301, 302, 303, 307, 308):\n            raise urllib.error.HTTPError(req.full_url, code, msg, headers, fp)\n\n        new_method = req.get_method()\n        new_data = req.data\n\n        # Technically the Cookie header should be in unredirected_hdrs,\n        # however in practice some may set it in normal headers anyway.\n        # We will remove it here to prevent any leaks.\n        remove_headers = ['Cookie']\n\n        # A 303 must either use GET or HEAD for subsequent request\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.4\n        if code == 303 and req.get_method() != 'HEAD':\n            new_method = 'GET'\n        # 301 and 302 redirects are commonly turned into a GET from a POST\n        # for subsequent requests by browsers, so we'll do the same.\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.2\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.3\n        elif code in (301, 302) and req.get_method() == 'POST':\n            new_method = 'GET'\n\n        # only remove payload if method changed (e.g. POST to GET)\n        if new_method != req.get_method():\n            new_data = None\n            remove_headers.extend(['Content-Length', 'Content-Type'])\n\n        new_headers = {k: v for k, v in req.headers.items() if k.title() not in remove_headers}\n\n        return urllib.request.Request(\n            newurl, headers=new_headers, origin_req_host=req.origin_req_host,\n            unverifiable=True, method=new_method, data=new_data)\n\n\ndef extract_timezone(date_str):\n    m = re.search(\n        r'''(?x)\n            ^.{8,}?                                              # >=8 char non-TZ prefix, if present\n            (?P<tz>Z|                                            # just the UTC Z, or\n                (?:(?<=.\\b\\d{4}|\\b\\d{2}:\\d\\d)|                   # preceded by 4 digits or hh:mm or\n                   (?<!.\\b[a-zA-Z]{3}|[a-zA-Z]{4}|..\\b\\d\\d))     # not preceded by 3 alpha word or >= 4 alpha or 2 digits\n                   [ ]?                                          # optional space\n                (?P<sign>\\+|-)                                   # +/-\n                (?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})       # hh[:]mm\n            $)\n        ''', date_str)\n    if not m:\n        m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n        timezone = TIMEZONE_NAMES.get(m and m.group('tz').strip())\n        if timezone is not None:\n            date_str = date_str[:-len(m.group('tz'))]\n        timezone = datetime.timedelta(hours=timezone or 0)\n    else:\n        date_str = date_str[:-len(m.group('tz'))]\n        if not m.group('sign'):\n            timezone = datetime.timedelta()\n        else:\n            sign = 1 if m.group('sign') == '+' else -1\n            timezone = datetime.timedelta(\n                hours=sign * int(m.group('hours')),\n                minutes=sign * int(m.group('minutes')))\n    return timezone, date_str\n\n\ndef parse_iso8601(date_str, delimiter='T', timezone=None):\n    \"\"\" Return a UNIX timestamp from the given date \"\"\"\n\n    if date_str is None:\n        return None\n\n    date_str = re.sub(r'\\.[0-9]+', '', date_str)\n\n    if timezone is None:\n        timezone, date_str = extract_timezone(date_str)\n\n    with contextlib.suppress(ValueError):\n        date_format = f'%Y-%m-%d{delimiter}%H:%M:%S'\n        dt = datetime.datetime.strptime(date_str, date_format) - timezone\n        return calendar.timegm(dt.timetuple())\n\n\ndef date_formats(day_first=True):\n    return DATE_FORMATS_DAY_FIRST if day_first else DATE_FORMATS_MONTH_FIRST\n\n\ndef unified_strdate(date_str, day_first=True):\n    \"\"\"Return a string with the date in the format YYYYMMDD\"\"\"\n\n    if date_str is None:\n        return None\n    upload_date = None\n    # Replace commas\n    date_str = date_str.replace(',', ' ')\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n    _, date_str = extract_timezone(date_str)\n\n    for expression in date_formats(day_first):\n        with contextlib.suppress(ValueError):\n            upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n    if upload_date is None:\n        timetuple = email.utils.parsedate_tz(date_str)\n        if timetuple:\n            with contextlib.suppress(ValueError):\n                upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    if upload_date is not None:\n        return str(upload_date)\n\n\ndef unified_timestamp(date_str, day_first=True):\n    if not isinstance(date_str, str):\n        return None\n\n    date_str = re.sub(r'\\s+', ' ', re.sub(\n        r'(?i)[,|]|(mon|tues?|wed(nes)?|thu(rs)?|fri|sat(ur)?)(day)?', '', date_str))\n\n    pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0\n    timezone, date_str = extract_timezone(date_str)\n\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n\n    # Remove unrecognized timezones from ISO 8601 alike timestamps\n    m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n    if m:\n        date_str = date_str[:-len(m.group('tz'))]\n\n    # Python only supports microseconds, so remove nanoseconds\n    m = re.search(r'^([0-9]{4,}-[0-9]{1,2}-[0-9]{1,2}T[0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\\.[0-9]{6})[0-9]+$', date_str)\n    if m:\n        date_str = m.group(1)\n\n    for expression in date_formats(day_first):\n        with contextlib.suppress(ValueError):\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n            return calendar.timegm(dt.timetuple())\n\n    timetuple = email.utils.parsedate_tz(date_str)\n    if timetuple:\n        return calendar.timegm(timetuple) + pm_delta * 3600 - timezone.total_seconds()\n\n\ndef determine_ext(url, default_ext='unknown_video'):\n    if url is None or '.' not in url:\n        return default_ext\n    guess = url.partition('?')[0].rpartition('.')[2]\n    if re.match(r'^[A-Za-z0-9]+$', guess):\n        return guess\n    # Try extract ext from URLs like http://example.com/foo/bar.mp4/?download\n    elif guess.rstrip('/') in KNOWN_EXTENSIONS:\n        return guess.rstrip('/')\n    else:\n        return default_ext\n\n\ndef subtitles_filename(filename, sub_lang, sub_format, expected_real_ext=None):\n    return replace_extension(filename, sub_lang + '.' + sub_format, expected_real_ext)\n\n\ndef datetime_from_str(date_str, precision='auto', format='%Y%m%d'):\n    R\"\"\"\n    Return a datetime object from a string.\n    Supported format:\n        (now|today|yesterday|DATE)([+-]\\d+(microsecond|second|minute|hour|day|week|month|year)s?)?\n\n    @param format       strftime format of DATE\n    @param precision    Round the datetime object: auto|microsecond|second|minute|hour|day\n                        auto: round to the unit provided in date_str (if applicable).\n    \"\"\"\n    auto_precision = False\n    if precision == 'auto':\n        auto_precision = True\n        precision = 'microsecond'\n    today = datetime_round(datetime.datetime.utcnow(), precision)\n    if date_str in ('now', 'today'):\n        return today\n    if date_str == 'yesterday':\n        return today - datetime.timedelta(days=1)\n    match = re.match(\n        r'(?P<start>.+)(?P<sign>[+-])(?P<time>\\d+)(?P<unit>microsecond|second|minute|hour|day|week|month|year)s?',\n        date_str)\n    if match is not None:\n        start_time = datetime_from_str(match.group('start'), precision, format)\n        time = int(match.group('time')) * (-1 if match.group('sign') == '-' else 1)\n        unit = match.group('unit')\n        if unit == 'month' or unit == 'year':\n            new_date = datetime_add_months(start_time, time * 12 if unit == 'year' else time)\n            unit = 'day'\n        else:\n            if unit == 'week':\n                unit = 'day'\n                time *= 7\n            delta = datetime.timedelta(**{unit + 's': time})\n            new_date = start_time + delta\n        if auto_precision:\n            return datetime_round(new_date, unit)\n        return new_date\n\n    return datetime_round(datetime.datetime.strptime(date_str, format), precision)\n\n\ndef date_from_str(date_str, format='%Y%m%d', strict=False):\n    R\"\"\"\n    Return a date object from a string using datetime_from_str\n\n    @param strict  Restrict allowed patterns to \"YYYYMMDD\" and\n                   (now|today|yesterday)(-\\d+(day|week|month|year)s?)?\n    \"\"\"\n    if strict and not re.fullmatch(r'\\d{8}|(now|today|yesterday)(-\\d+(day|week|month|year)s?)?', date_str):\n        raise ValueError(f'Invalid date format \"{date_str}\"')\n    return datetime_from_str(date_str, precision='microsecond', format=format).date()\n\n\ndef datetime_add_months(dt, months):\n    \"\"\"Increment/Decrement a datetime object by months.\"\"\"\n    month = dt.month + months - 1\n    year = dt.year + month // 12\n    month = month % 12 + 1\n    day = min(dt.day, calendar.monthrange(year, month)[1])\n    return dt.replace(year, month, day)\n\n\ndef datetime_round(dt, precision='day'):\n    \"\"\"\n    Round a datetime object's time to a specific precision\n    \"\"\"\n    if precision == 'microsecond':\n        return dt\n\n    unit_seconds = {\n        'day': 86400,\n        'hour': 3600,\n        'minute': 60,\n        'second': 1,\n    }\n    roundto = lambda x, n: ((x + n / 2) // n) * n\n    timestamp = calendar.timegm(dt.timetuple())\n    return datetime.datetime.utcfromtimestamp(roundto(timestamp, unit_seconds[precision]))\n\n\ndef hyphenate_date(date_str):\n    \"\"\"\n    Convert a date in 'YYYYMMDD' format to 'YYYY-MM-DD' format\"\"\"\n    match = re.match(r'^(\\d\\d\\d\\d)(\\d\\d)(\\d\\d)$', date_str)\n    if match is not None:\n        return '-'.join(match.groups())\n    else:\n        return date_str\n\n\nclass DateRange:\n    \"\"\"Represents a time interval between two dates\"\"\"\n\n    def __init__(self, start=None, end=None):\n        \"\"\"start and end must be strings in the format accepted by date\"\"\"\n        if start is not None:\n            self.start = date_from_str(start, strict=True)\n        else:\n            self.start = datetime.datetime.min.date()\n        if end is not None:\n            self.end = date_from_str(end, strict=True)\n        else:\n            self.end = datetime.datetime.max.date()\n        if self.start > self.end:\n            raise ValueError('Date range: \"%s\" , the start date must be before the end date' % self)\n\n    @classmethod\n    def day(cls, day):\n        \"\"\"Returns a range that only contains the given day\"\"\"\n        return cls(day, day)\n\n    def __contains__(self, date):\n        \"\"\"Check if the date is in the range\"\"\"\n        if not isinstance(date, datetime.date):\n            date = date_from_str(date)\n        return self.start <= date <= self.end\n\n    def __repr__(self):\n        return f'{__name__}.{type(self).__name__}({self.start.isoformat()!r}, {self.end.isoformat()!r})'\n\n    def __eq__(self, other):\n        return (isinstance(other, DateRange)\n                and self.start == other.start and self.end == other.end)\n\n\n@functools.cache\ndef system_identifier():\n    python_implementation = platform.python_implementation()\n    if python_implementation == 'PyPy' and hasattr(sys, 'pypy_version_info'):\n        python_implementation += ' version %d.%d.%d' % sys.pypy_version_info[:3]\n    libc_ver = []\n    with contextlib.suppress(OSError):  # We may not have access to the executable\n        libc_ver = platform.libc_ver()\n\n    return 'Python %s (%s %s %s) - %s (%s%s)' % (\n        platform.python_version(),\n        python_implementation,\n        platform.machine(),\n        platform.architecture()[0],\n        platform.platform(),\n        ssl.OPENSSL_VERSION,\n        format_field(join_nonempty(*libc_ver, delim=' '), None, ', %s'),\n    )\n\n\n@functools.cache\ndef get_windows_version():\n    ''' Get Windows version. returns () if it's not running on Windows '''\n    if compat_os_name == 'nt':\n        return version_tuple(platform.win32_ver()[1])\n    else:\n        return ()\n\n\ndef write_string(s, out=None, encoding=None):\n    assert isinstance(s, str)\n    out = out or sys.stderr\n    # `sys.stderr` might be `None` (Ref: https://github.com/pyinstaller/pyinstaller/pull/7217)\n    if not out:\n        return\n\n    if compat_os_name == 'nt' and supports_terminal_sequences(out):\n        s = re.sub(r'([\\r\\n]+)', r' \\1', s)\n\n    enc, buffer = None, out\n    if 'b' in getattr(out, 'mode', ''):\n        enc = encoding or preferredencoding()\n    elif hasattr(out, 'buffer'):\n        buffer = out.buffer\n        enc = encoding or getattr(out, 'encoding', None) or preferredencoding()\n\n    buffer.write(s.encode(enc, 'ignore') if enc else s)\n    out.flush()\n\n\ndef deprecation_warning(msg, *, printer=None, stacklevel=0, **kwargs):\n    from .. import _IN_CLI\n    if _IN_CLI:\n        if msg in deprecation_warning._cache:\n            return\n        deprecation_warning._cache.add(msg)\n        if printer:\n            return printer(f'{msg}{bug_reports_message()}', **kwargs)\n        return write_string(f'ERROR: {msg}{bug_reports_message()}\\n', **kwargs)\n    else:\n        import warnings\n        warnings.warn(DeprecationWarning(msg), stacklevel=stacklevel + 3)\n\n\ndeprecation_warning._cache = set()\n\n\ndef bytes_to_intlist(bs):\n    if not bs:\n        return []\n    if isinstance(bs[0], int):  # Python 3\n        return list(bs)\n    else:\n        return [ord(c) for c in bs]\n\n\ndef intlist_to_bytes(xs):\n    if not xs:\n        return b''\n    return struct.pack('%dB' % len(xs), *xs)\n\n\nclass LockingUnsupportedError(OSError):\n    msg = 'File locking is not supported'\n\n    def __init__(self):\n        super().__init__(self.msg)\n\n\n# Cross-platform file locking\nif sys.platform == 'win32':\n    import ctypes\n    import ctypes.wintypes\n    import msvcrt\n\n    class OVERLAPPED(ctypes.Structure):\n        _fields_ = [\n            ('Internal', ctypes.wintypes.LPVOID),\n            ('InternalHigh', ctypes.wintypes.LPVOID),\n            ('Offset', ctypes.wintypes.DWORD),\n            ('OffsetHigh', ctypes.wintypes.DWORD),\n            ('hEvent', ctypes.wintypes.HANDLE),\n        ]\n\n    kernel32 = ctypes.WinDLL('kernel32')\n    LockFileEx = kernel32.LockFileEx\n    LockFileEx.argtypes = [\n        ctypes.wintypes.HANDLE,     # hFile\n        ctypes.wintypes.DWORD,      # dwFlags\n        ctypes.wintypes.DWORD,      # dwReserved\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh\n        ctypes.POINTER(OVERLAPPED)  # Overlapped\n    ]\n    LockFileEx.restype = ctypes.wintypes.BOOL\n    UnlockFileEx = kernel32.UnlockFileEx\n    UnlockFileEx.argtypes = [\n        ctypes.wintypes.HANDLE,     # hFile\n        ctypes.wintypes.DWORD,      # dwReserved\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh\n        ctypes.POINTER(OVERLAPPED)  # Overlapped\n    ]\n    UnlockFileEx.restype = ctypes.wintypes.BOOL\n    whole_low = 0xffffffff\n    whole_high = 0x7fffffff\n\n    def _lock_file(f, exclusive, block):\n        overlapped = OVERLAPPED()\n        overlapped.Offset = 0\n        overlapped.OffsetHigh = 0\n        overlapped.hEvent = 0\n        f._lock_file_overlapped_p = ctypes.pointer(overlapped)\n\n        if not LockFileEx(msvcrt.get_osfhandle(f.fileno()),\n                          (0x2 if exclusive else 0x0) | (0x0 if block else 0x1),\n                          0, whole_low, whole_high, f._lock_file_overlapped_p):\n            # NB: No argument form of \"ctypes.FormatError\" does not work on PyPy\n            raise BlockingIOError(f'Locking file failed: {ctypes.FormatError(ctypes.GetLastError())!r}')\n\n    def _unlock_file(f):\n        assert f._lock_file_overlapped_p\n        handle = msvcrt.get_osfhandle(f.fileno())\n        if not UnlockFileEx(handle, 0, whole_low, whole_high, f._lock_file_overlapped_p):\n            raise OSError('Unlocking file failed: %r' % ctypes.FormatError())\n\nelse:\n    try:\n        import fcntl\n\n        def _lock_file(f, exclusive, block):\n            flags = fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH\n            if not block:\n                flags |= fcntl.LOCK_NB\n            try:\n                fcntl.flock(f, flags)\n            except BlockingIOError:\n                raise\n            except OSError:  # AOSP does not have flock()\n                fcntl.lockf(f, flags)\n\n        def _unlock_file(f):\n            with contextlib.suppress(OSError):\n                return fcntl.flock(f, fcntl.LOCK_UN)\n            with contextlib.suppress(OSError):\n                return fcntl.lockf(f, fcntl.LOCK_UN)  # AOSP does not have flock()\n            return fcntl.flock(f, fcntl.LOCK_UN | fcntl.LOCK_NB)  # virtiofs needs LOCK_NB on unlocking\n\n    except ImportError:\n\n        def _lock_file(f, exclusive, block):\n            raise LockingUnsupportedError()\n\n        def _unlock_file(f):\n            raise LockingUnsupportedError()\n\n\nclass locked_file:\n    locked = False\n\n    def __init__(self, filename, mode, block=True, encoding=None):\n        if mode not in {'r', 'rb', 'a', 'ab', 'w', 'wb'}:\n            raise NotImplementedError(mode)\n        self.mode, self.block = mode, block\n\n        writable = any(f in mode for f in 'wax+')\n        readable = any(f in mode for f in 'r+')\n        flags = functools.reduce(operator.ior, (\n            getattr(os, 'O_CLOEXEC', 0),  # UNIX only\n            getattr(os, 'O_BINARY', 0),  # Windows only\n            getattr(os, 'O_NOINHERIT', 0),  # Windows only\n            os.O_CREAT if writable else 0,  # O_TRUNC only after locking\n            os.O_APPEND if 'a' in mode else 0,\n            os.O_EXCL if 'x' in mode else 0,\n            os.O_RDONLY if not writable else os.O_RDWR if readable else os.O_WRONLY,\n        ))\n\n        self.f = os.fdopen(os.open(filename, flags, 0o666), mode, encoding=encoding)\n\n    def __enter__(self):\n        exclusive = 'r' not in self.mode\n        try:\n            _lock_file(self.f, exclusive, self.block)\n            self.locked = True\n        except OSError:\n            self.f.close()\n            raise\n        if 'w' in self.mode:\n            try:\n                self.f.truncate()\n            except OSError as e:\n                if e.errno not in (\n                    errno.ESPIPE,  # Illegal seek - expected for FIFO\n                    errno.EINVAL,  # Invalid argument - expected for /dev/null\n                ):\n                    raise\n        return self\n\n    def unlock(self):\n        if not self.locked:\n            return\n        try:\n            _unlock_file(self.f)\n        finally:\n            self.locked = False\n\n    def __exit__(self, *_):\n        try:\n            self.unlock()\n        finally:\n            self.f.close()\n\n    open = __enter__\n    close = __exit__\n\n    def __getattr__(self, attr):\n        return getattr(self.f, attr)\n\n    def __iter__(self):\n        return iter(self.f)\n\n\n@functools.cache\ndef get_filesystem_encoding():\n    encoding = sys.getfilesystemencoding()\n    return encoding if encoding is not None else 'utf-8'\n\n\ndef shell_quote(args):\n    quoted_args = []\n    encoding = get_filesystem_encoding()\n    for a in args:\n        if isinstance(a, bytes):\n            # We may get a filename encoded with 'encodeFilename'\n            a = a.decode(encoding)\n        quoted_args.append(compat_shlex_quote(a))\n    return ' '.join(quoted_args)\n\n\ndef smuggle_url(url, data):\n    \"\"\" Pass additional data in a URL for internal use. \"\"\"\n\n    url, idata = unsmuggle_url(url, {})\n    data.update(idata)\n    sdata = urllib.parse.urlencode(\n        {'__youtubedl_smuggle': json.dumps(data)})\n    return url + '#' + sdata\n\n\ndef unsmuggle_url(smug_url, default=None):\n    if '#__youtubedl_smuggle' not in smug_url:\n        return smug_url, default\n    url, _, sdata = smug_url.rpartition('#')\n    jsond = urllib.parse.parse_qs(sdata)['__youtubedl_smuggle'][0]\n    data = json.loads(jsond)\n    return url, data\n\n\ndef format_decimal_suffix(num, fmt='%d%s', *, factor=1000):\n    \"\"\" Formats numbers with decimal sufixes like K, M, etc \"\"\"\n    num, factor = float_or_none(num), float(factor)\n    if num is None or num < 0:\n        return None\n    POSSIBLE_SUFFIXES = 'kMGTPEZY'\n    exponent = 0 if num == 0 else min(int(math.log(num, factor)), len(POSSIBLE_SUFFIXES))\n    suffix = ['', *POSSIBLE_SUFFIXES][exponent]\n    if factor == 1024:\n        suffix = {'k': 'Ki', '': ''}.get(suffix, f'{suffix}i')\n    converted = num / (factor ** exponent)\n    return fmt % (converted, suffix)\n\n\ndef format_bytes(bytes):\n    return format_decimal_suffix(bytes, '%.2f%sB', factor=1024) or 'N/A'\n\n\ndef lookup_unit_table(unit_table, s, strict=False):\n    num_re = NUMBER_RE if strict else NUMBER_RE.replace(R'\\.', '[,.]')\n    units_re = '|'.join(re.escape(u) for u in unit_table)\n    m = (re.fullmatch if strict else re.match)(\n        rf'(?P<num>{num_re})\\s*(?P<unit>{units_re})\\b', s)\n    if not m:\n        return None\n\n    num = float(m.group('num').replace(',', '.'))\n    mult = unit_table[m.group('unit')]\n    return round(num * mult)\n\n\ndef parse_bytes(s):\n    \"\"\"Parse a string indicating a byte quantity into an integer\"\"\"\n    return lookup_unit_table(\n        {u: 1024**i for i, u in enumerate(['', *'KMGTPEZY'])},\n        s.upper(), strict=True)\n\n\ndef parse_filesize(s):\n    if s is None:\n        return None\n\n    # The lower-case forms are of course incorrect and unofficial,\n    # but we support those too\n    _UNIT_TABLE = {\n        'B': 1,\n        'b': 1,\n        'bytes': 1,\n        'KiB': 1024,\n        'KB': 1000,\n        'kB': 1024,\n        'Kb': 1000,\n        'kb': 1000,\n        'kilobytes': 1000,\n        'kibibytes': 1024,\n        'MiB': 1024 ** 2,\n        'MB': 1000 ** 2,\n        'mB': 1024 ** 2,\n        'Mb': 1000 ** 2,\n        'mb': 1000 ** 2,\n        'megabytes': 1000 ** 2,\n        'mebibytes': 1024 ** 2,\n        'GiB': 1024 ** 3,\n        'GB': 1000 ** 3,\n        'gB': 1024 ** 3,\n        'Gb': 1000 ** 3,\n        'gb': 1000 ** 3,\n        'gigabytes': 1000 ** 3,\n        'gibibytes': 1024 ** 3,\n        'TiB': 1024 ** 4,\n        'TB': 1000 ** 4,\n        'tB': 1024 ** 4,\n        'Tb': 1000 ** 4,\n        'tb': 1000 ** 4,\n        'terabytes': 1000 ** 4,\n        'tebibytes': 1024 ** 4,\n        'PiB': 1024 ** 5,\n        'PB': 1000 ** 5,\n        'pB': 1024 ** 5,\n        'Pb': 1000 ** 5,\n        'pb': 1000 ** 5,\n        'petabytes': 1000 ** 5,\n        'pebibytes': 1024 ** 5,\n        'EiB': 1024 ** 6,\n        'EB': 1000 ** 6,\n        'eB': 1024 ** 6,\n        'Eb': 1000 ** 6,\n        'eb': 1000 ** 6,\n        'exabytes': 1000 ** 6,\n        'exbibytes': 1024 ** 6,\n        'ZiB': 1024 ** 7,\n        'ZB': 1000 ** 7,\n        'zB': 1024 ** 7,\n        'Zb': 1000 ** 7,\n        'zb': 1000 ** 7,\n        'zettabytes': 1000 ** 7,\n        'zebibytes': 1024 ** 7,\n        'YiB': 1024 ** 8,\n        'YB': 1000 ** 8,\n        'yB': 1024 ** 8,\n        'Yb': 1000 ** 8,\n        'yb': 1000 ** 8,\n        'yottabytes': 1000 ** 8,\n        'yobibytes': 1024 ** 8,\n    }\n\n    return lookup_unit_table(_UNIT_TABLE, s)\n\n\ndef parse_count(s):\n    if s is None:\n        return None\n\n    s = re.sub(r'^[^\\d]+\\s', '', s).strip()\n\n    if re.match(r'^[\\d,.]+$', s):\n        return str_to_int(s)\n\n    _UNIT_TABLE = {\n        'k': 1000,\n        'K': 1000,\n        'm': 1000 ** 2,\n        'M': 1000 ** 2,\n        'kk': 1000 ** 2,\n        'KK': 1000 ** 2,\n        'b': 1000 ** 3,\n        'B': 1000 ** 3,\n    }\n\n    ret = lookup_unit_table(_UNIT_TABLE, s)\n    if ret is not None:\n        return ret\n\n    mobj = re.match(r'([\\d,.]+)(?:$|\\s)', s)\n    if mobj:\n        return str_to_int(mobj.group(1))\n\n\ndef parse_resolution(s, *, lenient=False):\n    if s is None:\n        return {}\n\n    if lenient:\n        mobj = re.search(r'(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)', s)\n    else:\n        mobj = re.search(r'(?<![a-zA-Z0-9])(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)(?![a-zA-Z0-9])', s)\n    if mobj:\n        return {\n            'width': int(mobj.group('w')),\n            'height': int(mobj.group('h')),\n        }\n\n    mobj = re.search(r'(?<![a-zA-Z0-9])(\\d+)[pPiI](?![a-zA-Z0-9])', s)\n    if mobj:\n        return {'height': int(mobj.group(1))}\n\n    mobj = re.search(r'\\b([48])[kK]\\b', s)\n    if mobj:\n        return {'height': int(mobj.group(1)) * 540}\n\n    return {}\n\n\ndef parse_bitrate(s):\n    if not isinstance(s, str):\n        return\n    mobj = re.search(r'\\b(\\d+)\\s*kbps', s)\n    if mobj:\n        return int(mobj.group(1))\n\n\ndef month_by_name(name, lang='en'):\n    \"\"\" Return the number of a month by (locale-independently) English name \"\"\"\n\n    month_names = MONTH_NAMES.get(lang, MONTH_NAMES['en'])\n\n    try:\n        return month_names.index(name) + 1\n    except ValueError:\n        return None\n\n\ndef month_by_abbreviation(abbrev):\n    \"\"\" Return the number of a month by (locale-independently) English\n        abbreviations \"\"\"\n\n    try:\n        return [s[:3] for s in ENGLISH_MONTH_NAMES].index(abbrev) + 1\n    except ValueError:\n        return None\n\n\ndef fix_xml_ampersands(xml_str):\n    \"\"\"Replace all the '&' by '&amp;' in XML\"\"\"\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',\n        '&amp;',\n        xml_str)\n\n\ndef setproctitle(title):\n    assert isinstance(title, str)\n\n    # Workaround for https://github.com/yt-dlp/yt-dlp/issues/4541\n    try:\n        import ctypes\n    except ImportError:\n        return\n\n    try:\n        libc = ctypes.cdll.LoadLibrary('libc.so.6')\n    except OSError:\n        return\n    except TypeError:\n        # LoadLibrary in Windows Python 2.7.13 only expects\n        # a bytestring, but since unicode_literals turns\n        # every string into a unicode string, it fails.\n        return\n    title_bytes = title.encode()\n    buf = ctypes.create_string_buffer(len(title_bytes))\n    buf.value = title_bytes\n    try:\n        libc.prctl(15, buf, 0, 0, 0)\n    except AttributeError:\n        return  # Strange libc, just skip this\n\n\ndef remove_start(s, start):\n    return s[len(start):] if s is not None and s.startswith(start) else s\n\n\ndef remove_end(s, end):\n    return s[:-len(end)] if s is not None and s.endswith(end) else s\n\n\ndef remove_quotes(s):\n    if s is None or len(s) < 2:\n        return s\n    for quote in ('\"', \"'\", ):\n        if s[0] == quote and s[-1] == quote:\n            return s[1:-1]\n    return s\n\n\ndef get_domain(url):\n    \"\"\"\n    This implementation is inconsistent, but is kept for compatibility.\n    Use this only for \"webpage_url_domain\"\n    \"\"\"\n    return remove_start(urllib.parse.urlparse(url).netloc, 'www.') or None\n\n\ndef url_basename(url):\n    path = urllib.parse.urlparse(url).path\n    return path.strip('/').split('/')[-1]\n\n\ndef base_url(url):\n    return re.match(r'https?://[^?#]+/', url).group()\n\n\ndef urljoin(base, path):\n    if isinstance(path, bytes):\n        path = path.decode()\n    if not isinstance(path, str) or not path:\n        return None\n    if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):\n        return path\n    if isinstance(base, bytes):\n        base = base.decode()\n    if not isinstance(base, str) or not re.match(\n            r'^(?:https?:)?//', base):\n        return None\n    return urllib.parse.urljoin(base, path)\n\n\nclass HEADRequest(urllib.request.Request):\n    def get_method(self):\n        return 'HEAD'\n\n\nclass PUTRequest(urllib.request.Request):\n    def get_method(self):\n        return 'PUT'\n\n\ndef int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):\n    if get_attr and v is not None:\n        v = getattr(v, get_attr, None)\n    try:\n        return int(v) * invscale // scale\n    except (ValueError, TypeError, OverflowError):\n        return default\n\n\ndef str_or_none(v, default=None):\n    return default if v is None else str(v)\n\n\ndef str_to_int(int_str):\n    \"\"\" A more relaxed version of int_or_none \"\"\"\n    if isinstance(int_str, int):\n        return int_str\n    elif isinstance(int_str, str):\n        int_str = re.sub(r'[,\\.\\+]', '', int_str)\n        return int_or_none(int_str)\n\n\ndef float_or_none(v, scale=1, invscale=1, default=None):\n    if v is None:\n        return default\n    try:\n        return float(v) * invscale / scale\n    except (ValueError, TypeError):\n        return default\n\n\ndef bool_or_none(v, default=None):\n    return v if isinstance(v, bool) else default\n\n\ndef strip_or_none(v, default=None):\n    return v.strip() if isinstance(v, str) else default\n\n\ndef url_or_none(url):\n    if not url or not isinstance(url, str):\n        return None\n    url = url.strip()\n    return url if re.match(r'^(?:(?:https?|rt(?:m(?:pt?[es]?|fp)|sp[su]?)|mms|ftps?):)?//', url) else None\n\n\ndef request_to_url(req):\n    if isinstance(req, urllib.request.Request):\n        return req.get_full_url()\n    else:\n        return req\n\n\ndef strftime_or_none(timestamp, date_format='%Y%m%d', default=None):\n    datetime_object = None\n    try:\n        if isinstance(timestamp, (int, float)):  # unix timestamp\n            # Using naive datetime here can break timestamp() in Windows\n            # Ref: https://github.com/yt-dlp/yt-dlp/issues/5185, https://github.com/python/cpython/issues/94414\n            # Also, datetime.datetime.fromtimestamp breaks for negative timestamps\n            # Ref: https://github.com/yt-dlp/yt-dlp/issues/6706#issuecomment-1496842642\n            datetime_object = (datetime.datetime.fromtimestamp(0, datetime.timezone.utc)\n                               + datetime.timedelta(seconds=timestamp))\n        elif isinstance(timestamp, str):  # assume YYYYMMDD\n            datetime_object = datetime.datetime.strptime(timestamp, '%Y%m%d')\n        date_format = re.sub(  # Support %s on windows\n            r'(?<!%)(%%)*%s', rf'\\g<1>{int(datetime_object.timestamp())}', date_format)\n        return datetime_object.strftime(date_format)\n    except (ValueError, TypeError, AttributeError):\n        return default\n\n\ndef parse_duration(s):\n    if not isinstance(s, str):\n        return None\n    s = s.strip()\n    if not s:\n        return None\n\n    days, hours, mins, secs, ms = [None] * 5\n    m = re.match(r'''(?x)\n            (?P<before_secs>\n                (?:(?:(?P<days>[0-9]+):)?(?P<hours>[0-9]+):)?(?P<mins>[0-9]+):)?\n            (?P<secs>(?(before_secs)[0-9]{1,2}|[0-9]+))\n            (?P<ms>[.:][0-9]+)?Z?$\n        ''', s)\n    if m:\n        days, hours, mins, secs, ms = m.group('days', 'hours', 'mins', 'secs', 'ms')\n    else:\n        m = re.match(\n            r'''(?ix)(?:P?\n                (?:\n                    [0-9]+\\s*y(?:ears?)?,?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*m(?:onths?)?,?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*w(?:eeks?)?,?\\s*\n                )?\n                (?:\n                    (?P<days>[0-9]+)\\s*d(?:ays?)?,?\\s*\n                )?\n                T)?\n                (?:\n                    (?P<hours>[0-9]+)\\s*h(?:ours?)?,?\\s*\n                )?\n                (?:\n                    (?P<mins>[0-9]+)\\s*m(?:in(?:ute)?s?)?,?\\s*\n                )?\n                (?:\n                    (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*s(?:ec(?:ond)?s?)?\\s*\n                )?Z?$''', s)\n        if m:\n            days, hours, mins, secs, ms = m.groups()\n        else:\n            m = re.match(r'(?i)(?:(?P<hours>[0-9.]+)\\s*(?:hours?)|(?P<mins>[0-9.]+)\\s*(?:mins?\\.?|minutes?)\\s*)Z?$', s)\n            if m:\n                hours, mins = m.groups()\n            else:\n                return None\n\n    if ms:\n        ms = ms.replace(':', '.')\n    return sum(float(part or 0) * mult for part, mult in (\n        (days, 86400), (hours, 3600), (mins, 60), (secs, 1), (ms, 1)))\n\n\ndef prepend_extension(filename, ext, expected_real_ext=None):\n    name, real_ext = os.path.splitext(filename)\n    return (\n        f'{name}.{ext}{real_ext}'\n        if not expected_real_ext or real_ext[1:] == expected_real_ext\n        else f'{filename}.{ext}')\n\n\ndef replace_extension(filename, ext, expected_real_ext=None):\n    name, real_ext = os.path.splitext(filename)\n    return '{}.{}'.format(\n        name if not expected_real_ext or real_ext[1:] == expected_real_ext else filename,\n        ext)\n\n\ndef check_executable(exe, args=[]):\n    \"\"\" Checks if the given binary is installed somewhere in PATH, and returns its name.\n    args can be a list of arguments for a short output (like -version) \"\"\"\n    try:\n        Popen.run([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    except OSError:\n        return False\n    return exe\n\n\ndef _get_exe_version_output(exe, args):\n    try:\n        # STDIN should be redirected too. On UNIX-like systems, ffmpeg triggers\n        # SIGTTOU if yt-dlp is run in the background.\n        # See https://github.com/ytdl-org/youtube-dl/issues/955#issuecomment-209789656\n        stdout, _, ret = Popen.run([encodeArgument(exe)] + args, text=True,\n                                   stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        if ret:\n            return None\n    except OSError:\n        return False\n    return stdout\n\n\ndef detect_exe_version(output, version_re=None, unrecognized='present'):\n    assert isinstance(output, str)\n    if version_re is None:\n        version_re = r'version\\s+([-0-9._a-zA-Z]+)'\n    m = re.search(version_re, output)\n    if m:\n        return m.group(1)\n    else:\n        return unrecognized\n\n\ndef get_exe_version(exe, args=['--version'],\n                    version_re=None, unrecognized=('present', 'broken')):\n    \"\"\" Returns the version of the specified executable,\n    or False if the executable is not present \"\"\"\n    unrecognized = variadic(unrecognized)\n    assert len(unrecognized) in (1, 2)\n    out = _get_exe_version_output(exe, args)\n    if out is None:\n        return unrecognized[-1]\n    return out and detect_exe_version(out, version_re, unrecognized[0])\n\n\ndef frange(start=0, stop=None, step=1):\n    \"\"\"Float range\"\"\"\n    if stop is None:\n        start, stop = 0, start\n    sign = [-1, 1][step > 0] if step else 0\n    while sign * start < sign * stop:\n        yield start\n        start += step\n\n\nclass LazyList(collections.abc.Sequence):\n    \"\"\"Lazy immutable list from an iterable\n    Note that slices of a LazyList are lists and not LazyList\"\"\"\n\n    class IndexError(IndexError):\n        pass\n\n    def __init__(self, iterable, *, reverse=False, _cache=None):\n        self._iterable = iter(iterable)\n        self._cache = [] if _cache is None else _cache\n        self._reversed = reverse\n\n    def __iter__(self):\n        if self._reversed:\n            # We need to consume the entire iterable to iterate in reverse\n            yield from self.exhaust()\n            return\n        yield from self._cache\n        for item in self._iterable:\n            self._cache.append(item)\n            yield item\n\n    def _exhaust(self):\n        self._cache.extend(self._iterable)\n        self._iterable = []  # Discard the emptied iterable to make it pickle-able\n        return self._cache\n\n    def exhaust(self):\n        \"\"\"Evaluate the entire iterable\"\"\"\n        return self._exhaust()[::-1 if self._reversed else 1]\n\n    @staticmethod\n    def _reverse_index(x):\n        return None if x is None else ~x\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            if self._reversed:\n                idx = slice(self._reverse_index(idx.start), self._reverse_index(idx.stop), -(idx.step or 1))\n            start, stop, step = idx.start, idx.stop, idx.step or 1\n        elif isinstance(idx, int):\n            if self._reversed:\n                idx = self._reverse_index(idx)\n            start, stop, step = idx, idx, 0\n        else:\n            raise TypeError('indices must be integers or slices')\n        if ((start or 0) < 0 or (stop or 0) < 0\n                or (start is None and step < 0)\n                or (stop is None and step > 0)):\n            # We need to consume the entire iterable to be able to slice from the end\n            # Obviously, never use this with infinite iterables\n            self._exhaust()\n            try:\n                return self._cache[idx]\n            except IndexError as e:\n                raise self.IndexError(e) from e\n        n = max(start or 0, stop or 0) - len(self._cache) + 1\n        if n > 0:\n            self._cache.extend(itertools.islice(self._iterable, n))\n        try:\n            return self._cache[idx]\n        except IndexError as e:\n            raise self.IndexError(e) from e\n\n    def __bool__(self):\n        try:\n            self[-1] if self._reversed else self[0]\n        except self.IndexError:\n            return False\n        return True\n\n    def __len__(self):\n        self._exhaust()\n        return len(self._cache)\n\n    def __reversed__(self):\n        return type(self)(self._iterable, reverse=not self._reversed, _cache=self._cache)\n\n    def __copy__(self):\n        return type(self)(self._iterable, reverse=self._reversed, _cache=self._cache)\n\n    def __repr__(self):\n        # repr and str should mimic a list. So we exhaust the iterable\n        return repr(self.exhaust())\n\n    def __str__(self):\n        return repr(self.exhaust())\n\n\nclass PagedList:\n\n    class IndexError(IndexError):\n        pass\n\n    def __len__(self):\n        # This is only useful for tests\n        return len(self.getslice())\n\n    def __init__(self, pagefunc, pagesize, use_cache=True):\n        self._pagefunc = pagefunc\n        self._pagesize = pagesize\n        self._pagecount = float('inf')\n        self._use_cache = use_cache\n        self._cache = {}\n\n    def getpage(self, pagenum):\n        page_results = self._cache.get(pagenum)\n        if page_results is None:\n            page_results = [] if pagenum > self._pagecount else list(self._pagefunc(pagenum))\n        if self._use_cache:\n            self._cache[pagenum] = page_results\n        return page_results\n\n    def getslice(self, start=0, end=None):\n        return list(self._getslice(start, end))\n\n    def _getslice(self, start, end):\n        raise NotImplementedError('This method must be implemented by subclasses')\n\n    def __getitem__(self, idx):\n        assert self._use_cache, 'Indexing PagedList requires cache'\n        if not isinstance(idx, int) or idx < 0:\n            raise TypeError('indices must be non-negative integers')\n        entries = self.getslice(idx, idx + 1)\n        if not entries:\n            raise self.IndexError()\n        return entries[0]\n\n\nclass OnDemandPagedList(PagedList):\n    \"\"\"Download pages until a page with less than maximum results\"\"\"\n\n    def _getslice(self, start, end):\n        for pagenum in itertools.count(start // self._pagesize):\n            firstid = pagenum * self._pagesize\n            nextfirstid = pagenum * self._pagesize + self._pagesize\n            if start >= nextfirstid:\n                continue\n\n            startv = (\n                start % self._pagesize\n                if firstid <= start < nextfirstid\n                else 0)\n            endv = (\n                ((end - 1) % self._pagesize) + 1\n                if (end is not None and firstid <= end <= nextfirstid)\n                else None)\n\n            try:\n                page_results = self.getpage(pagenum)\n            except Exception:\n                self._pagecount = pagenum - 1\n                raise\n            if startv != 0 or endv is not None:\n                page_results = page_results[startv:endv]\n            yield from page_results\n\n            # A little optimization - if current page is not \"full\", ie. does\n            # not contain page_size videos then we can assume that this page\n            # is the last one - there are no more ids on further pages -\n            # i.e. no need to query again.\n            if len(page_results) + startv < self._pagesize:\n                break\n\n            # If we got the whole page, but the next page is not interesting,\n            # break out early as well\n            if end == nextfirstid:\n                break\n\n\nclass InAdvancePagedList(PagedList):\n    \"\"\"PagedList with total number of pages known in advance\"\"\"\n\n    def __init__(self, pagefunc, pagecount, pagesize):\n        PagedList.__init__(self, pagefunc, pagesize, True)\n        self._pagecount = pagecount\n\n    def _getslice(self, start, end):\n        start_page = start // self._pagesize\n        end_page = self._pagecount if end is None else min(self._pagecount, end // self._pagesize + 1)\n        skip_elems = start - start_page * self._pagesize\n        only_more = None if end is None else end - start\n        for pagenum in range(start_page, end_page):\n            page_results = self.getpage(pagenum)\n            if skip_elems:\n                page_results = page_results[skip_elems:]\n                skip_elems = None\n            if only_more is not None:\n                if len(page_results) < only_more:\n                    only_more -= len(page_results)\n                else:\n                    yield from page_results[:only_more]\n                    break\n            yield from page_results\n\n\nclass PlaylistEntries:\n    MissingEntry = object()\n    is_exhausted = False\n\n    def __init__(self, ydl, info_dict):\n        self.ydl = ydl\n\n        # _entries must be assigned now since infodict can change during iteration\n        entries = info_dict.get('entries')\n        if entries is None:\n            raise EntryNotInPlaylist('There are no entries')\n        elif isinstance(entries, list):\n            self.is_exhausted = True\n\n        requested_entries = info_dict.get('requested_entries')\n        self.is_incomplete = requested_entries is not None\n        if self.is_incomplete:\n            assert self.is_exhausted\n            self._entries = [self.MissingEntry] * max(requested_entries or [0])\n            for i, entry in zip(requested_entries, entries):\n                self._entries[i - 1] = entry\n        elif isinstance(entries, (list, PagedList, LazyList)):\n            self._entries = entries\n        else:\n            self._entries = LazyList(entries)\n\n    PLAYLIST_ITEMS_RE = re.compile(r'''(?x)\n        (?P<start>[+-]?\\d+)?\n        (?P<range>[:-]\n            (?P<end>[+-]?\\d+|inf(?:inite)?)?\n            (?::(?P<step>[+-]?\\d+))?\n        )?''')\n\n    @classmethod\n    def parse_playlist_items(cls, string):\n        for segment in string.split(','):\n            if not segment:\n                raise ValueError('There is two or more consecutive commas')\n            mobj = cls.PLAYLIST_ITEMS_RE.fullmatch(segment)\n            if not mobj:\n                raise ValueError(f'{segment!r} is not a valid specification')\n            start, end, step, has_range = mobj.group('start', 'end', 'step', 'range')\n            if int_or_none(step) == 0:\n                raise ValueError(f'Step in {segment!r} cannot be zero')\n            yield slice(int_or_none(start), float_or_none(end), int_or_none(step)) if has_range else int(start)\n\n    def get_requested_items(self):\n        playlist_items = self.ydl.params.get('playlist_items')\n        playlist_start = self.ydl.params.get('playliststart', 1)\n        playlist_end = self.ydl.params.get('playlistend')\n        # For backwards compatibility, interpret -1 as whole list\n        if playlist_end in (-1, None):\n            playlist_end = ''\n        if not playlist_items:\n            playlist_items = f'{playlist_start}:{playlist_end}'\n        elif playlist_start != 1 or playlist_end:\n            self.ydl.report_warning('Ignoring playliststart and playlistend because playlistitems was given', only_once=True)\n\n        for index in self.parse_playlist_items(playlist_items):\n            for i, entry in self[index]:\n                yield i, entry\n                if not entry:\n                    continue\n                try:\n                    # The item may have just been added to archive. Don't break due to it\n                    if not self.ydl.params.get('lazy_playlist'):\n                        # TODO: Add auto-generated fields\n                        self.ydl._match_entry(entry, incomplete=True, silent=True)\n                except (ExistingVideoReached, RejectedVideoReached):\n                    return\n\n    def get_full_count(self):\n        if self.is_exhausted and not self.is_incomplete:\n            return len(self)\n        elif isinstance(self._entries, InAdvancePagedList):\n            if self._entries._pagesize == 1:\n                return self._entries._pagecount\n\n    @functools.cached_property\n    def _getter(self):\n        if isinstance(self._entries, list):\n            def get_entry(i):\n                try:\n                    entry = self._entries[i]\n                except IndexError:\n                    entry = self.MissingEntry\n                    if not self.is_incomplete:\n                        raise self.IndexError()\n                if entry is self.MissingEntry:\n                    raise EntryNotInPlaylist(f'Entry {i + 1} cannot be found')\n                return entry\n        else:\n            def get_entry(i):\n                try:\n                    return type(self.ydl)._handle_extraction_exceptions(lambda _, i: self._entries[i])(self.ydl, i)\n                except (LazyList.IndexError, PagedList.IndexError):\n                    raise self.IndexError()\n        return get_entry\n\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            idx = slice(idx, idx)\n\n        # NB: PlaylistEntries[1:10] => (0, 1, ... 9)\n        step = 1 if idx.step is None else idx.step\n        if idx.start is None:\n            start = 0 if step > 0 else len(self) - 1\n        else:\n            start = idx.start - 1 if idx.start >= 0 else len(self) + idx.start\n\n        # NB: Do not call len(self) when idx == [:]\n        if idx.stop is None:\n            stop = 0 if step < 0 else float('inf')\n        else:\n            stop = idx.stop - 1 if idx.stop >= 0 else len(self) + idx.stop\n        stop += [-1, 1][step > 0]\n\n        for i in frange(start, stop, step):\n            if i < 0:\n                continue\n            try:\n                entry = self._getter(i)\n            except self.IndexError:\n                self.is_exhausted = True\n                if step > 0:\n                    break\n                continue\n            yield i + 1, entry\n\n    def __len__(self):\n        return len(tuple(self[:]))\n\n    class IndexError(IndexError):\n        pass\n\n\ndef uppercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\U[0-9a-fA-F]{8}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)\n\n\ndef lowercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\u[0-9a-fA-F]{4}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)\n\n\ndef escape_rfc3986(s):\n    \"\"\"Escape non-ASCII characters as suggested by RFC 3986\"\"\"\n    return urllib.parse.quote(s, b\"%/;:@&=+$,!~*'()?#[]\")\n\n\ndef escape_url(url):\n    \"\"\"Escape URL as suggested by RFC 3986\"\"\"\n    url_parsed = urllib.parse.urlparse(url)\n    return url_parsed._replace(\n        netloc=url_parsed.netloc.encode('idna').decode('ascii'),\n        path=escape_rfc3986(url_parsed.path),\n        params=escape_rfc3986(url_parsed.params),\n        query=escape_rfc3986(url_parsed.query),\n        fragment=escape_rfc3986(url_parsed.fragment)\n    ).geturl()\n\n\ndef parse_qs(url, **kwargs):\n    return urllib.parse.parse_qs(urllib.parse.urlparse(url).query, **kwargs)\n\n\ndef read_batch_urls(batch_fd):\n    def fixup(url):\n        if not isinstance(url, str):\n            url = url.decode('utf-8', 'replace')\n        BOM_UTF8 = ('\\xef\\xbb\\xbf', '\\ufeff')\n        for bom in BOM_UTF8:\n            if url.startswith(bom):\n                url = url[len(bom):]\n        url = url.lstrip()\n        if not url or url.startswith(('#', ';', ']')):\n            return False\n        # \"#\" cannot be stripped out since it is part of the URI\n        # However, it can be safely stripped out if following a whitespace\n        return re.split(r'\\s#', url, 1)[0].rstrip()\n\n    with contextlib.closing(batch_fd) as fd:\n        return [url for url in map(fixup, fd) if url]\n\n\ndef urlencode_postdata(*args, **kargs):\n    return urllib.parse.urlencode(*args, **kargs).encode('ascii')\n\n\ndef update_url(url, *, query_update=None, **kwargs):\n    \"\"\"Replace URL components specified by kwargs\n       @param url           str or parse url tuple\n       @param query_update  update query\n       @returns             str\n    \"\"\"\n    if isinstance(url, str):\n        if not kwargs and not query_update:\n            return url\n        else:\n            url = urllib.parse.urlparse(url)\n    if query_update:\n        assert 'query' not in kwargs, 'query_update and query cannot be specified at the same time'\n        kwargs['query'] = urllib.parse.urlencode({\n            **urllib.parse.parse_qs(url.query),\n            **query_update\n        }, True)\n    return urllib.parse.urlunparse(url._replace(**kwargs))\n\n\ndef update_url_query(url, query):\n    return update_url(url, query_update=query)\n\n\ndef update_Request(req, url=None, data=None, headers=None, query=None):\n    req_headers = req.headers.copy()\n    req_headers.update(headers or {})\n    req_data = data or req.data\n    req_url = update_url_query(url or req.get_full_url(), query)\n    req_get_method = req.get_method()\n    if req_get_method == 'HEAD':\n        req_type = HEADRequest\n    elif req_get_method == 'PUT':\n        req_type = PUTRequest\n    else:\n        req_type = urllib.request.Request\n    new_req = req_type(\n        req_url, data=req_data, headers=req_headers,\n        origin_req_host=req.origin_req_host, unverifiable=req.unverifiable)\n    if hasattr(req, 'timeout'):\n        new_req.timeout = req.timeout\n    return new_req\n\n\ndef _multipart_encode_impl(data, boundary):\n    content_type = 'multipart/form-data; boundary=%s' % boundary\n\n    out = b''\n    for k, v in data.items():\n        out += b'--' + boundary.encode('ascii') + b'\\r\\n'\n        if isinstance(k, str):\n            k = k.encode()\n        if isinstance(v, str):\n            v = v.encode()\n        # RFC 2047 requires non-ASCII field names to be encoded, while RFC 7578\n        # suggests sending UTF-8 directly. Firefox sends UTF-8, too\n        content = b'Content-Disposition: form-data; name=\"' + k + b'\"\\r\\n\\r\\n' + v + b'\\r\\n'\n        if boundary.encode('ascii') in content:\n            raise ValueError('Boundary overlaps with data')\n        out += content\n\n    out += b'--' + boundary.encode('ascii') + b'--\\r\\n'\n\n    return out, content_type\n\n\ndef multipart_encode(data, boundary=None):\n    '''\n    Encode a dict to RFC 7578-compliant form-data\n\n    data:\n        A dict where keys and values can be either Unicode or bytes-like\n        objects.\n    boundary:\n        If specified a Unicode object, it's used as the boundary. Otherwise\n        a random boundary is generated.\n\n    Reference: https://tools.ietf.org/html/rfc7578\n    '''\n    has_specified_boundary = boundary is not None\n\n    while True:\n        if boundary is None:\n            boundary = '---------------' + str(random.randrange(0x0fffffff, 0xffffffff))\n\n        try:\n            out, content_type = _multipart_encode_impl(data, boundary)\n            break\n        except ValueError:\n            if has_specified_boundary:\n                raise\n            boundary = None\n\n    return out, content_type\n\n\ndef is_iterable_like(x, allowed_types=collections.abc.Iterable, blocked_types=NO_DEFAULT):\n    if blocked_types is NO_DEFAULT:\n        blocked_types = (str, bytes, collections.abc.Mapping)\n    return isinstance(x, allowed_types) and not isinstance(x, blocked_types)\n\n\ndef variadic(x, allowed_types=NO_DEFAULT):\n    if not isinstance(allowed_types, (tuple, type)):\n        deprecation_warning('allowed_types should be a tuple or a type')\n        allowed_types = tuple(allowed_types)\n    return x if is_iterable_like(x, blocked_types=allowed_types) else (x, )\n\n\ndef try_call(*funcs, expected_type=None, args=[], kwargs={}):\n    for f in funcs:\n        try:\n            val = f(*args, **kwargs)\n        except (AttributeError, KeyError, TypeError, IndexError, ValueError, ZeroDivisionError):\n            pass\n        else:\n            if expected_type is None or isinstance(val, expected_type):\n                return val\n\n\ndef try_get(src, getter, expected_type=None):\n    return try_call(*variadic(getter), args=(src,), expected_type=expected_type)\n\n\ndef filter_dict(dct, cndn=lambda _, v: v is not None):\n    return {k: v for k, v in dct.items() if cndn(k, v)}\n\n\ndef merge_dicts(*dicts):\n    merged = {}\n    for a_dict in dicts:\n        for k, v in a_dict.items():\n            if (v is not None and k not in merged\n                    or isinstance(v, str) and merged[k] == ''):\n                merged[k] = v\n    return merged\n\n\ndef encode_compat_str(string, encoding=preferredencoding(), errors='strict'):\n    return string if isinstance(string, str) else str(string, encoding, errors)\n\n\nUS_RATINGS = {\n    'G': 0,\n    'PG': 10,\n    'PG-13': 13,\n    'R': 16,\n    'NC': 18,\n}\n\n\nTV_PARENTAL_GUIDELINES = {\n    'TV-Y': 0,\n    'TV-Y7': 7,\n    'TV-G': 0,\n    'TV-PG': 0,\n    'TV-14': 14,\n    'TV-MA': 17,\n}\n\n\ndef parse_age_limit(s):\n    # isinstance(False, int) is True. So type() must be used instead\n    if type(s) is int:  # noqa: E721\n        return s if 0 <= s <= 21 else None\n    elif not isinstance(s, str):\n        return None\n    m = re.match(r'^(?P<age>\\d{1,2})\\+?$', s)\n    if m:\n        return int(m.group('age'))\n    s = s.upper()\n    if s in US_RATINGS:\n        return US_RATINGS[s]\n    m = re.match(r'^TV[_-]?(%s)$' % '|'.join(k[3:] for k in TV_PARENTAL_GUIDELINES), s)\n    if m:\n        return TV_PARENTAL_GUIDELINES['TV-' + m.group(1)]\n    return None\n\n\ndef strip_jsonp(code):\n    return re.sub(\n        r'''(?sx)^\n            (?:window\\.)?(?P<func_name>[a-zA-Z0-9_.$]*)\n            (?:\\s*&&\\s*(?P=func_name))?\n            \\s*\\(\\s*(?P<callback_data>.*)\\);?\n            \\s*?(?://[^\\n]*)*$''',\n        r'\\g<callback_data>', code)\n\n\ndef js_to_json(code, vars={}, *, strict=False):\n    # vars is a dict of var, val pairs to substitute\n    STRING_QUOTES = '\\'\"`'\n    STRING_RE = '|'.join(rf'{q}(?:\\\\.|[^\\\\{q}])*{q}' for q in STRING_QUOTES)\n    COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*\\n'\n    SKIP_RE = fr'\\s*(?:{COMMENT_RE})?\\s*'\n    INTEGER_TABLE = (\n        (fr'(?s)^(0[xX][0-9a-fA-F]+){SKIP_RE}:?$', 16),\n        (fr'(?s)^(0+[0-7]+){SKIP_RE}:?$', 8),\n    )\n\n    def process_escape(match):\n        JSON_PASSTHROUGH_ESCAPES = R'\"\\bfnrtu'\n        escape = match.group(1) or match.group(2)\n\n        return (Rf'\\{escape}' if escape in JSON_PASSTHROUGH_ESCAPES\n                else R'\\u00' if escape == 'x'\n                else '' if escape == '\\n'\n                else escape)\n\n    def template_substitute(match):\n        evaluated = js_to_json(match.group(1), vars, strict=strict)\n        if evaluated[0] == '\"':\n            return json.loads(evaluated)\n        return evaluated\n\n    def fix_kv(m):\n        v = m.group(0)\n        if v in ('true', 'false', 'null'):\n            return v\n        elif v in ('undefined', 'void 0'):\n            return 'null'\n        elif v.startswith('/*') or v.startswith('//') or v.startswith('!') or v == ',':\n            return ''\n\n        if v[0] in STRING_QUOTES:\n            v = re.sub(r'(?s)\\${([^}]+)}', template_substitute, v[1:-1]) if v[0] == '`' else v[1:-1]\n            escaped = re.sub(r'(?s)(\")|\\\\(.)', process_escape, v)\n            return f'\"{escaped}\"'\n\n        for regex, base in INTEGER_TABLE:\n            im = re.match(regex, v)\n            if im:\n                i = int(im.group(1), base)\n                return f'\"{i}\":' if v.endswith(':') else str(i)\n\n        if v in vars:\n            try:\n                if not strict:\n                    json.loads(vars[v])\n            except json.JSONDecodeError:\n                return json.dumps(vars[v])\n            else:\n                return vars[v]\n\n        if not strict:\n            return f'\"{v}\"'\n\n        raise ValueError(f'Unknown value: {v}')\n\n    def create_map(mobj):\n        return json.dumps(dict(json.loads(js_to_json(mobj.group(1) or '[]', vars=vars))))\n\n    code = re.sub(r'new Map\\((\\[.*?\\])?\\)', create_map, code)\n    if not strict:\n        code = re.sub(r'new Date\\((\".+\")\\)', r'\\g<1>', code)\n        code = re.sub(r'new \\w+\\((.*?)\\)', lambda m: json.dumps(m.group(0)), code)\n        code = re.sub(r'parseInt\\([^\\d]+(\\d+)[^\\d]+\\)', r'\\1', code)\n        code = re.sub(r'\\(function\\([^)]*\\)\\s*\\{[^}]*\\}\\s*\\)\\s*\\(\\s*([\"\\'][^)]*[\"\\'])\\s*\\)', r'\\1', code)\n\n    return re.sub(rf'''(?sx)\n        {STRING_RE}|\n        {COMMENT_RE}|,(?={SKIP_RE}[\\]}}])|\n        void\\s0|(?:(?<![0-9])[eE]|[a-df-zA-DF-Z_$])[.a-zA-Z_$0-9]*|\n        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{SKIP_RE}:)?|\n        [0-9]+(?={SKIP_RE}:)|\n        !+\n        ''', fix_kv, code)\n\n\ndef qualities(quality_ids):\n    \"\"\" Get a numeric quality value out of a list of possible values \"\"\"\n    def q(qid):\n        try:\n            return quality_ids.index(qid)\n        except ValueError:\n            return -1\n    return q\n\n\nPOSTPROCESS_WHEN = ('pre_process', 'after_filter', 'video', 'before_dl', 'post_process', 'after_move', 'after_video', 'playlist')\n\n\nDEFAULT_OUTTMPL = {\n    'default': '%(title)s [%(id)s].%(ext)s',\n    'chapter': '%(title)s - %(section_number)03d %(section_title)s [%(id)s].%(ext)s',\n}\nOUTTMPL_TYPES = {\n    'chapter': None,\n    'subtitle': None,\n    'thumbnail': None,\n    'description': 'description',\n    'annotation': 'annotations.xml',\n    'infojson': 'info.json',\n    'link': None,\n    'pl_video': None,\n    'pl_thumbnail': None,\n    'pl_description': 'description',\n    'pl_infojson': 'info.json',\n}\n\n# As of [1] format syntax is:\n#  %[mapping_key][conversion_flags][minimum_width][.precision][length_modifier]type\n# 1. https://docs.python.org/2/library/stdtypes.html#string-formatting\nSTR_FORMAT_RE_TMPL = r'''(?x)\n    (?<!%)(?P<prefix>(?:%%)*)\n    %\n    (?P<has_key>\\((?P<key>{0})\\))?\n    (?P<format>\n        (?P<conversion>[#0\\-+ ]+)?\n        (?P<min_width>\\d+)?\n        (?P<precision>\\.\\d+)?\n        (?P<len_mod>[hlL])?  # unused in python\n        {1}  # conversion type\n    )\n'''\n\n\nSTR_FORMAT_TYPES = 'diouxXeEfFgGcrsa'\n\n\ndef limit_length(s, length):\n    \"\"\" Add ellipses to overly long strings \"\"\"\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s\n\n\ndef version_tuple(v):\n    return tuple(int(e) for e in re.split(r'[-.]', v))\n\n\ndef is_outdated_version(version, limit, assume_new=True):\n    if not version:\n        return not assume_new\n    try:\n        return version_tuple(version) < version_tuple(limit)\n    except ValueError:\n        return not assume_new\n\n\ndef ytdl_is_updateable():\n    \"\"\" Returns if yt-dlp can be updated with -U \"\"\"\n\n    from ..update import is_non_updateable\n\n    return not is_non_updateable()\n\n\ndef args_to_str(args):\n    # Get a short string representation for a subprocess command\n    return ' '.join(compat_shlex_quote(a) for a in args)\n\n\ndef error_to_str(err):\n    return f'{type(err).__name__}: {err}'\n\n\ndef mimetype2ext(mt, default=NO_DEFAULT):\n    if not isinstance(mt, str):\n        if default is not NO_DEFAULT:\n            return default\n        return None\n\n    MAP = {\n        # video\n        '3gpp': '3gp',\n        'mp2t': 'ts',\n        'mp4': 'mp4',\n        'mpeg': 'mpeg',\n        'mpegurl': 'm3u8',\n        'quicktime': 'mov',\n        'webm': 'webm',\n        'vp9': 'vp9',\n        'x-flv': 'flv',\n        'x-m4v': 'm4v',\n        'x-matroska': 'mkv',\n        'x-mng': 'mng',\n        'x-mp4-fragmented': 'mp4',\n        'x-ms-asf': 'asf',\n        'x-ms-wmv': 'wmv',\n        'x-msvideo': 'avi',\n\n        # application (streaming playlists)\n        'dash+xml': 'mpd',\n        'f4m+xml': 'f4m',\n        'hds+xml': 'f4m',\n        'vnd.apple.mpegurl': 'm3u8',\n        'vnd.ms-sstr+xml': 'ism',\n        'x-mpegurl': 'm3u8',\n\n        # audio\n        'audio/mp4': 'm4a',\n        # Per RFC 3003, audio/mpeg can be .mp1, .mp2 or .mp3.\n        # Using .mp3 as it's the most popular one\n        'audio/mpeg': 'mp3',\n        'audio/webm': 'webm',\n        'audio/x-matroska': 'mka',\n        'audio/x-mpegurl': 'm3u',\n        'midi': 'mid',\n        'ogg': 'ogg',\n        'wav': 'wav',\n        'wave': 'wav',\n        'x-aac': 'aac',\n        'x-flac': 'flac',\n        'x-m4a': 'm4a',\n        'x-realaudio': 'ra',\n        'x-wav': 'wav',\n\n        # image\n        'avif': 'avif',\n        'bmp': 'bmp',\n        'gif': 'gif',\n        'jpeg': 'jpg',\n        'png': 'png',\n        'svg+xml': 'svg',\n        'tiff': 'tif',\n        'vnd.wap.wbmp': 'wbmp',\n        'webp': 'webp',\n        'x-icon': 'ico',\n        'x-jng': 'jng',\n        'x-ms-bmp': 'bmp',\n\n        # caption\n        'filmstrip+json': 'fs',\n        'smptett+xml': 'tt',\n        'ttaf+xml': 'dfxp',\n        'ttml+xml': 'ttml',\n        'x-ms-sami': 'sami',\n\n        # misc\n        'gzip': 'gz',\n        'json': 'json',\n        'xml': 'xml',\n        'zip': 'zip',\n    }\n\n    mimetype = mt.partition(';')[0].strip().lower()\n    _, _, subtype = mimetype.rpartition('/')\n\n    ext = traversal.traverse_obj(MAP, mimetype, subtype, subtype.rsplit('+')[-1])\n    if ext:\n        return ext\n    elif default is not NO_DEFAULT:\n        return default\n    return subtype.replace('+', '.')\n\n\ndef ext2mimetype(ext_or_url):\n    if not ext_or_url:\n        return None\n    if '.' not in ext_or_url:\n        ext_or_url = f'file.{ext_or_url}'\n    return mimetypes.guess_type(ext_or_url)[0]\n\n\ndef parse_codecs(codecs_str):\n    # http://tools.ietf.org/html/rfc6381\n    if not codecs_str:\n        return {}\n    split_codecs = list(filter(None, map(\n        str.strip, codecs_str.strip().strip(',').split(','))))\n    vcodec, acodec, scodec, hdr = None, None, None, None\n    for full_codec in split_codecs:\n        parts = re.sub(r'0+(?=\\d)', '', full_codec).split('.')\n        if parts[0] in ('avc1', 'avc2', 'avc3', 'avc4', 'vp9', 'vp8', 'hev1', 'hev2',\n                        'h263', 'h264', 'mp4v', 'hvc1', 'av1', 'theora', 'dvh1', 'dvhe'):\n            if vcodec:\n                continue\n            vcodec = full_codec\n            if parts[0] in ('dvh1', 'dvhe'):\n                hdr = 'DV'\n            elif parts[0] == 'av1' and traversal.traverse_obj(parts, 3) == '10':\n                hdr = 'HDR10'\n            elif parts[:2] == ['vp9', '2']:\n                hdr = 'HDR10'\n        elif parts[0] in ('flac', 'mp4a', 'opus', 'vorbis', 'mp3', 'aac', 'ac-4',\n                          'ac-3', 'ec-3', 'eac3', 'dtsc', 'dtse', 'dtsh', 'dtsl'):\n            acodec = acodec or full_codec\n        elif parts[0] in ('stpp', 'wvtt'):\n            scodec = scodec or full_codec\n        else:\n            write_string(f'WARNING: Unknown codec {full_codec}\\n')\n    if vcodec or acodec or scodec:\n        return {\n            'vcodec': vcodec or 'none',\n            'acodec': acodec or 'none',\n            'dynamic_range': hdr,\n            **({'scodec': scodec} if scodec is not None else {}),\n        }\n    elif len(split_codecs) == 2:\n        return {\n            'vcodec': split_codecs[0],\n            'acodec': split_codecs[1],\n        }\n    return {}\n\n\ndef get_compatible_ext(*, vcodecs, acodecs, vexts, aexts, preferences=None):\n    assert len(vcodecs) == len(vexts) and len(acodecs) == len(aexts)\n\n    allow_mkv = not preferences or 'mkv' in preferences\n\n    if allow_mkv and max(len(acodecs), len(vcodecs)) > 1:\n        return 'mkv'  # TODO: any other format allows this?\n\n    # TODO: All codecs supported by parse_codecs isn't handled here\n    COMPATIBLE_CODECS = {\n        'mp4': {\n            'av1', 'hevc', 'avc1', 'mp4a', 'ac-4',  # fourcc (m3u8, mpd)\n            'h264', 'aacl', 'ec-3',  # Set in ISM\n        },\n        'webm': {\n            'av1', 'vp9', 'vp8', 'opus', 'vrbs',\n            'vp9x', 'vp8x',  # in the webm spec\n        },\n    }\n\n    sanitize_codec = functools.partial(\n        try_get, getter=lambda x: x[0].split('.')[0].replace('0', '').lower())\n    vcodec, acodec = sanitize_codec(vcodecs), sanitize_codec(acodecs)\n\n    for ext in preferences or COMPATIBLE_CODECS.keys():\n        codec_set = COMPATIBLE_CODECS.get(ext, set())\n        if ext == 'mkv' or codec_set.issuperset((vcodec, acodec)):\n            return ext\n\n    COMPATIBLE_EXTS = (\n        {'mp3', 'mp4', 'm4a', 'm4p', 'm4b', 'm4r', 'm4v', 'ismv', 'isma', 'mov'},\n        {'webm', 'weba'},\n    )\n    for ext in preferences or vexts:\n        current_exts = {ext, *vexts, *aexts}\n        if ext == 'mkv' or current_exts == {ext} or any(\n                ext_sets.issuperset(current_exts) for ext_sets in COMPATIBLE_EXTS):\n            return ext\n    return 'mkv' if allow_mkv else preferences[-1]\n\n\ndef urlhandle_detect_ext(url_handle, default=NO_DEFAULT):\n    getheader = url_handle.headers.get\n\n    cd = getheader('Content-Disposition')\n    if cd:\n        m = re.match(r'attachment;\\s*filename=\"(?P<filename>[^\"]+)\"', cd)\n        if m:\n            e = determine_ext(m.group('filename'), default_ext=None)\n            if e:\n                return e\n\n    meta_ext = getheader('x-amz-meta-name')\n    if meta_ext:\n        e = meta_ext.rpartition('.')[2]\n        if e:\n            return e\n\n    return mimetype2ext(getheader('Content-Type'), default=default)\n\n\ndef encode_data_uri(data, mime_type):\n    return 'data:%s;base64,%s' % (mime_type, base64.b64encode(data).decode('ascii'))\n\n\ndef age_restricted(content_limit, age_limit):\n    \"\"\" Returns True iff the content should be blocked \"\"\"\n\n    if age_limit is None:  # No limit set\n        return False\n    if content_limit is None:\n        return False  # Content available for everyone\n    return age_limit < content_limit\n\n\n# List of known byte-order-marks (BOM)\nBOMS = [\n    (b'\\xef\\xbb\\xbf', 'utf-8'),\n    (b'\\x00\\x00\\xfe\\xff', 'utf-32-be'),\n    (b'\\xff\\xfe\\x00\\x00', 'utf-32-le'),\n    (b'\\xff\\xfe', 'utf-16-le'),\n    (b'\\xfe\\xff', 'utf-16-be'),\n]\n\n\ndef is_html(first_bytes):\n    \"\"\" Detect whether a file contains HTML by examining its first bytes. \"\"\"\n\n    encoding = 'utf-8'\n    for bom, enc in BOMS:\n        while first_bytes.startswith(bom):\n            encoding, first_bytes = enc, first_bytes[len(bom):]\n\n    return re.match(r'^\\s*<', first_bytes.decode(encoding, 'replace'))\n\n\ndef determine_protocol(info_dict):\n    protocol = info_dict.get('protocol')\n    if protocol is not None:\n        return protocol\n\n    url = sanitize_url(info_dict['url'])\n    if url.startswith('rtmp'):\n        return 'rtmp'\n    elif url.startswith('mms'):\n        return 'mms'\n    elif url.startswith('rtsp'):\n        return 'rtsp'\n\n    ext = determine_ext(url)\n    if ext == 'm3u8':\n        return 'm3u8' if info_dict.get('is_live') else 'm3u8_native'\n    elif ext == 'f4m':\n        return 'f4m'\n\n    return urllib.parse.urlparse(url).scheme\n\n\ndef render_table(header_row, data, delim=False, extra_gap=0, hide_empty=False):\n    \"\"\" Render a list of rows, each as a list of values.\n    Text after a \\t will be right aligned \"\"\"\n    def width(string):\n        return len(remove_terminal_sequences(string).replace('\\t', ''))\n\n    def get_max_lens(table):\n        return [max(width(str(v)) for v in col) for col in zip(*table)]\n\n    def filter_using_list(row, filterArray):\n        return [col for take, col in itertools.zip_longest(filterArray, row, fillvalue=True) if take]\n\n    max_lens = get_max_lens(data) if hide_empty else []\n    header_row = filter_using_list(header_row, max_lens)\n    data = [filter_using_list(row, max_lens) for row in data]\n\n    table = [header_row] + data\n    max_lens = get_max_lens(table)\n    extra_gap += 1\n    if delim:\n        table = [header_row, [delim * (ml + extra_gap) for ml in max_lens]] + data\n        table[1][-1] = table[1][-1][:-extra_gap * len(delim)]  # Remove extra_gap from end of delimiter\n    for row in table:\n        for pos, text in enumerate(map(str, row)):\n            if '\\t' in text:\n                row[pos] = text.replace('\\t', ' ' * (max_lens[pos] - width(text))) + ' ' * extra_gap\n            else:\n                row[pos] = text + ' ' * (max_lens[pos] - width(text) + extra_gap)\n    ret = '\\n'.join(''.join(row).rstrip() for row in table)\n    return ret\n\n\ndef _match_one(filter_part, dct, incomplete):\n    # TODO: Generalize code with YoutubeDL._build_format_filter\n    STRING_OPERATORS = {\n        '*=': operator.contains,\n        '^=': lambda attr, value: attr.startswith(value),\n        '$=': lambda attr, value: attr.endswith(value),\n        '~=': lambda attr, value: re.search(value, attr),\n    }\n    COMPARISON_OPERATORS = {\n        **STRING_OPERATORS,\n        '<=': operator.le,  # \"<=\" must be defined above \"<\"\n        '<': operator.lt,\n        '>=': operator.ge,\n        '>': operator.gt,\n        '=': operator.eq,\n    }\n\n    if isinstance(incomplete, bool):\n        is_incomplete = lambda _: incomplete\n    else:\n        is_incomplete = lambda k: k in incomplete\n\n    operator_rex = re.compile(r'''(?x)\n        (?P<key>[a-z_]+)\n        \\s*(?P<negation>!\\s*)?(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n        (?:\n            (?P<quote>[\"\\'])(?P<quotedstrval>.+?)(?P=quote)|\n            (?P<strval>.+?)\n        )\n        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))\n    m = operator_rex.fullmatch(filter_part.strip())\n    if m:\n        m = m.groupdict()\n        unnegated_op = COMPARISON_OPERATORS[m['op']]\n        if m['negation']:\n            op = lambda attr, value: not unnegated_op(attr, value)\n        else:\n            op = unnegated_op\n        comparison_value = m['quotedstrval'] or m['strval'] or m['intval']\n        if m['quote']:\n            comparison_value = comparison_value.replace(r'\\%s' % m['quote'], m['quote'])\n        actual_value = dct.get(m['key'])\n        numeric_comparison = None\n        if isinstance(actual_value, (int, float)):\n            # If the original field is a string and matching comparisonvalue is\n            # a number we should respect the origin of the original field\n            # and process comparison value as a string (see\n            # https://github.com/ytdl-org/youtube-dl/issues/11082)\n            try:\n                numeric_comparison = int(comparison_value)\n            except ValueError:\n                numeric_comparison = parse_filesize(comparison_value)\n                if numeric_comparison is None:\n                    numeric_comparison = parse_filesize(f'{comparison_value}B')\n                if numeric_comparison is None:\n                    numeric_comparison = parse_duration(comparison_value)\n        if numeric_comparison is not None and m['op'] in STRING_OPERATORS:\n            raise ValueError('Operator %s only supports string values!' % m['op'])\n        if actual_value is None:\n            return is_incomplete(m['key']) or m['none_inclusive']\n        return op(actual_value, comparison_value if numeric_comparison is None else numeric_comparison)\n\n    UNARY_OPERATORS = {\n        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),\n        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),\n    }\n    operator_rex = re.compile(r'''(?x)\n        (?P<op>%s)\\s*(?P<key>[a-z_]+)\n        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))\n    m = operator_rex.fullmatch(filter_part.strip())\n    if m:\n        op = UNARY_OPERATORS[m.group('op')]\n        actual_value = dct.get(m.group('key'))\n        if is_incomplete(m.group('key')) and actual_value is None:\n            return True\n        return op(actual_value)\n\n    raise ValueError('Invalid filter part %r' % filter_part)\n\n\ndef match_str(filter_str, dct, incomplete=False):\n    \"\"\" Filter a dictionary with a simple string syntax.\n    @returns           Whether the filter passes\n    @param incomplete  Set of keys that is expected to be missing from dct.\n                       Can be True/False to indicate all/none of the keys may be missing.\n                       All conditions on incomplete keys pass if the key is missing\n    \"\"\"\n    return all(\n        _match_one(filter_part.replace(r'\\&', '&'), dct, incomplete)\n        for filter_part in re.split(r'(?<!\\\\)&', filter_str))\n\n\ndef match_filter_func(filters, breaking_filters=None):\n    if not filters and not breaking_filters:\n        return None\n    breaking_filters = match_filter_func(breaking_filters) or (lambda _, __: None)\n    filters = set(variadic(filters or []))\n\n    interactive = '-' in filters\n    if interactive:\n        filters.remove('-')\n\n    def _match_func(info_dict, incomplete=False):\n        ret = breaking_filters(info_dict, incomplete)\n        if ret is not None:\n            raise RejectedVideoReached(ret)\n\n        if not filters or any(match_str(f, info_dict, incomplete) for f in filters):\n            return NO_DEFAULT if interactive and not incomplete else None\n        else:\n            video_title = info_dict.get('title') or info_dict.get('id') or 'entry'\n            filter_str = ') | ('.join(map(str.strip, filters))\n            return f'{video_title} does not pass filter ({filter_str}), skipping ..'\n    return _match_func\n\n\nclass download_range_func:\n    def __init__(self, chapters, ranges, from_info=False):\n        self.chapters, self.ranges, self.from_info = chapters, ranges, from_info\n\n    def __call__(self, info_dict, ydl):\n\n        warning = ('There are no chapters matching the regex' if info_dict.get('chapters')\n                   else 'Cannot match chapters since chapter information is unavailable')\n        for regex in self.chapters or []:\n            for i, chapter in enumerate(info_dict.get('chapters') or []):\n                if re.search(regex, chapter['title']):\n                    warning = None\n                    yield {**chapter, 'index': i}\n        if self.chapters and warning:\n            ydl.to_screen(f'[info] {info_dict[\"id\"]}: {warning}')\n\n        for start, end in self.ranges or []:\n            yield {\n                'start_time': self._handle_negative_timestamp(start, info_dict),\n                'end_time': self._handle_negative_timestamp(end, info_dict),\n            }\n\n        if self.from_info and (info_dict.get('start_time') or info_dict.get('end_time')):\n            yield {\n                'start_time': info_dict.get('start_time') or 0,\n                'end_time': info_dict.get('end_time') or float('inf'),\n            }\n        elif not self.ranges and not self.chapters:\n            yield {}\n\n    @staticmethod\n    def _handle_negative_timestamp(time, info):\n        return max(info['duration'] + time, 0) if info.get('duration') and time < 0 else time\n\n    def __eq__(self, other):\n        return (isinstance(other, download_range_func)\n                and self.chapters == other.chapters and self.ranges == other.ranges)\n\n    def __repr__(self):\n        return f'{__name__}.{type(self).__name__}({self.chapters}, {self.ranges})'\n\n\ndef parse_dfxp_time_expr(time_expr):\n    if not time_expr:\n        return\n\n    mobj = re.match(rf'^(?P<time_offset>{NUMBER_RE})s?$', time_expr)\n    if mobj:\n        return float(mobj.group('time_offset'))\n\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)\n    if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))\n\n\ndef srt_subtitles_timecode(seconds):\n    return '%02d:%02d:%02d,%03d' % timetuple_from_msec(seconds * 1000)\n\n\ndef ass_subtitles_timecode(seconds):\n    time = timetuple_from_msec(seconds * 1000)\n    return '%01d:%02d:%02d.%02d' % (*time[:-1], time.milliseconds / 10)\n\n\ndef dfxp2srt(dfxp_data):\n    '''\n    @param dfxp_data A bytes-like object containing DFXP data\n    @returns A unicode object containing converted SRT data\n    '''\n    LEGACY_NAMESPACES = (\n        (b'http://www.w3.org/ns/ttml', [\n            b'http://www.w3.org/2004/11/ttaf1',\n            b'http://www.w3.org/2006/04/ttaf1',\n            b'http://www.w3.org/2006/10/ttaf1',\n        ]),\n        (b'http://www.w3.org/ns/ttml#styling', [\n            b'http://www.w3.org/ns/ttml#style',\n        ]),\n    )\n\n    SUPPORTED_STYLING = [\n        'color',\n        'fontFamily',\n        'fontSize',\n        'fontStyle',\n        'fontWeight',\n        'textDecoration'\n    ]\n\n    _x = functools.partial(xpath_with_ns, ns_map={\n        'xml': 'http://www.w3.org/XML/1998/namespace',\n        'ttml': 'http://www.w3.org/ns/ttml',\n        'tts': 'http://www.w3.org/ns/ttml#styling',\n    })\n\n    styles = {}\n    default_style = {}\n\n    class TTMLPElementParser:\n        _out = ''\n        _unclosed_elements = []\n        _applied_styles = []\n\n        def start(self, tag, attrib):\n            if tag in (_x('ttml:br'), 'br'):\n                self._out += '\\n'\n            else:\n                unclosed_elements = []\n                style = {}\n                element_style_id = attrib.get('style')\n                if default_style:\n                    style.update(default_style)\n                if element_style_id:\n                    style.update(styles.get(element_style_id, {}))\n                for prop in SUPPORTED_STYLING:\n                    prop_val = attrib.get(_x('tts:' + prop))\n                    if prop_val:\n                        style[prop] = prop_val\n                if style:\n                    font = ''\n                    for k, v in sorted(style.items()):\n                        if self._applied_styles and self._applied_styles[-1].get(k) == v:\n                            continue\n                        if k == 'color':\n                            font += ' color=\"%s\"' % v\n                        elif k == 'fontSize':\n                            font += ' size=\"%s\"' % v\n                        elif k == 'fontFamily':\n                            font += ' face=\"%s\"' % v\n                        elif k == 'fontWeight' and v == 'bold':\n                            self._out += '<b>'\n                            unclosed_elements.append('b')\n                        elif k == 'fontStyle' and v == 'italic':\n                            self._out += '<i>'\n                            unclosed_elements.append('i')\n                        elif k == 'textDecoration' and v == 'underline':\n                            self._out += '<u>'\n                            unclosed_elements.append('u')\n                    if font:\n                        self._out += '<font' + font + '>'\n                        unclosed_elements.append('font')\n                    applied_style = {}\n                    if self._applied_styles:\n                        applied_style.update(self._applied_styles[-1])\n                    applied_style.update(style)\n                    self._applied_styles.append(applied_style)\n                self._unclosed_elements.append(unclosed_elements)\n\n        def end(self, tag):\n            if tag not in (_x('ttml:br'), 'br'):\n                unclosed_elements = self._unclosed_elements.pop()\n                for element in reversed(unclosed_elements):\n                    self._out += '</%s>' % element\n                if unclosed_elements and self._applied_styles:\n                    self._applied_styles.pop()\n\n        def data(self, data):\n            self._out += data\n\n        def close(self):\n            return self._out.strip()\n\n    # Fix UTF-8 encoded file wrongly marked as UTF-16. See https://github.com/yt-dlp/yt-dlp/issues/6543#issuecomment-1477169870\n    # This will not trigger false positives since only UTF-8 text is being replaced\n    dfxp_data = dfxp_data.replace(b'encoding=\\'UTF-16\\'', b'encoding=\\'UTF-8\\'')\n\n    def parse_node(node):\n        target = TTMLPElementParser()\n        parser = xml.etree.ElementTree.XMLParser(target=target)\n        parser.feed(xml.etree.ElementTree.tostring(node))\n        return parser.close()\n\n    for k, v in LEGACY_NAMESPACES:\n        for ns in v:\n            dfxp_data = dfxp_data.replace(ns, k)\n\n    dfxp = compat_etree_fromstring(dfxp_data)\n    out = []\n    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')\n\n    if not paras:\n        raise ValueError('Invalid dfxp/TTML subtitle')\n\n    repeat = False\n    while True:\n        for style in dfxp.findall(_x('.//ttml:style')):\n            style_id = style.get('id') or style.get(_x('xml:id'))\n            if not style_id:\n                continue\n            parent_style_id = style.get('style')\n            if parent_style_id:\n                if parent_style_id not in styles:\n                    repeat = True\n                    continue\n                styles[style_id] = styles[parent_style_id].copy()\n            for prop in SUPPORTED_STYLING:\n                prop_val = style.get(_x('tts:' + prop))\n                if prop_val:\n                    styles.setdefault(style_id, {})[prop] = prop_val\n        if repeat:\n            repeat = False\n        else:\n            break\n\n    for p in ('body', 'div'):\n        ele = xpath_element(dfxp, [_x('.//ttml:' + p), './/' + p])\n        if ele is None:\n            continue\n        style = styles.get(ele.get('style'))\n        if not style:\n            continue\n        default_style.update(style)\n\n    for para, index in zip(paras, itertools.count(1)):\n        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))\n        end_time = parse_dfxp_time_expr(para.attrib.get('end'))\n        dur = parse_dfxp_time_expr(para.attrib.get('dur'))\n        if begin_time is None:\n            continue\n        if not end_time:\n            if not dur:\n                continue\n            end_time = begin_time + dur\n        out.append('%d\\n%s --> %s\\n%s\\n\\n' % (\n            index,\n            srt_subtitles_timecode(begin_time),\n            srt_subtitles_timecode(end_time),\n            parse_node(para)))\n\n    return ''.join(out)\n\n\ndef cli_option(params, command_option, param, separator=None):\n    param = params.get(param)\n    return ([] if param is None\n            else [command_option, str(param)] if separator is None\n            else [f'{command_option}{separator}{param}'])\n\n\ndef cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n    param = params.get(param)\n    assert param in (True, False, None)\n    return cli_option({True: true_value, False: false_value}, command_option, param, separator)\n\n\ndef cli_valueless_option(params, command_option, param, expected_value=True):\n    return [command_option] if params.get(param) == expected_value else []\n\n\ndef cli_configuration_args(argdict, keys, default=[], use_compat=True):\n    if isinstance(argdict, (list, tuple)):  # for backward compatibility\n        if use_compat:\n            return argdict\n        else:\n            argdict = None\n    if argdict is None:\n        return default\n    assert isinstance(argdict, dict)\n\n    assert isinstance(keys, (list, tuple))\n    for key_list in keys:\n        arg_list = list(filter(\n            lambda x: x is not None,\n            [argdict.get(key.lower()) for key in variadic(key_list)]))\n        if arg_list:\n            return [arg for args in arg_list for arg in args]\n    return default\n\n\ndef _configuration_args(main_key, argdict, exe, keys=None, default=[], use_compat=True):\n    main_key, exe = main_key.lower(), exe.lower()\n    root_key = exe if main_key == exe else f'{main_key}+{exe}'\n    keys = [f'{root_key}{k}' for k in (keys or [''])]\n    if root_key in keys:\n        if main_key != exe:\n            keys.append((main_key, exe))\n        keys.append('default')\n    else:\n        use_compat = False\n    return cli_configuration_args(argdict, keys, default, use_compat)\n\n\nclass ISO639Utils:\n    # See http://www.loc.gov/standards/iso639-2/ISO-639-2_utf-8.txt\n    _lang_map = {\n        'aa': 'aar',\n        'ab': 'abk',\n        'ae': 'ave',\n        'af': 'afr',\n        'ak': 'aka',\n        'am': 'amh',\n        'an': 'arg',\n        'ar': 'ara',\n        'as': 'asm',\n        'av': 'ava',\n        'ay': 'aym',\n        'az': 'aze',\n        'ba': 'bak',\n        'be': 'bel',\n        'bg': 'bul',\n        'bh': 'bih',\n        'bi': 'bis',\n        'bm': 'bam',\n        'bn': 'ben',\n        'bo': 'bod',\n        'br': 'bre',\n        'bs': 'bos',\n        'ca': 'cat',\n        'ce': 'che',\n        'ch': 'cha',\n        'co': 'cos',\n        'cr': 'cre',\n        'cs': 'ces',\n        'cu': 'chu',\n        'cv': 'chv',\n        'cy': 'cym',\n        'da': 'dan',\n        'de': 'deu',\n        'dv': 'div',\n        'dz': 'dzo',\n        'ee': 'ewe',\n        'el': 'ell',\n        'en': 'eng',\n        'eo': 'epo',\n        'es': 'spa',\n        'et': 'est',\n        'eu': 'eus',\n        'fa': 'fas',\n        'ff': 'ful',\n        'fi': 'fin',\n        'fj': 'fij',\n        'fo': 'fao',\n        'fr': 'fra',\n        'fy': 'fry',\n        'ga': 'gle',\n        'gd': 'gla',\n        'gl': 'glg',\n        'gn': 'grn',\n        'gu': 'guj',\n        'gv': 'glv',\n        'ha': 'hau',\n        'he': 'heb',\n        'iw': 'heb',  # Replaced by he in 1989 revision\n        'hi': 'hin',\n        'ho': 'hmo',\n        'hr': 'hrv',\n        'ht': 'hat',\n        'hu': 'hun',\n        'hy': 'hye',\n        'hz': 'her',\n        'ia': 'ina',\n        'id': 'ind',\n        'in': 'ind',  # Replaced by id in 1989 revision\n        'ie': 'ile',\n        'ig': 'ibo',\n        'ii': 'iii',\n        'ik': 'ipk',\n        'io': 'ido',\n        'is': 'isl',\n        'it': 'ita',\n        'iu': 'iku',\n        'ja': 'jpn',\n        'jv': 'jav',\n        'ka': 'kat',\n        'kg': 'kon',\n        'ki': 'kik',\n        'kj': 'kua',\n        'kk': 'kaz',\n        'kl': 'kal',\n        'km': 'khm',\n        'kn': 'kan',\n        'ko': 'kor',\n        'kr': 'kau',\n        'ks': 'kas',\n        'ku': 'kur',\n        'kv': 'kom',\n        'kw': 'cor',\n        'ky': 'kir',\n        'la': 'lat',\n        'lb': 'ltz',\n        'lg': 'lug',\n        'li': 'lim',\n        'ln': 'lin',\n        'lo': 'lao',\n        'lt': 'lit',\n        'lu': 'lub',\n        'lv': 'lav',\n        'mg': 'mlg',\n        'mh': 'mah',\n        'mi': 'mri',\n        'mk': 'mkd',\n        'ml': 'mal',\n        'mn': 'mon',\n        'mr': 'mar',\n        'ms': 'msa',\n        'mt': 'mlt',\n        'my': 'mya',\n        'na': 'nau',\n        'nb': 'nob',\n        'nd': 'nde',\n        'ne': 'nep',\n        'ng': 'ndo',\n        'nl': 'nld',\n        'nn': 'nno',\n        'no': 'nor',\n        'nr': 'nbl',\n        'nv': 'nav',\n        'ny': 'nya',\n        'oc': 'oci',\n        'oj': 'oji',\n        'om': 'orm',\n        'or': 'ori',\n        'os': 'oss',\n        'pa': 'pan',\n        'pe': 'per',\n        'pi': 'pli',\n        'pl': 'pol',\n        'ps': 'pus',\n        'pt': 'por',\n        'qu': 'que',\n        'rm': 'roh',\n        'rn': 'run',\n        'ro': 'ron',\n        'ru': 'rus',\n        'rw': 'kin',\n        'sa': 'san',\n        'sc': 'srd',\n        'sd': 'snd',\n        'se': 'sme',\n        'sg': 'sag',\n        'si': 'sin',\n        'sk': 'slk',\n        'sl': 'slv',\n        'sm': 'smo',\n        'sn': 'sna',\n        'so': 'som',\n        'sq': 'sqi',\n        'sr': 'srp',\n        'ss': 'ssw',\n        'st': 'sot',\n        'su': 'sun',\n        'sv': 'swe',\n        'sw': 'swa',\n        'ta': 'tam',\n        'te': 'tel',\n        'tg': 'tgk',\n        'th': 'tha',\n        'ti': 'tir',\n        'tk': 'tuk',\n        'tl': 'tgl',\n        'tn': 'tsn',\n        'to': 'ton',\n        'tr': 'tur',\n        'ts': 'tso',\n        'tt': 'tat',\n        'tw': 'twi',\n        'ty': 'tah',\n        'ug': 'uig',\n        'uk': 'ukr',\n        'ur': 'urd',\n        'uz': 'uzb',\n        've': 'ven',\n        'vi': 'vie',\n        'vo': 'vol',\n        'wa': 'wln',\n        'wo': 'wol',\n        'xh': 'xho',\n        'yi': 'yid',\n        'ji': 'yid',  # Replaced by yi in 1989 revision\n        'yo': 'yor',\n        'za': 'zha',\n        'zh': 'zho',\n        'zu': 'zul',\n    }\n\n    @classmethod\n    def short2long(cls, code):\n        \"\"\"Convert language code from ISO 639-1 to ISO 639-2/T\"\"\"\n        return cls._lang_map.get(code[:2])\n\n    @classmethod\n    def long2short(cls, code):\n        \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"\n        for short_name, long_name in cls._lang_map.items():\n            if long_name == code:\n                return short_name\n\n\nclass ISO3166Utils:\n    # From http://data.okfn.org/data/core/country-list\n    _country_map = {\n        'AF': 'Afghanistan',\n        'AX': '\u00c5land Islands',\n        'AL': 'Albania',\n        'DZ': 'Algeria',\n        'AS': 'American Samoa',\n        'AD': 'Andorra',\n        'AO': 'Angola',\n        'AI': 'Anguilla',\n        'AQ': 'Antarctica',\n        'AG': 'Antigua and Barbuda',\n        'AR': 'Argentina',\n        'AM': 'Armenia',\n        'AW': 'Aruba',\n        'AU': 'Australia',\n        'AT': 'Austria',\n        'AZ': 'Azerbaijan',\n        'BS': 'Bahamas',\n        'BH': 'Bahrain',\n        'BD': 'Bangladesh',\n        'BB': 'Barbados',\n        'BY': 'Belarus',\n        'BE': 'Belgium',\n        'BZ': 'Belize',\n        'BJ': 'Benin',\n        'BM': 'Bermuda',\n        'BT': 'Bhutan',\n        'BO': 'Bolivia, Plurinational State of',\n        'BQ': 'Bonaire, Sint Eustatius and Saba',\n        'BA': 'Bosnia and Herzegovina',\n        'BW': 'Botswana',\n        'BV': 'Bouvet Island',\n        'BR': 'Brazil',\n        'IO': 'British Indian Ocean Territory',\n        'BN': 'Brunei Darussalam',\n        'BG': 'Bulgaria',\n        'BF': 'Burkina Faso',\n        'BI': 'Burundi',\n        'KH': 'Cambodia',\n        'CM': 'Cameroon',\n        'CA': 'Canada',\n        'CV': 'Cape Verde',\n        'KY': 'Cayman Islands',\n        'CF': 'Central African Republic',\n        'TD': 'Chad',\n        'CL': 'Chile',\n        'CN': 'China',\n        'CX': 'Christmas Island',\n        'CC': 'Cocos (Keeling) Islands',\n        'CO': 'Colombia',\n        'KM': 'Comoros',\n        'CG': 'Congo',\n        'CD': 'Congo, the Democratic Republic of the',\n        'CK': 'Cook Islands',\n        'CR': 'Costa Rica',\n        'CI': 'C\u00f4te d\\'Ivoire',\n        'HR': 'Croatia',\n        'CU': 'Cuba',\n        'CW': 'Cura\u00e7ao',\n        'CY': 'Cyprus',\n        'CZ': 'Czech Republic',\n        'DK': 'Denmark',\n        'DJ': 'Djibouti',\n        'DM': 'Dominica',\n        'DO': 'Dominican Republic',\n        'EC': 'Ecuador',\n        'EG': 'Egypt',\n        'SV': 'El Salvador',\n        'GQ': 'Equatorial Guinea',\n        'ER': 'Eritrea',\n        'EE': 'Estonia',\n        'ET': 'Ethiopia',\n        'FK': 'Falkland Islands (Malvinas)',\n        'FO': 'Faroe Islands',\n        'FJ': 'Fiji',\n        'FI': 'Finland',\n        'FR': 'France',\n        'GF': 'French Guiana',\n        'PF': 'French Polynesia',\n        'TF': 'French Southern Territories',\n        'GA': 'Gabon',\n        'GM': 'Gambia',\n        'GE': 'Georgia',\n        'DE': 'Germany',\n        'GH': 'Ghana',\n        'GI': 'Gibraltar',\n        'GR': 'Greece',\n        'GL': 'Greenland',\n        'GD': 'Grenada',\n        'GP': 'Guadeloupe',\n        'GU': 'Guam',\n        'GT': 'Guatemala',\n        'GG': 'Guernsey',\n        'GN': 'Guinea',\n        'GW': 'Guinea-Bissau',\n        'GY': 'Guyana',\n        'HT': 'Haiti',\n        'HM': 'Heard Island and McDonald Islands',\n        'VA': 'Holy See (Vatican City State)',\n        'HN': 'Honduras',\n        'HK': 'Hong Kong',\n        'HU': 'Hungary',\n        'IS': 'Iceland',\n        'IN': 'India',\n        'ID': 'Indonesia',\n        'IR': 'Iran, Islamic Republic of',\n        'IQ': 'Iraq',\n        'IE': 'Ireland',\n        'IM': 'Isle of Man',\n        'IL': 'Israel',\n        'IT': 'Italy',\n        'JM': 'Jamaica',\n        'JP': 'Japan',\n        'JE': 'Jersey',\n        'JO': 'Jordan',\n        'KZ': 'Kazakhstan',\n        'KE': 'Kenya',\n        'KI': 'Kiribati',\n        'KP': 'Korea, Democratic People\\'s Republic of',\n        'KR': 'Korea, Republic of',\n        'KW': 'Kuwait',\n        'KG': 'Kyrgyzstan',\n        'LA': 'Lao People\\'s Democratic Republic',\n        'LV': 'Latvia',\n        'LB': 'Lebanon',\n        'LS': 'Lesotho',\n        'LR': 'Liberia',\n        'LY': 'Libya',\n        'LI': 'Liechtenstein',\n        'LT': 'Lithuania',\n        'LU': 'Luxembourg',\n        'MO': 'Macao',\n        'MK': 'Macedonia, the Former Yugoslav Republic of',\n        'MG': 'Madagascar',\n        'MW': 'Malawi',\n        'MY': 'Malaysia',\n        'MV': 'Maldives',\n        'ML': 'Mali',\n        'MT': 'Malta',\n        'MH': 'Marshall Islands',\n        'MQ': 'Martinique',\n        'MR': 'Mauritania',\n        'MU': 'Mauritius',\n        'YT': 'Mayotte',\n        'MX': 'Mexico',\n        'FM': 'Micronesia, Federated States of',\n        'MD': 'Moldova, Republic of',\n        'MC': 'Monaco',\n        'MN': 'Mongolia',\n        'ME': 'Montenegro',\n        'MS': 'Montserrat',\n        'MA': 'Morocco',\n        'MZ': 'Mozambique',\n        'MM': 'Myanmar',\n        'NA': 'Namibia',\n        'NR': 'Nauru',\n        'NP': 'Nepal',\n        'NL': 'Netherlands',\n        'NC': 'New Caledonia',\n        'NZ': 'New Zealand',\n        'NI': 'Nicaragua',\n        'NE': 'Niger',\n        'NG': 'Nigeria',\n        'NU': 'Niue',\n        'NF': 'Norfolk Island',\n        'MP': 'Northern Mariana Islands',\n        'NO': 'Norway',\n        'OM': 'Oman',\n        'PK': 'Pakistan',\n        'PW': 'Palau',\n        'PS': 'Palestine, State of',\n        'PA': 'Panama',\n        'PG': 'Papua New Guinea',\n        'PY': 'Paraguay',\n        'PE': 'Peru',\n        'PH': 'Philippines',\n        'PN': 'Pitcairn',\n        'PL': 'Poland',\n        'PT': 'Portugal',\n        'PR': 'Puerto Rico',\n        'QA': 'Qatar',\n        'RE': 'R\u00e9union',\n        'RO': 'Romania',\n        'RU': 'Russian Federation',\n        'RW': 'Rwanda',\n        'BL': 'Saint Barth\u00e9lemy',\n        'SH': 'Saint Helena, Ascension and Tristan da Cunha',\n        'KN': 'Saint Kitts and Nevis',\n        'LC': 'Saint Lucia',\n        'MF': 'Saint Martin (French part)',\n        'PM': 'Saint Pierre and Miquelon',\n        'VC': 'Saint Vincent and the Grenadines',\n        'WS': 'Samoa',\n        'SM': 'San Marino',\n        'ST': 'Sao Tome and Principe',\n        'SA': 'Saudi Arabia',\n        'SN': 'Senegal',\n        'RS': 'Serbia',\n        'SC': 'Seychelles',\n        'SL': 'Sierra Leone',\n        'SG': 'Singapore',\n        'SX': 'Sint Maarten (Dutch part)',\n        'SK': 'Slovakia',\n        'SI': 'Slovenia',\n        'SB': 'Solomon Islands',\n        'SO': 'Somalia',\n        'ZA': 'South Africa',\n        'GS': 'South Georgia and the South Sandwich Islands',\n        'SS': 'South Sudan',\n        'ES': 'Spain',\n        'LK': 'Sri Lanka',\n        'SD': 'Sudan',\n        'SR': 'Suriname',\n        'SJ': 'Svalbard and Jan Mayen',\n        'SZ': 'Swaziland',\n        'SE': 'Sweden',\n        'CH': 'Switzerland',\n        'SY': 'Syrian Arab Republic',\n        'TW': 'Taiwan, Province of China',\n        'TJ': 'Tajikistan',\n        'TZ': 'Tanzania, United Republic of',\n        'TH': 'Thailand',\n        'TL': 'Timor-Leste',\n        'TG': 'Togo',\n        'TK': 'Tokelau',\n        'TO': 'Tonga',\n        'TT': 'Trinidad and Tobago',\n        'TN': 'Tunisia',\n        'TR': 'Turkey',\n        'TM': 'Turkmenistan',\n        'TC': 'Turks and Caicos Islands',\n        'TV': 'Tuvalu',\n        'UG': 'Uganda',\n        'UA': 'Ukraine',\n        'AE': 'United Arab Emirates',\n        'GB': 'United Kingdom',\n        'US': 'United States',\n        'UM': 'United States Minor Outlying Islands',\n        'UY': 'Uruguay',\n        'UZ': 'Uzbekistan',\n        'VU': 'Vanuatu',\n        'VE': 'Venezuela, Bolivarian Republic of',\n        'VN': 'Viet Nam',\n        'VG': 'Virgin Islands, British',\n        'VI': 'Virgin Islands, U.S.',\n        'WF': 'Wallis and Futuna',\n        'EH': 'Western Sahara',\n        'YE': 'Yemen',\n        'ZM': 'Zambia',\n        'ZW': 'Zimbabwe',\n        # Not ISO 3166 codes, but used for IP blocks\n        'AP': 'Asia/Pacific Region',\n        'EU': 'Europe',\n    }\n\n    @classmethod\n    def short2full(cls, code):\n        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"\n        return cls._country_map.get(code.upper())\n\n\nclass GeoUtils:\n    # Major IPv4 address blocks per country\n    _country_ip_map = {\n        'AD': '46.172.224.0/19',\n        'AE': '94.200.0.0/13',\n        'AF': '149.54.0.0/17',\n        'AG': '209.59.64.0/18',\n        'AI': '204.14.248.0/21',\n        'AL': '46.99.0.0/16',\n        'AM': '46.70.0.0/15',\n        'AO': '105.168.0.0/13',\n        'AP': '182.50.184.0/21',\n        'AQ': '23.154.160.0/24',\n        'AR': '181.0.0.0/12',\n        'AS': '202.70.112.0/20',\n        'AT': '77.116.0.0/14',\n        'AU': '1.128.0.0/11',\n        'AW': '181.41.0.0/18',\n        'AX': '185.217.4.0/22',\n        'AZ': '5.197.0.0/16',\n        'BA': '31.176.128.0/17',\n        'BB': '65.48.128.0/17',\n        'BD': '114.130.0.0/16',\n        'BE': '57.0.0.0/8',\n        'BF': '102.178.0.0/15',\n        'BG': '95.42.0.0/15',\n        'BH': '37.131.0.0/17',\n        'BI': '154.117.192.0/18',\n        'BJ': '137.255.0.0/16',\n        'BL': '185.212.72.0/23',\n        'BM': '196.12.64.0/18',\n        'BN': '156.31.0.0/16',\n        'BO': '161.56.0.0/16',\n        'BQ': '161.0.80.0/20',\n        'BR': '191.128.0.0/12',\n        'BS': '24.51.64.0/18',\n        'BT': '119.2.96.0/19',\n        'BW': '168.167.0.0/16',\n        'BY': '178.120.0.0/13',\n        'BZ': '179.42.192.0/18',\n        'CA': '99.224.0.0/11',\n        'CD': '41.243.0.0/16',\n        'CF': '197.242.176.0/21',\n        'CG': '160.113.0.0/16',\n        'CH': '85.0.0.0/13',\n        'CI': '102.136.0.0/14',\n        'CK': '202.65.32.0/19',\n        'CL': '152.172.0.0/14',\n        'CM': '102.244.0.0/14',\n        'CN': '36.128.0.0/10',\n        'CO': '181.240.0.0/12',\n        'CR': '201.192.0.0/12',\n        'CU': '152.206.0.0/15',\n        'CV': '165.90.96.0/19',\n        'CW': '190.88.128.0/17',\n        'CY': '31.153.0.0/16',\n        'CZ': '88.100.0.0/14',\n        'DE': '53.0.0.0/8',\n        'DJ': '197.241.0.0/17',\n        'DK': '87.48.0.0/12',\n        'DM': '192.243.48.0/20',\n        'DO': '152.166.0.0/15',\n        'DZ': '41.96.0.0/12',\n        'EC': '186.68.0.0/15',\n        'EE': '90.190.0.0/15',\n        'EG': '156.160.0.0/11',\n        'ER': '196.200.96.0/20',\n        'ES': '88.0.0.0/11',\n        'ET': '196.188.0.0/14',\n        'EU': '2.16.0.0/13',\n        'FI': '91.152.0.0/13',\n        'FJ': '144.120.0.0/16',\n        'FK': '80.73.208.0/21',\n        'FM': '119.252.112.0/20',\n        'FO': '88.85.32.0/19',\n        'FR': '90.0.0.0/9',\n        'GA': '41.158.0.0/15',\n        'GB': '25.0.0.0/8',\n        'GD': '74.122.88.0/21',\n        'GE': '31.146.0.0/16',\n        'GF': '161.22.64.0/18',\n        'GG': '62.68.160.0/19',\n        'GH': '154.160.0.0/12',\n        'GI': '95.164.0.0/16',\n        'GL': '88.83.0.0/19',\n        'GM': '160.182.0.0/15',\n        'GN': '197.149.192.0/18',\n        'GP': '104.250.0.0/19',\n        'GQ': '105.235.224.0/20',\n        'GR': '94.64.0.0/13',\n        'GT': '168.234.0.0/16',\n        'GU': '168.123.0.0/16',\n        'GW': '197.214.80.0/20',\n        'GY': '181.41.64.0/18',\n        'HK': '113.252.0.0/14',\n        'HN': '181.210.0.0/16',\n        'HR': '93.136.0.0/13',\n        'HT': '148.102.128.0/17',\n        'HU': '84.0.0.0/14',\n        'ID': '39.192.0.0/10',\n        'IE': '87.32.0.0/12',\n        'IL': '79.176.0.0/13',\n        'IM': '5.62.80.0/20',\n        'IN': '117.192.0.0/10',\n        'IO': '203.83.48.0/21',\n        'IQ': '37.236.0.0/14',\n        'IR': '2.176.0.0/12',\n        'IS': '82.221.0.0/16',\n        'IT': '79.0.0.0/10',\n        'JE': '87.244.64.0/18',\n        'JM': '72.27.0.0/17',\n        'JO': '176.29.0.0/16',\n        'JP': '133.0.0.0/8',\n        'KE': '105.48.0.0/12',\n        'KG': '158.181.128.0/17',\n        'KH': '36.37.128.0/17',\n        'KI': '103.25.140.0/22',\n        'KM': '197.255.224.0/20',\n        'KN': '198.167.192.0/19',\n        'KP': '175.45.176.0/22',\n        'KR': '175.192.0.0/10',\n        'KW': '37.36.0.0/14',\n        'KY': '64.96.0.0/15',\n        'KZ': '2.72.0.0/13',\n        'LA': '115.84.64.0/18',\n        'LB': '178.135.0.0/16',\n        'LC': '24.92.144.0/20',\n        'LI': '82.117.0.0/19',\n        'LK': '112.134.0.0/15',\n        'LR': '102.183.0.0/16',\n        'LS': '129.232.0.0/17',\n        'LT': '78.56.0.0/13',\n        'LU': '188.42.0.0/16',\n        'LV': '46.109.0.0/16',\n        'LY': '41.252.0.0/14',\n        'MA': '105.128.0.0/11',\n        'MC': '88.209.64.0/18',\n        'MD': '37.246.0.0/16',\n        'ME': '178.175.0.0/17',\n        'MF': '74.112.232.0/21',\n        'MG': '154.126.0.0/17',\n        'MH': '117.103.88.0/21',\n        'MK': '77.28.0.0/15',\n        'ML': '154.118.128.0/18',\n        'MM': '37.111.0.0/17',\n        'MN': '49.0.128.0/17',\n        'MO': '60.246.0.0/16',\n        'MP': '202.88.64.0/20',\n        'MQ': '109.203.224.0/19',\n        'MR': '41.188.64.0/18',\n        'MS': '208.90.112.0/22',\n        'MT': '46.11.0.0/16',\n        'MU': '105.16.0.0/12',\n        'MV': '27.114.128.0/18',\n        'MW': '102.70.0.0/15',\n        'MX': '187.192.0.0/11',\n        'MY': '175.136.0.0/13',\n        'MZ': '197.218.0.0/15',\n        'NA': '41.182.0.0/16',\n        'NC': '101.101.0.0/18',\n        'NE': '197.214.0.0/18',\n        'NF': '203.17.240.0/22',\n        'NG': '105.112.0.0/12',\n        'NI': '186.76.0.0/15',\n        'NL': '145.96.0.0/11',\n        'NO': '84.208.0.0/13',\n        'NP': '36.252.0.0/15',\n        'NR': '203.98.224.0/19',\n        'NU': '49.156.48.0/22',\n        'NZ': '49.224.0.0/14',\n        'OM': '5.36.0.0/15',\n        'PA': '186.72.0.0/15',\n        'PE': '186.160.0.0/14',\n        'PF': '123.50.64.0/18',\n        'PG': '124.240.192.0/19',\n        'PH': '49.144.0.0/13',\n        'PK': '39.32.0.0/11',\n        'PL': '83.0.0.0/11',\n        'PM': '70.36.0.0/20',\n        'PR': '66.50.0.0/16',\n        'PS': '188.161.0.0/16',\n        'PT': '85.240.0.0/13',\n        'PW': '202.124.224.0/20',\n        'PY': '181.120.0.0/14',\n        'QA': '37.210.0.0/15',\n        'RE': '102.35.0.0/16',\n        'RO': '79.112.0.0/13',\n        'RS': '93.86.0.0/15',\n        'RU': '5.136.0.0/13',\n        'RW': '41.186.0.0/16',\n        'SA': '188.48.0.0/13',\n        'SB': '202.1.160.0/19',\n        'SC': '154.192.0.0/11',\n        'SD': '102.120.0.0/13',\n        'SE': '78.64.0.0/12',\n        'SG': '8.128.0.0/10',\n        'SI': '188.196.0.0/14',\n        'SK': '78.98.0.0/15',\n        'SL': '102.143.0.0/17',\n        'SM': '89.186.32.0/19',\n        'SN': '41.82.0.0/15',\n        'SO': '154.115.192.0/18',\n        'SR': '186.179.128.0/17',\n        'SS': '105.235.208.0/21',\n        'ST': '197.159.160.0/19',\n        'SV': '168.243.0.0/16',\n        'SX': '190.102.0.0/20',\n        'SY': '5.0.0.0/16',\n        'SZ': '41.84.224.0/19',\n        'TC': '65.255.48.0/20',\n        'TD': '154.68.128.0/19',\n        'TG': '196.168.0.0/14',\n        'TH': '171.96.0.0/13',\n        'TJ': '85.9.128.0/18',\n        'TK': '27.96.24.0/21',\n        'TL': '180.189.160.0/20',\n        'TM': '95.85.96.0/19',\n        'TN': '197.0.0.0/11',\n        'TO': '175.176.144.0/21',\n        'TR': '78.160.0.0/11',\n        'TT': '186.44.0.0/15',\n        'TV': '202.2.96.0/19',\n        'TW': '120.96.0.0/11',\n        'TZ': '156.156.0.0/14',\n        'UA': '37.52.0.0/14',\n        'UG': '102.80.0.0/13',\n        'US': '6.0.0.0/8',\n        'UY': '167.56.0.0/13',\n        'UZ': '84.54.64.0/18',\n        'VA': '212.77.0.0/19',\n        'VC': '207.191.240.0/21',\n        'VE': '186.88.0.0/13',\n        'VG': '66.81.192.0/20',\n        'VI': '146.226.0.0/16',\n        'VN': '14.160.0.0/11',\n        'VU': '202.80.32.0/20',\n        'WF': '117.20.32.0/21',\n        'WS': '202.4.32.0/19',\n        'YE': '134.35.0.0/16',\n        'YT': '41.242.116.0/22',\n        'ZA': '41.0.0.0/11',\n        'ZM': '102.144.0.0/13',\n        'ZW': '102.177.192.0/18',\n    }\n\n    @classmethod\n    def random_ipv4(cls, code_or_block):\n        if len(code_or_block) == 2:\n            block = cls._country_ip_map.get(code_or_block.upper())\n            if not block:\n                return None\n        else:\n            block = code_or_block\n        addr, preflen = block.split('/')\n        addr_min = struct.unpack('!L', socket.inet_aton(addr))[0]\n        addr_max = addr_min | (0xffffffff >> int(preflen))\n        return str(socket.inet_ntoa(\n            struct.pack('!L', random.randint(addr_min, addr_max))))\n\n\nclass PerRequestProxyHandler(urllib.request.ProxyHandler):\n    def __init__(self, proxies=None):\n        # Set default handlers\n        for type in ('http', 'https'):\n            setattr(self, '%s_open' % type,\n                    lambda r, proxy='__noproxy__', type=type, meth=self.proxy_open:\n                        meth(r, proxy, type))\n        urllib.request.ProxyHandler.__init__(self, proxies)\n\n    def proxy_open(self, req, proxy, type):\n        req_proxy = req.headers.get('Ytdl-request-proxy')\n        if req_proxy is not None:\n            proxy = req_proxy\n            del req.headers['Ytdl-request-proxy']\n\n        if proxy == '__noproxy__':\n            return None  # No Proxy\n        if urllib.parse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n            req.add_header('Ytdl-socks-proxy', proxy)\n            # yt-dlp's http/https handlers do wrapping the socket with socks\n            return None\n        return urllib.request.ProxyHandler.proxy_open(\n            self, req, proxy, type)\n\n\n# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is\n# released into Public Domain\n# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387\n\ndef long_to_bytes(n, blocksize=0):\n    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n    Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front of the\n    byte string with binary zeros so that the length is a multiple of\n    blocksize.\n    \"\"\"\n    # after much testing, this algorithm was deemed to be the fastest\n    s = b''\n    n = int(n)\n    while n > 0:\n        s = struct.pack('>I', n & 0xffffffff) + s\n        n = n >> 32\n    # strip off leading zeros\n    for i in range(len(s)):\n        if s[i] != b'\\000'[0]:\n            break\n    else:\n        # only happens when n == 0\n        s = b'\\000'\n        i = 0\n    s = s[i:]\n    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n    # de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n    return s\n\n\ndef bytes_to_long(s):\n    \"\"\"bytes_to_long(string) : long\n    Convert a byte string to a long integer.\n\n    This is (essentially) the inverse of long_to_bytes().\n    \"\"\"\n    acc = 0\n    length = len(s)\n    if length % 4:\n        extra = (4 - length % 4)\n        s = b'\\000' * extra + s\n        length = length + extra\n    for i in range(0, length, 4):\n        acc = (acc << 32) + struct.unpack('>I', s[i:i + 4])[0]\n    return acc\n\n\ndef ohdave_rsa_encrypt(data, exponent, modulus):\n    '''\n    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n\n    Input:\n        data: data to encrypt, bytes-like object\n        exponent, modulus: parameter e and N of RSA algorithm, both integer\n    Output: hex string of encrypted data\n\n    Limitation: supports one block encryption only\n    '''\n\n    payload = int(binascii.hexlify(data[::-1]), 16)\n    encrypted = pow(payload, exponent, modulus)\n    return '%x' % encrypted\n\n\ndef pkcs1pad(data, length):\n    \"\"\"\n    Padding input data with PKCS#1 scheme\n\n    @param {int[]} data        input data\n    @param {int}   length      target length\n    @returns {int[]}           padded data\n    \"\"\"\n    if len(data) > length - 11:\n        raise ValueError('Input data too long for PKCS#1 padding')\n\n    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n    return [0, 2] + pseudo_random + [0] + data\n\n\ndef _base_n_table(n, table):\n    if not table and not n:\n        raise ValueError('Either table or n must be specified')\n    table = (table or '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')[:n]\n\n    if n and n != len(table):\n        raise ValueError(f'base {n} exceeds table length {len(table)}')\n    return table\n\n\ndef encode_base_n(num, n=None, table=None):\n    \"\"\"Convert given int to a base-n string\"\"\"\n    table = _base_n_table(n, table)\n    if not num:\n        return table[0]\n\n    result, base = '', len(table)\n    while num:\n        result = table[num % base] + result\n        num = num // base\n    return result\n\n\ndef decode_base_n(string, n=None, table=None):\n    \"\"\"Convert given base-n string to int\"\"\"\n    table = {char: index for index, char in enumerate(_base_n_table(n, table))}\n    result, base = 0, len(table)\n    for char in string:\n        result = result * base + table[char]\n    return result\n\n\ndef decode_packed_codes(code):\n    mobj = re.search(PACKED_CODES_RE, code)\n    obfuscated_code, base, count, symbols = mobj.groups()\n    base = int(base)\n    count = int(count)\n    symbols = symbols.split('|')\n    symbol_table = {}\n\n    while count:\n        count -= 1\n        base_n_count = encode_base_n(count, base)\n        symbol_table[base_n_count] = symbols[count] or base_n_count\n\n    return re.sub(\n        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n        obfuscated_code)\n\n\ndef caesar(s, alphabet, shift):\n    if shift == 0:\n        return s\n    l = len(alphabet)\n    return ''.join(\n        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n        for c in s)\n\n\ndef rot47(s):\n    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)\n\n\ndef parse_m3u8_attributes(attrib):\n    info = {}\n    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n        if val.startswith('\"'):\n            val = val[1:-1]\n        info[key] = val\n    return info\n\n\ndef urshift(val, n):\n    return val >> n if val >= 0 else (val + 0x100000000) >> n\n\n\ndef write_xattr(path, key, value):\n    # Windows: Write xattrs to NTFS Alternate Data Streams:\n    # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29\n    if compat_os_name == 'nt':\n        assert ':' not in key\n        assert os.path.exists(path)\n\n        try:\n            with open(f'{path}:{key}', 'wb') as f:\n                f.write(value)\n        except OSError as e:\n            raise XAttrMetadataError(e.errno, e.strerror)\n        return\n\n    # UNIX Method 1. Use xattrs/pyxattrs modules\n\n    setxattr = None\n    if getattr(xattr, '_yt_dlp__identifier', None) == 'pyxattr':\n        # Unicode arguments are not supported in pyxattr until version 0.5.0\n        # See https://github.com/ytdl-org/youtube-dl/issues/5498\n        if version_tuple(xattr.__version__) >= (0, 5, 0):\n            setxattr = xattr.set\n    elif xattr:\n        setxattr = xattr.setxattr\n\n    if setxattr:\n        try:\n            setxattr(path, key, value)\n        except OSError as e:\n            raise XAttrMetadataError(e.errno, e.strerror)\n        return\n\n    # UNIX Method 2. Use setfattr/xattr executables\n    exe = ('setfattr' if check_executable('setfattr', ['--version'])\n           else 'xattr' if check_executable('xattr', ['-h']) else None)\n    if not exe:\n        raise XAttrUnavailableError(\n            'Couldn\\'t find a tool to set the xattrs. Install either the python \"xattr\" or \"pyxattr\" modules or the '\n            + ('\"xattr\" binary' if sys.platform != 'linux' else 'GNU \"attr\" package (which contains the \"setfattr\" tool)'))\n\n    value = value.decode()\n    try:\n        _, stderr, returncode = Popen.run(\n            [exe, '-w', key, value, path] if exe == 'xattr' else [exe, '-n', key, '-v', value, path],\n            text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n    except OSError as e:\n        raise XAttrMetadataError(e.errno, e.strerror)\n    if returncode:\n        raise XAttrMetadataError(returncode, stderr)\n\n\ndef random_birthday(year_field, month_field, day_field):\n    start_date = datetime.date(1950, 1, 1)\n    end_date = datetime.date(1995, 12, 31)\n    offset = random.randint(0, (end_date - start_date).days)\n    random_date = start_date + datetime.timedelta(offset)\n    return {\n        year_field: str(random_date.year),\n        month_field: str(random_date.month),\n        day_field: str(random_date.day),\n    }\n\n\ndef find_available_port(interface=''):\n    try:\n        with socket.socket() as sock:\n            sock.bind((interface, 0))\n            return sock.getsockname()[1]\n    except OSError:\n        return None\n\n\n# Templates for internet shortcut files, which are plain text files.\nDOT_URL_LINK_TEMPLATE = '''\\\n[InternetShortcut]\nURL=%(url)s\n'''\n\nDOT_WEBLOC_LINK_TEMPLATE = '''\\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n\\t<key>URL</key>\n\\t<string>%(url)s</string>\n</dict>\n</plist>\n'''\n\nDOT_DESKTOP_LINK_TEMPLATE = '''\\\n[Desktop Entry]\nEncoding=UTF-8\nName=%(filename)s\nType=Link\nURL=%(url)s\nIcon=text-html\n'''\n\nLINK_TEMPLATES = {\n    'url': DOT_URL_LINK_TEMPLATE,\n    'desktop': DOT_DESKTOP_LINK_TEMPLATE,\n    'webloc': DOT_WEBLOC_LINK_TEMPLATE,\n}\n\n\ndef iri_to_uri(iri):\n    \"\"\"\n    Converts an IRI (Internationalized Resource Identifier, allowing Unicode characters) to a URI (Uniform Resource Identifier, ASCII-only).\n\n    The function doesn't add an additional layer of escaping; e.g., it doesn't escape `%3C` as `%253C`. Instead, it percent-escapes characters with an underlying UTF-8 encoding *besides* those already escaped, leaving the URI intact.\n    \"\"\"\n\n    iri_parts = urllib.parse.urlparse(iri)\n\n    if '[' in iri_parts.netloc:\n        raise ValueError('IPv6 URIs are not, yet, supported.')\n        # Querying `.netloc`, when there's only one bracket, also raises a ValueError.\n\n    # The `safe` argument values, that the following code uses, contain the characters that should not be percent-encoded. Everything else but letters, digits and '_.-' will be percent-encoded with an underlying UTF-8 encoding. Everything already percent-encoded will be left as is.\n\n    net_location = ''\n    if iri_parts.username:\n        net_location += urllib.parse.quote(iri_parts.username, safe=r\"!$%&'()*+,~\")\n        if iri_parts.password is not None:\n            net_location += ':' + urllib.parse.quote(iri_parts.password, safe=r\"!$%&'()*+,~\")\n        net_location += '@'\n\n    net_location += iri_parts.hostname.encode('idna').decode()  # Punycode for Unicode hostnames.\n    # The 'idna' encoding produces ASCII text.\n    if iri_parts.port is not None and iri_parts.port != 80:\n        net_location += ':' + str(iri_parts.port)\n\n    return urllib.parse.urlunparse(\n        (iri_parts.scheme,\n            net_location,\n\n            urllib.parse.quote_plus(iri_parts.path, safe=r\"!$%&'()*+,/:;=@|~\"),\n\n            # Unsure about the `safe` argument, since this is a legacy way of handling parameters.\n            urllib.parse.quote_plus(iri_parts.params, safe=r\"!$%&'()*+,/:;=@|~\"),\n\n            # Not totally sure about the `safe` argument, since the source does not explicitly mention the query URI component.\n            urllib.parse.quote_plus(iri_parts.query, safe=r\"!$%&'()*+,/:;=?@{|}~\"),\n\n            urllib.parse.quote_plus(iri_parts.fragment, safe=r\"!#$%&'()*+,/:;=?@{|}~\")))\n\n    # Source for `safe` arguments: https://url.spec.whatwg.org/#percent-encoded-bytes.\n\n\ndef to_high_limit_path(path):\n    if sys.platform in ['win32', 'cygwin']:\n        # Work around MAX_PATH limitation on Windows. The maximum allowed length for the individual path segments may still be quite limited.\n        return '\\\\\\\\?\\\\' + os.path.abspath(path)\n\n    return path\n\n\ndef format_field(obj, field=None, template='%s', ignore=NO_DEFAULT, default='', func=IDENTITY):\n    val = traversal.traverse_obj(obj, *variadic(field))\n    if not val if ignore is NO_DEFAULT else val in variadic(ignore):\n        return default\n    return template % func(val)\n\n\ndef clean_podcast_url(url):\n    url = re.sub(r'''(?x)\n        (?:\n            (?:\n                chtbl\\.com/track|\n                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/\n                play\\.podtrac\\.com\n            )/[^/]+|\n            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure\n            flex\\.acast\\.com|\n            pd(?:\n                cn\\.co| # https://podcorn.com/analytics-prefix/\n                st\\.fm # https://podsights.com/docs/\n            )/e\n        )/''', '', url)\n    return re.sub(r'^\\w+://(\\w+://)', r'\\1', url)\n\n\n_HEX_TABLE = '0123456789abcdef'\n\n\ndef random_uuidv4():\n    return re.sub(r'[xy]', lambda x: _HEX_TABLE[random.randint(0, 15)], 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx')\n\n\ndef make_dir(path, to_screen=None):\n    try:\n        dn = os.path.dirname(path)\n        if dn:\n            os.makedirs(dn, exist_ok=True)\n        return True\n    except OSError as err:\n        if callable(to_screen) is not None:\n            to_screen(f'unable to create directory {err}')\n        return False\n\n\ndef get_executable_path():\n    from ..update import _get_variant_and_executable_path\n\n    return os.path.dirname(os.path.abspath(_get_variant_and_executable_path()[1]))\n\n\ndef get_user_config_dirs(package_name):\n    # .config (e.g. ~/.config/package_name)\n    xdg_config_home = os.getenv('XDG_CONFIG_HOME') or compat_expanduser('~/.config')\n    yield os.path.join(xdg_config_home, package_name)\n\n    # appdata (%APPDATA%/package_name)\n    appdata_dir = os.getenv('appdata')\n    if appdata_dir:\n        yield os.path.join(appdata_dir, package_name)\n\n    # home (~/.package_name)\n    yield os.path.join(compat_expanduser('~'), f'.{package_name}')\n\n\ndef get_system_config_dirs(package_name):\n    # /etc/package_name\n    yield os.path.join('/etc', package_name)\n\n\ndef time_seconds(**kwargs):\n    \"\"\"\n    Returns TZ-aware time in seconds since the epoch (1970-01-01T00:00:00Z)\n    \"\"\"\n    return time.time() + datetime.timedelta(**kwargs).total_seconds()\n\n\n# create a JSON Web Signature (jws) with HS256 algorithm\n# the resulting format is in JWS Compact Serialization\n# implemented following JWT https://www.rfc-editor.org/rfc/rfc7519.html\n# implemented following JWS https://www.rfc-editor.org/rfc/rfc7515.html\ndef jwt_encode_hs256(payload_data, key, headers={}):\n    header_data = {\n        'alg': 'HS256',\n        'typ': 'JWT',\n    }\n    if headers:\n        header_data.update(headers)\n    header_b64 = base64.b64encode(json.dumps(header_data).encode())\n    payload_b64 = base64.b64encode(json.dumps(payload_data).encode())\n    h = hmac.new(key.encode(), header_b64 + b'.' + payload_b64, hashlib.sha256)\n    signature_b64 = base64.b64encode(h.digest())\n    token = header_b64 + b'.' + payload_b64 + b'.' + signature_b64\n    return token\n\n\n# can be extended in future to verify the signature and parse header and return the algorithm used if it's not HS256\ndef jwt_decode_hs256(jwt):\n    header_b64, payload_b64, signature_b64 = jwt.split('.')\n    # add trailing ='s that may have been stripped, superfluous ='s are ignored\n    payload_data = json.loads(base64.urlsafe_b64decode(f'{payload_b64}==='))\n    return payload_data\n\n\nWINDOWS_VT_MODE = False if compat_os_name == 'nt' else None\n\n\n@functools.cache\ndef supports_terminal_sequences(stream):\n    if compat_os_name == 'nt':\n        if not WINDOWS_VT_MODE:\n            return False\n    elif not os.getenv('TERM'):\n        return False\n    try:\n        return stream.isatty()\n    except BaseException:\n        return False\n\n\ndef windows_enable_vt_mode():\n    \"\"\"Ref: https://bugs.python.org/issue30075 \"\"\"\n    if get_windows_version() < (10, 0, 10586):\n        return\n\n    import ctypes\n    import ctypes.wintypes\n    import msvcrt\n\n    ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x0004\n\n    dll = ctypes.WinDLL('kernel32', use_last_error=False)\n    handle = os.open('CONOUT$', os.O_RDWR)\n    try:\n        h_out = ctypes.wintypes.HANDLE(msvcrt.get_osfhandle(handle))\n        dw_original_mode = ctypes.wintypes.DWORD()\n        success = dll.GetConsoleMode(h_out, ctypes.byref(dw_original_mode))\n        if not success:\n            raise Exception('GetConsoleMode failed')\n\n        success = dll.SetConsoleMode(h_out, ctypes.wintypes.DWORD(\n            dw_original_mode.value | ENABLE_VIRTUAL_TERMINAL_PROCESSING))\n        if not success:\n            raise Exception('SetConsoleMode failed')\n    finally:\n        os.close(handle)\n\n    global WINDOWS_VT_MODE\n    WINDOWS_VT_MODE = True\n    supports_terminal_sequences.cache_clear()\n\n\n_terminal_sequences_re = re.compile('\\033\\\\[[^m]+m')\n\n\ndef remove_terminal_sequences(string):\n    return _terminal_sequences_re.sub('', string)\n\n\ndef number_of_digits(number):\n    return len('%d' % number)\n\n\ndef join_nonempty(*values, delim='-', from_dict=None):\n    if from_dict is not None:\n        values = (traversal.traverse_obj(from_dict, variadic(v)) for v in values)\n    return delim.join(map(str, filter(None, values)))\n\n\ndef scale_thumbnails_to_max_format_width(formats, thumbnails, url_width_re):\n    \"\"\"\n    Find the largest format dimensions in terms of video width and, for each thumbnail:\n    * Modify the URL: Match the width with the provided regex and replace with the former width\n    * Update dimensions\n\n    This function is useful with video services that scale the provided thumbnails on demand\n    \"\"\"\n    _keys = ('width', 'height')\n    max_dimensions = max(\n        (tuple(format.get(k) or 0 for k in _keys) for format in formats),\n        default=(0, 0))\n    if not max_dimensions[0]:\n        return thumbnails\n    return [\n        merge_dicts(\n            {'url': re.sub(url_width_re, str(max_dimensions[0]), thumbnail['url'])},\n            dict(zip(_keys, max_dimensions)), thumbnail)\n        for thumbnail in thumbnails\n    ]\n\n\ndef parse_http_range(range):\n    \"\"\" Parse value of \"Range\" or \"Content-Range\" HTTP header into tuple. \"\"\"\n    if not range:\n        return None, None, None\n    crg = re.search(r'bytes[ =](\\d+)-(\\d+)?(?:/(\\d+))?', range)\n    if not crg:\n        return None, None, None\n    return int(crg.group(1)), int_or_none(crg.group(2)), int_or_none(crg.group(3))\n\n\ndef read_stdin(what):\n    eof = 'Ctrl+Z' if compat_os_name == 'nt' else 'Ctrl+D'\n    write_string(f'Reading {what} from STDIN - EOF ({eof}) to end:\\n')\n    return sys.stdin\n\n\ndef determine_file_encoding(data):\n    \"\"\"\n    Detect the text encoding used\n    @returns (encoding, bytes to skip)\n    \"\"\"\n\n    # BOM marks are given priority over declarations\n    for bom, enc in BOMS:\n        if data.startswith(bom):\n            return enc, len(bom)\n\n    # Strip off all null bytes to match even when UTF-16 or UTF-32 is used.\n    # We ignore the endianness to get a good enough match\n    data = data.replace(b'\\0', b'')\n    mobj = re.match(rb'(?m)^#\\s*coding\\s*:\\s*(\\S+)\\s*$', data)\n    return mobj.group(1).decode() if mobj else None, 0\n\n\nclass Config:\n    own_args = None\n    parsed_args = None\n    filename = None\n    __initialized = False\n\n    def __init__(self, parser, label=None):\n        self.parser, self.label = parser, label\n        self._loaded_paths, self.configs = set(), []\n\n    def init(self, args=None, filename=None):\n        assert not self.__initialized\n        self.own_args, self.filename = args, filename\n        return self.load_configs()\n\n    def load_configs(self):\n        directory = ''\n        if self.filename:\n            location = os.path.realpath(self.filename)\n            directory = os.path.dirname(location)\n            if location in self._loaded_paths:\n                return False\n            self._loaded_paths.add(location)\n\n        self.__initialized = True\n        opts, _ = self.parser.parse_known_args(self.own_args)\n        self.parsed_args = self.own_args\n        for location in opts.config_locations or []:\n            if location == '-':\n                if location in self._loaded_paths:\n                    continue\n                self._loaded_paths.add(location)\n                self.append_config(shlex.split(read_stdin('options'), comments=True), label='stdin')\n                continue\n            location = os.path.join(directory, expand_path(location))\n            if os.path.isdir(location):\n                location = os.path.join(location, 'yt-dlp.conf')\n            if not os.path.exists(location):\n                self.parser.error(f'config location {location} does not exist')\n            self.append_config(self.read_file(location), location)\n        return True\n\n    def __str__(self):\n        label = join_nonempty(\n            self.label, 'config', f'\"{self.filename}\"' if self.filename else '',\n            delim=' ')\n        return join_nonempty(\n            self.own_args is not None and f'{label[0].upper()}{label[1:]}: {self.hide_login_info(self.own_args)}',\n            *(f'\\n{c}'.replace('\\n', '\\n| ')[1:] for c in self.configs),\n            delim='\\n')\n\n    @staticmethod\n    def read_file(filename, default=[]):\n        try:\n            optionf = open(filename, 'rb')\n        except OSError:\n            return default  # silently skip if file is not present\n        try:\n            enc, skip = determine_file_encoding(optionf.read(512))\n            optionf.seek(skip, io.SEEK_SET)\n        except OSError:\n            enc = None  # silently skip read errors\n        try:\n            # FIXME: https://github.com/ytdl-org/youtube-dl/commit/dfe5fa49aed02cf36ba9f743b11b0903554b5e56\n            contents = optionf.read().decode(enc or preferredencoding())\n            res = shlex.split(contents, comments=True)\n        except Exception as err:\n            raise ValueError(f'Unable to parse \"{filename}\": {err}')\n        finally:\n            optionf.close()\n        return res\n\n    @staticmethod\n    def hide_login_info(opts):\n        PRIVATE_OPTS = {'-p', '--password', '-u', '--username', '--video-password', '--ap-password', '--ap-username'}\n        eqre = re.compile('^(?P<key>' + ('|'.join(re.escape(po) for po in PRIVATE_OPTS)) + ')=.+$')\n\n        def _scrub_eq(o):\n            m = eqre.match(o)\n            if m:\n                return m.group('key') + '=PRIVATE'\n            else:\n                return o\n\n        opts = list(map(_scrub_eq, opts))\n        for idx, opt in enumerate(opts):\n            if opt in PRIVATE_OPTS and idx + 1 < len(opts):\n                opts[idx + 1] = 'PRIVATE'\n        return opts\n\n    def append_config(self, *args, label=None):\n        config = type(self)(self.parser, label)\n        config._loaded_paths = self._loaded_paths\n        if config.init(*args):\n            self.configs.append(config)\n\n    @property\n    def all_args(self):\n        for config in reversed(self.configs):\n            yield from config.all_args\n        yield from self.parsed_args or []\n\n    def parse_known_args(self, **kwargs):\n        return self.parser.parse_known_args(self.all_args, **kwargs)\n\n    def parse_args(self):\n        return self.parser.parse_args(self.all_args)\n\n\nclass WebSocketsWrapper:\n    \"\"\"Wraps websockets module to use in non-async scopes\"\"\"\n    pool = None\n\n    def __init__(self, url, headers=None, connect=True):\n        self.loop = asyncio.new_event_loop()\n        # XXX: \"loop\" is deprecated\n        self.conn = websockets.connect(\n            url, extra_headers=headers, ping_interval=None,\n            close_timeout=float('inf'), loop=self.loop, ping_timeout=float('inf'))\n        if connect:\n            self.__enter__()\n        atexit.register(self.__exit__, None, None, None)\n\n    def __enter__(self):\n        if not self.pool:\n            self.pool = self.run_with_loop(self.conn.__aenter__(), self.loop)\n        return self\n\n    def send(self, *args):\n        self.run_with_loop(self.pool.send(*args), self.loop)\n\n    def recv(self, *args):\n        return self.run_with_loop(self.pool.recv(*args), self.loop)\n\n    def __exit__(self, type, value, traceback):\n        try:\n            return self.run_with_loop(self.conn.__aexit__(type, value, traceback), self.loop)\n        finally:\n            self.loop.close()\n            self._cancel_all_tasks(self.loop)\n\n    # taken from https://github.com/python/cpython/blob/3.9/Lib/asyncio/runners.py with modifications\n    # for contributors: If there's any new library using asyncio needs to be run in non-async, move these function out of this class\n    @staticmethod\n    def run_with_loop(main, loop):\n        if not asyncio.iscoroutine(main):\n            raise ValueError(f'a coroutine was expected, got {main!r}')\n\n        try:\n            return loop.run_until_complete(main)\n        finally:\n            loop.run_until_complete(loop.shutdown_asyncgens())\n            if hasattr(loop, 'shutdown_default_executor'):\n                loop.run_until_complete(loop.shutdown_default_executor())\n\n    @staticmethod\n    def _cancel_all_tasks(loop):\n        to_cancel = asyncio.all_tasks(loop)\n\n        if not to_cancel:\n            return\n\n        for task in to_cancel:\n            task.cancel()\n\n        # XXX: \"loop\" is removed in python 3.10+\n        loop.run_until_complete(\n            asyncio.gather(*to_cancel, loop=loop, return_exceptions=True))\n\n        for task in to_cancel:\n            if task.cancelled():\n                continue\n            if task.exception() is not None:\n                loop.call_exception_handler({\n                    'message': 'unhandled exception during asyncio.run() shutdown',\n                    'exception': task.exception(),\n                    'task': task,\n                })\n\n\ndef merge_headers(*dicts):\n    \"\"\"Merge dicts of http headers case insensitively, prioritizing the latter ones\"\"\"\n    return {k.title(): v for k, v in itertools.chain.from_iterable(map(dict.items, dicts))}\n\n\ndef cached_method(f):\n    \"\"\"Cache a method\"\"\"\n    signature = inspect.signature(f)\n\n    @functools.wraps(f)\n    def wrapper(self, *args, **kwargs):\n        bound_args = signature.bind(self, *args, **kwargs)\n        bound_args.apply_defaults()\n        key = tuple(bound_args.arguments.values())[1:]\n\n        cache = vars(self).setdefault('_cached_method__cache', {}).setdefault(f.__name__, {})\n        if key not in cache:\n            cache[key] = f(self, *args, **kwargs)\n        return cache[key]\n    return wrapper\n\n\nclass classproperty:\n    \"\"\"property access for class methods with optional caching\"\"\"\n    def __new__(cls, func=None, *args, **kwargs):\n        if not func:\n            return functools.partial(cls, *args, **kwargs)\n        return super().__new__(cls)\n\n    def __init__(self, func, *, cache=False):\n        functools.update_wrapper(self, func)\n        self.func = func\n        self._cache = {} if cache else None\n\n    def __get__(self, _, cls):\n        if self._cache is None:\n            return self.func(cls)\n        elif cls not in self._cache:\n            self._cache[cls] = self.func(cls)\n        return self._cache[cls]\n\n\nclass function_with_repr:\n    def __init__(self, func, repr_=None):\n        functools.update_wrapper(self, func)\n        self.func, self.__repr = func, repr_\n\n    def __call__(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n    def __repr__(self):\n        if self.__repr:\n            return self.__repr\n        return f'{self.func.__module__}.{self.func.__qualname__}'\n\n\nclass Namespace(types.SimpleNamespace):\n    \"\"\"Immutable namespace\"\"\"\n\n    def __iter__(self):\n        return iter(self.__dict__.values())\n\n    @property\n    def items_(self):\n        return self.__dict__.items()\n\n\nMEDIA_EXTENSIONS = Namespace(\n    common_video=('avi', 'flv', 'mkv', 'mov', 'mp4', 'webm'),\n    video=('3g2', '3gp', 'f4v', 'mk3d', 'divx', 'mpg', 'ogv', 'm4v', 'wmv'),\n    common_audio=('aiff', 'alac', 'flac', 'm4a', 'mka', 'mp3', 'ogg', 'opus', 'wav'),\n    audio=('aac', 'ape', 'asf', 'f4a', 'f4b', 'm4b', 'm4p', 'm4r', 'oga', 'ogx', 'spx', 'vorbis', 'wma', 'weba'),\n    thumbnails=('jpg', 'png', 'webp'),\n    storyboards=('mhtml', ),\n    subtitles=('srt', 'vtt', 'ass', 'lrc'),\n    manifests=('f4f', 'f4m', 'm3u8', 'smil', 'mpd'),\n)\nMEDIA_EXTENSIONS.video += MEDIA_EXTENSIONS.common_video\nMEDIA_EXTENSIONS.audio += MEDIA_EXTENSIONS.common_audio\n\nKNOWN_EXTENSIONS = (*MEDIA_EXTENSIONS.video, *MEDIA_EXTENSIONS.audio, *MEDIA_EXTENSIONS.manifests)\n\n\nclass RetryManager:\n    \"\"\"Usage:\n        for retry in RetryManager(...):\n            try:\n                ...\n            except SomeException as err:\n                retry.error = err\n                continue\n    \"\"\"\n    attempt, _error = 0, None\n\n    def __init__(self, _retries, _error_callback, **kwargs):\n        self.retries = _retries or 0\n        self.error_callback = functools.partial(_error_callback, **kwargs)\n\n    def _should_retry(self):\n        return self._error is not NO_DEFAULT and self.attempt <= self.retries\n\n    @property\n    def error(self):\n        if self._error is NO_DEFAULT:\n            return None\n        return self._error\n\n    @error.setter\n    def error(self, value):\n        self._error = value\n\n    def __iter__(self):\n        while self._should_retry():\n            self.error = NO_DEFAULT\n            self.attempt += 1\n            yield self\n            if self.error:\n                self.error_callback(self.error, self.attempt, self.retries)\n\n    @staticmethod\n    def report_retry(e, count, retries, *, sleep_func, info, warn, error=None, suffix=None):\n        \"\"\"Utility function for reporting retries\"\"\"\n        if count > retries:\n            if error:\n                return error(f'{e}. Giving up after {count - 1} retries') if count > 1 else error(str(e))\n            raise e\n\n        if not count:\n            return warn(e)\n        elif isinstance(e, ExtractorError):\n            e = remove_end(str_or_none(e.cause) or e.orig_msg, '.')\n        warn(f'{e}. Retrying{format_field(suffix, None, \" %s\")} ({count}/{retries})...')\n\n        delay = float_or_none(sleep_func(n=count - 1)) if callable(sleep_func) else sleep_func\n        if delay:\n            info(f'Sleeping {delay:.2f} seconds ...')\n            time.sleep(delay)\n\n\ndef make_archive_id(ie, video_id):\n    ie_key = ie if isinstance(ie, str) else ie.ie_key()\n    return f'{ie_key.lower()} {video_id}'\n\n\ndef truncate_string(s, left, right=0):\n    assert left > 3 and right >= 0\n    if s is None or len(s) <= left + right:\n        return s\n    return f'{s[:left-3]}...{s[-right:] if right else \"\"}'\n\n\ndef orderedSet_from_options(options, alias_dict, *, use_regex=False, start=None):\n    assert 'all' in alias_dict, '\"all\" alias is required'\n    requested = list(start or [])\n    for val in options:\n        discard = val.startswith('-')\n        if discard:\n            val = val[1:]\n\n        if val in alias_dict:\n            val = alias_dict[val] if not discard else [\n                i[1:] if i.startswith('-') else f'-{i}' for i in alias_dict[val]]\n            # NB: Do not allow regex in aliases for performance\n            requested = orderedSet_from_options(val, alias_dict, start=requested)\n            continue\n\n        current = (filter(re.compile(val, re.I).fullmatch, alias_dict['all']) if use_regex\n                   else [val] if val in alias_dict['all'] else None)\n        if current is None:\n            raise ValueError(val)\n\n        if discard:\n            for item in current:\n                while item in requested:\n                    requested.remove(item)\n        else:\n            requested.extend(current)\n\n    return orderedSet(requested)\n\n\n# TODO: Rewrite\nclass FormatSorter:\n    regex = r' *((?P<reverse>\\+)?(?P<field>[a-zA-Z0-9_]+)((?P<separator>[~:])(?P<limit>.*?))?)? *$'\n\n    default = ('hidden', 'aud_or_vid', 'hasvid', 'ie_pref', 'lang', 'quality',\n               'res', 'fps', 'hdr:12', 'vcodec:vp9.2', 'channels', 'acodec',\n               'size', 'br', 'asr', 'proto', 'ext', 'hasaud', 'source', 'id')  # These must not be aliases\n    ytdl_default = ('hasaud', 'lang', 'quality', 'tbr', 'filesize', 'vbr',\n                    'height', 'width', 'proto', 'vext', 'abr', 'aext',\n                    'fps', 'fs_approx', 'source', 'id')\n\n    settings = {\n        'vcodec': {'type': 'ordered', 'regex': True,\n                   'order': ['av0?1', 'vp0?9.2', 'vp0?9', '[hx]265|he?vc?', '[hx]264|avc', 'vp0?8', 'mp4v|h263', 'theora', '', None, 'none']},\n        'acodec': {'type': 'ordered', 'regex': True,\n                   'order': ['[af]lac', 'wav|aiff', 'opus', 'vorbis|ogg', 'aac', 'mp?4a?', 'mp3', 'ac-?4', 'e-?a?c-?3', 'ac-?3', 'dts', '', None, 'none']},\n        'hdr': {'type': 'ordered', 'regex': True, 'field': 'dynamic_range',\n                'order': ['dv', '(hdr)?12', r'(hdr)?10\\+', '(hdr)?10', 'hlg', '', 'sdr', None]},\n        'proto': {'type': 'ordered', 'regex': True, 'field': 'protocol',\n                  'order': ['(ht|f)tps', '(ht|f)tp$', 'm3u8.*', '.*dash', 'websocket_frag', 'rtmpe?', '', 'mms|rtsp', 'ws|websocket', 'f4']},\n        'vext': {'type': 'ordered', 'field': 'video_ext',\n                 'order': ('mp4', 'mov', 'webm', 'flv', '', 'none'),\n                 'order_free': ('webm', 'mp4', 'mov', 'flv', '', 'none')},\n        'aext': {'type': 'ordered', 'regex': True, 'field': 'audio_ext',\n                 'order': ('m4a', 'aac', 'mp3', 'ogg', 'opus', 'web[am]', '', 'none'),\n                 'order_free': ('ogg', 'opus', 'web[am]', 'mp3', 'm4a', 'aac', '', 'none')},\n        'hidden': {'visible': False, 'forced': True, 'type': 'extractor', 'max': -1000},\n        'aud_or_vid': {'visible': False, 'forced': True, 'type': 'multiple',\n                       'field': ('vcodec', 'acodec'),\n                       'function': lambda it: int(any(v != 'none' for v in it))},\n        'ie_pref': {'priority': True, 'type': 'extractor'},\n        'hasvid': {'priority': True, 'field': 'vcodec', 'type': 'boolean', 'not_in_list': ('none',)},\n        'hasaud': {'field': 'acodec', 'type': 'boolean', 'not_in_list': ('none',)},\n        'lang': {'convert': 'float', 'field': 'language_preference', 'default': -1},\n        'quality': {'convert': 'float', 'default': -1},\n        'filesize': {'convert': 'bytes'},\n        'fs_approx': {'convert': 'bytes', 'field': 'filesize_approx'},\n        'id': {'convert': 'string', 'field': 'format_id'},\n        'height': {'convert': 'float_none'},\n        'width': {'convert': 'float_none'},\n        'fps': {'convert': 'float_none'},\n        'channels': {'convert': 'float_none', 'field': 'audio_channels'},\n        'tbr': {'convert': 'float_none'},\n        'vbr': {'convert': 'float_none'},\n        'abr': {'convert': 'float_none'},\n        'asr': {'convert': 'float_none'},\n        'source': {'convert': 'float', 'field': 'source_preference', 'default': -1},\n\n        'codec': {'type': 'combined', 'field': ('vcodec', 'acodec')},\n        'br': {'type': 'multiple', 'field': ('tbr', 'vbr', 'abr'), 'convert': 'float_none',\n               'function': lambda it: next(filter(None, it), None)},\n        'size': {'type': 'multiple', 'field': ('filesize', 'fs_approx'), 'convert': 'bytes',\n                 'function': lambda it: next(filter(None, it), None)},\n        'ext': {'type': 'combined', 'field': ('vext', 'aext')},\n        'res': {'type': 'multiple', 'field': ('height', 'width'),\n                'function': lambda it: (lambda l: min(l) if l else 0)(tuple(filter(None, it)))},\n\n        # Actual field names\n        'format_id': {'type': 'alias', 'field': 'id'},\n        'preference': {'type': 'alias', 'field': 'ie_pref'},\n        'language_preference': {'type': 'alias', 'field': 'lang'},\n        'source_preference': {'type': 'alias', 'field': 'source'},\n        'protocol': {'type': 'alias', 'field': 'proto'},\n        'filesize_approx': {'type': 'alias', 'field': 'fs_approx'},\n        'audio_channels': {'type': 'alias', 'field': 'channels'},\n\n        # Deprecated\n        'dimension': {'type': 'alias', 'field': 'res', 'deprecated': True},\n        'resolution': {'type': 'alias', 'field': 'res', 'deprecated': True},\n        'extension': {'type': 'alias', 'field': 'ext', 'deprecated': True},\n        'bitrate': {'type': 'alias', 'field': 'br', 'deprecated': True},\n        'total_bitrate': {'type': 'alias', 'field': 'tbr', 'deprecated': True},\n        'video_bitrate': {'type': 'alias', 'field': 'vbr', 'deprecated': True},\n        'audio_bitrate': {'type': 'alias', 'field': 'abr', 'deprecated': True},\n        'framerate': {'type': 'alias', 'field': 'fps', 'deprecated': True},\n        'filesize_estimate': {'type': 'alias', 'field': 'size', 'deprecated': True},\n        'samplerate': {'type': 'alias', 'field': 'asr', 'deprecated': True},\n        'video_ext': {'type': 'alias', 'field': 'vext', 'deprecated': True},\n        'audio_ext': {'type': 'alias', 'field': 'aext', 'deprecated': True},\n        'video_codec': {'type': 'alias', 'field': 'vcodec', 'deprecated': True},\n        'audio_codec': {'type': 'alias', 'field': 'acodec', 'deprecated': True},\n        'video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},\n        'has_video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},\n        'audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},\n        'has_audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},\n        'extractor': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},\n        'extractor_preference': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},\n    }\n\n    def __init__(self, ydl, field_preference):\n        self.ydl = ydl\n        self._order = []\n        self.evaluate_params(self.ydl.params, field_preference)\n        if ydl.params.get('verbose'):\n            self.print_verbose_info(self.ydl.write_debug)\n\n    def _get_field_setting(self, field, key):\n        if field not in self.settings:\n            if key in ('forced', 'priority'):\n                return False\n            self.ydl.deprecated_feature(f'Using arbitrary fields ({field}) for format sorting is '\n                                        'deprecated and may be removed in a future version')\n            self.settings[field] = {}\n        propObj = self.settings[field]\n        if key not in propObj:\n            type = propObj.get('type')\n            if key == 'field':\n                default = 'preference' if type == 'extractor' else (field,) if type in ('combined', 'multiple') else field\n            elif key == 'convert':\n                default = 'order' if type == 'ordered' else 'float_string' if field else 'ignore'\n            else:\n                default = {'type': 'field', 'visible': True, 'order': [], 'not_in_list': (None,)}.get(key, None)\n            propObj[key] = default\n        return propObj[key]\n\n    def _resolve_field_value(self, field, value, convertNone=False):\n        if value is None:\n            if not convertNone:\n                return None\n        else:\n            value = value.lower()\n        conversion = self._get_field_setting(field, 'convert')\n        if conversion == 'ignore':\n            return None\n        if conversion == 'string':\n            return value\n        elif conversion == 'float_none':\n            return float_or_none(value)\n        elif conversion == 'bytes':\n            return parse_bytes(value)\n        elif conversion == 'order':\n            order_list = (self._use_free_order and self._get_field_setting(field, 'order_free')) or self._get_field_setting(field, 'order')\n            use_regex = self._get_field_setting(field, 'regex')\n            list_length = len(order_list)\n            empty_pos = order_list.index('') if '' in order_list else list_length + 1\n            if use_regex and value is not None:\n                for i, regex in enumerate(order_list):\n                    if regex and re.match(regex, value):\n                        return list_length - i\n                return list_length - empty_pos  # not in list\n            else:  # not regex or  value = None\n                return list_length - (order_list.index(value) if value in order_list else empty_pos)\n        else:\n            if value.isnumeric():\n                return float(value)\n            else:\n                self.settings[field]['convert'] = 'string'\n                return value\n\n    def evaluate_params(self, params, sort_extractor):\n        self._use_free_order = params.get('prefer_free_formats', False)\n        self._sort_user = params.get('format_sort', [])\n        self._sort_extractor = sort_extractor\n\n        def add_item(field, reverse, closest, limit_text):\n            field = field.lower()\n            if field in self._order:\n                return\n            self._order.append(field)\n            limit = self._resolve_field_value(field, limit_text)\n            data = {\n                'reverse': reverse,\n                'closest': False if limit is None else closest,\n                'limit_text': limit_text,\n                'limit': limit}\n            if field in self.settings:\n                self.settings[field].update(data)\n            else:\n                self.settings[field] = data\n\n        sort_list = (\n            tuple(field for field in self.default if self._get_field_setting(field, 'forced'))\n            + (tuple() if params.get('format_sort_force', False)\n                else tuple(field for field in self.default if self._get_field_setting(field, 'priority')))\n            + tuple(self._sort_user) + tuple(sort_extractor) + self.default)\n\n        for item in sort_list:\n            match = re.match(self.regex, item)\n            if match is None:\n                raise ExtractorError('Invalid format sort string \"%s\" given by extractor' % item)\n            field = match.group('field')\n            if field is None:\n                continue\n            if self._get_field_setting(field, 'type') == 'alias':\n                alias, field = field, self._get_field_setting(field, 'field')\n                if self._get_field_setting(alias, 'deprecated'):\n                    self.ydl.deprecated_feature(f'Format sorting alias {alias} is deprecated and may '\n                                                f'be removed in a future version. Please use {field} instead')\n            reverse = match.group('reverse') is not None\n            closest = match.group('separator') == '~'\n            limit_text = match.group('limit')\n\n            has_limit = limit_text is not None\n            has_multiple_fields = self._get_field_setting(field, 'type') == 'combined'\n            has_multiple_limits = has_limit and has_multiple_fields and not self._get_field_setting(field, 'same_limit')\n\n            fields = self._get_field_setting(field, 'field') if has_multiple_fields else (field,)\n            limits = limit_text.split(':') if has_multiple_limits else (limit_text,) if has_limit else tuple()\n            limit_count = len(limits)\n            for (i, f) in enumerate(fields):\n                add_item(f, reverse, closest,\n                         limits[i] if i < limit_count\n                         else limits[0] if has_limit and not has_multiple_limits\n                         else None)\n\n    def print_verbose_info(self, write_debug):\n        if self._sort_user:\n            write_debug('Sort order given by user: %s' % ', '.join(self._sort_user))\n        if self._sort_extractor:\n            write_debug('Sort order given by extractor: %s' % ', '.join(self._sort_extractor))\n        write_debug('Formats sorted by: %s' % ', '.join(['%s%s%s' % (\n            '+' if self._get_field_setting(field, 'reverse') else '', field,\n            '%s%s(%s)' % ('~' if self._get_field_setting(field, 'closest') else ':',\n                          self._get_field_setting(field, 'limit_text'),\n                          self._get_field_setting(field, 'limit'))\n            if self._get_field_setting(field, 'limit_text') is not None else '')\n            for field in self._order if self._get_field_setting(field, 'visible')]))\n\n    def _calculate_field_preference_from_value(self, format, field, type, value):\n        reverse = self._get_field_setting(field, 'reverse')\n        closest = self._get_field_setting(field, 'closest')\n        limit = self._get_field_setting(field, 'limit')\n\n        if type == 'extractor':\n            maximum = self._get_field_setting(field, 'max')\n            if value is None or (maximum is not None and value >= maximum):\n                value = -1\n        elif type == 'boolean':\n            in_list = self._get_field_setting(field, 'in_list')\n            not_in_list = self._get_field_setting(field, 'not_in_list')\n            value = 0 if ((in_list is None or value in in_list) and (not_in_list is None or value not in not_in_list)) else -1\n        elif type == 'ordered':\n            value = self._resolve_field_value(field, value, True)\n\n        # try to convert to number\n        val_num = float_or_none(value, default=self._get_field_setting(field, 'default'))\n        is_num = self._get_field_setting(field, 'convert') != 'string' and val_num is not None\n        if is_num:\n            value = val_num\n\n        return ((-10, 0) if value is None\n                else (1, value, 0) if not is_num  # if a field has mixed strings and numbers, strings are sorted higher\n                else (0, -abs(value - limit), value - limit if reverse else limit - value) if closest\n                else (0, value, 0) if not reverse and (limit is None or value <= limit)\n                else (0, -value, 0) if limit is None or (reverse and value == limit) or value > limit\n                else (-1, value, 0))\n\n    def _calculate_field_preference(self, format, field):\n        type = self._get_field_setting(field, 'type')  # extractor, boolean, ordered, field, multiple\n        get_value = lambda f: format.get(self._get_field_setting(f, 'field'))\n        if type == 'multiple':\n            type = 'field'  # Only 'field' is allowed in multiple for now\n            actual_fields = self._get_field_setting(field, 'field')\n\n            value = self._get_field_setting(field, 'function')(get_value(f) for f in actual_fields)\n        else:\n            value = get_value(field)\n        return self._calculate_field_preference_from_value(format, field, type, value)\n\n    def calculate_preference(self, format):\n        # Determine missing protocol\n        if not format.get('protocol'):\n            format['protocol'] = determine_protocol(format)\n\n        # Determine missing ext\n        if not format.get('ext') and 'url' in format:\n            format['ext'] = determine_ext(format['url'])\n        if format.get('vcodec') == 'none':\n            format['audio_ext'] = format['ext'] if format.get('acodec') != 'none' else 'none'\n            format['video_ext'] = 'none'\n        else:\n            format['video_ext'] = format['ext']\n            format['audio_ext'] = 'none'\n        # if format.get('preference') is None and format.get('ext') in ('f4f', 'f4m'):  # Not supported?\n        #    format['preference'] = -1000\n\n        if format.get('preference') is None and format.get('ext') == 'flv' and re.match('[hx]265|he?vc?', format.get('vcodec') or ''):\n            # HEVC-over-FLV is out-of-spec by FLV's original spec\n            # ref. https://trac.ffmpeg.org/ticket/6389\n            # ref. https://github.com/yt-dlp/yt-dlp/pull/5821\n            format['preference'] = -100\n\n        # Determine missing bitrates\n        if format.get('vcodec') == 'none':\n            format['vbr'] = 0\n        if format.get('acodec') == 'none':\n            format['abr'] = 0\n        if not format.get('vbr') and format.get('vcodec') != 'none':\n            format['vbr'] = try_call(lambda: format['tbr'] - format['abr']) or None\n        if not format.get('abr') and format.get('acodec') != 'none':\n            format['abr'] = try_call(lambda: format['tbr'] - format['vbr']) or None\n        if not format.get('tbr'):\n            format['tbr'] = try_call(lambda: format['vbr'] + format['abr']) or None\n\n        return tuple(self._calculate_field_preference(format, field) for field in self._order)\n", "code_before": "import asyncio\nimport atexit\nimport base64\nimport binascii\nimport calendar\nimport codecs\nimport collections\nimport collections.abc\nimport contextlib\nimport datetime\nimport email.header\nimport email.utils\nimport errno\nimport gzip\nimport hashlib\nimport hmac\nimport html.entities\nimport html.parser\nimport http.client\nimport http.cookiejar\nimport inspect\nimport io\nimport itertools\nimport json\nimport locale\nimport math\nimport mimetypes\nimport netrc\nimport operator\nimport os\nimport platform\nimport random\nimport re\nimport shlex\nimport socket\nimport ssl\nimport struct\nimport subprocess\nimport sys\nimport tempfile\nimport time\nimport traceback\nimport types\nimport unicodedata\nimport urllib.error\nimport urllib.parse\nimport urllib.request\nimport xml.etree.ElementTree\nimport zlib\n\nfrom . import traversal\n\nfrom ..compat import functools  # isort: split\nfrom ..compat import (\n    compat_etree_fromstring,\n    compat_expanduser,\n    compat_HTMLParseError,\n    compat_os_name,\n    compat_shlex_quote,\n)\nfrom ..dependencies import brotli, certifi, websockets, xattr\nfrom ..socks import ProxyType, sockssocket\n\n__name__ = __name__.rsplit('.', 1)[0]  # Pretend to be the parent module\n\n# This is not clearly defined otherwise\ncompiled_regex_type = type(re.compile(''))\n\n\ndef random_user_agent():\n    _USER_AGENT_TPL = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/%s Safari/537.36'\n    _CHROME_VERSIONS = (\n        '90.0.4430.212',\n        '90.0.4430.24',\n        '90.0.4430.70',\n        '90.0.4430.72',\n        '90.0.4430.85',\n        '90.0.4430.93',\n        '91.0.4472.101',\n        '91.0.4472.106',\n        '91.0.4472.114',\n        '91.0.4472.124',\n        '91.0.4472.164',\n        '91.0.4472.19',\n        '91.0.4472.77',\n        '92.0.4515.107',\n        '92.0.4515.115',\n        '92.0.4515.131',\n        '92.0.4515.159',\n        '92.0.4515.43',\n        '93.0.4556.0',\n        '93.0.4577.15',\n        '93.0.4577.63',\n        '93.0.4577.82',\n        '94.0.4606.41',\n        '94.0.4606.54',\n        '94.0.4606.61',\n        '94.0.4606.71',\n        '94.0.4606.81',\n        '94.0.4606.85',\n        '95.0.4638.17',\n        '95.0.4638.50',\n        '95.0.4638.54',\n        '95.0.4638.69',\n        '95.0.4638.74',\n        '96.0.4664.18',\n        '96.0.4664.45',\n        '96.0.4664.55',\n        '96.0.4664.93',\n        '97.0.4692.20',\n    )\n    return _USER_AGENT_TPL % random.choice(_CHROME_VERSIONS)\n\n\nSUPPORTED_ENCODINGS = [\n    'gzip', 'deflate'\n]\nif brotli:\n    SUPPORTED_ENCODINGS.append('br')\n\nstd_headers = {\n    'User-Agent': random_user_agent(),\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en-us,en;q=0.5',\n    'Sec-Fetch-Mode': 'navigate',\n}\n\n\nUSER_AGENTS = {\n    'Safari': 'Mozilla/5.0 (X11; Linux x86_64; rv:10.0) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27',\n}\n\n\nclass NO_DEFAULT:\n    pass\n\n\ndef IDENTITY(x):\n    return x\n\n\nENGLISH_MONTH_NAMES = [\n    'January', 'February', 'March', 'April', 'May', 'June',\n    'July', 'August', 'September', 'October', 'November', 'December']\n\nMONTH_NAMES = {\n    'en': ENGLISH_MONTH_NAMES,\n    'fr': [\n        'janvier', 'f\u00e9vrier', 'mars', 'avril', 'mai', 'juin',\n        'juillet', 'ao\u00fbt', 'septembre', 'octobre', 'novembre', 'd\u00e9cembre'],\n    # these follow the genitive grammatical case (dope\u0142niacz)\n    # some websites might be using nominative, which will require another month list\n    # https://en.wikibooks.org/wiki/Polish/Noun_cases\n    'pl': ['stycznia', 'lutego', 'marca', 'kwietnia', 'maja', 'czerwca',\n           'lipca', 'sierpnia', 'wrze\u015bnia', 'pa\u017adziernika', 'listopada', 'grudnia'],\n}\n\n# From https://github.com/python/cpython/blob/3.11/Lib/email/_parseaddr.py#L36-L42\nTIMEZONE_NAMES = {\n    'UT': 0, 'UTC': 0, 'GMT': 0, 'Z': 0,\n    'AST': -4, 'ADT': -3,  # Atlantic (used in Canada)\n    'EST': -5, 'EDT': -4,  # Eastern\n    'CST': -6, 'CDT': -5,  # Central\n    'MST': -7, 'MDT': -6,  # Mountain\n    'PST': -8, 'PDT': -7   # Pacific\n}\n\n# needed for sanitizing filenames in restricted mode\nACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',\n                        itertools.chain('AAAAAA', ['AE'], 'CEEEEIIIIDNOOOOOOO', ['OE'], 'UUUUUY', ['TH', 'ss'],\n                                        'aaaaaa', ['ae'], 'ceeeeiiiionooooooo', ['oe'], 'uuuuuy', ['th'], 'y')))\n\nDATE_FORMATS = (\n    '%d %B %Y',\n    '%d %b %Y',\n    '%B %d %Y',\n    '%B %dst %Y',\n    '%B %dnd %Y',\n    '%B %drd %Y',\n    '%B %dth %Y',\n    '%b %d %Y',\n    '%b %dst %Y',\n    '%b %dnd %Y',\n    '%b %drd %Y',\n    '%b %dth %Y',\n    '%b %dst %Y %I:%M',\n    '%b %dnd %Y %I:%M',\n    '%b %drd %Y %I:%M',\n    '%b %dth %Y %I:%M',\n    '%Y %m %d',\n    '%Y-%m-%d',\n    '%Y.%m.%d.',\n    '%Y/%m/%d',\n    '%Y/%m/%d %H:%M',\n    '%Y/%m/%d %H:%M:%S',\n    '%Y%m%d%H%M',\n    '%Y%m%d%H%M%S',\n    '%Y%m%d',\n    '%Y-%m-%d %H:%M',\n    '%Y-%m-%d %H:%M:%S',\n    '%Y-%m-%d %H:%M:%S.%f',\n    '%Y-%m-%d %H:%M:%S:%f',\n    '%d.%m.%Y %H:%M',\n    '%d.%m.%Y %H.%M',\n    '%Y-%m-%dT%H:%M:%SZ',\n    '%Y-%m-%dT%H:%M:%S.%fZ',\n    '%Y-%m-%dT%H:%M:%S.%f0Z',\n    '%Y-%m-%dT%H:%M:%S',\n    '%Y-%m-%dT%H:%M:%S.%f',\n    '%Y-%m-%dT%H:%M',\n    '%b %d %Y at %H:%M',\n    '%b %d %Y at %H:%M:%S',\n    '%B %d %Y at %H:%M',\n    '%B %d %Y at %H:%M:%S',\n    '%H:%M %d-%b-%Y',\n)\n\nDATE_FORMATS_DAY_FIRST = list(DATE_FORMATS)\nDATE_FORMATS_DAY_FIRST.extend([\n    '%d-%m-%Y',\n    '%d.%m.%Y',\n    '%d.%m.%y',\n    '%d/%m/%Y',\n    '%d/%m/%y',\n    '%d/%m/%Y %H:%M:%S',\n    '%d-%m-%Y %H:%M',\n    '%H:%M %d/%m/%Y',\n])\n\nDATE_FORMATS_MONTH_FIRST = list(DATE_FORMATS)\nDATE_FORMATS_MONTH_FIRST.extend([\n    '%m-%d-%Y',\n    '%m.%d.%Y',\n    '%m/%d/%Y',\n    '%m/%d/%y',\n    '%m/%d/%Y %H:%M:%S',\n])\n\nPACKED_CODES_RE = r\"}\\('(.+)',(\\d+),(\\d+),'([^']+)'\\.split\\('\\|'\\)\"\nJSON_LD_RE = r'(?is)<script[^>]+type=([\"\\']?)application/ld\\+json\\1[^>]*>\\s*(?P<json_ld>{.+?}|\\[.+?\\])\\s*</script>'\n\nNUMBER_RE = r'\\d+(?:\\.\\d+)?'\n\n\n@functools.cache\ndef preferredencoding():\n    \"\"\"Get preferred encoding.\n\n    Returns the best encoding scheme for the system, based on\n    locale.getpreferredencoding() and some further tweaks.\n    \"\"\"\n    try:\n        pref = locale.getpreferredencoding()\n        'TEST'.encode(pref)\n    except Exception:\n        pref = 'UTF-8'\n\n    return pref\n\n\ndef write_json_file(obj, fn):\n    \"\"\" Encode obj as JSON and write it to fn, atomically if possible \"\"\"\n\n    tf = tempfile.NamedTemporaryFile(\n        prefix=f'{os.path.basename(fn)}.', dir=os.path.dirname(fn),\n        suffix='.tmp', delete=False, mode='w', encoding='utf-8')\n\n    try:\n        with tf:\n            json.dump(obj, tf, ensure_ascii=False)\n        if sys.platform == 'win32':\n            # Need to remove existing file on Windows, else os.rename raises\n            # WindowsError or FileExistsError.\n            with contextlib.suppress(OSError):\n                os.unlink(fn)\n        with contextlib.suppress(OSError):\n            mask = os.umask(0)\n            os.umask(mask)\n            os.chmod(tf.name, 0o666 & ~mask)\n        os.rename(tf.name, fn)\n    except Exception:\n        with contextlib.suppress(OSError):\n            os.remove(tf.name)\n        raise\n\n\ndef find_xpath_attr(node, xpath, key, val=None):\n    \"\"\" Find the xpath xpath[@key=val] \"\"\"\n    assert re.match(r'^[a-zA-Z_-]+$', key)\n    expr = xpath + ('[@%s]' % key if val is None else f\"[@{key}='{val}']\")\n    return node.find(expr)\n\n# On python2.6 the xml.etree.ElementTree.Element methods don't support\n# the namespace parameter\n\n\ndef xpath_with_ns(path, ns_map):\n    components = [c.split(':') for c in path.split('/')]\n    replaced = []\n    for c in components:\n        if len(c) == 1:\n            replaced.append(c[0])\n        else:\n            ns, tag = c\n            replaced.append('{%s}%s' % (ns_map[ns], tag))\n    return '/'.join(replaced)\n\n\ndef xpath_element(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    def _find_xpath(xpath):\n        return node.find(xpath)\n\n    if isinstance(xpath, str):\n        n = _find_xpath(xpath)\n    else:\n        for xp in xpath:\n            n = _find_xpath(xp)\n            if n is not None:\n                break\n\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element %s' % name)\n        else:\n            return None\n    return n\n\n\ndef xpath_text(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    n = xpath_element(node, xpath, name, fatal=fatal, default=default)\n    if n is None or n == default:\n        return n\n    if n.text is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element\\'s text %s' % name)\n        else:\n            return None\n    return n.text\n\n\ndef xpath_attr(node, xpath, key, name=None, fatal=False, default=NO_DEFAULT):\n    n = find_xpath_attr(node, xpath, key)\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = f'{xpath}[@{key}]' if name is None else name\n            raise ExtractorError('Could not find XML attribute %s' % name)\n        else:\n            return None\n    return n.attrib[key]\n\n\ndef get_element_by_id(id, html, **kwargs):\n    \"\"\"Return the content of the tag with the specified ID in the passed HTML document\"\"\"\n    return get_element_by_attribute('id', id, html, **kwargs)\n\n\ndef get_element_html_by_id(id, html, **kwargs):\n    \"\"\"Return the html of the tag with the specified ID in the passed HTML document\"\"\"\n    return get_element_html_by_attribute('id', id, html, **kwargs)\n\n\ndef get_element_by_class(class_name, html):\n    \"\"\"Return the content of the first tag with the specified class in the passed HTML document\"\"\"\n    retval = get_elements_by_class(class_name, html)\n    return retval[0] if retval else None\n\n\ndef get_element_html_by_class(class_name, html):\n    \"\"\"Return the html of the first tag with the specified class in the passed HTML document\"\"\"\n    retval = get_elements_html_by_class(class_name, html)\n    return retval[0] if retval else None\n\n\ndef get_element_by_attribute(attribute, value, html, **kwargs):\n    retval = get_elements_by_attribute(attribute, value, html, **kwargs)\n    return retval[0] if retval else None\n\n\ndef get_element_html_by_attribute(attribute, value, html, **kargs):\n    retval = get_elements_html_by_attribute(attribute, value, html, **kargs)\n    return retval[0] if retval else None\n\n\ndef get_elements_by_class(class_name, html, **kargs):\n    \"\"\"Return the content of all tags with the specified class in the passed HTML document as a list\"\"\"\n    return get_elements_by_attribute(\n        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),\n        html, escape_value=False)\n\n\ndef get_elements_html_by_class(class_name, html):\n    \"\"\"Return the html of all tags with the specified class in the passed HTML document as a list\"\"\"\n    return get_elements_html_by_attribute(\n        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),\n        html, escape_value=False)\n\n\ndef get_elements_by_attribute(*args, **kwargs):\n    \"\"\"Return the content of the tag with the specified attribute in the passed HTML document\"\"\"\n    return [content for content, _ in get_elements_text_and_html_by_attribute(*args, **kwargs)]\n\n\ndef get_elements_html_by_attribute(*args, **kwargs):\n    \"\"\"Return the html of the tag with the specified attribute in the passed HTML document\"\"\"\n    return [whole for _, whole in get_elements_text_and_html_by_attribute(*args, **kwargs)]\n\n\ndef get_elements_text_and_html_by_attribute(attribute, value, html, *, tag=r'[\\w:.-]+', escape_value=True):\n    \"\"\"\n    Return the text (content) and the html (whole) of the tag with the specified\n    attribute in the passed HTML document\n    \"\"\"\n    if not value:\n        return\n\n    quote = '' if re.match(r'''[\\s\"'`=<>]''', value) else '?'\n\n    value = re.escape(value) if escape_value else value\n\n    partial_element_re = rf'''(?x)\n        <(?P<tag>{tag})\n         (?:\\s(?:[^>\"']|\"[^\"]*\"|'[^']*')*)?\n         \\s{re.escape(attribute)}\\s*=\\s*(?P<_q>['\"]{quote})(?-x:{value})(?P=_q)\n        '''\n\n    for m in re.finditer(partial_element_re, html):\n        content, whole = get_element_text_and_html_by_tag(m.group('tag'), html[m.start():])\n\n        yield (\n            unescapeHTML(re.sub(r'^(?P<q>[\"\\'])(?P<content>.*)(?P=q)$', r'\\g<content>', content, flags=re.DOTALL)),\n            whole\n        )\n\n\nclass HTMLBreakOnClosingTagParser(html.parser.HTMLParser):\n    \"\"\"\n    HTML parser which raises HTMLBreakOnClosingTagException upon reaching the\n    closing tag for the first opening tag it has encountered, and can be used\n    as a context manager\n    \"\"\"\n\n    class HTMLBreakOnClosingTagException(Exception):\n        pass\n\n    def __init__(self):\n        self.tagstack = collections.deque()\n        html.parser.HTMLParser.__init__(self)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *_):\n        self.close()\n\n    def close(self):\n        # handle_endtag does not return upon raising HTMLBreakOnClosingTagException,\n        # so data remains buffered; we no longer have any interest in it, thus\n        # override this method to discard it\n        pass\n\n    def handle_starttag(self, tag, _):\n        self.tagstack.append(tag)\n\n    def handle_endtag(self, tag):\n        if not self.tagstack:\n            raise compat_HTMLParseError('no tags in the stack')\n        while self.tagstack:\n            inner_tag = self.tagstack.pop()\n            if inner_tag == tag:\n                break\n        else:\n            raise compat_HTMLParseError(f'matching opening tag for closing {tag} tag not found')\n        if not self.tagstack:\n            raise self.HTMLBreakOnClosingTagException()\n\n\n# XXX: This should be far less strict\ndef get_element_text_and_html_by_tag(tag, html):\n    \"\"\"\n    For the first element with the specified tag in the passed HTML document\n    return its' content (text) and the whole element (html)\n    \"\"\"\n    def find_or_raise(haystack, needle, exc):\n        try:\n            return haystack.index(needle)\n        except ValueError:\n            raise exc\n    closing_tag = f'</{tag}>'\n    whole_start = find_or_raise(\n        html, f'<{tag}', compat_HTMLParseError(f'opening {tag} tag not found'))\n    content_start = find_or_raise(\n        html[whole_start:], '>', compat_HTMLParseError(f'malformed opening {tag} tag'))\n    content_start += whole_start + 1\n    with HTMLBreakOnClosingTagParser() as parser:\n        parser.feed(html[whole_start:content_start])\n        if not parser.tagstack or parser.tagstack[0] != tag:\n            raise compat_HTMLParseError(f'parser did not match opening {tag} tag')\n        offset = content_start\n        while offset < len(html):\n            next_closing_tag_start = find_or_raise(\n                html[offset:], closing_tag,\n                compat_HTMLParseError(f'closing {tag} tag not found'))\n            next_closing_tag_end = next_closing_tag_start + len(closing_tag)\n            try:\n                parser.feed(html[offset:offset + next_closing_tag_end])\n                offset += next_closing_tag_end\n            except HTMLBreakOnClosingTagParser.HTMLBreakOnClosingTagException:\n                return html[content_start:offset + next_closing_tag_start], \\\n                    html[whole_start:offset + next_closing_tag_end]\n        raise compat_HTMLParseError('unexpected end of html')\n\n\nclass HTMLAttributeParser(html.parser.HTMLParser):\n    \"\"\"Trivial HTML parser to gather the attributes for a single element\"\"\"\n\n    def __init__(self):\n        self.attrs = {}\n        html.parser.HTMLParser.__init__(self)\n\n    def handle_starttag(self, tag, attrs):\n        self.attrs = dict(attrs)\n        raise compat_HTMLParseError('done')\n\n\nclass HTMLListAttrsParser(html.parser.HTMLParser):\n    \"\"\"HTML parser to gather the attributes for the elements of a list\"\"\"\n\n    def __init__(self):\n        html.parser.HTMLParser.__init__(self)\n        self.items = []\n        self._level = 0\n\n    def handle_starttag(self, tag, attrs):\n        if tag == 'li' and self._level == 0:\n            self.items.append(dict(attrs))\n        self._level += 1\n\n    def handle_endtag(self, tag):\n        self._level -= 1\n\n\ndef extract_attributes(html_element):\n    \"\"\"Given a string for an HTML element such as\n    <el\n         a=\"foo\" B=\"bar\" c=\"&98;az\" d=boz\n         empty= noval entity=\"&amp;\"\n         sq='\"' dq=\"'\"\n    >\n    Decode and return a dictionary of attributes.\n    {\n        'a': 'foo', 'b': 'bar', c: 'baz', d: 'boz',\n        'empty': '', 'noval': None, 'entity': '&',\n        'sq': '\"', 'dq': '\\''\n    }.\n    \"\"\"\n    parser = HTMLAttributeParser()\n    with contextlib.suppress(compat_HTMLParseError):\n        parser.feed(html_element)\n        parser.close()\n    return parser.attrs\n\n\ndef parse_list(webpage):\n    \"\"\"Given a string for an series of HTML <li> elements,\n    return a dictionary of their attributes\"\"\"\n    parser = HTMLListAttrsParser()\n    parser.feed(webpage)\n    parser.close()\n    return parser.items\n\n\ndef clean_html(html):\n    \"\"\"Clean an HTML snippet into a readable string\"\"\"\n\n    if html is None:  # Convenience for sanitizing descriptions etc.\n        return html\n\n    html = re.sub(r'\\s+', ' ', html)\n    html = re.sub(r'(?u)\\s?<\\s?br\\s?/?\\s?>\\s?', '\\n', html)\n    html = re.sub(r'(?u)<\\s?/\\s?p\\s?>\\s?<\\s?p[^>]*>', '\\n', html)\n    # Strip html tags\n    html = re.sub('<.*?>', '', html)\n    # Replace html entities\n    html = unescapeHTML(html)\n    return html.strip()\n\n\nclass LenientJSONDecoder(json.JSONDecoder):\n    # TODO: Write tests\n    def __init__(self, *args, transform_source=None, ignore_extra=False, close_objects=0, **kwargs):\n        self.transform_source, self.ignore_extra = transform_source, ignore_extra\n        self._close_attempts = 2 * close_objects\n        super().__init__(*args, **kwargs)\n\n    @staticmethod\n    def _close_object(err):\n        doc = err.doc[:err.pos]\n        # We need to add comma first to get the correct error message\n        if err.msg.startswith('Expecting \\',\\''):\n            return doc + ','\n        elif not doc.endswith(','):\n            return\n\n        if err.msg.startswith('Expecting property name'):\n            return doc[:-1] + '}'\n        elif err.msg.startswith('Expecting value'):\n            return doc[:-1] + ']'\n\n    def decode(self, s):\n        if self.transform_source:\n            s = self.transform_source(s)\n        for attempt in range(self._close_attempts + 1):\n            try:\n                if self.ignore_extra:\n                    return self.raw_decode(s.lstrip())[0]\n                return super().decode(s)\n            except json.JSONDecodeError as e:\n                if e.pos is None:\n                    raise\n                elif attempt < self._close_attempts:\n                    s = self._close_object(e)\n                    if s is not None:\n                        continue\n                raise type(e)(f'{e.msg} in {s[e.pos-10:e.pos+10]!r}', s, e.pos)\n        assert False, 'Too many attempts to decode JSON'\n\n\ndef sanitize_open(filename, open_mode):\n    \"\"\"Try to open the given filename, and slightly tweak it if this fails.\n\n    Attempts to open the given filename. If this fails, it tries to change\n    the filename slightly, step by step, until it's either able to open it\n    or it fails and raises a final exception, like the standard open()\n    function.\n\n    It returns the tuple (stream, definitive_file_name).\n    \"\"\"\n    if filename == '-':\n        if sys.platform == 'win32':\n            import msvcrt\n\n            # stdout may be any IO stream, e.g. when using contextlib.redirect_stdout\n            with contextlib.suppress(io.UnsupportedOperation):\n                msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n        return (sys.stdout.buffer if hasattr(sys.stdout, 'buffer') else sys.stdout, filename)\n\n    for attempt in range(2):\n        try:\n            try:\n                if sys.platform == 'win32':\n                    # FIXME: An exclusive lock also locks the file from being read.\n                    # Since windows locks are mandatory, don't lock the file on windows (for now).\n                    # Ref: https://github.com/yt-dlp/yt-dlp/issues/3124\n                    raise LockingUnsupportedError()\n                stream = locked_file(filename, open_mode, block=False).__enter__()\n            except OSError:\n                stream = open(filename, open_mode)\n            return stream, filename\n        except OSError as err:\n            if attempt or err.errno in (errno.EACCES,):\n                raise\n            old_filename, filename = filename, sanitize_path(filename)\n            if old_filename == filename:\n                raise\n\n\ndef timeconvert(timestr):\n    \"\"\"Convert RFC 2822 defined time string into system timestamp\"\"\"\n    timestamp = None\n    timetuple = email.utils.parsedate_tz(timestr)\n    if timetuple is not None:\n        timestamp = email.utils.mktime_tz(timetuple)\n    return timestamp\n\n\ndef sanitize_filename(s, restricted=False, is_id=NO_DEFAULT):\n    \"\"\"Sanitizes a string so it could be used as part of a filename.\n    @param restricted   Use a stricter subset of allowed characters\n    @param is_id        Whether this is an ID that should be kept unchanged if possible.\n                        If unset, yt-dlp's new sanitization rules are in effect\n    \"\"\"\n    if s == '':\n        return ''\n\n    def replace_insane(char):\n        if restricted and char in ACCENT_CHARS:\n            return ACCENT_CHARS[char]\n        elif not restricted and char == '\\n':\n            return '\\0 '\n        elif is_id is NO_DEFAULT and not restricted and char in '\"*:<>?|/\\\\':\n            # Replace with their full-width unicode counterparts\n            return {'/': '\\u29F8', '\\\\': '\\u29f9'}.get(char, chr(ord(char) + 0xfee0))\n        elif char == '?' or ord(char) < 32 or ord(char) == 127:\n            return ''\n        elif char == '\"':\n            return '' if restricted else '\\''\n        elif char == ':':\n            return '\\0_\\0-' if restricted else '\\0 \\0-'\n        elif char in '\\\\/|*<>':\n            return '\\0_'\n        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace() or ord(char) > 127):\n            return '\\0_'\n        return char\n\n    # Replace look-alike Unicode glyphs\n    if restricted and (is_id is NO_DEFAULT or not is_id):\n        s = unicodedata.normalize('NFKC', s)\n    s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s)  # Handle timestamps\n    result = ''.join(map(replace_insane, s))\n    if is_id is NO_DEFAULT:\n        result = re.sub(r'(\\0.)(?:(?=\\1)..)+', r'\\1', result)  # Remove repeated substitute chars\n        STRIP_RE = r'(?:\\0.|[ _-])*'\n        result = re.sub(f'^\\0.{STRIP_RE}|{STRIP_RE}\\0.$', '', result)  # Remove substitute chars from start/end\n    result = result.replace('\\0', '') or '_'\n\n    if not is_id:\n        while '__' in result:\n            result = result.replace('__', '_')\n        result = result.strip('_')\n        # Common case of \"Foreign band name - English song title\"\n        if restricted and result.startswith('-_'):\n            result = result[2:]\n        if result.startswith('-'):\n            result = '_' + result[len('-'):]\n        result = result.lstrip('.')\n        if not result:\n            result = '_'\n    return result\n\n\ndef sanitize_path(s, force=False):\n    \"\"\"Sanitizes and normalizes path on Windows\"\"\"\n    if sys.platform == 'win32':\n        force = False\n        drive_or_unc, _ = os.path.splitdrive(s)\n    elif force:\n        drive_or_unc = ''\n    else:\n        return s\n\n    norm_path = os.path.normpath(remove_start(s, drive_or_unc)).split(os.path.sep)\n    if drive_or_unc:\n        norm_path.pop(0)\n    sanitized_path = [\n        path_part if path_part in ['.', '..'] else re.sub(r'(?:[/<>:\"\\|\\\\?\\*]|[\\s.]$)', '#', path_part)\n        for path_part in norm_path]\n    if drive_or_unc:\n        sanitized_path.insert(0, drive_or_unc + os.path.sep)\n    elif force and s and s[0] == os.path.sep:\n        sanitized_path.insert(0, os.path.sep)\n    return os.path.join(*sanitized_path)\n\n\ndef sanitize_url(url, *, scheme='http'):\n    # Prepend protocol-less URLs with `http:` scheme in order to mitigate\n    # the number of unwanted failures due to missing protocol\n    if url is None:\n        return\n    elif url.startswith('//'):\n        return f'{scheme}:{url}'\n    # Fix some common typos seen so far\n    COMMON_TYPOS = (\n        # https://github.com/ytdl-org/youtube-dl/issues/15649\n        (r'^httpss://', r'https://'),\n        # https://bx1.be/lives/direct-tv/\n        (r'^rmtp([es]?)://', r'rtmp\\1://'),\n    )\n    for mistake, fixup in COMMON_TYPOS:\n        if re.match(mistake, url):\n            return re.sub(mistake, fixup, url)\n    return url\n\n\ndef extract_basic_auth(url):\n    parts = urllib.parse.urlsplit(url)\n    if parts.username is None:\n        return url, None\n    url = urllib.parse.urlunsplit(parts._replace(netloc=(\n        parts.hostname if parts.port is None\n        else '%s:%d' % (parts.hostname, parts.port))))\n    auth_payload = base64.b64encode(\n        ('%s:%s' % (parts.username, parts.password or '')).encode())\n    return url, f'Basic {auth_payload.decode()}'\n\n\ndef sanitized_Request(url, *args, **kwargs):\n    url, auth_header = extract_basic_auth(escape_url(sanitize_url(url)))\n    if auth_header is not None:\n        headers = args[1] if len(args) >= 2 else kwargs.setdefault('headers', {})\n        headers['Authorization'] = auth_header\n    return urllib.request.Request(url, *args, **kwargs)\n\n\ndef expand_path(s):\n    \"\"\"Expand shell variables and ~\"\"\"\n    return os.path.expandvars(compat_expanduser(s))\n\n\ndef orderedSet(iterable, *, lazy=False):\n    \"\"\"Remove all duplicates from the input iterable\"\"\"\n    def _iter():\n        seen = []  # Do not use set since the items can be unhashable\n        for x in iterable:\n            if x not in seen:\n                seen.append(x)\n                yield x\n\n    return _iter() if lazy else list(_iter())\n\n\ndef _htmlentity_transform(entity_with_semicolon):\n    \"\"\"Transforms an HTML entity to a character.\"\"\"\n    entity = entity_with_semicolon[:-1]\n\n    # Known non-numeric HTML entity\n    if entity in html.entities.name2codepoint:\n        return chr(html.entities.name2codepoint[entity])\n\n    # TODO: HTML5 allows entities without a semicolon.\n    # E.g. '&Eacuteric' should be decoded as '\u00c9ric'.\n    if entity_with_semicolon in html.entities.html5:\n        return html.entities.html5[entity_with_semicolon]\n\n    mobj = re.match(r'#(x[0-9a-fA-F]+|[0-9]+)', entity)\n    if mobj is not None:\n        numstr = mobj.group(1)\n        if numstr.startswith('x'):\n            base = 16\n            numstr = '0%s' % numstr\n        else:\n            base = 10\n        # See https://github.com/ytdl-org/youtube-dl/issues/7518\n        with contextlib.suppress(ValueError):\n            return chr(int(numstr, base))\n\n    # Unknown entity in name, return its literal representation\n    return '&%s;' % entity\n\n\ndef unescapeHTML(s):\n    if s is None:\n        return None\n    assert isinstance(s, str)\n\n    return re.sub(\n        r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)\n\n\ndef escapeHTML(text):\n    return (\n        text\n        .replace('&', '&amp;')\n        .replace('<', '&lt;')\n        .replace('>', '&gt;')\n        .replace('\"', '&quot;')\n        .replace(\"'\", '&#39;')\n    )\n\n\nclass netrc_from_content(netrc.netrc):\n    def __init__(self, content):\n        self.hosts, self.macros = {}, {}\n        with io.StringIO(content) as stream:\n            self._parse('-', stream, False)\n\n\nclass Popen(subprocess.Popen):\n    if sys.platform == 'win32':\n        _startupinfo = subprocess.STARTUPINFO()\n        _startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n    else:\n        _startupinfo = None\n\n    @staticmethod\n    def _fix_pyinstaller_ld_path(env):\n        \"\"\"Restore LD_LIBRARY_PATH when using PyInstaller\n            Ref: https://github.com/pyinstaller/pyinstaller/blob/develop/doc/runtime-information.rst#ld_library_path--libpath-considerations\n                 https://github.com/yt-dlp/yt-dlp/issues/4573\n        \"\"\"\n        if not hasattr(sys, '_MEIPASS'):\n            return\n\n        def _fix(key):\n            orig = env.get(f'{key}_ORIG')\n            if orig is None:\n                env.pop(key, None)\n            else:\n                env[key] = orig\n\n        _fix('LD_LIBRARY_PATH')  # Linux\n        _fix('DYLD_LIBRARY_PATH')  # macOS\n\n    def __init__(self, *args, env=None, text=False, **kwargs):\n        if env is None:\n            env = os.environ.copy()\n        self._fix_pyinstaller_ld_path(env)\n\n        self.__text_mode = kwargs.get('encoding') or kwargs.get('errors') or text or kwargs.get('universal_newlines')\n        if text is True:\n            kwargs['universal_newlines'] = True  # For 3.6 compatibility\n            kwargs.setdefault('encoding', 'utf-8')\n            kwargs.setdefault('errors', 'replace')\n        super().__init__(*args, env=env, **kwargs, startupinfo=self._startupinfo)\n\n    def communicate_or_kill(self, *args, **kwargs):\n        try:\n            return self.communicate(*args, **kwargs)\n        except BaseException:  # Including KeyboardInterrupt\n            self.kill(timeout=None)\n            raise\n\n    def kill(self, *, timeout=0):\n        super().kill()\n        if timeout != 0:\n            self.wait(timeout=timeout)\n\n    @classmethod\n    def run(cls, *args, timeout=None, **kwargs):\n        with cls(*args, **kwargs) as proc:\n            default = '' if proc.__text_mode else b''\n            stdout, stderr = proc.communicate_or_kill(timeout=timeout)\n            return stdout or default, stderr or default, proc.returncode\n\n\ndef encodeArgument(s):\n    # Legacy code that uses byte strings\n    # Uncomment the following line after fixing all post processors\n    # assert isinstance(s, str), 'Internal error: %r should be of type %r, is %r' % (s, str, type(s))\n    return s if isinstance(s, str) else s.decode('ascii')\n\n\n_timetuple = collections.namedtuple('Time', ('hours', 'minutes', 'seconds', 'milliseconds'))\n\n\ndef timetuple_from_msec(msec):\n    secs, msec = divmod(msec, 1000)\n    mins, secs = divmod(secs, 60)\n    hrs, mins = divmod(mins, 60)\n    return _timetuple(hrs, mins, secs, msec)\n\n\ndef formatSeconds(secs, delim=':', msec=False):\n    time = timetuple_from_msec(secs * 1000)\n    if time.hours:\n        ret = '%d%s%02d%s%02d' % (time.hours, delim, time.minutes, delim, time.seconds)\n    elif time.minutes:\n        ret = '%d%s%02d' % (time.minutes, delim, time.seconds)\n    else:\n        ret = '%d' % time.seconds\n    return '%s.%03d' % (ret, time.milliseconds) if msec else ret\n\n\ndef _ssl_load_windows_store_certs(ssl_context, storename):\n    # Code adapted from _load_windows_store_certs in https://github.com/python/cpython/blob/main/Lib/ssl.py\n    try:\n        certs = [cert for cert, encoding, trust in ssl.enum_certificates(storename)\n                 if encoding == 'x509_asn' and (\n                     trust is True or ssl.Purpose.SERVER_AUTH.oid in trust)]\n    except PermissionError:\n        return\n    for cert in certs:\n        with contextlib.suppress(ssl.SSLError):\n            ssl_context.load_verify_locations(cadata=cert)\n\n\ndef make_HTTPS_handler(params, **kwargs):\n    opts_check_certificate = not params.get('nocheckcertificate')\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n    context.check_hostname = opts_check_certificate\n    if params.get('legacyserverconnect'):\n        context.options |= 4  # SSL_OP_LEGACY_SERVER_CONNECT\n        # Allow use of weaker ciphers in Python 3.10+. See https://bugs.python.org/issue43998\n        context.set_ciphers('DEFAULT')\n    elif (\n        sys.version_info < (3, 10)\n        and ssl.OPENSSL_VERSION_INFO >= (1, 1, 1)\n        and not ssl.OPENSSL_VERSION.startswith('LibreSSL')\n    ):\n        # Backport the default SSL ciphers and minimum TLS version settings from Python 3.10 [1].\n        # This is to ensure consistent behavior across Python versions, and help avoid fingerprinting\n        # in some situations [2][3].\n        # Python 3.10 only supports OpenSSL 1.1.1+ [4]. Because this change is likely\n        # untested on older versions, we only apply this to OpenSSL 1.1.1+ to be safe.\n        # LibreSSL is excluded until further investigation due to cipher support issues [5][6].\n        # 1. https://github.com/python/cpython/commit/e983252b516edb15d4338b0a47631b59ef1e2536\n        # 2. https://github.com/yt-dlp/yt-dlp/issues/4627\n        # 3. https://github.com/yt-dlp/yt-dlp/pull/5294\n        # 4. https://peps.python.org/pep-0644/\n        # 5. https://peps.python.org/pep-0644/#libressl-support\n        # 6. https://github.com/yt-dlp/yt-dlp/commit/5b9f253fa0aee996cf1ed30185d4b502e00609c4#commitcomment-89054368\n        context.set_ciphers('@SECLEVEL=2:ECDH+AESGCM:ECDH+CHACHA20:ECDH+AES:DHE+AES:!aNULL:!eNULL:!aDSS:!SHA1:!AESCCM')\n        context.minimum_version = ssl.TLSVersion.TLSv1_2\n\n    context.verify_mode = ssl.CERT_REQUIRED if opts_check_certificate else ssl.CERT_NONE\n    if opts_check_certificate:\n        if certifi and 'no-certifi' not in params.get('compat_opts', []):\n            context.load_verify_locations(cafile=certifi.where())\n        else:\n            try:\n                context.load_default_certs()\n                # Work around the issue in load_default_certs when there are bad certificates. See:\n                # https://github.com/yt-dlp/yt-dlp/issues/1060,\n                # https://bugs.python.org/issue35665, https://bugs.python.org/issue45312\n            except ssl.SSLError:\n                # enum_certificates is not present in mingw python. See https://github.com/yt-dlp/yt-dlp/issues/1151\n                if sys.platform == 'win32' and hasattr(ssl, 'enum_certificates'):\n                    for storename in ('CA', 'ROOT'):\n                        _ssl_load_windows_store_certs(context, storename)\n                context.set_default_verify_paths()\n\n    client_certfile = params.get('client_certificate')\n    if client_certfile:\n        try:\n            context.load_cert_chain(\n                client_certfile, keyfile=params.get('client_certificate_key'),\n                password=params.get('client_certificate_password'))\n        except ssl.SSLError:\n            raise YoutubeDLError('Unable to load client certificate')\n\n    # Some servers may reject requests if ALPN extension is not sent. See:\n    # https://github.com/python/cpython/issues/85140\n    # https://github.com/yt-dlp/yt-dlp/issues/3878\n    with contextlib.suppress(NotImplementedError):\n        context.set_alpn_protocols(['http/1.1'])\n\n    return YoutubeDLHTTPSHandler(params, context=context, **kwargs)\n\n\ndef bug_reports_message(before=';'):\n    from ..update import REPOSITORY\n\n    msg = (f'please report this issue on  https://github.com/{REPOSITORY}/issues?q= , '\n           'filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U')\n\n    before = before.rstrip()\n    if not before or before.endswith(('.', '!', '?')):\n        msg = msg[0].title() + msg[1:]\n\n    return (before + ' ' if before else '') + msg\n\n\nclass YoutubeDLError(Exception):\n    \"\"\"Base exception for YoutubeDL errors.\"\"\"\n    msg = None\n\n    def __init__(self, msg=None):\n        if msg is not None:\n            self.msg = msg\n        elif self.msg is None:\n            self.msg = type(self).__name__\n        super().__init__(self.msg)\n\n\nnetwork_exceptions = [urllib.error.URLError, http.client.HTTPException, socket.error]\nif hasattr(ssl, 'CertificateError'):\n    network_exceptions.append(ssl.CertificateError)\nnetwork_exceptions = tuple(network_exceptions)\n\n\nclass ExtractorError(YoutubeDLError):\n    \"\"\"Error during info extraction.\"\"\"\n\n    def __init__(self, msg, tb=None, expected=False, cause=None, video_id=None, ie=None):\n        \"\"\" tb, if given, is the original traceback (so that it can be printed out).\n        If expected is set, this is a normal error message and most likely not a bug in yt-dlp.\n        \"\"\"\n        if sys.exc_info()[0] in network_exceptions:\n            expected = True\n\n        self.orig_msg = str(msg)\n        self.traceback = tb\n        self.expected = expected\n        self.cause = cause\n        self.video_id = video_id\n        self.ie = ie\n        self.exc_info = sys.exc_info()  # preserve original exception\n        if isinstance(self.exc_info[1], ExtractorError):\n            self.exc_info = self.exc_info[1].exc_info\n        super().__init__(self.__msg)\n\n    @property\n    def __msg(self):\n        return ''.join((\n            format_field(self.ie, None, '[%s] '),\n            format_field(self.video_id, None, '%s: '),\n            self.orig_msg,\n            format_field(self.cause, None, ' (caused by %r)'),\n            '' if self.expected else bug_reports_message()))\n\n    def format_traceback(self):\n        return join_nonempty(\n            self.traceback and ''.join(traceback.format_tb(self.traceback)),\n            self.cause and ''.join(traceback.format_exception(None, self.cause, self.cause.__traceback__)[1:]),\n            delim='\\n') or None\n\n    def __setattr__(self, name, value):\n        super().__setattr__(name, value)\n        if getattr(self, 'msg', None) and name not in ('msg', 'args'):\n            self.msg = self.__msg or type(self).__name__\n            self.args = (self.msg, )  # Cannot be property\n\n\nclass UnsupportedError(ExtractorError):\n    def __init__(self, url):\n        super().__init__(\n            'Unsupported URL: %s' % url, expected=True)\n        self.url = url\n\n\nclass RegexNotFoundError(ExtractorError):\n    \"\"\"Error when a regex didn't match\"\"\"\n    pass\n\n\nclass GeoRestrictedError(ExtractorError):\n    \"\"\"Geographic restriction Error exception.\n\n    This exception may be thrown when a video is not available from your\n    geographic location due to geographic restrictions imposed by a website.\n    \"\"\"\n\n    def __init__(self, msg, countries=None, **kwargs):\n        kwargs['expected'] = True\n        super().__init__(msg, **kwargs)\n        self.countries = countries\n\n\nclass UserNotLive(ExtractorError):\n    \"\"\"Error when a channel/user is not live\"\"\"\n\n    def __init__(self, msg=None, **kwargs):\n        kwargs['expected'] = True\n        super().__init__(msg or 'The channel is not currently live', **kwargs)\n\n\nclass DownloadError(YoutubeDLError):\n    \"\"\"Download Error exception.\n\n    This exception may be thrown by FileDownloader objects if they are not\n    configured to continue on errors. They will contain the appropriate\n    error message.\n    \"\"\"\n\n    def __init__(self, msg, exc_info=None):\n        \"\"\" exc_info, if given, is the original exception that caused the trouble (as returned by sys.exc_info()). \"\"\"\n        super().__init__(msg)\n        self.exc_info = exc_info\n\n\nclass EntryNotInPlaylist(YoutubeDLError):\n    \"\"\"Entry not in playlist exception.\n\n    This exception will be thrown by YoutubeDL when a requested entry\n    is not found in the playlist info_dict\n    \"\"\"\n    msg = 'Entry not found in info'\n\n\nclass SameFileError(YoutubeDLError):\n    \"\"\"Same File exception.\n\n    This exception will be thrown by FileDownloader objects if they detect\n    multiple files would have to be downloaded to the same file on disk.\n    \"\"\"\n    msg = 'Fixed output name but more than one file to download'\n\n    def __init__(self, filename=None):\n        if filename is not None:\n            self.msg += f': {filename}'\n        super().__init__(self.msg)\n\n\nclass PostProcessingError(YoutubeDLError):\n    \"\"\"Post Processing exception.\n\n    This exception may be raised by PostProcessor's .run() method to\n    indicate an error in the postprocessing task.\n    \"\"\"\n\n\nclass DownloadCancelled(YoutubeDLError):\n    \"\"\" Exception raised when the download queue should be interrupted \"\"\"\n    msg = 'The download was cancelled'\n\n\nclass ExistingVideoReached(DownloadCancelled):\n    \"\"\" --break-on-existing triggered \"\"\"\n    msg = 'Encountered a video that is already in the archive, stopping due to --break-on-existing'\n\n\nclass RejectedVideoReached(DownloadCancelled):\n    \"\"\" --break-match-filter triggered \"\"\"\n    msg = 'Encountered a video that did not match filter, stopping due to --break-match-filter'\n\n\nclass MaxDownloadsReached(DownloadCancelled):\n    \"\"\" --max-downloads limit has been reached. \"\"\"\n    msg = 'Maximum number of downloads reached, stopping due to --max-downloads'\n\n\nclass ReExtractInfo(YoutubeDLError):\n    \"\"\" Video info needs to be re-extracted. \"\"\"\n\n    def __init__(self, msg, expected=False):\n        super().__init__(msg)\n        self.expected = expected\n\n\nclass ThrottledDownload(ReExtractInfo):\n    \"\"\" Download speed below --throttled-rate. \"\"\"\n    msg = 'The download speed is below throttle limit'\n\n    def __init__(self):\n        super().__init__(self.msg, expected=False)\n\n\nclass UnavailableVideoError(YoutubeDLError):\n    \"\"\"Unavailable Format exception.\n\n    This exception will be thrown when a video is requested\n    in a format that is not available for that video.\n    \"\"\"\n    msg = 'Unable to download video'\n\n    def __init__(self, err=None):\n        if err is not None:\n            self.msg += f': {err}'\n        super().__init__(self.msg)\n\n\nclass ContentTooShortError(YoutubeDLError):\n    \"\"\"Content Too Short exception.\n\n    This exception may be raised by FileDownloader objects when a file they\n    download is too small for what the server announced first, indicating\n    the connection was probably interrupted.\n    \"\"\"\n\n    def __init__(self, downloaded, expected):\n        super().__init__(f'Downloaded {downloaded} bytes, expected {expected} bytes')\n        # Both in bytes\n        self.downloaded = downloaded\n        self.expected = expected\n\n\nclass XAttrMetadataError(YoutubeDLError):\n    def __init__(self, code=None, msg='Unknown error'):\n        super().__init__(msg)\n        self.code = code\n        self.msg = msg\n\n        # Parsing code and msg\n        if (self.code in (errno.ENOSPC, errno.EDQUOT)\n                or 'No space left' in self.msg or 'Disk quota exceeded' in self.msg):\n            self.reason = 'NO_SPACE'\n        elif self.code == errno.E2BIG or 'Argument list too long' in self.msg:\n            self.reason = 'VALUE_TOO_LONG'\n        else:\n            self.reason = 'NOT_SUPPORTED'\n\n\nclass XAttrUnavailableError(YoutubeDLError):\n    pass\n\n\ndef _create_http_connection(ydl_handler, http_class, is_https, *args, **kwargs):\n    hc = http_class(*args, **kwargs)\n    source_address = ydl_handler._params.get('source_address')\n\n    if source_address is not None:\n        # This is to workaround _create_connection() from socket where it will try all\n        # address data from getaddrinfo() including IPv6. This filters the result from\n        # getaddrinfo() based on the source_address value.\n        # This is based on the cpython socket.create_connection() function.\n        # https://github.com/python/cpython/blob/master/Lib/socket.py#L691\n        def _create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None):\n            host, port = address\n            err = None\n            addrs = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\n            af = socket.AF_INET if '.' in source_address[0] else socket.AF_INET6\n            ip_addrs = [addr for addr in addrs if addr[0] == af]\n            if addrs and not ip_addrs:\n                ip_version = 'v4' if af == socket.AF_INET else 'v6'\n                raise OSError(\n                    \"No remote IP%s addresses available for connect, can't use '%s' as source address\"\n                    % (ip_version, source_address[0]))\n            for res in ip_addrs:\n                af, socktype, proto, canonname, sa = res\n                sock = None\n                try:\n                    sock = socket.socket(af, socktype, proto)\n                    if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:\n                        sock.settimeout(timeout)\n                    sock.bind(source_address)\n                    sock.connect(sa)\n                    err = None  # Explicitly break reference cycle\n                    return sock\n                except OSError as _:\n                    err = _\n                    if sock is not None:\n                        sock.close()\n            if err is not None:\n                raise err\n            else:\n                raise OSError('getaddrinfo returns an empty list')\n        if hasattr(hc, '_create_connection'):\n            hc._create_connection = _create_connection\n        hc.source_address = (source_address, 0)\n\n    return hc\n\n\nclass YoutubeDLHandler(urllib.request.HTTPHandler):\n    \"\"\"Handler for HTTP requests and responses.\n\n    This class, when installed with an OpenerDirector, automatically adds\n    the standard headers to every HTTP request and handles gzipped, deflated and\n    brotli responses from web servers.\n\n    Part of this code was copied from:\n\n    http://techknack.net/python-urllib2-handlers/\n\n    Andrew Rowls, the author of that code, agreed to release it to the\n    public domain.\n    \"\"\"\n\n    def __init__(self, params, *args, **kwargs):\n        urllib.request.HTTPHandler.__init__(self, *args, **kwargs)\n        self._params = params\n\n    def http_open(self, req):\n        conn_class = http.client.HTTPConnection\n\n        socks_proxy = req.headers.get('Ytdl-socks-proxy')\n        if socks_proxy:\n            conn_class = make_socks_conn_class(conn_class, socks_proxy)\n            del req.headers['Ytdl-socks-proxy']\n\n        return self.do_open(functools.partial(\n            _create_http_connection, self, conn_class, False),\n            req)\n\n    @staticmethod\n    def deflate(data):\n        if not data:\n            return data\n        try:\n            return zlib.decompress(data, -zlib.MAX_WBITS)\n        except zlib.error:\n            return zlib.decompress(data)\n\n    @staticmethod\n    def brotli(data):\n        if not data:\n            return data\n        return brotli.decompress(data)\n\n    @staticmethod\n    def gz(data):\n        gz = gzip.GzipFile(fileobj=io.BytesIO(data), mode='rb')\n        try:\n            return gz.read()\n        except OSError as original_oserror:\n            # There may be junk add the end of the file\n            # See http://stackoverflow.com/q/4928560/35070 for details\n            for i in range(1, 1024):\n                try:\n                    gz = gzip.GzipFile(fileobj=io.BytesIO(data[:-i]), mode='rb')\n                    return gz.read()\n                except OSError:\n                    continue\n            else:\n                raise original_oserror\n\n    def http_request(self, req):\n        # According to RFC 3986, URLs can not contain non-ASCII characters, however this is not\n        # always respected by websites, some tend to give out URLs with non percent-encoded\n        # non-ASCII characters (see telemb.py, ard.py [#3412])\n        # urllib chokes on URLs with non-ASCII characters (see http://bugs.python.org/issue3991)\n        # To work around aforementioned issue we will replace request's original URL with\n        # percent-encoded one\n        # Since redirects are also affected (e.g. http://www.southpark.de/alle-episoden/s18e09)\n        # the code of this workaround has been moved here from YoutubeDL.urlopen()\n        url = req.get_full_url()\n        url_escaped = escape_url(url)\n\n        # Substitute URL if any change after escaping\n        if url != url_escaped:\n            req = update_Request(req, url=url_escaped)\n\n        for h, v in self._params.get('http_headers', std_headers).items():\n            # Capitalize is needed because of Python bug 2275: http://bugs.python.org/issue2275\n            # The dict keys are capitalized because of this bug by urllib\n            if h.capitalize() not in req.headers:\n                req.add_header(h, v)\n\n        if 'Youtubedl-no-compression' in req.headers:  # deprecated\n            req.headers.pop('Youtubedl-no-compression', None)\n            req.add_header('Accept-encoding', 'identity')\n\n        if 'Accept-encoding' not in req.headers:\n            req.add_header('Accept-encoding', ', '.join(SUPPORTED_ENCODINGS))\n\n        return super().do_request_(req)\n\n    def http_response(self, req, resp):\n        old_resp = resp\n\n        # Content-Encoding header lists the encodings in order that they were applied [1].\n        # To decompress, we simply do the reverse.\n        # [1]: https://datatracker.ietf.org/doc/html/rfc9110#name-content-encoding\n        decoded_response = None\n        for encoding in (e.strip() for e in reversed(resp.headers.get('Content-encoding', '').split(','))):\n            if encoding == 'gzip':\n                decoded_response = self.gz(decoded_response or resp.read())\n            elif encoding == 'deflate':\n                decoded_response = self.deflate(decoded_response or resp.read())\n            elif encoding == 'br' and brotli:\n                decoded_response = self.brotli(decoded_response or resp.read())\n\n        if decoded_response is not None:\n            resp = urllib.request.addinfourl(io.BytesIO(decoded_response), old_resp.headers, old_resp.url, old_resp.code)\n            resp.msg = old_resp.msg\n        # Percent-encode redirect URL of Location HTTP header to satisfy RFC 3986 (see\n        # https://github.com/ytdl-org/youtube-dl/issues/6457).\n        if 300 <= resp.code < 400:\n            location = resp.headers.get('Location')\n            if location:\n                # As of RFC 2616 default charset is iso-8859-1 that is respected by python 3\n                location = location.encode('iso-8859-1').decode()\n                location_escaped = escape_url(location)\n                if location != location_escaped:\n                    del resp.headers['Location']\n                    resp.headers['Location'] = location_escaped\n        return resp\n\n    https_request = http_request\n    https_response = http_response\n\n\ndef make_socks_conn_class(base_class, socks_proxy):\n    assert issubclass(base_class, (\n        http.client.HTTPConnection, http.client.HTTPSConnection))\n\n    url_components = urllib.parse.urlparse(socks_proxy)\n    if url_components.scheme.lower() == 'socks5':\n        socks_type = ProxyType.SOCKS5\n    elif url_components.scheme.lower() in ('socks', 'socks4'):\n        socks_type = ProxyType.SOCKS4\n    elif url_components.scheme.lower() == 'socks4a':\n        socks_type = ProxyType.SOCKS4A\n\n    def unquote_if_non_empty(s):\n        if not s:\n            return s\n        return urllib.parse.unquote_plus(s)\n\n    proxy_args = (\n        socks_type,\n        url_components.hostname, url_components.port or 1080,\n        True,  # Remote DNS\n        unquote_if_non_empty(url_components.username),\n        unquote_if_non_empty(url_components.password),\n    )\n\n    class SocksConnection(base_class):\n        def connect(self):\n            self.sock = sockssocket()\n            self.sock.setproxy(*proxy_args)\n            if isinstance(self.timeout, (int, float)):\n                self.sock.settimeout(self.timeout)\n            self.sock.connect((self.host, self.port))\n\n            if isinstance(self, http.client.HTTPSConnection):\n                if hasattr(self, '_context'):  # Python > 2.6\n                    self.sock = self._context.wrap_socket(\n                        self.sock, server_hostname=self.host)\n                else:\n                    self.sock = ssl.wrap_socket(self.sock)\n\n    return SocksConnection\n\n\nclass YoutubeDLHTTPSHandler(urllib.request.HTTPSHandler):\n    def __init__(self, params, https_conn_class=None, *args, **kwargs):\n        urllib.request.HTTPSHandler.__init__(self, *args, **kwargs)\n        self._https_conn_class = https_conn_class or http.client.HTTPSConnection\n        self._params = params\n\n    def https_open(self, req):\n        kwargs = {}\n        conn_class = self._https_conn_class\n\n        if hasattr(self, '_context'):  # python > 2.6\n            kwargs['context'] = self._context\n        if hasattr(self, '_check_hostname'):  # python 3.x\n            kwargs['check_hostname'] = self._check_hostname\n\n        socks_proxy = req.headers.get('Ytdl-socks-proxy')\n        if socks_proxy:\n            conn_class = make_socks_conn_class(conn_class, socks_proxy)\n            del req.headers['Ytdl-socks-proxy']\n\n        try:\n            return self.do_open(\n                functools.partial(_create_http_connection, self, conn_class, True), req, **kwargs)\n        except urllib.error.URLError as e:\n            if (isinstance(e.reason, ssl.SSLError)\n                    and getattr(e.reason, 'reason', None) == 'SSLV3_ALERT_HANDSHAKE_FAILURE'):\n                raise YoutubeDLError('SSLV3_ALERT_HANDSHAKE_FAILURE: Try using --legacy-server-connect')\n            raise\n\n\ndef is_path_like(f):\n    return isinstance(f, (str, bytes, os.PathLike))\n\n\nclass YoutubeDLCookieProcessor(urllib.request.HTTPCookieProcessor):\n    def __init__(self, cookiejar=None):\n        urllib.request.HTTPCookieProcessor.__init__(self, cookiejar)\n\n    def http_response(self, request, response):\n        return urllib.request.HTTPCookieProcessor.http_response(self, request, response)\n\n    https_request = urllib.request.HTTPCookieProcessor.http_request\n    https_response = http_response\n\n\nclass YoutubeDLRedirectHandler(urllib.request.HTTPRedirectHandler):\n    \"\"\"YoutubeDL redirect handler\n\n    The code is based on HTTPRedirectHandler implementation from CPython [1].\n\n    This redirect handler fixes and improves the logic to better align with RFC7261\n     and what browsers tend to do [2][3]\n\n    1. https://github.com/python/cpython/blob/master/Lib/urllib/request.py\n    2. https://datatracker.ietf.org/doc/html/rfc7231\n    3. https://github.com/python/cpython/issues/91306\n    \"\"\"\n\n    http_error_301 = http_error_303 = http_error_307 = http_error_308 = urllib.request.HTTPRedirectHandler.http_error_302\n\n    def redirect_request(self, req, fp, code, msg, headers, newurl):\n        if code not in (301, 302, 303, 307, 308):\n            raise urllib.error.HTTPError(req.full_url, code, msg, headers, fp)\n\n        new_method = req.get_method()\n        new_data = req.data\n        remove_headers = []\n        # A 303 must either use GET or HEAD for subsequent request\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.4\n        if code == 303 and req.get_method() != 'HEAD':\n            new_method = 'GET'\n        # 301 and 302 redirects are commonly turned into a GET from a POST\n        # for subsequent requests by browsers, so we'll do the same.\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.2\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.3\n        elif code in (301, 302) and req.get_method() == 'POST':\n            new_method = 'GET'\n\n        # only remove payload if method changed (e.g. POST to GET)\n        if new_method != req.get_method():\n            new_data = None\n            remove_headers.extend(['Content-Length', 'Content-Type'])\n\n        new_headers = {k: v for k, v in req.headers.items() if k.lower() not in remove_headers}\n\n        return urllib.request.Request(\n            newurl, headers=new_headers, origin_req_host=req.origin_req_host,\n            unverifiable=True, method=new_method, data=new_data)\n\n\ndef extract_timezone(date_str):\n    m = re.search(\n        r'''(?x)\n            ^.{8,}?                                              # >=8 char non-TZ prefix, if present\n            (?P<tz>Z|                                            # just the UTC Z, or\n                (?:(?<=.\\b\\d{4}|\\b\\d{2}:\\d\\d)|                   # preceded by 4 digits or hh:mm or\n                   (?<!.\\b[a-zA-Z]{3}|[a-zA-Z]{4}|..\\b\\d\\d))     # not preceded by 3 alpha word or >= 4 alpha or 2 digits\n                   [ ]?                                          # optional space\n                (?P<sign>\\+|-)                                   # +/-\n                (?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})       # hh[:]mm\n            $)\n        ''', date_str)\n    if not m:\n        m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n        timezone = TIMEZONE_NAMES.get(m and m.group('tz').strip())\n        if timezone is not None:\n            date_str = date_str[:-len(m.group('tz'))]\n        timezone = datetime.timedelta(hours=timezone or 0)\n    else:\n        date_str = date_str[:-len(m.group('tz'))]\n        if not m.group('sign'):\n            timezone = datetime.timedelta()\n        else:\n            sign = 1 if m.group('sign') == '+' else -1\n            timezone = datetime.timedelta(\n                hours=sign * int(m.group('hours')),\n                minutes=sign * int(m.group('minutes')))\n    return timezone, date_str\n\n\ndef parse_iso8601(date_str, delimiter='T', timezone=None):\n    \"\"\" Return a UNIX timestamp from the given date \"\"\"\n\n    if date_str is None:\n        return None\n\n    date_str = re.sub(r'\\.[0-9]+', '', date_str)\n\n    if timezone is None:\n        timezone, date_str = extract_timezone(date_str)\n\n    with contextlib.suppress(ValueError):\n        date_format = f'%Y-%m-%d{delimiter}%H:%M:%S'\n        dt = datetime.datetime.strptime(date_str, date_format) - timezone\n        return calendar.timegm(dt.timetuple())\n\n\ndef date_formats(day_first=True):\n    return DATE_FORMATS_DAY_FIRST if day_first else DATE_FORMATS_MONTH_FIRST\n\n\ndef unified_strdate(date_str, day_first=True):\n    \"\"\"Return a string with the date in the format YYYYMMDD\"\"\"\n\n    if date_str is None:\n        return None\n    upload_date = None\n    # Replace commas\n    date_str = date_str.replace(',', ' ')\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n    _, date_str = extract_timezone(date_str)\n\n    for expression in date_formats(day_first):\n        with contextlib.suppress(ValueError):\n            upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n    if upload_date is None:\n        timetuple = email.utils.parsedate_tz(date_str)\n        if timetuple:\n            with contextlib.suppress(ValueError):\n                upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    if upload_date is not None:\n        return str(upload_date)\n\n\ndef unified_timestamp(date_str, day_first=True):\n    if not isinstance(date_str, str):\n        return None\n\n    date_str = re.sub(r'\\s+', ' ', re.sub(\n        r'(?i)[,|]|(mon|tues?|wed(nes)?|thu(rs)?|fri|sat(ur)?)(day)?', '', date_str))\n\n    pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0\n    timezone, date_str = extract_timezone(date_str)\n\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n\n    # Remove unrecognized timezones from ISO 8601 alike timestamps\n    m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n    if m:\n        date_str = date_str[:-len(m.group('tz'))]\n\n    # Python only supports microseconds, so remove nanoseconds\n    m = re.search(r'^([0-9]{4,}-[0-9]{1,2}-[0-9]{1,2}T[0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\\.[0-9]{6})[0-9]+$', date_str)\n    if m:\n        date_str = m.group(1)\n\n    for expression in date_formats(day_first):\n        with contextlib.suppress(ValueError):\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n            return calendar.timegm(dt.timetuple())\n\n    timetuple = email.utils.parsedate_tz(date_str)\n    if timetuple:\n        return calendar.timegm(timetuple) + pm_delta * 3600 - timezone.total_seconds()\n\n\ndef determine_ext(url, default_ext='unknown_video'):\n    if url is None or '.' not in url:\n        return default_ext\n    guess = url.partition('?')[0].rpartition('.')[2]\n    if re.match(r'^[A-Za-z0-9]+$', guess):\n        return guess\n    # Try extract ext from URLs like http://example.com/foo/bar.mp4/?download\n    elif guess.rstrip('/') in KNOWN_EXTENSIONS:\n        return guess.rstrip('/')\n    else:\n        return default_ext\n\n\ndef subtitles_filename(filename, sub_lang, sub_format, expected_real_ext=None):\n    return replace_extension(filename, sub_lang + '.' + sub_format, expected_real_ext)\n\n\ndef datetime_from_str(date_str, precision='auto', format='%Y%m%d'):\n    R\"\"\"\n    Return a datetime object from a string.\n    Supported format:\n        (now|today|yesterday|DATE)([+-]\\d+(microsecond|second|minute|hour|day|week|month|year)s?)?\n\n    @param format       strftime format of DATE\n    @param precision    Round the datetime object: auto|microsecond|second|minute|hour|day\n                        auto: round to the unit provided in date_str (if applicable).\n    \"\"\"\n    auto_precision = False\n    if precision == 'auto':\n        auto_precision = True\n        precision = 'microsecond'\n    today = datetime_round(datetime.datetime.utcnow(), precision)\n    if date_str in ('now', 'today'):\n        return today\n    if date_str == 'yesterday':\n        return today - datetime.timedelta(days=1)\n    match = re.match(\n        r'(?P<start>.+)(?P<sign>[+-])(?P<time>\\d+)(?P<unit>microsecond|second|minute|hour|day|week|month|year)s?',\n        date_str)\n    if match is not None:\n        start_time = datetime_from_str(match.group('start'), precision, format)\n        time = int(match.group('time')) * (-1 if match.group('sign') == '-' else 1)\n        unit = match.group('unit')\n        if unit == 'month' or unit == 'year':\n            new_date = datetime_add_months(start_time, time * 12 if unit == 'year' else time)\n            unit = 'day'\n        else:\n            if unit == 'week':\n                unit = 'day'\n                time *= 7\n            delta = datetime.timedelta(**{unit + 's': time})\n            new_date = start_time + delta\n        if auto_precision:\n            return datetime_round(new_date, unit)\n        return new_date\n\n    return datetime_round(datetime.datetime.strptime(date_str, format), precision)\n\n\ndef date_from_str(date_str, format='%Y%m%d', strict=False):\n    R\"\"\"\n    Return a date object from a string using datetime_from_str\n\n    @param strict  Restrict allowed patterns to \"YYYYMMDD\" and\n                   (now|today|yesterday)(-\\d+(day|week|month|year)s?)?\n    \"\"\"\n    if strict and not re.fullmatch(r'\\d{8}|(now|today|yesterday)(-\\d+(day|week|month|year)s?)?', date_str):\n        raise ValueError(f'Invalid date format \"{date_str}\"')\n    return datetime_from_str(date_str, precision='microsecond', format=format).date()\n\n\ndef datetime_add_months(dt, months):\n    \"\"\"Increment/Decrement a datetime object by months.\"\"\"\n    month = dt.month + months - 1\n    year = dt.year + month // 12\n    month = month % 12 + 1\n    day = min(dt.day, calendar.monthrange(year, month)[1])\n    return dt.replace(year, month, day)\n\n\ndef datetime_round(dt, precision='day'):\n    \"\"\"\n    Round a datetime object's time to a specific precision\n    \"\"\"\n    if precision == 'microsecond':\n        return dt\n\n    unit_seconds = {\n        'day': 86400,\n        'hour': 3600,\n        'minute': 60,\n        'second': 1,\n    }\n    roundto = lambda x, n: ((x + n / 2) // n) * n\n    timestamp = calendar.timegm(dt.timetuple())\n    return datetime.datetime.utcfromtimestamp(roundto(timestamp, unit_seconds[precision]))\n\n\ndef hyphenate_date(date_str):\n    \"\"\"\n    Convert a date in 'YYYYMMDD' format to 'YYYY-MM-DD' format\"\"\"\n    match = re.match(r'^(\\d\\d\\d\\d)(\\d\\d)(\\d\\d)$', date_str)\n    if match is not None:\n        return '-'.join(match.groups())\n    else:\n        return date_str\n\n\nclass DateRange:\n    \"\"\"Represents a time interval between two dates\"\"\"\n\n    def __init__(self, start=None, end=None):\n        \"\"\"start and end must be strings in the format accepted by date\"\"\"\n        if start is not None:\n            self.start = date_from_str(start, strict=True)\n        else:\n            self.start = datetime.datetime.min.date()\n        if end is not None:\n            self.end = date_from_str(end, strict=True)\n        else:\n            self.end = datetime.datetime.max.date()\n        if self.start > self.end:\n            raise ValueError('Date range: \"%s\" , the start date must be before the end date' % self)\n\n    @classmethod\n    def day(cls, day):\n        \"\"\"Returns a range that only contains the given day\"\"\"\n        return cls(day, day)\n\n    def __contains__(self, date):\n        \"\"\"Check if the date is in the range\"\"\"\n        if not isinstance(date, datetime.date):\n            date = date_from_str(date)\n        return self.start <= date <= self.end\n\n    def __repr__(self):\n        return f'{__name__}.{type(self).__name__}({self.start.isoformat()!r}, {self.end.isoformat()!r})'\n\n    def __eq__(self, other):\n        return (isinstance(other, DateRange)\n                and self.start == other.start and self.end == other.end)\n\n\n@functools.cache\ndef system_identifier():\n    python_implementation = platform.python_implementation()\n    if python_implementation == 'PyPy' and hasattr(sys, 'pypy_version_info'):\n        python_implementation += ' version %d.%d.%d' % sys.pypy_version_info[:3]\n    libc_ver = []\n    with contextlib.suppress(OSError):  # We may not have access to the executable\n        libc_ver = platform.libc_ver()\n\n    return 'Python %s (%s %s %s) - %s (%s%s)' % (\n        platform.python_version(),\n        python_implementation,\n        platform.machine(),\n        platform.architecture()[0],\n        platform.platform(),\n        ssl.OPENSSL_VERSION,\n        format_field(join_nonempty(*libc_ver, delim=' '), None, ', %s'),\n    )\n\n\n@functools.cache\ndef get_windows_version():\n    ''' Get Windows version. returns () if it's not running on Windows '''\n    if compat_os_name == 'nt':\n        return version_tuple(platform.win32_ver()[1])\n    else:\n        return ()\n\n\ndef write_string(s, out=None, encoding=None):\n    assert isinstance(s, str)\n    out = out or sys.stderr\n    # `sys.stderr` might be `None` (Ref: https://github.com/pyinstaller/pyinstaller/pull/7217)\n    if not out:\n        return\n\n    if compat_os_name == 'nt' and supports_terminal_sequences(out):\n        s = re.sub(r'([\\r\\n]+)', r' \\1', s)\n\n    enc, buffer = None, out\n    if 'b' in getattr(out, 'mode', ''):\n        enc = encoding or preferredencoding()\n    elif hasattr(out, 'buffer'):\n        buffer = out.buffer\n        enc = encoding or getattr(out, 'encoding', None) or preferredencoding()\n\n    buffer.write(s.encode(enc, 'ignore') if enc else s)\n    out.flush()\n\n\ndef deprecation_warning(msg, *, printer=None, stacklevel=0, **kwargs):\n    from .. import _IN_CLI\n    if _IN_CLI:\n        if msg in deprecation_warning._cache:\n            return\n        deprecation_warning._cache.add(msg)\n        if printer:\n            return printer(f'{msg}{bug_reports_message()}', **kwargs)\n        return write_string(f'ERROR: {msg}{bug_reports_message()}\\n', **kwargs)\n    else:\n        import warnings\n        warnings.warn(DeprecationWarning(msg), stacklevel=stacklevel + 3)\n\n\ndeprecation_warning._cache = set()\n\n\ndef bytes_to_intlist(bs):\n    if not bs:\n        return []\n    if isinstance(bs[0], int):  # Python 3\n        return list(bs)\n    else:\n        return [ord(c) for c in bs]\n\n\ndef intlist_to_bytes(xs):\n    if not xs:\n        return b''\n    return struct.pack('%dB' % len(xs), *xs)\n\n\nclass LockingUnsupportedError(OSError):\n    msg = 'File locking is not supported'\n\n    def __init__(self):\n        super().__init__(self.msg)\n\n\n# Cross-platform file locking\nif sys.platform == 'win32':\n    import ctypes\n    import ctypes.wintypes\n    import msvcrt\n\n    class OVERLAPPED(ctypes.Structure):\n        _fields_ = [\n            ('Internal', ctypes.wintypes.LPVOID),\n            ('InternalHigh', ctypes.wintypes.LPVOID),\n            ('Offset', ctypes.wintypes.DWORD),\n            ('OffsetHigh', ctypes.wintypes.DWORD),\n            ('hEvent', ctypes.wintypes.HANDLE),\n        ]\n\n    kernel32 = ctypes.WinDLL('kernel32')\n    LockFileEx = kernel32.LockFileEx\n    LockFileEx.argtypes = [\n        ctypes.wintypes.HANDLE,     # hFile\n        ctypes.wintypes.DWORD,      # dwFlags\n        ctypes.wintypes.DWORD,      # dwReserved\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh\n        ctypes.POINTER(OVERLAPPED)  # Overlapped\n    ]\n    LockFileEx.restype = ctypes.wintypes.BOOL\n    UnlockFileEx = kernel32.UnlockFileEx\n    UnlockFileEx.argtypes = [\n        ctypes.wintypes.HANDLE,     # hFile\n        ctypes.wintypes.DWORD,      # dwReserved\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh\n        ctypes.POINTER(OVERLAPPED)  # Overlapped\n    ]\n    UnlockFileEx.restype = ctypes.wintypes.BOOL\n    whole_low = 0xffffffff\n    whole_high = 0x7fffffff\n\n    def _lock_file(f, exclusive, block):\n        overlapped = OVERLAPPED()\n        overlapped.Offset = 0\n        overlapped.OffsetHigh = 0\n        overlapped.hEvent = 0\n        f._lock_file_overlapped_p = ctypes.pointer(overlapped)\n\n        if not LockFileEx(msvcrt.get_osfhandle(f.fileno()),\n                          (0x2 if exclusive else 0x0) | (0x0 if block else 0x1),\n                          0, whole_low, whole_high, f._lock_file_overlapped_p):\n            # NB: No argument form of \"ctypes.FormatError\" does not work on PyPy\n            raise BlockingIOError(f'Locking file failed: {ctypes.FormatError(ctypes.GetLastError())!r}')\n\n    def _unlock_file(f):\n        assert f._lock_file_overlapped_p\n        handle = msvcrt.get_osfhandle(f.fileno())\n        if not UnlockFileEx(handle, 0, whole_low, whole_high, f._lock_file_overlapped_p):\n            raise OSError('Unlocking file failed: %r' % ctypes.FormatError())\n\nelse:\n    try:\n        import fcntl\n\n        def _lock_file(f, exclusive, block):\n            flags = fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH\n            if not block:\n                flags |= fcntl.LOCK_NB\n            try:\n                fcntl.flock(f, flags)\n            except BlockingIOError:\n                raise\n            except OSError:  # AOSP does not have flock()\n                fcntl.lockf(f, flags)\n\n        def _unlock_file(f):\n            with contextlib.suppress(OSError):\n                return fcntl.flock(f, fcntl.LOCK_UN)\n            with contextlib.suppress(OSError):\n                return fcntl.lockf(f, fcntl.LOCK_UN)  # AOSP does not have flock()\n            return fcntl.flock(f, fcntl.LOCK_UN | fcntl.LOCK_NB)  # virtiofs needs LOCK_NB on unlocking\n\n    except ImportError:\n\n        def _lock_file(f, exclusive, block):\n            raise LockingUnsupportedError()\n\n        def _unlock_file(f):\n            raise LockingUnsupportedError()\n\n\nclass locked_file:\n    locked = False\n\n    def __init__(self, filename, mode, block=True, encoding=None):\n        if mode not in {'r', 'rb', 'a', 'ab', 'w', 'wb'}:\n            raise NotImplementedError(mode)\n        self.mode, self.block = mode, block\n\n        writable = any(f in mode for f in 'wax+')\n        readable = any(f in mode for f in 'r+')\n        flags = functools.reduce(operator.ior, (\n            getattr(os, 'O_CLOEXEC', 0),  # UNIX only\n            getattr(os, 'O_BINARY', 0),  # Windows only\n            getattr(os, 'O_NOINHERIT', 0),  # Windows only\n            os.O_CREAT if writable else 0,  # O_TRUNC only after locking\n            os.O_APPEND if 'a' in mode else 0,\n            os.O_EXCL if 'x' in mode else 0,\n            os.O_RDONLY if not writable else os.O_RDWR if readable else os.O_WRONLY,\n        ))\n\n        self.f = os.fdopen(os.open(filename, flags, 0o666), mode, encoding=encoding)\n\n    def __enter__(self):\n        exclusive = 'r' not in self.mode\n        try:\n            _lock_file(self.f, exclusive, self.block)\n            self.locked = True\n        except OSError:\n            self.f.close()\n            raise\n        if 'w' in self.mode:\n            try:\n                self.f.truncate()\n            except OSError as e:\n                if e.errno not in (\n                    errno.ESPIPE,  # Illegal seek - expected for FIFO\n                    errno.EINVAL,  # Invalid argument - expected for /dev/null\n                ):\n                    raise\n        return self\n\n    def unlock(self):\n        if not self.locked:\n            return\n        try:\n            _unlock_file(self.f)\n        finally:\n            self.locked = False\n\n    def __exit__(self, *_):\n        try:\n            self.unlock()\n        finally:\n            self.f.close()\n\n    open = __enter__\n    close = __exit__\n\n    def __getattr__(self, attr):\n        return getattr(self.f, attr)\n\n    def __iter__(self):\n        return iter(self.f)\n\n\n@functools.cache\ndef get_filesystem_encoding():\n    encoding = sys.getfilesystemencoding()\n    return encoding if encoding is not None else 'utf-8'\n\n\ndef shell_quote(args):\n    quoted_args = []\n    encoding = get_filesystem_encoding()\n    for a in args:\n        if isinstance(a, bytes):\n            # We may get a filename encoded with 'encodeFilename'\n            a = a.decode(encoding)\n        quoted_args.append(compat_shlex_quote(a))\n    return ' '.join(quoted_args)\n\n\ndef smuggle_url(url, data):\n    \"\"\" Pass additional data in a URL for internal use. \"\"\"\n\n    url, idata = unsmuggle_url(url, {})\n    data.update(idata)\n    sdata = urllib.parse.urlencode(\n        {'__youtubedl_smuggle': json.dumps(data)})\n    return url + '#' + sdata\n\n\ndef unsmuggle_url(smug_url, default=None):\n    if '#__youtubedl_smuggle' not in smug_url:\n        return smug_url, default\n    url, _, sdata = smug_url.rpartition('#')\n    jsond = urllib.parse.parse_qs(sdata)['__youtubedl_smuggle'][0]\n    data = json.loads(jsond)\n    return url, data\n\n\ndef format_decimal_suffix(num, fmt='%d%s', *, factor=1000):\n    \"\"\" Formats numbers with decimal sufixes like K, M, etc \"\"\"\n    num, factor = float_or_none(num), float(factor)\n    if num is None or num < 0:\n        return None\n    POSSIBLE_SUFFIXES = 'kMGTPEZY'\n    exponent = 0 if num == 0 else min(int(math.log(num, factor)), len(POSSIBLE_SUFFIXES))\n    suffix = ['', *POSSIBLE_SUFFIXES][exponent]\n    if factor == 1024:\n        suffix = {'k': 'Ki', '': ''}.get(suffix, f'{suffix}i')\n    converted = num / (factor ** exponent)\n    return fmt % (converted, suffix)\n\n\ndef format_bytes(bytes):\n    return format_decimal_suffix(bytes, '%.2f%sB', factor=1024) or 'N/A'\n\n\ndef lookup_unit_table(unit_table, s, strict=False):\n    num_re = NUMBER_RE if strict else NUMBER_RE.replace(R'\\.', '[,.]')\n    units_re = '|'.join(re.escape(u) for u in unit_table)\n    m = (re.fullmatch if strict else re.match)(\n        rf'(?P<num>{num_re})\\s*(?P<unit>{units_re})\\b', s)\n    if not m:\n        return None\n\n    num = float(m.group('num').replace(',', '.'))\n    mult = unit_table[m.group('unit')]\n    return round(num * mult)\n\n\ndef parse_bytes(s):\n    \"\"\"Parse a string indicating a byte quantity into an integer\"\"\"\n    return lookup_unit_table(\n        {u: 1024**i for i, u in enumerate(['', *'KMGTPEZY'])},\n        s.upper(), strict=True)\n\n\ndef parse_filesize(s):\n    if s is None:\n        return None\n\n    # The lower-case forms are of course incorrect and unofficial,\n    # but we support those too\n    _UNIT_TABLE = {\n        'B': 1,\n        'b': 1,\n        'bytes': 1,\n        'KiB': 1024,\n        'KB': 1000,\n        'kB': 1024,\n        'Kb': 1000,\n        'kb': 1000,\n        'kilobytes': 1000,\n        'kibibytes': 1024,\n        'MiB': 1024 ** 2,\n        'MB': 1000 ** 2,\n        'mB': 1024 ** 2,\n        'Mb': 1000 ** 2,\n        'mb': 1000 ** 2,\n        'megabytes': 1000 ** 2,\n        'mebibytes': 1024 ** 2,\n        'GiB': 1024 ** 3,\n        'GB': 1000 ** 3,\n        'gB': 1024 ** 3,\n        'Gb': 1000 ** 3,\n        'gb': 1000 ** 3,\n        'gigabytes': 1000 ** 3,\n        'gibibytes': 1024 ** 3,\n        'TiB': 1024 ** 4,\n        'TB': 1000 ** 4,\n        'tB': 1024 ** 4,\n        'Tb': 1000 ** 4,\n        'tb': 1000 ** 4,\n        'terabytes': 1000 ** 4,\n        'tebibytes': 1024 ** 4,\n        'PiB': 1024 ** 5,\n        'PB': 1000 ** 5,\n        'pB': 1024 ** 5,\n        'Pb': 1000 ** 5,\n        'pb': 1000 ** 5,\n        'petabytes': 1000 ** 5,\n        'pebibytes': 1024 ** 5,\n        'EiB': 1024 ** 6,\n        'EB': 1000 ** 6,\n        'eB': 1024 ** 6,\n        'Eb': 1000 ** 6,\n        'eb': 1000 ** 6,\n        'exabytes': 1000 ** 6,\n        'exbibytes': 1024 ** 6,\n        'ZiB': 1024 ** 7,\n        'ZB': 1000 ** 7,\n        'zB': 1024 ** 7,\n        'Zb': 1000 ** 7,\n        'zb': 1000 ** 7,\n        'zettabytes': 1000 ** 7,\n        'zebibytes': 1024 ** 7,\n        'YiB': 1024 ** 8,\n        'YB': 1000 ** 8,\n        'yB': 1024 ** 8,\n        'Yb': 1000 ** 8,\n        'yb': 1000 ** 8,\n        'yottabytes': 1000 ** 8,\n        'yobibytes': 1024 ** 8,\n    }\n\n    return lookup_unit_table(_UNIT_TABLE, s)\n\n\ndef parse_count(s):\n    if s is None:\n        return None\n\n    s = re.sub(r'^[^\\d]+\\s', '', s).strip()\n\n    if re.match(r'^[\\d,.]+$', s):\n        return str_to_int(s)\n\n    _UNIT_TABLE = {\n        'k': 1000,\n        'K': 1000,\n        'm': 1000 ** 2,\n        'M': 1000 ** 2,\n        'kk': 1000 ** 2,\n        'KK': 1000 ** 2,\n        'b': 1000 ** 3,\n        'B': 1000 ** 3,\n    }\n\n    ret = lookup_unit_table(_UNIT_TABLE, s)\n    if ret is not None:\n        return ret\n\n    mobj = re.match(r'([\\d,.]+)(?:$|\\s)', s)\n    if mobj:\n        return str_to_int(mobj.group(1))\n\n\ndef parse_resolution(s, *, lenient=False):\n    if s is None:\n        return {}\n\n    if lenient:\n        mobj = re.search(r'(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)', s)\n    else:\n        mobj = re.search(r'(?<![a-zA-Z0-9])(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)(?![a-zA-Z0-9])', s)\n    if mobj:\n        return {\n            'width': int(mobj.group('w')),\n            'height': int(mobj.group('h')),\n        }\n\n    mobj = re.search(r'(?<![a-zA-Z0-9])(\\d+)[pPiI](?![a-zA-Z0-9])', s)\n    if mobj:\n        return {'height': int(mobj.group(1))}\n\n    mobj = re.search(r'\\b([48])[kK]\\b', s)\n    if mobj:\n        return {'height': int(mobj.group(1)) * 540}\n\n    return {}\n\n\ndef parse_bitrate(s):\n    if not isinstance(s, str):\n        return\n    mobj = re.search(r'\\b(\\d+)\\s*kbps', s)\n    if mobj:\n        return int(mobj.group(1))\n\n\ndef month_by_name(name, lang='en'):\n    \"\"\" Return the number of a month by (locale-independently) English name \"\"\"\n\n    month_names = MONTH_NAMES.get(lang, MONTH_NAMES['en'])\n\n    try:\n        return month_names.index(name) + 1\n    except ValueError:\n        return None\n\n\ndef month_by_abbreviation(abbrev):\n    \"\"\" Return the number of a month by (locale-independently) English\n        abbreviations \"\"\"\n\n    try:\n        return [s[:3] for s in ENGLISH_MONTH_NAMES].index(abbrev) + 1\n    except ValueError:\n        return None\n\n\ndef fix_xml_ampersands(xml_str):\n    \"\"\"Replace all the '&' by '&amp;' in XML\"\"\"\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',\n        '&amp;',\n        xml_str)\n\n\ndef setproctitle(title):\n    assert isinstance(title, str)\n\n    # Workaround for https://github.com/yt-dlp/yt-dlp/issues/4541\n    try:\n        import ctypes\n    except ImportError:\n        return\n\n    try:\n        libc = ctypes.cdll.LoadLibrary('libc.so.6')\n    except OSError:\n        return\n    except TypeError:\n        # LoadLibrary in Windows Python 2.7.13 only expects\n        # a bytestring, but since unicode_literals turns\n        # every string into a unicode string, it fails.\n        return\n    title_bytes = title.encode()\n    buf = ctypes.create_string_buffer(len(title_bytes))\n    buf.value = title_bytes\n    try:\n        libc.prctl(15, buf, 0, 0, 0)\n    except AttributeError:\n        return  # Strange libc, just skip this\n\n\ndef remove_start(s, start):\n    return s[len(start):] if s is not None and s.startswith(start) else s\n\n\ndef remove_end(s, end):\n    return s[:-len(end)] if s is not None and s.endswith(end) else s\n\n\ndef remove_quotes(s):\n    if s is None or len(s) < 2:\n        return s\n    for quote in ('\"', \"'\", ):\n        if s[0] == quote and s[-1] == quote:\n            return s[1:-1]\n    return s\n\n\ndef get_domain(url):\n    \"\"\"\n    This implementation is inconsistent, but is kept for compatibility.\n    Use this only for \"webpage_url_domain\"\n    \"\"\"\n    return remove_start(urllib.parse.urlparse(url).netloc, 'www.') or None\n\n\ndef url_basename(url):\n    path = urllib.parse.urlparse(url).path\n    return path.strip('/').split('/')[-1]\n\n\ndef base_url(url):\n    return re.match(r'https?://[^?#]+/', url).group()\n\n\ndef urljoin(base, path):\n    if isinstance(path, bytes):\n        path = path.decode()\n    if not isinstance(path, str) or not path:\n        return None\n    if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):\n        return path\n    if isinstance(base, bytes):\n        base = base.decode()\n    if not isinstance(base, str) or not re.match(\n            r'^(?:https?:)?//', base):\n        return None\n    return urllib.parse.urljoin(base, path)\n\n\nclass HEADRequest(urllib.request.Request):\n    def get_method(self):\n        return 'HEAD'\n\n\nclass PUTRequest(urllib.request.Request):\n    def get_method(self):\n        return 'PUT'\n\n\ndef int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):\n    if get_attr and v is not None:\n        v = getattr(v, get_attr, None)\n    try:\n        return int(v) * invscale // scale\n    except (ValueError, TypeError, OverflowError):\n        return default\n\n\ndef str_or_none(v, default=None):\n    return default if v is None else str(v)\n\n\ndef str_to_int(int_str):\n    \"\"\" A more relaxed version of int_or_none \"\"\"\n    if isinstance(int_str, int):\n        return int_str\n    elif isinstance(int_str, str):\n        int_str = re.sub(r'[,\\.\\+]', '', int_str)\n        return int_or_none(int_str)\n\n\ndef float_or_none(v, scale=1, invscale=1, default=None):\n    if v is None:\n        return default\n    try:\n        return float(v) * invscale / scale\n    except (ValueError, TypeError):\n        return default\n\n\ndef bool_or_none(v, default=None):\n    return v if isinstance(v, bool) else default\n\n\ndef strip_or_none(v, default=None):\n    return v.strip() if isinstance(v, str) else default\n\n\ndef url_or_none(url):\n    if not url or not isinstance(url, str):\n        return None\n    url = url.strip()\n    return url if re.match(r'^(?:(?:https?|rt(?:m(?:pt?[es]?|fp)|sp[su]?)|mms|ftps?):)?//', url) else None\n\n\ndef request_to_url(req):\n    if isinstance(req, urllib.request.Request):\n        return req.get_full_url()\n    else:\n        return req\n\n\ndef strftime_or_none(timestamp, date_format='%Y%m%d', default=None):\n    datetime_object = None\n    try:\n        if isinstance(timestamp, (int, float)):  # unix timestamp\n            # Using naive datetime here can break timestamp() in Windows\n            # Ref: https://github.com/yt-dlp/yt-dlp/issues/5185, https://github.com/python/cpython/issues/94414\n            # Also, datetime.datetime.fromtimestamp breaks for negative timestamps\n            # Ref: https://github.com/yt-dlp/yt-dlp/issues/6706#issuecomment-1496842642\n            datetime_object = (datetime.datetime.fromtimestamp(0, datetime.timezone.utc)\n                               + datetime.timedelta(seconds=timestamp))\n        elif isinstance(timestamp, str):  # assume YYYYMMDD\n            datetime_object = datetime.datetime.strptime(timestamp, '%Y%m%d')\n        date_format = re.sub(  # Support %s on windows\n            r'(?<!%)(%%)*%s', rf'\\g<1>{int(datetime_object.timestamp())}', date_format)\n        return datetime_object.strftime(date_format)\n    except (ValueError, TypeError, AttributeError):\n        return default\n\n\ndef parse_duration(s):\n    if not isinstance(s, str):\n        return None\n    s = s.strip()\n    if not s:\n        return None\n\n    days, hours, mins, secs, ms = [None] * 5\n    m = re.match(r'''(?x)\n            (?P<before_secs>\n                (?:(?:(?P<days>[0-9]+):)?(?P<hours>[0-9]+):)?(?P<mins>[0-9]+):)?\n            (?P<secs>(?(before_secs)[0-9]{1,2}|[0-9]+))\n            (?P<ms>[.:][0-9]+)?Z?$\n        ''', s)\n    if m:\n        days, hours, mins, secs, ms = m.group('days', 'hours', 'mins', 'secs', 'ms')\n    else:\n        m = re.match(\n            r'''(?ix)(?:P?\n                (?:\n                    [0-9]+\\s*y(?:ears?)?,?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*m(?:onths?)?,?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*w(?:eeks?)?,?\\s*\n                )?\n                (?:\n                    (?P<days>[0-9]+)\\s*d(?:ays?)?,?\\s*\n                )?\n                T)?\n                (?:\n                    (?P<hours>[0-9]+)\\s*h(?:ours?)?,?\\s*\n                )?\n                (?:\n                    (?P<mins>[0-9]+)\\s*m(?:in(?:ute)?s?)?,?\\s*\n                )?\n                (?:\n                    (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*s(?:ec(?:ond)?s?)?\\s*\n                )?Z?$''', s)\n        if m:\n            days, hours, mins, secs, ms = m.groups()\n        else:\n            m = re.match(r'(?i)(?:(?P<hours>[0-9.]+)\\s*(?:hours?)|(?P<mins>[0-9.]+)\\s*(?:mins?\\.?|minutes?)\\s*)Z?$', s)\n            if m:\n                hours, mins = m.groups()\n            else:\n                return None\n\n    if ms:\n        ms = ms.replace(':', '.')\n    return sum(float(part or 0) * mult for part, mult in (\n        (days, 86400), (hours, 3600), (mins, 60), (secs, 1), (ms, 1)))\n\n\ndef prepend_extension(filename, ext, expected_real_ext=None):\n    name, real_ext = os.path.splitext(filename)\n    return (\n        f'{name}.{ext}{real_ext}'\n        if not expected_real_ext or real_ext[1:] == expected_real_ext\n        else f'{filename}.{ext}')\n\n\ndef replace_extension(filename, ext, expected_real_ext=None):\n    name, real_ext = os.path.splitext(filename)\n    return '{}.{}'.format(\n        name if not expected_real_ext or real_ext[1:] == expected_real_ext else filename,\n        ext)\n\n\ndef check_executable(exe, args=[]):\n    \"\"\" Checks if the given binary is installed somewhere in PATH, and returns its name.\n    args can be a list of arguments for a short output (like -version) \"\"\"\n    try:\n        Popen.run([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    except OSError:\n        return False\n    return exe\n\n\ndef _get_exe_version_output(exe, args):\n    try:\n        # STDIN should be redirected too. On UNIX-like systems, ffmpeg triggers\n        # SIGTTOU if yt-dlp is run in the background.\n        # See https://github.com/ytdl-org/youtube-dl/issues/955#issuecomment-209789656\n        stdout, _, ret = Popen.run([encodeArgument(exe)] + args, text=True,\n                                   stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        if ret:\n            return None\n    except OSError:\n        return False\n    return stdout\n\n\ndef detect_exe_version(output, version_re=None, unrecognized='present'):\n    assert isinstance(output, str)\n    if version_re is None:\n        version_re = r'version\\s+([-0-9._a-zA-Z]+)'\n    m = re.search(version_re, output)\n    if m:\n        return m.group(1)\n    else:\n        return unrecognized\n\n\ndef get_exe_version(exe, args=['--version'],\n                    version_re=None, unrecognized=('present', 'broken')):\n    \"\"\" Returns the version of the specified executable,\n    or False if the executable is not present \"\"\"\n    unrecognized = variadic(unrecognized)\n    assert len(unrecognized) in (1, 2)\n    out = _get_exe_version_output(exe, args)\n    if out is None:\n        return unrecognized[-1]\n    return out and detect_exe_version(out, version_re, unrecognized[0])\n\n\ndef frange(start=0, stop=None, step=1):\n    \"\"\"Float range\"\"\"\n    if stop is None:\n        start, stop = 0, start\n    sign = [-1, 1][step > 0] if step else 0\n    while sign * start < sign * stop:\n        yield start\n        start += step\n\n\nclass LazyList(collections.abc.Sequence):\n    \"\"\"Lazy immutable list from an iterable\n    Note that slices of a LazyList are lists and not LazyList\"\"\"\n\n    class IndexError(IndexError):\n        pass\n\n    def __init__(self, iterable, *, reverse=False, _cache=None):\n        self._iterable = iter(iterable)\n        self._cache = [] if _cache is None else _cache\n        self._reversed = reverse\n\n    def __iter__(self):\n        if self._reversed:\n            # We need to consume the entire iterable to iterate in reverse\n            yield from self.exhaust()\n            return\n        yield from self._cache\n        for item in self._iterable:\n            self._cache.append(item)\n            yield item\n\n    def _exhaust(self):\n        self._cache.extend(self._iterable)\n        self._iterable = []  # Discard the emptied iterable to make it pickle-able\n        return self._cache\n\n    def exhaust(self):\n        \"\"\"Evaluate the entire iterable\"\"\"\n        return self._exhaust()[::-1 if self._reversed else 1]\n\n    @staticmethod\n    def _reverse_index(x):\n        return None if x is None else ~x\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            if self._reversed:\n                idx = slice(self._reverse_index(idx.start), self._reverse_index(idx.stop), -(idx.step or 1))\n            start, stop, step = idx.start, idx.stop, idx.step or 1\n        elif isinstance(idx, int):\n            if self._reversed:\n                idx = self._reverse_index(idx)\n            start, stop, step = idx, idx, 0\n        else:\n            raise TypeError('indices must be integers or slices')\n        if ((start or 0) < 0 or (stop or 0) < 0\n                or (start is None and step < 0)\n                or (stop is None and step > 0)):\n            # We need to consume the entire iterable to be able to slice from the end\n            # Obviously, never use this with infinite iterables\n            self._exhaust()\n            try:\n                return self._cache[idx]\n            except IndexError as e:\n                raise self.IndexError(e) from e\n        n = max(start or 0, stop or 0) - len(self._cache) + 1\n        if n > 0:\n            self._cache.extend(itertools.islice(self._iterable, n))\n        try:\n            return self._cache[idx]\n        except IndexError as e:\n            raise self.IndexError(e) from e\n\n    def __bool__(self):\n        try:\n            self[-1] if self._reversed else self[0]\n        except self.IndexError:\n            return False\n        return True\n\n    def __len__(self):\n        self._exhaust()\n        return len(self._cache)\n\n    def __reversed__(self):\n        return type(self)(self._iterable, reverse=not self._reversed, _cache=self._cache)\n\n    def __copy__(self):\n        return type(self)(self._iterable, reverse=self._reversed, _cache=self._cache)\n\n    def __repr__(self):\n        # repr and str should mimic a list. So we exhaust the iterable\n        return repr(self.exhaust())\n\n    def __str__(self):\n        return repr(self.exhaust())\n\n\nclass PagedList:\n\n    class IndexError(IndexError):\n        pass\n\n    def __len__(self):\n        # This is only useful for tests\n        return len(self.getslice())\n\n    def __init__(self, pagefunc, pagesize, use_cache=True):\n        self._pagefunc = pagefunc\n        self._pagesize = pagesize\n        self._pagecount = float('inf')\n        self._use_cache = use_cache\n        self._cache = {}\n\n    def getpage(self, pagenum):\n        page_results = self._cache.get(pagenum)\n        if page_results is None:\n            page_results = [] if pagenum > self._pagecount else list(self._pagefunc(pagenum))\n        if self._use_cache:\n            self._cache[pagenum] = page_results\n        return page_results\n\n    def getslice(self, start=0, end=None):\n        return list(self._getslice(start, end))\n\n    def _getslice(self, start, end):\n        raise NotImplementedError('This method must be implemented by subclasses')\n\n    def __getitem__(self, idx):\n        assert self._use_cache, 'Indexing PagedList requires cache'\n        if not isinstance(idx, int) or idx < 0:\n            raise TypeError('indices must be non-negative integers')\n        entries = self.getslice(idx, idx + 1)\n        if not entries:\n            raise self.IndexError()\n        return entries[0]\n\n\nclass OnDemandPagedList(PagedList):\n    \"\"\"Download pages until a page with less than maximum results\"\"\"\n\n    def _getslice(self, start, end):\n        for pagenum in itertools.count(start // self._pagesize):\n            firstid = pagenum * self._pagesize\n            nextfirstid = pagenum * self._pagesize + self._pagesize\n            if start >= nextfirstid:\n                continue\n\n            startv = (\n                start % self._pagesize\n                if firstid <= start < nextfirstid\n                else 0)\n            endv = (\n                ((end - 1) % self._pagesize) + 1\n                if (end is not None and firstid <= end <= nextfirstid)\n                else None)\n\n            try:\n                page_results = self.getpage(pagenum)\n            except Exception:\n                self._pagecount = pagenum - 1\n                raise\n            if startv != 0 or endv is not None:\n                page_results = page_results[startv:endv]\n            yield from page_results\n\n            # A little optimization - if current page is not \"full\", ie. does\n            # not contain page_size videos then we can assume that this page\n            # is the last one - there are no more ids on further pages -\n            # i.e. no need to query again.\n            if len(page_results) + startv < self._pagesize:\n                break\n\n            # If we got the whole page, but the next page is not interesting,\n            # break out early as well\n            if end == nextfirstid:\n                break\n\n\nclass InAdvancePagedList(PagedList):\n    \"\"\"PagedList with total number of pages known in advance\"\"\"\n\n    def __init__(self, pagefunc, pagecount, pagesize):\n        PagedList.__init__(self, pagefunc, pagesize, True)\n        self._pagecount = pagecount\n\n    def _getslice(self, start, end):\n        start_page = start // self._pagesize\n        end_page = self._pagecount if end is None else min(self._pagecount, end // self._pagesize + 1)\n        skip_elems = start - start_page * self._pagesize\n        only_more = None if end is None else end - start\n        for pagenum in range(start_page, end_page):\n            page_results = self.getpage(pagenum)\n            if skip_elems:\n                page_results = page_results[skip_elems:]\n                skip_elems = None\n            if only_more is not None:\n                if len(page_results) < only_more:\n                    only_more -= len(page_results)\n                else:\n                    yield from page_results[:only_more]\n                    break\n            yield from page_results\n\n\nclass PlaylistEntries:\n    MissingEntry = object()\n    is_exhausted = False\n\n    def __init__(self, ydl, info_dict):\n        self.ydl = ydl\n\n        # _entries must be assigned now since infodict can change during iteration\n        entries = info_dict.get('entries')\n        if entries is None:\n            raise EntryNotInPlaylist('There are no entries')\n        elif isinstance(entries, list):\n            self.is_exhausted = True\n\n        requested_entries = info_dict.get('requested_entries')\n        self.is_incomplete = requested_entries is not None\n        if self.is_incomplete:\n            assert self.is_exhausted\n            self._entries = [self.MissingEntry] * max(requested_entries or [0])\n            for i, entry in zip(requested_entries, entries):\n                self._entries[i - 1] = entry\n        elif isinstance(entries, (list, PagedList, LazyList)):\n            self._entries = entries\n        else:\n            self._entries = LazyList(entries)\n\n    PLAYLIST_ITEMS_RE = re.compile(r'''(?x)\n        (?P<start>[+-]?\\d+)?\n        (?P<range>[:-]\n            (?P<end>[+-]?\\d+|inf(?:inite)?)?\n            (?::(?P<step>[+-]?\\d+))?\n        )?''')\n\n    @classmethod\n    def parse_playlist_items(cls, string):\n        for segment in string.split(','):\n            if not segment:\n                raise ValueError('There is two or more consecutive commas')\n            mobj = cls.PLAYLIST_ITEMS_RE.fullmatch(segment)\n            if not mobj:\n                raise ValueError(f'{segment!r} is not a valid specification')\n            start, end, step, has_range = mobj.group('start', 'end', 'step', 'range')\n            if int_or_none(step) == 0:\n                raise ValueError(f'Step in {segment!r} cannot be zero')\n            yield slice(int_or_none(start), float_or_none(end), int_or_none(step)) if has_range else int(start)\n\n    def get_requested_items(self):\n        playlist_items = self.ydl.params.get('playlist_items')\n        playlist_start = self.ydl.params.get('playliststart', 1)\n        playlist_end = self.ydl.params.get('playlistend')\n        # For backwards compatibility, interpret -1 as whole list\n        if playlist_end in (-1, None):\n            playlist_end = ''\n        if not playlist_items:\n            playlist_items = f'{playlist_start}:{playlist_end}'\n        elif playlist_start != 1 or playlist_end:\n            self.ydl.report_warning('Ignoring playliststart and playlistend because playlistitems was given', only_once=True)\n\n        for index in self.parse_playlist_items(playlist_items):\n            for i, entry in self[index]:\n                yield i, entry\n                if not entry:\n                    continue\n                try:\n                    # The item may have just been added to archive. Don't break due to it\n                    if not self.ydl.params.get('lazy_playlist'):\n                        # TODO: Add auto-generated fields\n                        self.ydl._match_entry(entry, incomplete=True, silent=True)\n                except (ExistingVideoReached, RejectedVideoReached):\n                    return\n\n    def get_full_count(self):\n        if self.is_exhausted and not self.is_incomplete:\n            return len(self)\n        elif isinstance(self._entries, InAdvancePagedList):\n            if self._entries._pagesize == 1:\n                return self._entries._pagecount\n\n    @functools.cached_property\n    def _getter(self):\n        if isinstance(self._entries, list):\n            def get_entry(i):\n                try:\n                    entry = self._entries[i]\n                except IndexError:\n                    entry = self.MissingEntry\n                    if not self.is_incomplete:\n                        raise self.IndexError()\n                if entry is self.MissingEntry:\n                    raise EntryNotInPlaylist(f'Entry {i + 1} cannot be found')\n                return entry\n        else:\n            def get_entry(i):\n                try:\n                    return type(self.ydl)._handle_extraction_exceptions(lambda _, i: self._entries[i])(self.ydl, i)\n                except (LazyList.IndexError, PagedList.IndexError):\n                    raise self.IndexError()\n        return get_entry\n\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            idx = slice(idx, idx)\n\n        # NB: PlaylistEntries[1:10] => (0, 1, ... 9)\n        step = 1 if idx.step is None else idx.step\n        if idx.start is None:\n            start = 0 if step > 0 else len(self) - 1\n        else:\n            start = idx.start - 1 if idx.start >= 0 else len(self) + idx.start\n\n        # NB: Do not call len(self) when idx == [:]\n        if idx.stop is None:\n            stop = 0 if step < 0 else float('inf')\n        else:\n            stop = idx.stop - 1 if idx.stop >= 0 else len(self) + idx.stop\n        stop += [-1, 1][step > 0]\n\n        for i in frange(start, stop, step):\n            if i < 0:\n                continue\n            try:\n                entry = self._getter(i)\n            except self.IndexError:\n                self.is_exhausted = True\n                if step > 0:\n                    break\n                continue\n            yield i + 1, entry\n\n    def __len__(self):\n        return len(tuple(self[:]))\n\n    class IndexError(IndexError):\n        pass\n\n\ndef uppercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\U[0-9a-fA-F]{8}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)\n\n\ndef lowercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\u[0-9a-fA-F]{4}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)\n\n\ndef escape_rfc3986(s):\n    \"\"\"Escape non-ASCII characters as suggested by RFC 3986\"\"\"\n    return urllib.parse.quote(s, b\"%/;:@&=+$,!~*'()?#[]\")\n\n\ndef escape_url(url):\n    \"\"\"Escape URL as suggested by RFC 3986\"\"\"\n    url_parsed = urllib.parse.urlparse(url)\n    return url_parsed._replace(\n        netloc=url_parsed.netloc.encode('idna').decode('ascii'),\n        path=escape_rfc3986(url_parsed.path),\n        params=escape_rfc3986(url_parsed.params),\n        query=escape_rfc3986(url_parsed.query),\n        fragment=escape_rfc3986(url_parsed.fragment)\n    ).geturl()\n\n\ndef parse_qs(url, **kwargs):\n    return urllib.parse.parse_qs(urllib.parse.urlparse(url).query, **kwargs)\n\n\ndef read_batch_urls(batch_fd):\n    def fixup(url):\n        if not isinstance(url, str):\n            url = url.decode('utf-8', 'replace')\n        BOM_UTF8 = ('\\xef\\xbb\\xbf', '\\ufeff')\n        for bom in BOM_UTF8:\n            if url.startswith(bom):\n                url = url[len(bom):]\n        url = url.lstrip()\n        if not url or url.startswith(('#', ';', ']')):\n            return False\n        # \"#\" cannot be stripped out since it is part of the URI\n        # However, it can be safely stripped out if following a whitespace\n        return re.split(r'\\s#', url, 1)[0].rstrip()\n\n    with contextlib.closing(batch_fd) as fd:\n        return [url for url in map(fixup, fd) if url]\n\n\ndef urlencode_postdata(*args, **kargs):\n    return urllib.parse.urlencode(*args, **kargs).encode('ascii')\n\n\ndef update_url(url, *, query_update=None, **kwargs):\n    \"\"\"Replace URL components specified by kwargs\n       @param url           str or parse url tuple\n       @param query_update  update query\n       @returns             str\n    \"\"\"\n    if isinstance(url, str):\n        if not kwargs and not query_update:\n            return url\n        else:\n            url = urllib.parse.urlparse(url)\n    if query_update:\n        assert 'query' not in kwargs, 'query_update and query cannot be specified at the same time'\n        kwargs['query'] = urllib.parse.urlencode({\n            **urllib.parse.parse_qs(url.query),\n            **query_update\n        }, True)\n    return urllib.parse.urlunparse(url._replace(**kwargs))\n\n\ndef update_url_query(url, query):\n    return update_url(url, query_update=query)\n\n\ndef update_Request(req, url=None, data=None, headers=None, query=None):\n    req_headers = req.headers.copy()\n    req_headers.update(headers or {})\n    req_data = data or req.data\n    req_url = update_url_query(url or req.get_full_url(), query)\n    req_get_method = req.get_method()\n    if req_get_method == 'HEAD':\n        req_type = HEADRequest\n    elif req_get_method == 'PUT':\n        req_type = PUTRequest\n    else:\n        req_type = urllib.request.Request\n    new_req = req_type(\n        req_url, data=req_data, headers=req_headers,\n        origin_req_host=req.origin_req_host, unverifiable=req.unverifiable)\n    if hasattr(req, 'timeout'):\n        new_req.timeout = req.timeout\n    return new_req\n\n\ndef _multipart_encode_impl(data, boundary):\n    content_type = 'multipart/form-data; boundary=%s' % boundary\n\n    out = b''\n    for k, v in data.items():\n        out += b'--' + boundary.encode('ascii') + b'\\r\\n'\n        if isinstance(k, str):\n            k = k.encode()\n        if isinstance(v, str):\n            v = v.encode()\n        # RFC 2047 requires non-ASCII field names to be encoded, while RFC 7578\n        # suggests sending UTF-8 directly. Firefox sends UTF-8, too\n        content = b'Content-Disposition: form-data; name=\"' + k + b'\"\\r\\n\\r\\n' + v + b'\\r\\n'\n        if boundary.encode('ascii') in content:\n            raise ValueError('Boundary overlaps with data')\n        out += content\n\n    out += b'--' + boundary.encode('ascii') + b'--\\r\\n'\n\n    return out, content_type\n\n\ndef multipart_encode(data, boundary=None):\n    '''\n    Encode a dict to RFC 7578-compliant form-data\n\n    data:\n        A dict where keys and values can be either Unicode or bytes-like\n        objects.\n    boundary:\n        If specified a Unicode object, it's used as the boundary. Otherwise\n        a random boundary is generated.\n\n    Reference: https://tools.ietf.org/html/rfc7578\n    '''\n    has_specified_boundary = boundary is not None\n\n    while True:\n        if boundary is None:\n            boundary = '---------------' + str(random.randrange(0x0fffffff, 0xffffffff))\n\n        try:\n            out, content_type = _multipart_encode_impl(data, boundary)\n            break\n        except ValueError:\n            if has_specified_boundary:\n                raise\n            boundary = None\n\n    return out, content_type\n\n\ndef is_iterable_like(x, allowed_types=collections.abc.Iterable, blocked_types=NO_DEFAULT):\n    if blocked_types is NO_DEFAULT:\n        blocked_types = (str, bytes, collections.abc.Mapping)\n    return isinstance(x, allowed_types) and not isinstance(x, blocked_types)\n\n\ndef variadic(x, allowed_types=NO_DEFAULT):\n    if not isinstance(allowed_types, (tuple, type)):\n        deprecation_warning('allowed_types should be a tuple or a type')\n        allowed_types = tuple(allowed_types)\n    return x if is_iterable_like(x, blocked_types=allowed_types) else (x, )\n\n\ndef try_call(*funcs, expected_type=None, args=[], kwargs={}):\n    for f in funcs:\n        try:\n            val = f(*args, **kwargs)\n        except (AttributeError, KeyError, TypeError, IndexError, ValueError, ZeroDivisionError):\n            pass\n        else:\n            if expected_type is None or isinstance(val, expected_type):\n                return val\n\n\ndef try_get(src, getter, expected_type=None):\n    return try_call(*variadic(getter), args=(src,), expected_type=expected_type)\n\n\ndef filter_dict(dct, cndn=lambda _, v: v is not None):\n    return {k: v for k, v in dct.items() if cndn(k, v)}\n\n\ndef merge_dicts(*dicts):\n    merged = {}\n    for a_dict in dicts:\n        for k, v in a_dict.items():\n            if (v is not None and k not in merged\n                    or isinstance(v, str) and merged[k] == ''):\n                merged[k] = v\n    return merged\n\n\ndef encode_compat_str(string, encoding=preferredencoding(), errors='strict'):\n    return string if isinstance(string, str) else str(string, encoding, errors)\n\n\nUS_RATINGS = {\n    'G': 0,\n    'PG': 10,\n    'PG-13': 13,\n    'R': 16,\n    'NC': 18,\n}\n\n\nTV_PARENTAL_GUIDELINES = {\n    'TV-Y': 0,\n    'TV-Y7': 7,\n    'TV-G': 0,\n    'TV-PG': 0,\n    'TV-14': 14,\n    'TV-MA': 17,\n}\n\n\ndef parse_age_limit(s):\n    # isinstance(False, int) is True. So type() must be used instead\n    if type(s) is int:  # noqa: E721\n        return s if 0 <= s <= 21 else None\n    elif not isinstance(s, str):\n        return None\n    m = re.match(r'^(?P<age>\\d{1,2})\\+?$', s)\n    if m:\n        return int(m.group('age'))\n    s = s.upper()\n    if s in US_RATINGS:\n        return US_RATINGS[s]\n    m = re.match(r'^TV[_-]?(%s)$' % '|'.join(k[3:] for k in TV_PARENTAL_GUIDELINES), s)\n    if m:\n        return TV_PARENTAL_GUIDELINES['TV-' + m.group(1)]\n    return None\n\n\ndef strip_jsonp(code):\n    return re.sub(\n        r'''(?sx)^\n            (?:window\\.)?(?P<func_name>[a-zA-Z0-9_.$]*)\n            (?:\\s*&&\\s*(?P=func_name))?\n            \\s*\\(\\s*(?P<callback_data>.*)\\);?\n            \\s*?(?://[^\\n]*)*$''',\n        r'\\g<callback_data>', code)\n\n\ndef js_to_json(code, vars={}, *, strict=False):\n    # vars is a dict of var, val pairs to substitute\n    STRING_QUOTES = '\\'\"`'\n    STRING_RE = '|'.join(rf'{q}(?:\\\\.|[^\\\\{q}])*{q}' for q in STRING_QUOTES)\n    COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*\\n'\n    SKIP_RE = fr'\\s*(?:{COMMENT_RE})?\\s*'\n    INTEGER_TABLE = (\n        (fr'(?s)^(0[xX][0-9a-fA-F]+){SKIP_RE}:?$', 16),\n        (fr'(?s)^(0+[0-7]+){SKIP_RE}:?$', 8),\n    )\n\n    def process_escape(match):\n        JSON_PASSTHROUGH_ESCAPES = R'\"\\bfnrtu'\n        escape = match.group(1) or match.group(2)\n\n        return (Rf'\\{escape}' if escape in JSON_PASSTHROUGH_ESCAPES\n                else R'\\u00' if escape == 'x'\n                else '' if escape == '\\n'\n                else escape)\n\n    def template_substitute(match):\n        evaluated = js_to_json(match.group(1), vars, strict=strict)\n        if evaluated[0] == '\"':\n            return json.loads(evaluated)\n        return evaluated\n\n    def fix_kv(m):\n        v = m.group(0)\n        if v in ('true', 'false', 'null'):\n            return v\n        elif v in ('undefined', 'void 0'):\n            return 'null'\n        elif v.startswith('/*') or v.startswith('//') or v.startswith('!') or v == ',':\n            return ''\n\n        if v[0] in STRING_QUOTES:\n            v = re.sub(r'(?s)\\${([^}]+)}', template_substitute, v[1:-1]) if v[0] == '`' else v[1:-1]\n            escaped = re.sub(r'(?s)(\")|\\\\(.)', process_escape, v)\n            return f'\"{escaped}\"'\n\n        for regex, base in INTEGER_TABLE:\n            im = re.match(regex, v)\n            if im:\n                i = int(im.group(1), base)\n                return f'\"{i}\":' if v.endswith(':') else str(i)\n\n        if v in vars:\n            try:\n                if not strict:\n                    json.loads(vars[v])\n            except json.JSONDecodeError:\n                return json.dumps(vars[v])\n            else:\n                return vars[v]\n\n        if not strict:\n            return f'\"{v}\"'\n\n        raise ValueError(f'Unknown value: {v}')\n\n    def create_map(mobj):\n        return json.dumps(dict(json.loads(js_to_json(mobj.group(1) or '[]', vars=vars))))\n\n    code = re.sub(r'new Map\\((\\[.*?\\])?\\)', create_map, code)\n    if not strict:\n        code = re.sub(r'new Date\\((\".+\")\\)', r'\\g<1>', code)\n        code = re.sub(r'new \\w+\\((.*?)\\)', lambda m: json.dumps(m.group(0)), code)\n        code = re.sub(r'parseInt\\([^\\d]+(\\d+)[^\\d]+\\)', r'\\1', code)\n        code = re.sub(r'\\(function\\([^)]*\\)\\s*\\{[^}]*\\}\\s*\\)\\s*\\(\\s*([\"\\'][^)]*[\"\\'])\\s*\\)', r'\\1', code)\n\n    return re.sub(rf'''(?sx)\n        {STRING_RE}|\n        {COMMENT_RE}|,(?={SKIP_RE}[\\]}}])|\n        void\\s0|(?:(?<![0-9])[eE]|[a-df-zA-DF-Z_$])[.a-zA-Z_$0-9]*|\n        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{SKIP_RE}:)?|\n        [0-9]+(?={SKIP_RE}:)|\n        !+\n        ''', fix_kv, code)\n\n\ndef qualities(quality_ids):\n    \"\"\" Get a numeric quality value out of a list of possible values \"\"\"\n    def q(qid):\n        try:\n            return quality_ids.index(qid)\n        except ValueError:\n            return -1\n    return q\n\n\nPOSTPROCESS_WHEN = ('pre_process', 'after_filter', 'video', 'before_dl', 'post_process', 'after_move', 'after_video', 'playlist')\n\n\nDEFAULT_OUTTMPL = {\n    'default': '%(title)s [%(id)s].%(ext)s',\n    'chapter': '%(title)s - %(section_number)03d %(section_title)s [%(id)s].%(ext)s',\n}\nOUTTMPL_TYPES = {\n    'chapter': None,\n    'subtitle': None,\n    'thumbnail': None,\n    'description': 'description',\n    'annotation': 'annotations.xml',\n    'infojson': 'info.json',\n    'link': None,\n    'pl_video': None,\n    'pl_thumbnail': None,\n    'pl_description': 'description',\n    'pl_infojson': 'info.json',\n}\n\n# As of [1] format syntax is:\n#  %[mapping_key][conversion_flags][minimum_width][.precision][length_modifier]type\n# 1. https://docs.python.org/2/library/stdtypes.html#string-formatting\nSTR_FORMAT_RE_TMPL = r'''(?x)\n    (?<!%)(?P<prefix>(?:%%)*)\n    %\n    (?P<has_key>\\((?P<key>{0})\\))?\n    (?P<format>\n        (?P<conversion>[#0\\-+ ]+)?\n        (?P<min_width>\\d+)?\n        (?P<precision>\\.\\d+)?\n        (?P<len_mod>[hlL])?  # unused in python\n        {1}  # conversion type\n    )\n'''\n\n\nSTR_FORMAT_TYPES = 'diouxXeEfFgGcrsa'\n\n\ndef limit_length(s, length):\n    \"\"\" Add ellipses to overly long strings \"\"\"\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s\n\n\ndef version_tuple(v):\n    return tuple(int(e) for e in re.split(r'[-.]', v))\n\n\ndef is_outdated_version(version, limit, assume_new=True):\n    if not version:\n        return not assume_new\n    try:\n        return version_tuple(version) < version_tuple(limit)\n    except ValueError:\n        return not assume_new\n\n\ndef ytdl_is_updateable():\n    \"\"\" Returns if yt-dlp can be updated with -U \"\"\"\n\n    from ..update import is_non_updateable\n\n    return not is_non_updateable()\n\n\ndef args_to_str(args):\n    # Get a short string representation for a subprocess command\n    return ' '.join(compat_shlex_quote(a) for a in args)\n\n\ndef error_to_str(err):\n    return f'{type(err).__name__}: {err}'\n\n\ndef mimetype2ext(mt, default=NO_DEFAULT):\n    if not isinstance(mt, str):\n        if default is not NO_DEFAULT:\n            return default\n        return None\n\n    MAP = {\n        # video\n        '3gpp': '3gp',\n        'mp2t': 'ts',\n        'mp4': 'mp4',\n        'mpeg': 'mpeg',\n        'mpegurl': 'm3u8',\n        'quicktime': 'mov',\n        'webm': 'webm',\n        'vp9': 'vp9',\n        'x-flv': 'flv',\n        'x-m4v': 'm4v',\n        'x-matroska': 'mkv',\n        'x-mng': 'mng',\n        'x-mp4-fragmented': 'mp4',\n        'x-ms-asf': 'asf',\n        'x-ms-wmv': 'wmv',\n        'x-msvideo': 'avi',\n\n        # application (streaming playlists)\n        'dash+xml': 'mpd',\n        'f4m+xml': 'f4m',\n        'hds+xml': 'f4m',\n        'vnd.apple.mpegurl': 'm3u8',\n        'vnd.ms-sstr+xml': 'ism',\n        'x-mpegurl': 'm3u8',\n\n        # audio\n        'audio/mp4': 'm4a',\n        # Per RFC 3003, audio/mpeg can be .mp1, .mp2 or .mp3.\n        # Using .mp3 as it's the most popular one\n        'audio/mpeg': 'mp3',\n        'audio/webm': 'webm',\n        'audio/x-matroska': 'mka',\n        'audio/x-mpegurl': 'm3u',\n        'midi': 'mid',\n        'ogg': 'ogg',\n        'wav': 'wav',\n        'wave': 'wav',\n        'x-aac': 'aac',\n        'x-flac': 'flac',\n        'x-m4a': 'm4a',\n        'x-realaudio': 'ra',\n        'x-wav': 'wav',\n\n        # image\n        'avif': 'avif',\n        'bmp': 'bmp',\n        'gif': 'gif',\n        'jpeg': 'jpg',\n        'png': 'png',\n        'svg+xml': 'svg',\n        'tiff': 'tif',\n        'vnd.wap.wbmp': 'wbmp',\n        'webp': 'webp',\n        'x-icon': 'ico',\n        'x-jng': 'jng',\n        'x-ms-bmp': 'bmp',\n\n        # caption\n        'filmstrip+json': 'fs',\n        'smptett+xml': 'tt',\n        'ttaf+xml': 'dfxp',\n        'ttml+xml': 'ttml',\n        'x-ms-sami': 'sami',\n\n        # misc\n        'gzip': 'gz',\n        'json': 'json',\n        'xml': 'xml',\n        'zip': 'zip',\n    }\n\n    mimetype = mt.partition(';')[0].strip().lower()\n    _, _, subtype = mimetype.rpartition('/')\n\n    ext = traversal.traverse_obj(MAP, mimetype, subtype, subtype.rsplit('+')[-1])\n    if ext:\n        return ext\n    elif default is not NO_DEFAULT:\n        return default\n    return subtype.replace('+', '.')\n\n\ndef ext2mimetype(ext_or_url):\n    if not ext_or_url:\n        return None\n    if '.' not in ext_or_url:\n        ext_or_url = f'file.{ext_or_url}'\n    return mimetypes.guess_type(ext_or_url)[0]\n\n\ndef parse_codecs(codecs_str):\n    # http://tools.ietf.org/html/rfc6381\n    if not codecs_str:\n        return {}\n    split_codecs = list(filter(None, map(\n        str.strip, codecs_str.strip().strip(',').split(','))))\n    vcodec, acodec, scodec, hdr = None, None, None, None\n    for full_codec in split_codecs:\n        parts = re.sub(r'0+(?=\\d)', '', full_codec).split('.')\n        if parts[0] in ('avc1', 'avc2', 'avc3', 'avc4', 'vp9', 'vp8', 'hev1', 'hev2',\n                        'h263', 'h264', 'mp4v', 'hvc1', 'av1', 'theora', 'dvh1', 'dvhe'):\n            if vcodec:\n                continue\n            vcodec = full_codec\n            if parts[0] in ('dvh1', 'dvhe'):\n                hdr = 'DV'\n            elif parts[0] == 'av1' and traversal.traverse_obj(parts, 3) == '10':\n                hdr = 'HDR10'\n            elif parts[:2] == ['vp9', '2']:\n                hdr = 'HDR10'\n        elif parts[0] in ('flac', 'mp4a', 'opus', 'vorbis', 'mp3', 'aac', 'ac-4',\n                          'ac-3', 'ec-3', 'eac3', 'dtsc', 'dtse', 'dtsh', 'dtsl'):\n            acodec = acodec or full_codec\n        elif parts[0] in ('stpp', 'wvtt'):\n            scodec = scodec or full_codec\n        else:\n            write_string(f'WARNING: Unknown codec {full_codec}\\n')\n    if vcodec or acodec or scodec:\n        return {\n            'vcodec': vcodec or 'none',\n            'acodec': acodec or 'none',\n            'dynamic_range': hdr,\n            **({'scodec': scodec} if scodec is not None else {}),\n        }\n    elif len(split_codecs) == 2:\n        return {\n            'vcodec': split_codecs[0],\n            'acodec': split_codecs[1],\n        }\n    return {}\n\n\ndef get_compatible_ext(*, vcodecs, acodecs, vexts, aexts, preferences=None):\n    assert len(vcodecs) == len(vexts) and len(acodecs) == len(aexts)\n\n    allow_mkv = not preferences or 'mkv' in preferences\n\n    if allow_mkv and max(len(acodecs), len(vcodecs)) > 1:\n        return 'mkv'  # TODO: any other format allows this?\n\n    # TODO: All codecs supported by parse_codecs isn't handled here\n    COMPATIBLE_CODECS = {\n        'mp4': {\n            'av1', 'hevc', 'avc1', 'mp4a', 'ac-4',  # fourcc (m3u8, mpd)\n            'h264', 'aacl', 'ec-3',  # Set in ISM\n        },\n        'webm': {\n            'av1', 'vp9', 'vp8', 'opus', 'vrbs',\n            'vp9x', 'vp8x',  # in the webm spec\n        },\n    }\n\n    sanitize_codec = functools.partial(\n        try_get, getter=lambda x: x[0].split('.')[0].replace('0', '').lower())\n    vcodec, acodec = sanitize_codec(vcodecs), sanitize_codec(acodecs)\n\n    for ext in preferences or COMPATIBLE_CODECS.keys():\n        codec_set = COMPATIBLE_CODECS.get(ext, set())\n        if ext == 'mkv' or codec_set.issuperset((vcodec, acodec)):\n            return ext\n\n    COMPATIBLE_EXTS = (\n        {'mp3', 'mp4', 'm4a', 'm4p', 'm4b', 'm4r', 'm4v', 'ismv', 'isma', 'mov'},\n        {'webm', 'weba'},\n    )\n    for ext in preferences or vexts:\n        current_exts = {ext, *vexts, *aexts}\n        if ext == 'mkv' or current_exts == {ext} or any(\n                ext_sets.issuperset(current_exts) for ext_sets in COMPATIBLE_EXTS):\n            return ext\n    return 'mkv' if allow_mkv else preferences[-1]\n\n\ndef urlhandle_detect_ext(url_handle, default=NO_DEFAULT):\n    getheader = url_handle.headers.get\n\n    cd = getheader('Content-Disposition')\n    if cd:\n        m = re.match(r'attachment;\\s*filename=\"(?P<filename>[^\"]+)\"', cd)\n        if m:\n            e = determine_ext(m.group('filename'), default_ext=None)\n            if e:\n                return e\n\n    meta_ext = getheader('x-amz-meta-name')\n    if meta_ext:\n        e = meta_ext.rpartition('.')[2]\n        if e:\n            return e\n\n    return mimetype2ext(getheader('Content-Type'), default=default)\n\n\ndef encode_data_uri(data, mime_type):\n    return 'data:%s;base64,%s' % (mime_type, base64.b64encode(data).decode('ascii'))\n\n\ndef age_restricted(content_limit, age_limit):\n    \"\"\" Returns True iff the content should be blocked \"\"\"\n\n    if age_limit is None:  # No limit set\n        return False\n    if content_limit is None:\n        return False  # Content available for everyone\n    return age_limit < content_limit\n\n\n# List of known byte-order-marks (BOM)\nBOMS = [\n    (b'\\xef\\xbb\\xbf', 'utf-8'),\n    (b'\\x00\\x00\\xfe\\xff', 'utf-32-be'),\n    (b'\\xff\\xfe\\x00\\x00', 'utf-32-le'),\n    (b'\\xff\\xfe', 'utf-16-le'),\n    (b'\\xfe\\xff', 'utf-16-be'),\n]\n\n\ndef is_html(first_bytes):\n    \"\"\" Detect whether a file contains HTML by examining its first bytes. \"\"\"\n\n    encoding = 'utf-8'\n    for bom, enc in BOMS:\n        while first_bytes.startswith(bom):\n            encoding, first_bytes = enc, first_bytes[len(bom):]\n\n    return re.match(r'^\\s*<', first_bytes.decode(encoding, 'replace'))\n\n\ndef determine_protocol(info_dict):\n    protocol = info_dict.get('protocol')\n    if protocol is not None:\n        return protocol\n\n    url = sanitize_url(info_dict['url'])\n    if url.startswith('rtmp'):\n        return 'rtmp'\n    elif url.startswith('mms'):\n        return 'mms'\n    elif url.startswith('rtsp'):\n        return 'rtsp'\n\n    ext = determine_ext(url)\n    if ext == 'm3u8':\n        return 'm3u8' if info_dict.get('is_live') else 'm3u8_native'\n    elif ext == 'f4m':\n        return 'f4m'\n\n    return urllib.parse.urlparse(url).scheme\n\n\ndef render_table(header_row, data, delim=False, extra_gap=0, hide_empty=False):\n    \"\"\" Render a list of rows, each as a list of values.\n    Text after a \\t will be right aligned \"\"\"\n    def width(string):\n        return len(remove_terminal_sequences(string).replace('\\t', ''))\n\n    def get_max_lens(table):\n        return [max(width(str(v)) for v in col) for col in zip(*table)]\n\n    def filter_using_list(row, filterArray):\n        return [col for take, col in itertools.zip_longest(filterArray, row, fillvalue=True) if take]\n\n    max_lens = get_max_lens(data) if hide_empty else []\n    header_row = filter_using_list(header_row, max_lens)\n    data = [filter_using_list(row, max_lens) for row in data]\n\n    table = [header_row] + data\n    max_lens = get_max_lens(table)\n    extra_gap += 1\n    if delim:\n        table = [header_row, [delim * (ml + extra_gap) for ml in max_lens]] + data\n        table[1][-1] = table[1][-1][:-extra_gap * len(delim)]  # Remove extra_gap from end of delimiter\n    for row in table:\n        for pos, text in enumerate(map(str, row)):\n            if '\\t' in text:\n                row[pos] = text.replace('\\t', ' ' * (max_lens[pos] - width(text))) + ' ' * extra_gap\n            else:\n                row[pos] = text + ' ' * (max_lens[pos] - width(text) + extra_gap)\n    ret = '\\n'.join(''.join(row).rstrip() for row in table)\n    return ret\n\n\ndef _match_one(filter_part, dct, incomplete):\n    # TODO: Generalize code with YoutubeDL._build_format_filter\n    STRING_OPERATORS = {\n        '*=': operator.contains,\n        '^=': lambda attr, value: attr.startswith(value),\n        '$=': lambda attr, value: attr.endswith(value),\n        '~=': lambda attr, value: re.search(value, attr),\n    }\n    COMPARISON_OPERATORS = {\n        **STRING_OPERATORS,\n        '<=': operator.le,  # \"<=\" must be defined above \"<\"\n        '<': operator.lt,\n        '>=': operator.ge,\n        '>': operator.gt,\n        '=': operator.eq,\n    }\n\n    if isinstance(incomplete, bool):\n        is_incomplete = lambda _: incomplete\n    else:\n        is_incomplete = lambda k: k in incomplete\n\n    operator_rex = re.compile(r'''(?x)\n        (?P<key>[a-z_]+)\n        \\s*(?P<negation>!\\s*)?(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n        (?:\n            (?P<quote>[\"\\'])(?P<quotedstrval>.+?)(?P=quote)|\n            (?P<strval>.+?)\n        )\n        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))\n    m = operator_rex.fullmatch(filter_part.strip())\n    if m:\n        m = m.groupdict()\n        unnegated_op = COMPARISON_OPERATORS[m['op']]\n        if m['negation']:\n            op = lambda attr, value: not unnegated_op(attr, value)\n        else:\n            op = unnegated_op\n        comparison_value = m['quotedstrval'] or m['strval'] or m['intval']\n        if m['quote']:\n            comparison_value = comparison_value.replace(r'\\%s' % m['quote'], m['quote'])\n        actual_value = dct.get(m['key'])\n        numeric_comparison = None\n        if isinstance(actual_value, (int, float)):\n            # If the original field is a string and matching comparisonvalue is\n            # a number we should respect the origin of the original field\n            # and process comparison value as a string (see\n            # https://github.com/ytdl-org/youtube-dl/issues/11082)\n            try:\n                numeric_comparison = int(comparison_value)\n            except ValueError:\n                numeric_comparison = parse_filesize(comparison_value)\n                if numeric_comparison is None:\n                    numeric_comparison = parse_filesize(f'{comparison_value}B')\n                if numeric_comparison is None:\n                    numeric_comparison = parse_duration(comparison_value)\n        if numeric_comparison is not None and m['op'] in STRING_OPERATORS:\n            raise ValueError('Operator %s only supports string values!' % m['op'])\n        if actual_value is None:\n            return is_incomplete(m['key']) or m['none_inclusive']\n        return op(actual_value, comparison_value if numeric_comparison is None else numeric_comparison)\n\n    UNARY_OPERATORS = {\n        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),\n        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),\n    }\n    operator_rex = re.compile(r'''(?x)\n        (?P<op>%s)\\s*(?P<key>[a-z_]+)\n        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))\n    m = operator_rex.fullmatch(filter_part.strip())\n    if m:\n        op = UNARY_OPERATORS[m.group('op')]\n        actual_value = dct.get(m.group('key'))\n        if is_incomplete(m.group('key')) and actual_value is None:\n            return True\n        return op(actual_value)\n\n    raise ValueError('Invalid filter part %r' % filter_part)\n\n\ndef match_str(filter_str, dct, incomplete=False):\n    \"\"\" Filter a dictionary with a simple string syntax.\n    @returns           Whether the filter passes\n    @param incomplete  Set of keys that is expected to be missing from dct.\n                       Can be True/False to indicate all/none of the keys may be missing.\n                       All conditions on incomplete keys pass if the key is missing\n    \"\"\"\n    return all(\n        _match_one(filter_part.replace(r'\\&', '&'), dct, incomplete)\n        for filter_part in re.split(r'(?<!\\\\)&', filter_str))\n\n\ndef match_filter_func(filters, breaking_filters=None):\n    if not filters and not breaking_filters:\n        return None\n    breaking_filters = match_filter_func(breaking_filters) or (lambda _, __: None)\n    filters = set(variadic(filters or []))\n\n    interactive = '-' in filters\n    if interactive:\n        filters.remove('-')\n\n    def _match_func(info_dict, incomplete=False):\n        ret = breaking_filters(info_dict, incomplete)\n        if ret is not None:\n            raise RejectedVideoReached(ret)\n\n        if not filters or any(match_str(f, info_dict, incomplete) for f in filters):\n            return NO_DEFAULT if interactive and not incomplete else None\n        else:\n            video_title = info_dict.get('title') or info_dict.get('id') or 'entry'\n            filter_str = ') | ('.join(map(str.strip, filters))\n            return f'{video_title} does not pass filter ({filter_str}), skipping ..'\n    return _match_func\n\n\nclass download_range_func:\n    def __init__(self, chapters, ranges, from_info=False):\n        self.chapters, self.ranges, self.from_info = chapters, ranges, from_info\n\n    def __call__(self, info_dict, ydl):\n\n        warning = ('There are no chapters matching the regex' if info_dict.get('chapters')\n                   else 'Cannot match chapters since chapter information is unavailable')\n        for regex in self.chapters or []:\n            for i, chapter in enumerate(info_dict.get('chapters') or []):\n                if re.search(regex, chapter['title']):\n                    warning = None\n                    yield {**chapter, 'index': i}\n        if self.chapters and warning:\n            ydl.to_screen(f'[info] {info_dict[\"id\"]}: {warning}')\n\n        for start, end in self.ranges or []:\n            yield {\n                'start_time': self._handle_negative_timestamp(start, info_dict),\n                'end_time': self._handle_negative_timestamp(end, info_dict),\n            }\n\n        if self.from_info and (info_dict.get('start_time') or info_dict.get('end_time')):\n            yield {\n                'start_time': info_dict.get('start_time') or 0,\n                'end_time': info_dict.get('end_time') or float('inf'),\n            }\n        elif not self.ranges and not self.chapters:\n            yield {}\n\n    @staticmethod\n    def _handle_negative_timestamp(time, info):\n        return max(info['duration'] + time, 0) if info.get('duration') and time < 0 else time\n\n    def __eq__(self, other):\n        return (isinstance(other, download_range_func)\n                and self.chapters == other.chapters and self.ranges == other.ranges)\n\n    def __repr__(self):\n        return f'{__name__}.{type(self).__name__}({self.chapters}, {self.ranges})'\n\n\ndef parse_dfxp_time_expr(time_expr):\n    if not time_expr:\n        return\n\n    mobj = re.match(rf'^(?P<time_offset>{NUMBER_RE})s?$', time_expr)\n    if mobj:\n        return float(mobj.group('time_offset'))\n\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)\n    if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))\n\n\ndef srt_subtitles_timecode(seconds):\n    return '%02d:%02d:%02d,%03d' % timetuple_from_msec(seconds * 1000)\n\n\ndef ass_subtitles_timecode(seconds):\n    time = timetuple_from_msec(seconds * 1000)\n    return '%01d:%02d:%02d.%02d' % (*time[:-1], time.milliseconds / 10)\n\n\ndef dfxp2srt(dfxp_data):\n    '''\n    @param dfxp_data A bytes-like object containing DFXP data\n    @returns A unicode object containing converted SRT data\n    '''\n    LEGACY_NAMESPACES = (\n        (b'http://www.w3.org/ns/ttml', [\n            b'http://www.w3.org/2004/11/ttaf1',\n            b'http://www.w3.org/2006/04/ttaf1',\n            b'http://www.w3.org/2006/10/ttaf1',\n        ]),\n        (b'http://www.w3.org/ns/ttml#styling', [\n            b'http://www.w3.org/ns/ttml#style',\n        ]),\n    )\n\n    SUPPORTED_STYLING = [\n        'color',\n        'fontFamily',\n        'fontSize',\n        'fontStyle',\n        'fontWeight',\n        'textDecoration'\n    ]\n\n    _x = functools.partial(xpath_with_ns, ns_map={\n        'xml': 'http://www.w3.org/XML/1998/namespace',\n        'ttml': 'http://www.w3.org/ns/ttml',\n        'tts': 'http://www.w3.org/ns/ttml#styling',\n    })\n\n    styles = {}\n    default_style = {}\n\n    class TTMLPElementParser:\n        _out = ''\n        _unclosed_elements = []\n        _applied_styles = []\n\n        def start(self, tag, attrib):\n            if tag in (_x('ttml:br'), 'br'):\n                self._out += '\\n'\n            else:\n                unclosed_elements = []\n                style = {}\n                element_style_id = attrib.get('style')\n                if default_style:\n                    style.update(default_style)\n                if element_style_id:\n                    style.update(styles.get(element_style_id, {}))\n                for prop in SUPPORTED_STYLING:\n                    prop_val = attrib.get(_x('tts:' + prop))\n                    if prop_val:\n                        style[prop] = prop_val\n                if style:\n                    font = ''\n                    for k, v in sorted(style.items()):\n                        if self._applied_styles and self._applied_styles[-1].get(k) == v:\n                            continue\n                        if k == 'color':\n                            font += ' color=\"%s\"' % v\n                        elif k == 'fontSize':\n                            font += ' size=\"%s\"' % v\n                        elif k == 'fontFamily':\n                            font += ' face=\"%s\"' % v\n                        elif k == 'fontWeight' and v == 'bold':\n                            self._out += '<b>'\n                            unclosed_elements.append('b')\n                        elif k == 'fontStyle' and v == 'italic':\n                            self._out += '<i>'\n                            unclosed_elements.append('i')\n                        elif k == 'textDecoration' and v == 'underline':\n                            self._out += '<u>'\n                            unclosed_elements.append('u')\n                    if font:\n                        self._out += '<font' + font + '>'\n                        unclosed_elements.append('font')\n                    applied_style = {}\n                    if self._applied_styles:\n                        applied_style.update(self._applied_styles[-1])\n                    applied_style.update(style)\n                    self._applied_styles.append(applied_style)\n                self._unclosed_elements.append(unclosed_elements)\n\n        def end(self, tag):\n            if tag not in (_x('ttml:br'), 'br'):\n                unclosed_elements = self._unclosed_elements.pop()\n                for element in reversed(unclosed_elements):\n                    self._out += '</%s>' % element\n                if unclosed_elements and self._applied_styles:\n                    self._applied_styles.pop()\n\n        def data(self, data):\n            self._out += data\n\n        def close(self):\n            return self._out.strip()\n\n    # Fix UTF-8 encoded file wrongly marked as UTF-16. See https://github.com/yt-dlp/yt-dlp/issues/6543#issuecomment-1477169870\n    # This will not trigger false positives since only UTF-8 text is being replaced\n    dfxp_data = dfxp_data.replace(b'encoding=\\'UTF-16\\'', b'encoding=\\'UTF-8\\'')\n\n    def parse_node(node):\n        target = TTMLPElementParser()\n        parser = xml.etree.ElementTree.XMLParser(target=target)\n        parser.feed(xml.etree.ElementTree.tostring(node))\n        return parser.close()\n\n    for k, v in LEGACY_NAMESPACES:\n        for ns in v:\n            dfxp_data = dfxp_data.replace(ns, k)\n\n    dfxp = compat_etree_fromstring(dfxp_data)\n    out = []\n    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')\n\n    if not paras:\n        raise ValueError('Invalid dfxp/TTML subtitle')\n\n    repeat = False\n    while True:\n        for style in dfxp.findall(_x('.//ttml:style')):\n            style_id = style.get('id') or style.get(_x('xml:id'))\n            if not style_id:\n                continue\n            parent_style_id = style.get('style')\n            if parent_style_id:\n                if parent_style_id not in styles:\n                    repeat = True\n                    continue\n                styles[style_id] = styles[parent_style_id].copy()\n            for prop in SUPPORTED_STYLING:\n                prop_val = style.get(_x('tts:' + prop))\n                if prop_val:\n                    styles.setdefault(style_id, {})[prop] = prop_val\n        if repeat:\n            repeat = False\n        else:\n            break\n\n    for p in ('body', 'div'):\n        ele = xpath_element(dfxp, [_x('.//ttml:' + p), './/' + p])\n        if ele is None:\n            continue\n        style = styles.get(ele.get('style'))\n        if not style:\n            continue\n        default_style.update(style)\n\n    for para, index in zip(paras, itertools.count(1)):\n        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))\n        end_time = parse_dfxp_time_expr(para.attrib.get('end'))\n        dur = parse_dfxp_time_expr(para.attrib.get('dur'))\n        if begin_time is None:\n            continue\n        if not end_time:\n            if not dur:\n                continue\n            end_time = begin_time + dur\n        out.append('%d\\n%s --> %s\\n%s\\n\\n' % (\n            index,\n            srt_subtitles_timecode(begin_time),\n            srt_subtitles_timecode(end_time),\n            parse_node(para)))\n\n    return ''.join(out)\n\n\ndef cli_option(params, command_option, param, separator=None):\n    param = params.get(param)\n    return ([] if param is None\n            else [command_option, str(param)] if separator is None\n            else [f'{command_option}{separator}{param}'])\n\n\ndef cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n    param = params.get(param)\n    assert param in (True, False, None)\n    return cli_option({True: true_value, False: false_value}, command_option, param, separator)\n\n\ndef cli_valueless_option(params, command_option, param, expected_value=True):\n    return [command_option] if params.get(param) == expected_value else []\n\n\ndef cli_configuration_args(argdict, keys, default=[], use_compat=True):\n    if isinstance(argdict, (list, tuple)):  # for backward compatibility\n        if use_compat:\n            return argdict\n        else:\n            argdict = None\n    if argdict is None:\n        return default\n    assert isinstance(argdict, dict)\n\n    assert isinstance(keys, (list, tuple))\n    for key_list in keys:\n        arg_list = list(filter(\n            lambda x: x is not None,\n            [argdict.get(key.lower()) for key in variadic(key_list)]))\n        if arg_list:\n            return [arg for args in arg_list for arg in args]\n    return default\n\n\ndef _configuration_args(main_key, argdict, exe, keys=None, default=[], use_compat=True):\n    main_key, exe = main_key.lower(), exe.lower()\n    root_key = exe if main_key == exe else f'{main_key}+{exe}'\n    keys = [f'{root_key}{k}' for k in (keys or [''])]\n    if root_key in keys:\n        if main_key != exe:\n            keys.append((main_key, exe))\n        keys.append('default')\n    else:\n        use_compat = False\n    return cli_configuration_args(argdict, keys, default, use_compat)\n\n\nclass ISO639Utils:\n    # See http://www.loc.gov/standards/iso639-2/ISO-639-2_utf-8.txt\n    _lang_map = {\n        'aa': 'aar',\n        'ab': 'abk',\n        'ae': 'ave',\n        'af': 'afr',\n        'ak': 'aka',\n        'am': 'amh',\n        'an': 'arg',\n        'ar': 'ara',\n        'as': 'asm',\n        'av': 'ava',\n        'ay': 'aym',\n        'az': 'aze',\n        'ba': 'bak',\n        'be': 'bel',\n        'bg': 'bul',\n        'bh': 'bih',\n        'bi': 'bis',\n        'bm': 'bam',\n        'bn': 'ben',\n        'bo': 'bod',\n        'br': 'bre',\n        'bs': 'bos',\n        'ca': 'cat',\n        'ce': 'che',\n        'ch': 'cha',\n        'co': 'cos',\n        'cr': 'cre',\n        'cs': 'ces',\n        'cu': 'chu',\n        'cv': 'chv',\n        'cy': 'cym',\n        'da': 'dan',\n        'de': 'deu',\n        'dv': 'div',\n        'dz': 'dzo',\n        'ee': 'ewe',\n        'el': 'ell',\n        'en': 'eng',\n        'eo': 'epo',\n        'es': 'spa',\n        'et': 'est',\n        'eu': 'eus',\n        'fa': 'fas',\n        'ff': 'ful',\n        'fi': 'fin',\n        'fj': 'fij',\n        'fo': 'fao',\n        'fr': 'fra',\n        'fy': 'fry',\n        'ga': 'gle',\n        'gd': 'gla',\n        'gl': 'glg',\n        'gn': 'grn',\n        'gu': 'guj',\n        'gv': 'glv',\n        'ha': 'hau',\n        'he': 'heb',\n        'iw': 'heb',  # Replaced by he in 1989 revision\n        'hi': 'hin',\n        'ho': 'hmo',\n        'hr': 'hrv',\n        'ht': 'hat',\n        'hu': 'hun',\n        'hy': 'hye',\n        'hz': 'her',\n        'ia': 'ina',\n        'id': 'ind',\n        'in': 'ind',  # Replaced by id in 1989 revision\n        'ie': 'ile',\n        'ig': 'ibo',\n        'ii': 'iii',\n        'ik': 'ipk',\n        'io': 'ido',\n        'is': 'isl',\n        'it': 'ita',\n        'iu': 'iku',\n        'ja': 'jpn',\n        'jv': 'jav',\n        'ka': 'kat',\n        'kg': 'kon',\n        'ki': 'kik',\n        'kj': 'kua',\n        'kk': 'kaz',\n        'kl': 'kal',\n        'km': 'khm',\n        'kn': 'kan',\n        'ko': 'kor',\n        'kr': 'kau',\n        'ks': 'kas',\n        'ku': 'kur',\n        'kv': 'kom',\n        'kw': 'cor',\n        'ky': 'kir',\n        'la': 'lat',\n        'lb': 'ltz',\n        'lg': 'lug',\n        'li': 'lim',\n        'ln': 'lin',\n        'lo': 'lao',\n        'lt': 'lit',\n        'lu': 'lub',\n        'lv': 'lav',\n        'mg': 'mlg',\n        'mh': 'mah',\n        'mi': 'mri',\n        'mk': 'mkd',\n        'ml': 'mal',\n        'mn': 'mon',\n        'mr': 'mar',\n        'ms': 'msa',\n        'mt': 'mlt',\n        'my': 'mya',\n        'na': 'nau',\n        'nb': 'nob',\n        'nd': 'nde',\n        'ne': 'nep',\n        'ng': 'ndo',\n        'nl': 'nld',\n        'nn': 'nno',\n        'no': 'nor',\n        'nr': 'nbl',\n        'nv': 'nav',\n        'ny': 'nya',\n        'oc': 'oci',\n        'oj': 'oji',\n        'om': 'orm',\n        'or': 'ori',\n        'os': 'oss',\n        'pa': 'pan',\n        'pe': 'per',\n        'pi': 'pli',\n        'pl': 'pol',\n        'ps': 'pus',\n        'pt': 'por',\n        'qu': 'que',\n        'rm': 'roh',\n        'rn': 'run',\n        'ro': 'ron',\n        'ru': 'rus',\n        'rw': 'kin',\n        'sa': 'san',\n        'sc': 'srd',\n        'sd': 'snd',\n        'se': 'sme',\n        'sg': 'sag',\n        'si': 'sin',\n        'sk': 'slk',\n        'sl': 'slv',\n        'sm': 'smo',\n        'sn': 'sna',\n        'so': 'som',\n        'sq': 'sqi',\n        'sr': 'srp',\n        'ss': 'ssw',\n        'st': 'sot',\n        'su': 'sun',\n        'sv': 'swe',\n        'sw': 'swa',\n        'ta': 'tam',\n        'te': 'tel',\n        'tg': 'tgk',\n        'th': 'tha',\n        'ti': 'tir',\n        'tk': 'tuk',\n        'tl': 'tgl',\n        'tn': 'tsn',\n        'to': 'ton',\n        'tr': 'tur',\n        'ts': 'tso',\n        'tt': 'tat',\n        'tw': 'twi',\n        'ty': 'tah',\n        'ug': 'uig',\n        'uk': 'ukr',\n        'ur': 'urd',\n        'uz': 'uzb',\n        've': 'ven',\n        'vi': 'vie',\n        'vo': 'vol',\n        'wa': 'wln',\n        'wo': 'wol',\n        'xh': 'xho',\n        'yi': 'yid',\n        'ji': 'yid',  # Replaced by yi in 1989 revision\n        'yo': 'yor',\n        'za': 'zha',\n        'zh': 'zho',\n        'zu': 'zul',\n    }\n\n    @classmethod\n    def short2long(cls, code):\n        \"\"\"Convert language code from ISO 639-1 to ISO 639-2/T\"\"\"\n        return cls._lang_map.get(code[:2])\n\n    @classmethod\n    def long2short(cls, code):\n        \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"\n        for short_name, long_name in cls._lang_map.items():\n            if long_name == code:\n                return short_name\n\n\nclass ISO3166Utils:\n    # From http://data.okfn.org/data/core/country-list\n    _country_map = {\n        'AF': 'Afghanistan',\n        'AX': '\u00c5land Islands',\n        'AL': 'Albania',\n        'DZ': 'Algeria',\n        'AS': 'American Samoa',\n        'AD': 'Andorra',\n        'AO': 'Angola',\n        'AI': 'Anguilla',\n        'AQ': 'Antarctica',\n        'AG': 'Antigua and Barbuda',\n        'AR': 'Argentina',\n        'AM': 'Armenia',\n        'AW': 'Aruba',\n        'AU': 'Australia',\n        'AT': 'Austria',\n        'AZ': 'Azerbaijan',\n        'BS': 'Bahamas',\n        'BH': 'Bahrain',\n        'BD': 'Bangladesh',\n        'BB': 'Barbados',\n        'BY': 'Belarus',\n        'BE': 'Belgium',\n        'BZ': 'Belize',\n        'BJ': 'Benin',\n        'BM': 'Bermuda',\n        'BT': 'Bhutan',\n        'BO': 'Bolivia, Plurinational State of',\n        'BQ': 'Bonaire, Sint Eustatius and Saba',\n        'BA': 'Bosnia and Herzegovina',\n        'BW': 'Botswana',\n        'BV': 'Bouvet Island',\n        'BR': 'Brazil',\n        'IO': 'British Indian Ocean Territory',\n        'BN': 'Brunei Darussalam',\n        'BG': 'Bulgaria',\n        'BF': 'Burkina Faso',\n        'BI': 'Burundi',\n        'KH': 'Cambodia',\n        'CM': 'Cameroon',\n        'CA': 'Canada',\n        'CV': 'Cape Verde',\n        'KY': 'Cayman Islands',\n        'CF': 'Central African Republic',\n        'TD': 'Chad',\n        'CL': 'Chile',\n        'CN': 'China',\n        'CX': 'Christmas Island',\n        'CC': 'Cocos (Keeling) Islands',\n        'CO': 'Colombia',\n        'KM': 'Comoros',\n        'CG': 'Congo',\n        'CD': 'Congo, the Democratic Republic of the',\n        'CK': 'Cook Islands',\n        'CR': 'Costa Rica',\n        'CI': 'C\u00f4te d\\'Ivoire',\n        'HR': 'Croatia',\n        'CU': 'Cuba',\n        'CW': 'Cura\u00e7ao',\n        'CY': 'Cyprus',\n        'CZ': 'Czech Republic',\n        'DK': 'Denmark',\n        'DJ': 'Djibouti',\n        'DM': 'Dominica',\n        'DO': 'Dominican Republic',\n        'EC': 'Ecuador',\n        'EG': 'Egypt',\n        'SV': 'El Salvador',\n        'GQ': 'Equatorial Guinea',\n        'ER': 'Eritrea',\n        'EE': 'Estonia',\n        'ET': 'Ethiopia',\n        'FK': 'Falkland Islands (Malvinas)',\n        'FO': 'Faroe Islands',\n        'FJ': 'Fiji',\n        'FI': 'Finland',\n        'FR': 'France',\n        'GF': 'French Guiana',\n        'PF': 'French Polynesia',\n        'TF': 'French Southern Territories',\n        'GA': 'Gabon',\n        'GM': 'Gambia',\n        'GE': 'Georgia',\n        'DE': 'Germany',\n        'GH': 'Ghana',\n        'GI': 'Gibraltar',\n        'GR': 'Greece',\n        'GL': 'Greenland',\n        'GD': 'Grenada',\n        'GP': 'Guadeloupe',\n        'GU': 'Guam',\n        'GT': 'Guatemala',\n        'GG': 'Guernsey',\n        'GN': 'Guinea',\n        'GW': 'Guinea-Bissau',\n        'GY': 'Guyana',\n        'HT': 'Haiti',\n        'HM': 'Heard Island and McDonald Islands',\n        'VA': 'Holy See (Vatican City State)',\n        'HN': 'Honduras',\n        'HK': 'Hong Kong',\n        'HU': 'Hungary',\n        'IS': 'Iceland',\n        'IN': 'India',\n        'ID': 'Indonesia',\n        'IR': 'Iran, Islamic Republic of',\n        'IQ': 'Iraq',\n        'IE': 'Ireland',\n        'IM': 'Isle of Man',\n        'IL': 'Israel',\n        'IT': 'Italy',\n        'JM': 'Jamaica',\n        'JP': 'Japan',\n        'JE': 'Jersey',\n        'JO': 'Jordan',\n        'KZ': 'Kazakhstan',\n        'KE': 'Kenya',\n        'KI': 'Kiribati',\n        'KP': 'Korea, Democratic People\\'s Republic of',\n        'KR': 'Korea, Republic of',\n        'KW': 'Kuwait',\n        'KG': 'Kyrgyzstan',\n        'LA': 'Lao People\\'s Democratic Republic',\n        'LV': 'Latvia',\n        'LB': 'Lebanon',\n        'LS': 'Lesotho',\n        'LR': 'Liberia',\n        'LY': 'Libya',\n        'LI': 'Liechtenstein',\n        'LT': 'Lithuania',\n        'LU': 'Luxembourg',\n        'MO': 'Macao',\n        'MK': 'Macedonia, the Former Yugoslav Republic of',\n        'MG': 'Madagascar',\n        'MW': 'Malawi',\n        'MY': 'Malaysia',\n        'MV': 'Maldives',\n        'ML': 'Mali',\n        'MT': 'Malta',\n        'MH': 'Marshall Islands',\n        'MQ': 'Martinique',\n        'MR': 'Mauritania',\n        'MU': 'Mauritius',\n        'YT': 'Mayotte',\n        'MX': 'Mexico',\n        'FM': 'Micronesia, Federated States of',\n        'MD': 'Moldova, Republic of',\n        'MC': 'Monaco',\n        'MN': 'Mongolia',\n        'ME': 'Montenegro',\n        'MS': 'Montserrat',\n        'MA': 'Morocco',\n        'MZ': 'Mozambique',\n        'MM': 'Myanmar',\n        'NA': 'Namibia',\n        'NR': 'Nauru',\n        'NP': 'Nepal',\n        'NL': 'Netherlands',\n        'NC': 'New Caledonia',\n        'NZ': 'New Zealand',\n        'NI': 'Nicaragua',\n        'NE': 'Niger',\n        'NG': 'Nigeria',\n        'NU': 'Niue',\n        'NF': 'Norfolk Island',\n        'MP': 'Northern Mariana Islands',\n        'NO': 'Norway',\n        'OM': 'Oman',\n        'PK': 'Pakistan',\n        'PW': 'Palau',\n        'PS': 'Palestine, State of',\n        'PA': 'Panama',\n        'PG': 'Papua New Guinea',\n        'PY': 'Paraguay',\n        'PE': 'Peru',\n        'PH': 'Philippines',\n        'PN': 'Pitcairn',\n        'PL': 'Poland',\n        'PT': 'Portugal',\n        'PR': 'Puerto Rico',\n        'QA': 'Qatar',\n        'RE': 'R\u00e9union',\n        'RO': 'Romania',\n        'RU': 'Russian Federation',\n        'RW': 'Rwanda',\n        'BL': 'Saint Barth\u00e9lemy',\n        'SH': 'Saint Helena, Ascension and Tristan da Cunha',\n        'KN': 'Saint Kitts and Nevis',\n        'LC': 'Saint Lucia',\n        'MF': 'Saint Martin (French part)',\n        'PM': 'Saint Pierre and Miquelon',\n        'VC': 'Saint Vincent and the Grenadines',\n        'WS': 'Samoa',\n        'SM': 'San Marino',\n        'ST': 'Sao Tome and Principe',\n        'SA': 'Saudi Arabia',\n        'SN': 'Senegal',\n        'RS': 'Serbia',\n        'SC': 'Seychelles',\n        'SL': 'Sierra Leone',\n        'SG': 'Singapore',\n        'SX': 'Sint Maarten (Dutch part)',\n        'SK': 'Slovakia',\n        'SI': 'Slovenia',\n        'SB': 'Solomon Islands',\n        'SO': 'Somalia',\n        'ZA': 'South Africa',\n        'GS': 'South Georgia and the South Sandwich Islands',\n        'SS': 'South Sudan',\n        'ES': 'Spain',\n        'LK': 'Sri Lanka',\n        'SD': 'Sudan',\n        'SR': 'Suriname',\n        'SJ': 'Svalbard and Jan Mayen',\n        'SZ': 'Swaziland',\n        'SE': 'Sweden',\n        'CH': 'Switzerland',\n        'SY': 'Syrian Arab Republic',\n        'TW': 'Taiwan, Province of China',\n        'TJ': 'Tajikistan',\n        'TZ': 'Tanzania, United Republic of',\n        'TH': 'Thailand',\n        'TL': 'Timor-Leste',\n        'TG': 'Togo',\n        'TK': 'Tokelau',\n        'TO': 'Tonga',\n        'TT': 'Trinidad and Tobago',\n        'TN': 'Tunisia',\n        'TR': 'Turkey',\n        'TM': 'Turkmenistan',\n        'TC': 'Turks and Caicos Islands',\n        'TV': 'Tuvalu',\n        'UG': 'Uganda',\n        'UA': 'Ukraine',\n        'AE': 'United Arab Emirates',\n        'GB': 'United Kingdom',\n        'US': 'United States',\n        'UM': 'United States Minor Outlying Islands',\n        'UY': 'Uruguay',\n        'UZ': 'Uzbekistan',\n        'VU': 'Vanuatu',\n        'VE': 'Venezuela, Bolivarian Republic of',\n        'VN': 'Viet Nam',\n        'VG': 'Virgin Islands, British',\n        'VI': 'Virgin Islands, U.S.',\n        'WF': 'Wallis and Futuna',\n        'EH': 'Western Sahara',\n        'YE': 'Yemen',\n        'ZM': 'Zambia',\n        'ZW': 'Zimbabwe',\n        # Not ISO 3166 codes, but used for IP blocks\n        'AP': 'Asia/Pacific Region',\n        'EU': 'Europe',\n    }\n\n    @classmethod\n    def short2full(cls, code):\n        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"\n        return cls._country_map.get(code.upper())\n\n\nclass GeoUtils:\n    # Major IPv4 address blocks per country\n    _country_ip_map = {\n        'AD': '46.172.224.0/19',\n        'AE': '94.200.0.0/13',\n        'AF': '149.54.0.0/17',\n        'AG': '209.59.64.0/18',\n        'AI': '204.14.248.0/21',\n        'AL': '46.99.0.0/16',\n        'AM': '46.70.0.0/15',\n        'AO': '105.168.0.0/13',\n        'AP': '182.50.184.0/21',\n        'AQ': '23.154.160.0/24',\n        'AR': '181.0.0.0/12',\n        'AS': '202.70.112.0/20',\n        'AT': '77.116.0.0/14',\n        'AU': '1.128.0.0/11',\n        'AW': '181.41.0.0/18',\n        'AX': '185.217.4.0/22',\n        'AZ': '5.197.0.0/16',\n        'BA': '31.176.128.0/17',\n        'BB': '65.48.128.0/17',\n        'BD': '114.130.0.0/16',\n        'BE': '57.0.0.0/8',\n        'BF': '102.178.0.0/15',\n        'BG': '95.42.0.0/15',\n        'BH': '37.131.0.0/17',\n        'BI': '154.117.192.0/18',\n        'BJ': '137.255.0.0/16',\n        'BL': '185.212.72.0/23',\n        'BM': '196.12.64.0/18',\n        'BN': '156.31.0.0/16',\n        'BO': '161.56.0.0/16',\n        'BQ': '161.0.80.0/20',\n        'BR': '191.128.0.0/12',\n        'BS': '24.51.64.0/18',\n        'BT': '119.2.96.0/19',\n        'BW': '168.167.0.0/16',\n        'BY': '178.120.0.0/13',\n        'BZ': '179.42.192.0/18',\n        'CA': '99.224.0.0/11',\n        'CD': '41.243.0.0/16',\n        'CF': '197.242.176.0/21',\n        'CG': '160.113.0.0/16',\n        'CH': '85.0.0.0/13',\n        'CI': '102.136.0.0/14',\n        'CK': '202.65.32.0/19',\n        'CL': '152.172.0.0/14',\n        'CM': '102.244.0.0/14',\n        'CN': '36.128.0.0/10',\n        'CO': '181.240.0.0/12',\n        'CR': '201.192.0.0/12',\n        'CU': '152.206.0.0/15',\n        'CV': '165.90.96.0/19',\n        'CW': '190.88.128.0/17',\n        'CY': '31.153.0.0/16',\n        'CZ': '88.100.0.0/14',\n        'DE': '53.0.0.0/8',\n        'DJ': '197.241.0.0/17',\n        'DK': '87.48.0.0/12',\n        'DM': '192.243.48.0/20',\n        'DO': '152.166.0.0/15',\n        'DZ': '41.96.0.0/12',\n        'EC': '186.68.0.0/15',\n        'EE': '90.190.0.0/15',\n        'EG': '156.160.0.0/11',\n        'ER': '196.200.96.0/20',\n        'ES': '88.0.0.0/11',\n        'ET': '196.188.0.0/14',\n        'EU': '2.16.0.0/13',\n        'FI': '91.152.0.0/13',\n        'FJ': '144.120.0.0/16',\n        'FK': '80.73.208.0/21',\n        'FM': '119.252.112.0/20',\n        'FO': '88.85.32.0/19',\n        'FR': '90.0.0.0/9',\n        'GA': '41.158.0.0/15',\n        'GB': '25.0.0.0/8',\n        'GD': '74.122.88.0/21',\n        'GE': '31.146.0.0/16',\n        'GF': '161.22.64.0/18',\n        'GG': '62.68.160.0/19',\n        'GH': '154.160.0.0/12',\n        'GI': '95.164.0.0/16',\n        'GL': '88.83.0.0/19',\n        'GM': '160.182.0.0/15',\n        'GN': '197.149.192.0/18',\n        'GP': '104.250.0.0/19',\n        'GQ': '105.235.224.0/20',\n        'GR': '94.64.0.0/13',\n        'GT': '168.234.0.0/16',\n        'GU': '168.123.0.0/16',\n        'GW': '197.214.80.0/20',\n        'GY': '181.41.64.0/18',\n        'HK': '113.252.0.0/14',\n        'HN': '181.210.0.0/16',\n        'HR': '93.136.0.0/13',\n        'HT': '148.102.128.0/17',\n        'HU': '84.0.0.0/14',\n        'ID': '39.192.0.0/10',\n        'IE': '87.32.0.0/12',\n        'IL': '79.176.0.0/13',\n        'IM': '5.62.80.0/20',\n        'IN': '117.192.0.0/10',\n        'IO': '203.83.48.0/21',\n        'IQ': '37.236.0.0/14',\n        'IR': '2.176.0.0/12',\n        'IS': '82.221.0.0/16',\n        'IT': '79.0.0.0/10',\n        'JE': '87.244.64.0/18',\n        'JM': '72.27.0.0/17',\n        'JO': '176.29.0.0/16',\n        'JP': '133.0.0.0/8',\n        'KE': '105.48.0.0/12',\n        'KG': '158.181.128.0/17',\n        'KH': '36.37.128.0/17',\n        'KI': '103.25.140.0/22',\n        'KM': '197.255.224.0/20',\n        'KN': '198.167.192.0/19',\n        'KP': '175.45.176.0/22',\n        'KR': '175.192.0.0/10',\n        'KW': '37.36.0.0/14',\n        'KY': '64.96.0.0/15',\n        'KZ': '2.72.0.0/13',\n        'LA': '115.84.64.0/18',\n        'LB': '178.135.0.0/16',\n        'LC': '24.92.144.0/20',\n        'LI': '82.117.0.0/19',\n        'LK': '112.134.0.0/15',\n        'LR': '102.183.0.0/16',\n        'LS': '129.232.0.0/17',\n        'LT': '78.56.0.0/13',\n        'LU': '188.42.0.0/16',\n        'LV': '46.109.0.0/16',\n        'LY': '41.252.0.0/14',\n        'MA': '105.128.0.0/11',\n        'MC': '88.209.64.0/18',\n        'MD': '37.246.0.0/16',\n        'ME': '178.175.0.0/17',\n        'MF': '74.112.232.0/21',\n        'MG': '154.126.0.0/17',\n        'MH': '117.103.88.0/21',\n        'MK': '77.28.0.0/15',\n        'ML': '154.118.128.0/18',\n        'MM': '37.111.0.0/17',\n        'MN': '49.0.128.0/17',\n        'MO': '60.246.0.0/16',\n        'MP': '202.88.64.0/20',\n        'MQ': '109.203.224.0/19',\n        'MR': '41.188.64.0/18',\n        'MS': '208.90.112.0/22',\n        'MT': '46.11.0.0/16',\n        'MU': '105.16.0.0/12',\n        'MV': '27.114.128.0/18',\n        'MW': '102.70.0.0/15',\n        'MX': '187.192.0.0/11',\n        'MY': '175.136.0.0/13',\n        'MZ': '197.218.0.0/15',\n        'NA': '41.182.0.0/16',\n        'NC': '101.101.0.0/18',\n        'NE': '197.214.0.0/18',\n        'NF': '203.17.240.0/22',\n        'NG': '105.112.0.0/12',\n        'NI': '186.76.0.0/15',\n        'NL': '145.96.0.0/11',\n        'NO': '84.208.0.0/13',\n        'NP': '36.252.0.0/15',\n        'NR': '203.98.224.0/19',\n        'NU': '49.156.48.0/22',\n        'NZ': '49.224.0.0/14',\n        'OM': '5.36.0.0/15',\n        'PA': '186.72.0.0/15',\n        'PE': '186.160.0.0/14',\n        'PF': '123.50.64.0/18',\n        'PG': '124.240.192.0/19',\n        'PH': '49.144.0.0/13',\n        'PK': '39.32.0.0/11',\n        'PL': '83.0.0.0/11',\n        'PM': '70.36.0.0/20',\n        'PR': '66.50.0.0/16',\n        'PS': '188.161.0.0/16',\n        'PT': '85.240.0.0/13',\n        'PW': '202.124.224.0/20',\n        'PY': '181.120.0.0/14',\n        'QA': '37.210.0.0/15',\n        'RE': '102.35.0.0/16',\n        'RO': '79.112.0.0/13',\n        'RS': '93.86.0.0/15',\n        'RU': '5.136.0.0/13',\n        'RW': '41.186.0.0/16',\n        'SA': '188.48.0.0/13',\n        'SB': '202.1.160.0/19',\n        'SC': '154.192.0.0/11',\n        'SD': '102.120.0.0/13',\n        'SE': '78.64.0.0/12',\n        'SG': '8.128.0.0/10',\n        'SI': '188.196.0.0/14',\n        'SK': '78.98.0.0/15',\n        'SL': '102.143.0.0/17',\n        'SM': '89.186.32.0/19',\n        'SN': '41.82.0.0/15',\n        'SO': '154.115.192.0/18',\n        'SR': '186.179.128.0/17',\n        'SS': '105.235.208.0/21',\n        'ST': '197.159.160.0/19',\n        'SV': '168.243.0.0/16',\n        'SX': '190.102.0.0/20',\n        'SY': '5.0.0.0/16',\n        'SZ': '41.84.224.0/19',\n        'TC': '65.255.48.0/20',\n        'TD': '154.68.128.0/19',\n        'TG': '196.168.0.0/14',\n        'TH': '171.96.0.0/13',\n        'TJ': '85.9.128.0/18',\n        'TK': '27.96.24.0/21',\n        'TL': '180.189.160.0/20',\n        'TM': '95.85.96.0/19',\n        'TN': '197.0.0.0/11',\n        'TO': '175.176.144.0/21',\n        'TR': '78.160.0.0/11',\n        'TT': '186.44.0.0/15',\n        'TV': '202.2.96.0/19',\n        'TW': '120.96.0.0/11',\n        'TZ': '156.156.0.0/14',\n        'UA': '37.52.0.0/14',\n        'UG': '102.80.0.0/13',\n        'US': '6.0.0.0/8',\n        'UY': '167.56.0.0/13',\n        'UZ': '84.54.64.0/18',\n        'VA': '212.77.0.0/19',\n        'VC': '207.191.240.0/21',\n        'VE': '186.88.0.0/13',\n        'VG': '66.81.192.0/20',\n        'VI': '146.226.0.0/16',\n        'VN': '14.160.0.0/11',\n        'VU': '202.80.32.0/20',\n        'WF': '117.20.32.0/21',\n        'WS': '202.4.32.0/19',\n        'YE': '134.35.0.0/16',\n        'YT': '41.242.116.0/22',\n        'ZA': '41.0.0.0/11',\n        'ZM': '102.144.0.0/13',\n        'ZW': '102.177.192.0/18',\n    }\n\n    @classmethod\n    def random_ipv4(cls, code_or_block):\n        if len(code_or_block) == 2:\n            block = cls._country_ip_map.get(code_or_block.upper())\n            if not block:\n                return None\n        else:\n            block = code_or_block\n        addr, preflen = block.split('/')\n        addr_min = struct.unpack('!L', socket.inet_aton(addr))[0]\n        addr_max = addr_min | (0xffffffff >> int(preflen))\n        return str(socket.inet_ntoa(\n            struct.pack('!L', random.randint(addr_min, addr_max))))\n\n\nclass PerRequestProxyHandler(urllib.request.ProxyHandler):\n    def __init__(self, proxies=None):\n        # Set default handlers\n        for type in ('http', 'https'):\n            setattr(self, '%s_open' % type,\n                    lambda r, proxy='__noproxy__', type=type, meth=self.proxy_open:\n                        meth(r, proxy, type))\n        urllib.request.ProxyHandler.__init__(self, proxies)\n\n    def proxy_open(self, req, proxy, type):\n        req_proxy = req.headers.get('Ytdl-request-proxy')\n        if req_proxy is not None:\n            proxy = req_proxy\n            del req.headers['Ytdl-request-proxy']\n\n        if proxy == '__noproxy__':\n            return None  # No Proxy\n        if urllib.parse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n            req.add_header('Ytdl-socks-proxy', proxy)\n            # yt-dlp's http/https handlers do wrapping the socket with socks\n            return None\n        return urllib.request.ProxyHandler.proxy_open(\n            self, req, proxy, type)\n\n\n# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is\n# released into Public Domain\n# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387\n\ndef long_to_bytes(n, blocksize=0):\n    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n    Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front of the\n    byte string with binary zeros so that the length is a multiple of\n    blocksize.\n    \"\"\"\n    # after much testing, this algorithm was deemed to be the fastest\n    s = b''\n    n = int(n)\n    while n > 0:\n        s = struct.pack('>I', n & 0xffffffff) + s\n        n = n >> 32\n    # strip off leading zeros\n    for i in range(len(s)):\n        if s[i] != b'\\000'[0]:\n            break\n    else:\n        # only happens when n == 0\n        s = b'\\000'\n        i = 0\n    s = s[i:]\n    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n    # de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n    return s\n\n\ndef bytes_to_long(s):\n    \"\"\"bytes_to_long(string) : long\n    Convert a byte string to a long integer.\n\n    This is (essentially) the inverse of long_to_bytes().\n    \"\"\"\n    acc = 0\n    length = len(s)\n    if length % 4:\n        extra = (4 - length % 4)\n        s = b'\\000' * extra + s\n        length = length + extra\n    for i in range(0, length, 4):\n        acc = (acc << 32) + struct.unpack('>I', s[i:i + 4])[0]\n    return acc\n\n\ndef ohdave_rsa_encrypt(data, exponent, modulus):\n    '''\n    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n\n    Input:\n        data: data to encrypt, bytes-like object\n        exponent, modulus: parameter e and N of RSA algorithm, both integer\n    Output: hex string of encrypted data\n\n    Limitation: supports one block encryption only\n    '''\n\n    payload = int(binascii.hexlify(data[::-1]), 16)\n    encrypted = pow(payload, exponent, modulus)\n    return '%x' % encrypted\n\n\ndef pkcs1pad(data, length):\n    \"\"\"\n    Padding input data with PKCS#1 scheme\n\n    @param {int[]} data        input data\n    @param {int}   length      target length\n    @returns {int[]}           padded data\n    \"\"\"\n    if len(data) > length - 11:\n        raise ValueError('Input data too long for PKCS#1 padding')\n\n    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n    return [0, 2] + pseudo_random + [0] + data\n\n\ndef _base_n_table(n, table):\n    if not table and not n:\n        raise ValueError('Either table or n must be specified')\n    table = (table or '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')[:n]\n\n    if n and n != len(table):\n        raise ValueError(f'base {n} exceeds table length {len(table)}')\n    return table\n\n\ndef encode_base_n(num, n=None, table=None):\n    \"\"\"Convert given int to a base-n string\"\"\"\n    table = _base_n_table(n, table)\n    if not num:\n        return table[0]\n\n    result, base = '', len(table)\n    while num:\n        result = table[num % base] + result\n        num = num // base\n    return result\n\n\ndef decode_base_n(string, n=None, table=None):\n    \"\"\"Convert given base-n string to int\"\"\"\n    table = {char: index for index, char in enumerate(_base_n_table(n, table))}\n    result, base = 0, len(table)\n    for char in string:\n        result = result * base + table[char]\n    return result\n\n\ndef decode_packed_codes(code):\n    mobj = re.search(PACKED_CODES_RE, code)\n    obfuscated_code, base, count, symbols = mobj.groups()\n    base = int(base)\n    count = int(count)\n    symbols = symbols.split('|')\n    symbol_table = {}\n\n    while count:\n        count -= 1\n        base_n_count = encode_base_n(count, base)\n        symbol_table[base_n_count] = symbols[count] or base_n_count\n\n    return re.sub(\n        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n        obfuscated_code)\n\n\ndef caesar(s, alphabet, shift):\n    if shift == 0:\n        return s\n    l = len(alphabet)\n    return ''.join(\n        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n        for c in s)\n\n\ndef rot47(s):\n    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)\n\n\ndef parse_m3u8_attributes(attrib):\n    info = {}\n    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n        if val.startswith('\"'):\n            val = val[1:-1]\n        info[key] = val\n    return info\n\n\ndef urshift(val, n):\n    return val >> n if val >= 0 else (val + 0x100000000) >> n\n\n\ndef write_xattr(path, key, value):\n    # Windows: Write xattrs to NTFS Alternate Data Streams:\n    # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29\n    if compat_os_name == 'nt':\n        assert ':' not in key\n        assert os.path.exists(path)\n\n        try:\n            with open(f'{path}:{key}', 'wb') as f:\n                f.write(value)\n        except OSError as e:\n            raise XAttrMetadataError(e.errno, e.strerror)\n        return\n\n    # UNIX Method 1. Use xattrs/pyxattrs modules\n\n    setxattr = None\n    if getattr(xattr, '_yt_dlp__identifier', None) == 'pyxattr':\n        # Unicode arguments are not supported in pyxattr until version 0.5.0\n        # See https://github.com/ytdl-org/youtube-dl/issues/5498\n        if version_tuple(xattr.__version__) >= (0, 5, 0):\n            setxattr = xattr.set\n    elif xattr:\n        setxattr = xattr.setxattr\n\n    if setxattr:\n        try:\n            setxattr(path, key, value)\n        except OSError as e:\n            raise XAttrMetadataError(e.errno, e.strerror)\n        return\n\n    # UNIX Method 2. Use setfattr/xattr executables\n    exe = ('setfattr' if check_executable('setfattr', ['--version'])\n           else 'xattr' if check_executable('xattr', ['-h']) else None)\n    if not exe:\n        raise XAttrUnavailableError(\n            'Couldn\\'t find a tool to set the xattrs. Install either the python \"xattr\" or \"pyxattr\" modules or the '\n            + ('\"xattr\" binary' if sys.platform != 'linux' else 'GNU \"attr\" package (which contains the \"setfattr\" tool)'))\n\n    value = value.decode()\n    try:\n        _, stderr, returncode = Popen.run(\n            [exe, '-w', key, value, path] if exe == 'xattr' else [exe, '-n', key, '-v', value, path],\n            text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n    except OSError as e:\n        raise XAttrMetadataError(e.errno, e.strerror)\n    if returncode:\n        raise XAttrMetadataError(returncode, stderr)\n\n\ndef random_birthday(year_field, month_field, day_field):\n    start_date = datetime.date(1950, 1, 1)\n    end_date = datetime.date(1995, 12, 31)\n    offset = random.randint(0, (end_date - start_date).days)\n    random_date = start_date + datetime.timedelta(offset)\n    return {\n        year_field: str(random_date.year),\n        month_field: str(random_date.month),\n        day_field: str(random_date.day),\n    }\n\n\ndef find_available_port(interface=''):\n    try:\n        with socket.socket() as sock:\n            sock.bind((interface, 0))\n            return sock.getsockname()[1]\n    except OSError:\n        return None\n\n\n# Templates for internet shortcut files, which are plain text files.\nDOT_URL_LINK_TEMPLATE = '''\\\n[InternetShortcut]\nURL=%(url)s\n'''\n\nDOT_WEBLOC_LINK_TEMPLATE = '''\\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n\\t<key>URL</key>\n\\t<string>%(url)s</string>\n</dict>\n</plist>\n'''\n\nDOT_DESKTOP_LINK_TEMPLATE = '''\\\n[Desktop Entry]\nEncoding=UTF-8\nName=%(filename)s\nType=Link\nURL=%(url)s\nIcon=text-html\n'''\n\nLINK_TEMPLATES = {\n    'url': DOT_URL_LINK_TEMPLATE,\n    'desktop': DOT_DESKTOP_LINK_TEMPLATE,\n    'webloc': DOT_WEBLOC_LINK_TEMPLATE,\n}\n\n\ndef iri_to_uri(iri):\n    \"\"\"\n    Converts an IRI (Internationalized Resource Identifier, allowing Unicode characters) to a URI (Uniform Resource Identifier, ASCII-only).\n\n    The function doesn't add an additional layer of escaping; e.g., it doesn't escape `%3C` as `%253C`. Instead, it percent-escapes characters with an underlying UTF-8 encoding *besides* those already escaped, leaving the URI intact.\n    \"\"\"\n\n    iri_parts = urllib.parse.urlparse(iri)\n\n    if '[' in iri_parts.netloc:\n        raise ValueError('IPv6 URIs are not, yet, supported.')\n        # Querying `.netloc`, when there's only one bracket, also raises a ValueError.\n\n    # The `safe` argument values, that the following code uses, contain the characters that should not be percent-encoded. Everything else but letters, digits and '_.-' will be percent-encoded with an underlying UTF-8 encoding. Everything already percent-encoded will be left as is.\n\n    net_location = ''\n    if iri_parts.username:\n        net_location += urllib.parse.quote(iri_parts.username, safe=r\"!$%&'()*+,~\")\n        if iri_parts.password is not None:\n            net_location += ':' + urllib.parse.quote(iri_parts.password, safe=r\"!$%&'()*+,~\")\n        net_location += '@'\n\n    net_location += iri_parts.hostname.encode('idna').decode()  # Punycode for Unicode hostnames.\n    # The 'idna' encoding produces ASCII text.\n    if iri_parts.port is not None and iri_parts.port != 80:\n        net_location += ':' + str(iri_parts.port)\n\n    return urllib.parse.urlunparse(\n        (iri_parts.scheme,\n            net_location,\n\n            urllib.parse.quote_plus(iri_parts.path, safe=r\"!$%&'()*+,/:;=@|~\"),\n\n            # Unsure about the `safe` argument, since this is a legacy way of handling parameters.\n            urllib.parse.quote_plus(iri_parts.params, safe=r\"!$%&'()*+,/:;=@|~\"),\n\n            # Not totally sure about the `safe` argument, since the source does not explicitly mention the query URI component.\n            urllib.parse.quote_plus(iri_parts.query, safe=r\"!$%&'()*+,/:;=?@{|}~\"),\n\n            urllib.parse.quote_plus(iri_parts.fragment, safe=r\"!#$%&'()*+,/:;=?@{|}~\")))\n\n    # Source for `safe` arguments: https://url.spec.whatwg.org/#percent-encoded-bytes.\n\n\ndef to_high_limit_path(path):\n    if sys.platform in ['win32', 'cygwin']:\n        # Work around MAX_PATH limitation on Windows. The maximum allowed length for the individual path segments may still be quite limited.\n        return '\\\\\\\\?\\\\' + os.path.abspath(path)\n\n    return path\n\n\ndef format_field(obj, field=None, template='%s', ignore=NO_DEFAULT, default='', func=IDENTITY):\n    val = traversal.traverse_obj(obj, *variadic(field))\n    if not val if ignore is NO_DEFAULT else val in variadic(ignore):\n        return default\n    return template % func(val)\n\n\ndef clean_podcast_url(url):\n    url = re.sub(r'''(?x)\n        (?:\n            (?:\n                chtbl\\.com/track|\n                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/\n                play\\.podtrac\\.com\n            )/[^/]+|\n            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure\n            flex\\.acast\\.com|\n            pd(?:\n                cn\\.co| # https://podcorn.com/analytics-prefix/\n                st\\.fm # https://podsights.com/docs/\n            )/e\n        )/''', '', url)\n    return re.sub(r'^\\w+://(\\w+://)', r'\\1', url)\n\n\n_HEX_TABLE = '0123456789abcdef'\n\n\ndef random_uuidv4():\n    return re.sub(r'[xy]', lambda x: _HEX_TABLE[random.randint(0, 15)], 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx')\n\n\ndef make_dir(path, to_screen=None):\n    try:\n        dn = os.path.dirname(path)\n        if dn:\n            os.makedirs(dn, exist_ok=True)\n        return True\n    except OSError as err:\n        if callable(to_screen) is not None:\n            to_screen(f'unable to create directory {err}')\n        return False\n\n\ndef get_executable_path():\n    from ..update import _get_variant_and_executable_path\n\n    return os.path.dirname(os.path.abspath(_get_variant_and_executable_path()[1]))\n\n\ndef get_user_config_dirs(package_name):\n    # .config (e.g. ~/.config/package_name)\n    xdg_config_home = os.getenv('XDG_CONFIG_HOME') or compat_expanduser('~/.config')\n    yield os.path.join(xdg_config_home, package_name)\n\n    # appdata (%APPDATA%/package_name)\n    appdata_dir = os.getenv('appdata')\n    if appdata_dir:\n        yield os.path.join(appdata_dir, package_name)\n\n    # home (~/.package_name)\n    yield os.path.join(compat_expanduser('~'), f'.{package_name}')\n\n\ndef get_system_config_dirs(package_name):\n    # /etc/package_name\n    yield os.path.join('/etc', package_name)\n\n\ndef time_seconds(**kwargs):\n    \"\"\"\n    Returns TZ-aware time in seconds since the epoch (1970-01-01T00:00:00Z)\n    \"\"\"\n    return time.time() + datetime.timedelta(**kwargs).total_seconds()\n\n\n# create a JSON Web Signature (jws) with HS256 algorithm\n# the resulting format is in JWS Compact Serialization\n# implemented following JWT https://www.rfc-editor.org/rfc/rfc7519.html\n# implemented following JWS https://www.rfc-editor.org/rfc/rfc7515.html\ndef jwt_encode_hs256(payload_data, key, headers={}):\n    header_data = {\n        'alg': 'HS256',\n        'typ': 'JWT',\n    }\n    if headers:\n        header_data.update(headers)\n    header_b64 = base64.b64encode(json.dumps(header_data).encode())\n    payload_b64 = base64.b64encode(json.dumps(payload_data).encode())\n    h = hmac.new(key.encode(), header_b64 + b'.' + payload_b64, hashlib.sha256)\n    signature_b64 = base64.b64encode(h.digest())\n    token = header_b64 + b'.' + payload_b64 + b'.' + signature_b64\n    return token\n\n\n# can be extended in future to verify the signature and parse header and return the algorithm used if it's not HS256\ndef jwt_decode_hs256(jwt):\n    header_b64, payload_b64, signature_b64 = jwt.split('.')\n    # add trailing ='s that may have been stripped, superfluous ='s are ignored\n    payload_data = json.loads(base64.urlsafe_b64decode(f'{payload_b64}==='))\n    return payload_data\n\n\nWINDOWS_VT_MODE = False if compat_os_name == 'nt' else None\n\n\n@functools.cache\ndef supports_terminal_sequences(stream):\n    if compat_os_name == 'nt':\n        if not WINDOWS_VT_MODE:\n            return False\n    elif not os.getenv('TERM'):\n        return False\n    try:\n        return stream.isatty()\n    except BaseException:\n        return False\n\n\ndef windows_enable_vt_mode():\n    \"\"\"Ref: https://bugs.python.org/issue30075 \"\"\"\n    if get_windows_version() < (10, 0, 10586):\n        return\n\n    import ctypes\n    import ctypes.wintypes\n    import msvcrt\n\n    ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x0004\n\n    dll = ctypes.WinDLL('kernel32', use_last_error=False)\n    handle = os.open('CONOUT$', os.O_RDWR)\n    try:\n        h_out = ctypes.wintypes.HANDLE(msvcrt.get_osfhandle(handle))\n        dw_original_mode = ctypes.wintypes.DWORD()\n        success = dll.GetConsoleMode(h_out, ctypes.byref(dw_original_mode))\n        if not success:\n            raise Exception('GetConsoleMode failed')\n\n        success = dll.SetConsoleMode(h_out, ctypes.wintypes.DWORD(\n            dw_original_mode.value | ENABLE_VIRTUAL_TERMINAL_PROCESSING))\n        if not success:\n            raise Exception('SetConsoleMode failed')\n    finally:\n        os.close(handle)\n\n    global WINDOWS_VT_MODE\n    WINDOWS_VT_MODE = True\n    supports_terminal_sequences.cache_clear()\n\n\n_terminal_sequences_re = re.compile('\\033\\\\[[^m]+m')\n\n\ndef remove_terminal_sequences(string):\n    return _terminal_sequences_re.sub('', string)\n\n\ndef number_of_digits(number):\n    return len('%d' % number)\n\n\ndef join_nonempty(*values, delim='-', from_dict=None):\n    if from_dict is not None:\n        values = (traversal.traverse_obj(from_dict, variadic(v)) for v in values)\n    return delim.join(map(str, filter(None, values)))\n\n\ndef scale_thumbnails_to_max_format_width(formats, thumbnails, url_width_re):\n    \"\"\"\n    Find the largest format dimensions in terms of video width and, for each thumbnail:\n    * Modify the URL: Match the width with the provided regex and replace with the former width\n    * Update dimensions\n\n    This function is useful with video services that scale the provided thumbnails on demand\n    \"\"\"\n    _keys = ('width', 'height')\n    max_dimensions = max(\n        (tuple(format.get(k) or 0 for k in _keys) for format in formats),\n        default=(0, 0))\n    if not max_dimensions[0]:\n        return thumbnails\n    return [\n        merge_dicts(\n            {'url': re.sub(url_width_re, str(max_dimensions[0]), thumbnail['url'])},\n            dict(zip(_keys, max_dimensions)), thumbnail)\n        for thumbnail in thumbnails\n    ]\n\n\ndef parse_http_range(range):\n    \"\"\" Parse value of \"Range\" or \"Content-Range\" HTTP header into tuple. \"\"\"\n    if not range:\n        return None, None, None\n    crg = re.search(r'bytes[ =](\\d+)-(\\d+)?(?:/(\\d+))?', range)\n    if not crg:\n        return None, None, None\n    return int(crg.group(1)), int_or_none(crg.group(2)), int_or_none(crg.group(3))\n\n\ndef read_stdin(what):\n    eof = 'Ctrl+Z' if compat_os_name == 'nt' else 'Ctrl+D'\n    write_string(f'Reading {what} from STDIN - EOF ({eof}) to end:\\n')\n    return sys.stdin\n\n\ndef determine_file_encoding(data):\n    \"\"\"\n    Detect the text encoding used\n    @returns (encoding, bytes to skip)\n    \"\"\"\n\n    # BOM marks are given priority over declarations\n    for bom, enc in BOMS:\n        if data.startswith(bom):\n            return enc, len(bom)\n\n    # Strip off all null bytes to match even when UTF-16 or UTF-32 is used.\n    # We ignore the endianness to get a good enough match\n    data = data.replace(b'\\0', b'')\n    mobj = re.match(rb'(?m)^#\\s*coding\\s*:\\s*(\\S+)\\s*$', data)\n    return mobj.group(1).decode() if mobj else None, 0\n\n\nclass Config:\n    own_args = None\n    parsed_args = None\n    filename = None\n    __initialized = False\n\n    def __init__(self, parser, label=None):\n        self.parser, self.label = parser, label\n        self._loaded_paths, self.configs = set(), []\n\n    def init(self, args=None, filename=None):\n        assert not self.__initialized\n        self.own_args, self.filename = args, filename\n        return self.load_configs()\n\n    def load_configs(self):\n        directory = ''\n        if self.filename:\n            location = os.path.realpath(self.filename)\n            directory = os.path.dirname(location)\n            if location in self._loaded_paths:\n                return False\n            self._loaded_paths.add(location)\n\n        self.__initialized = True\n        opts, _ = self.parser.parse_known_args(self.own_args)\n        self.parsed_args = self.own_args\n        for location in opts.config_locations or []:\n            if location == '-':\n                if location in self._loaded_paths:\n                    continue\n                self._loaded_paths.add(location)\n                self.append_config(shlex.split(read_stdin('options'), comments=True), label='stdin')\n                continue\n            location = os.path.join(directory, expand_path(location))\n            if os.path.isdir(location):\n                location = os.path.join(location, 'yt-dlp.conf')\n            if not os.path.exists(location):\n                self.parser.error(f'config location {location} does not exist')\n            self.append_config(self.read_file(location), location)\n        return True\n\n    def __str__(self):\n        label = join_nonempty(\n            self.label, 'config', f'\"{self.filename}\"' if self.filename else '',\n            delim=' ')\n        return join_nonempty(\n            self.own_args is not None and f'{label[0].upper()}{label[1:]}: {self.hide_login_info(self.own_args)}',\n            *(f'\\n{c}'.replace('\\n', '\\n| ')[1:] for c in self.configs),\n            delim='\\n')\n\n    @staticmethod\n    def read_file(filename, default=[]):\n        try:\n            optionf = open(filename, 'rb')\n        except OSError:\n            return default  # silently skip if file is not present\n        try:\n            enc, skip = determine_file_encoding(optionf.read(512))\n            optionf.seek(skip, io.SEEK_SET)\n        except OSError:\n            enc = None  # silently skip read errors\n        try:\n            # FIXME: https://github.com/ytdl-org/youtube-dl/commit/dfe5fa49aed02cf36ba9f743b11b0903554b5e56\n            contents = optionf.read().decode(enc or preferredencoding())\n            res = shlex.split(contents, comments=True)\n        except Exception as err:\n            raise ValueError(f'Unable to parse \"{filename}\": {err}')\n        finally:\n            optionf.close()\n        return res\n\n    @staticmethod\n    def hide_login_info(opts):\n        PRIVATE_OPTS = {'-p', '--password', '-u', '--username', '--video-password', '--ap-password', '--ap-username'}\n        eqre = re.compile('^(?P<key>' + ('|'.join(re.escape(po) for po in PRIVATE_OPTS)) + ')=.+$')\n\n        def _scrub_eq(o):\n            m = eqre.match(o)\n            if m:\n                return m.group('key') + '=PRIVATE'\n            else:\n                return o\n\n        opts = list(map(_scrub_eq, opts))\n        for idx, opt in enumerate(opts):\n            if opt in PRIVATE_OPTS and idx + 1 < len(opts):\n                opts[idx + 1] = 'PRIVATE'\n        return opts\n\n    def append_config(self, *args, label=None):\n        config = type(self)(self.parser, label)\n        config._loaded_paths = self._loaded_paths\n        if config.init(*args):\n            self.configs.append(config)\n\n    @property\n    def all_args(self):\n        for config in reversed(self.configs):\n            yield from config.all_args\n        yield from self.parsed_args or []\n\n    def parse_known_args(self, **kwargs):\n        return self.parser.parse_known_args(self.all_args, **kwargs)\n\n    def parse_args(self):\n        return self.parser.parse_args(self.all_args)\n\n\nclass WebSocketsWrapper:\n    \"\"\"Wraps websockets module to use in non-async scopes\"\"\"\n    pool = None\n\n    def __init__(self, url, headers=None, connect=True):\n        self.loop = asyncio.new_event_loop()\n        # XXX: \"loop\" is deprecated\n        self.conn = websockets.connect(\n            url, extra_headers=headers, ping_interval=None,\n            close_timeout=float('inf'), loop=self.loop, ping_timeout=float('inf'))\n        if connect:\n            self.__enter__()\n        atexit.register(self.__exit__, None, None, None)\n\n    def __enter__(self):\n        if not self.pool:\n            self.pool = self.run_with_loop(self.conn.__aenter__(), self.loop)\n        return self\n\n    def send(self, *args):\n        self.run_with_loop(self.pool.send(*args), self.loop)\n\n    def recv(self, *args):\n        return self.run_with_loop(self.pool.recv(*args), self.loop)\n\n    def __exit__(self, type, value, traceback):\n        try:\n            return self.run_with_loop(self.conn.__aexit__(type, value, traceback), self.loop)\n        finally:\n            self.loop.close()\n            self._cancel_all_tasks(self.loop)\n\n    # taken from https://github.com/python/cpython/blob/3.9/Lib/asyncio/runners.py with modifications\n    # for contributors: If there's any new library using asyncio needs to be run in non-async, move these function out of this class\n    @staticmethod\n    def run_with_loop(main, loop):\n        if not asyncio.iscoroutine(main):\n            raise ValueError(f'a coroutine was expected, got {main!r}')\n\n        try:\n            return loop.run_until_complete(main)\n        finally:\n            loop.run_until_complete(loop.shutdown_asyncgens())\n            if hasattr(loop, 'shutdown_default_executor'):\n                loop.run_until_complete(loop.shutdown_default_executor())\n\n    @staticmethod\n    def _cancel_all_tasks(loop):\n        to_cancel = asyncio.all_tasks(loop)\n\n        if not to_cancel:\n            return\n\n        for task in to_cancel:\n            task.cancel()\n\n        # XXX: \"loop\" is removed in python 3.10+\n        loop.run_until_complete(\n            asyncio.gather(*to_cancel, loop=loop, return_exceptions=True))\n\n        for task in to_cancel:\n            if task.cancelled():\n                continue\n            if task.exception() is not None:\n                loop.call_exception_handler({\n                    'message': 'unhandled exception during asyncio.run() shutdown',\n                    'exception': task.exception(),\n                    'task': task,\n                })\n\n\ndef merge_headers(*dicts):\n    \"\"\"Merge dicts of http headers case insensitively, prioritizing the latter ones\"\"\"\n    return {k.title(): v for k, v in itertools.chain.from_iterable(map(dict.items, dicts))}\n\n\ndef cached_method(f):\n    \"\"\"Cache a method\"\"\"\n    signature = inspect.signature(f)\n\n    @functools.wraps(f)\n    def wrapper(self, *args, **kwargs):\n        bound_args = signature.bind(self, *args, **kwargs)\n        bound_args.apply_defaults()\n        key = tuple(bound_args.arguments.values())[1:]\n\n        cache = vars(self).setdefault('_cached_method__cache', {}).setdefault(f.__name__, {})\n        if key not in cache:\n            cache[key] = f(self, *args, **kwargs)\n        return cache[key]\n    return wrapper\n\n\nclass classproperty:\n    \"\"\"property access for class methods with optional caching\"\"\"\n    def __new__(cls, func=None, *args, **kwargs):\n        if not func:\n            return functools.partial(cls, *args, **kwargs)\n        return super().__new__(cls)\n\n    def __init__(self, func, *, cache=False):\n        functools.update_wrapper(self, func)\n        self.func = func\n        self._cache = {} if cache else None\n\n    def __get__(self, _, cls):\n        if self._cache is None:\n            return self.func(cls)\n        elif cls not in self._cache:\n            self._cache[cls] = self.func(cls)\n        return self._cache[cls]\n\n\nclass function_with_repr:\n    def __init__(self, func, repr_=None):\n        functools.update_wrapper(self, func)\n        self.func, self.__repr = func, repr_\n\n    def __call__(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n    def __repr__(self):\n        if self.__repr:\n            return self.__repr\n        return f'{self.func.__module__}.{self.func.__qualname__}'\n\n\nclass Namespace(types.SimpleNamespace):\n    \"\"\"Immutable namespace\"\"\"\n\n    def __iter__(self):\n        return iter(self.__dict__.values())\n\n    @property\n    def items_(self):\n        return self.__dict__.items()\n\n\nMEDIA_EXTENSIONS = Namespace(\n    common_video=('avi', 'flv', 'mkv', 'mov', 'mp4', 'webm'),\n    video=('3g2', '3gp', 'f4v', 'mk3d', 'divx', 'mpg', 'ogv', 'm4v', 'wmv'),\n    common_audio=('aiff', 'alac', 'flac', 'm4a', 'mka', 'mp3', 'ogg', 'opus', 'wav'),\n    audio=('aac', 'ape', 'asf', 'f4a', 'f4b', 'm4b', 'm4p', 'm4r', 'oga', 'ogx', 'spx', 'vorbis', 'wma', 'weba'),\n    thumbnails=('jpg', 'png', 'webp'),\n    storyboards=('mhtml', ),\n    subtitles=('srt', 'vtt', 'ass', 'lrc'),\n    manifests=('f4f', 'f4m', 'm3u8', 'smil', 'mpd'),\n)\nMEDIA_EXTENSIONS.video += MEDIA_EXTENSIONS.common_video\nMEDIA_EXTENSIONS.audio += MEDIA_EXTENSIONS.common_audio\n\nKNOWN_EXTENSIONS = (*MEDIA_EXTENSIONS.video, *MEDIA_EXTENSIONS.audio, *MEDIA_EXTENSIONS.manifests)\n\n\nclass RetryManager:\n    \"\"\"Usage:\n        for retry in RetryManager(...):\n            try:\n                ...\n            except SomeException as err:\n                retry.error = err\n                continue\n    \"\"\"\n    attempt, _error = 0, None\n\n    def __init__(self, _retries, _error_callback, **kwargs):\n        self.retries = _retries or 0\n        self.error_callback = functools.partial(_error_callback, **kwargs)\n\n    def _should_retry(self):\n        return self._error is not NO_DEFAULT and self.attempt <= self.retries\n\n    @property\n    def error(self):\n        if self._error is NO_DEFAULT:\n            return None\n        return self._error\n\n    @error.setter\n    def error(self, value):\n        self._error = value\n\n    def __iter__(self):\n        while self._should_retry():\n            self.error = NO_DEFAULT\n            self.attempt += 1\n            yield self\n            if self.error:\n                self.error_callback(self.error, self.attempt, self.retries)\n\n    @staticmethod\n    def report_retry(e, count, retries, *, sleep_func, info, warn, error=None, suffix=None):\n        \"\"\"Utility function for reporting retries\"\"\"\n        if count > retries:\n            if error:\n                return error(f'{e}. Giving up after {count - 1} retries') if count > 1 else error(str(e))\n            raise e\n\n        if not count:\n            return warn(e)\n        elif isinstance(e, ExtractorError):\n            e = remove_end(str_or_none(e.cause) or e.orig_msg, '.')\n        warn(f'{e}. Retrying{format_field(suffix, None, \" %s\")} ({count}/{retries})...')\n\n        delay = float_or_none(sleep_func(n=count - 1)) if callable(sleep_func) else sleep_func\n        if delay:\n            info(f'Sleeping {delay:.2f} seconds ...')\n            time.sleep(delay)\n\n\ndef make_archive_id(ie, video_id):\n    ie_key = ie if isinstance(ie, str) else ie.ie_key()\n    return f'{ie_key.lower()} {video_id}'\n\n\ndef truncate_string(s, left, right=0):\n    assert left > 3 and right >= 0\n    if s is None or len(s) <= left + right:\n        return s\n    return f'{s[:left-3]}...{s[-right:] if right else \"\"}'\n\n\ndef orderedSet_from_options(options, alias_dict, *, use_regex=False, start=None):\n    assert 'all' in alias_dict, '\"all\" alias is required'\n    requested = list(start or [])\n    for val in options:\n        discard = val.startswith('-')\n        if discard:\n            val = val[1:]\n\n        if val in alias_dict:\n            val = alias_dict[val] if not discard else [\n                i[1:] if i.startswith('-') else f'-{i}' for i in alias_dict[val]]\n            # NB: Do not allow regex in aliases for performance\n            requested = orderedSet_from_options(val, alias_dict, start=requested)\n            continue\n\n        current = (filter(re.compile(val, re.I).fullmatch, alias_dict['all']) if use_regex\n                   else [val] if val in alias_dict['all'] else None)\n        if current is None:\n            raise ValueError(val)\n\n        if discard:\n            for item in current:\n                while item in requested:\n                    requested.remove(item)\n        else:\n            requested.extend(current)\n\n    return orderedSet(requested)\n\n\n# TODO: Rewrite\nclass FormatSorter:\n    regex = r' *((?P<reverse>\\+)?(?P<field>[a-zA-Z0-9_]+)((?P<separator>[~:])(?P<limit>.*?))?)? *$'\n\n    default = ('hidden', 'aud_or_vid', 'hasvid', 'ie_pref', 'lang', 'quality',\n               'res', 'fps', 'hdr:12', 'vcodec:vp9.2', 'channels', 'acodec',\n               'size', 'br', 'asr', 'proto', 'ext', 'hasaud', 'source', 'id')  # These must not be aliases\n    ytdl_default = ('hasaud', 'lang', 'quality', 'tbr', 'filesize', 'vbr',\n                    'height', 'width', 'proto', 'vext', 'abr', 'aext',\n                    'fps', 'fs_approx', 'source', 'id')\n\n    settings = {\n        'vcodec': {'type': 'ordered', 'regex': True,\n                   'order': ['av0?1', 'vp0?9.2', 'vp0?9', '[hx]265|he?vc?', '[hx]264|avc', 'vp0?8', 'mp4v|h263', 'theora', '', None, 'none']},\n        'acodec': {'type': 'ordered', 'regex': True,\n                   'order': ['[af]lac', 'wav|aiff', 'opus', 'vorbis|ogg', 'aac', 'mp?4a?', 'mp3', 'ac-?4', 'e-?a?c-?3', 'ac-?3', 'dts', '', None, 'none']},\n        'hdr': {'type': 'ordered', 'regex': True, 'field': 'dynamic_range',\n                'order': ['dv', '(hdr)?12', r'(hdr)?10\\+', '(hdr)?10', 'hlg', '', 'sdr', None]},\n        'proto': {'type': 'ordered', 'regex': True, 'field': 'protocol',\n                  'order': ['(ht|f)tps', '(ht|f)tp$', 'm3u8.*', '.*dash', 'websocket_frag', 'rtmpe?', '', 'mms|rtsp', 'ws|websocket', 'f4']},\n        'vext': {'type': 'ordered', 'field': 'video_ext',\n                 'order': ('mp4', 'mov', 'webm', 'flv', '', 'none'),\n                 'order_free': ('webm', 'mp4', 'mov', 'flv', '', 'none')},\n        'aext': {'type': 'ordered', 'regex': True, 'field': 'audio_ext',\n                 'order': ('m4a', 'aac', 'mp3', 'ogg', 'opus', 'web[am]', '', 'none'),\n                 'order_free': ('ogg', 'opus', 'web[am]', 'mp3', 'm4a', 'aac', '', 'none')},\n        'hidden': {'visible': False, 'forced': True, 'type': 'extractor', 'max': -1000},\n        'aud_or_vid': {'visible': False, 'forced': True, 'type': 'multiple',\n                       'field': ('vcodec', 'acodec'),\n                       'function': lambda it: int(any(v != 'none' for v in it))},\n        'ie_pref': {'priority': True, 'type': 'extractor'},\n        'hasvid': {'priority': True, 'field': 'vcodec', 'type': 'boolean', 'not_in_list': ('none',)},\n        'hasaud': {'field': 'acodec', 'type': 'boolean', 'not_in_list': ('none',)},\n        'lang': {'convert': 'float', 'field': 'language_preference', 'default': -1},\n        'quality': {'convert': 'float', 'default': -1},\n        'filesize': {'convert': 'bytes'},\n        'fs_approx': {'convert': 'bytes', 'field': 'filesize_approx'},\n        'id': {'convert': 'string', 'field': 'format_id'},\n        'height': {'convert': 'float_none'},\n        'width': {'convert': 'float_none'},\n        'fps': {'convert': 'float_none'},\n        'channels': {'convert': 'float_none', 'field': 'audio_channels'},\n        'tbr': {'convert': 'float_none'},\n        'vbr': {'convert': 'float_none'},\n        'abr': {'convert': 'float_none'},\n        'asr': {'convert': 'float_none'},\n        'source': {'convert': 'float', 'field': 'source_preference', 'default': -1},\n\n        'codec': {'type': 'combined', 'field': ('vcodec', 'acodec')},\n        'br': {'type': 'multiple', 'field': ('tbr', 'vbr', 'abr'), 'convert': 'float_none',\n               'function': lambda it: next(filter(None, it), None)},\n        'size': {'type': 'multiple', 'field': ('filesize', 'fs_approx'), 'convert': 'bytes',\n                 'function': lambda it: next(filter(None, it), None)},\n        'ext': {'type': 'combined', 'field': ('vext', 'aext')},\n        'res': {'type': 'multiple', 'field': ('height', 'width'),\n                'function': lambda it: (lambda l: min(l) if l else 0)(tuple(filter(None, it)))},\n\n        # Actual field names\n        'format_id': {'type': 'alias', 'field': 'id'},\n        'preference': {'type': 'alias', 'field': 'ie_pref'},\n        'language_preference': {'type': 'alias', 'field': 'lang'},\n        'source_preference': {'type': 'alias', 'field': 'source'},\n        'protocol': {'type': 'alias', 'field': 'proto'},\n        'filesize_approx': {'type': 'alias', 'field': 'fs_approx'},\n        'audio_channels': {'type': 'alias', 'field': 'channels'},\n\n        # Deprecated\n        'dimension': {'type': 'alias', 'field': 'res', 'deprecated': True},\n        'resolution': {'type': 'alias', 'field': 'res', 'deprecated': True},\n        'extension': {'type': 'alias', 'field': 'ext', 'deprecated': True},\n        'bitrate': {'type': 'alias', 'field': 'br', 'deprecated': True},\n        'total_bitrate': {'type': 'alias', 'field': 'tbr', 'deprecated': True},\n        'video_bitrate': {'type': 'alias', 'field': 'vbr', 'deprecated': True},\n        'audio_bitrate': {'type': 'alias', 'field': 'abr', 'deprecated': True},\n        'framerate': {'type': 'alias', 'field': 'fps', 'deprecated': True},\n        'filesize_estimate': {'type': 'alias', 'field': 'size', 'deprecated': True},\n        'samplerate': {'type': 'alias', 'field': 'asr', 'deprecated': True},\n        'video_ext': {'type': 'alias', 'field': 'vext', 'deprecated': True},\n        'audio_ext': {'type': 'alias', 'field': 'aext', 'deprecated': True},\n        'video_codec': {'type': 'alias', 'field': 'vcodec', 'deprecated': True},\n        'audio_codec': {'type': 'alias', 'field': 'acodec', 'deprecated': True},\n        'video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},\n        'has_video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},\n        'audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},\n        'has_audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},\n        'extractor': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},\n        'extractor_preference': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},\n    }\n\n    def __init__(self, ydl, field_preference):\n        self.ydl = ydl\n        self._order = []\n        self.evaluate_params(self.ydl.params, field_preference)\n        if ydl.params.get('verbose'):\n            self.print_verbose_info(self.ydl.write_debug)\n\n    def _get_field_setting(self, field, key):\n        if field not in self.settings:\n            if key in ('forced', 'priority'):\n                return False\n            self.ydl.deprecated_feature(f'Using arbitrary fields ({field}) for format sorting is '\n                                        'deprecated and may be removed in a future version')\n            self.settings[field] = {}\n        propObj = self.settings[field]\n        if key not in propObj:\n            type = propObj.get('type')\n            if key == 'field':\n                default = 'preference' if type == 'extractor' else (field,) if type in ('combined', 'multiple') else field\n            elif key == 'convert':\n                default = 'order' if type == 'ordered' else 'float_string' if field else 'ignore'\n            else:\n                default = {'type': 'field', 'visible': True, 'order': [], 'not_in_list': (None,)}.get(key, None)\n            propObj[key] = default\n        return propObj[key]\n\n    def _resolve_field_value(self, field, value, convertNone=False):\n        if value is None:\n            if not convertNone:\n                return None\n        else:\n            value = value.lower()\n        conversion = self._get_field_setting(field, 'convert')\n        if conversion == 'ignore':\n            return None\n        if conversion == 'string':\n            return value\n        elif conversion == 'float_none':\n            return float_or_none(value)\n        elif conversion == 'bytes':\n            return parse_bytes(value)\n        elif conversion == 'order':\n            order_list = (self._use_free_order and self._get_field_setting(field, 'order_free')) or self._get_field_setting(field, 'order')\n            use_regex = self._get_field_setting(field, 'regex')\n            list_length = len(order_list)\n            empty_pos = order_list.index('') if '' in order_list else list_length + 1\n            if use_regex and value is not None:\n                for i, regex in enumerate(order_list):\n                    if regex and re.match(regex, value):\n                        return list_length - i\n                return list_length - empty_pos  # not in list\n            else:  # not regex or  value = None\n                return list_length - (order_list.index(value) if value in order_list else empty_pos)\n        else:\n            if value.isnumeric():\n                return float(value)\n            else:\n                self.settings[field]['convert'] = 'string'\n                return value\n\n    def evaluate_params(self, params, sort_extractor):\n        self._use_free_order = params.get('prefer_free_formats', False)\n        self._sort_user = params.get('format_sort', [])\n        self._sort_extractor = sort_extractor\n\n        def add_item(field, reverse, closest, limit_text):\n            field = field.lower()\n            if field in self._order:\n                return\n            self._order.append(field)\n            limit = self._resolve_field_value(field, limit_text)\n            data = {\n                'reverse': reverse,\n                'closest': False if limit is None else closest,\n                'limit_text': limit_text,\n                'limit': limit}\n            if field in self.settings:\n                self.settings[field].update(data)\n            else:\n                self.settings[field] = data\n\n        sort_list = (\n            tuple(field for field in self.default if self._get_field_setting(field, 'forced'))\n            + (tuple() if params.get('format_sort_force', False)\n                else tuple(field for field in self.default if self._get_field_setting(field, 'priority')))\n            + tuple(self._sort_user) + tuple(sort_extractor) + self.default)\n\n        for item in sort_list:\n            match = re.match(self.regex, item)\n            if match is None:\n                raise ExtractorError('Invalid format sort string \"%s\" given by extractor' % item)\n            field = match.group('field')\n            if field is None:\n                continue\n            if self._get_field_setting(field, 'type') == 'alias':\n                alias, field = field, self._get_field_setting(field, 'field')\n                if self._get_field_setting(alias, 'deprecated'):\n                    self.ydl.deprecated_feature(f'Format sorting alias {alias} is deprecated and may '\n                                                f'be removed in a future version. Please use {field} instead')\n            reverse = match.group('reverse') is not None\n            closest = match.group('separator') == '~'\n            limit_text = match.group('limit')\n\n            has_limit = limit_text is not None\n            has_multiple_fields = self._get_field_setting(field, 'type') == 'combined'\n            has_multiple_limits = has_limit and has_multiple_fields and not self._get_field_setting(field, 'same_limit')\n\n            fields = self._get_field_setting(field, 'field') if has_multiple_fields else (field,)\n            limits = limit_text.split(':') if has_multiple_limits else (limit_text,) if has_limit else tuple()\n            limit_count = len(limits)\n            for (i, f) in enumerate(fields):\n                add_item(f, reverse, closest,\n                         limits[i] if i < limit_count\n                         else limits[0] if has_limit and not has_multiple_limits\n                         else None)\n\n    def print_verbose_info(self, write_debug):\n        if self._sort_user:\n            write_debug('Sort order given by user: %s' % ', '.join(self._sort_user))\n        if self._sort_extractor:\n            write_debug('Sort order given by extractor: %s' % ', '.join(self._sort_extractor))\n        write_debug('Formats sorted by: %s' % ', '.join(['%s%s%s' % (\n            '+' if self._get_field_setting(field, 'reverse') else '', field,\n            '%s%s(%s)' % ('~' if self._get_field_setting(field, 'closest') else ':',\n                          self._get_field_setting(field, 'limit_text'),\n                          self._get_field_setting(field, 'limit'))\n            if self._get_field_setting(field, 'limit_text') is not None else '')\n            for field in self._order if self._get_field_setting(field, 'visible')]))\n\n    def _calculate_field_preference_from_value(self, format, field, type, value):\n        reverse = self._get_field_setting(field, 'reverse')\n        closest = self._get_field_setting(field, 'closest')\n        limit = self._get_field_setting(field, 'limit')\n\n        if type == 'extractor':\n            maximum = self._get_field_setting(field, 'max')\n            if value is None or (maximum is not None and value >= maximum):\n                value = -1\n        elif type == 'boolean':\n            in_list = self._get_field_setting(field, 'in_list')\n            not_in_list = self._get_field_setting(field, 'not_in_list')\n            value = 0 if ((in_list is None or value in in_list) and (not_in_list is None or value not in not_in_list)) else -1\n        elif type == 'ordered':\n            value = self._resolve_field_value(field, value, True)\n\n        # try to convert to number\n        val_num = float_or_none(value, default=self._get_field_setting(field, 'default'))\n        is_num = self._get_field_setting(field, 'convert') != 'string' and val_num is not None\n        if is_num:\n            value = val_num\n\n        return ((-10, 0) if value is None\n                else (1, value, 0) if not is_num  # if a field has mixed strings and numbers, strings are sorted higher\n                else (0, -abs(value - limit), value - limit if reverse else limit - value) if closest\n                else (0, value, 0) if not reverse and (limit is None or value <= limit)\n                else (0, -value, 0) if limit is None or (reverse and value == limit) or value > limit\n                else (-1, value, 0))\n\n    def _calculate_field_preference(self, format, field):\n        type = self._get_field_setting(field, 'type')  # extractor, boolean, ordered, field, multiple\n        get_value = lambda f: format.get(self._get_field_setting(f, 'field'))\n        if type == 'multiple':\n            type = 'field'  # Only 'field' is allowed in multiple for now\n            actual_fields = self._get_field_setting(field, 'field')\n\n            value = self._get_field_setting(field, 'function')(get_value(f) for f in actual_fields)\n        else:\n            value = get_value(field)\n        return self._calculate_field_preference_from_value(format, field, type, value)\n\n    def calculate_preference(self, format):\n        # Determine missing protocol\n        if not format.get('protocol'):\n            format['protocol'] = determine_protocol(format)\n\n        # Determine missing ext\n        if not format.get('ext') and 'url' in format:\n            format['ext'] = determine_ext(format['url'])\n        if format.get('vcodec') == 'none':\n            format['audio_ext'] = format['ext'] if format.get('acodec') != 'none' else 'none'\n            format['video_ext'] = 'none'\n        else:\n            format['video_ext'] = format['ext']\n            format['audio_ext'] = 'none'\n        # if format.get('preference') is None and format.get('ext') in ('f4f', 'f4m'):  # Not supported?\n        #    format['preference'] = -1000\n\n        if format.get('preference') is None and format.get('ext') == 'flv' and re.match('[hx]265|he?vc?', format.get('vcodec') or ''):\n            # HEVC-over-FLV is out-of-spec by FLV's original spec\n            # ref. https://trac.ffmpeg.org/ticket/6389\n            # ref. https://github.com/yt-dlp/yt-dlp/pull/5821\n            format['preference'] = -100\n\n        # Determine missing bitrates\n        if format.get('vcodec') == 'none':\n            format['vbr'] = 0\n        if format.get('acodec') == 'none':\n            format['abr'] = 0\n        if not format.get('vbr') and format.get('vcodec') != 'none':\n            format['vbr'] = try_call(lambda: format['tbr'] - format['abr']) or None\n        if not format.get('abr') and format.get('acodec') != 'none':\n            format['abr'] = try_call(lambda: format['tbr'] - format['vbr']) or None\n        if not format.get('tbr'):\n            format['tbr'] = try_call(lambda: format['vbr'] + format['abr']) or None\n\n        return tuple(self._calculate_field_preference(format, field) for field in self._order)\n", "patch": "@@ -1556,7 +1556,12 @@ def redirect_request(self, req, fp, code, msg, headers, newurl):\n \n         new_method = req.get_method()\n         new_data = req.data\n-        remove_headers = []\n+\n+        # Technically the Cookie header should be in unredirected_hdrs,\n+        # however in practice some may set it in normal headers anyway.\n+        # We will remove it here to prevent any leaks.\n+        remove_headers = ['Cookie']\n+\n         # A 303 must either use GET or HEAD for subsequent request\n         # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.4\n         if code == 303 and req.get_method() != 'HEAD':\n@@ -1573,7 +1578,7 @@ def redirect_request(self, req, fp, code, msg, headers, newurl):\n             new_data = None\n             remove_headers.extend(['Content-Length', 'Content-Type'])\n \n-        new_headers = {k: v for k, v in req.headers.items() if k.lower() not in remove_headers}\n+        new_headers = {k: v for k, v in req.headers.items() if k.title() not in remove_headers}\n \n         return urllib.request.Request(\n             newurl, headers=new_headers, origin_req_host=req.origin_req_host,", "file_path": "files/2023_7/630", "file_language": "py", "file_name": "yt_dlp/utils/_utils.py", "outdated_file_modify": 1, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
