{"index": 5415, "cve_id": "CVE-2021-21273", "cwe_id": ["CWE-601"], "cve_language": "Python", "cve_description": "Synapse is a Matrix reference homeserver written in python (pypi package matrix-synapse). Matrix is an ecosystem for open federated Instant Messaging and VoIP. In Synapse before version 1.25.0, requests to user provided domains were not restricted to external IP addresses when calculating the key validity for third-party invite events and sending push notifications. This could cause Synapse to make requests to internal infrastructure. The type of request was not controlled by the user, although limited modification of request bodies was possible. For the most thorough protection server administrators should remove the deprecated `federation_ip_range_blacklist` from their settings after upgrading to Synapse v1.25.0 which will result in Synapse using the improved default IP address restrictions. See the new `ip_range_blacklist` and `ip_range_whitelist` settings if more specific control is necessary.", "cvss": "6.1", "publish_date": "February 26, 2021", "AV": "NETWORK", "AC": "NETWORK", "PR": "NONE", "UI": "REQUIRED", "S": "CHANGED", "C": "LOW", "I": "LOW", "A": "NONE", "commit_id": "30fba6210834a4ecd91badf0c8f3eb278b72e746", "commit_message": "Apply an IP range blacklist to push and key revocation requests. (#8821)\n\nReplaces the `federation_ip_range_blacklist` configuration setting with an\r\n`ip_range_blacklist` setting with wider scope. It now applies to:\r\n\r\n* Federation\r\n* Identity servers\r\n* Push notifications\r\n* Checking key validitity for third-party invite events\r\n\r\nThe old `federation_ip_range_blacklist` setting is still honored if present, but\r\nwith reduced scope (it only applies to federation and identity servers).", "commit_date": "2020-12-02T16:09:24Z", "project": "matrix-org/synapse", "url": "https://api.github.com/repos/matrix-org/synapse/commits/30fba6210834a4ecd91badf0c8f3eb278b72e746", "html_url": "https://github.com/matrix-org/synapse/commit/30fba6210834a4ecd91badf0c8f3eb278b72e746", "windows_before": [{"commit_id": "c5b6abd53d1549c7a7cc30c644dd8921bfc10ea2", "commit_date": "Wed Dec 2 15:22:37 2020 +0000", "commit_message": "Correctly handle unpersisted events when calculating auth chain difference. (#8827)", "files_name": ["changelog.d/8827.bugfix", "synapse/state/v2.py", "tests/state/test_v2.py", "tests/storage/test_event_federation.py"]}, {"commit_id": "693516e7566fdde677f5d836d45c44fbd8722ba9", "commit_date": "Wed Dec 2 15:21:00 2020 +0000", "commit_message": "Add `create_resource_dict` method to HomeserverTestCase", "files_name": ["tests/unittest.py"]}, {"commit_id": "0fed46ebe5abc524f10708ce1d5849e53dbab8af", "commit_date": "Wed Dec 2 16:18:41 2020 +0100", "commit_message": "Add missing prometheus rules for persisted events (#8802)", "files_name": ["changelog.d/8802.doc", "contrib/prometheus/synapse-v2.rules"]}, {"commit_id": "c4675e1b24f06a72c323c8131eab4998b4e71af1", "commit_date": "Wed Dec 2 10:01:15 2020 -0500", "commit_message": "Add additional validation for the admin register endpoint. (#8837)", "files_name": ["changelog.d/8837.bugfix", "synapse/rest/admin/users.py"]}, {"commit_id": "e41720d85f792b4bdc1ffb430b0a04967ad813c2", "commit_date": "Wed Dec 2 09:17:42 2020 -0500", "commit_message": "Minor changes to the CHANGES doc.", "files_name": ["CHANGES.md"]}, {"commit_id": "c67af840aa015647f6e237935e02eb163151aa5c", "commit_date": "Wed Dec 2 09:03:12 2020 -0500", "commit_message": "Minor fixes to changelog.", "files_name": ["CHANGES.md"]}, {"commit_id": "53b12688dd2a9d7d8076122d48f088c5c3e9939e", "commit_date": "Wed Dec 2 08:57:51 2020 -0500", "commit_message": "1.24.0rc1", "files_name": ["CHANGES.md", "changelog.d/8565.misc", "changelog.d/8617.feature", "changelog.d/8630.feature", "changelog.d/8731.misc", "changelog.d/8734.doc", "changelog.d/8744.bugfix", "changelog.d/8751.misc", "changelog.d/8754.misc", "changelog.d/8757.misc", "changelog.d/8758.misc", "changelog.d/8759.misc", "changelog.d/8760.misc", "changelog.d/8761.misc", "changelog.d/8765.misc", "changelog.d/8770.misc", "changelog.d/8771.doc", "changelog.d/8772.misc", "changelog.d/8773.misc", "changelog.d/8774.misc", "changelog.d/8776.bugfix", "changelog.d/8777.misc", "changelog.d/8779.doc", "changelog.d/8784.misc", "changelog.d/8785.removal", "changelog.d/8793.doc", "changelog.d/8795.doc", "changelog.d/8798.bugfix", "changelog.d/8799.bugfix", "changelog.d/8800.misc", "changelog.d/8801.feature", "changelog.d/8804.feature", "changelog.d/8806.misc", "changelog.d/8809.misc", "changelog.d/8812.misc", "changelog.d/8815.misc", "changelog.d/8817.bugfix", "changelog.d/8818.doc", "changelog.d/8819.misc", "changelog.d/8820.feature", "changelog.d/8822.doc", "changelog.d/8823.bugfix", "changelog.d/8824.doc", "changelog.d/8833.removal", "changelog.d/8835.bugfix", "changelog.d/8843.feature", "changelog.d/8845.misc", "changelog.d/8847.misc", "changelog.d/8848.bugfix", "changelog.d/8849.misc", "changelog.d/8850.misc", "changelog.d/8851.misc", "changelog.d/8854.misc", "changelog.d/8855.feature", "synapse/__init__.py"]}, {"commit_id": "8388384a640d3381b5858d3fb1d2ea0a8c9c059c", "commit_date": "Wed Dec 2 07:45:42 2020 -0500", "commit_message": "Fix a regression when grandfathering SAML users. (#8855)", "files_name": ["changelog.d/8855.feature", "synapse/handlers/oidc_handler.py", "synapse/handlers/saml_handler.py", "synapse/handlers/sso.py", "tests/handlers/test_oidc.py", "tests/handlers/test_saml.py"]}, {"commit_id": "c21bdc813f5c21153cded05bcd0a57b5836f09fe", "commit_date": "Wed Dec 2 07:09:21 2020 -0500", "commit_message": "Add basic SAML tests for mapping users. (#8800)", "files_name": [".buildkite/scripts/test_old_deps.sh", "changelog.d/8800.misc", "synapse/handlers/saml_handler.py", "tests/handlers/test_oidc.py", "tests/handlers/test_saml.py"]}, {"commit_id": "d3ed93504bb6bb8ad138e356e3c74b6a7286299b", "commit_date": "Wed Dec 2 10:38:50 2020 +0000", "commit_message": "Create a `PasswordProvider` wrapper object (#8849)", "files_name": ["changelog.d/8849.misc", "synapse/handlers/auth.py", "tests/handlers/test_password_providers.py"]}, {"commit_id": "edb3d3f82716c2b5c903ddb4d0df155e06c5c9e9", "commit_date": "Wed Dec 2 10:38:18 2020 +0000", "commit_message": "Allow specifying room version in 'RestHelper.create_room_as' and add typing (#8854)", "files_name": ["changelog.d/8854.misc", "tests/rest/client/v1/utils.py"]}, {"commit_id": "4d9496559d25ba36eaea45d73e67e79b9d936450", "commit_date": "Tue Dec 1 17:42:26 2020 +0000", "commit_message": "Support \"identifier\" dicts in UIA (#8848)", "files_name": ["changelog.d/8848.bugfix", "synapse/handlers/auth.py", "synapse/rest/client/v1/login.py", "tests/handlers/test_password_providers.py", "tests/rest/client/v2_alpha/test_auth.py"]}, {"commit_id": "9edff901d1eaca3c72ab4a0b31ff14d1472c6331", "commit_date": "Tue Dec 1 15:52:49 2020 +0000", "commit_message": "Add missing `ordering` to background updates (#8850)", "files_name": ["changelog.d/8850.misc", "synapse/storage/databases/main/schema/delta/58/07add_method_to_thumbnail_constraint.sql.postgres", "synapse/storage/databases/main/schema/delta/58/12room_stats.sql", "synapse/storage/databases/main/schema/delta/58/22users_have_local_media.sql", "synapse/storage/databases/main/schema/delta/58/23e2e_cross_signing_keys_idx.sql"]}, {"commit_id": "3f0cba657ceb6e5286f52b06a74e969e4af9a146", "commit_date": "Tue Dec 1 10:24:56 2020 -0300", "commit_message": "Allow Date header through CORS (#8804)", "files_name": ["changelog.d/8804.feature", "synapse/http/server.py"]}, {"commit_id": "89f79307306ed117d9dcfe46a31a3fe1a1a5ceae", "commit_date": "Tue Dec 1 13:04:03 2020 +0000", "commit_message": "Don't offer password login when it is disabled (#8835)", "files_name": ["changelog.d/8835.bugfix", "synapse/handlers/auth.py", "tests/handlers/test_password_providers.py"]}, {"commit_id": "ddc43436838e19a7dd16860389bd76c74578dae7", "commit_date": "Tue Dec 1 11:10:42 2020 +0000", "commit_message": "Add some tests for `password_auth_providers` (#8819)", "files_name": ["changelog.d/8819.misc", "mypy.ini", "tests/handlers/test_password_providers.py"]}, {"commit_id": "09ac0569fe910ce4599a07e385522380ce1af6e1", "commit_date": "Tue Dec 1 11:04:57 2020 +0000", "commit_message": "Fix broken testcase (#8851)", "files_name": ["changelog.d/8851.misc", "tests/rest/admin/test_media.py"]}, {"commit_id": "d1be293f0002555016a2edd630272dbe09355347", "commit_date": "Tue Dec 1 10:34:52 2020 +0000", "commit_message": "Fix typo in password_auth_providers doc", "files_name": ["docs/password_auth_providers.md"]}, {"commit_id": "59e18a1333526b922b318c4165ec09570e80bf5c", "commit_date": "Mon Nov 30 19:20:56 2020 +0000", "commit_message": "Simplify appservice login code (#8847)", "files_name": ["changelog.d/8847.misc", "synapse/rest/client/v1/login.py"]}, {"commit_id": "9f0f274fe0bd9dd6ccd7e2b53e2536cb6d02ae9b", "commit_date": "Mon Nov 30 19:59:29 2020 +0100", "commit_message": "Allow per-room profile to be used for server notice user (#8799)", "files_name": ["changelog.d/8799.bugfix", "synapse/handlers/room_member.py"]}, {"commit_id": "f8d13ca13d9dd0c669a2a1b5bc390d7830c89239", "commit_date": "Mon Nov 30 18:44:09 2020 +0000", "commit_message": "Drop (almost) unused index on event_json (#8845)", "files_name": ["changelog.d/8845.misc", "synapse/storage/databases/main/purge_events.py", "synapse/storage/databases/main/schema/delta/58/24drop_event_json_index.sql"]}, {"commit_id": "17fa58bdd1c23b9019d080fd98873aa5182f56c0", "commit_date": "Mon Nov 30 18:43:54 2020 +0000", "commit_message": "Add a config option to change whether unread push notification counts are per-message or per-room (#8820)", "files_name": ["changelog.d/8820.feature", "docs/sample_config.yaml"]}], "windows_after": [{"commit_id": "7ea85302f3ce59cdb38bceb1c2aeea2d690e2dbb", "commit_date": "Wed Dec 2 15:26:25 2020 +0000", "commit_message": "fix up various test cases", "files_name": ["synapse/federation/transport/server.py", "tests/handlers/test_typing.py", "tests/replication/_base.py", "tests/server.py", "tests/unittest.py"]}, {"commit_id": "90cf1eec44940f8ede4a7f0490da43021c84a13a", "commit_date": "Wed Dec 2 15:12:02 2020 +0000", "commit_message": "Remove redundant mocking", "files_name": ["tests/api/test_filtering.py", "tests/handlers/test_directory.py", "tests/handlers/test_profile.py", "tests/storage/test_redaction.py", "tests/storage/test_roommember.py"]}, {"commit_id": "76469898ee797db232adaccb9fd547bddab2fe59", "commit_date": "Wed Dec 2 18:22:01 2020 +0000", "commit_message": "Factor out FakeResponse from test_oidc", "files_name": ["tests/handlers/test_oidc.py", "tests/test_utils/__init__.py"]}, {"commit_id": "c834f1d67a7feeaebc353d0170f99a618bf32b5b", "commit_date": "Wed Dec 2 17:40:31 2020 +0000", "commit_message": "remove unused `resource_for_federation`", "files_name": ["tests/handlers/test_typing.py", "tests/utils.py"]}, {"commit_id": "b751624ff8cd110ebfd312f13b330c9ac7cd0d86", "commit_date": "Wed Dec 2 15:13:05 2020 +0000", "commit_message": "remove unused DeferredMockCallable", "files_name": ["tests/utils.py"]}, {"commit_id": "92ce4a52582e9561a2af2c36fe8d8eb07e94654d", "commit_date": "Wed Dec 2 15:14:35 2020 +0000", "commit_message": "changelog", "files_name": ["changelog.d/8861.misc"]}, {"commit_id": "0bac276890567ef3a3fafd7f5b7b5cac91a1031b", "commit_date": "Tue Dec 1 00:15:36 2020 +0000", "commit_message": "UIA: offer only available auth flows", "files_name": ["synapse/handlers/auth.py", "synapse/storage/databases/main/registration.py", "synapse/storage/databases/main/schema/delta/58/25user_external_ids_user_id_idx.sql", "tests/rest/client/v1/utils.py", "tests/rest/client/v2_alpha/test_auth.py", "tests/server.py"]}, {"commit_id": "935732768c7ffd508c0b401287c79bf605d6b5a3", "commit_date": "Wed Dec 2 17:14:07 2020 +0000", "commit_message": "newsfile", "files_name": ["changelog.d/8858.bugfix"]}, {"commit_id": "f347f0cd581bb2dd8322a4e97c75d6b25d79db8a", "commit_date": "Wed Dec 2 18:58:25 2020 +0000", "commit_message": "remove unused FakeResponse (#8864)", "files_name": ["changelog.d/8864.misc", "tests/rest/media/v1/test_url_preview.py"]}, {"commit_id": "ed5172852ae79dec341a81feeb1b8b99bb1875d6", "commit_date": "Wed Dec 2 20:06:53 2020 +0000", "commit_message": "Merge pull request #8858 from matrix-org/rav/sso_uia", "files_name": ["269ba1bc847b1df27e5ebba3d8cd1954a57561e4 - Wed Dec 2 20:08:46 2020 +0000 : Merge remote-tracking branch 'origin/develop' into rav/remove_unused_mocks", "66f75c5b74196c624af6a39d824c93b4a37d63c2 - Thu Dec 3 10:02:47 2020 +0000 : Merge pull request #8861 from matrix-org/rav/remove_unused_mocks", "cf3b8156bec7d137894ebc3f6998c3c81a3854aa - Thu Dec 3 15:41:19 2020 +0000 : Fix errorcode for disabled registration (#8867)", "changelog.d/8867.bugfix", "synapse/rest/client/v2_alpha/register.py", "tests/rest/client/v2_alpha/test_register.py"]}, {"commit_id": "6e4f71c0574adf9bb314982713c38b1c0064d293", "commit_date": "Fri Dec 4 10:14:15 2020 +0000", "commit_message": "Fix a buglet in the SAML username mapping provider doc (#8873)", "files_name": ["changelog.d/8873.doc", "docs/sso_mapping_providers.md"]}, {"commit_id": "295c209cdd9364a5f277470da66d06a3d4133ad7", "commit_date": "Fri Dec 4 08:01:06 2020 -0500", "commit_message": "Remove version pin prometheus_client dependency (#8875)", "files_name": ["changelog.d/8875.misc", "docker/Dockerfile", "synapse/python_dependencies.py"]}, {"commit_id": "22c6c19f91d7325c82eddfada696826adad69e5b", "commit_date": "Fri Dec 4 08:25:15 2020 -0500", "commit_message": "Fix a regression that mapping providers should be able to redirect users. (#8878)", "files_name": ["changelog.d/8878.bugfix", "docs/sso_mapping_providers.md", "synapse/handlers/oidc_handler.py", "synapse/handlers/sso.py", "tests/handlers/test_oidc.py", "tests/handlers/test_saml.py"]}, {"commit_id": "693dab487c8de75ca1e7573474a3d4429ce8b313", "commit_date": "Fri Dec 4 08:48:04 2020 -0500", "commit_message": "1.24.0rc2", "files_name": ["CHANGES.md", "changelog.d/8875.misc", "changelog.d/8878.bugfix", "synapse/__init__.py"]}, {"commit_id": "2602514f34aab76934d27791400d7405c1da6336", "commit_date": "Fri Dec 4 09:00:32 2020 -0500", "commit_message": "Minor update to CHANGES.", "files_name": ["CHANGES.md"]}, {"commit_id": "112f6bd49e54732e88523533f3b2d4b271be54e1", "commit_date": "Fri Dec 4 09:14:31 2020 -0500", "commit_message": "Merge tag 'v1.24.0rc2' into develop", "files_name": ["df3e6a23a74e85005881bd5bead3443f4f758095 - Fri Dec 4 10:26:09 2020 -0500 : Do not 500 if the content-length is not provided when uploading media. (#8862)", "changelog.d/8862.bugfix", "synapse/rest/media/v1/upload_resource.py"]}, {"commit_id": "b774c555d821170e4f16de7d48f01484c3a1d740", "commit_date": "Fri Dec 4 10:51:56 2020 -0500", "commit_message": "Add additional validation to pusher URLs. (#8865)", "files_name": ["changelog.d/8865.bugfix", "synapse/push/__init__.py", "synapse/push/httppusher.py", "tests/push/test_http.py", "tests/replication/test_pusher_shard.py", "tests/rest/admin/test_user.py"]}, {"commit_id": "df4b1e9c74d56d79c274149b0dfb0fd5305c7659", "commit_date": "Fri Dec 4 15:52:49 2020 +0000", "commit_message": "Pass room_id to get_auth_chain_difference (#8879)", "files_name": ["changelog.d/8879.misc", "synapse/state/__init__.py", "synapse/state/v2.py", "synapse/storage/databases/main/event_federation.py", "tests/state/test_v2.py", "tests/storage/test_event_federation.py"]}, {"commit_id": "96358cb42410a4be6268eaa3ffec229c550208ea", "commit_date": "Fri Dec 4 10:56:28 2020 -0500", "commit_message": "Add authentication to replication endpoints. (#8853)", "files_name": ["changelog.d/8853.feature", "docs/sample_config.yaml", "docs/workers.md", "synapse/config/workers.py", "synapse/replication/http/_base.py", "tests/replication/test_auth.py", "tests/replication/test_client_reader_shard.py"]}, {"commit_id": "02e588856ae26865cd407dd6302aa3deecffe198", "commit_date": "Mon Dec 7 07:10:22 2020 -0500", "commit_message": "Add type hints to the push mailer module. (#8882)", "files_name": ["changelog.d/8882.misc", "mypy.ini", "synapse/push/mailer.py"]}, {"commit_id": "92d87c68824c17e02a8d9e7ca4f9c44b78426cfb", "commit_date": "Mon Dec 7 09:59:38 2020 -0500", "commit_message": "Add type hints for HTTP and email pushers. (#8880)", "files_name": ["changelog.d/8880.misc", "mypy.ini", "synapse/push/__init__.py", "synapse/push/emailpusher.py", "synapse/push/httppusher.py", "synapse/push/push_tools.py", "synapse/push/pusher.py", "synapse/push/pusherpool.py"]}, {"commit_id": "1f3748f03398f8f91ec5121312aa79dd58306ec1", "commit_date": "Mon Dec 7 10:00:08 2020 -0500", "commit_message": "Do not raise a 500 exception when previewing empty media. (#8883)", "files_name": ["changelog.d/8883.bugfix", "synapse/rest/media/v1/preview_url_resource.py", "tests/test_preview.py"]}, {"commit_id": "ff1f0ee09472b554832fb39952f389d01a4233ac", "commit_date": "Mon Dec 7 19:13:07 2020 +0000", "commit_message": "Call set_avatar_url with target_user, not user_id (#8872)", "files_name": ["changelog.d/8872.bugfix", "synapse/rest/admin/users.py", "tests/rest/admin/test_user.py"]}, {"commit_id": "025fa06fc743bda7c4769b19991c40a1fb5d12ba", "commit_date": "Tue Dec 8 14:03:08 2020 +0000", "commit_message": "Clarify config template comments (#8891)", "files_name": ["changelog.d/8891.doc", "docs/sample_config.yaml", "synapse/config/emailconfig.py", "synapse/config/sso.py"]}, {"commit_id": "36ba73f53d9919c7639d4c7269fabdb1857fb7a1", "commit_date": "Tue Dec 8 14:03:38 2020 +0000", "commit_message": "Simplify the flow for SSO UIA (#8881)", "files_name": ["changelog.d/8881.misc", "mypy.ini", "synapse/handlers/_base.py", "synapse/handlers/auth.py", "synapse/handlers/oidc_handler.py", "synapse/handlers/saml_handler.py", "synapse/handlers/sso.py"]}, {"commit_id": "ab7a24cc6bbffa5ba67b42731c45b1d4d33f3ae3", "commit_date": "Tue Dec 8 14:04:35 2020 +0000", "commit_message": "Better formatting for config errors from modules (#8874)", "files_name": ["changelog.d/8874.feature", "synapse/app/homeserver.py", "synapse/config/_base.py", "synapse/config/_base.pyi", "synapse/config/_util.py", "synapse/config/oidc_config.py", "synapse/config/password_auth_providers.py"]}], "parents": [{"commit_id_before": "c5b6abd53d1549c7a7cc30c644dd8921bfc10ea2", "url_before": "https://api.github.com/repos/matrix-org/synapse/commits/c5b6abd53d1549c7a7cc30c644dd8921bfc10ea2", "html_url_before": "https://github.com/matrix-org/synapse/commit/c5b6abd53d1549c7a7cc30c644dd8921bfc10ea2"}], "details": [{"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/changelog.d%2F8821.bugfix", "code": "Apply the `federation_ip_range_blacklist` to push and key revocation requests.\n", "code_before": "", "patch": "@@ -0,0 +1 @@\n+Apply the `federation_ip_range_blacklist` to push and key revocation requests.", "file_path": "files/2021_2/15", "file_language": "bugfix", "file_name": "changelog.d/8821.bugfix", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/docs%2Fsample_config.yaml", "code": "# This file is maintained as an up-to-date snapshot of the default\n# homeserver.yaml configuration generated by Synapse.\n#\n# It is intended to act as a reference for the default configuration,\n# helping admins keep track of new options and other changes, and compare\n# their configs with the current default.  As such, many of the actual\n# config values shown are placeholders.\n#\n# It is *not* intended to be copied and used as the basis for a real\n# homeserver.yaml. Instead, if you are starting from scratch, please generate\n# a fresh config using Synapse by following the instructions in INSTALL.md.\n\n# Configuration options that take a time period can be set using a number\n# followed by a letter. Letters have the following meanings:\n# s = second\n# m = minute\n# h = hour\n# d = day\n# w = week\n# y = year\n# For example, setting redaction_retention_period: 5m would remove redacted\n# messages from the database after 5 minutes, rather than 5 months.\n\n################################################################################\n\n# Configuration file for Synapse.\n#\n# This is a YAML file: see [1] for a quick introduction. Note in particular\n# that *indentation is important*: all the elements of a list or dictionary\n# should have the same indentation.\n#\n# [1] https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html\n\n## Server ##\n\n# The public-facing domain of the server\n#\n# The server_name name will appear at the end of usernames and room addresses\n# created on this server. For example if the server_name was example.com,\n# usernames on this server would be in the format @user:example.com\n#\n# In most cases you should avoid using a matrix specific subdomain such as\n# matrix.example.com or synapse.example.com as the server_name for the same\n# reasons you wouldn't use user@email.example.com as your email address.\n# See https://github.com/matrix-org/synapse/blob/master/docs/delegate.md\n# for information on how to host Synapse on a subdomain while preserving\n# a clean server_name.\n#\n# The server_name cannot be changed later so it is important to\n# configure this correctly before you start Synapse. It should be all\n# lowercase and may contain an explicit port.\n# Examples: matrix.org, localhost:8080\n#\nserver_name: \"SERVERNAME\"\n\n# When running as a daemon, the file to store the pid in\n#\npid_file: DATADIR/homeserver.pid\n\n# The absolute URL to the web client which /_matrix/client will redirect\n# to if 'webclient' is configured under the 'listeners' configuration.\n#\n# This option can be also set to the filesystem path to the web client\n# which will be served at /_matrix/client/ if 'webclient' is configured\n# under the 'listeners' configuration, however this is a security risk:\n# https://github.com/matrix-org/synapse#security-note\n#\n#web_client_location: https://riot.example.com/\n\n# The public-facing base URL that clients use to access this HS\n# (not including _matrix/...). This is the same URL a user would\n# enter into the 'custom HS URL' field on their client. If you\n# use synapse with a reverse proxy, this should be the URL to reach\n# synapse via the proxy.\n#\n#public_baseurl: https://example.com/\n\n# Set the soft limit on the number of file descriptors synapse can use\n# Zero is used to indicate synapse should set the soft limit to the\n# hard limit.\n#\n#soft_file_limit: 0\n\n# Set to false to disable presence tracking on this homeserver.\n#\n#use_presence: false\n\n# Whether to require authentication to retrieve profile data (avatars,\n# display names) of other users through the client API. Defaults to\n# 'false'. Note that profile data is also available via the federation\n# API, so this setting is of limited value if federation is enabled on\n# the server.\n#\n#require_auth_for_profile_requests: true\n\n# Uncomment to require a user to share a room with another user in order\n# to retrieve their profile information. Only checked on Client-Server\n# requests. Profile requests from other servers should be checked by the\n# requesting server. Defaults to 'false'.\n#\n#limit_profile_requests_to_users_who_share_rooms: true\n\n# If set to 'true', removes the need for authentication to access the server's\n# public rooms directory through the client API, meaning that anyone can\n# query the room directory. Defaults to 'false'.\n#\n#allow_public_rooms_without_auth: true\n\n# If set to 'true', allows any other homeserver to fetch the server's public\n# rooms directory via federation. Defaults to 'false'.\n#\n#allow_public_rooms_over_federation: true\n\n# The default room version for newly created rooms.\n#\n# Known room versions are listed here:\n# https://matrix.org/docs/spec/#complete-list-of-room-versions\n#\n# For example, for room version 1, default_room_version should be set\n# to \"1\".\n#\n#default_room_version: \"6\"\n\n# The GC threshold parameters to pass to `gc.set_threshold`, if defined\n#\n#gc_thresholds: [700, 10, 10]\n\n# Set the limit on the returned events in the timeline in the get\n# and sync operations. The default value is 100. -1 means no upper limit.\n#\n# Uncomment the following to increase the limit to 5000.\n#\n#filter_timeline_limit: 5000\n\n# Whether room invites to users on this server should be blocked\n# (except those sent by local server admins). The default is False.\n#\n#block_non_admin_invites: true\n\n# Room searching\n#\n# If disabled, new messages will not be indexed for searching and users\n# will receive errors when searching for messages. Defaults to enabled.\n#\n#enable_search: false\n\n# List of ports that Synapse should listen on, their purpose and their\n# configuration.\n#\n# Options for each listener include:\n#\n#   port: the TCP port to bind to\n#\n#   bind_addresses: a list of local addresses to listen on. The default is\n#       'all local interfaces'.\n#\n#   type: the type of listener. Normally 'http', but other valid options are:\n#       'manhole' (see docs/manhole.md),\n#       'metrics' (see docs/metrics-howto.md),\n#       'replication' (see docs/workers.md).\n#\n#   tls: set to true to enable TLS for this listener. Will use the TLS\n#       key/cert specified in tls_private_key_path / tls_certificate_path.\n#\n#   x_forwarded: Only valid for an 'http' listener. Set to true to use the\n#       X-Forwarded-For header as the client IP. Useful when Synapse is\n#       behind a reverse-proxy.\n#\n#   resources: Only valid for an 'http' listener. A list of resources to host\n#       on this port. Options for each resource are:\n#\n#       names: a list of names of HTTP resources. See below for a list of\n#           valid resource names.\n#\n#       compress: set to true to enable HTTP compression for this resource.\n#\n#   additional_resources: Only valid for an 'http' listener. A map of\n#        additional endpoints which should be loaded via dynamic modules.\n#\n# Valid resource names are:\n#\n#   client: the client-server API (/_matrix/client), and the synapse admin\n#       API (/_synapse/admin). Also implies 'media' and 'static'.\n#\n#   consent: user consent forms (/_matrix/consent). See\n#       docs/consent_tracking.md.\n#\n#   federation: the server-server API (/_matrix/federation). Also implies\n#       'media', 'keys', 'openid'\n#\n#   keys: the key discovery API (/_matrix/keys).\n#\n#   media: the media API (/_matrix/media).\n#\n#   metrics: the metrics interface. See docs/metrics-howto.md.\n#\n#   openid: OpenID authentication.\n#\n#   replication: the HTTP replication API (/_synapse/replication). See\n#       docs/workers.md.\n#\n#   static: static resources under synapse/static (/_matrix/static). (Mostly\n#       useful for 'fallback authentication'.)\n#\n#   webclient: A web client. Requires web_client_location to be set.\n#\nlisteners:\n  # TLS-enabled listener: for when matrix traffic is sent directly to synapse.\n  #\n  # Disabled by default. To enable it, uncomment the following. (Note that you\n  # will also need to give Synapse a TLS key and certificate: see the TLS section\n  # below.)\n  #\n  #- port: 8448\n  #  type: http\n  #  tls: true\n  #  resources:\n  #    - names: [client, federation]\n\n  # Unsecure HTTP listener: for when matrix traffic passes through a reverse proxy\n  # that unwraps TLS.\n  #\n  # If you plan to use a reverse proxy, please see\n  # https://github.com/matrix-org/synapse/blob/master/docs/reverse_proxy.md.\n  #\n  - port: 8008\n    tls: false\n    type: http\n    x_forwarded: true\n    bind_addresses: ['::1', '127.0.0.1']\n\n    resources:\n      - names: [client, federation]\n        compress: false\n\n    # example additional_resources:\n    #\n    #additional_resources:\n    #  \"/_matrix/my/custom/endpoint\":\n    #    module: my_module.CustomRequestHandler\n    #    config: {}\n\n  # Turn on the twisted ssh manhole service on localhost on the given\n  # port.\n  #\n  #- port: 9000\n  #  bind_addresses: ['::1', '127.0.0.1']\n  #  type: manhole\n\n# Forward extremities can build up in a room due to networking delays between\n# homeservers. Once this happens in a large room, calculation of the state of\n# that room can become quite expensive. To mitigate this, once the number of\n# forward extremities reaches a given threshold, Synapse will send an\n# org.matrix.dummy_event event, which will reduce the forward extremities\n# in the room.\n#\n# This setting defines the threshold (i.e. number of forward extremities in the\n# room) at which dummy events are sent. The default value is 10.\n#\n#dummy_events_threshold: 5\n\n\n## Homeserver blocking ##\n\n# How to reach the server admin, used in ResourceLimitError\n#\n#admin_contact: 'mailto:admin@server.com'\n\n# Global blocking\n#\n#hs_disabled: false\n#hs_disabled_message: 'Human readable reason for why the HS is blocked'\n\n# Monthly Active User Blocking\n#\n# Used in cases where the admin or server owner wants to limit to the\n# number of monthly active users.\n#\n# 'limit_usage_by_mau' disables/enables monthly active user blocking. When\n# enabled and a limit is reached the server returns a 'ResourceLimitError'\n# with error type Codes.RESOURCE_LIMIT_EXCEEDED\n#\n# 'max_mau_value' is the hard limit of monthly active users above which\n# the server will start blocking user actions.\n#\n# 'mau_trial_days' is a means to add a grace period for active users. It\n# means that users must be active for this number of days before they\n# can be considered active and guards against the case where lots of users\n# sign up in a short space of time never to return after their initial\n# session.\n#\n# 'mau_limit_alerting' is a means of limiting client side alerting\n# should the mau limit be reached. This is useful for small instances\n# where the admin has 5 mau seats (say) for 5 specific people and no\n# interest increasing the mau limit further. Defaults to True, which\n# means that alerting is enabled\n#\n#limit_usage_by_mau: false\n#max_mau_value: 50\n#mau_trial_days: 2\n#mau_limit_alerting: false\n\n# If enabled, the metrics for the number of monthly active users will\n# be populated, however no one will be limited. If limit_usage_by_mau\n# is true, this is implied to be true.\n#\n#mau_stats_only: false\n\n# Sometimes the server admin will want to ensure certain accounts are\n# never blocked by mau checking. These accounts are specified here.\n#\n#mau_limit_reserved_threepids:\n#  - medium: 'email'\n#    address: 'reserved_user@example.com'\n\n# Used by phonehome stats to group together related servers.\n#server_context: context\n\n# Resource-constrained homeserver settings\n#\n# When this is enabled, the room \"complexity\" will be checked before a user\n# joins a new remote room. If it is above the complexity limit, the server will\n# disallow joining, or will instantly leave.\n#\n# Room complexity is an arbitrary measure based on factors such as the number of\n# users in the room.\n#\nlimit_remote_rooms:\n  # Uncomment to enable room complexity checking.\n  #\n  #enabled: true\n\n  # the limit above which rooms cannot be joined. The default is 1.0.\n  #\n  #complexity: 0.5\n\n  # override the error which is returned when the room is too complex.\n  #\n  #complexity_error: \"This room is too complex.\"\n\n  # allow server admins to join complex rooms. Default is false.\n  #\n  #admins_can_join: true\n\n# Whether to require a user to be in the room to add an alias to it.\n# Defaults to 'true'.\n#\n#require_membership_for_aliases: false\n\n# Whether to allow per-room membership profiles through the send of membership\n# events with profile information that differ from the target's global profile.\n# Defaults to 'true'.\n#\n#allow_per_room_profiles: false\n\n# How long to keep redacted events in unredacted form in the database. After\n# this period redacted events get replaced with their redacted form in the DB.\n#\n# Defaults to `7d`. Set to `null` to disable.\n#\n#redaction_retention_period: 28d\n\n# How long to track users' last seen time and IPs in the database.\n#\n# Defaults to `28d`. Set to `null` to disable clearing out of old rows.\n#\n#user_ips_max_age: 14d\n\n# Message retention policy at the server level.\n#\n# Room admins and mods can define a retention period for their rooms using the\n# 'm.room.retention' state event, and server admins can cap this period by setting\n# the 'allowed_lifetime_min' and 'allowed_lifetime_max' config options.\n#\n# If this feature is enabled, Synapse will regularly look for and purge events\n# which are older than the room's maximum retention period. Synapse will also\n# filter events received over federation so that events that should have been\n# purged are ignored and not stored again.\n#\nretention:\n  # The message retention policies feature is disabled by default. Uncomment the\n  # following line to enable it.\n  #\n  #enabled: true\n\n  # Default retention policy. If set, Synapse will apply it to rooms that lack the\n  # 'm.room.retention' state event. Currently, the value of 'min_lifetime' doesn't\n  # matter much because Synapse doesn't take it into account yet.\n  #\n  #default_policy:\n  #  min_lifetime: 1d\n  #  max_lifetime: 1y\n\n  # Retention policy limits. If set, and the state of a room contains a\n  # 'm.room.retention' event in its state which contains a 'min_lifetime' or a\n  # 'max_lifetime' that's out of these bounds, Synapse will cap the room's policy\n  # to these limits when running purge jobs.\n  #\n  #allowed_lifetime_min: 1d\n  #allowed_lifetime_max: 1y\n\n  # Server admins can define the settings of the background jobs purging the\n  # events which lifetime has expired under the 'purge_jobs' section.\n  #\n  # If no configuration is provided, a single job will be set up to delete expired\n  # events in every room daily.\n  #\n  # Each job's configuration defines which range of message lifetimes the job\n  # takes care of. For example, if 'shortest_max_lifetime' is '2d' and\n  # 'longest_max_lifetime' is '3d', the job will handle purging expired events in\n  # rooms whose state defines a 'max_lifetime' that's both higher than 2 days, and\n  # lower than or equal to 3 days. Both the minimum and the maximum value of a\n  # range are optional, e.g. a job with no 'shortest_max_lifetime' and a\n  # 'longest_max_lifetime' of '3d' will handle every room with a retention policy\n  # which 'max_lifetime' is lower than or equal to three days.\n  #\n  # The rationale for this per-job configuration is that some rooms might have a\n  # retention policy with a low 'max_lifetime', where history needs to be purged\n  # of outdated messages on a more frequent basis than for the rest of the rooms\n  # (e.g. every 12h), but not want that purge to be performed by a job that's\n  # iterating over every room it knows, which could be heavy on the server.\n  #\n  # If any purge job is configured, it is strongly recommended to have at least\n  # a single job with neither 'shortest_max_lifetime' nor 'longest_max_lifetime'\n  # set, or one job without 'shortest_max_lifetime' and one job without\n  # 'longest_max_lifetime' set. Otherwise some rooms might be ignored, even if\n  # 'allowed_lifetime_min' and 'allowed_lifetime_max' are set, because capping a\n  # room's policy to these values is done after the policies are retrieved from\n  # Synapse's database (which is done using the range specified in a purge job's\n  # configuration).\n  #\n  #purge_jobs:\n  #  - longest_max_lifetime: 3d\n  #    interval: 12h\n  #  - shortest_max_lifetime: 3d\n  #    interval: 1d\n\n# Inhibits the /requestToken endpoints from returning an error that might leak\n# information about whether an e-mail address is in use or not on this\n# homeserver.\n# Note that for some endpoints the error situation is the e-mail already being\n# used, and for others the error is entering the e-mail being unused.\n# If this option is enabled, instead of returning an error, these endpoints will\n# act as if no error happened and return a fake session ID ('sid') to clients.\n#\n#request_token_inhibit_3pid_errors: true\n\n# A list of domains that the domain portion of 'next_link' parameters\n# must match.\n#\n# This parameter is optionally provided by clients while requesting\n# validation of an email or phone number, and maps to a link that\n# users will be automatically redirected to after validation\n# succeeds. Clients can make use this parameter to aid the validation\n# process.\n#\n# The whitelist is applied whether the homeserver or an\n# identity server is handling validation.\n#\n# The default value is no whitelist functionality; all domains are\n# allowed. Setting this value to an empty list will instead disallow\n# all domains.\n#\n#next_link_domain_whitelist: [\"matrix.org\"]\n\n\n## TLS ##\n\n# PEM-encoded X509 certificate for TLS.\n# This certificate, as of Synapse 1.0, will need to be a valid and verifiable\n# certificate, signed by a recognised Certificate Authority.\n#\n# See 'ACME support' below to enable auto-provisioning this certificate via\n# Let's Encrypt.\n#\n# If supplying your own, be sure to use a `.pem` file that includes the\n# full certificate chain including any intermediate certificates (for\n# instance, if using certbot, use `fullchain.pem` as your certificate,\n# not `cert.pem`).\n#\n#tls_certificate_path: \"CONFDIR/SERVERNAME.tls.crt\"\n\n# PEM-encoded private key for TLS\n#\n#tls_private_key_path: \"CONFDIR/SERVERNAME.tls.key\"\n\n# Whether to verify TLS server certificates for outbound federation requests.\n#\n# Defaults to `true`. To disable certificate verification, uncomment the\n# following line.\n#\n#federation_verify_certificates: false\n\n# The minimum TLS version that will be used for outbound federation requests.\n#\n# Defaults to `1`. Configurable to `1`, `1.1`, `1.2`, or `1.3`. Note\n# that setting this value higher than `1.2` will prevent federation to most\n# of the public Matrix network: only configure it to `1.3` if you have an\n# entirely private federation setup and you can ensure TLS 1.3 support.\n#\n#federation_client_minimum_tls_version: 1.2\n\n# Skip federation certificate verification on the following whitelist\n# of domains.\n#\n# This setting should only be used in very specific cases, such as\n# federation over Tor hidden services and similar. For private networks\n# of homeservers, you likely want to use a private CA instead.\n#\n# Only effective if federation_verify_certicates is `true`.\n#\n#federation_certificate_verification_whitelist:\n#  - lon.example.com\n#  - *.domain.com\n#  - *.onion\n\n# List of custom certificate authorities for federation traffic.\n#\n# This setting should only normally be used within a private network of\n# homeservers.\n#\n# Note that this list will replace those that are provided by your\n# operating environment. Certificates must be in PEM format.\n#\n#federation_custom_ca_list:\n#  - myCA1.pem\n#  - myCA2.pem\n#  - myCA3.pem\n\n# ACME support: This will configure Synapse to request a valid TLS certificate\n# for your configured `server_name` via Let's Encrypt.\n#\n# Note that ACME v1 is now deprecated, and Synapse currently doesn't support\n# ACME v2. This means that this feature currently won't work with installs set\n# up after November 2019. For more info, and alternative solutions, see\n# https://github.com/matrix-org/synapse/blob/master/docs/ACME.md#deprecation-of-acme-v1\n#\n# Note that provisioning a certificate in this way requires port 80 to be\n# routed to Synapse so that it can complete the http-01 ACME challenge.\n# By default, if you enable ACME support, Synapse will attempt to listen on\n# port 80 for incoming http-01 challenges - however, this will likely fail\n# with 'Permission denied' or a similar error.\n#\n# There are a couple of potential solutions to this:\n#\n#  * If you already have an Apache, Nginx, or similar listening on port 80,\n#    you can configure Synapse to use an alternate port, and have your web\n#    server forward the requests. For example, assuming you set 'port: 8009'\n#    below, on Apache, you would write:\n#\n#    ProxyPass /.well-known/acme-challenge http://localhost:8009/.well-known/acme-challenge\n#\n#  * Alternatively, you can use something like `authbind` to give Synapse\n#    permission to listen on port 80.\n#\nacme:\n    # ACME support is disabled by default. Set this to `true` and uncomment\n    # tls_certificate_path and tls_private_key_path above to enable it.\n    #\n    enabled: false\n\n    # Endpoint to use to request certificates. If you only want to test,\n    # use Let's Encrypt's staging url:\n    #     https://acme-staging.api.letsencrypt.org/directory\n    #\n    #url: https://acme-v01.api.letsencrypt.org/directory\n\n    # Port number to listen on for the HTTP-01 challenge. Change this if\n    # you are forwarding connections through Apache/Nginx/etc.\n    #\n    port: 80\n\n    # Local addresses to listen on for incoming connections.\n    # Again, you may want to change this if you are forwarding connections\n    # through Apache/Nginx/etc.\n    #\n    bind_addresses: ['::', '0.0.0.0']\n\n    # How many days remaining on a certificate before it is renewed.\n    #\n    reprovision_threshold: 30\n\n    # The domain that the certificate should be for. Normally this\n    # should be the same as your Matrix domain (i.e., 'server_name'), but,\n    # by putting a file at 'https://<server_name>/.well-known/matrix/server',\n    # you can delegate incoming traffic to another server. If you do that,\n    # you should give the target of the delegation here.\n    #\n    # For example: if your 'server_name' is 'example.com', but\n    # 'https://example.com/.well-known/matrix/server' delegates to\n    # 'matrix.example.com', you should put 'matrix.example.com' here.\n    #\n    # If not set, defaults to your 'server_name'.\n    #\n    domain: matrix.example.com\n\n    # file to use for the account key. This will be generated if it doesn't\n    # exist.\n    #\n    # If unspecified, we will use CONFDIR/client.key.\n    #\n    account_key_file: DATADIR/acme_account.key\n\n# List of allowed TLS fingerprints for this server to publish along\n# with the signing keys for this server. Other matrix servers that\n# make HTTPS requests to this server will check that the TLS\n# certificates returned by this server match one of the fingerprints.\n#\n# Synapse automatically adds the fingerprint of its own certificate\n# to the list. So if federation traffic is handled directly by synapse\n# then no modification to the list is required.\n#\n# If synapse is run behind a load balancer that handles the TLS then it\n# will be necessary to add the fingerprints of the certificates used by\n# the loadbalancers to this list if they are different to the one\n# synapse is using.\n#\n# Homeservers are permitted to cache the list of TLS fingerprints\n# returned in the key responses up to the \"valid_until_ts\" returned in\n# key. It may be necessary to publish the fingerprints of a new\n# certificate and wait until the \"valid_until_ts\" of the previous key\n# responses have passed before deploying it.\n#\n# You can calculate a fingerprint from a given TLS listener via:\n# openssl s_client -connect $host:$port < /dev/null 2> /dev/null |\n#   openssl x509 -outform DER | openssl sha256 -binary | base64 | tr -d '='\n# or by checking matrix.org/federationtester/api/report?server_name=$host\n#\n#tls_fingerprints: [{\"sha256\": \"<base64_encoded_sha256_fingerprint>\"}]\n\n\n## Federation ##\n\n# Restrict federation to the following whitelist of domains.\n# N.B. we recommend also firewalling your federation listener to limit\n# inbound federation traffic as early as possible, rather than relying\n# purely on this application-layer restriction.  If not specified, the\n# default is to whitelist everything.\n#\n#federation_domain_whitelist:\n#  - lon.example.com\n#  - nyc.example.com\n#  - syd.example.com\n\n# Prevent outgoing requests from being sent to the following blacklisted IP address\n# CIDR ranges. If this option is not specified, or specified with an empty list,\n# no IP range blacklist will be enforced.\n#\n# The blacklist applies to the outbound requests for federation, identity servers,\n# push servers, and for checking key validitity for third-party invite events.\n#\n# (0.0.0.0 and :: are always blacklisted, whether or not they are explicitly\n# listed here, since they correspond to unroutable addresses.)\n#\n# This option replaces federation_ip_range_blacklist in Synapse v1.24.0.\n#\nip_range_blacklist:\n  - '127.0.0.0/8'\n  - '10.0.0.0/8'\n  - '172.16.0.0/12'\n  - '192.168.0.0/16'\n  - '100.64.0.0/10'\n  - '169.254.0.0/16'\n  - '::1/128'\n  - 'fe80::/64'\n  - 'fc00::/7'\n\n# Report prometheus metrics on the age of PDUs being sent to and received from\n# the following domains. This can be used to give an idea of \"delay\" on inbound\n# and outbound federation, though be aware that any delay can be due to problems\n# at either end or with the intermediate network.\n#\n# By default, no domains are monitored in this way.\n#\n#federation_metrics_domains:\n#  - matrix.org\n#  - example.com\n\n\n## Caching ##\n\n# Caching can be configured through the following options.\n#\n# A cache 'factor' is a multiplier that can be applied to each of\n# Synapse's caches in order to increase or decrease the maximum\n# number of entries that can be stored.\n\n# The number of events to cache in memory. Not affected by\n# caches.global_factor.\n#\n#event_cache_size: 10K\n\ncaches:\n   # Controls the global cache factor, which is the default cache factor\n   # for all caches if a specific factor for that cache is not otherwise\n   # set.\n   #\n   # This can also be set by the \"SYNAPSE_CACHE_FACTOR\" environment\n   # variable. Setting by environment variable takes priority over\n   # setting through the config file.\n   #\n   # Defaults to 0.5, which will half the size of all caches.\n   #\n   #global_factor: 1.0\n\n   # A dictionary of cache name to cache factor for that individual\n   # cache. Overrides the global cache factor for a given cache.\n   #\n   # These can also be set through environment variables comprised\n   # of \"SYNAPSE_CACHE_FACTOR_\" + the name of the cache in capital\n   # letters and underscores. Setting by environment variable\n   # takes priority over setting through the config file.\n   # Ex. SYNAPSE_CACHE_FACTOR_GET_USERS_WHO_SHARE_ROOM_WITH_USER=2.0\n   #\n   # Some caches have '*' and other characters that are not\n   # alphanumeric or underscores. These caches can be named with or\n   # without the special characters stripped. For example, to specify\n   # the cache factor for `*stateGroupCache*` via an environment\n   # variable would be `SYNAPSE_CACHE_FACTOR_STATEGROUPCACHE=2.0`.\n   #\n   per_cache_factors:\n     #get_users_who_share_room_with_user: 2.0\n\n\n## Database ##\n\n# The 'database' setting defines the database that synapse uses to store all of\n# its data.\n#\n# 'name' gives the database engine to use: either 'sqlite3' (for SQLite) or\n# 'psycopg2' (for PostgreSQL).\n#\n# 'args' gives options which are passed through to the database engine,\n# except for options starting 'cp_', which are used to configure the Twisted\n# connection pool. For a reference to valid arguments, see:\n#   * for sqlite: https://docs.python.org/3/library/sqlite3.html#sqlite3.connect\n#   * for postgres: https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-PARAMKEYWORDS\n#   * for the connection pool: https://twistedmatrix.com/documents/current/api/twisted.enterprise.adbapi.ConnectionPool.html#__init__\n#\n#\n# Example SQLite configuration:\n#\n#database:\n#  name: sqlite3\n#  args:\n#    database: /path/to/homeserver.db\n#\n#\n# Example Postgres configuration:\n#\n#database:\n#  name: psycopg2\n#  args:\n#    user: synapse_user\n#    password: secretpassword\n#    database: synapse\n#    host: localhost\n#    cp_min: 5\n#    cp_max: 10\n#\n# For more information on using Synapse with Postgres, see `docs/postgres.md`.\n#\ndatabase:\n  name: sqlite3\n  args:\n    database: DATADIR/homeserver.db\n\n\n## Logging ##\n\n# A yaml python logging config file as described by\n# https://docs.python.org/3.7/library/logging.config.html#configuration-dictionary-schema\n#\nlog_config: \"CONFDIR/SERVERNAME.log.config\"\n\n\n## Ratelimiting ##\n\n# Ratelimiting settings for client actions (registration, login, messaging).\n#\n# Each ratelimiting configuration is made of two parameters:\n#   - per_second: number of requests a client can send per second.\n#   - burst_count: number of requests a client can send before being throttled.\n#\n# Synapse currently uses the following configurations:\n#   - one for messages that ratelimits sending based on the account the client\n#     is using\n#   - one for registration that ratelimits registration requests based on the\n#     client's IP address.\n#   - one for login that ratelimits login requests based on the client's IP\n#     address.\n#   - one for login that ratelimits login requests based on the account the\n#     client is attempting to log into.\n#   - one for login that ratelimits login requests based on the account the\n#     client is attempting to log into, based on the amount of failed login\n#     attempts for this account.\n#   - one for ratelimiting redactions by room admins. If this is not explicitly\n#     set then it uses the same ratelimiting as per rc_message. This is useful\n#     to allow room admins to deal with abuse quickly.\n#   - two for ratelimiting number of rooms a user can join, \"local\" for when\n#     users are joining rooms the server is already in (this is cheap) vs\n#     \"remote\" for when users are trying to join rooms not on the server (which\n#     can be more expensive)\n#\n# The defaults are as shown below.\n#\n#rc_message:\n#  per_second: 0.2\n#  burst_count: 10\n#\n#rc_registration:\n#  per_second: 0.17\n#  burst_count: 3\n#\n#rc_login:\n#  address:\n#    per_second: 0.17\n#    burst_count: 3\n#  account:\n#    per_second: 0.17\n#    burst_count: 3\n#  failed_attempts:\n#    per_second: 0.17\n#    burst_count: 3\n#\n#rc_admin_redaction:\n#  per_second: 1\n#  burst_count: 50\n#\n#rc_joins:\n#  local:\n#    per_second: 0.1\n#    burst_count: 3\n#  remote:\n#    per_second: 0.01\n#    burst_count: 3\n\n\n# Ratelimiting settings for incoming federation\n#\n# The rc_federation configuration is made up of the following settings:\n#   - window_size: window size in milliseconds\n#   - sleep_limit: number of federation requests from a single server in\n#     a window before the server will delay processing the request.\n#   - sleep_delay: duration in milliseconds to delay processing events\n#     from remote servers by if they go over the sleep limit.\n#   - reject_limit: maximum number of concurrent federation requests\n#     allowed from a single server\n#   - concurrent: number of federation requests to concurrently process\n#     from a single server\n#\n# The defaults are as shown below.\n#\n#rc_federation:\n#  window_size: 1000\n#  sleep_limit: 10\n#  sleep_delay: 500\n#  reject_limit: 50\n#  concurrent: 3\n\n# Target outgoing federation transaction frequency for sending read-receipts,\n# per-room.\n#\n# If we end up trying to send out more read-receipts, they will get buffered up\n# into fewer transactions.\n#\n#federation_rr_transactions_per_room_per_second: 50\n\n\n\n## Media Store ##\n\n# Enable the media store service in the Synapse master. Uncomment the\n# following if you are using a separate media store worker.\n#\n#enable_media_repo: false\n\n# Directory where uploaded images and attachments are stored.\n#\nmedia_store_path: \"DATADIR/media_store\"\n\n# Media storage providers allow media to be stored in different\n# locations.\n#\n#media_storage_providers:\n#  - module: file_system\n#    # Whether to store newly uploaded local files\n#    store_local: false\n#    # Whether to store newly downloaded remote files\n#    store_remote: false\n#    # Whether to wait for successful storage for local uploads\n#    store_synchronous: false\n#    config:\n#       directory: /mnt/some/other/directory\n\n# The largest allowed upload size in bytes\n#\n#max_upload_size: 50M\n\n# Maximum number of pixels that will be thumbnailed\n#\n#max_image_pixels: 32M\n\n# Whether to generate new thumbnails on the fly to precisely match\n# the resolution requested by the client. If true then whenever\n# a new resolution is requested by the client the server will\n# generate a new thumbnail. If false the server will pick a thumbnail\n# from a precalculated list.\n#\n#dynamic_thumbnails: false\n\n# List of thumbnails to precalculate when an image is uploaded.\n#\n#thumbnail_sizes:\n#  - width: 32\n#    height: 32\n#    method: crop\n#  - width: 96\n#    height: 96\n#    method: crop\n#  - width: 320\n#    height: 240\n#    method: scale\n#  - width: 640\n#    height: 480\n#    method: scale\n#  - width: 800\n#    height: 600\n#    method: scale\n\n# Is the preview URL API enabled?\n#\n# 'false' by default: uncomment the following to enable it (and specify a\n# url_preview_ip_range_blacklist blacklist).\n#\n#url_preview_enabled: true\n\n# List of IP address CIDR ranges that the URL preview spider is denied\n# from accessing.  There are no defaults: you must explicitly\n# specify a list for URL previewing to work.  You should specify any\n# internal services in your network that you do not want synapse to try\n# to connect to, otherwise anyone in any Matrix room could cause your\n# synapse to issue arbitrary GET requests to your internal services,\n# causing serious security issues.\n#\n# (0.0.0.0 and :: are always blacklisted, whether or not they are explicitly\n# listed here, since they correspond to unroutable addresses.)\n#\n# This must be specified if url_preview_enabled is set. It is recommended that\n# you uncomment the following list as a starting point.\n#\n#url_preview_ip_range_blacklist:\n#  - '127.0.0.0/8'\n#  - '10.0.0.0/8'\n#  - '172.16.0.0/12'\n#  - '192.168.0.0/16'\n#  - '100.64.0.0/10'\n#  - '169.254.0.0/16'\n#  - '::1/128'\n#  - 'fe80::/64'\n#  - 'fc00::/7'\n\n# List of IP address CIDR ranges that the URL preview spider is allowed\n# to access even if they are specified in url_preview_ip_range_blacklist.\n# This is useful for specifying exceptions to wide-ranging blacklisted\n# target IP ranges - e.g. for enabling URL previews for a specific private\n# website only visible in your network.\n#\n#url_preview_ip_range_whitelist:\n#   - '192.168.1.1'\n\n# Optional list of URL matches that the URL preview spider is\n# denied from accessing.  You should use url_preview_ip_range_blacklist\n# in preference to this, otherwise someone could define a public DNS\n# entry that points to a private IP address and circumvent the blacklist.\n# This is more useful if you know there is an entire shape of URL that\n# you know that will never want synapse to try to spider.\n#\n# Each list entry is a dictionary of url component attributes as returned\n# by urlparse.urlsplit as applied to the absolute form of the URL.  See\n# https://docs.python.org/2/library/urlparse.html#urlparse.urlsplit\n# The values of the dictionary are treated as an filename match pattern\n# applied to that component of URLs, unless they start with a ^ in which\n# case they are treated as a regular expression match.  If all the\n# specified component matches for a given list item succeed, the URL is\n# blacklisted.\n#\n#url_preview_url_blacklist:\n#  # blacklist any URL with a username in its URI\n#  - username: '*'\n#\n#  # blacklist all *.google.com URLs\n#  - netloc: 'google.com'\n#  - netloc: '*.google.com'\n#\n#  # blacklist all plain HTTP URLs\n#  - scheme: 'http'\n#\n#  # blacklist http(s)://www.acme.com/foo\n#  - netloc: 'www.acme.com'\n#    path: '/foo'\n#\n#  # blacklist any URL with a literal IPv4 address\n#  - netloc: '^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$'\n\n# The largest allowed URL preview spidering size in bytes\n#\n#max_spider_size: 10M\n\n# A list of values for the Accept-Language HTTP header used when\n# downloading webpages during URL preview generation. This allows\n# Synapse to specify the preferred languages that URL previews should\n# be in when communicating with remote servers.\n#\n# Each value is a IETF language tag; a 2-3 letter identifier for a\n# language, optionally followed by subtags separated by '-', specifying\n# a country or region variant.\n#\n# Multiple values can be provided, and a weight can be added to each by\n# using quality value syntax (;q=). '*' translates to any language.\n#\n# Defaults to \"en\".\n#\n# Example:\n#\n# url_preview_accept_language:\n#   - en-UK\n#   - en-US;q=0.9\n#   - fr;q=0.8\n#   - *;q=0.7\n#\nurl_preview_accept_language:\n#   - en\n\n\n## Captcha ##\n# See docs/CAPTCHA_SETUP.md for full details of configuring this.\n\n# This homeserver's ReCAPTCHA public key. Must be specified if\n# enable_registration_captcha is enabled.\n#\n#recaptcha_public_key: \"YOUR_PUBLIC_KEY\"\n\n# This homeserver's ReCAPTCHA private key. Must be specified if\n# enable_registration_captcha is enabled.\n#\n#recaptcha_private_key: \"YOUR_PRIVATE_KEY\"\n\n# Uncomment to enable ReCaptcha checks when registering, preventing signup\n# unless a captcha is answered. Requires a valid ReCaptcha\n# public/private key. Defaults to 'false'.\n#\n#enable_registration_captcha: true\n\n# The API endpoint to use for verifying m.login.recaptcha responses.\n# Defaults to \"https://www.recaptcha.net/recaptcha/api/siteverify\".\n#\n#recaptcha_siteverify_api: \"https://my.recaptcha.site\"\n\n\n## TURN ##\n\n# The public URIs of the TURN server to give to clients\n#\n#turn_uris: []\n\n# The shared secret used to compute passwords for the TURN server\n#\n#turn_shared_secret: \"YOUR_SHARED_SECRET\"\n\n# The Username and password if the TURN server needs them and\n# does not use a token\n#\n#turn_username: \"TURNSERVER_USERNAME\"\n#turn_password: \"TURNSERVER_PASSWORD\"\n\n# How long generated TURN credentials last\n#\n#turn_user_lifetime: 1h\n\n# Whether guests should be allowed to use the TURN server.\n# This defaults to True, otherwise VoIP will be unreliable for guests.\n# However, it does introduce a slight security risk as it allows users to\n# connect to arbitrary endpoints without having first signed up for a\n# valid account (e.g. by passing a CAPTCHA).\n#\n#turn_allow_guests: true\n\n\n## Registration ##\n#\n# Registration can be rate-limited using the parameters in the \"Ratelimiting\"\n# section of this file.\n\n# Enable registration for new users.\n#\n#enable_registration: false\n\n# Optional account validity configuration. This allows for accounts to be denied\n# any request after a given period.\n#\n# Once this feature is enabled, Synapse will look for registered users without an\n# expiration date at startup and will add one to every account it found using the\n# current settings at that time.\n# This means that, if a validity period is set, and Synapse is restarted (it will\n# then derive an expiration date from the current validity period), and some time\n# after that the validity period changes and Synapse is restarted, the users'\n# expiration dates won't be updated unless their account is manually renewed. This\n# date will be randomly selected within a range [now + period - d ; now + period],\n# where d is equal to 10% of the validity period.\n#\naccount_validity:\n  # The account validity feature is disabled by default. Uncomment the\n  # following line to enable it.\n  #\n  #enabled: true\n\n  # The period after which an account is valid after its registration. When\n  # renewing the account, its validity period will be extended by this amount\n  # of time. This parameter is required when using the account validity\n  # feature.\n  #\n  #period: 6w\n\n  # The amount of time before an account's expiry date at which Synapse will\n  # send an email to the account's email address with a renewal link. By\n  # default, no such emails are sent.\n  #\n  # If you enable this setting, you will also need to fill out the 'email' and\n  # 'public_baseurl' configuration sections.\n  #\n  #renew_at: 1w\n\n  # The subject of the email sent out with the renewal link. '%(app)s' can be\n  # used as a placeholder for the 'app_name' parameter from the 'email'\n  # section.\n  #\n  # Note that the placeholder must be written '%(app)s', including the\n  # trailing 's'.\n  #\n  # If this is not set, a default value is used.\n  #\n  #renew_email_subject: \"Renew your %(app)s account\"\n\n  # Directory in which Synapse will try to find templates for the HTML files to\n  # serve to the user when trying to renew an account. If not set, default\n  # templates from within the Synapse package will be used.\n  #\n  #template_dir: \"res/templates\"\n\n  # File within 'template_dir' giving the HTML to be displayed to the user after\n  # they successfully renewed their account. If not set, default text is used.\n  #\n  #account_renewed_html_path: \"account_renewed.html\"\n\n  # File within 'template_dir' giving the HTML to be displayed when the user\n  # tries to renew an account with an invalid renewal token. If not set,\n  # default text is used.\n  #\n  #invalid_token_html_path: \"invalid_token.html\"\n\n# Time that a user's session remains valid for, after they log in.\n#\n# Note that this is not currently compatible with guest logins.\n#\n# Note also that this is calculated at login time: changes are not applied\n# retrospectively to users who have already logged in.\n#\n# By default, this is infinite.\n#\n#session_lifetime: 24h\n\n# The user must provide all of the below types of 3PID when registering.\n#\n#registrations_require_3pid:\n#  - email\n#  - msisdn\n\n# Explicitly disable asking for MSISDNs from the registration\n# flow (overrides registrations_require_3pid if MSISDNs are set as required)\n#\n#disable_msisdn_registration: true\n\n# Mandate that users are only allowed to associate certain formats of\n# 3PIDs with accounts on this server.\n#\n#allowed_local_3pids:\n#  - medium: email\n#    pattern: '.*@matrix\\.org'\n#  - medium: email\n#    pattern: '.*@vector\\.im'\n#  - medium: msisdn\n#    pattern: '\\+44'\n\n# Enable 3PIDs lookup requests to identity servers from this server.\n#\n#enable_3pid_lookup: true\n\n# If set, allows registration of standard or admin accounts by anyone who\n# has the shared secret, even if registration is otherwise disabled.\n#\n#registration_shared_secret: <PRIVATE STRING>\n\n# Set the number of bcrypt rounds used to generate password hash.\n# Larger numbers increase the work factor needed to generate the hash.\n# The default number is 12 (which equates to 2^12 rounds).\n# N.B. that increasing this will exponentially increase the time required\n# to register or login - e.g. 24 => 2^24 rounds which will take >20 mins.\n#\n#bcrypt_rounds: 12\n\n# Allows users to register as guests without a password/email/etc, and\n# participate in rooms hosted on this server which have been made\n# accessible to anonymous users.\n#\n#allow_guest_access: false\n\n# The identity server which we suggest that clients should use when users log\n# in on this server.\n#\n# (By default, no suggestion is made, so it is left up to the client.\n# This setting is ignored unless public_baseurl is also set.)\n#\n#default_identity_server: https://matrix.org\n\n# Handle threepid (email/phone etc) registration and password resets through a set of\n# *trusted* identity servers. Note that this allows the configured identity server to\n# reset passwords for accounts!\n#\n# Be aware that if `email` is not set, and SMTP options have not been\n# configured in the email config block, registration and user password resets via\n# email will be globally disabled.\n#\n# Additionally, if `msisdn` is not set, registration and password resets via msisdn\n# will be disabled regardless, and users will not be able to associate an msisdn\n# identifier to their account. This is due to Synapse currently not supporting\n# any method of sending SMS messages on its own.\n#\n# To enable using an identity server for operations regarding a particular third-party\n# identifier type, set the value to the URL of that identity server as shown in the\n# examples below.\n#\n# Servers handling the these requests must answer the `/requestToken` endpoints defined\n# by the Matrix Identity Service API specification:\n# https://matrix.org/docs/spec/identity_service/latest\n#\n# If a delegate is specified, the config option public_baseurl must also be filled out.\n#\naccount_threepid_delegates:\n    #email: https://example.com     # Delegate email sending to example.com\n    #msisdn: http://localhost:8090  # Delegate SMS sending to this local process\n\n# Whether users are allowed to change their displayname after it has\n# been initially set. Useful when provisioning users based on the\n# contents of a third-party directory.\n#\n# Does not apply to server administrators. Defaults to 'true'\n#\n#enable_set_displayname: false\n\n# Whether users are allowed to change their avatar after it has been\n# initially set. Useful when provisioning users based on the contents\n# of a third-party directory.\n#\n# Does not apply to server administrators. Defaults to 'true'\n#\n#enable_set_avatar_url: false\n\n# Whether users can change the 3PIDs associated with their accounts\n# (email address and msisdn).\n#\n# Defaults to 'true'\n#\n#enable_3pid_changes: false\n\n# Users who register on this homeserver will automatically be joined\n# to these rooms.\n#\n# By default, any room aliases included in this list will be created\n# as a publicly joinable room when the first user registers for the\n# homeserver. This behaviour can be customised with the settings below.\n#\n#auto_join_rooms:\n#  - \"#example:example.com\"\n\n# Where auto_join_rooms are specified, setting this flag ensures that the\n# the rooms exist by creating them when the first user on the\n# homeserver registers.\n#\n# By default the auto-created rooms are publicly joinable from any federated\n# server. Use the autocreate_auto_join_rooms_federated and\n# autocreate_auto_join_room_preset settings below to customise this behaviour.\n#\n# Setting to false means that if the rooms are not manually created,\n# users cannot be auto-joined since they do not exist.\n#\n# Defaults to true. Uncomment the following line to disable automatically\n# creating auto-join rooms.\n#\n#autocreate_auto_join_rooms: false\n\n# Whether the auto_join_rooms that are auto-created are available via\n# federation. Only has an effect if autocreate_auto_join_rooms is true.\n#\n# Note that whether a room is federated cannot be modified after\n# creation.\n#\n# Defaults to true: the room will be joinable from other servers.\n# Uncomment the following to prevent users from other homeservers from\n# joining these rooms.\n#\n#autocreate_auto_join_rooms_federated: false\n\n# The room preset to use when auto-creating one of auto_join_rooms. Only has an\n# effect if autocreate_auto_join_rooms is true.\n#\n# This can be one of \"public_chat\", \"private_chat\", or \"trusted_private_chat\".\n# If a value of \"private_chat\" or \"trusted_private_chat\" is used then\n# auto_join_mxid_localpart must also be configured.\n#\n# Defaults to \"public_chat\", meaning that the room is joinable by anyone, including\n# federated servers if autocreate_auto_join_rooms_federated is true (the default).\n# Uncomment the following to require an invitation to join these rooms.\n#\n#autocreate_auto_join_room_preset: private_chat\n\n# The local part of the user id which is used to create auto_join_rooms if\n# autocreate_auto_join_rooms is true. If this is not provided then the\n# initial user account that registers will be used to create the rooms.\n#\n# The user id is also used to invite new users to any auto-join rooms which\n# are set to invite-only.\n#\n# It *must* be configured if autocreate_auto_join_room_preset is set to\n# \"private_chat\" or \"trusted_private_chat\".\n#\n# Note that this must be specified in order for new users to be correctly\n# invited to any auto-join rooms which have been set to invite-only (either\n# at the time of creation or subsequently).\n#\n# Note that, if the room already exists, this user must be joined and\n# have the appropriate permissions to invite new members.\n#\n#auto_join_mxid_localpart: system\n\n# When auto_join_rooms is specified, setting this flag to false prevents\n# guest accounts from being automatically joined to the rooms.\n#\n# Defaults to true.\n#\n#auto_join_rooms_for_guests: false\n\n\n## Metrics ###\n\n# Enable collection and rendering of performance metrics\n#\n#enable_metrics: false\n\n# Enable sentry integration\n# NOTE: While attempts are made to ensure that the logs don't contain\n# any sensitive information, this cannot be guaranteed. By enabling\n# this option the sentry server may therefore receive sensitive\n# information, and it in turn may then diseminate sensitive information\n# through insecure notification channels if so configured.\n#\n#sentry:\n#    dsn: \"...\"\n\n# Flags to enable Prometheus metrics which are not suitable to be\n# enabled by default, either for performance reasons or limited use.\n#\nmetrics_flags:\n    # Publish synapse_federation_known_servers, a gauge of the number of\n    # servers this homeserver knows about, including itself. May cause\n    # performance problems on large homeservers.\n    #\n    #known_servers: true\n\n# Whether or not to report anonymized homeserver usage statistics.\n#\n#report_stats: true|false\n\n# The endpoint to report the anonymized homeserver usage statistics to.\n# Defaults to https://matrix.org/report-usage-stats/push\n#\n#report_stats_endpoint: https://example.com/report-usage-stats/push\n\n\n## API Configuration ##\n\n# A list of event types that will be included in the room_invite_state\n#\n#room_invite_state_types:\n#  - \"m.room.join_rules\"\n#  - \"m.room.canonical_alias\"\n#  - \"m.room.avatar\"\n#  - \"m.room.encryption\"\n#  - \"m.room.name\"\n\n\n# A list of application service config files to use\n#\n#app_service_config_files:\n#  - app_service_1.yaml\n#  - app_service_2.yaml\n\n# Uncomment to enable tracking of application service IP addresses. Implicitly\n# enables MAU tracking for application service users.\n#\n#track_appservice_user_ips: true\n\n\n# a secret which is used to sign access tokens. If none is specified,\n# the registration_shared_secret is used, if one is given; otherwise,\n# a secret key is derived from the signing key.\n#\n#macaroon_secret_key: <PRIVATE STRING>\n\n# a secret which is used to calculate HMACs for form values, to stop\n# falsification of values. Must be specified for the User Consent\n# forms to work.\n#\n#form_secret: <PRIVATE STRING>\n\n## Signing Keys ##\n\n# Path to the signing key to sign messages with\n#\nsigning_key_path: \"CONFDIR/SERVERNAME.signing.key\"\n\n# The keys that the server used to sign messages with but won't use\n# to sign new messages.\n#\nold_signing_keys:\n  # For each key, `key` should be the base64-encoded public key, and\n  # `expired_ts`should be the time (in milliseconds since the unix epoch) that\n  # it was last used.\n  #\n  # It is possible to build an entry from an old signing.key file using the\n  # `export_signing_key` script which is provided with synapse.\n  #\n  # For example:\n  #\n  #\"ed25519:id\": { key: \"base64string\", expired_ts: 123456789123 }\n\n# How long key response published by this server is valid for.\n# Used to set the valid_until_ts in /key/v2 APIs.\n# Determines how quickly servers will query to check which keys\n# are still valid.\n#\n#key_refresh_interval: 1d\n\n# The trusted servers to download signing keys from.\n#\n# When we need to fetch a signing key, each server is tried in parallel.\n#\n# Normally, the connection to the key server is validated via TLS certificates.\n# Additional security can be provided by configuring a `verify key`, which\n# will make synapse check that the response is signed by that key.\n#\n# This setting supercedes an older setting named `perspectives`. The old format\n# is still supported for backwards-compatibility, but it is deprecated.\n#\n# 'trusted_key_servers' defaults to matrix.org, but using it will generate a\n# warning on start-up. To suppress this warning, set\n# 'suppress_key_server_warning' to true.\n#\n# Options for each entry in the list include:\n#\n#    server_name: the name of the server. required.\n#\n#    verify_keys: an optional map from key id to base64-encoded public key.\n#       If specified, we will check that the response is signed by at least\n#       one of the given keys.\n#\n#    accept_keys_insecurely: a boolean. Normally, if `verify_keys` is unset,\n#       and federation_verify_certificates is not `true`, synapse will refuse\n#       to start, because this would allow anyone who can spoof DNS responses\n#       to masquerade as the trusted key server. If you know what you are doing\n#       and are sure that your network environment provides a secure connection\n#       to the key server, you can set this to `true` to override this\n#       behaviour.\n#\n# An example configuration might look like:\n#\n#trusted_key_servers:\n#  - server_name: \"my_trusted_server.example.com\"\n#    verify_keys:\n#      \"ed25519:auto\": \"abcdefghijklmnopqrstuvwxyzabcdefghijklmopqr\"\n#  - server_name: \"my_other_trusted_server.example.com\"\n#\ntrusted_key_servers:\n  - server_name: \"matrix.org\"\n\n# Uncomment the following to disable the warning that is emitted when the\n# trusted_key_servers include 'matrix.org'. See above.\n#\n#suppress_key_server_warning: true\n\n# The signing keys to use when acting as a trusted key server. If not specified\n# defaults to the server signing key.\n#\n# Can contain multiple keys, one per line.\n#\n#key_server_signing_keys_path: \"key_server_signing_keys.key\"\n\n\n## Single sign-on integration ##\n\n# The following settings can be used to make Synapse use a single sign-on\n# provider for authentication, instead of its internal password database.\n#\n# You will probably also want to set the following options to `false` to\n# disable the regular login/registration flows:\n#   * enable_registration\n#   * password_config.enabled\n#\n# You will also want to investigate the settings under the \"sso\" configuration\n# section below.\n\n# Enable SAML2 for registration and login. Uses pysaml2.\n#\n# At least one of `sp_config` or `config_path` must be set in this section to\n# enable SAML login.\n#\n# Once SAML support is enabled, a metadata file will be exposed at\n# https://<server>:<port>/_matrix/saml2/metadata.xml, which you may be able to\n# use to configure your SAML IdP with. Alternatively, you can manually configure\n# the IdP to use an ACS location of\n# https://<server>:<port>/_matrix/saml2/authn_response.\n#\nsaml2_config:\n  # `sp_config` is the configuration for the pysaml2 Service Provider.\n  # See pysaml2 docs for format of config.\n  #\n  # Default values will be used for the 'entityid' and 'service' settings,\n  # so it is not normally necessary to specify them unless you need to\n  # override them.\n  #\n  sp_config:\n    # Point this to the IdP's metadata. You must provide either a local\n    # file via the `local` attribute or (preferably) a URL via the\n    # `remote` attribute.\n    #\n    #metadata:\n    #  local: [\"saml2/idp.xml\"]\n    #  remote:\n    #    - url: https://our_idp/metadata.xml\n\n    # Allowed clock difference in seconds between the homeserver and IdP.\n    #\n    # Uncomment the below to increase the accepted time difference from 0 to 3 seconds.\n    #\n    #accepted_time_diff: 3\n\n    # By default, the user has to go to our login page first. If you'd like\n    # to allow IdP-initiated login, set 'allow_unsolicited: true' in a\n    # 'service.sp' section:\n    #\n    #service:\n    #  sp:\n    #    allow_unsolicited: true\n\n    # The examples below are just used to generate our metadata xml, and you\n    # may well not need them, depending on your setup. Alternatively you\n    # may need a whole lot more detail - see the pysaml2 docs!\n\n    #description: [\"My awesome SP\", \"en\"]\n    #name: [\"Test SP\", \"en\"]\n\n    #ui_info:\n    #  display_name:\n    #    - lang: en\n    #      text: \"Display Name is the descriptive name of your service.\"\n    #  description:\n    #    - lang: en\n    #      text: \"Description should be a short paragraph explaining the purpose of the service.\"\n    #  information_url:\n    #    - lang: en\n    #      text: \"https://example.com/terms-of-service\"\n    #  privacy_statement_url:\n    #    - lang: en\n    #      text: \"https://example.com/privacy-policy\"\n    #  keywords:\n    #    - lang: en\n    #      text: [\"Matrix\", \"Element\"]\n    #  logo:\n    #    - lang: en\n    #      text: \"https://example.com/logo.svg\"\n    #      width: \"200\"\n    #      height: \"80\"\n\n    #organization:\n    #  name: Example com\n    #  display_name:\n    #    - [\"Example co\", \"en\"]\n    #  url: \"http://example.com\"\n\n    #contact_person:\n    #  - given_name: Bob\n    #    sur_name: \"the Sysadmin\"\n    #    email_address\": [\"admin@example.com\"]\n    #    contact_type\": technical\n\n  # Instead of putting the config inline as above, you can specify a\n  # separate pysaml2 configuration file:\n  #\n  #config_path: \"CONFDIR/sp_conf.py\"\n\n  # The lifetime of a SAML session. This defines how long a user has to\n  # complete the authentication process, if allow_unsolicited is unset.\n  # The default is 15 minutes.\n  #\n  #saml_session_lifetime: 5m\n\n  # An external module can be provided here as a custom solution to\n  # mapping attributes returned from a saml provider onto a matrix user.\n  #\n  user_mapping_provider:\n    # The custom module's class. Uncomment to use a custom module.\n    #\n    #module: mapping_provider.SamlMappingProvider\n\n    # Custom configuration values for the module. Below options are\n    # intended for the built-in provider, they should be changed if\n    # using a custom module. This section will be passed as a Python\n    # dictionary to the module's `parse_config` method.\n    #\n    config:\n      # The SAML attribute (after mapping via the attribute maps) to use\n      # to derive the Matrix ID from. 'uid' by default.\n      #\n      # Note: This used to be configured by the\n      # saml2_config.mxid_source_attribute option. If that is still\n      # defined, its value will be used instead.\n      #\n      #mxid_source_attribute: displayName\n\n      # The mapping system to use for mapping the saml attribute onto a\n      # matrix ID.\n      #\n      # Options include:\n      #  * 'hexencode' (which maps unpermitted characters to '=xx')\n      #  * 'dotreplace' (which replaces unpermitted characters with\n      #     '.').\n      # The default is 'hexencode'.\n      #\n      # Note: This used to be configured by the\n      # saml2_config.mxid_mapping option. If that is still defined, its\n      # value will be used instead.\n      #\n      #mxid_mapping: dotreplace\n\n  # In previous versions of synapse, the mapping from SAML attribute to\n  # MXID was always calculated dynamically rather than stored in a\n  # table. For backwards- compatibility, we will look for user_ids\n  # matching such a pattern before creating a new account.\n  #\n  # This setting controls the SAML attribute which will be used for this\n  # backwards-compatibility lookup. Typically it should be 'uid', but if\n  # the attribute maps are changed, it may be necessary to change it.\n  #\n  # The default is 'uid'.\n  #\n  #grandfathered_mxid_source_attribute: upn\n\n  # It is possible to configure Synapse to only allow logins if SAML attributes\n  # match particular values. The requirements can be listed under\n  # `attribute_requirements` as shown below. All of the listed attributes must\n  # match for the login to be permitted.\n  #\n  #attribute_requirements:\n  #  - attribute: userGroup\n  #    value: \"staff\"\n  #  - attribute: department\n  #    value: \"sales\"\n\n  # If the metadata XML contains multiple IdP entities then the `idp_entityid`\n  # option must be set to the entity to redirect users to.\n  #\n  # Most deployments only have a single IdP entity and so should omit this\n  # option.\n  #\n  #idp_entityid: 'https://our_idp/entityid'\n\n\n# Enable OpenID Connect (OIDC) / OAuth 2.0 for registration and login.\n#\n# See https://github.com/matrix-org/synapse/blob/master/docs/openid.md\n# for some example configurations.\n#\noidc_config:\n  # Uncomment the following to enable authorization against an OpenID Connect\n  # server. Defaults to false.\n  #\n  #enabled: true\n\n  # Uncomment the following to disable use of the OIDC discovery mechanism to\n  # discover endpoints. Defaults to true.\n  #\n  #discover: false\n\n  # the OIDC issuer. Used to validate tokens and (if discovery is enabled) to\n  # discover the provider's endpoints.\n  #\n  # Required if 'enabled' is true.\n  #\n  #issuer: \"https://accounts.example.com/\"\n\n  # oauth2 client id to use.\n  #\n  # Required if 'enabled' is true.\n  #\n  #client_id: \"provided-by-your-issuer\"\n\n  # oauth2 client secret to use.\n  #\n  # Required if 'enabled' is true.\n  #\n  #client_secret: \"provided-by-your-issuer\"\n\n  # auth method to use when exchanging the token.\n  # Valid values are 'client_secret_basic' (default), 'client_secret_post' and\n  # 'none'.\n  #\n  #client_auth_method: client_secret_post\n\n  # list of scopes to request. This should normally include the \"openid\" scope.\n  # Defaults to [\"openid\"].\n  #\n  #scopes: [\"openid\", \"profile\"]\n\n  # the oauth2 authorization endpoint. Required if provider discovery is disabled.\n  #\n  #authorization_endpoint: \"https://accounts.example.com/oauth2/auth\"\n\n  # the oauth2 token endpoint. Required if provider discovery is disabled.\n  #\n  #token_endpoint: \"https://accounts.example.com/oauth2/token\"\n\n  # the OIDC userinfo endpoint. Required if discovery is disabled and the\n  # \"openid\" scope is not requested.\n  #\n  #userinfo_endpoint: \"https://accounts.example.com/userinfo\"\n\n  # URI where to fetch the JWKS. Required if discovery is disabled and the\n  # \"openid\" scope is used.\n  #\n  #jwks_uri: \"https://accounts.example.com/.well-known/jwks.json\"\n\n  # Uncomment to skip metadata verification. Defaults to false.\n  #\n  # Use this if you are connecting to a provider that is not OpenID Connect\n  # compliant.\n  # Avoid this in production.\n  #\n  #skip_verification: true\n\n  # Whether to fetch the user profile from the userinfo endpoint. Valid\n  # values are: \"auto\" or \"userinfo_endpoint\".\n  #\n  # Defaults to \"auto\", which fetches the userinfo endpoint if \"openid\" is included\n  # in `scopes`. Uncomment the following to always fetch the userinfo endpoint.\n  #\n  #user_profile_method: \"userinfo_endpoint\"\n\n  # Uncomment to allow a user logging in via OIDC to match a pre-existing account instead\n  # of failing. This could be used if switching from password logins to OIDC. Defaults to false.\n  #\n  #allow_existing_users: true\n\n  # An external module can be provided here as a custom solution to mapping\n  # attributes returned from a OIDC provider onto a matrix user.\n  #\n  user_mapping_provider:\n    # The custom module's class. Uncomment to use a custom module.\n    # Default is 'synapse.handlers.oidc_handler.JinjaOidcMappingProvider'.\n    #\n    # See https://github.com/matrix-org/synapse/blob/master/docs/sso_mapping_providers.md#openid-mapping-providers\n    # for information on implementing a custom mapping provider.\n    #\n    #module: mapping_provider.OidcMappingProvider\n\n    # Custom configuration values for the module. This section will be passed as\n    # a Python dictionary to the user mapping provider module's `parse_config`\n    # method.\n    #\n    # The examples below are intended for the default provider: they should be\n    # changed if using a custom provider.\n    #\n    config:\n      # name of the claim containing a unique identifier for the user.\n      # Defaults to `sub`, which OpenID Connect compliant providers should provide.\n      #\n      #subject_claim: \"sub\"\n\n      # Jinja2 template for the localpart of the MXID.\n      #\n      # When rendering, this template is given the following variables:\n      #   * user: The claims returned by the UserInfo Endpoint and/or in the ID\n      #     Token\n      #\n      # This must be configured if using the default mapping provider.\n      #\n      localpart_template: \"{{ user.preferred_username }}\"\n\n      # Jinja2 template for the display name to set on first login.\n      #\n      # If unset, no displayname will be set.\n      #\n      #display_name_template: \"{{ user.given_name }} {{ user.last_name }}\"\n\n      # Jinja2 templates for extra attributes to send back to the client during\n      # login.\n      #\n      # Note that these are non-standard and clients will ignore them without modifications.\n      #\n      #extra_attributes:\n        #birthdate: \"{{ user.birthdate }}\"\n\n\n\n# Enable Central Authentication Service (CAS) for registration and login.\n#\ncas_config:\n  # Uncomment the following to enable authorization against a CAS server.\n  # Defaults to false.\n  #\n  #enabled: true\n\n  # The URL of the CAS authorization endpoint.\n  #\n  #server_url: \"https://cas-server.com\"\n\n  # The public URL of the homeserver.\n  #\n  #service_url: \"https://homeserver.domain.com:8448\"\n\n  # The attribute of the CAS response to use as the display name.\n  #\n  # If unset, no displayname will be set.\n  #\n  #displayname_attribute: name\n\n  # It is possible to configure Synapse to only allow logins if CAS attributes\n  # match particular values. All of the keys in the mapping below must exist\n  # and the values must match the given value. Alternately if the given value\n  # is None then any value is allowed (the attribute just must exist).\n  # All of the listed attributes must match for the login to be permitted.\n  #\n  #required_attributes:\n  #  userGroup: \"staff\"\n  #  department: None\n\n\n# Additional settings to use with single-sign on systems such as OpenID Connect,\n# SAML2 and CAS.\n#\nsso:\n    # A list of client URLs which are whitelisted so that the user does not\n    # have to confirm giving access to their account to the URL. Any client\n    # whose URL starts with an entry in the following list will not be subject\n    # to an additional confirmation step after the SSO login is completed.\n    #\n    # WARNING: An entry such as \"https://my.client\" is insecure, because it\n    # will also match \"https://my.client.evil.site\", exposing your users to\n    # phishing attacks from evil.site. To avoid this, include a slash after the\n    # hostname: \"https://my.client/\".\n    #\n    # If public_baseurl is set, then the login fallback page (used by clients\n    # that don't natively support the required login flows) is whitelisted in\n    # addition to any URLs in this list.\n    #\n    # By default, this list is empty.\n    #\n    #client_whitelist:\n    #  - https://riot.im/develop\n    #  - https://my.custom.client/\n\n    # Directory in which Synapse will try to find the template files below.\n    # If not set, default templates from within the Synapse package will be used.\n    #\n    # DO NOT UNCOMMENT THIS SETTING unless you want to customise the templates.\n    # If you *do* uncomment it, you will need to make sure that all the templates\n    # below are in the directory.\n    #\n    # Synapse will look for the following templates in this directory:\n    #\n    # * HTML page for a confirmation step before redirecting back to the client\n    #   with the login token: 'sso_redirect_confirm.html'.\n    #\n    #   When rendering, this template is given three variables:\n    #     * redirect_url: the URL the user is about to be redirected to. Needs\n    #                     manual escaping (see\n    #                     https://jinja.palletsprojects.com/en/2.11.x/templates/#html-escaping).\n    #\n    #     * display_url: the same as `redirect_url`, but with the query\n    #                    parameters stripped. The intention is to have a\n    #                    human-readable URL to show to users, not to use it as\n    #                    the final address to redirect to. Needs manual escaping\n    #                    (see https://jinja.palletsprojects.com/en/2.11.x/templates/#html-escaping).\n    #\n    #     * server_name: the homeserver's name.\n    #\n    # * HTML page which notifies the user that they are authenticating to confirm\n    #   an operation on their account during the user interactive authentication\n    #   process: 'sso_auth_confirm.html'.\n    #\n    #   When rendering, this template is given the following variables:\n    #     * redirect_url: the URL the user is about to be redirected to. Needs\n    #                     manual escaping (see\n    #                     https://jinja.palletsprojects.com/en/2.11.x/templates/#html-escaping).\n    #\n    #     * description: the operation which the user is being asked to confirm\n    #\n    # * HTML page shown after a successful user interactive authentication session:\n    #   'sso_auth_success.html'.\n    #\n    #   Note that this page must include the JavaScript which notifies of a successful authentication\n    #   (see https://matrix.org/docs/spec/client_server/r0.6.0#fallback).\n    #\n    #   This template has no additional variables.\n    #\n    # * HTML page shown during single sign-on if a deactivated user (according to Synapse's database)\n    #   attempts to login: 'sso_account_deactivated.html'.\n    #\n    #   This template has no additional variables.\n    #\n    # * HTML page to display to users if something goes wrong during the\n    #   OpenID Connect authentication process: 'sso_error.html'.\n    #\n    #   When rendering, this template is given two variables:\n    #     * error: the technical name of the error\n    #     * error_description: a human-readable message for the error\n    #\n    # You can see the default templates at:\n    # https://github.com/matrix-org/synapse/tree/master/synapse/res/templates\n    #\n    #template_dir: \"res/templates\"\n\n\n# JSON web token integration. The following settings can be used to make\n# Synapse JSON web tokens for authentication, instead of its internal\n# password database.\n#\n# Each JSON Web Token needs to contain a \"sub\" (subject) claim, which is\n# used as the localpart of the mxid.\n#\n# Additionally, the expiration time (\"exp\"), not before time (\"nbf\"),\n# and issued at (\"iat\") claims are validated if present.\n#\n# Note that this is a non-standard login type and client support is\n# expected to be non-existent.\n#\n# See https://github.com/matrix-org/synapse/blob/master/docs/jwt.md.\n#\n#jwt_config:\n    # Uncomment the following to enable authorization using JSON web\n    # tokens. Defaults to false.\n    #\n    #enabled: true\n\n    # This is either the private shared secret or the public key used to\n    # decode the contents of the JSON web token.\n    #\n    # Required if 'enabled' is true.\n    #\n    #secret: \"provided-by-your-issuer\"\n\n    # The algorithm used to sign the JSON web token.\n    #\n    # Supported algorithms are listed at\n    # https://pyjwt.readthedocs.io/en/latest/algorithms.html\n    #\n    # Required if 'enabled' is true.\n    #\n    #algorithm: \"provided-by-your-issuer\"\n\n    # The issuer to validate the \"iss\" claim against.\n    #\n    # Optional, if provided the \"iss\" claim will be required and\n    # validated for all JSON web tokens.\n    #\n    #issuer: \"provided-by-your-issuer\"\n\n    # A list of audiences to validate the \"aud\" claim against.\n    #\n    # Optional, if provided the \"aud\" claim will be required and\n    # validated for all JSON web tokens.\n    #\n    # Note that if the \"aud\" claim is included in a JSON web token then\n    # validation will fail without configuring audiences.\n    #\n    #audiences:\n    #    - \"provided-by-your-issuer\"\n\n\npassword_config:\n   # Uncomment to disable password login\n   #\n   #enabled: false\n\n   # Uncomment to disable authentication against the local password\n   # database. This is ignored if `enabled` is false, and is only useful\n   # if you have other password_providers.\n   #\n   #localdb_enabled: false\n\n   # Uncomment and change to a secret random string for extra security.\n   # DO NOT CHANGE THIS AFTER INITIAL SETUP!\n   #\n   #pepper: \"EVEN_MORE_SECRET\"\n\n   # Define and enforce a password policy. Each parameter is optional.\n   # This is an implementation of MSC2000.\n   #\n   policy:\n      # Whether to enforce the password policy.\n      # Defaults to 'false'.\n      #\n      #enabled: true\n\n      # Minimum accepted length for a password.\n      # Defaults to 0.\n      #\n      #minimum_length: 15\n\n      # Whether a password must contain at least one digit.\n      # Defaults to 'false'.\n      #\n      #require_digit: true\n\n      # Whether a password must contain at least one symbol.\n      # A symbol is any character that's not a number or a letter.\n      # Defaults to 'false'.\n      #\n      #require_symbol: true\n\n      # Whether a password must contain at least one lowercase letter.\n      # Defaults to 'false'.\n      #\n      #require_lowercase: true\n\n      # Whether a password must contain at least one lowercase letter.\n      # Defaults to 'false'.\n      #\n      #require_uppercase: true\n\n\n# Configuration for sending emails from Synapse.\n#\nemail:\n  # The hostname of the outgoing SMTP server to use. Defaults to 'localhost'.\n  #\n  #smtp_host: mail.server\n\n  # The port on the mail server for outgoing SMTP. Defaults to 25.\n  #\n  #smtp_port: 587\n\n  # Username/password for authentication to the SMTP server. By default, no\n  # authentication is attempted.\n  #\n  #smtp_user: \"exampleusername\"\n  #smtp_pass: \"examplepassword\"\n\n  # Uncomment the following to require TLS transport security for SMTP.\n  # By default, Synapse will connect over plain text, and will then switch to\n  # TLS via STARTTLS *if the SMTP server supports it*. If this option is set,\n  # Synapse will refuse to connect unless the server supports STARTTLS.\n  #\n  #require_transport_security: true\n\n  # notif_from defines the \"From\" address to use when sending emails.\n  # It must be set if email sending is enabled.\n  #\n  # The placeholder '%(app)s' will be replaced by the application name,\n  # which is normally 'app_name' (below), but may be overridden by the\n  # Matrix client application.\n  #\n  # Note that the placeholder must be written '%(app)s', including the\n  # trailing 's'.\n  #\n  #notif_from: \"Your Friendly %(app)s homeserver <noreply@example.com>\"\n\n  # app_name defines the default value for '%(app)s' in notif_from and email\n  # subjects. It defaults to 'Matrix'.\n  #\n  #app_name: my_branded_matrix_server\n\n  # Uncomment the following to enable sending emails for messages that the user\n  # has missed. Disabled by default.\n  #\n  #enable_notifs: true\n\n  # Uncomment the following to disable automatic subscription to email\n  # notifications for new users. Enabled by default.\n  #\n  #notif_for_new_users: false\n\n  # Custom URL for client links within the email notifications. By default\n  # links will be based on \"https://matrix.to\".\n  #\n  # (This setting used to be called riot_base_url; the old name is still\n  # supported for backwards-compatibility but is now deprecated.)\n  #\n  #client_base_url: \"http://localhost/riot\"\n\n  # Configure the time that a validation email will expire after sending.\n  # Defaults to 1h.\n  #\n  #validation_token_lifetime: 15m\n\n  # Directory in which Synapse will try to find the template files below.\n  # If not set, default templates from within the Synapse package will be used.\n  #\n  # Do not uncomment this setting unless you want to customise the templates.\n  #\n  # Synapse will look for the following templates in this directory:\n  #\n  # * The contents of email notifications of missed events: 'notif_mail.html' and\n  #   'notif_mail.txt'.\n  #\n  # * The contents of account expiry notice emails: 'notice_expiry.html' and\n  #   'notice_expiry.txt'.\n  #\n  # * The contents of password reset emails sent by the homeserver:\n  #   'password_reset.html' and 'password_reset.txt'\n  #\n  # * An HTML page that a user will see when they follow the link in the password\n  #   reset email. The user will be asked to confirm the action before their\n  #   password is reset: 'password_reset_confirmation.html'\n  #\n  # * HTML pages for success and failure that a user will see when they confirm\n  #   the password reset flow using the page above: 'password_reset_success.html'\n  #   and 'password_reset_failure.html'\n  #\n  # * The contents of address verification emails sent during registration:\n  #   'registration.html' and 'registration.txt'\n  #\n  # * HTML pages for success and failure that a user will see when they follow\n  #   the link in an address verification email sent during registration:\n  #   'registration_success.html' and 'registration_failure.html'\n  #\n  # * The contents of address verification emails sent when an address is added\n  #   to a Matrix account: 'add_threepid.html' and 'add_threepid.txt'\n  #\n  # * HTML pages for success and failure that a user will see when they follow\n  #   the link in an address verification email sent when an address is added\n  #   to a Matrix account: 'add_threepid_success.html' and\n  #   'add_threepid_failure.html'\n  #\n  # You can see the default templates at:\n  # https://github.com/matrix-org/synapse/tree/master/synapse/res/templates\n  #\n  #template_dir: \"res/templates\"\n\n  # Subjects to use when sending emails from Synapse.\n  #\n  # The placeholder '%(app)s' will be replaced with the value of the 'app_name'\n  # setting above, or by a value dictated by the Matrix client application.\n  #\n  # If a subject isn't overridden in this configuration file, the value used as\n  # its example will be used.\n  #\n  #subjects:\n\n    # Subjects for notification emails.\n    #\n    # On top of the '%(app)s' placeholder, these can use the following\n    # placeholders:\n    #\n    #   * '%(person)s', which will be replaced by the display name of the user(s)\n    #      that sent the message(s), e.g. \"Alice and Bob\".\n    #   * '%(room)s', which will be replaced by the name of the room the\n    #      message(s) have been sent to, e.g. \"My super room\".\n    #\n    # See the example provided for each setting to see which placeholder can be\n    # used and how to use them.\n    #\n    # Subject to use to notify about one message from one or more user(s) in a\n    # room which has a name.\n    #message_from_person_in_room: \"[%(app)s] You have a message on %(app)s from %(person)s in the %(room)s room...\"\n    #\n    # Subject to use to notify about one message from one or more user(s) in a\n    # room which doesn't have a name.\n    #message_from_person: \"[%(app)s] You have a message on %(app)s from %(person)s...\"\n    #\n    # Subject to use to notify about multiple messages from one or more users in\n    # a room which doesn't have a name.\n    #messages_from_person: \"[%(app)s] You have messages on %(app)s from %(person)s...\"\n    #\n    # Subject to use to notify about multiple messages in a room which has a\n    # name.\n    #messages_in_room: \"[%(app)s] You have messages on %(app)s in the %(room)s room...\"\n    #\n    # Subject to use to notify about multiple messages in multiple rooms.\n    #messages_in_room_and_others: \"[%(app)s] You have messages on %(app)s in the %(room)s room and others...\"\n    #\n    # Subject to use to notify about multiple messages from multiple persons in\n    # multiple rooms. This is similar to the setting above except it's used when\n    # the room in which the notification was triggered has no name.\n    #messages_from_person_and_others: \"[%(app)s] You have messages on %(app)s from %(person)s and others...\"\n    #\n    # Subject to use to notify about an invite to a room which has a name.\n    #invite_from_person_to_room: \"[%(app)s] %(person)s has invited you to join the %(room)s room on %(app)s...\"\n    #\n    # Subject to use to notify about an invite to a room which doesn't have a\n    # name.\n    #invite_from_person: \"[%(app)s] %(person)s has invited you to chat on %(app)s...\"\n\n    # Subject for emails related to account administration.\n    #\n    # On top of the '%(app)s' placeholder, these one can use the\n    # '%(server_name)s' placeholder, which will be replaced by the value of the\n    # 'server_name' setting in your Synapse configuration.\n    #\n    # Subject to use when sending a password reset email.\n    #password_reset: \"[%(server_name)s] Password reset\"\n    #\n    # Subject to use when sending a verification email to assert an address's\n    # ownership.\n    #email_validation: \"[%(server_name)s] Validate your email\"\n\n\n# Password providers allow homeserver administrators to integrate\n# their Synapse installation with existing authentication methods\n# ex. LDAP, external tokens, etc.\n#\n# For more information and known implementations, please see\n# https://github.com/matrix-org/synapse/blob/master/docs/password_auth_providers.md\n#\n# Note: instances wishing to use SAML or CAS authentication should\n# instead use the `saml2_config` or `cas_config` options,\n# respectively.\n#\npassword_providers:\n#    # Example config for an LDAP auth provider\n#    - module: \"ldap_auth_provider.LdapAuthProvider\"\n#      config:\n#        enabled: true\n#        uri: \"ldap://ldap.example.com:389\"\n#        start_tls: true\n#        base: \"ou=users,dc=example,dc=com\"\n#        attributes:\n#           uid: \"cn\"\n#           mail: \"email\"\n#           name: \"givenName\"\n#        #bind_dn:\n#        #bind_password:\n#        #filter: \"(objectClass=posixAccount)\"\n\n\n\n## Push ##\n\npush:\n  # Clients requesting push notifications can either have the body of\n  # the message sent in the notification poke along with other details\n  # like the sender, or just the event ID and room ID (`event_id_only`).\n  # If clients choose the former, this option controls whether the\n  # notification request includes the content of the event (other details\n  # like the sender are still included). For `event_id_only` push, it\n  # has no effect.\n  #\n  # For modern android devices the notification content will still appear\n  # because it is loaded by the app. iPhone, however will send a\n  # notification saying only that a message arrived and who it came from.\n  #\n  # The default value is \"true\" to include message details. Uncomment to only\n  # include the event ID and room ID in push notification payloads.\n  #\n  #include_content: false\n\n  # When a push notification is received, an unread count is also sent.\n  # This number can either be calculated as the number of unread messages\n  # for the user, or the number of *rooms* the user has unread messages in.\n  #\n  # The default value is \"true\", meaning push clients will see the number of\n  # rooms with unread messages in them. Uncomment to instead send the number\n  # of unread messages.\n  #\n  #group_unread_count_by_room: false\n\n\n# Spam checkers are third-party modules that can block specific actions\n# of local users, such as creating rooms and registering undesirable\n# usernames, as well as remote users by redacting incoming events.\n#\nspam_checker:\n   #- module: \"my_custom_project.SuperSpamChecker\"\n   #  config:\n   #    example_option: 'things'\n   #- module: \"some_other_project.BadEventStopper\"\n   #  config:\n   #    example_stop_events_from: ['@bad:example.com']\n\n\n## Rooms ##\n\n# Controls whether locally-created rooms should be end-to-end encrypted by\n# default.\n#\n# Possible options are \"all\", \"invite\", and \"off\". They are defined as:\n#\n# * \"all\": any locally-created room\n# * \"invite\": any room created with the \"private_chat\" or \"trusted_private_chat\"\n#             room creation presets\n# * \"off\": this option will take no effect\n#\n# The default value is \"off\".\n#\n# Note that this option will only affect rooms created after it is set. It\n# will also not affect rooms created by other servers.\n#\n#encryption_enabled_by_default_for_room_type: invite\n\n\n# Uncomment to allow non-server-admin users to create groups on this server\n#\n#enable_group_creation: true\n\n# If enabled, non server admins can only create groups with local parts\n# starting with this prefix\n#\n#group_creation_prefix: \"unofficial/\"\n\n\n\n# User Directory configuration\n#\n# 'enabled' defines whether users can search the user directory. If\n# false then empty responses are returned to all queries. Defaults to\n# true.\n#\n# 'search_all_users' defines whether to search all users visible to your HS\n# when searching the user directory, rather than limiting to users visible\n# in public rooms.  Defaults to false.  If you set it True, you'll have to\n# rebuild the user_directory search indexes, see\n# https://github.com/matrix-org/synapse/blob/master/docs/user_directory.md\n#\n#user_directory:\n#  enabled: true\n#  search_all_users: false\n\n\n# User Consent configuration\n#\n# for detailed instructions, see\n# https://github.com/matrix-org/synapse/blob/master/docs/consent_tracking.md\n#\n# Parts of this section are required if enabling the 'consent' resource under\n# 'listeners', in particular 'template_dir' and 'version'.\n#\n# 'template_dir' gives the location of the templates for the HTML forms.\n# This directory should contain one subdirectory per language (eg, 'en', 'fr'),\n# and each language directory should contain the policy document (named as\n# '<version>.html') and a success page (success.html).\n#\n# 'version' specifies the 'current' version of the policy document. It defines\n# the version to be served by the consent resource if there is no 'v'\n# parameter.\n#\n# 'server_notice_content', if enabled, will send a user a \"Server Notice\"\n# asking them to consent to the privacy policy. The 'server_notices' section\n# must also be configured for this to work. Notices will *not* be sent to\n# guest users unless 'send_server_notice_to_guests' is set to true.\n#\n# 'block_events_error', if set, will block any attempts to send events\n# until the user consents to the privacy policy. The value of the setting is\n# used as the text of the error.\n#\n# 'require_at_registration', if enabled, will add a step to the registration\n# process, similar to how captcha works. Users will be required to accept the\n# policy before their account is created.\n#\n# 'policy_name' is the display name of the policy users will see when registering\n# for an account. Has no effect unless `require_at_registration` is enabled.\n# Defaults to \"Privacy Policy\".\n#\n#user_consent:\n#  template_dir: res/templates/privacy\n#  version: 1.0\n#  server_notice_content:\n#    msgtype: m.text\n#    body: >-\n#      To continue using this homeserver you must review and agree to the\n#      terms and conditions at %(consent_uri)s\n#  send_server_notice_to_guests: true\n#  block_events_error: >-\n#    To continue using this homeserver you must review and agree to the\n#    terms and conditions at %(consent_uri)s\n#  require_at_registration: false\n#  policy_name: Privacy Policy\n#\n\n\n\n# Local statistics collection. Used in populating the room directory.\n#\n# 'bucket_size' controls how large each statistics timeslice is. It can\n# be defined in a human readable short form -- e.g. \"1d\", \"1y\".\n#\n# 'retention' controls how long historical statistics will be kept for.\n# It can be defined in a human readable short form -- e.g. \"1d\", \"1y\".\n#\n#\n#stats:\n#   enabled: true\n#   bucket_size: 1d\n#   retention: 1y\n\n\n# Server Notices room configuration\n#\n# Uncomment this section to enable a room which can be used to send notices\n# from the server to users. It is a special room which cannot be left; notices\n# come from a special \"notices\" user id.\n#\n# If you uncomment this section, you *must* define the system_mxid_localpart\n# setting, which defines the id of the user which will be used to send the\n# notices.\n#\n# It's also possible to override the room name, the display name of the\n# \"notices\" user, and the avatar for the user.\n#\n#server_notices:\n#  system_mxid_localpart: notices\n#  system_mxid_display_name: \"Server Notices\"\n#  system_mxid_avatar_url: \"mxc://server.com/oumMVlgDnLYFaPVkExemNVVZ\"\n#  room_name: \"Server Notices\"\n\n\n\n# Uncomment to disable searching the public room list. When disabled\n# blocks searching local and remote room lists for local and remote\n# users by always returning an empty list for all queries.\n#\n#enable_room_list_search: false\n\n# The `alias_creation` option controls who's allowed to create aliases\n# on this server.\n#\n# The format of this option is a list of rules that contain globs that\n# match against user_id, room_id and the new alias (fully qualified with\n# server name). The action in the first rule that matches is taken,\n# which can currently either be \"allow\" or \"deny\".\n#\n# Missing user_id/room_id/alias fields default to \"*\".\n#\n# If no rules match the request is denied. An empty list means no one\n# can create aliases.\n#\n# Options for the rules include:\n#\n#   user_id: Matches against the creator of the alias\n#   alias: Matches against the alias being created\n#   room_id: Matches against the room ID the alias is being pointed at\n#   action: Whether to \"allow\" or \"deny\" the request if the rule matches\n#\n# The default is:\n#\n#alias_creation_rules:\n#  - user_id: \"*\"\n#    alias: \"*\"\n#    room_id: \"*\"\n#    action: allow\n\n# The `room_list_publication_rules` option controls who can publish and\n# which rooms can be published in the public room list.\n#\n# The format of this option is the same as that for\n# `alias_creation_rules`.\n#\n# If the room has one or more aliases associated with it, only one of\n# the aliases needs to match the alias rule. If there are no aliases\n# then only rules with `alias: *` match.\n#\n# If no rules match the request is denied. An empty list means no one\n# can publish rooms.\n#\n# Options for the rules include:\n#\n#   user_id: Matches against the creator of the alias\n#   room_id: Matches against the room ID being published\n#   alias: Matches against any current local or canonical aliases\n#            associated with the room\n#   action: Whether to \"allow\" or \"deny\" the request if the rule matches\n#\n# The default is:\n#\n#room_list_publication_rules:\n#  - user_id: \"*\"\n#    alias: \"*\"\n#    room_id: \"*\"\n#    action: allow\n\n\n# Server admins can define a Python module that implements extra rules for\n# allowing or denying incoming events. In order to work, this module needs to\n# override the methods defined in synapse/events/third_party_rules.py.\n#\n# This feature is designed to be used in closed federations only, where each\n# participating server enforces the same rules.\n#\n#third_party_event_rules:\n#  module: \"my_custom_project.SuperRulesSet\"\n#  config:\n#    example_option: 'things'\n\n\n## Opentracing ##\n\n# These settings enable opentracing, which implements distributed tracing.\n# This allows you to observe the causal chains of events across servers\n# including requests, key lookups etc., across any server running\n# synapse or any other other services which supports opentracing\n# (specifically those implemented with Jaeger).\n#\nopentracing:\n    # tracing is disabled by default. Uncomment the following line to enable it.\n    #\n    #enabled: true\n\n    # The list of homeservers we wish to send and receive span contexts and span baggage.\n    # See docs/opentracing.rst\n    # This is a list of regexes which are matched against the server_name of the\n    # homeserver.\n    #\n    # By default, it is empty, so no servers are matched.\n    #\n    #homeserver_whitelist:\n    #  - \".*\"\n\n    # Jaeger can be configured to sample traces at different rates.\n    # All configuration options provided by Jaeger can be set here.\n    # Jaeger's configuration mostly related to trace sampling which\n    # is documented here:\n    # https://www.jaegertracing.io/docs/1.13/sampling/.\n    #\n    #jaeger_config:\n    #  sampler:\n    #    type: const\n    #    param: 1\n\n    #  Logging whether spans were started and reported\n    #\n    #  logging:\n    #    false\n\n\n## Workers ##\n\n# Disables sending of outbound federation transactions on the main process.\n# Uncomment if using a federation sender worker.\n#\n#send_federation: false\n\n# It is possible to run multiple federation sender workers, in which case the\n# work is balanced across them.\n#\n# This configuration must be shared between all federation sender workers, and if\n# changed all federation sender workers must be stopped at the same time and then\n# started, to ensure that all instances are running with the same config (otherwise\n# events may be dropped).\n#\n#federation_sender_instances:\n#  - federation_sender1\n\n# When using workers this should be a map from `worker_name` to the\n# HTTP replication listener of the worker, if configured.\n#\n#instance_map:\n#  worker1:\n#    host: localhost\n#    port: 8034\n\n# Experimental: When using workers you can define which workers should\n# handle event persistence and typing notifications. Any worker\n# specified here must also be in the `instance_map`.\n#\n#stream_writers:\n#  events: worker1\n#  typing: worker1\n\n# The worker that is used to run background tasks (e.g. cleaning up expired\n# data). If not provided this defaults to the main process.\n#\n#run_background_tasks_on: worker1\n\n\n# Configuration for Redis when using workers. This *must* be enabled when\n# using workers (unless using old style direct TCP configuration).\n#\nredis:\n  # Uncomment the below to enable Redis support.\n  #\n  #enabled: true\n\n  # Optional host and port to use to connect to redis. Defaults to\n  # localhost and 6379\n  #\n  #host: localhost\n  #port: 6379\n\n  # Optional password if configured on the Redis instance\n  #\n  #password: <secret_password>\n", "code_before": "# This file is maintained as an up-to-date snapshot of the default\n# homeserver.yaml configuration generated by Synapse.\n#\n# It is intended to act as a reference for the default configuration,\n# helping admins keep track of new options and other changes, and compare\n# their configs with the current default.  As such, many of the actual\n# config values shown are placeholders.\n#\n# It is *not* intended to be copied and used as the basis for a real\n# homeserver.yaml. Instead, if you are starting from scratch, please generate\n# a fresh config using Synapse by following the instructions in INSTALL.md.\n\n# Configuration options that take a time period can be set using a number\n# followed by a letter. Letters have the following meanings:\n# s = second\n# m = minute\n# h = hour\n# d = day\n# w = week\n# y = year\n# For example, setting redaction_retention_period: 5m would remove redacted\n# messages from the database after 5 minutes, rather than 5 months.\n\n################################################################################\n\n# Configuration file for Synapse.\n#\n# This is a YAML file: see [1] for a quick introduction. Note in particular\n# that *indentation is important*: all the elements of a list or dictionary\n# should have the same indentation.\n#\n# [1] https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html\n\n## Server ##\n\n# The public-facing domain of the server\n#\n# The server_name name will appear at the end of usernames and room addresses\n# created on this server. For example if the server_name was example.com,\n# usernames on this server would be in the format @user:example.com\n#\n# In most cases you should avoid using a matrix specific subdomain such as\n# matrix.example.com or synapse.example.com as the server_name for the same\n# reasons you wouldn't use user@email.example.com as your email address.\n# See https://github.com/matrix-org/synapse/blob/master/docs/delegate.md\n# for information on how to host Synapse on a subdomain while preserving\n# a clean server_name.\n#\n# The server_name cannot be changed later so it is important to\n# configure this correctly before you start Synapse. It should be all\n# lowercase and may contain an explicit port.\n# Examples: matrix.org, localhost:8080\n#\nserver_name: \"SERVERNAME\"\n\n# When running as a daemon, the file to store the pid in\n#\npid_file: DATADIR/homeserver.pid\n\n# The absolute URL to the web client which /_matrix/client will redirect\n# to if 'webclient' is configured under the 'listeners' configuration.\n#\n# This option can be also set to the filesystem path to the web client\n# which will be served at /_matrix/client/ if 'webclient' is configured\n# under the 'listeners' configuration, however this is a security risk:\n# https://github.com/matrix-org/synapse#security-note\n#\n#web_client_location: https://riot.example.com/\n\n# The public-facing base URL that clients use to access this HS\n# (not including _matrix/...). This is the same URL a user would\n# enter into the 'custom HS URL' field on their client. If you\n# use synapse with a reverse proxy, this should be the URL to reach\n# synapse via the proxy.\n#\n#public_baseurl: https://example.com/\n\n# Set the soft limit on the number of file descriptors synapse can use\n# Zero is used to indicate synapse should set the soft limit to the\n# hard limit.\n#\n#soft_file_limit: 0\n\n# Set to false to disable presence tracking on this homeserver.\n#\n#use_presence: false\n\n# Whether to require authentication to retrieve profile data (avatars,\n# display names) of other users through the client API. Defaults to\n# 'false'. Note that profile data is also available via the federation\n# API, so this setting is of limited value if federation is enabled on\n# the server.\n#\n#require_auth_for_profile_requests: true\n\n# Uncomment to require a user to share a room with another user in order\n# to retrieve their profile information. Only checked on Client-Server\n# requests. Profile requests from other servers should be checked by the\n# requesting server. Defaults to 'false'.\n#\n#limit_profile_requests_to_users_who_share_rooms: true\n\n# If set to 'true', removes the need for authentication to access the server's\n# public rooms directory through the client API, meaning that anyone can\n# query the room directory. Defaults to 'false'.\n#\n#allow_public_rooms_without_auth: true\n\n# If set to 'true', allows any other homeserver to fetch the server's public\n# rooms directory via federation. Defaults to 'false'.\n#\n#allow_public_rooms_over_federation: true\n\n# The default room version for newly created rooms.\n#\n# Known room versions are listed here:\n# https://matrix.org/docs/spec/#complete-list-of-room-versions\n#\n# For example, for room version 1, default_room_version should be set\n# to \"1\".\n#\n#default_room_version: \"6\"\n\n# The GC threshold parameters to pass to `gc.set_threshold`, if defined\n#\n#gc_thresholds: [700, 10, 10]\n\n# Set the limit on the returned events in the timeline in the get\n# and sync operations. The default value is 100. -1 means no upper limit.\n#\n# Uncomment the following to increase the limit to 5000.\n#\n#filter_timeline_limit: 5000\n\n# Whether room invites to users on this server should be blocked\n# (except those sent by local server admins). The default is False.\n#\n#block_non_admin_invites: true\n\n# Room searching\n#\n# If disabled, new messages will not be indexed for searching and users\n# will receive errors when searching for messages. Defaults to enabled.\n#\n#enable_search: false\n\n# List of ports that Synapse should listen on, their purpose and their\n# configuration.\n#\n# Options for each listener include:\n#\n#   port: the TCP port to bind to\n#\n#   bind_addresses: a list of local addresses to listen on. The default is\n#       'all local interfaces'.\n#\n#   type: the type of listener. Normally 'http', but other valid options are:\n#       'manhole' (see docs/manhole.md),\n#       'metrics' (see docs/metrics-howto.md),\n#       'replication' (see docs/workers.md).\n#\n#   tls: set to true to enable TLS for this listener. Will use the TLS\n#       key/cert specified in tls_private_key_path / tls_certificate_path.\n#\n#   x_forwarded: Only valid for an 'http' listener. Set to true to use the\n#       X-Forwarded-For header as the client IP. Useful when Synapse is\n#       behind a reverse-proxy.\n#\n#   resources: Only valid for an 'http' listener. A list of resources to host\n#       on this port. Options for each resource are:\n#\n#       names: a list of names of HTTP resources. See below for a list of\n#           valid resource names.\n#\n#       compress: set to true to enable HTTP compression for this resource.\n#\n#   additional_resources: Only valid for an 'http' listener. A map of\n#        additional endpoints which should be loaded via dynamic modules.\n#\n# Valid resource names are:\n#\n#   client: the client-server API (/_matrix/client), and the synapse admin\n#       API (/_synapse/admin). Also implies 'media' and 'static'.\n#\n#   consent: user consent forms (/_matrix/consent). See\n#       docs/consent_tracking.md.\n#\n#   federation: the server-server API (/_matrix/federation). Also implies\n#       'media', 'keys', 'openid'\n#\n#   keys: the key discovery API (/_matrix/keys).\n#\n#   media: the media API (/_matrix/media).\n#\n#   metrics: the metrics interface. See docs/metrics-howto.md.\n#\n#   openid: OpenID authentication.\n#\n#   replication: the HTTP replication API (/_synapse/replication). See\n#       docs/workers.md.\n#\n#   static: static resources under synapse/static (/_matrix/static). (Mostly\n#       useful for 'fallback authentication'.)\n#\n#   webclient: A web client. Requires web_client_location to be set.\n#\nlisteners:\n  # TLS-enabled listener: for when matrix traffic is sent directly to synapse.\n  #\n  # Disabled by default. To enable it, uncomment the following. (Note that you\n  # will also need to give Synapse a TLS key and certificate: see the TLS section\n  # below.)\n  #\n  #- port: 8448\n  #  type: http\n  #  tls: true\n  #  resources:\n  #    - names: [client, federation]\n\n  # Unsecure HTTP listener: for when matrix traffic passes through a reverse proxy\n  # that unwraps TLS.\n  #\n  # If you plan to use a reverse proxy, please see\n  # https://github.com/matrix-org/synapse/blob/master/docs/reverse_proxy.md.\n  #\n  - port: 8008\n    tls: false\n    type: http\n    x_forwarded: true\n    bind_addresses: ['::1', '127.0.0.1']\n\n    resources:\n      - names: [client, federation]\n        compress: false\n\n    # example additional_resources:\n    #\n    #additional_resources:\n    #  \"/_matrix/my/custom/endpoint\":\n    #    module: my_module.CustomRequestHandler\n    #    config: {}\n\n  # Turn on the twisted ssh manhole service on localhost on the given\n  # port.\n  #\n  #- port: 9000\n  #  bind_addresses: ['::1', '127.0.0.1']\n  #  type: manhole\n\n# Forward extremities can build up in a room due to networking delays between\n# homeservers. Once this happens in a large room, calculation of the state of\n# that room can become quite expensive. To mitigate this, once the number of\n# forward extremities reaches a given threshold, Synapse will send an\n# org.matrix.dummy_event event, which will reduce the forward extremities\n# in the room.\n#\n# This setting defines the threshold (i.e. number of forward extremities in the\n# room) at which dummy events are sent. The default value is 10.\n#\n#dummy_events_threshold: 5\n\n\n## Homeserver blocking ##\n\n# How to reach the server admin, used in ResourceLimitError\n#\n#admin_contact: 'mailto:admin@server.com'\n\n# Global blocking\n#\n#hs_disabled: false\n#hs_disabled_message: 'Human readable reason for why the HS is blocked'\n\n# Monthly Active User Blocking\n#\n# Used in cases where the admin or server owner wants to limit to the\n# number of monthly active users.\n#\n# 'limit_usage_by_mau' disables/enables monthly active user blocking. When\n# enabled and a limit is reached the server returns a 'ResourceLimitError'\n# with error type Codes.RESOURCE_LIMIT_EXCEEDED\n#\n# 'max_mau_value' is the hard limit of monthly active users above which\n# the server will start blocking user actions.\n#\n# 'mau_trial_days' is a means to add a grace period for active users. It\n# means that users must be active for this number of days before they\n# can be considered active and guards against the case where lots of users\n# sign up in a short space of time never to return after their initial\n# session.\n#\n# 'mau_limit_alerting' is a means of limiting client side alerting\n# should the mau limit be reached. This is useful for small instances\n# where the admin has 5 mau seats (say) for 5 specific people and no\n# interest increasing the mau limit further. Defaults to True, which\n# means that alerting is enabled\n#\n#limit_usage_by_mau: false\n#max_mau_value: 50\n#mau_trial_days: 2\n#mau_limit_alerting: false\n\n# If enabled, the metrics for the number of monthly active users will\n# be populated, however no one will be limited. If limit_usage_by_mau\n# is true, this is implied to be true.\n#\n#mau_stats_only: false\n\n# Sometimes the server admin will want to ensure certain accounts are\n# never blocked by mau checking. These accounts are specified here.\n#\n#mau_limit_reserved_threepids:\n#  - medium: 'email'\n#    address: 'reserved_user@example.com'\n\n# Used by phonehome stats to group together related servers.\n#server_context: context\n\n# Resource-constrained homeserver settings\n#\n# When this is enabled, the room \"complexity\" will be checked before a user\n# joins a new remote room. If it is above the complexity limit, the server will\n# disallow joining, or will instantly leave.\n#\n# Room complexity is an arbitrary measure based on factors such as the number of\n# users in the room.\n#\nlimit_remote_rooms:\n  # Uncomment to enable room complexity checking.\n  #\n  #enabled: true\n\n  # the limit above which rooms cannot be joined. The default is 1.0.\n  #\n  #complexity: 0.5\n\n  # override the error which is returned when the room is too complex.\n  #\n  #complexity_error: \"This room is too complex.\"\n\n  # allow server admins to join complex rooms. Default is false.\n  #\n  #admins_can_join: true\n\n# Whether to require a user to be in the room to add an alias to it.\n# Defaults to 'true'.\n#\n#require_membership_for_aliases: false\n\n# Whether to allow per-room membership profiles through the send of membership\n# events with profile information that differ from the target's global profile.\n# Defaults to 'true'.\n#\n#allow_per_room_profiles: false\n\n# How long to keep redacted events in unredacted form in the database. After\n# this period redacted events get replaced with their redacted form in the DB.\n#\n# Defaults to `7d`. Set to `null` to disable.\n#\n#redaction_retention_period: 28d\n\n# How long to track users' last seen time and IPs in the database.\n#\n# Defaults to `28d`. Set to `null` to disable clearing out of old rows.\n#\n#user_ips_max_age: 14d\n\n# Message retention policy at the server level.\n#\n# Room admins and mods can define a retention period for their rooms using the\n# 'm.room.retention' state event, and server admins can cap this period by setting\n# the 'allowed_lifetime_min' and 'allowed_lifetime_max' config options.\n#\n# If this feature is enabled, Synapse will regularly look for and purge events\n# which are older than the room's maximum retention period. Synapse will also\n# filter events received over federation so that events that should have been\n# purged are ignored and not stored again.\n#\nretention:\n  # The message retention policies feature is disabled by default. Uncomment the\n  # following line to enable it.\n  #\n  #enabled: true\n\n  # Default retention policy. If set, Synapse will apply it to rooms that lack the\n  # 'm.room.retention' state event. Currently, the value of 'min_lifetime' doesn't\n  # matter much because Synapse doesn't take it into account yet.\n  #\n  #default_policy:\n  #  min_lifetime: 1d\n  #  max_lifetime: 1y\n\n  # Retention policy limits. If set, and the state of a room contains a\n  # 'm.room.retention' event in its state which contains a 'min_lifetime' or a\n  # 'max_lifetime' that's out of these bounds, Synapse will cap the room's policy\n  # to these limits when running purge jobs.\n  #\n  #allowed_lifetime_min: 1d\n  #allowed_lifetime_max: 1y\n\n  # Server admins can define the settings of the background jobs purging the\n  # events which lifetime has expired under the 'purge_jobs' section.\n  #\n  # If no configuration is provided, a single job will be set up to delete expired\n  # events in every room daily.\n  #\n  # Each job's configuration defines which range of message lifetimes the job\n  # takes care of. For example, if 'shortest_max_lifetime' is '2d' and\n  # 'longest_max_lifetime' is '3d', the job will handle purging expired events in\n  # rooms whose state defines a 'max_lifetime' that's both higher than 2 days, and\n  # lower than or equal to 3 days. Both the minimum and the maximum value of a\n  # range are optional, e.g. a job with no 'shortest_max_lifetime' and a\n  # 'longest_max_lifetime' of '3d' will handle every room with a retention policy\n  # which 'max_lifetime' is lower than or equal to three days.\n  #\n  # The rationale for this per-job configuration is that some rooms might have a\n  # retention policy with a low 'max_lifetime', where history needs to be purged\n  # of outdated messages on a more frequent basis than for the rest of the rooms\n  # (e.g. every 12h), but not want that purge to be performed by a job that's\n  # iterating over every room it knows, which could be heavy on the server.\n  #\n  # If any purge job is configured, it is strongly recommended to have at least\n  # a single job with neither 'shortest_max_lifetime' nor 'longest_max_lifetime'\n  # set, or one job without 'shortest_max_lifetime' and one job without\n  # 'longest_max_lifetime' set. Otherwise some rooms might be ignored, even if\n  # 'allowed_lifetime_min' and 'allowed_lifetime_max' are set, because capping a\n  # room's policy to these values is done after the policies are retrieved from\n  # Synapse's database (which is done using the range specified in a purge job's\n  # configuration).\n  #\n  #purge_jobs:\n  #  - longest_max_lifetime: 3d\n  #    interval: 12h\n  #  - shortest_max_lifetime: 3d\n  #    interval: 1d\n\n# Inhibits the /requestToken endpoints from returning an error that might leak\n# information about whether an e-mail address is in use or not on this\n# homeserver.\n# Note that for some endpoints the error situation is the e-mail already being\n# used, and for others the error is entering the e-mail being unused.\n# If this option is enabled, instead of returning an error, these endpoints will\n# act as if no error happened and return a fake session ID ('sid') to clients.\n#\n#request_token_inhibit_3pid_errors: true\n\n# A list of domains that the domain portion of 'next_link' parameters\n# must match.\n#\n# This parameter is optionally provided by clients while requesting\n# validation of an email or phone number, and maps to a link that\n# users will be automatically redirected to after validation\n# succeeds. Clients can make use this parameter to aid the validation\n# process.\n#\n# The whitelist is applied whether the homeserver or an\n# identity server is handling validation.\n#\n# The default value is no whitelist functionality; all domains are\n# allowed. Setting this value to an empty list will instead disallow\n# all domains.\n#\n#next_link_domain_whitelist: [\"matrix.org\"]\n\n\n## TLS ##\n\n# PEM-encoded X509 certificate for TLS.\n# This certificate, as of Synapse 1.0, will need to be a valid and verifiable\n# certificate, signed by a recognised Certificate Authority.\n#\n# See 'ACME support' below to enable auto-provisioning this certificate via\n# Let's Encrypt.\n#\n# If supplying your own, be sure to use a `.pem` file that includes the\n# full certificate chain including any intermediate certificates (for\n# instance, if using certbot, use `fullchain.pem` as your certificate,\n# not `cert.pem`).\n#\n#tls_certificate_path: \"CONFDIR/SERVERNAME.tls.crt\"\n\n# PEM-encoded private key for TLS\n#\n#tls_private_key_path: \"CONFDIR/SERVERNAME.tls.key\"\n\n# Whether to verify TLS server certificates for outbound federation requests.\n#\n# Defaults to `true`. To disable certificate verification, uncomment the\n# following line.\n#\n#federation_verify_certificates: false\n\n# The minimum TLS version that will be used for outbound federation requests.\n#\n# Defaults to `1`. Configurable to `1`, `1.1`, `1.2`, or `1.3`. Note\n# that setting this value higher than `1.2` will prevent federation to most\n# of the public Matrix network: only configure it to `1.3` if you have an\n# entirely private federation setup and you can ensure TLS 1.3 support.\n#\n#federation_client_minimum_tls_version: 1.2\n\n# Skip federation certificate verification on the following whitelist\n# of domains.\n#\n# This setting should only be used in very specific cases, such as\n# federation over Tor hidden services and similar. For private networks\n# of homeservers, you likely want to use a private CA instead.\n#\n# Only effective if federation_verify_certicates is `true`.\n#\n#federation_certificate_verification_whitelist:\n#  - lon.example.com\n#  - *.domain.com\n#  - *.onion\n\n# List of custom certificate authorities for federation traffic.\n#\n# This setting should only normally be used within a private network of\n# homeservers.\n#\n# Note that this list will replace those that are provided by your\n# operating environment. Certificates must be in PEM format.\n#\n#federation_custom_ca_list:\n#  - myCA1.pem\n#  - myCA2.pem\n#  - myCA3.pem\n\n# ACME support: This will configure Synapse to request a valid TLS certificate\n# for your configured `server_name` via Let's Encrypt.\n#\n# Note that ACME v1 is now deprecated, and Synapse currently doesn't support\n# ACME v2. This means that this feature currently won't work with installs set\n# up after November 2019. For more info, and alternative solutions, see\n# https://github.com/matrix-org/synapse/blob/master/docs/ACME.md#deprecation-of-acme-v1\n#\n# Note that provisioning a certificate in this way requires port 80 to be\n# routed to Synapse so that it can complete the http-01 ACME challenge.\n# By default, if you enable ACME support, Synapse will attempt to listen on\n# port 80 for incoming http-01 challenges - however, this will likely fail\n# with 'Permission denied' or a similar error.\n#\n# There are a couple of potential solutions to this:\n#\n#  * If you already have an Apache, Nginx, or similar listening on port 80,\n#    you can configure Synapse to use an alternate port, and have your web\n#    server forward the requests. For example, assuming you set 'port: 8009'\n#    below, on Apache, you would write:\n#\n#    ProxyPass /.well-known/acme-challenge http://localhost:8009/.well-known/acme-challenge\n#\n#  * Alternatively, you can use something like `authbind` to give Synapse\n#    permission to listen on port 80.\n#\nacme:\n    # ACME support is disabled by default. Set this to `true` and uncomment\n    # tls_certificate_path and tls_private_key_path above to enable it.\n    #\n    enabled: false\n\n    # Endpoint to use to request certificates. If you only want to test,\n    # use Let's Encrypt's staging url:\n    #     https://acme-staging.api.letsencrypt.org/directory\n    #\n    #url: https://acme-v01.api.letsencrypt.org/directory\n\n    # Port number to listen on for the HTTP-01 challenge. Change this if\n    # you are forwarding connections through Apache/Nginx/etc.\n    #\n    port: 80\n\n    # Local addresses to listen on for incoming connections.\n    # Again, you may want to change this if you are forwarding connections\n    # through Apache/Nginx/etc.\n    #\n    bind_addresses: ['::', '0.0.0.0']\n\n    # How many days remaining on a certificate before it is renewed.\n    #\n    reprovision_threshold: 30\n\n    # The domain that the certificate should be for. Normally this\n    # should be the same as your Matrix domain (i.e., 'server_name'), but,\n    # by putting a file at 'https://<server_name>/.well-known/matrix/server',\n    # you can delegate incoming traffic to another server. If you do that,\n    # you should give the target of the delegation here.\n    #\n    # For example: if your 'server_name' is 'example.com', but\n    # 'https://example.com/.well-known/matrix/server' delegates to\n    # 'matrix.example.com', you should put 'matrix.example.com' here.\n    #\n    # If not set, defaults to your 'server_name'.\n    #\n    domain: matrix.example.com\n\n    # file to use for the account key. This will be generated if it doesn't\n    # exist.\n    #\n    # If unspecified, we will use CONFDIR/client.key.\n    #\n    account_key_file: DATADIR/acme_account.key\n\n# List of allowed TLS fingerprints for this server to publish along\n# with the signing keys for this server. Other matrix servers that\n# make HTTPS requests to this server will check that the TLS\n# certificates returned by this server match one of the fingerprints.\n#\n# Synapse automatically adds the fingerprint of its own certificate\n# to the list. So if federation traffic is handled directly by synapse\n# then no modification to the list is required.\n#\n# If synapse is run behind a load balancer that handles the TLS then it\n# will be necessary to add the fingerprints of the certificates used by\n# the loadbalancers to this list if they are different to the one\n# synapse is using.\n#\n# Homeservers are permitted to cache the list of TLS fingerprints\n# returned in the key responses up to the \"valid_until_ts\" returned in\n# key. It may be necessary to publish the fingerprints of a new\n# certificate and wait until the \"valid_until_ts\" of the previous key\n# responses have passed before deploying it.\n#\n# You can calculate a fingerprint from a given TLS listener via:\n# openssl s_client -connect $host:$port < /dev/null 2> /dev/null |\n#   openssl x509 -outform DER | openssl sha256 -binary | base64 | tr -d '='\n# or by checking matrix.org/federationtester/api/report?server_name=$host\n#\n#tls_fingerprints: [{\"sha256\": \"<base64_encoded_sha256_fingerprint>\"}]\n\n\n## Federation ##\n\n# Restrict federation to the following whitelist of domains.\n# N.B. we recommend also firewalling your federation listener to limit\n# inbound federation traffic as early as possible, rather than relying\n# purely on this application-layer restriction.  If not specified, the\n# default is to whitelist everything.\n#\n#federation_domain_whitelist:\n#  - lon.example.com\n#  - nyc.example.com\n#  - syd.example.com\n\n# Prevent federation requests from being sent to the following\n# blacklist IP address CIDR ranges. If this option is not specified, or\n# specified with an empty list, no ip range blacklist will be enforced.\n#\n# As of Synapse v1.4.0 this option also affects any outbound requests to identity\n# servers provided by user input.\n#\n# (0.0.0.0 and :: are always blacklisted, whether or not they are explicitly\n# listed here, since they correspond to unroutable addresses.)\n#\nfederation_ip_range_blacklist:\n  - '127.0.0.0/8'\n  - '10.0.0.0/8'\n  - '172.16.0.0/12'\n  - '192.168.0.0/16'\n  - '100.64.0.0/10'\n  - '169.254.0.0/16'\n  - '::1/128'\n  - 'fe80::/64'\n  - 'fc00::/7'\n\n# Report prometheus metrics on the age of PDUs being sent to and received from\n# the following domains. This can be used to give an idea of \"delay\" on inbound\n# and outbound federation, though be aware that any delay can be due to problems\n# at either end or with the intermediate network.\n#\n# By default, no domains are monitored in this way.\n#\n#federation_metrics_domains:\n#  - matrix.org\n#  - example.com\n\n\n## Caching ##\n\n# Caching can be configured through the following options.\n#\n# A cache 'factor' is a multiplier that can be applied to each of\n# Synapse's caches in order to increase or decrease the maximum\n# number of entries that can be stored.\n\n# The number of events to cache in memory. Not affected by\n# caches.global_factor.\n#\n#event_cache_size: 10K\n\ncaches:\n   # Controls the global cache factor, which is the default cache factor\n   # for all caches if a specific factor for that cache is not otherwise\n   # set.\n   #\n   # This can also be set by the \"SYNAPSE_CACHE_FACTOR\" environment\n   # variable. Setting by environment variable takes priority over\n   # setting through the config file.\n   #\n   # Defaults to 0.5, which will half the size of all caches.\n   #\n   #global_factor: 1.0\n\n   # A dictionary of cache name to cache factor for that individual\n   # cache. Overrides the global cache factor for a given cache.\n   #\n   # These can also be set through environment variables comprised\n   # of \"SYNAPSE_CACHE_FACTOR_\" + the name of the cache in capital\n   # letters and underscores. Setting by environment variable\n   # takes priority over setting through the config file.\n   # Ex. SYNAPSE_CACHE_FACTOR_GET_USERS_WHO_SHARE_ROOM_WITH_USER=2.0\n   #\n   # Some caches have '*' and other characters that are not\n   # alphanumeric or underscores. These caches can be named with or\n   # without the special characters stripped. For example, to specify\n   # the cache factor for `*stateGroupCache*` via an environment\n   # variable would be `SYNAPSE_CACHE_FACTOR_STATEGROUPCACHE=2.0`.\n   #\n   per_cache_factors:\n     #get_users_who_share_room_with_user: 2.0\n\n\n## Database ##\n\n# The 'database' setting defines the database that synapse uses to store all of\n# its data.\n#\n# 'name' gives the database engine to use: either 'sqlite3' (for SQLite) or\n# 'psycopg2' (for PostgreSQL).\n#\n# 'args' gives options which are passed through to the database engine,\n# except for options starting 'cp_', which are used to configure the Twisted\n# connection pool. For a reference to valid arguments, see:\n#   * for sqlite: https://docs.python.org/3/library/sqlite3.html#sqlite3.connect\n#   * for postgres: https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-PARAMKEYWORDS\n#   * for the connection pool: https://twistedmatrix.com/documents/current/api/twisted.enterprise.adbapi.ConnectionPool.html#__init__\n#\n#\n# Example SQLite configuration:\n#\n#database:\n#  name: sqlite3\n#  args:\n#    database: /path/to/homeserver.db\n#\n#\n# Example Postgres configuration:\n#\n#database:\n#  name: psycopg2\n#  args:\n#    user: synapse_user\n#    password: secretpassword\n#    database: synapse\n#    host: localhost\n#    cp_min: 5\n#    cp_max: 10\n#\n# For more information on using Synapse with Postgres, see `docs/postgres.md`.\n#\ndatabase:\n  name: sqlite3\n  args:\n    database: DATADIR/homeserver.db\n\n\n## Logging ##\n\n# A yaml python logging config file as described by\n# https://docs.python.org/3.7/library/logging.config.html#configuration-dictionary-schema\n#\nlog_config: \"CONFDIR/SERVERNAME.log.config\"\n\n\n## Ratelimiting ##\n\n# Ratelimiting settings for client actions (registration, login, messaging).\n#\n# Each ratelimiting configuration is made of two parameters:\n#   - per_second: number of requests a client can send per second.\n#   - burst_count: number of requests a client can send before being throttled.\n#\n# Synapse currently uses the following configurations:\n#   - one for messages that ratelimits sending based on the account the client\n#     is using\n#   - one for registration that ratelimits registration requests based on the\n#     client's IP address.\n#   - one for login that ratelimits login requests based on the client's IP\n#     address.\n#   - one for login that ratelimits login requests based on the account the\n#     client is attempting to log into.\n#   - one for login that ratelimits login requests based on the account the\n#     client is attempting to log into, based on the amount of failed login\n#     attempts for this account.\n#   - one for ratelimiting redactions by room admins. If this is not explicitly\n#     set then it uses the same ratelimiting as per rc_message. This is useful\n#     to allow room admins to deal with abuse quickly.\n#   - two for ratelimiting number of rooms a user can join, \"local\" for when\n#     users are joining rooms the server is already in (this is cheap) vs\n#     \"remote\" for when users are trying to join rooms not on the server (which\n#     can be more expensive)\n#\n# The defaults are as shown below.\n#\n#rc_message:\n#  per_second: 0.2\n#  burst_count: 10\n#\n#rc_registration:\n#  per_second: 0.17\n#  burst_count: 3\n#\n#rc_login:\n#  address:\n#    per_second: 0.17\n#    burst_count: 3\n#  account:\n#    per_second: 0.17\n#    burst_count: 3\n#  failed_attempts:\n#    per_second: 0.17\n#    burst_count: 3\n#\n#rc_admin_redaction:\n#  per_second: 1\n#  burst_count: 50\n#\n#rc_joins:\n#  local:\n#    per_second: 0.1\n#    burst_count: 3\n#  remote:\n#    per_second: 0.01\n#    burst_count: 3\n\n\n# Ratelimiting settings for incoming federation\n#\n# The rc_federation configuration is made up of the following settings:\n#   - window_size: window size in milliseconds\n#   - sleep_limit: number of federation requests from a single server in\n#     a window before the server will delay processing the request.\n#   - sleep_delay: duration in milliseconds to delay processing events\n#     from remote servers by if they go over the sleep limit.\n#   - reject_limit: maximum number of concurrent federation requests\n#     allowed from a single server\n#   - concurrent: number of federation requests to concurrently process\n#     from a single server\n#\n# The defaults are as shown below.\n#\n#rc_federation:\n#  window_size: 1000\n#  sleep_limit: 10\n#  sleep_delay: 500\n#  reject_limit: 50\n#  concurrent: 3\n\n# Target outgoing federation transaction frequency for sending read-receipts,\n# per-room.\n#\n# If we end up trying to send out more read-receipts, they will get buffered up\n# into fewer transactions.\n#\n#federation_rr_transactions_per_room_per_second: 50\n\n\n\n## Media Store ##\n\n# Enable the media store service in the Synapse master. Uncomment the\n# following if you are using a separate media store worker.\n#\n#enable_media_repo: false\n\n# Directory where uploaded images and attachments are stored.\n#\nmedia_store_path: \"DATADIR/media_store\"\n\n# Media storage providers allow media to be stored in different\n# locations.\n#\n#media_storage_providers:\n#  - module: file_system\n#    # Whether to store newly uploaded local files\n#    store_local: false\n#    # Whether to store newly downloaded remote files\n#    store_remote: false\n#    # Whether to wait for successful storage for local uploads\n#    store_synchronous: false\n#    config:\n#       directory: /mnt/some/other/directory\n\n# The largest allowed upload size in bytes\n#\n#max_upload_size: 50M\n\n# Maximum number of pixels that will be thumbnailed\n#\n#max_image_pixels: 32M\n\n# Whether to generate new thumbnails on the fly to precisely match\n# the resolution requested by the client. If true then whenever\n# a new resolution is requested by the client the server will\n# generate a new thumbnail. If false the server will pick a thumbnail\n# from a precalculated list.\n#\n#dynamic_thumbnails: false\n\n# List of thumbnails to precalculate when an image is uploaded.\n#\n#thumbnail_sizes:\n#  - width: 32\n#    height: 32\n#    method: crop\n#  - width: 96\n#    height: 96\n#    method: crop\n#  - width: 320\n#    height: 240\n#    method: scale\n#  - width: 640\n#    height: 480\n#    method: scale\n#  - width: 800\n#    height: 600\n#    method: scale\n\n# Is the preview URL API enabled?\n#\n# 'false' by default: uncomment the following to enable it (and specify a\n# url_preview_ip_range_blacklist blacklist).\n#\n#url_preview_enabled: true\n\n# List of IP address CIDR ranges that the URL preview spider is denied\n# from accessing.  There are no defaults: you must explicitly\n# specify a list for URL previewing to work.  You should specify any\n# internal services in your network that you do not want synapse to try\n# to connect to, otherwise anyone in any Matrix room could cause your\n# synapse to issue arbitrary GET requests to your internal services,\n# causing serious security issues.\n#\n# (0.0.0.0 and :: are always blacklisted, whether or not they are explicitly\n# listed here, since they correspond to unroutable addresses.)\n#\n# This must be specified if url_preview_enabled is set. It is recommended that\n# you uncomment the following list as a starting point.\n#\n#url_preview_ip_range_blacklist:\n#  - '127.0.0.0/8'\n#  - '10.0.0.0/8'\n#  - '172.16.0.0/12'\n#  - '192.168.0.0/16'\n#  - '100.64.0.0/10'\n#  - '169.254.0.0/16'\n#  - '::1/128'\n#  - 'fe80::/64'\n#  - 'fc00::/7'\n\n# List of IP address CIDR ranges that the URL preview spider is allowed\n# to access even if they are specified in url_preview_ip_range_blacklist.\n# This is useful for specifying exceptions to wide-ranging blacklisted\n# target IP ranges - e.g. for enabling URL previews for a specific private\n# website only visible in your network.\n#\n#url_preview_ip_range_whitelist:\n#   - '192.168.1.1'\n\n# Optional list of URL matches that the URL preview spider is\n# denied from accessing.  You should use url_preview_ip_range_blacklist\n# in preference to this, otherwise someone could define a public DNS\n# entry that points to a private IP address and circumvent the blacklist.\n# This is more useful if you know there is an entire shape of URL that\n# you know that will never want synapse to try to spider.\n#\n# Each list entry is a dictionary of url component attributes as returned\n# by urlparse.urlsplit as applied to the absolute form of the URL.  See\n# https://docs.python.org/2/library/urlparse.html#urlparse.urlsplit\n# The values of the dictionary are treated as an filename match pattern\n# applied to that component of URLs, unless they start with a ^ in which\n# case they are treated as a regular expression match.  If all the\n# specified component matches for a given list item succeed, the URL is\n# blacklisted.\n#\n#url_preview_url_blacklist:\n#  # blacklist any URL with a username in its URI\n#  - username: '*'\n#\n#  # blacklist all *.google.com URLs\n#  - netloc: 'google.com'\n#  - netloc: '*.google.com'\n#\n#  # blacklist all plain HTTP URLs\n#  - scheme: 'http'\n#\n#  # blacklist http(s)://www.acme.com/foo\n#  - netloc: 'www.acme.com'\n#    path: '/foo'\n#\n#  # blacklist any URL with a literal IPv4 address\n#  - netloc: '^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$'\n\n# The largest allowed URL preview spidering size in bytes\n#\n#max_spider_size: 10M\n\n# A list of values for the Accept-Language HTTP header used when\n# downloading webpages during URL preview generation. This allows\n# Synapse to specify the preferred languages that URL previews should\n# be in when communicating with remote servers.\n#\n# Each value is a IETF language tag; a 2-3 letter identifier for a\n# language, optionally followed by subtags separated by '-', specifying\n# a country or region variant.\n#\n# Multiple values can be provided, and a weight can be added to each by\n# using quality value syntax (;q=). '*' translates to any language.\n#\n# Defaults to \"en\".\n#\n# Example:\n#\n# url_preview_accept_language:\n#   - en-UK\n#   - en-US;q=0.9\n#   - fr;q=0.8\n#   - *;q=0.7\n#\nurl_preview_accept_language:\n#   - en\n\n\n## Captcha ##\n# See docs/CAPTCHA_SETUP.md for full details of configuring this.\n\n# This homeserver's ReCAPTCHA public key. Must be specified if\n# enable_registration_captcha is enabled.\n#\n#recaptcha_public_key: \"YOUR_PUBLIC_KEY\"\n\n# This homeserver's ReCAPTCHA private key. Must be specified if\n# enable_registration_captcha is enabled.\n#\n#recaptcha_private_key: \"YOUR_PRIVATE_KEY\"\n\n# Uncomment to enable ReCaptcha checks when registering, preventing signup\n# unless a captcha is answered. Requires a valid ReCaptcha\n# public/private key. Defaults to 'false'.\n#\n#enable_registration_captcha: true\n\n# The API endpoint to use for verifying m.login.recaptcha responses.\n# Defaults to \"https://www.recaptcha.net/recaptcha/api/siteverify\".\n#\n#recaptcha_siteverify_api: \"https://my.recaptcha.site\"\n\n\n## TURN ##\n\n# The public URIs of the TURN server to give to clients\n#\n#turn_uris: []\n\n# The shared secret used to compute passwords for the TURN server\n#\n#turn_shared_secret: \"YOUR_SHARED_SECRET\"\n\n# The Username and password if the TURN server needs them and\n# does not use a token\n#\n#turn_username: \"TURNSERVER_USERNAME\"\n#turn_password: \"TURNSERVER_PASSWORD\"\n\n# How long generated TURN credentials last\n#\n#turn_user_lifetime: 1h\n\n# Whether guests should be allowed to use the TURN server.\n# This defaults to True, otherwise VoIP will be unreliable for guests.\n# However, it does introduce a slight security risk as it allows users to\n# connect to arbitrary endpoints without having first signed up for a\n# valid account (e.g. by passing a CAPTCHA).\n#\n#turn_allow_guests: true\n\n\n## Registration ##\n#\n# Registration can be rate-limited using the parameters in the \"Ratelimiting\"\n# section of this file.\n\n# Enable registration for new users.\n#\n#enable_registration: false\n\n# Optional account validity configuration. This allows for accounts to be denied\n# any request after a given period.\n#\n# Once this feature is enabled, Synapse will look for registered users without an\n# expiration date at startup and will add one to every account it found using the\n# current settings at that time.\n# This means that, if a validity period is set, and Synapse is restarted (it will\n# then derive an expiration date from the current validity period), and some time\n# after that the validity period changes and Synapse is restarted, the users'\n# expiration dates won't be updated unless their account is manually renewed. This\n# date will be randomly selected within a range [now + period - d ; now + period],\n# where d is equal to 10% of the validity period.\n#\naccount_validity:\n  # The account validity feature is disabled by default. Uncomment the\n  # following line to enable it.\n  #\n  #enabled: true\n\n  # The period after which an account is valid after its registration. When\n  # renewing the account, its validity period will be extended by this amount\n  # of time. This parameter is required when using the account validity\n  # feature.\n  #\n  #period: 6w\n\n  # The amount of time before an account's expiry date at which Synapse will\n  # send an email to the account's email address with a renewal link. By\n  # default, no such emails are sent.\n  #\n  # If you enable this setting, you will also need to fill out the 'email' and\n  # 'public_baseurl' configuration sections.\n  #\n  #renew_at: 1w\n\n  # The subject of the email sent out with the renewal link. '%(app)s' can be\n  # used as a placeholder for the 'app_name' parameter from the 'email'\n  # section.\n  #\n  # Note that the placeholder must be written '%(app)s', including the\n  # trailing 's'.\n  #\n  # If this is not set, a default value is used.\n  #\n  #renew_email_subject: \"Renew your %(app)s account\"\n\n  # Directory in which Synapse will try to find templates for the HTML files to\n  # serve to the user when trying to renew an account. If not set, default\n  # templates from within the Synapse package will be used.\n  #\n  #template_dir: \"res/templates\"\n\n  # File within 'template_dir' giving the HTML to be displayed to the user after\n  # they successfully renewed their account. If not set, default text is used.\n  #\n  #account_renewed_html_path: \"account_renewed.html\"\n\n  # File within 'template_dir' giving the HTML to be displayed when the user\n  # tries to renew an account with an invalid renewal token. If not set,\n  # default text is used.\n  #\n  #invalid_token_html_path: \"invalid_token.html\"\n\n# Time that a user's session remains valid for, after they log in.\n#\n# Note that this is not currently compatible with guest logins.\n#\n# Note also that this is calculated at login time: changes are not applied\n# retrospectively to users who have already logged in.\n#\n# By default, this is infinite.\n#\n#session_lifetime: 24h\n\n# The user must provide all of the below types of 3PID when registering.\n#\n#registrations_require_3pid:\n#  - email\n#  - msisdn\n\n# Explicitly disable asking for MSISDNs from the registration\n# flow (overrides registrations_require_3pid if MSISDNs are set as required)\n#\n#disable_msisdn_registration: true\n\n# Mandate that users are only allowed to associate certain formats of\n# 3PIDs with accounts on this server.\n#\n#allowed_local_3pids:\n#  - medium: email\n#    pattern: '.*@matrix\\.org'\n#  - medium: email\n#    pattern: '.*@vector\\.im'\n#  - medium: msisdn\n#    pattern: '\\+44'\n\n# Enable 3PIDs lookup requests to identity servers from this server.\n#\n#enable_3pid_lookup: true\n\n# If set, allows registration of standard or admin accounts by anyone who\n# has the shared secret, even if registration is otherwise disabled.\n#\n#registration_shared_secret: <PRIVATE STRING>\n\n# Set the number of bcrypt rounds used to generate password hash.\n# Larger numbers increase the work factor needed to generate the hash.\n# The default number is 12 (which equates to 2^12 rounds).\n# N.B. that increasing this will exponentially increase the time required\n# to register or login - e.g. 24 => 2^24 rounds which will take >20 mins.\n#\n#bcrypt_rounds: 12\n\n# Allows users to register as guests without a password/email/etc, and\n# participate in rooms hosted on this server which have been made\n# accessible to anonymous users.\n#\n#allow_guest_access: false\n\n# The identity server which we suggest that clients should use when users log\n# in on this server.\n#\n# (By default, no suggestion is made, so it is left up to the client.\n# This setting is ignored unless public_baseurl is also set.)\n#\n#default_identity_server: https://matrix.org\n\n# Handle threepid (email/phone etc) registration and password resets through a set of\n# *trusted* identity servers. Note that this allows the configured identity server to\n# reset passwords for accounts!\n#\n# Be aware that if `email` is not set, and SMTP options have not been\n# configured in the email config block, registration and user password resets via\n# email will be globally disabled.\n#\n# Additionally, if `msisdn` is not set, registration and password resets via msisdn\n# will be disabled regardless, and users will not be able to associate an msisdn\n# identifier to their account. This is due to Synapse currently not supporting\n# any method of sending SMS messages on its own.\n#\n# To enable using an identity server for operations regarding a particular third-party\n# identifier type, set the value to the URL of that identity server as shown in the\n# examples below.\n#\n# Servers handling the these requests must answer the `/requestToken` endpoints defined\n# by the Matrix Identity Service API specification:\n# https://matrix.org/docs/spec/identity_service/latest\n#\n# If a delegate is specified, the config option public_baseurl must also be filled out.\n#\naccount_threepid_delegates:\n    #email: https://example.com     # Delegate email sending to example.com\n    #msisdn: http://localhost:8090  # Delegate SMS sending to this local process\n\n# Whether users are allowed to change their displayname after it has\n# been initially set. Useful when provisioning users based on the\n# contents of a third-party directory.\n#\n# Does not apply to server administrators. Defaults to 'true'\n#\n#enable_set_displayname: false\n\n# Whether users are allowed to change their avatar after it has been\n# initially set. Useful when provisioning users based on the contents\n# of a third-party directory.\n#\n# Does not apply to server administrators. Defaults to 'true'\n#\n#enable_set_avatar_url: false\n\n# Whether users can change the 3PIDs associated with their accounts\n# (email address and msisdn).\n#\n# Defaults to 'true'\n#\n#enable_3pid_changes: false\n\n# Users who register on this homeserver will automatically be joined\n# to these rooms.\n#\n# By default, any room aliases included in this list will be created\n# as a publicly joinable room when the first user registers for the\n# homeserver. This behaviour can be customised with the settings below.\n#\n#auto_join_rooms:\n#  - \"#example:example.com\"\n\n# Where auto_join_rooms are specified, setting this flag ensures that the\n# the rooms exist by creating them when the first user on the\n# homeserver registers.\n#\n# By default the auto-created rooms are publicly joinable from any federated\n# server. Use the autocreate_auto_join_rooms_federated and\n# autocreate_auto_join_room_preset settings below to customise this behaviour.\n#\n# Setting to false means that if the rooms are not manually created,\n# users cannot be auto-joined since they do not exist.\n#\n# Defaults to true. Uncomment the following line to disable automatically\n# creating auto-join rooms.\n#\n#autocreate_auto_join_rooms: false\n\n# Whether the auto_join_rooms that are auto-created are available via\n# federation. Only has an effect if autocreate_auto_join_rooms is true.\n#\n# Note that whether a room is federated cannot be modified after\n# creation.\n#\n# Defaults to true: the room will be joinable from other servers.\n# Uncomment the following to prevent users from other homeservers from\n# joining these rooms.\n#\n#autocreate_auto_join_rooms_federated: false\n\n# The room preset to use when auto-creating one of auto_join_rooms. Only has an\n# effect if autocreate_auto_join_rooms is true.\n#\n# This can be one of \"public_chat\", \"private_chat\", or \"trusted_private_chat\".\n# If a value of \"private_chat\" or \"trusted_private_chat\" is used then\n# auto_join_mxid_localpart must also be configured.\n#\n# Defaults to \"public_chat\", meaning that the room is joinable by anyone, including\n# federated servers if autocreate_auto_join_rooms_federated is true (the default).\n# Uncomment the following to require an invitation to join these rooms.\n#\n#autocreate_auto_join_room_preset: private_chat\n\n# The local part of the user id which is used to create auto_join_rooms if\n# autocreate_auto_join_rooms is true. If this is not provided then the\n# initial user account that registers will be used to create the rooms.\n#\n# The user id is also used to invite new users to any auto-join rooms which\n# are set to invite-only.\n#\n# It *must* be configured if autocreate_auto_join_room_preset is set to\n# \"private_chat\" or \"trusted_private_chat\".\n#\n# Note that this must be specified in order for new users to be correctly\n# invited to any auto-join rooms which have been set to invite-only (either\n# at the time of creation or subsequently).\n#\n# Note that, if the room already exists, this user must be joined and\n# have the appropriate permissions to invite new members.\n#\n#auto_join_mxid_localpart: system\n\n# When auto_join_rooms is specified, setting this flag to false prevents\n# guest accounts from being automatically joined to the rooms.\n#\n# Defaults to true.\n#\n#auto_join_rooms_for_guests: false\n\n\n## Metrics ###\n\n# Enable collection and rendering of performance metrics\n#\n#enable_metrics: false\n\n# Enable sentry integration\n# NOTE: While attempts are made to ensure that the logs don't contain\n# any sensitive information, this cannot be guaranteed. By enabling\n# this option the sentry server may therefore receive sensitive\n# information, and it in turn may then diseminate sensitive information\n# through insecure notification channels if so configured.\n#\n#sentry:\n#    dsn: \"...\"\n\n# Flags to enable Prometheus metrics which are not suitable to be\n# enabled by default, either for performance reasons or limited use.\n#\nmetrics_flags:\n    # Publish synapse_federation_known_servers, a gauge of the number of\n    # servers this homeserver knows about, including itself. May cause\n    # performance problems on large homeservers.\n    #\n    #known_servers: true\n\n# Whether or not to report anonymized homeserver usage statistics.\n#\n#report_stats: true|false\n\n# The endpoint to report the anonymized homeserver usage statistics to.\n# Defaults to https://matrix.org/report-usage-stats/push\n#\n#report_stats_endpoint: https://example.com/report-usage-stats/push\n\n\n## API Configuration ##\n\n# A list of event types that will be included in the room_invite_state\n#\n#room_invite_state_types:\n#  - \"m.room.join_rules\"\n#  - \"m.room.canonical_alias\"\n#  - \"m.room.avatar\"\n#  - \"m.room.encryption\"\n#  - \"m.room.name\"\n\n\n# A list of application service config files to use\n#\n#app_service_config_files:\n#  - app_service_1.yaml\n#  - app_service_2.yaml\n\n# Uncomment to enable tracking of application service IP addresses. Implicitly\n# enables MAU tracking for application service users.\n#\n#track_appservice_user_ips: true\n\n\n# a secret which is used to sign access tokens. If none is specified,\n# the registration_shared_secret is used, if one is given; otherwise,\n# a secret key is derived from the signing key.\n#\n#macaroon_secret_key: <PRIVATE STRING>\n\n# a secret which is used to calculate HMACs for form values, to stop\n# falsification of values. Must be specified for the User Consent\n# forms to work.\n#\n#form_secret: <PRIVATE STRING>\n\n## Signing Keys ##\n\n# Path to the signing key to sign messages with\n#\nsigning_key_path: \"CONFDIR/SERVERNAME.signing.key\"\n\n# The keys that the server used to sign messages with but won't use\n# to sign new messages.\n#\nold_signing_keys:\n  # For each key, `key` should be the base64-encoded public key, and\n  # `expired_ts`should be the time (in milliseconds since the unix epoch) that\n  # it was last used.\n  #\n  # It is possible to build an entry from an old signing.key file using the\n  # `export_signing_key` script which is provided with synapse.\n  #\n  # For example:\n  #\n  #\"ed25519:id\": { key: \"base64string\", expired_ts: 123456789123 }\n\n# How long key response published by this server is valid for.\n# Used to set the valid_until_ts in /key/v2 APIs.\n# Determines how quickly servers will query to check which keys\n# are still valid.\n#\n#key_refresh_interval: 1d\n\n# The trusted servers to download signing keys from.\n#\n# When we need to fetch a signing key, each server is tried in parallel.\n#\n# Normally, the connection to the key server is validated via TLS certificates.\n# Additional security can be provided by configuring a `verify key`, which\n# will make synapse check that the response is signed by that key.\n#\n# This setting supercedes an older setting named `perspectives`. The old format\n# is still supported for backwards-compatibility, but it is deprecated.\n#\n# 'trusted_key_servers' defaults to matrix.org, but using it will generate a\n# warning on start-up. To suppress this warning, set\n# 'suppress_key_server_warning' to true.\n#\n# Options for each entry in the list include:\n#\n#    server_name: the name of the server. required.\n#\n#    verify_keys: an optional map from key id to base64-encoded public key.\n#       If specified, we will check that the response is signed by at least\n#       one of the given keys.\n#\n#    accept_keys_insecurely: a boolean. Normally, if `verify_keys` is unset,\n#       and federation_verify_certificates is not `true`, synapse will refuse\n#       to start, because this would allow anyone who can spoof DNS responses\n#       to masquerade as the trusted key server. If you know what you are doing\n#       and are sure that your network environment provides a secure connection\n#       to the key server, you can set this to `true` to override this\n#       behaviour.\n#\n# An example configuration might look like:\n#\n#trusted_key_servers:\n#  - server_name: \"my_trusted_server.example.com\"\n#    verify_keys:\n#      \"ed25519:auto\": \"abcdefghijklmnopqrstuvwxyzabcdefghijklmopqr\"\n#  - server_name: \"my_other_trusted_server.example.com\"\n#\ntrusted_key_servers:\n  - server_name: \"matrix.org\"\n\n# Uncomment the following to disable the warning that is emitted when the\n# trusted_key_servers include 'matrix.org'. See above.\n#\n#suppress_key_server_warning: true\n\n# The signing keys to use when acting as a trusted key server. If not specified\n# defaults to the server signing key.\n#\n# Can contain multiple keys, one per line.\n#\n#key_server_signing_keys_path: \"key_server_signing_keys.key\"\n\n\n## Single sign-on integration ##\n\n# The following settings can be used to make Synapse use a single sign-on\n# provider for authentication, instead of its internal password database.\n#\n# You will probably also want to set the following options to `false` to\n# disable the regular login/registration flows:\n#   * enable_registration\n#   * password_config.enabled\n#\n# You will also want to investigate the settings under the \"sso\" configuration\n# section below.\n\n# Enable SAML2 for registration and login. Uses pysaml2.\n#\n# At least one of `sp_config` or `config_path` must be set in this section to\n# enable SAML login.\n#\n# Once SAML support is enabled, a metadata file will be exposed at\n# https://<server>:<port>/_matrix/saml2/metadata.xml, which you may be able to\n# use to configure your SAML IdP with. Alternatively, you can manually configure\n# the IdP to use an ACS location of\n# https://<server>:<port>/_matrix/saml2/authn_response.\n#\nsaml2_config:\n  # `sp_config` is the configuration for the pysaml2 Service Provider.\n  # See pysaml2 docs for format of config.\n  #\n  # Default values will be used for the 'entityid' and 'service' settings,\n  # so it is not normally necessary to specify them unless you need to\n  # override them.\n  #\n  sp_config:\n    # Point this to the IdP's metadata. You must provide either a local\n    # file via the `local` attribute or (preferably) a URL via the\n    # `remote` attribute.\n    #\n    #metadata:\n    #  local: [\"saml2/idp.xml\"]\n    #  remote:\n    #    - url: https://our_idp/metadata.xml\n\n    # Allowed clock difference in seconds between the homeserver and IdP.\n    #\n    # Uncomment the below to increase the accepted time difference from 0 to 3 seconds.\n    #\n    #accepted_time_diff: 3\n\n    # By default, the user has to go to our login page first. If you'd like\n    # to allow IdP-initiated login, set 'allow_unsolicited: true' in a\n    # 'service.sp' section:\n    #\n    #service:\n    #  sp:\n    #    allow_unsolicited: true\n\n    # The examples below are just used to generate our metadata xml, and you\n    # may well not need them, depending on your setup. Alternatively you\n    # may need a whole lot more detail - see the pysaml2 docs!\n\n    #description: [\"My awesome SP\", \"en\"]\n    #name: [\"Test SP\", \"en\"]\n\n    #ui_info:\n    #  display_name:\n    #    - lang: en\n    #      text: \"Display Name is the descriptive name of your service.\"\n    #  description:\n    #    - lang: en\n    #      text: \"Description should be a short paragraph explaining the purpose of the service.\"\n    #  information_url:\n    #    - lang: en\n    #      text: \"https://example.com/terms-of-service\"\n    #  privacy_statement_url:\n    #    - lang: en\n    #      text: \"https://example.com/privacy-policy\"\n    #  keywords:\n    #    - lang: en\n    #      text: [\"Matrix\", \"Element\"]\n    #  logo:\n    #    - lang: en\n    #      text: \"https://example.com/logo.svg\"\n    #      width: \"200\"\n    #      height: \"80\"\n\n    #organization:\n    #  name: Example com\n    #  display_name:\n    #    - [\"Example co\", \"en\"]\n    #  url: \"http://example.com\"\n\n    #contact_person:\n    #  - given_name: Bob\n    #    sur_name: \"the Sysadmin\"\n    #    email_address\": [\"admin@example.com\"]\n    #    contact_type\": technical\n\n  # Instead of putting the config inline as above, you can specify a\n  # separate pysaml2 configuration file:\n  #\n  #config_path: \"CONFDIR/sp_conf.py\"\n\n  # The lifetime of a SAML session. This defines how long a user has to\n  # complete the authentication process, if allow_unsolicited is unset.\n  # The default is 15 minutes.\n  #\n  #saml_session_lifetime: 5m\n\n  # An external module can be provided here as a custom solution to\n  # mapping attributes returned from a saml provider onto a matrix user.\n  #\n  user_mapping_provider:\n    # The custom module's class. Uncomment to use a custom module.\n    #\n    #module: mapping_provider.SamlMappingProvider\n\n    # Custom configuration values for the module. Below options are\n    # intended for the built-in provider, they should be changed if\n    # using a custom module. This section will be passed as a Python\n    # dictionary to the module's `parse_config` method.\n    #\n    config:\n      # The SAML attribute (after mapping via the attribute maps) to use\n      # to derive the Matrix ID from. 'uid' by default.\n      #\n      # Note: This used to be configured by the\n      # saml2_config.mxid_source_attribute option. If that is still\n      # defined, its value will be used instead.\n      #\n      #mxid_source_attribute: displayName\n\n      # The mapping system to use for mapping the saml attribute onto a\n      # matrix ID.\n      #\n      # Options include:\n      #  * 'hexencode' (which maps unpermitted characters to '=xx')\n      #  * 'dotreplace' (which replaces unpermitted characters with\n      #     '.').\n      # The default is 'hexencode'.\n      #\n      # Note: This used to be configured by the\n      # saml2_config.mxid_mapping option. If that is still defined, its\n      # value will be used instead.\n      #\n      #mxid_mapping: dotreplace\n\n  # In previous versions of synapse, the mapping from SAML attribute to\n  # MXID was always calculated dynamically rather than stored in a\n  # table. For backwards- compatibility, we will look for user_ids\n  # matching such a pattern before creating a new account.\n  #\n  # This setting controls the SAML attribute which will be used for this\n  # backwards-compatibility lookup. Typically it should be 'uid', but if\n  # the attribute maps are changed, it may be necessary to change it.\n  #\n  # The default is 'uid'.\n  #\n  #grandfathered_mxid_source_attribute: upn\n\n  # It is possible to configure Synapse to only allow logins if SAML attributes\n  # match particular values. The requirements can be listed under\n  # `attribute_requirements` as shown below. All of the listed attributes must\n  # match for the login to be permitted.\n  #\n  #attribute_requirements:\n  #  - attribute: userGroup\n  #    value: \"staff\"\n  #  - attribute: department\n  #    value: \"sales\"\n\n  # If the metadata XML contains multiple IdP entities then the `idp_entityid`\n  # option must be set to the entity to redirect users to.\n  #\n  # Most deployments only have a single IdP entity and so should omit this\n  # option.\n  #\n  #idp_entityid: 'https://our_idp/entityid'\n\n\n# Enable OpenID Connect (OIDC) / OAuth 2.0 for registration and login.\n#\n# See https://github.com/matrix-org/synapse/blob/master/docs/openid.md\n# for some example configurations.\n#\noidc_config:\n  # Uncomment the following to enable authorization against an OpenID Connect\n  # server. Defaults to false.\n  #\n  #enabled: true\n\n  # Uncomment the following to disable use of the OIDC discovery mechanism to\n  # discover endpoints. Defaults to true.\n  #\n  #discover: false\n\n  # the OIDC issuer. Used to validate tokens and (if discovery is enabled) to\n  # discover the provider's endpoints.\n  #\n  # Required if 'enabled' is true.\n  #\n  #issuer: \"https://accounts.example.com/\"\n\n  # oauth2 client id to use.\n  #\n  # Required if 'enabled' is true.\n  #\n  #client_id: \"provided-by-your-issuer\"\n\n  # oauth2 client secret to use.\n  #\n  # Required if 'enabled' is true.\n  #\n  #client_secret: \"provided-by-your-issuer\"\n\n  # auth method to use when exchanging the token.\n  # Valid values are 'client_secret_basic' (default), 'client_secret_post' and\n  # 'none'.\n  #\n  #client_auth_method: client_secret_post\n\n  # list of scopes to request. This should normally include the \"openid\" scope.\n  # Defaults to [\"openid\"].\n  #\n  #scopes: [\"openid\", \"profile\"]\n\n  # the oauth2 authorization endpoint. Required if provider discovery is disabled.\n  #\n  #authorization_endpoint: \"https://accounts.example.com/oauth2/auth\"\n\n  # the oauth2 token endpoint. Required if provider discovery is disabled.\n  #\n  #token_endpoint: \"https://accounts.example.com/oauth2/token\"\n\n  # the OIDC userinfo endpoint. Required if discovery is disabled and the\n  # \"openid\" scope is not requested.\n  #\n  #userinfo_endpoint: \"https://accounts.example.com/userinfo\"\n\n  # URI where to fetch the JWKS. Required if discovery is disabled and the\n  # \"openid\" scope is used.\n  #\n  #jwks_uri: \"https://accounts.example.com/.well-known/jwks.json\"\n\n  # Uncomment to skip metadata verification. Defaults to false.\n  #\n  # Use this if you are connecting to a provider that is not OpenID Connect\n  # compliant.\n  # Avoid this in production.\n  #\n  #skip_verification: true\n\n  # Whether to fetch the user profile from the userinfo endpoint. Valid\n  # values are: \"auto\" or \"userinfo_endpoint\".\n  #\n  # Defaults to \"auto\", which fetches the userinfo endpoint if \"openid\" is included\n  # in `scopes`. Uncomment the following to always fetch the userinfo endpoint.\n  #\n  #user_profile_method: \"userinfo_endpoint\"\n\n  # Uncomment to allow a user logging in via OIDC to match a pre-existing account instead\n  # of failing. This could be used if switching from password logins to OIDC. Defaults to false.\n  #\n  #allow_existing_users: true\n\n  # An external module can be provided here as a custom solution to mapping\n  # attributes returned from a OIDC provider onto a matrix user.\n  #\n  user_mapping_provider:\n    # The custom module's class. Uncomment to use a custom module.\n    # Default is 'synapse.handlers.oidc_handler.JinjaOidcMappingProvider'.\n    #\n    # See https://github.com/matrix-org/synapse/blob/master/docs/sso_mapping_providers.md#openid-mapping-providers\n    # for information on implementing a custom mapping provider.\n    #\n    #module: mapping_provider.OidcMappingProvider\n\n    # Custom configuration values for the module. This section will be passed as\n    # a Python dictionary to the user mapping provider module's `parse_config`\n    # method.\n    #\n    # The examples below are intended for the default provider: they should be\n    # changed if using a custom provider.\n    #\n    config:\n      # name of the claim containing a unique identifier for the user.\n      # Defaults to `sub`, which OpenID Connect compliant providers should provide.\n      #\n      #subject_claim: \"sub\"\n\n      # Jinja2 template for the localpart of the MXID.\n      #\n      # When rendering, this template is given the following variables:\n      #   * user: The claims returned by the UserInfo Endpoint and/or in the ID\n      #     Token\n      #\n      # This must be configured if using the default mapping provider.\n      #\n      localpart_template: \"{{ user.preferred_username }}\"\n\n      # Jinja2 template for the display name to set on first login.\n      #\n      # If unset, no displayname will be set.\n      #\n      #display_name_template: \"{{ user.given_name }} {{ user.last_name }}\"\n\n      # Jinja2 templates for extra attributes to send back to the client during\n      # login.\n      #\n      # Note that these are non-standard and clients will ignore them without modifications.\n      #\n      #extra_attributes:\n        #birthdate: \"{{ user.birthdate }}\"\n\n\n\n# Enable Central Authentication Service (CAS) for registration and login.\n#\ncas_config:\n  # Uncomment the following to enable authorization against a CAS server.\n  # Defaults to false.\n  #\n  #enabled: true\n\n  # The URL of the CAS authorization endpoint.\n  #\n  #server_url: \"https://cas-server.com\"\n\n  # The public URL of the homeserver.\n  #\n  #service_url: \"https://homeserver.domain.com:8448\"\n\n  # The attribute of the CAS response to use as the display name.\n  #\n  # If unset, no displayname will be set.\n  #\n  #displayname_attribute: name\n\n  # It is possible to configure Synapse to only allow logins if CAS attributes\n  # match particular values. All of the keys in the mapping below must exist\n  # and the values must match the given value. Alternately if the given value\n  # is None then any value is allowed (the attribute just must exist).\n  # All of the listed attributes must match for the login to be permitted.\n  #\n  #required_attributes:\n  #  userGroup: \"staff\"\n  #  department: None\n\n\n# Additional settings to use with single-sign on systems such as OpenID Connect,\n# SAML2 and CAS.\n#\nsso:\n    # A list of client URLs which are whitelisted so that the user does not\n    # have to confirm giving access to their account to the URL. Any client\n    # whose URL starts with an entry in the following list will not be subject\n    # to an additional confirmation step after the SSO login is completed.\n    #\n    # WARNING: An entry such as \"https://my.client\" is insecure, because it\n    # will also match \"https://my.client.evil.site\", exposing your users to\n    # phishing attacks from evil.site. To avoid this, include a slash after the\n    # hostname: \"https://my.client/\".\n    #\n    # If public_baseurl is set, then the login fallback page (used by clients\n    # that don't natively support the required login flows) is whitelisted in\n    # addition to any URLs in this list.\n    #\n    # By default, this list is empty.\n    #\n    #client_whitelist:\n    #  - https://riot.im/develop\n    #  - https://my.custom.client/\n\n    # Directory in which Synapse will try to find the template files below.\n    # If not set, default templates from within the Synapse package will be used.\n    #\n    # DO NOT UNCOMMENT THIS SETTING unless you want to customise the templates.\n    # If you *do* uncomment it, you will need to make sure that all the templates\n    # below are in the directory.\n    #\n    # Synapse will look for the following templates in this directory:\n    #\n    # * HTML page for a confirmation step before redirecting back to the client\n    #   with the login token: 'sso_redirect_confirm.html'.\n    #\n    #   When rendering, this template is given three variables:\n    #     * redirect_url: the URL the user is about to be redirected to. Needs\n    #                     manual escaping (see\n    #                     https://jinja.palletsprojects.com/en/2.11.x/templates/#html-escaping).\n    #\n    #     * display_url: the same as `redirect_url`, but with the query\n    #                    parameters stripped. The intention is to have a\n    #                    human-readable URL to show to users, not to use it as\n    #                    the final address to redirect to. Needs manual escaping\n    #                    (see https://jinja.palletsprojects.com/en/2.11.x/templates/#html-escaping).\n    #\n    #     * server_name: the homeserver's name.\n    #\n    # * HTML page which notifies the user that they are authenticating to confirm\n    #   an operation on their account during the user interactive authentication\n    #   process: 'sso_auth_confirm.html'.\n    #\n    #   When rendering, this template is given the following variables:\n    #     * redirect_url: the URL the user is about to be redirected to. Needs\n    #                     manual escaping (see\n    #                     https://jinja.palletsprojects.com/en/2.11.x/templates/#html-escaping).\n    #\n    #     * description: the operation which the user is being asked to confirm\n    #\n    # * HTML page shown after a successful user interactive authentication session:\n    #   'sso_auth_success.html'.\n    #\n    #   Note that this page must include the JavaScript which notifies of a successful authentication\n    #   (see https://matrix.org/docs/spec/client_server/r0.6.0#fallback).\n    #\n    #   This template has no additional variables.\n    #\n    # * HTML page shown during single sign-on if a deactivated user (according to Synapse's database)\n    #   attempts to login: 'sso_account_deactivated.html'.\n    #\n    #   This template has no additional variables.\n    #\n    # * HTML page to display to users if something goes wrong during the\n    #   OpenID Connect authentication process: 'sso_error.html'.\n    #\n    #   When rendering, this template is given two variables:\n    #     * error: the technical name of the error\n    #     * error_description: a human-readable message for the error\n    #\n    # You can see the default templates at:\n    # https://github.com/matrix-org/synapse/tree/master/synapse/res/templates\n    #\n    #template_dir: \"res/templates\"\n\n\n# JSON web token integration. The following settings can be used to make\n# Synapse JSON web tokens for authentication, instead of its internal\n# password database.\n#\n# Each JSON Web Token needs to contain a \"sub\" (subject) claim, which is\n# used as the localpart of the mxid.\n#\n# Additionally, the expiration time (\"exp\"), not before time (\"nbf\"),\n# and issued at (\"iat\") claims are validated if present.\n#\n# Note that this is a non-standard login type and client support is\n# expected to be non-existent.\n#\n# See https://github.com/matrix-org/synapse/blob/master/docs/jwt.md.\n#\n#jwt_config:\n    # Uncomment the following to enable authorization using JSON web\n    # tokens. Defaults to false.\n    #\n    #enabled: true\n\n    # This is either the private shared secret or the public key used to\n    # decode the contents of the JSON web token.\n    #\n    # Required if 'enabled' is true.\n    #\n    #secret: \"provided-by-your-issuer\"\n\n    # The algorithm used to sign the JSON web token.\n    #\n    # Supported algorithms are listed at\n    # https://pyjwt.readthedocs.io/en/latest/algorithms.html\n    #\n    # Required if 'enabled' is true.\n    #\n    #algorithm: \"provided-by-your-issuer\"\n\n    # The issuer to validate the \"iss\" claim against.\n    #\n    # Optional, if provided the \"iss\" claim will be required and\n    # validated for all JSON web tokens.\n    #\n    #issuer: \"provided-by-your-issuer\"\n\n    # A list of audiences to validate the \"aud\" claim against.\n    #\n    # Optional, if provided the \"aud\" claim will be required and\n    # validated for all JSON web tokens.\n    #\n    # Note that if the \"aud\" claim is included in a JSON web token then\n    # validation will fail without configuring audiences.\n    #\n    #audiences:\n    #    - \"provided-by-your-issuer\"\n\n\npassword_config:\n   # Uncomment to disable password login\n   #\n   #enabled: false\n\n   # Uncomment to disable authentication against the local password\n   # database. This is ignored if `enabled` is false, and is only useful\n   # if you have other password_providers.\n   #\n   #localdb_enabled: false\n\n   # Uncomment and change to a secret random string for extra security.\n   # DO NOT CHANGE THIS AFTER INITIAL SETUP!\n   #\n   #pepper: \"EVEN_MORE_SECRET\"\n\n   # Define and enforce a password policy. Each parameter is optional.\n   # This is an implementation of MSC2000.\n   #\n   policy:\n      # Whether to enforce the password policy.\n      # Defaults to 'false'.\n      #\n      #enabled: true\n\n      # Minimum accepted length for a password.\n      # Defaults to 0.\n      #\n      #minimum_length: 15\n\n      # Whether a password must contain at least one digit.\n      # Defaults to 'false'.\n      #\n      #require_digit: true\n\n      # Whether a password must contain at least one symbol.\n      # A symbol is any character that's not a number or a letter.\n      # Defaults to 'false'.\n      #\n      #require_symbol: true\n\n      # Whether a password must contain at least one lowercase letter.\n      # Defaults to 'false'.\n      #\n      #require_lowercase: true\n\n      # Whether a password must contain at least one lowercase letter.\n      # Defaults to 'false'.\n      #\n      #require_uppercase: true\n\n\n# Configuration for sending emails from Synapse.\n#\nemail:\n  # The hostname of the outgoing SMTP server to use. Defaults to 'localhost'.\n  #\n  #smtp_host: mail.server\n\n  # The port on the mail server for outgoing SMTP. Defaults to 25.\n  #\n  #smtp_port: 587\n\n  # Username/password for authentication to the SMTP server. By default, no\n  # authentication is attempted.\n  #\n  #smtp_user: \"exampleusername\"\n  #smtp_pass: \"examplepassword\"\n\n  # Uncomment the following to require TLS transport security for SMTP.\n  # By default, Synapse will connect over plain text, and will then switch to\n  # TLS via STARTTLS *if the SMTP server supports it*. If this option is set,\n  # Synapse will refuse to connect unless the server supports STARTTLS.\n  #\n  #require_transport_security: true\n\n  # notif_from defines the \"From\" address to use when sending emails.\n  # It must be set if email sending is enabled.\n  #\n  # The placeholder '%(app)s' will be replaced by the application name,\n  # which is normally 'app_name' (below), but may be overridden by the\n  # Matrix client application.\n  #\n  # Note that the placeholder must be written '%(app)s', including the\n  # trailing 's'.\n  #\n  #notif_from: \"Your Friendly %(app)s homeserver <noreply@example.com>\"\n\n  # app_name defines the default value for '%(app)s' in notif_from and email\n  # subjects. It defaults to 'Matrix'.\n  #\n  #app_name: my_branded_matrix_server\n\n  # Uncomment the following to enable sending emails for messages that the user\n  # has missed. Disabled by default.\n  #\n  #enable_notifs: true\n\n  # Uncomment the following to disable automatic subscription to email\n  # notifications for new users. Enabled by default.\n  #\n  #notif_for_new_users: false\n\n  # Custom URL for client links within the email notifications. By default\n  # links will be based on \"https://matrix.to\".\n  #\n  # (This setting used to be called riot_base_url; the old name is still\n  # supported for backwards-compatibility but is now deprecated.)\n  #\n  #client_base_url: \"http://localhost/riot\"\n\n  # Configure the time that a validation email will expire after sending.\n  # Defaults to 1h.\n  #\n  #validation_token_lifetime: 15m\n\n  # Directory in which Synapse will try to find the template files below.\n  # If not set, default templates from within the Synapse package will be used.\n  #\n  # Do not uncomment this setting unless you want to customise the templates.\n  #\n  # Synapse will look for the following templates in this directory:\n  #\n  # * The contents of email notifications of missed events: 'notif_mail.html' and\n  #   'notif_mail.txt'.\n  #\n  # * The contents of account expiry notice emails: 'notice_expiry.html' and\n  #   'notice_expiry.txt'.\n  #\n  # * The contents of password reset emails sent by the homeserver:\n  #   'password_reset.html' and 'password_reset.txt'\n  #\n  # * An HTML page that a user will see when they follow the link in the password\n  #   reset email. The user will be asked to confirm the action before their\n  #   password is reset: 'password_reset_confirmation.html'\n  #\n  # * HTML pages for success and failure that a user will see when they confirm\n  #   the password reset flow using the page above: 'password_reset_success.html'\n  #   and 'password_reset_failure.html'\n  #\n  # * The contents of address verification emails sent during registration:\n  #   'registration.html' and 'registration.txt'\n  #\n  # * HTML pages for success and failure that a user will see when they follow\n  #   the link in an address verification email sent during registration:\n  #   'registration_success.html' and 'registration_failure.html'\n  #\n  # * The contents of address verification emails sent when an address is added\n  #   to a Matrix account: 'add_threepid.html' and 'add_threepid.txt'\n  #\n  # * HTML pages for success and failure that a user will see when they follow\n  #   the link in an address verification email sent when an address is added\n  #   to a Matrix account: 'add_threepid_success.html' and\n  #   'add_threepid_failure.html'\n  #\n  # You can see the default templates at:\n  # https://github.com/matrix-org/synapse/tree/master/synapse/res/templates\n  #\n  #template_dir: \"res/templates\"\n\n  # Subjects to use when sending emails from Synapse.\n  #\n  # The placeholder '%(app)s' will be replaced with the value of the 'app_name'\n  # setting above, or by a value dictated by the Matrix client application.\n  #\n  # If a subject isn't overridden in this configuration file, the value used as\n  # its example will be used.\n  #\n  #subjects:\n\n    # Subjects for notification emails.\n    #\n    # On top of the '%(app)s' placeholder, these can use the following\n    # placeholders:\n    #\n    #   * '%(person)s', which will be replaced by the display name of the user(s)\n    #      that sent the message(s), e.g. \"Alice and Bob\".\n    #   * '%(room)s', which will be replaced by the name of the room the\n    #      message(s) have been sent to, e.g. \"My super room\".\n    #\n    # See the example provided for each setting to see which placeholder can be\n    # used and how to use them.\n    #\n    # Subject to use to notify about one message from one or more user(s) in a\n    # room which has a name.\n    #message_from_person_in_room: \"[%(app)s] You have a message on %(app)s from %(person)s in the %(room)s room...\"\n    #\n    # Subject to use to notify about one message from one or more user(s) in a\n    # room which doesn't have a name.\n    #message_from_person: \"[%(app)s] You have a message on %(app)s from %(person)s...\"\n    #\n    # Subject to use to notify about multiple messages from one or more users in\n    # a room which doesn't have a name.\n    #messages_from_person: \"[%(app)s] You have messages on %(app)s from %(person)s...\"\n    #\n    # Subject to use to notify about multiple messages in a room which has a\n    # name.\n    #messages_in_room: \"[%(app)s] You have messages on %(app)s in the %(room)s room...\"\n    #\n    # Subject to use to notify about multiple messages in multiple rooms.\n    #messages_in_room_and_others: \"[%(app)s] You have messages on %(app)s in the %(room)s room and others...\"\n    #\n    # Subject to use to notify about multiple messages from multiple persons in\n    # multiple rooms. This is similar to the setting above except it's used when\n    # the room in which the notification was triggered has no name.\n    #messages_from_person_and_others: \"[%(app)s] You have messages on %(app)s from %(person)s and others...\"\n    #\n    # Subject to use to notify about an invite to a room which has a name.\n    #invite_from_person_to_room: \"[%(app)s] %(person)s has invited you to join the %(room)s room on %(app)s...\"\n    #\n    # Subject to use to notify about an invite to a room which doesn't have a\n    # name.\n    #invite_from_person: \"[%(app)s] %(person)s has invited you to chat on %(app)s...\"\n\n    # Subject for emails related to account administration.\n    #\n    # On top of the '%(app)s' placeholder, these one can use the\n    # '%(server_name)s' placeholder, which will be replaced by the value of the\n    # 'server_name' setting in your Synapse configuration.\n    #\n    # Subject to use when sending a password reset email.\n    #password_reset: \"[%(server_name)s] Password reset\"\n    #\n    # Subject to use when sending a verification email to assert an address's\n    # ownership.\n    #email_validation: \"[%(server_name)s] Validate your email\"\n\n\n# Password providers allow homeserver administrators to integrate\n# their Synapse installation with existing authentication methods\n# ex. LDAP, external tokens, etc.\n#\n# For more information and known implementations, please see\n# https://github.com/matrix-org/synapse/blob/master/docs/password_auth_providers.md\n#\n# Note: instances wishing to use SAML or CAS authentication should\n# instead use the `saml2_config` or `cas_config` options,\n# respectively.\n#\npassword_providers:\n#    # Example config for an LDAP auth provider\n#    - module: \"ldap_auth_provider.LdapAuthProvider\"\n#      config:\n#        enabled: true\n#        uri: \"ldap://ldap.example.com:389\"\n#        start_tls: true\n#        base: \"ou=users,dc=example,dc=com\"\n#        attributes:\n#           uid: \"cn\"\n#           mail: \"email\"\n#           name: \"givenName\"\n#        #bind_dn:\n#        #bind_password:\n#        #filter: \"(objectClass=posixAccount)\"\n\n\n\n## Push ##\n\npush:\n  # Clients requesting push notifications can either have the body of\n  # the message sent in the notification poke along with other details\n  # like the sender, or just the event ID and room ID (`event_id_only`).\n  # If clients choose the former, this option controls whether the\n  # notification request includes the content of the event (other details\n  # like the sender are still included). For `event_id_only` push, it\n  # has no effect.\n  #\n  # For modern android devices the notification content will still appear\n  # because it is loaded by the app. iPhone, however will send a\n  # notification saying only that a message arrived and who it came from.\n  #\n  # The default value is \"true\" to include message details. Uncomment to only\n  # include the event ID and room ID in push notification payloads.\n  #\n  #include_content: false\n\n  # When a push notification is received, an unread count is also sent.\n  # This number can either be calculated as the number of unread messages\n  # for the user, or the number of *rooms* the user has unread messages in.\n  #\n  # The default value is \"true\", meaning push clients will see the number of\n  # rooms with unread messages in them. Uncomment to instead send the number\n  # of unread messages.\n  #\n  #group_unread_count_by_room: false\n\n\n# Spam checkers are third-party modules that can block specific actions\n# of local users, such as creating rooms and registering undesirable\n# usernames, as well as remote users by redacting incoming events.\n#\nspam_checker:\n   #- module: \"my_custom_project.SuperSpamChecker\"\n   #  config:\n   #    example_option: 'things'\n   #- module: \"some_other_project.BadEventStopper\"\n   #  config:\n   #    example_stop_events_from: ['@bad:example.com']\n\n\n## Rooms ##\n\n# Controls whether locally-created rooms should be end-to-end encrypted by\n# default.\n#\n# Possible options are \"all\", \"invite\", and \"off\". They are defined as:\n#\n# * \"all\": any locally-created room\n# * \"invite\": any room created with the \"private_chat\" or \"trusted_private_chat\"\n#             room creation presets\n# * \"off\": this option will take no effect\n#\n# The default value is \"off\".\n#\n# Note that this option will only affect rooms created after it is set. It\n# will also not affect rooms created by other servers.\n#\n#encryption_enabled_by_default_for_room_type: invite\n\n\n# Uncomment to allow non-server-admin users to create groups on this server\n#\n#enable_group_creation: true\n\n# If enabled, non server admins can only create groups with local parts\n# starting with this prefix\n#\n#group_creation_prefix: \"unofficial/\"\n\n\n\n# User Directory configuration\n#\n# 'enabled' defines whether users can search the user directory. If\n# false then empty responses are returned to all queries. Defaults to\n# true.\n#\n# 'search_all_users' defines whether to search all users visible to your HS\n# when searching the user directory, rather than limiting to users visible\n# in public rooms.  Defaults to false.  If you set it True, you'll have to\n# rebuild the user_directory search indexes, see\n# https://github.com/matrix-org/synapse/blob/master/docs/user_directory.md\n#\n#user_directory:\n#  enabled: true\n#  search_all_users: false\n\n\n# User Consent configuration\n#\n# for detailed instructions, see\n# https://github.com/matrix-org/synapse/blob/master/docs/consent_tracking.md\n#\n# Parts of this section are required if enabling the 'consent' resource under\n# 'listeners', in particular 'template_dir' and 'version'.\n#\n# 'template_dir' gives the location of the templates for the HTML forms.\n# This directory should contain one subdirectory per language (eg, 'en', 'fr'),\n# and each language directory should contain the policy document (named as\n# '<version>.html') and a success page (success.html).\n#\n# 'version' specifies the 'current' version of the policy document. It defines\n# the version to be served by the consent resource if there is no 'v'\n# parameter.\n#\n# 'server_notice_content', if enabled, will send a user a \"Server Notice\"\n# asking them to consent to the privacy policy. The 'server_notices' section\n# must also be configured for this to work. Notices will *not* be sent to\n# guest users unless 'send_server_notice_to_guests' is set to true.\n#\n# 'block_events_error', if set, will block any attempts to send events\n# until the user consents to the privacy policy. The value of the setting is\n# used as the text of the error.\n#\n# 'require_at_registration', if enabled, will add a step to the registration\n# process, similar to how captcha works. Users will be required to accept the\n# policy before their account is created.\n#\n# 'policy_name' is the display name of the policy users will see when registering\n# for an account. Has no effect unless `require_at_registration` is enabled.\n# Defaults to \"Privacy Policy\".\n#\n#user_consent:\n#  template_dir: res/templates/privacy\n#  version: 1.0\n#  server_notice_content:\n#    msgtype: m.text\n#    body: >-\n#      To continue using this homeserver you must review and agree to the\n#      terms and conditions at %(consent_uri)s\n#  send_server_notice_to_guests: true\n#  block_events_error: >-\n#    To continue using this homeserver you must review and agree to the\n#    terms and conditions at %(consent_uri)s\n#  require_at_registration: false\n#  policy_name: Privacy Policy\n#\n\n\n\n# Local statistics collection. Used in populating the room directory.\n#\n# 'bucket_size' controls how large each statistics timeslice is. It can\n# be defined in a human readable short form -- e.g. \"1d\", \"1y\".\n#\n# 'retention' controls how long historical statistics will be kept for.\n# It can be defined in a human readable short form -- e.g. \"1d\", \"1y\".\n#\n#\n#stats:\n#   enabled: true\n#   bucket_size: 1d\n#   retention: 1y\n\n\n# Server Notices room configuration\n#\n# Uncomment this section to enable a room which can be used to send notices\n# from the server to users. It is a special room which cannot be left; notices\n# come from a special \"notices\" user id.\n#\n# If you uncomment this section, you *must* define the system_mxid_localpart\n# setting, which defines the id of the user which will be used to send the\n# notices.\n#\n# It's also possible to override the room name, the display name of the\n# \"notices\" user, and the avatar for the user.\n#\n#server_notices:\n#  system_mxid_localpart: notices\n#  system_mxid_display_name: \"Server Notices\"\n#  system_mxid_avatar_url: \"mxc://server.com/oumMVlgDnLYFaPVkExemNVVZ\"\n#  room_name: \"Server Notices\"\n\n\n\n# Uncomment to disable searching the public room list. When disabled\n# blocks searching local and remote room lists for local and remote\n# users by always returning an empty list for all queries.\n#\n#enable_room_list_search: false\n\n# The `alias_creation` option controls who's allowed to create aliases\n# on this server.\n#\n# The format of this option is a list of rules that contain globs that\n# match against user_id, room_id and the new alias (fully qualified with\n# server name). The action in the first rule that matches is taken,\n# which can currently either be \"allow\" or \"deny\".\n#\n# Missing user_id/room_id/alias fields default to \"*\".\n#\n# If no rules match the request is denied. An empty list means no one\n# can create aliases.\n#\n# Options for the rules include:\n#\n#   user_id: Matches against the creator of the alias\n#   alias: Matches against the alias being created\n#   room_id: Matches against the room ID the alias is being pointed at\n#   action: Whether to \"allow\" or \"deny\" the request if the rule matches\n#\n# The default is:\n#\n#alias_creation_rules:\n#  - user_id: \"*\"\n#    alias: \"*\"\n#    room_id: \"*\"\n#    action: allow\n\n# The `room_list_publication_rules` option controls who can publish and\n# which rooms can be published in the public room list.\n#\n# The format of this option is the same as that for\n# `alias_creation_rules`.\n#\n# If the room has one or more aliases associated with it, only one of\n# the aliases needs to match the alias rule. If there are no aliases\n# then only rules with `alias: *` match.\n#\n# If no rules match the request is denied. An empty list means no one\n# can publish rooms.\n#\n# Options for the rules include:\n#\n#   user_id: Matches against the creator of the alias\n#   room_id: Matches against the room ID being published\n#   alias: Matches against any current local or canonical aliases\n#            associated with the room\n#   action: Whether to \"allow\" or \"deny\" the request if the rule matches\n#\n# The default is:\n#\n#room_list_publication_rules:\n#  - user_id: \"*\"\n#    alias: \"*\"\n#    room_id: \"*\"\n#    action: allow\n\n\n# Server admins can define a Python module that implements extra rules for\n# allowing or denying incoming events. In order to work, this module needs to\n# override the methods defined in synapse/events/third_party_rules.py.\n#\n# This feature is designed to be used in closed federations only, where each\n# participating server enforces the same rules.\n#\n#third_party_event_rules:\n#  module: \"my_custom_project.SuperRulesSet\"\n#  config:\n#    example_option: 'things'\n\n\n## Opentracing ##\n\n# These settings enable opentracing, which implements distributed tracing.\n# This allows you to observe the causal chains of events across servers\n# including requests, key lookups etc., across any server running\n# synapse or any other other services which supports opentracing\n# (specifically those implemented with Jaeger).\n#\nopentracing:\n    # tracing is disabled by default. Uncomment the following line to enable it.\n    #\n    #enabled: true\n\n    # The list of homeservers we wish to send and receive span contexts and span baggage.\n    # See docs/opentracing.rst\n    # This is a list of regexes which are matched against the server_name of the\n    # homeserver.\n    #\n    # By default, it is empty, so no servers are matched.\n    #\n    #homeserver_whitelist:\n    #  - \".*\"\n\n    # Jaeger can be configured to sample traces at different rates.\n    # All configuration options provided by Jaeger can be set here.\n    # Jaeger's configuration mostly related to trace sampling which\n    # is documented here:\n    # https://www.jaegertracing.io/docs/1.13/sampling/.\n    #\n    #jaeger_config:\n    #  sampler:\n    #    type: const\n    #    param: 1\n\n    #  Logging whether spans were started and reported\n    #\n    #  logging:\n    #    false\n\n\n## Workers ##\n\n# Disables sending of outbound federation transactions on the main process.\n# Uncomment if using a federation sender worker.\n#\n#send_federation: false\n\n# It is possible to run multiple federation sender workers, in which case the\n# work is balanced across them.\n#\n# This configuration must be shared between all federation sender workers, and if\n# changed all federation sender workers must be stopped at the same time and then\n# started, to ensure that all instances are running with the same config (otherwise\n# events may be dropped).\n#\n#federation_sender_instances:\n#  - federation_sender1\n\n# When using workers this should be a map from `worker_name` to the\n# HTTP replication listener of the worker, if configured.\n#\n#instance_map:\n#  worker1:\n#    host: localhost\n#    port: 8034\n\n# Experimental: When using workers you can define which workers should\n# handle event persistence and typing notifications. Any worker\n# specified here must also be in the `instance_map`.\n#\n#stream_writers:\n#  events: worker1\n#  typing: worker1\n\n# The worker that is used to run background tasks (e.g. cleaning up expired\n# data). If not provided this defaults to the main process.\n#\n#run_background_tasks_on: worker1\n\n\n# Configuration for Redis when using workers. This *must* be enabled when\n# using workers (unless using old style direct TCP configuration).\n#\nredis:\n  # Uncomment the below to enable Redis support.\n  #\n  #enabled: true\n\n  # Optional host and port to use to connect to redis. Defaults to\n  # localhost and 6379\n  #\n  #host: localhost\n  #port: 6379\n\n  # Optional password if configured on the Redis instance\n  #\n  #password: <secret_password>\n", "patch": "@@ -642,17 +642,19 @@ acme:\n #  - nyc.example.com\n #  - syd.example.com\n \n-# Prevent federation requests from being sent to the following\n-# blacklist IP address CIDR ranges. If this option is not specified, or\n-# specified with an empty list, no ip range blacklist will be enforced.\n+# Prevent outgoing requests from being sent to the following blacklisted IP address\n+# CIDR ranges. If this option is not specified, or specified with an empty list,\n+# no IP range blacklist will be enforced.\n #\n-# As of Synapse v1.4.0 this option also affects any outbound requests to identity\n-# servers provided by user input.\n+# The blacklist applies to the outbound requests for federation, identity servers,\n+# push servers, and for checking key validitity for third-party invite events.\n #\n # (0.0.0.0 and :: are always blacklisted, whether or not they are explicitly\n # listed here, since they correspond to unroutable addresses.)\n #\n-federation_ip_range_blacklist:\n+# This option replaces federation_ip_range_blacklist in Synapse v1.24.0.\n+#\n+ip_range_blacklist:\n   - '127.0.0.0/8'\n   - '10.0.0.0/8'\n   - '172.16.0.0/12'", "file_path": "files/2021_2/16", "file_language": "yaml", "file_name": "docs/sample_config.yaml", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Fapp%2Fgeneric_worker.py", "code": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Copyright 2016 OpenMarket Ltd\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport contextlib\nimport logging\nimport sys\nfrom typing import Dict, Iterable, Optional, Set\n\nfrom typing_extensions import ContextManager\n\nfrom twisted.internet import address, reactor\n\nimport synapse\nimport synapse.events\nfrom synapse.api.errors import HttpResponseException, RequestSendFailed, SynapseError\nfrom synapse.api.urls import (\n    CLIENT_API_PREFIX,\n    FEDERATION_PREFIX,\n    LEGACY_MEDIA_PREFIX,\n    MEDIA_PREFIX,\n    SERVER_KEY_V2_PREFIX,\n)\nfrom synapse.app import _base\nfrom synapse.config._base import ConfigError\nfrom synapse.config.homeserver import HomeServerConfig\nfrom synapse.config.logger import setup_logging\nfrom synapse.config.server import ListenerConfig\nfrom synapse.federation import send_queue\nfrom synapse.federation.transport.server import TransportLayerServer\nfrom synapse.handlers.presence import (\n    BasePresenceHandler,\n    PresenceState,\n    get_interested_parties,\n)\nfrom synapse.http.server import JsonResource, OptionsResource\nfrom synapse.http.servlet import RestServlet, parse_json_object_from_request\nfrom synapse.http.site import SynapseSite\nfrom synapse.logging.context import LoggingContext\nfrom synapse.metrics import METRICS_PREFIX, MetricsResource, RegistryProxy\nfrom synapse.metrics.background_process_metrics import run_as_background_process\nfrom synapse.replication.http import REPLICATION_PREFIX, ReplicationRestResource\nfrom synapse.replication.http.presence import (\n    ReplicationBumpPresenceActiveTime,\n    ReplicationPresenceSetState,\n)\nfrom synapse.replication.slave.storage._base import BaseSlavedStore\nfrom synapse.replication.slave.storage.account_data import SlavedAccountDataStore\nfrom synapse.replication.slave.storage.appservice import SlavedApplicationServiceStore\nfrom synapse.replication.slave.storage.client_ips import SlavedClientIpStore\nfrom synapse.replication.slave.storage.deviceinbox import SlavedDeviceInboxStore\nfrom synapse.replication.slave.storage.devices import SlavedDeviceStore\nfrom synapse.replication.slave.storage.directory import DirectoryStore\nfrom synapse.replication.slave.storage.events import SlavedEventStore\nfrom synapse.replication.slave.storage.filtering import SlavedFilteringStore\nfrom synapse.replication.slave.storage.groups import SlavedGroupServerStore\nfrom synapse.replication.slave.storage.keys import SlavedKeyStore\nfrom synapse.replication.slave.storage.presence import SlavedPresenceStore\nfrom synapse.replication.slave.storage.profile import SlavedProfileStore\nfrom synapse.replication.slave.storage.push_rule import SlavedPushRuleStore\nfrom synapse.replication.slave.storage.pushers import SlavedPusherStore\nfrom synapse.replication.slave.storage.receipts import SlavedReceiptsStore\nfrom synapse.replication.slave.storage.registration import SlavedRegistrationStore\nfrom synapse.replication.slave.storage.room import RoomStore\nfrom synapse.replication.slave.storage.transactions import SlavedTransactionStore\nfrom synapse.replication.tcp.client import ReplicationDataHandler\nfrom synapse.replication.tcp.commands import ClearUserSyncsCommand\nfrom synapse.replication.tcp.streams import (\n    AccountDataStream,\n    DeviceListsStream,\n    GroupServerStream,\n    PresenceStream,\n    PushersStream,\n    PushRulesStream,\n    ReceiptsStream,\n    TagAccountDataStream,\n    ToDeviceStream,\n)\nfrom synapse.rest.admin import register_servlets_for_media_repo\nfrom synapse.rest.client.v1 import events\nfrom synapse.rest.client.v1.initial_sync import InitialSyncRestServlet\nfrom synapse.rest.client.v1.login import LoginRestServlet\nfrom synapse.rest.client.v1.profile import (\n    ProfileAvatarURLRestServlet,\n    ProfileDisplaynameRestServlet,\n    ProfileRestServlet,\n)\nfrom synapse.rest.client.v1.push_rule import PushRuleRestServlet\nfrom synapse.rest.client.v1.room import (\n    JoinedRoomMemberListRestServlet,\n    JoinRoomAliasServlet,\n    PublicRoomListRestServlet,\n    RoomEventContextServlet,\n    RoomInitialSyncRestServlet,\n    RoomMemberListRestServlet,\n    RoomMembershipRestServlet,\n    RoomMessageListRestServlet,\n    RoomSendEventRestServlet,\n    RoomStateEventRestServlet,\n    RoomStateRestServlet,\n    RoomTypingRestServlet,\n)\nfrom synapse.rest.client.v1.voip import VoipRestServlet\nfrom synapse.rest.client.v2_alpha import groups, sync, user_directory\nfrom synapse.rest.client.v2_alpha._base import client_patterns\nfrom synapse.rest.client.v2_alpha.account import ThreepidRestServlet\nfrom synapse.rest.client.v2_alpha.account_data import (\n    AccountDataServlet,\n    RoomAccountDataServlet,\n)\nfrom synapse.rest.client.v2_alpha.keys import KeyChangesServlet, KeyQueryServlet\nfrom synapse.rest.client.v2_alpha.register import RegisterRestServlet\nfrom synapse.rest.client.versions import VersionsRestServlet\nfrom synapse.rest.health import HealthResource\nfrom synapse.rest.key.v2 import KeyApiV2Resource\nfrom synapse.server import HomeServer, cache_in_self\nfrom synapse.storage.databases.main.censor_events import CensorEventsStore\nfrom synapse.storage.databases.main.client_ips import ClientIpWorkerStore\nfrom synapse.storage.databases.main.media_repository import MediaRepositoryStore\nfrom synapse.storage.databases.main.metrics import ServerMetricsStore\nfrom synapse.storage.databases.main.monthly_active_users import (\n    MonthlyActiveUsersWorkerStore,\n)\nfrom synapse.storage.databases.main.presence import UserPresenceState\nfrom synapse.storage.databases.main.search import SearchWorkerStore\nfrom synapse.storage.databases.main.stats import StatsStore\nfrom synapse.storage.databases.main.transactions import TransactionWorkerStore\nfrom synapse.storage.databases.main.ui_auth import UIAuthWorkerStore\nfrom synapse.storage.databases.main.user_directory import UserDirectoryStore\nfrom synapse.types import ReadReceipt\nfrom synapse.util.async_helpers import Linearizer\nfrom synapse.util.httpresourcetree import create_resource_tree\nfrom synapse.util.manhole import manhole\nfrom synapse.util.versionstring import get_version_string\n\nlogger = logging.getLogger(\"synapse.app.generic_worker\")\n\n\nclass PresenceStatusStubServlet(RestServlet):\n    \"\"\"If presence is disabled this servlet can be used to stub out setting\n    presence status.\n    \"\"\"\n\n    PATTERNS = client_patterns(\"/presence/(?P<user_id>[^/]*)/status\")\n\n    def __init__(self, hs):\n        super().__init__()\n        self.auth = hs.get_auth()\n\n    async def on_GET(self, request, user_id):\n        await self.auth.get_user_by_req(request)\n        return 200, {\"presence\": \"offline\"}\n\n    async def on_PUT(self, request, user_id):\n        await self.auth.get_user_by_req(request)\n        return 200, {}\n\n\nclass KeyUploadServlet(RestServlet):\n    \"\"\"An implementation of the `KeyUploadServlet` that responds to read only\n    requests, but otherwise proxies through to the master instance.\n    \"\"\"\n\n    PATTERNS = client_patterns(\"/keys/upload(/(?P<device_id>[^/]+))?$\")\n\n    def __init__(self, hs):\n        \"\"\"\n        Args:\n            hs (synapse.server.HomeServer): server\n        \"\"\"\n        super().__init__()\n        self.auth = hs.get_auth()\n        self.store = hs.get_datastore()\n        self.http_client = hs.get_simple_http_client()\n        self.main_uri = hs.config.worker_main_http_uri\n\n    async def on_POST(self, request, device_id):\n        requester = await self.auth.get_user_by_req(request, allow_guest=True)\n        user_id = requester.user.to_string()\n        body = parse_json_object_from_request(request)\n\n        if device_id is not None:\n            # passing the device_id here is deprecated; however, we allow it\n            # for now for compatibility with older clients.\n            if requester.device_id is not None and device_id != requester.device_id:\n                logger.warning(\n                    \"Client uploading keys for a different device \"\n                    \"(logged in as %s, uploading for %s)\",\n                    requester.device_id,\n                    device_id,\n                )\n        else:\n            device_id = requester.device_id\n\n        if device_id is None:\n            raise SynapseError(\n                400, \"To upload keys, you must pass device_id when authenticating\"\n            )\n\n        if body:\n            # They're actually trying to upload something, proxy to main synapse.\n\n            # Proxy headers from the original request, such as the auth headers\n            # (in case the access token is there) and the original IP /\n            # User-Agent of the request.\n            headers = {\n                header: request.requestHeaders.getRawHeaders(header, [])\n                for header in (b\"Authorization\", b\"User-Agent\")\n            }\n            # Add the previous hop the the X-Forwarded-For header.\n            x_forwarded_for = request.requestHeaders.getRawHeaders(\n                b\"X-Forwarded-For\", []\n            )\n            if isinstance(request.client, (address.IPv4Address, address.IPv6Address)):\n                previous_host = request.client.host.encode(\"ascii\")\n                # If the header exists, add to the comma-separated list of the first\n                # instance of the header. Otherwise, generate a new header.\n                if x_forwarded_for:\n                    x_forwarded_for = [\n                        x_forwarded_for[0] + b\", \" + previous_host\n                    ] + x_forwarded_for[1:]\n                else:\n                    x_forwarded_for = [previous_host]\n            headers[b\"X-Forwarded-For\"] = x_forwarded_for\n\n            try:\n                result = await self.http_client.post_json_get_json(\n                    self.main_uri + request.uri.decode(\"ascii\"), body, headers=headers\n                )\n            except HttpResponseException as e:\n                raise e.to_synapse_error() from e\n            except RequestSendFailed as e:\n                raise SynapseError(502, \"Failed to talk to master\") from e\n\n            return 200, result\n        else:\n            # Just interested in counts.\n            result = await self.store.count_e2e_one_time_keys(user_id, device_id)\n            return 200, {\"one_time_key_counts\": result}\n\n\nclass _NullContextManager(ContextManager[None]):\n    \"\"\"A context manager which does nothing.\"\"\"\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\nUPDATE_SYNCING_USERS_MS = 10 * 1000\n\n\nclass GenericWorkerPresence(BasePresenceHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.hs = hs\n        self.is_mine_id = hs.is_mine_id\n\n        self._presence_enabled = hs.config.use_presence\n\n        # The number of ongoing syncs on this process, by user id.\n        # Empty if _presence_enabled is false.\n        self._user_to_num_current_syncs = {}  # type: Dict[str, int]\n\n        self.notifier = hs.get_notifier()\n        self.instance_id = hs.get_instance_id()\n\n        # user_id -> last_sync_ms. Lists the users that have stopped syncing\n        # but we haven't notified the master of that yet\n        self.users_going_offline = {}\n\n        self._bump_active_client = ReplicationBumpPresenceActiveTime.make_client(hs)\n        self._set_state_client = ReplicationPresenceSetState.make_client(hs)\n\n        self._send_stop_syncing_loop = self.clock.looping_call(\n            self.send_stop_syncing, UPDATE_SYNCING_USERS_MS\n        )\n\n        hs.get_reactor().addSystemEventTrigger(\n            \"before\",\n            \"shutdown\",\n            run_as_background_process,\n            \"generic_presence.on_shutdown\",\n            self._on_shutdown,\n        )\n\n    def _on_shutdown(self):\n        if self._presence_enabled:\n            self.hs.get_tcp_replication().send_command(\n                ClearUserSyncsCommand(self.instance_id)\n            )\n\n    def send_user_sync(self, user_id, is_syncing, last_sync_ms):\n        if self._presence_enabled:\n            self.hs.get_tcp_replication().send_user_sync(\n                self.instance_id, user_id, is_syncing, last_sync_ms\n            )\n\n    def mark_as_coming_online(self, user_id):\n        \"\"\"A user has started syncing. Send a UserSync to the master, unless they\n        had recently stopped syncing.\n\n        Args:\n            user_id (str)\n        \"\"\"\n        going_offline = self.users_going_offline.pop(user_id, None)\n        if not going_offline:\n            # Safe to skip because we haven't yet told the master they were offline\n            self.send_user_sync(user_id, True, self.clock.time_msec())\n\n    def mark_as_going_offline(self, user_id):\n        \"\"\"A user has stopped syncing. We wait before notifying the master as\n        its likely they'll come back soon. This allows us to avoid sending\n        a stopped syncing immediately followed by a started syncing notification\n        to the master\n\n        Args:\n            user_id (str)\n        \"\"\"\n        self.users_going_offline[user_id] = self.clock.time_msec()\n\n    def send_stop_syncing(self):\n        \"\"\"Check if there are any users who have stopped syncing a while ago\n        and haven't come back yet. If there are poke the master about them.\n        \"\"\"\n        now = self.clock.time_msec()\n        for user_id, last_sync_ms in list(self.users_going_offline.items()):\n            if now - last_sync_ms > UPDATE_SYNCING_USERS_MS:\n                self.users_going_offline.pop(user_id, None)\n                self.send_user_sync(user_id, False, last_sync_ms)\n\n    async def user_syncing(\n        self, user_id: str, affect_presence: bool\n    ) -> ContextManager[None]:\n        \"\"\"Record that a user is syncing.\n\n        Called by the sync and events servlets to record that a user has connected to\n        this worker and is waiting for some events.\n        \"\"\"\n        if not affect_presence or not self._presence_enabled:\n            return _NullContextManager()\n\n        curr_sync = self._user_to_num_current_syncs.get(user_id, 0)\n        self._user_to_num_current_syncs[user_id] = curr_sync + 1\n\n        # If we went from no in flight sync to some, notify replication\n        if self._user_to_num_current_syncs[user_id] == 1:\n            self.mark_as_coming_online(user_id)\n\n        def _end():\n            # We check that the user_id is in user_to_num_current_syncs because\n            # user_to_num_current_syncs may have been cleared if we are\n            # shutting down.\n            if user_id in self._user_to_num_current_syncs:\n                self._user_to_num_current_syncs[user_id] -= 1\n\n                # If we went from one in flight sync to non, notify replication\n                if self._user_to_num_current_syncs[user_id] == 0:\n                    self.mark_as_going_offline(user_id)\n\n        @contextlib.contextmanager\n        def _user_syncing():\n            try:\n                yield\n            finally:\n                _end()\n\n        return _user_syncing()\n\n    async def notify_from_replication(self, states, stream_id):\n        parties = await get_interested_parties(self.store, states)\n        room_ids_to_states, users_to_states = parties\n\n        self.notifier.on_new_event(\n            \"presence_key\",\n            stream_id,\n            rooms=room_ids_to_states.keys(),\n            users=users_to_states.keys(),\n        )\n\n    async def process_replication_rows(self, token, rows):\n        states = [\n            UserPresenceState(\n                row.user_id,\n                row.state,\n                row.last_active_ts,\n                row.last_federation_update_ts,\n                row.last_user_sync_ts,\n                row.status_msg,\n                row.currently_active,\n            )\n            for row in rows\n        ]\n\n        for state in states:\n            self.user_to_current_state[state.user_id] = state\n\n        stream_id = token\n        await self.notify_from_replication(states, stream_id)\n\n    def get_currently_syncing_users_for_replication(self) -> Iterable[str]:\n        return [\n            user_id\n            for user_id, count in self._user_to_num_current_syncs.items()\n            if count > 0\n        ]\n\n    async def set_state(self, target_user, state, ignore_status_msg=False):\n        \"\"\"Set the presence state of the user.\n        \"\"\"\n        presence = state[\"presence\"]\n\n        valid_presence = (\n            PresenceState.ONLINE,\n            PresenceState.UNAVAILABLE,\n            PresenceState.OFFLINE,\n        )\n        if presence not in valid_presence:\n            raise SynapseError(400, \"Invalid presence state\")\n\n        user_id = target_user.to_string()\n\n        # If presence is disabled, no-op\n        if not self.hs.config.use_presence:\n            return\n\n        # Proxy request to master\n        await self._set_state_client(\n            user_id=user_id, state=state, ignore_status_msg=ignore_status_msg\n        )\n\n    async def bump_presence_active_time(self, user):\n        \"\"\"We've seen the user do something that indicates they're interacting\n        with the app.\n        \"\"\"\n        # If presence is disabled, no-op\n        if not self.hs.config.use_presence:\n            return\n\n        # Proxy request to master\n        user_id = user.to_string()\n        await self._bump_active_client(user_id=user_id)\n\n\nclass GenericWorkerSlavedStore(\n    # FIXME(#3714): We need to add UserDirectoryStore as we write directly\n    # rather than going via the correct worker.\n    UserDirectoryStore,\n    StatsStore,\n    UIAuthWorkerStore,\n    SlavedDeviceInboxStore,\n    SlavedDeviceStore,\n    SlavedReceiptsStore,\n    SlavedPushRuleStore,\n    SlavedGroupServerStore,\n    SlavedAccountDataStore,\n    SlavedPusherStore,\n    CensorEventsStore,\n    ClientIpWorkerStore,\n    SlavedEventStore,\n    SlavedKeyStore,\n    RoomStore,\n    DirectoryStore,\n    SlavedApplicationServiceStore,\n    SlavedRegistrationStore,\n    SlavedTransactionStore,\n    SlavedProfileStore,\n    SlavedClientIpStore,\n    SlavedPresenceStore,\n    SlavedFilteringStore,\n    MonthlyActiveUsersWorkerStore,\n    MediaRepositoryStore,\n    ServerMetricsStore,\n    SearchWorkerStore,\n    TransactionWorkerStore,\n    BaseSlavedStore,\n):\n    pass\n\n\nclass GenericWorkerServer(HomeServer):\n    DATASTORE_CLASS = GenericWorkerSlavedStore\n\n    def _listen_http(self, listener_config: ListenerConfig):\n        port = listener_config.port\n        bind_addresses = listener_config.bind_addresses\n\n        assert listener_config.http_options is not None\n\n        site_tag = listener_config.http_options.tag\n        if site_tag is None:\n            site_tag = port\n\n        # We always include a health resource.\n        resources = {\"/health\": HealthResource()}\n\n        for res in listener_config.http_options.resources:\n            for name in res.names:\n                if name == \"metrics\":\n                    resources[METRICS_PREFIX] = MetricsResource(RegistryProxy)\n                elif name == \"client\":\n                    resource = JsonResource(self, canonical_json=False)\n\n                    PublicRoomListRestServlet(self).register(resource)\n                    RoomMemberListRestServlet(self).register(resource)\n                    JoinedRoomMemberListRestServlet(self).register(resource)\n                    RoomStateRestServlet(self).register(resource)\n                    RoomEventContextServlet(self).register(resource)\n                    RoomMessageListRestServlet(self).register(resource)\n                    RegisterRestServlet(self).register(resource)\n                    LoginRestServlet(self).register(resource)\n                    ThreepidRestServlet(self).register(resource)\n                    KeyQueryServlet(self).register(resource)\n                    KeyChangesServlet(self).register(resource)\n                    VoipRestServlet(self).register(resource)\n                    PushRuleRestServlet(self).register(resource)\n                    VersionsRestServlet(self).register(resource)\n                    RoomSendEventRestServlet(self).register(resource)\n                    RoomMembershipRestServlet(self).register(resource)\n                    RoomStateEventRestServlet(self).register(resource)\n                    JoinRoomAliasServlet(self).register(resource)\n                    ProfileAvatarURLRestServlet(self).register(resource)\n                    ProfileDisplaynameRestServlet(self).register(resource)\n                    ProfileRestServlet(self).register(resource)\n                    KeyUploadServlet(self).register(resource)\n                    AccountDataServlet(self).register(resource)\n                    RoomAccountDataServlet(self).register(resource)\n                    RoomTypingRestServlet(self).register(resource)\n\n                    sync.register_servlets(self, resource)\n                    events.register_servlets(self, resource)\n                    InitialSyncRestServlet(self).register(resource)\n                    RoomInitialSyncRestServlet(self).register(resource)\n\n                    user_directory.register_servlets(self, resource)\n\n                    # If presence is disabled, use the stub servlet that does\n                    # not allow sending presence\n                    if not self.config.use_presence:\n                        PresenceStatusStubServlet(self).register(resource)\n\n                    groups.register_servlets(self, resource)\n\n                    resources.update({CLIENT_API_PREFIX: resource})\n                elif name == \"federation\":\n                    resources.update({FEDERATION_PREFIX: TransportLayerServer(self)})\n                elif name == \"media\":\n                    if self.config.can_load_media_repo:\n                        media_repo = self.get_media_repository_resource()\n\n                        # We need to serve the admin servlets for media on the\n                        # worker.\n                        admin_resource = JsonResource(self, canonical_json=False)\n                        register_servlets_for_media_repo(self, admin_resource)\n\n                        resources.update(\n                            {\n                                MEDIA_PREFIX: media_repo,\n                                LEGACY_MEDIA_PREFIX: media_repo,\n                                \"/_synapse/admin\": admin_resource,\n                            }\n                        )\n                    else:\n                        logger.warning(\n                            \"A 'media' listener is configured but the media\"\n                            \" repository is disabled. Ignoring.\"\n                        )\n\n                if name == \"openid\" and \"federation\" not in res.names:\n                    # Only load the openid resource separately if federation resource\n                    # is not specified since federation resource includes openid\n                    # resource.\n                    resources.update(\n                        {\n                            FEDERATION_PREFIX: TransportLayerServer(\n                                self, servlet_groups=[\"openid\"]\n                            )\n                        }\n                    )\n\n                if name in [\"keys\", \"federation\"]:\n                    resources[SERVER_KEY_V2_PREFIX] = KeyApiV2Resource(self)\n\n                if name == \"replication\":\n                    resources[REPLICATION_PREFIX] = ReplicationRestResource(self)\n\n        root_resource = create_resource_tree(resources, OptionsResource())\n\n        _base.listen_tcp(\n            bind_addresses,\n            port,\n            SynapseSite(\n                \"synapse.access.http.%s\" % (site_tag,),\n                site_tag,\n                listener_config,\n                root_resource,\n                self.version_string,\n            ),\n            reactor=self.get_reactor(),\n        )\n\n        logger.info(\"Synapse worker now listening on port %d\", port)\n\n    def start_listening(self, listeners: Iterable[ListenerConfig]):\n        for listener in listeners:\n            if listener.type == \"http\":\n                self._listen_http(listener)\n            elif listener.type == \"manhole\":\n                _base.listen_tcp(\n                    listener.bind_addresses,\n                    listener.port,\n                    manhole(\n                        username=\"matrix\", password=\"rabbithole\", globals={\"hs\": self}\n                    ),\n                )\n            elif listener.type == \"metrics\":\n                if not self.get_config().enable_metrics:\n                    logger.warning(\n                        (\n                            \"Metrics listener configured, but \"\n                            \"enable_metrics is not True!\"\n                        )\n                    )\n                else:\n                    _base.listen_metrics(listener.bind_addresses, listener.port)\n            else:\n                logger.warning(\"Unsupported listener type: %s\", listener.type)\n\n        self.get_tcp_replication().start_replication(self)\n\n    async def remove_pusher(self, app_id, push_key, user_id):\n        self.get_tcp_replication().send_remove_pusher(app_id, push_key, user_id)\n\n    @cache_in_self\n    def get_replication_data_handler(self):\n        return GenericWorkerReplicationHandler(self)\n\n    @cache_in_self\n    def get_presence_handler(self):\n        return GenericWorkerPresence(self)\n\n\nclass GenericWorkerReplicationHandler(ReplicationDataHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        self.store = hs.get_datastore()\n        self.presence_handler = hs.get_presence_handler()  # type: GenericWorkerPresence\n        self.notifier = hs.get_notifier()\n\n        self.notify_pushers = hs.config.start_pushers\n        self.pusher_pool = hs.get_pusherpool()\n\n        self.send_handler = None  # type: Optional[FederationSenderHandler]\n        if hs.config.send_federation:\n            self.send_handler = FederationSenderHandler(hs)\n\n    async def on_rdata(self, stream_name, instance_name, token, rows):\n        await super().on_rdata(stream_name, instance_name, token, rows)\n        await self._process_and_notify(stream_name, instance_name, token, rows)\n\n    async def _process_and_notify(self, stream_name, instance_name, token, rows):\n        try:\n            if self.send_handler:\n                await self.send_handler.process_replication_rows(\n                    stream_name, token, rows\n                )\n\n            if stream_name == PushRulesStream.NAME:\n                self.notifier.on_new_event(\n                    \"push_rules_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name in (AccountDataStream.NAME, TagAccountDataStream.NAME):\n                self.notifier.on_new_event(\n                    \"account_data_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name == ReceiptsStream.NAME:\n                self.notifier.on_new_event(\n                    \"receipt_key\", token, rooms=[row.room_id for row in rows]\n                )\n                await self.pusher_pool.on_new_receipts(\n                    token, token, {row.room_id for row in rows}\n                )\n            elif stream_name == ToDeviceStream.NAME:\n                entities = [row.entity for row in rows if row.entity.startswith(\"@\")]\n                if entities:\n                    self.notifier.on_new_event(\"to_device_key\", token, users=entities)\n            elif stream_name == DeviceListsStream.NAME:\n                all_room_ids = set()  # type: Set[str]\n                for row in rows:\n                    if row.entity.startswith(\"@\"):\n                        room_ids = await self.store.get_rooms_for_user(row.entity)\n                        all_room_ids.update(room_ids)\n                self.notifier.on_new_event(\"device_list_key\", token, rooms=all_room_ids)\n            elif stream_name == PresenceStream.NAME:\n                await self.presence_handler.process_replication_rows(token, rows)\n            elif stream_name == GroupServerStream.NAME:\n                self.notifier.on_new_event(\n                    \"groups_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name == PushersStream.NAME:\n                for row in rows:\n                    if row.deleted:\n                        self.stop_pusher(row.user_id, row.app_id, row.pushkey)\n                    else:\n                        await self.start_pusher(row.user_id, row.app_id, row.pushkey)\n        except Exception:\n            logger.exception(\"Error processing replication\")\n\n    async def on_position(self, stream_name: str, instance_name: str, token: int):\n        await super().on_position(stream_name, instance_name, token)\n        # Also call on_rdata to ensure that stream positions are properly reset.\n        await self.on_rdata(stream_name, instance_name, token, [])\n\n    def stop_pusher(self, user_id, app_id, pushkey):\n        if not self.notify_pushers:\n            return\n\n        key = \"%s:%s\" % (app_id, pushkey)\n        pushers_for_user = self.pusher_pool.pushers.get(user_id, {})\n        pusher = pushers_for_user.pop(key, None)\n        if pusher is None:\n            return\n        logger.info(\"Stopping pusher %r / %r\", user_id, key)\n        pusher.on_stop()\n\n    async def start_pusher(self, user_id, app_id, pushkey):\n        if not self.notify_pushers:\n            return\n\n        key = \"%s:%s\" % (app_id, pushkey)\n        logger.info(\"Starting pusher %r / %r\", user_id, key)\n        return await self.pusher_pool.start_pusher_by_id(app_id, pushkey, user_id)\n\n    def on_remote_server_up(self, server: str):\n        \"\"\"Called when get a new REMOTE_SERVER_UP command.\"\"\"\n\n        # Let's wake up the transaction queue for the server in case we have\n        # pending stuff to send to it.\n        if self.send_handler:\n            self.send_handler.wake_destination(server)\n\n\nclass FederationSenderHandler:\n    \"\"\"Processes the fedration replication stream\n\n    This class is only instantiate on the worker responsible for sending outbound\n    federation transactions. It receives rows from the replication stream and forwards\n    the appropriate entries to the FederationSender class.\n    \"\"\"\n\n    def __init__(self, hs: GenericWorkerServer):\n        self.store = hs.get_datastore()\n        self._is_mine_id = hs.is_mine_id\n        self.federation_sender = hs.get_federation_sender()\n        self._hs = hs\n\n        # Stores the latest position in the federation stream we've gotten up\n        # to. This is always set before we use it.\n        self.federation_position = None\n\n        self._fed_position_linearizer = Linearizer(name=\"_fed_position_linearizer\")\n\n    def on_start(self):\n        # There may be some events that are persisted but haven't been sent,\n        # so send them now.\n        self.federation_sender.notify_new_events(\n            self.store.get_room_max_stream_ordering()\n        )\n\n    def wake_destination(self, server: str):\n        self.federation_sender.wake_destination(server)\n\n    async def process_replication_rows(self, stream_name, token, rows):\n        # The federation stream contains things that we want to send out, e.g.\n        # presence, typing, etc.\n        if stream_name == \"federation\":\n            send_queue.process_rows_for_federation(self.federation_sender, rows)\n            await self.update_token(token)\n\n        # ... and when new receipts happen\n        elif stream_name == ReceiptsStream.NAME:\n            await self._on_new_receipts(rows)\n\n        # ... as well as device updates and messages\n        elif stream_name == DeviceListsStream.NAME:\n            # The entities are either user IDs (starting with '@') whose devices\n            # have changed, or remote servers that we need to tell about\n            # changes.\n            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}\n            for host in hosts:\n                self.federation_sender.send_device_messages(host)\n\n        elif stream_name == ToDeviceStream.NAME:\n            # The to_device stream includes stuff to be pushed to both local\n            # clients and remote servers, so we ignore entities that start with\n            # '@' (since they'll be local users rather than destinations).\n            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}\n            for host in hosts:\n                self.federation_sender.send_device_messages(host)\n\n    async def _on_new_receipts(self, rows):\n        \"\"\"\n        Args:\n            rows (Iterable[synapse.replication.tcp.streams.ReceiptsStream.ReceiptsStreamRow]):\n                new receipts to be processed\n        \"\"\"\n        for receipt in rows:\n            # we only want to send on receipts for our own users\n            if not self._is_mine_id(receipt.user_id):\n                continue\n            receipt_info = ReadReceipt(\n                receipt.room_id,\n                receipt.receipt_type,\n                receipt.user_id,\n                [receipt.event_id],\n                receipt.data,\n            )\n            await self.federation_sender.send_read_receipt(receipt_info)\n\n    async def update_token(self, token):\n        \"\"\"Update the record of where we have processed to in the federation stream.\n\n        Called after we have processed a an update received over replication. Sends\n        a FEDERATION_ACK back to the master, and stores the token that we have processed\n         in `federation_stream_position` so that we can restart where we left off.\n        \"\"\"\n        self.federation_position = token\n\n        # We save and send the ACK to master asynchronously, so we don't block\n        # processing on persistence. We don't need to do this operation for\n        # every single RDATA we receive, we just need to do it periodically.\n\n        if self._fed_position_linearizer.is_queued(None):\n            # There is already a task queued up to save and send the token, so\n            # no need to queue up another task.\n            return\n\n        run_as_background_process(\"_save_and_send_ack\", self._save_and_send_ack)\n\n    async def _save_and_send_ack(self):\n        \"\"\"Save the current federation position in the database and send an ACK\n        to master with where we're up to.\n        \"\"\"\n        try:\n            # We linearize here to ensure we don't have races updating the token\n            #\n            # XXX this appears to be redundant, since the ReplicationCommandHandler\n            # has a linearizer which ensures that we only process one line of\n            # replication data at a time. Should we remove it, or is it doing useful\n            # service for robustness? Or could we replace it with an assertion that\n            # we're not being re-entered?\n\n            with (await self._fed_position_linearizer.queue(None)):\n                # We persist and ack the same position, so we take a copy of it\n                # here as otherwise it can get modified from underneath us.\n                current_position = self.federation_position\n\n                await self.store.update_federation_out_pos(\n                    \"federation\", current_position\n                )\n\n                # We ACK this token over replication so that the master can drop\n                # its in memory queues\n                self._hs.get_tcp_replication().send_federation_ack(current_position)\n        except Exception:\n            logger.exception(\"Error updating federation stream position\")\n\n\ndef start(config_options):\n    try:\n        config = HomeServerConfig.load_config(\"Synapse worker\", config_options)\n    except ConfigError as e:\n        sys.stderr.write(\"\\n\" + str(e) + \"\\n\")\n        sys.exit(1)\n\n    # For backwards compatibility let any of the old app names.\n    assert config.worker_app in (\n        \"synapse.app.appservice\",\n        \"synapse.app.client_reader\",\n        \"synapse.app.event_creator\",\n        \"synapse.app.federation_reader\",\n        \"synapse.app.federation_sender\",\n        \"synapse.app.frontend_proxy\",\n        \"synapse.app.generic_worker\",\n        \"synapse.app.media_repository\",\n        \"synapse.app.pusher\",\n        \"synapse.app.synchrotron\",\n        \"synapse.app.user_dir\",\n    )\n\n    if config.worker_app == \"synapse.app.appservice\":\n        if config.appservice.notify_appservices:\n            sys.stderr.write(\n                \"\\nThe appservices must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``notify_appservices: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the appservice to start since they will be disabled in the main config\n        config.appservice.notify_appservices = True\n    else:\n        # For other worker types we force this to off.\n        config.appservice.notify_appservices = False\n\n    if config.worker_app == \"synapse.app.pusher\":\n        if config.server.start_pushers:\n            sys.stderr.write(\n                \"\\nThe pushers must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``start_pushers: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.server.start_pushers = True\n    else:\n        # For other worker types we force this to off.\n        config.server.start_pushers = False\n\n    if config.worker_app == \"synapse.app.user_dir\":\n        if config.server.update_user_directory:\n            sys.stderr.write(\n                \"\\nThe update_user_directory must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``update_user_directory: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.server.update_user_directory = True\n    else:\n        # For other worker types we force this to off.\n        config.server.update_user_directory = False\n\n    if config.worker_app == \"synapse.app.federation_sender\":\n        if config.worker.send_federation:\n            sys.stderr.write(\n                \"\\nThe send_federation must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``send_federation: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.worker.send_federation = True\n    else:\n        # For other worker types we force this to off.\n        config.worker.send_federation = False\n\n    synapse.events.USE_FROZEN_DICTS = config.use_frozen_dicts\n\n    hs = GenericWorkerServer(\n        config.server_name,\n        config=config,\n        version_string=\"Synapse/\" + get_version_string(synapse),\n    )\n\n    setup_logging(hs, config, use_worker_options=True)\n\n    hs.setup()\n\n    # Ensure the replication streamer is always started in case we write to any\n    # streams. Will no-op if no streams can be written to by this worker.\n    hs.get_replication_streamer()\n\n    reactor.addSystemEventTrigger(\n        \"before\", \"startup\", _base.start, hs, config.worker_listeners\n    )\n\n    _base.start_worker_reactor(\"synapse-generic-worker\", config)\n\n\nif __name__ == \"__main__\":\n    with LoggingContext(\"main\"):\n        start(sys.argv[1:])\n", "code_before": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Copyright 2016 OpenMarket Ltd\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport contextlib\nimport logging\nimport sys\nfrom typing import Dict, Iterable, Optional, Set\n\nfrom typing_extensions import ContextManager\n\nfrom twisted.internet import address, reactor\n\nimport synapse\nimport synapse.events\nfrom synapse.api.errors import HttpResponseException, RequestSendFailed, SynapseError\nfrom synapse.api.urls import (\n    CLIENT_API_PREFIX,\n    FEDERATION_PREFIX,\n    LEGACY_MEDIA_PREFIX,\n    MEDIA_PREFIX,\n    SERVER_KEY_V2_PREFIX,\n)\nfrom synapse.app import _base\nfrom synapse.config._base import ConfigError\nfrom synapse.config.homeserver import HomeServerConfig\nfrom synapse.config.logger import setup_logging\nfrom synapse.config.server import ListenerConfig\nfrom synapse.federation import send_queue\nfrom synapse.federation.transport.server import TransportLayerServer\nfrom synapse.handlers.presence import (\n    BasePresenceHandler,\n    PresenceState,\n    get_interested_parties,\n)\nfrom synapse.http.server import JsonResource, OptionsResource\nfrom synapse.http.servlet import RestServlet, parse_json_object_from_request\nfrom synapse.http.site import SynapseSite\nfrom synapse.logging.context import LoggingContext\nfrom synapse.metrics import METRICS_PREFIX, MetricsResource, RegistryProxy\nfrom synapse.metrics.background_process_metrics import run_as_background_process\nfrom synapse.replication.http import REPLICATION_PREFIX, ReplicationRestResource\nfrom synapse.replication.http.presence import (\n    ReplicationBumpPresenceActiveTime,\n    ReplicationPresenceSetState,\n)\nfrom synapse.replication.slave.storage._base import BaseSlavedStore\nfrom synapse.replication.slave.storage.account_data import SlavedAccountDataStore\nfrom synapse.replication.slave.storage.appservice import SlavedApplicationServiceStore\nfrom synapse.replication.slave.storage.client_ips import SlavedClientIpStore\nfrom synapse.replication.slave.storage.deviceinbox import SlavedDeviceInboxStore\nfrom synapse.replication.slave.storage.devices import SlavedDeviceStore\nfrom synapse.replication.slave.storage.directory import DirectoryStore\nfrom synapse.replication.slave.storage.events import SlavedEventStore\nfrom synapse.replication.slave.storage.filtering import SlavedFilteringStore\nfrom synapse.replication.slave.storage.groups import SlavedGroupServerStore\nfrom synapse.replication.slave.storage.keys import SlavedKeyStore\nfrom synapse.replication.slave.storage.presence import SlavedPresenceStore\nfrom synapse.replication.slave.storage.profile import SlavedProfileStore\nfrom synapse.replication.slave.storage.push_rule import SlavedPushRuleStore\nfrom synapse.replication.slave.storage.pushers import SlavedPusherStore\nfrom synapse.replication.slave.storage.receipts import SlavedReceiptsStore\nfrom synapse.replication.slave.storage.registration import SlavedRegistrationStore\nfrom synapse.replication.slave.storage.room import RoomStore\nfrom synapse.replication.slave.storage.transactions import SlavedTransactionStore\nfrom synapse.replication.tcp.client import ReplicationDataHandler\nfrom synapse.replication.tcp.commands import ClearUserSyncsCommand\nfrom synapse.replication.tcp.streams import (\n    AccountDataStream,\n    DeviceListsStream,\n    GroupServerStream,\n    PresenceStream,\n    PushersStream,\n    PushRulesStream,\n    ReceiptsStream,\n    TagAccountDataStream,\n    ToDeviceStream,\n)\nfrom synapse.rest.admin import register_servlets_for_media_repo\nfrom synapse.rest.client.v1 import events\nfrom synapse.rest.client.v1.initial_sync import InitialSyncRestServlet\nfrom synapse.rest.client.v1.login import LoginRestServlet\nfrom synapse.rest.client.v1.profile import (\n    ProfileAvatarURLRestServlet,\n    ProfileDisplaynameRestServlet,\n    ProfileRestServlet,\n)\nfrom synapse.rest.client.v1.push_rule import PushRuleRestServlet\nfrom synapse.rest.client.v1.room import (\n    JoinedRoomMemberListRestServlet,\n    JoinRoomAliasServlet,\n    PublicRoomListRestServlet,\n    RoomEventContextServlet,\n    RoomInitialSyncRestServlet,\n    RoomMemberListRestServlet,\n    RoomMembershipRestServlet,\n    RoomMessageListRestServlet,\n    RoomSendEventRestServlet,\n    RoomStateEventRestServlet,\n    RoomStateRestServlet,\n    RoomTypingRestServlet,\n)\nfrom synapse.rest.client.v1.voip import VoipRestServlet\nfrom synapse.rest.client.v2_alpha import groups, sync, user_directory\nfrom synapse.rest.client.v2_alpha._base import client_patterns\nfrom synapse.rest.client.v2_alpha.account import ThreepidRestServlet\nfrom synapse.rest.client.v2_alpha.account_data import (\n    AccountDataServlet,\n    RoomAccountDataServlet,\n)\nfrom synapse.rest.client.v2_alpha.keys import KeyChangesServlet, KeyQueryServlet\nfrom synapse.rest.client.v2_alpha.register import RegisterRestServlet\nfrom synapse.rest.client.versions import VersionsRestServlet\nfrom synapse.rest.health import HealthResource\nfrom synapse.rest.key.v2 import KeyApiV2Resource\nfrom synapse.server import HomeServer, cache_in_self\nfrom synapse.storage.databases.main.censor_events import CensorEventsStore\nfrom synapse.storage.databases.main.client_ips import ClientIpWorkerStore\nfrom synapse.storage.databases.main.media_repository import MediaRepositoryStore\nfrom synapse.storage.databases.main.metrics import ServerMetricsStore\nfrom synapse.storage.databases.main.monthly_active_users import (\n    MonthlyActiveUsersWorkerStore,\n)\nfrom synapse.storage.databases.main.presence import UserPresenceState\nfrom synapse.storage.databases.main.search import SearchWorkerStore\nfrom synapse.storage.databases.main.stats import StatsStore\nfrom synapse.storage.databases.main.transactions import TransactionWorkerStore\nfrom synapse.storage.databases.main.ui_auth import UIAuthWorkerStore\nfrom synapse.storage.databases.main.user_directory import UserDirectoryStore\nfrom synapse.types import ReadReceipt\nfrom synapse.util.async_helpers import Linearizer\nfrom synapse.util.httpresourcetree import create_resource_tree\nfrom synapse.util.manhole import manhole\nfrom synapse.util.versionstring import get_version_string\n\nlogger = logging.getLogger(\"synapse.app.generic_worker\")\n\n\nclass PresenceStatusStubServlet(RestServlet):\n    \"\"\"If presence is disabled this servlet can be used to stub out setting\n    presence status.\n    \"\"\"\n\n    PATTERNS = client_patterns(\"/presence/(?P<user_id>[^/]*)/status\")\n\n    def __init__(self, hs):\n        super().__init__()\n        self.auth = hs.get_auth()\n\n    async def on_GET(self, request, user_id):\n        await self.auth.get_user_by_req(request)\n        return 200, {\"presence\": \"offline\"}\n\n    async def on_PUT(self, request, user_id):\n        await self.auth.get_user_by_req(request)\n        return 200, {}\n\n\nclass KeyUploadServlet(RestServlet):\n    \"\"\"An implementation of the `KeyUploadServlet` that responds to read only\n    requests, but otherwise proxies through to the master instance.\n    \"\"\"\n\n    PATTERNS = client_patterns(\"/keys/upload(/(?P<device_id>[^/]+))?$\")\n\n    def __init__(self, hs):\n        \"\"\"\n        Args:\n            hs (synapse.server.HomeServer): server\n        \"\"\"\n        super().__init__()\n        self.auth = hs.get_auth()\n        self.store = hs.get_datastore()\n        self.http_client = hs.get_simple_http_client()\n        self.main_uri = hs.config.worker_main_http_uri\n\n    async def on_POST(self, request, device_id):\n        requester = await self.auth.get_user_by_req(request, allow_guest=True)\n        user_id = requester.user.to_string()\n        body = parse_json_object_from_request(request)\n\n        if device_id is not None:\n            # passing the device_id here is deprecated; however, we allow it\n            # for now for compatibility with older clients.\n            if requester.device_id is not None and device_id != requester.device_id:\n                logger.warning(\n                    \"Client uploading keys for a different device \"\n                    \"(logged in as %s, uploading for %s)\",\n                    requester.device_id,\n                    device_id,\n                )\n        else:\n            device_id = requester.device_id\n\n        if device_id is None:\n            raise SynapseError(\n                400, \"To upload keys, you must pass device_id when authenticating\"\n            )\n\n        if body:\n            # They're actually trying to upload something, proxy to main synapse.\n\n            # Proxy headers from the original request, such as the auth headers\n            # (in case the access token is there) and the original IP /\n            # User-Agent of the request.\n            headers = {\n                header: request.requestHeaders.getRawHeaders(header, [])\n                for header in (b\"Authorization\", b\"User-Agent\")\n            }\n            # Add the previous hop the the X-Forwarded-For header.\n            x_forwarded_for = request.requestHeaders.getRawHeaders(\n                b\"X-Forwarded-For\", []\n            )\n            if isinstance(request.client, (address.IPv4Address, address.IPv6Address)):\n                previous_host = request.client.host.encode(\"ascii\")\n                # If the header exists, add to the comma-separated list of the first\n                # instance of the header. Otherwise, generate a new header.\n                if x_forwarded_for:\n                    x_forwarded_for = [\n                        x_forwarded_for[0] + b\", \" + previous_host\n                    ] + x_forwarded_for[1:]\n                else:\n                    x_forwarded_for = [previous_host]\n            headers[b\"X-Forwarded-For\"] = x_forwarded_for\n\n            try:\n                result = await self.http_client.post_json_get_json(\n                    self.main_uri + request.uri.decode(\"ascii\"), body, headers=headers\n                )\n            except HttpResponseException as e:\n                raise e.to_synapse_error() from e\n            except RequestSendFailed as e:\n                raise SynapseError(502, \"Failed to talk to master\") from e\n\n            return 200, result\n        else:\n            # Just interested in counts.\n            result = await self.store.count_e2e_one_time_keys(user_id, device_id)\n            return 200, {\"one_time_key_counts\": result}\n\n\nclass _NullContextManager(ContextManager[None]):\n    \"\"\"A context manager which does nothing.\"\"\"\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\nUPDATE_SYNCING_USERS_MS = 10 * 1000\n\n\nclass GenericWorkerPresence(BasePresenceHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.hs = hs\n        self.is_mine_id = hs.is_mine_id\n        self.http_client = hs.get_simple_http_client()\n\n        self._presence_enabled = hs.config.use_presence\n\n        # The number of ongoing syncs on this process, by user id.\n        # Empty if _presence_enabled is false.\n        self._user_to_num_current_syncs = {}  # type: Dict[str, int]\n\n        self.notifier = hs.get_notifier()\n        self.instance_id = hs.get_instance_id()\n\n        # user_id -> last_sync_ms. Lists the users that have stopped syncing\n        # but we haven't notified the master of that yet\n        self.users_going_offline = {}\n\n        self._bump_active_client = ReplicationBumpPresenceActiveTime.make_client(hs)\n        self._set_state_client = ReplicationPresenceSetState.make_client(hs)\n\n        self._send_stop_syncing_loop = self.clock.looping_call(\n            self.send_stop_syncing, UPDATE_SYNCING_USERS_MS\n        )\n\n        hs.get_reactor().addSystemEventTrigger(\n            \"before\",\n            \"shutdown\",\n            run_as_background_process,\n            \"generic_presence.on_shutdown\",\n            self._on_shutdown,\n        )\n\n    def _on_shutdown(self):\n        if self._presence_enabled:\n            self.hs.get_tcp_replication().send_command(\n                ClearUserSyncsCommand(self.instance_id)\n            )\n\n    def send_user_sync(self, user_id, is_syncing, last_sync_ms):\n        if self._presence_enabled:\n            self.hs.get_tcp_replication().send_user_sync(\n                self.instance_id, user_id, is_syncing, last_sync_ms\n            )\n\n    def mark_as_coming_online(self, user_id):\n        \"\"\"A user has started syncing. Send a UserSync to the master, unless they\n        had recently stopped syncing.\n\n        Args:\n            user_id (str)\n        \"\"\"\n        going_offline = self.users_going_offline.pop(user_id, None)\n        if not going_offline:\n            # Safe to skip because we haven't yet told the master they were offline\n            self.send_user_sync(user_id, True, self.clock.time_msec())\n\n    def mark_as_going_offline(self, user_id):\n        \"\"\"A user has stopped syncing. We wait before notifying the master as\n        its likely they'll come back soon. This allows us to avoid sending\n        a stopped syncing immediately followed by a started syncing notification\n        to the master\n\n        Args:\n            user_id (str)\n        \"\"\"\n        self.users_going_offline[user_id] = self.clock.time_msec()\n\n    def send_stop_syncing(self):\n        \"\"\"Check if there are any users who have stopped syncing a while ago\n        and haven't come back yet. If there are poke the master about them.\n        \"\"\"\n        now = self.clock.time_msec()\n        for user_id, last_sync_ms in list(self.users_going_offline.items()):\n            if now - last_sync_ms > UPDATE_SYNCING_USERS_MS:\n                self.users_going_offline.pop(user_id, None)\n                self.send_user_sync(user_id, False, last_sync_ms)\n\n    async def user_syncing(\n        self, user_id: str, affect_presence: bool\n    ) -> ContextManager[None]:\n        \"\"\"Record that a user is syncing.\n\n        Called by the sync and events servlets to record that a user has connected to\n        this worker and is waiting for some events.\n        \"\"\"\n        if not affect_presence or not self._presence_enabled:\n            return _NullContextManager()\n\n        curr_sync = self._user_to_num_current_syncs.get(user_id, 0)\n        self._user_to_num_current_syncs[user_id] = curr_sync + 1\n\n        # If we went from no in flight sync to some, notify replication\n        if self._user_to_num_current_syncs[user_id] == 1:\n            self.mark_as_coming_online(user_id)\n\n        def _end():\n            # We check that the user_id is in user_to_num_current_syncs because\n            # user_to_num_current_syncs may have been cleared if we are\n            # shutting down.\n            if user_id in self._user_to_num_current_syncs:\n                self._user_to_num_current_syncs[user_id] -= 1\n\n                # If we went from one in flight sync to non, notify replication\n                if self._user_to_num_current_syncs[user_id] == 0:\n                    self.mark_as_going_offline(user_id)\n\n        @contextlib.contextmanager\n        def _user_syncing():\n            try:\n                yield\n            finally:\n                _end()\n\n        return _user_syncing()\n\n    async def notify_from_replication(self, states, stream_id):\n        parties = await get_interested_parties(self.store, states)\n        room_ids_to_states, users_to_states = parties\n\n        self.notifier.on_new_event(\n            \"presence_key\",\n            stream_id,\n            rooms=room_ids_to_states.keys(),\n            users=users_to_states.keys(),\n        )\n\n    async def process_replication_rows(self, token, rows):\n        states = [\n            UserPresenceState(\n                row.user_id,\n                row.state,\n                row.last_active_ts,\n                row.last_federation_update_ts,\n                row.last_user_sync_ts,\n                row.status_msg,\n                row.currently_active,\n            )\n            for row in rows\n        ]\n\n        for state in states:\n            self.user_to_current_state[state.user_id] = state\n\n        stream_id = token\n        await self.notify_from_replication(states, stream_id)\n\n    def get_currently_syncing_users_for_replication(self) -> Iterable[str]:\n        return [\n            user_id\n            for user_id, count in self._user_to_num_current_syncs.items()\n            if count > 0\n        ]\n\n    async def set_state(self, target_user, state, ignore_status_msg=False):\n        \"\"\"Set the presence state of the user.\n        \"\"\"\n        presence = state[\"presence\"]\n\n        valid_presence = (\n            PresenceState.ONLINE,\n            PresenceState.UNAVAILABLE,\n            PresenceState.OFFLINE,\n        )\n        if presence not in valid_presence:\n            raise SynapseError(400, \"Invalid presence state\")\n\n        user_id = target_user.to_string()\n\n        # If presence is disabled, no-op\n        if not self.hs.config.use_presence:\n            return\n\n        # Proxy request to master\n        await self._set_state_client(\n            user_id=user_id, state=state, ignore_status_msg=ignore_status_msg\n        )\n\n    async def bump_presence_active_time(self, user):\n        \"\"\"We've seen the user do something that indicates they're interacting\n        with the app.\n        \"\"\"\n        # If presence is disabled, no-op\n        if not self.hs.config.use_presence:\n            return\n\n        # Proxy request to master\n        user_id = user.to_string()\n        await self._bump_active_client(user_id=user_id)\n\n\nclass GenericWorkerSlavedStore(\n    # FIXME(#3714): We need to add UserDirectoryStore as we write directly\n    # rather than going via the correct worker.\n    UserDirectoryStore,\n    StatsStore,\n    UIAuthWorkerStore,\n    SlavedDeviceInboxStore,\n    SlavedDeviceStore,\n    SlavedReceiptsStore,\n    SlavedPushRuleStore,\n    SlavedGroupServerStore,\n    SlavedAccountDataStore,\n    SlavedPusherStore,\n    CensorEventsStore,\n    ClientIpWorkerStore,\n    SlavedEventStore,\n    SlavedKeyStore,\n    RoomStore,\n    DirectoryStore,\n    SlavedApplicationServiceStore,\n    SlavedRegistrationStore,\n    SlavedTransactionStore,\n    SlavedProfileStore,\n    SlavedClientIpStore,\n    SlavedPresenceStore,\n    SlavedFilteringStore,\n    MonthlyActiveUsersWorkerStore,\n    MediaRepositoryStore,\n    ServerMetricsStore,\n    SearchWorkerStore,\n    TransactionWorkerStore,\n    BaseSlavedStore,\n):\n    pass\n\n\nclass GenericWorkerServer(HomeServer):\n    DATASTORE_CLASS = GenericWorkerSlavedStore\n\n    def _listen_http(self, listener_config: ListenerConfig):\n        port = listener_config.port\n        bind_addresses = listener_config.bind_addresses\n\n        assert listener_config.http_options is not None\n\n        site_tag = listener_config.http_options.tag\n        if site_tag is None:\n            site_tag = port\n\n        # We always include a health resource.\n        resources = {\"/health\": HealthResource()}\n\n        for res in listener_config.http_options.resources:\n            for name in res.names:\n                if name == \"metrics\":\n                    resources[METRICS_PREFIX] = MetricsResource(RegistryProxy)\n                elif name == \"client\":\n                    resource = JsonResource(self, canonical_json=False)\n\n                    PublicRoomListRestServlet(self).register(resource)\n                    RoomMemberListRestServlet(self).register(resource)\n                    JoinedRoomMemberListRestServlet(self).register(resource)\n                    RoomStateRestServlet(self).register(resource)\n                    RoomEventContextServlet(self).register(resource)\n                    RoomMessageListRestServlet(self).register(resource)\n                    RegisterRestServlet(self).register(resource)\n                    LoginRestServlet(self).register(resource)\n                    ThreepidRestServlet(self).register(resource)\n                    KeyQueryServlet(self).register(resource)\n                    KeyChangesServlet(self).register(resource)\n                    VoipRestServlet(self).register(resource)\n                    PushRuleRestServlet(self).register(resource)\n                    VersionsRestServlet(self).register(resource)\n                    RoomSendEventRestServlet(self).register(resource)\n                    RoomMembershipRestServlet(self).register(resource)\n                    RoomStateEventRestServlet(self).register(resource)\n                    JoinRoomAliasServlet(self).register(resource)\n                    ProfileAvatarURLRestServlet(self).register(resource)\n                    ProfileDisplaynameRestServlet(self).register(resource)\n                    ProfileRestServlet(self).register(resource)\n                    KeyUploadServlet(self).register(resource)\n                    AccountDataServlet(self).register(resource)\n                    RoomAccountDataServlet(self).register(resource)\n                    RoomTypingRestServlet(self).register(resource)\n\n                    sync.register_servlets(self, resource)\n                    events.register_servlets(self, resource)\n                    InitialSyncRestServlet(self).register(resource)\n                    RoomInitialSyncRestServlet(self).register(resource)\n\n                    user_directory.register_servlets(self, resource)\n\n                    # If presence is disabled, use the stub servlet that does\n                    # not allow sending presence\n                    if not self.config.use_presence:\n                        PresenceStatusStubServlet(self).register(resource)\n\n                    groups.register_servlets(self, resource)\n\n                    resources.update({CLIENT_API_PREFIX: resource})\n                elif name == \"federation\":\n                    resources.update({FEDERATION_PREFIX: TransportLayerServer(self)})\n                elif name == \"media\":\n                    if self.config.can_load_media_repo:\n                        media_repo = self.get_media_repository_resource()\n\n                        # We need to serve the admin servlets for media on the\n                        # worker.\n                        admin_resource = JsonResource(self, canonical_json=False)\n                        register_servlets_for_media_repo(self, admin_resource)\n\n                        resources.update(\n                            {\n                                MEDIA_PREFIX: media_repo,\n                                LEGACY_MEDIA_PREFIX: media_repo,\n                                \"/_synapse/admin\": admin_resource,\n                            }\n                        )\n                    else:\n                        logger.warning(\n                            \"A 'media' listener is configured but the media\"\n                            \" repository is disabled. Ignoring.\"\n                        )\n\n                if name == \"openid\" and \"federation\" not in res.names:\n                    # Only load the openid resource separately if federation resource\n                    # is not specified since federation resource includes openid\n                    # resource.\n                    resources.update(\n                        {\n                            FEDERATION_PREFIX: TransportLayerServer(\n                                self, servlet_groups=[\"openid\"]\n                            )\n                        }\n                    )\n\n                if name in [\"keys\", \"federation\"]:\n                    resources[SERVER_KEY_V2_PREFIX] = KeyApiV2Resource(self)\n\n                if name == \"replication\":\n                    resources[REPLICATION_PREFIX] = ReplicationRestResource(self)\n\n        root_resource = create_resource_tree(resources, OptionsResource())\n\n        _base.listen_tcp(\n            bind_addresses,\n            port,\n            SynapseSite(\n                \"synapse.access.http.%s\" % (site_tag,),\n                site_tag,\n                listener_config,\n                root_resource,\n                self.version_string,\n            ),\n            reactor=self.get_reactor(),\n        )\n\n        logger.info(\"Synapse worker now listening on port %d\", port)\n\n    def start_listening(self, listeners: Iterable[ListenerConfig]):\n        for listener in listeners:\n            if listener.type == \"http\":\n                self._listen_http(listener)\n            elif listener.type == \"manhole\":\n                _base.listen_tcp(\n                    listener.bind_addresses,\n                    listener.port,\n                    manhole(\n                        username=\"matrix\", password=\"rabbithole\", globals={\"hs\": self}\n                    ),\n                )\n            elif listener.type == \"metrics\":\n                if not self.get_config().enable_metrics:\n                    logger.warning(\n                        (\n                            \"Metrics listener configured, but \"\n                            \"enable_metrics is not True!\"\n                        )\n                    )\n                else:\n                    _base.listen_metrics(listener.bind_addresses, listener.port)\n            else:\n                logger.warning(\"Unsupported listener type: %s\", listener.type)\n\n        self.get_tcp_replication().start_replication(self)\n\n    async def remove_pusher(self, app_id, push_key, user_id):\n        self.get_tcp_replication().send_remove_pusher(app_id, push_key, user_id)\n\n    @cache_in_self\n    def get_replication_data_handler(self):\n        return GenericWorkerReplicationHandler(self)\n\n    @cache_in_self\n    def get_presence_handler(self):\n        return GenericWorkerPresence(self)\n\n\nclass GenericWorkerReplicationHandler(ReplicationDataHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        self.store = hs.get_datastore()\n        self.presence_handler = hs.get_presence_handler()  # type: GenericWorkerPresence\n        self.notifier = hs.get_notifier()\n\n        self.notify_pushers = hs.config.start_pushers\n        self.pusher_pool = hs.get_pusherpool()\n\n        self.send_handler = None  # type: Optional[FederationSenderHandler]\n        if hs.config.send_federation:\n            self.send_handler = FederationSenderHandler(hs)\n\n    async def on_rdata(self, stream_name, instance_name, token, rows):\n        await super().on_rdata(stream_name, instance_name, token, rows)\n        await self._process_and_notify(stream_name, instance_name, token, rows)\n\n    async def _process_and_notify(self, stream_name, instance_name, token, rows):\n        try:\n            if self.send_handler:\n                await self.send_handler.process_replication_rows(\n                    stream_name, token, rows\n                )\n\n            if stream_name == PushRulesStream.NAME:\n                self.notifier.on_new_event(\n                    \"push_rules_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name in (AccountDataStream.NAME, TagAccountDataStream.NAME):\n                self.notifier.on_new_event(\n                    \"account_data_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name == ReceiptsStream.NAME:\n                self.notifier.on_new_event(\n                    \"receipt_key\", token, rooms=[row.room_id for row in rows]\n                )\n                await self.pusher_pool.on_new_receipts(\n                    token, token, {row.room_id for row in rows}\n                )\n            elif stream_name == ToDeviceStream.NAME:\n                entities = [row.entity for row in rows if row.entity.startswith(\"@\")]\n                if entities:\n                    self.notifier.on_new_event(\"to_device_key\", token, users=entities)\n            elif stream_name == DeviceListsStream.NAME:\n                all_room_ids = set()  # type: Set[str]\n                for row in rows:\n                    if row.entity.startswith(\"@\"):\n                        room_ids = await self.store.get_rooms_for_user(row.entity)\n                        all_room_ids.update(room_ids)\n                self.notifier.on_new_event(\"device_list_key\", token, rooms=all_room_ids)\n            elif stream_name == PresenceStream.NAME:\n                await self.presence_handler.process_replication_rows(token, rows)\n            elif stream_name == GroupServerStream.NAME:\n                self.notifier.on_new_event(\n                    \"groups_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name == PushersStream.NAME:\n                for row in rows:\n                    if row.deleted:\n                        self.stop_pusher(row.user_id, row.app_id, row.pushkey)\n                    else:\n                        await self.start_pusher(row.user_id, row.app_id, row.pushkey)\n        except Exception:\n            logger.exception(\"Error processing replication\")\n\n    async def on_position(self, stream_name: str, instance_name: str, token: int):\n        await super().on_position(stream_name, instance_name, token)\n        # Also call on_rdata to ensure that stream positions are properly reset.\n        await self.on_rdata(stream_name, instance_name, token, [])\n\n    def stop_pusher(self, user_id, app_id, pushkey):\n        if not self.notify_pushers:\n            return\n\n        key = \"%s:%s\" % (app_id, pushkey)\n        pushers_for_user = self.pusher_pool.pushers.get(user_id, {})\n        pusher = pushers_for_user.pop(key, None)\n        if pusher is None:\n            return\n        logger.info(\"Stopping pusher %r / %r\", user_id, key)\n        pusher.on_stop()\n\n    async def start_pusher(self, user_id, app_id, pushkey):\n        if not self.notify_pushers:\n            return\n\n        key = \"%s:%s\" % (app_id, pushkey)\n        logger.info(\"Starting pusher %r / %r\", user_id, key)\n        return await self.pusher_pool.start_pusher_by_id(app_id, pushkey, user_id)\n\n    def on_remote_server_up(self, server: str):\n        \"\"\"Called when get a new REMOTE_SERVER_UP command.\"\"\"\n\n        # Let's wake up the transaction queue for the server in case we have\n        # pending stuff to send to it.\n        if self.send_handler:\n            self.send_handler.wake_destination(server)\n\n\nclass FederationSenderHandler:\n    \"\"\"Processes the fedration replication stream\n\n    This class is only instantiate on the worker responsible for sending outbound\n    federation transactions. It receives rows from the replication stream and forwards\n    the appropriate entries to the FederationSender class.\n    \"\"\"\n\n    def __init__(self, hs: GenericWorkerServer):\n        self.store = hs.get_datastore()\n        self._is_mine_id = hs.is_mine_id\n        self.federation_sender = hs.get_federation_sender()\n        self._hs = hs\n\n        # Stores the latest position in the federation stream we've gotten up\n        # to. This is always set before we use it.\n        self.federation_position = None\n\n        self._fed_position_linearizer = Linearizer(name=\"_fed_position_linearizer\")\n\n    def on_start(self):\n        # There may be some events that are persisted but haven't been sent,\n        # so send them now.\n        self.federation_sender.notify_new_events(\n            self.store.get_room_max_stream_ordering()\n        )\n\n    def wake_destination(self, server: str):\n        self.federation_sender.wake_destination(server)\n\n    async def process_replication_rows(self, stream_name, token, rows):\n        # The federation stream contains things that we want to send out, e.g.\n        # presence, typing, etc.\n        if stream_name == \"federation\":\n            send_queue.process_rows_for_federation(self.federation_sender, rows)\n            await self.update_token(token)\n\n        # ... and when new receipts happen\n        elif stream_name == ReceiptsStream.NAME:\n            await self._on_new_receipts(rows)\n\n        # ... as well as device updates and messages\n        elif stream_name == DeviceListsStream.NAME:\n            # The entities are either user IDs (starting with '@') whose devices\n            # have changed, or remote servers that we need to tell about\n            # changes.\n            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}\n            for host in hosts:\n                self.federation_sender.send_device_messages(host)\n\n        elif stream_name == ToDeviceStream.NAME:\n            # The to_device stream includes stuff to be pushed to both local\n            # clients and remote servers, so we ignore entities that start with\n            # '@' (since they'll be local users rather than destinations).\n            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}\n            for host in hosts:\n                self.federation_sender.send_device_messages(host)\n\n    async def _on_new_receipts(self, rows):\n        \"\"\"\n        Args:\n            rows (Iterable[synapse.replication.tcp.streams.ReceiptsStream.ReceiptsStreamRow]):\n                new receipts to be processed\n        \"\"\"\n        for receipt in rows:\n            # we only want to send on receipts for our own users\n            if not self._is_mine_id(receipt.user_id):\n                continue\n            receipt_info = ReadReceipt(\n                receipt.room_id,\n                receipt.receipt_type,\n                receipt.user_id,\n                [receipt.event_id],\n                receipt.data,\n            )\n            await self.federation_sender.send_read_receipt(receipt_info)\n\n    async def update_token(self, token):\n        \"\"\"Update the record of where we have processed to in the federation stream.\n\n        Called after we have processed a an update received over replication. Sends\n        a FEDERATION_ACK back to the master, and stores the token that we have processed\n         in `federation_stream_position` so that we can restart where we left off.\n        \"\"\"\n        self.federation_position = token\n\n        # We save and send the ACK to master asynchronously, so we don't block\n        # processing on persistence. We don't need to do this operation for\n        # every single RDATA we receive, we just need to do it periodically.\n\n        if self._fed_position_linearizer.is_queued(None):\n            # There is already a task queued up to save and send the token, so\n            # no need to queue up another task.\n            return\n\n        run_as_background_process(\"_save_and_send_ack\", self._save_and_send_ack)\n\n    async def _save_and_send_ack(self):\n        \"\"\"Save the current federation position in the database and send an ACK\n        to master with where we're up to.\n        \"\"\"\n        try:\n            # We linearize here to ensure we don't have races updating the token\n            #\n            # XXX this appears to be redundant, since the ReplicationCommandHandler\n            # has a linearizer which ensures that we only process one line of\n            # replication data at a time. Should we remove it, or is it doing useful\n            # service for robustness? Or could we replace it with an assertion that\n            # we're not being re-entered?\n\n            with (await self._fed_position_linearizer.queue(None)):\n                # We persist and ack the same position, so we take a copy of it\n                # here as otherwise it can get modified from underneath us.\n                current_position = self.federation_position\n\n                await self.store.update_federation_out_pos(\n                    \"federation\", current_position\n                )\n\n                # We ACK this token over replication so that the master can drop\n                # its in memory queues\n                self._hs.get_tcp_replication().send_federation_ack(current_position)\n        except Exception:\n            logger.exception(\"Error updating federation stream position\")\n\n\ndef start(config_options):\n    try:\n        config = HomeServerConfig.load_config(\"Synapse worker\", config_options)\n    except ConfigError as e:\n        sys.stderr.write(\"\\n\" + str(e) + \"\\n\")\n        sys.exit(1)\n\n    # For backwards compatibility let any of the old app names.\n    assert config.worker_app in (\n        \"synapse.app.appservice\",\n        \"synapse.app.client_reader\",\n        \"synapse.app.event_creator\",\n        \"synapse.app.federation_reader\",\n        \"synapse.app.federation_sender\",\n        \"synapse.app.frontend_proxy\",\n        \"synapse.app.generic_worker\",\n        \"synapse.app.media_repository\",\n        \"synapse.app.pusher\",\n        \"synapse.app.synchrotron\",\n        \"synapse.app.user_dir\",\n    )\n\n    if config.worker_app == \"synapse.app.appservice\":\n        if config.appservice.notify_appservices:\n            sys.stderr.write(\n                \"\\nThe appservices must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``notify_appservices: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the appservice to start since they will be disabled in the main config\n        config.appservice.notify_appservices = True\n    else:\n        # For other worker types we force this to off.\n        config.appservice.notify_appservices = False\n\n    if config.worker_app == \"synapse.app.pusher\":\n        if config.server.start_pushers:\n            sys.stderr.write(\n                \"\\nThe pushers must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``start_pushers: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.server.start_pushers = True\n    else:\n        # For other worker types we force this to off.\n        config.server.start_pushers = False\n\n    if config.worker_app == \"synapse.app.user_dir\":\n        if config.server.update_user_directory:\n            sys.stderr.write(\n                \"\\nThe update_user_directory must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``update_user_directory: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.server.update_user_directory = True\n    else:\n        # For other worker types we force this to off.\n        config.server.update_user_directory = False\n\n    if config.worker_app == \"synapse.app.federation_sender\":\n        if config.worker.send_federation:\n            sys.stderr.write(\n                \"\\nThe send_federation must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``send_federation: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.worker.send_federation = True\n    else:\n        # For other worker types we force this to off.\n        config.worker.send_federation = False\n\n    synapse.events.USE_FROZEN_DICTS = config.use_frozen_dicts\n\n    hs = GenericWorkerServer(\n        config.server_name,\n        config=config,\n        version_string=\"Synapse/\" + get_version_string(synapse),\n    )\n\n    setup_logging(hs, config, use_worker_options=True)\n\n    hs.setup()\n\n    # Ensure the replication streamer is always started in case we write to any\n    # streams. Will no-op if no streams can be written to by this worker.\n    hs.get_replication_streamer()\n\n    reactor.addSystemEventTrigger(\n        \"before\", \"startup\", _base.start, hs, config.worker_listeners\n    )\n\n    _base.start_worker_reactor(\"synapse-generic-worker\", config)\n\n\nif __name__ == \"__main__\":\n    with LoggingContext(\"main\"):\n        start(sys.argv[1:])\n", "patch": "@@ -266,7 +266,6 @@ def __init__(self, hs):\n         super().__init__(hs)\n         self.hs = hs\n         self.is_mine_id = hs.is_mine_id\n-        self.http_client = hs.get_simple_http_client()\n \n         self._presence_enabled = hs.config.use_presence\n ", "file_path": "files/2021_2/17", "file_language": "py", "file_name": "synapse/app/generic_worker.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class PresenceStatusStubServlet(RestServlet):\n    \"\"\"If presence is disabled this servlet can be used to stub out setting\n    presence status.\n    \"\"\"\n\n    PATTERNS = client_patterns(\"/presence/(?P<user_id>[^/]*)/status\")\n\n    def __init__(self, hs):\n        super().__init__()\n        self.auth = hs.get_auth()\n\n    async def on_GET(self, request, user_id):\n        await self.auth.get_user_by_req(request)\n        return 200, {\"presence\": \"offline\"}\n\n    async def on_PUT(self, request, user_id):\n        await self.auth.get_user_by_req(request)\n        return 200, {}", "target": 0}, {"function": "class KeyUploadServlet(RestServlet):\n    \"\"\"An implementation of the `KeyUploadServlet` that responds to read only\n    requests, but otherwise proxies through to the master instance.\n    \"\"\"\n\n    PATTERNS = client_patterns(\"/keys/upload(/(?P<device_id>[^/]+))?$\")\n\n    def __init__(self, hs):\n        \"\"\"\n        Args:\n            hs (synapse.server.HomeServer): server\n        \"\"\"\n        super().__init__()\n        self.auth = hs.get_auth()\n        self.store = hs.get_datastore()\n        self.http_client = hs.get_simple_http_client()\n        self.main_uri = hs.config.worker_main_http_uri\n\n    async def on_POST(self, request, device_id):\n        requester = await self.auth.get_user_by_req(request, allow_guest=True)\n        user_id = requester.user.to_string()\n        body = parse_json_object_from_request(request)\n\n        if device_id is not None:\n            # passing the device_id here is deprecated; however, we allow it\n            # for now for compatibility with older clients.\n            if requester.device_id is not None and device_id != requester.device_id:\n                logger.warning(\n                    \"Client uploading keys for a different device \"\n                    \"(logged in as %s, uploading for %s)\",\n                    requester.device_id,\n                    device_id,\n                )\n        else:\n            device_id = requester.device_id\n\n        if device_id is None:\n            raise SynapseError(\n                400, \"To upload keys, you must pass device_id when authenticating\"\n            )\n\n        if body:\n            # They're actually trying to upload something, proxy to main synapse.\n\n            # Proxy headers from the original request, such as the auth headers\n            # (in case the access token is there) and the original IP /\n            # User-Agent of the request.\n            headers = {\n                header: request.requestHeaders.getRawHeaders(header, [])\n                for header in (b\"Authorization\", b\"User-Agent\")\n            }\n            # Add the previous hop the the X-Forwarded-For header.\n            x_forwarded_for = request.requestHeaders.getRawHeaders(\n                b\"X-Forwarded-For\", []\n            )\n            if isinstance(request.client, (address.IPv4Address, address.IPv6Address)):\n                previous_host = request.client.host.encode(\"ascii\")\n                # If the header exists, add to the comma-separated list of the first\n                # instance of the header. Otherwise, generate a new header.\n                if x_forwarded_for:\n                    x_forwarded_for = [\n                        x_forwarded_for[0] + b\", \" + previous_host\n                    ] + x_forwarded_for[1:]\n                else:\n                    x_forwarded_for = [previous_host]\n            headers[b\"X-Forwarded-For\"] = x_forwarded_for\n\n            try:\n                result = await self.http_client.post_json_get_json(\n                    self.main_uri + request.uri.decode(\"ascii\"), body, headers=headers\n                )\n            except HttpResponseException as e:\n                raise e.to_synapse_error() from e\n            except RequestSendFailed as e:\n                raise SynapseError(502, \"Failed to talk to master\") from e\n\n            return 200, result\n        else:\n            # Just interested in counts.\n            result = await self.store.count_e2e_one_time_keys(user_id, device_id)\n            return 200, {\"one_time_key_counts\": result}", "target": 0}, {"function": "class _NullContextManager(ContextManager[None]):\n    \"\"\"A context manager which does nothing.\"\"\"\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass", "target": 0}, {"function": "class GenericWorkerPresence(BasePresenceHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.hs = hs\n        self.is_mine_id = hs.is_mine_id\n        self.http_client = hs.get_simple_http_client()\n\n        self._presence_enabled = hs.config.use_presence\n\n        # The number of ongoing syncs on this process, by user id.\n        # Empty if _presence_enabled is false.\n        self._user_to_num_current_syncs = {}  # type: Dict[str, int]\n\n        self.notifier = hs.get_notifier()\n        self.instance_id = hs.get_instance_id()\n\n        # user_id -> last_sync_ms. Lists the users that have stopped syncing\n        # but we haven't notified the master of that yet\n        self.users_going_offline = {}\n\n        self._bump_active_client = ReplicationBumpPresenceActiveTime.make_client(hs)\n        self._set_state_client = ReplicationPresenceSetState.make_client(hs)\n\n        self._send_stop_syncing_loop = self.clock.looping_call(\n            self.send_stop_syncing, UPDATE_SYNCING_USERS_MS\n        )\n\n        hs.get_reactor().addSystemEventTrigger(\n            \"before\",\n            \"shutdown\",\n            run_as_background_process,\n            \"generic_presence.on_shutdown\",\n            self._on_shutdown,\n        )\n\n    def _on_shutdown(self):\n        if self._presence_enabled:\n            self.hs.get_tcp_replication().send_command(\n                ClearUserSyncsCommand(self.instance_id)\n            )\n\n    def send_user_sync(self, user_id, is_syncing, last_sync_ms):\n        if self._presence_enabled:\n            self.hs.get_tcp_replication().send_user_sync(\n                self.instance_id, user_id, is_syncing, last_sync_ms\n            )\n\n    def mark_as_coming_online(self, user_id):\n        \"\"\"A user has started syncing. Send a UserSync to the master, unless they\n        had recently stopped syncing.\n\n        Args:\n            user_id (str)\n        \"\"\"\n        going_offline = self.users_going_offline.pop(user_id, None)\n        if not going_offline:\n            # Safe to skip because we haven't yet told the master they were offline\n            self.send_user_sync(user_id, True, self.clock.time_msec())\n\n    def mark_as_going_offline(self, user_id):\n        \"\"\"A user has stopped syncing. We wait before notifying the master as\n        its likely they'll come back soon. This allows us to avoid sending\n        a stopped syncing immediately followed by a started syncing notification\n        to the master\n\n        Args:\n            user_id (str)\n        \"\"\"\n        self.users_going_offline[user_id] = self.clock.time_msec()\n\n    def send_stop_syncing(self):\n        \"\"\"Check if there are any users who have stopped syncing a while ago\n        and haven't come back yet. If there are poke the master about them.\n        \"\"\"\n        now = self.clock.time_msec()\n        for user_id, last_sync_ms in list(self.users_going_offline.items()):\n            if now - last_sync_ms > UPDATE_SYNCING_USERS_MS:\n                self.users_going_offline.pop(user_id, None)\n                self.send_user_sync(user_id, False, last_sync_ms)\n\n    async def user_syncing(\n        self, user_id: str, affect_presence: bool\n    ) -> ContextManager[None]:\n        \"\"\"Record that a user is syncing.\n\n        Called by the sync and events servlets to record that a user has connected to\n        this worker and is waiting for some events.\n        \"\"\"\n        if not affect_presence or not self._presence_enabled:\n            return _NullContextManager()\n\n        curr_sync = self._user_to_num_current_syncs.get(user_id, 0)\n        self._user_to_num_current_syncs[user_id] = curr_sync + 1\n\n        # If we went from no in flight sync to some, notify replication\n        if self._user_to_num_current_syncs[user_id] == 1:\n            self.mark_as_coming_online(user_id)\n\n        def _end():\n            # We check that the user_id is in user_to_num_current_syncs because\n            # user_to_num_current_syncs may have been cleared if we are\n            # shutting down.\n            if user_id in self._user_to_num_current_syncs:\n                self._user_to_num_current_syncs[user_id] -= 1\n\n                # If we went from one in flight sync to non, notify replication\n                if self._user_to_num_current_syncs[user_id] == 0:\n                    self.mark_as_going_offline(user_id)\n\n        @contextlib.contextmanager\n        def _user_syncing():\n            try:\n                yield\n            finally:\n                _end()\n\n        return _user_syncing()\n\n    async def notify_from_replication(self, states, stream_id):\n        parties = await get_interested_parties(self.store, states)\n        room_ids_to_states, users_to_states = parties\n\n        self.notifier.on_new_event(\n            \"presence_key\",\n            stream_id,\n            rooms=room_ids_to_states.keys(),\n            users=users_to_states.keys(),\n        )\n\n    async def process_replication_rows(self, token, rows):\n        states = [\n            UserPresenceState(\n                row.user_id,\n                row.state,\n                row.last_active_ts,\n                row.last_federation_update_ts,\n                row.last_user_sync_ts,\n                row.status_msg,\n                row.currently_active,\n            )\n            for row in rows\n        ]\n\n        for state in states:\n            self.user_to_current_state[state.user_id] = state\n\n        stream_id = token\n        await self.notify_from_replication(states, stream_id)\n\n    def get_currently_syncing_users_for_replication(self) -> Iterable[str]:\n        return [\n            user_id\n            for user_id, count in self._user_to_num_current_syncs.items()\n            if count > 0\n        ]\n\n    async def set_state(self, target_user, state, ignore_status_msg=False):\n        \"\"\"Set the presence state of the user.\n        \"\"\"\n        presence = state[\"presence\"]\n\n        valid_presence = (\n            PresenceState.ONLINE,\n            PresenceState.UNAVAILABLE,\n            PresenceState.OFFLINE,\n        )\n        if presence not in valid_presence:\n            raise SynapseError(400, \"Invalid presence state\")\n\n        user_id = target_user.to_string()\n\n        # If presence is disabled, no-op\n        if not self.hs.config.use_presence:\n            return\n\n        # Proxy request to master\n        await self._set_state_client(\n            user_id=user_id, state=state, ignore_status_msg=ignore_status_msg\n        )\n\n    async def bump_presence_active_time(self, user):\n        \"\"\"We've seen the user do something that indicates they're interacting\n        with the app.\n        \"\"\"\n        # If presence is disabled, no-op\n        if not self.hs.config.use_presence:\n            return\n\n        # Proxy request to master\n        user_id = user.to_string()\n        await self._bump_active_client(user_id=user_id)", "target": 0}, {"function": "class GenericWorkerSlavedStore(\n    # FIXME(#3714): We need to add UserDirectoryStore as we write directly\n    # rather than going via the correct worker.\n    UserDirectoryStore,\n    StatsStore,\n    UIAuthWorkerStore,\n    SlavedDeviceInboxStore,\n    SlavedDeviceStore,\n    SlavedReceiptsStore,\n    SlavedPushRuleStore,\n    SlavedGroupServerStore,\n    SlavedAccountDataStore,\n    SlavedPusherStore,\n    CensorEventsStore,\n    ClientIpWorkerStore,\n    SlavedEventStore,\n    SlavedKeyStore,\n    RoomStore,\n    DirectoryStore,\n    SlavedApplicationServiceStore,\n    SlavedRegistrationStore,\n    SlavedTransactionStore,\n    SlavedProfileStore,\n    SlavedClientIpStore,\n    SlavedPresenceStore,\n    SlavedFilteringStore,\n    MonthlyActiveUsersWorkerStore,\n    MediaRepositoryStore,\n    ServerMetricsStore,\n    SearchWorkerStore,\n    TransactionWorkerStore,\n    BaseSlavedStore,\n):\n    pass", "target": 0}, {"function": "class GenericWorkerServer(HomeServer):\n    DATASTORE_CLASS = GenericWorkerSlavedStore\n\n    def _listen_http(self, listener_config: ListenerConfig):\n        port = listener_config.port\n        bind_addresses = listener_config.bind_addresses\n\n        assert listener_config.http_options is not None\n\n        site_tag = listener_config.http_options.tag\n        if site_tag is None:\n            site_tag = port\n\n        # We always include a health resource.\n        resources = {\"/health\": HealthResource()}\n\n        for res in listener_config.http_options.resources:\n            for name in res.names:\n                if name == \"metrics\":\n                    resources[METRICS_PREFIX] = MetricsResource(RegistryProxy)\n                elif name == \"client\":\n                    resource = JsonResource(self, canonical_json=False)\n\n                    PublicRoomListRestServlet(self).register(resource)\n                    RoomMemberListRestServlet(self).register(resource)\n                    JoinedRoomMemberListRestServlet(self).register(resource)\n                    RoomStateRestServlet(self).register(resource)\n                    RoomEventContextServlet(self).register(resource)\n                    RoomMessageListRestServlet(self).register(resource)\n                    RegisterRestServlet(self).register(resource)\n                    LoginRestServlet(self).register(resource)\n                    ThreepidRestServlet(self).register(resource)\n                    KeyQueryServlet(self).register(resource)\n                    KeyChangesServlet(self).register(resource)\n                    VoipRestServlet(self).register(resource)\n                    PushRuleRestServlet(self).register(resource)\n                    VersionsRestServlet(self).register(resource)\n                    RoomSendEventRestServlet(self).register(resource)\n                    RoomMembershipRestServlet(self).register(resource)\n                    RoomStateEventRestServlet(self).register(resource)\n                    JoinRoomAliasServlet(self).register(resource)\n                    ProfileAvatarURLRestServlet(self).register(resource)\n                    ProfileDisplaynameRestServlet(self).register(resource)\n                    ProfileRestServlet(self).register(resource)\n                    KeyUploadServlet(self).register(resource)\n                    AccountDataServlet(self).register(resource)\n                    RoomAccountDataServlet(self).register(resource)\n                    RoomTypingRestServlet(self).register(resource)\n\n                    sync.register_servlets(self, resource)\n                    events.register_servlets(self, resource)\n                    InitialSyncRestServlet(self).register(resource)\n                    RoomInitialSyncRestServlet(self).register(resource)\n\n                    user_directory.register_servlets(self, resource)\n\n                    # If presence is disabled, use the stub servlet that does\n                    # not allow sending presence\n                    if not self.config.use_presence:\n                        PresenceStatusStubServlet(self).register(resource)\n\n                    groups.register_servlets(self, resource)\n\n                    resources.update({CLIENT_API_PREFIX: resource})\n                elif name == \"federation\":\n                    resources.update({FEDERATION_PREFIX: TransportLayerServer(self)})\n                elif name == \"media\":\n                    if self.config.can_load_media_repo:\n                        media_repo = self.get_media_repository_resource()\n\n                        # We need to serve the admin servlets for media on the\n                        # worker.\n                        admin_resource = JsonResource(self, canonical_json=False)\n                        register_servlets_for_media_repo(self, admin_resource)\n\n                        resources.update(\n                            {\n                                MEDIA_PREFIX: media_repo,\n                                LEGACY_MEDIA_PREFIX: media_repo,\n                                \"/_synapse/admin\": admin_resource,\n                            }\n                        )\n                    else:\n                        logger.warning(\n                            \"A 'media' listener is configured but the media\"\n                            \" repository is disabled. Ignoring.\"\n                        )\n\n                if name == \"openid\" and \"federation\" not in res.names:\n                    # Only load the openid resource separately if federation resource\n                    # is not specified since federation resource includes openid\n                    # resource.\n                    resources.update(\n                        {\n                            FEDERATION_PREFIX: TransportLayerServer(\n                                self, servlet_groups=[\"openid\"]\n                            )\n                        }\n                    )\n\n                if name in [\"keys\", \"federation\"]:\n                    resources[SERVER_KEY_V2_PREFIX] = KeyApiV2Resource(self)\n\n                if name == \"replication\":\n                    resources[REPLICATION_PREFIX] = ReplicationRestResource(self)\n\n        root_resource = create_resource_tree(resources, OptionsResource())\n\n        _base.listen_tcp(\n            bind_addresses,\n            port,\n            SynapseSite(\n                \"synapse.access.http.%s\" % (site_tag,),\n                site_tag,\n                listener_config,\n                root_resource,\n                self.version_string,\n            ),\n            reactor=self.get_reactor(),\n        )\n\n        logger.info(\"Synapse worker now listening on port %d\", port)\n\n    def start_listening(self, listeners: Iterable[ListenerConfig]):\n        for listener in listeners:\n            if listener.type == \"http\":\n                self._listen_http(listener)\n            elif listener.type == \"manhole\":\n                _base.listen_tcp(\n                    listener.bind_addresses,\n                    listener.port,\n                    manhole(\n                        username=\"matrix\", password=\"rabbithole\", globals={\"hs\": self}\n                    ),\n                )\n            elif listener.type == \"metrics\":\n                if not self.get_config().enable_metrics:\n                    logger.warning(\n                        (\n                            \"Metrics listener configured, but \"\n                            \"enable_metrics is not True!\"\n                        )\n                    )\n                else:\n                    _base.listen_metrics(listener.bind_addresses, listener.port)\n            else:\n                logger.warning(\"Unsupported listener type: %s\", listener.type)\n\n        self.get_tcp_replication().start_replication(self)\n\n    async def remove_pusher(self, app_id, push_key, user_id):\n        self.get_tcp_replication().send_remove_pusher(app_id, push_key, user_id)\n\n    @cache_in_self\n    def get_replication_data_handler(self):\n        return GenericWorkerReplicationHandler(self)\n\n    @cache_in_self\n    def get_presence_handler(self):\n        return GenericWorkerPresence(self)", "target": 0}, {"function": "class GenericWorkerReplicationHandler(ReplicationDataHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        self.store = hs.get_datastore()\n        self.presence_handler = hs.get_presence_handler()  # type: GenericWorkerPresence\n        self.notifier = hs.get_notifier()\n\n        self.notify_pushers = hs.config.start_pushers\n        self.pusher_pool = hs.get_pusherpool()\n\n        self.send_handler = None  # type: Optional[FederationSenderHandler]\n        if hs.config.send_federation:\n            self.send_handler = FederationSenderHandler(hs)\n\n    async def on_rdata(self, stream_name, instance_name, token, rows):\n        await super().on_rdata(stream_name, instance_name, token, rows)\n        await self._process_and_notify(stream_name, instance_name, token, rows)\n\n    async def _process_and_notify(self, stream_name, instance_name, token, rows):\n        try:\n            if self.send_handler:\n                await self.send_handler.process_replication_rows(\n                    stream_name, token, rows\n                )\n\n            if stream_name == PushRulesStream.NAME:\n                self.notifier.on_new_event(\n                    \"push_rules_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name in (AccountDataStream.NAME, TagAccountDataStream.NAME):\n                self.notifier.on_new_event(\n                    \"account_data_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name == ReceiptsStream.NAME:\n                self.notifier.on_new_event(\n                    \"receipt_key\", token, rooms=[row.room_id for row in rows]\n                )\n                await self.pusher_pool.on_new_receipts(\n                    token, token, {row.room_id for row in rows}\n                )\n            elif stream_name == ToDeviceStream.NAME:\n                entities = [row.entity for row in rows if row.entity.startswith(\"@\")]\n                if entities:\n                    self.notifier.on_new_event(\"to_device_key\", token, users=entities)\n            elif stream_name == DeviceListsStream.NAME:\n                all_room_ids = set()  # type: Set[str]\n                for row in rows:\n                    if row.entity.startswith(\"@\"):\n                        room_ids = await self.store.get_rooms_for_user(row.entity)\n                        all_room_ids.update(room_ids)\n                self.notifier.on_new_event(\"device_list_key\", token, rooms=all_room_ids)\n            elif stream_name == PresenceStream.NAME:\n                await self.presence_handler.process_replication_rows(token, rows)\n            elif stream_name == GroupServerStream.NAME:\n                self.notifier.on_new_event(\n                    \"groups_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name == PushersStream.NAME:\n                for row in rows:\n                    if row.deleted:\n                        self.stop_pusher(row.user_id, row.app_id, row.pushkey)\n                    else:\n                        await self.start_pusher(row.user_id, row.app_id, row.pushkey)\n        except Exception:\n            logger.exception(\"Error processing replication\")\n\n    async def on_position(self, stream_name: str, instance_name: str, token: int):\n        await super().on_position(stream_name, instance_name, token)\n        # Also call on_rdata to ensure that stream positions are properly reset.\n        await self.on_rdata(stream_name, instance_name, token, [])\n\n    def stop_pusher(self, user_id, app_id, pushkey):\n        if not self.notify_pushers:\n            return\n\n        key = \"%s:%s\" % (app_id, pushkey)\n        pushers_for_user = self.pusher_pool.pushers.get(user_id, {})\n        pusher = pushers_for_user.pop(key, None)\n        if pusher is None:\n            return\n        logger.info(\"Stopping pusher %r / %r\", user_id, key)\n        pusher.on_stop()\n\n    async def start_pusher(self, user_id, app_id, pushkey):\n        if not self.notify_pushers:\n            return\n\n        key = \"%s:%s\" % (app_id, pushkey)\n        logger.info(\"Starting pusher %r / %r\", user_id, key)\n        return await self.pusher_pool.start_pusher_by_id(app_id, pushkey, user_id)\n\n    def on_remote_server_up(self, server: str):\n        \"\"\"Called when get a new REMOTE_SERVER_UP command.\"\"\"\n\n        # Let's wake up the transaction queue for the server in case we have\n        # pending stuff to send to it.\n        if self.send_handler:\n            self.send_handler.wake_destination(server)", "target": 0}, {"function": "class FederationSenderHandler:\n    \"\"\"Processes the fedration replication stream\n\n    This class is only instantiate on the worker responsible for sending outbound\n    federation transactions. It receives rows from the replication stream and forwards\n    the appropriate entries to the FederationSender class.\n    \"\"\"\n\n    def __init__(self, hs: GenericWorkerServer):\n        self.store = hs.get_datastore()\n        self._is_mine_id = hs.is_mine_id\n        self.federation_sender = hs.get_federation_sender()\n        self._hs = hs\n\n        # Stores the latest position in the federation stream we've gotten up\n        # to. This is always set before we use it.\n        self.federation_position = None\n\n        self._fed_position_linearizer = Linearizer(name=\"_fed_position_linearizer\")\n\n    def on_start(self):\n        # There may be some events that are persisted but haven't been sent,\n        # so send them now.\n        self.federation_sender.notify_new_events(\n            self.store.get_room_max_stream_ordering()\n        )\n\n    def wake_destination(self, server: str):\n        self.federation_sender.wake_destination(server)\n\n    async def process_replication_rows(self, stream_name, token, rows):\n        # The federation stream contains things that we want to send out, e.g.\n        # presence, typing, etc.\n        if stream_name == \"federation\":\n            send_queue.process_rows_for_federation(self.federation_sender, rows)\n            await self.update_token(token)\n\n        # ... and when new receipts happen\n        elif stream_name == ReceiptsStream.NAME:\n            await self._on_new_receipts(rows)\n\n        # ... as well as device updates and messages\n        elif stream_name == DeviceListsStream.NAME:\n            # The entities are either user IDs (starting with '@') whose devices\n            # have changed, or remote servers that we need to tell about\n            # changes.\n            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}\n            for host in hosts:\n                self.federation_sender.send_device_messages(host)\n\n        elif stream_name == ToDeviceStream.NAME:\n            # The to_device stream includes stuff to be pushed to both local\n            # clients and remote servers, so we ignore entities that start with\n            # '@' (since they'll be local users rather than destinations).\n            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}\n            for host in hosts:\n                self.federation_sender.send_device_messages(host)\n\n    async def _on_new_receipts(self, rows):\n        \"\"\"\n        Args:\n            rows (Iterable[synapse.replication.tcp.streams.ReceiptsStream.ReceiptsStreamRow]):\n                new receipts to be processed\n        \"\"\"\n        for receipt in rows:\n            # we only want to send on receipts for our own users\n            if not self._is_mine_id(receipt.user_id):\n                continue\n            receipt_info = ReadReceipt(\n                receipt.room_id,\n                receipt.receipt_type,\n                receipt.user_id,\n                [receipt.event_id],\n                receipt.data,\n            )\n            await self.federation_sender.send_read_receipt(receipt_info)\n\n    async def update_token(self, token):\n        \"\"\"Update the record of where we have processed to in the federation stream.\n\n        Called after we have processed a an update received over replication. Sends\n        a FEDERATION_ACK back to the master, and stores the token that we have processed\n         in `federation_stream_position` so that we can restart where we left off.\n        \"\"\"\n        self.federation_position = token\n\n        # We save and send the ACK to master asynchronously, so we don't block\n        # processing on persistence. We don't need to do this operation for\n        # every single RDATA we receive, we just need to do it periodically.\n\n        if self._fed_position_linearizer.is_queued(None):\n            # There is already a task queued up to save and send the token, so\n            # no need to queue up another task.\n            return\n\n        run_as_background_process(\"_save_and_send_ack\", self._save_and_send_ack)\n\n    async def _save_and_send_ack(self):\n        \"\"\"Save the current federation position in the database and send an ACK\n        to master with where we're up to.\n        \"\"\"\n        try:\n            # We linearize here to ensure we don't have races updating the token\n            #\n            # XXX this appears to be redundant, since the ReplicationCommandHandler\n            # has a linearizer which ensures that we only process one line of\n            # replication data at a time. Should we remove it, or is it doing useful\n            # service for robustness? Or could we replace it with an assertion that\n            # we're not being re-entered?\n\n            with (await self._fed_position_linearizer.queue(None)):\n                # We persist and ack the same position, so we take a copy of it\n                # here as otherwise it can get modified from underneath us.\n                current_position = self.federation_position\n\n                await self.store.update_federation_out_pos(\n                    \"federation\", current_position\n                )\n\n                # We ACK this token over replication so that the master can drop\n                # its in memory queues\n                self._hs.get_tcp_replication().send_federation_ack(current_position)\n        except Exception:\n            logger.exception(\"Error updating federation stream position\")", "target": 0}, {"function": "def start(config_options):\n    try:\n        config = HomeServerConfig.load_config(\"Synapse worker\", config_options)\n    except ConfigError as e:\n        sys.stderr.write(\"\\n\" + str(e) + \"\\n\")\n        sys.exit(1)\n\n    # For backwards compatibility let any of the old app names.\n    assert config.worker_app in (\n        \"synapse.app.appservice\",\n        \"synapse.app.client_reader\",\n        \"synapse.app.event_creator\",\n        \"synapse.app.federation_reader\",\n        \"synapse.app.federation_sender\",\n        \"synapse.app.frontend_proxy\",\n        \"synapse.app.generic_worker\",\n        \"synapse.app.media_repository\",\n        \"synapse.app.pusher\",\n        \"synapse.app.synchrotron\",\n        \"synapse.app.user_dir\",\n    )\n\n    if config.worker_app == \"synapse.app.appservice\":\n        if config.appservice.notify_appservices:\n            sys.stderr.write(\n                \"\\nThe appservices must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``notify_appservices: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the appservice to start since they will be disabled in the main config\n        config.appservice.notify_appservices = True\n    else:\n        # For other worker types we force this to off.\n        config.appservice.notify_appservices = False\n\n    if config.worker_app == \"synapse.app.pusher\":\n        if config.server.start_pushers:\n            sys.stderr.write(\n                \"\\nThe pushers must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``start_pushers: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.server.start_pushers = True\n    else:\n        # For other worker types we force this to off.\n        config.server.start_pushers = False\n\n    if config.worker_app == \"synapse.app.user_dir\":\n        if config.server.update_user_directory:\n            sys.stderr.write(\n                \"\\nThe update_user_directory must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``update_user_directory: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.server.update_user_directory = True\n    else:\n        # For other worker types we force this to off.\n        config.server.update_user_directory = False\n\n    if config.worker_app == \"synapse.app.federation_sender\":\n        if config.worker.send_federation:\n            sys.stderr.write(\n                \"\\nThe send_federation must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``send_federation: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.worker.send_federation = True\n    else:\n        # For other worker types we force this to off.\n        config.worker.send_federation = False\n\n    synapse.events.USE_FROZEN_DICTS = config.use_frozen_dicts\n\n    hs = GenericWorkerServer(\n        config.server_name,\n        config=config,\n        version_string=\"Synapse/\" + get_version_string(synapse),\n    )\n\n    setup_logging(hs, config, use_worker_options=True)\n\n    hs.setup()\n\n    # Ensure the replication streamer is always started in case we write to any\n    # streams. Will no-op if no streams can be written to by this worker.\n    hs.get_replication_streamer()\n\n    reactor.addSystemEventTrigger(\n        \"before\", \"startup\", _base.start, hs, config.worker_listeners\n    )\n\n    _base.start_worker_reactor(\"synapse-generic-worker\", config)", "target": 0}], "function_after": [{"function": "class PresenceStatusStubServlet(RestServlet):\n    \"\"\"If presence is disabled this servlet can be used to stub out setting\n    presence status.\n    \"\"\"\n\n    PATTERNS = client_patterns(\"/presence/(?P<user_id>[^/]*)/status\")\n\n    def __init__(self, hs):\n        super().__init__()\n        self.auth = hs.get_auth()\n\n    async def on_GET(self, request, user_id):\n        await self.auth.get_user_by_req(request)\n        return 200, {\"presence\": \"offline\"}\n\n    async def on_PUT(self, request, user_id):\n        await self.auth.get_user_by_req(request)\n        return 200, {}", "target": 0}, {"function": "class KeyUploadServlet(RestServlet):\n    \"\"\"An implementation of the `KeyUploadServlet` that responds to read only\n    requests, but otherwise proxies through to the master instance.\n    \"\"\"\n\n    PATTERNS = client_patterns(\"/keys/upload(/(?P<device_id>[^/]+))?$\")\n\n    def __init__(self, hs):\n        \"\"\"\n        Args:\n            hs (synapse.server.HomeServer): server\n        \"\"\"\n        super().__init__()\n        self.auth = hs.get_auth()\n        self.store = hs.get_datastore()\n        self.http_client = hs.get_simple_http_client()\n        self.main_uri = hs.config.worker_main_http_uri\n\n    async def on_POST(self, request, device_id):\n        requester = await self.auth.get_user_by_req(request, allow_guest=True)\n        user_id = requester.user.to_string()\n        body = parse_json_object_from_request(request)\n\n        if device_id is not None:\n            # passing the device_id here is deprecated; however, we allow it\n            # for now for compatibility with older clients.\n            if requester.device_id is not None and device_id != requester.device_id:\n                logger.warning(\n                    \"Client uploading keys for a different device \"\n                    \"(logged in as %s, uploading for %s)\",\n                    requester.device_id,\n                    device_id,\n                )\n        else:\n            device_id = requester.device_id\n\n        if device_id is None:\n            raise SynapseError(\n                400, \"To upload keys, you must pass device_id when authenticating\"\n            )\n\n        if body:\n            # They're actually trying to upload something, proxy to main synapse.\n\n            # Proxy headers from the original request, such as the auth headers\n            # (in case the access token is there) and the original IP /\n            # User-Agent of the request.\n            headers = {\n                header: request.requestHeaders.getRawHeaders(header, [])\n                for header in (b\"Authorization\", b\"User-Agent\")\n            }\n            # Add the previous hop the the X-Forwarded-For header.\n            x_forwarded_for = request.requestHeaders.getRawHeaders(\n                b\"X-Forwarded-For\", []\n            )\n            if isinstance(request.client, (address.IPv4Address, address.IPv6Address)):\n                previous_host = request.client.host.encode(\"ascii\")\n                # If the header exists, add to the comma-separated list of the first\n                # instance of the header. Otherwise, generate a new header.\n                if x_forwarded_for:\n                    x_forwarded_for = [\n                        x_forwarded_for[0] + b\", \" + previous_host\n                    ] + x_forwarded_for[1:]\n                else:\n                    x_forwarded_for = [previous_host]\n            headers[b\"X-Forwarded-For\"] = x_forwarded_for\n\n            try:\n                result = await self.http_client.post_json_get_json(\n                    self.main_uri + request.uri.decode(\"ascii\"), body, headers=headers\n                )\n            except HttpResponseException as e:\n                raise e.to_synapse_error() from e\n            except RequestSendFailed as e:\n                raise SynapseError(502, \"Failed to talk to master\") from e\n\n            return 200, result\n        else:\n            # Just interested in counts.\n            result = await self.store.count_e2e_one_time_keys(user_id, device_id)\n            return 200, {\"one_time_key_counts\": result}", "target": 0}, {"function": "class _NullContextManager(ContextManager[None]):\n    \"\"\"A context manager which does nothing.\"\"\"\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass", "target": 0}, {"function": "class GenericWorkerPresence(BasePresenceHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.hs = hs\n        self.is_mine_id = hs.is_mine_id\n\n        self._presence_enabled = hs.config.use_presence\n\n        # The number of ongoing syncs on this process, by user id.\n        # Empty if _presence_enabled is false.\n        self._user_to_num_current_syncs = {}  # type: Dict[str, int]\n\n        self.notifier = hs.get_notifier()\n        self.instance_id = hs.get_instance_id()\n\n        # user_id -> last_sync_ms. Lists the users that have stopped syncing\n        # but we haven't notified the master of that yet\n        self.users_going_offline = {}\n\n        self._bump_active_client = ReplicationBumpPresenceActiveTime.make_client(hs)\n        self._set_state_client = ReplicationPresenceSetState.make_client(hs)\n\n        self._send_stop_syncing_loop = self.clock.looping_call(\n            self.send_stop_syncing, UPDATE_SYNCING_USERS_MS\n        )\n\n        hs.get_reactor().addSystemEventTrigger(\n            \"before\",\n            \"shutdown\",\n            run_as_background_process,\n            \"generic_presence.on_shutdown\",\n            self._on_shutdown,\n        )\n\n    def _on_shutdown(self):\n        if self._presence_enabled:\n            self.hs.get_tcp_replication().send_command(\n                ClearUserSyncsCommand(self.instance_id)\n            )\n\n    def send_user_sync(self, user_id, is_syncing, last_sync_ms):\n        if self._presence_enabled:\n            self.hs.get_tcp_replication().send_user_sync(\n                self.instance_id, user_id, is_syncing, last_sync_ms\n            )\n\n    def mark_as_coming_online(self, user_id):\n        \"\"\"A user has started syncing. Send a UserSync to the master, unless they\n        had recently stopped syncing.\n\n        Args:\n            user_id (str)\n        \"\"\"\n        going_offline = self.users_going_offline.pop(user_id, None)\n        if not going_offline:\n            # Safe to skip because we haven't yet told the master they were offline\n            self.send_user_sync(user_id, True, self.clock.time_msec())\n\n    def mark_as_going_offline(self, user_id):\n        \"\"\"A user has stopped syncing. We wait before notifying the master as\n        its likely they'll come back soon. This allows us to avoid sending\n        a stopped syncing immediately followed by a started syncing notification\n        to the master\n\n        Args:\n            user_id (str)\n        \"\"\"\n        self.users_going_offline[user_id] = self.clock.time_msec()\n\n    def send_stop_syncing(self):\n        \"\"\"Check if there are any users who have stopped syncing a while ago\n        and haven't come back yet. If there are poke the master about them.\n        \"\"\"\n        now = self.clock.time_msec()\n        for user_id, last_sync_ms in list(self.users_going_offline.items()):\n            if now - last_sync_ms > UPDATE_SYNCING_USERS_MS:\n                self.users_going_offline.pop(user_id, None)\n                self.send_user_sync(user_id, False, last_sync_ms)\n\n    async def user_syncing(\n        self, user_id: str, affect_presence: bool\n    ) -> ContextManager[None]:\n        \"\"\"Record that a user is syncing.\n\n        Called by the sync and events servlets to record that a user has connected to\n        this worker and is waiting for some events.\n        \"\"\"\n        if not affect_presence or not self._presence_enabled:\n            return _NullContextManager()\n\n        curr_sync = self._user_to_num_current_syncs.get(user_id, 0)\n        self._user_to_num_current_syncs[user_id] = curr_sync + 1\n\n        # If we went from no in flight sync to some, notify replication\n        if self._user_to_num_current_syncs[user_id] == 1:\n            self.mark_as_coming_online(user_id)\n\n        def _end():\n            # We check that the user_id is in user_to_num_current_syncs because\n            # user_to_num_current_syncs may have been cleared if we are\n            # shutting down.\n            if user_id in self._user_to_num_current_syncs:\n                self._user_to_num_current_syncs[user_id] -= 1\n\n                # If we went from one in flight sync to non, notify replication\n                if self._user_to_num_current_syncs[user_id] == 0:\n                    self.mark_as_going_offline(user_id)\n\n        @contextlib.contextmanager\n        def _user_syncing():\n            try:\n                yield\n            finally:\n                _end()\n\n        return _user_syncing()\n\n    async def notify_from_replication(self, states, stream_id):\n        parties = await get_interested_parties(self.store, states)\n        room_ids_to_states, users_to_states = parties\n\n        self.notifier.on_new_event(\n            \"presence_key\",\n            stream_id,\n            rooms=room_ids_to_states.keys(),\n            users=users_to_states.keys(),\n        )\n\n    async def process_replication_rows(self, token, rows):\n        states = [\n            UserPresenceState(\n                row.user_id,\n                row.state,\n                row.last_active_ts,\n                row.last_federation_update_ts,\n                row.last_user_sync_ts,\n                row.status_msg,\n                row.currently_active,\n            )\n            for row in rows\n        ]\n\n        for state in states:\n            self.user_to_current_state[state.user_id] = state\n\n        stream_id = token\n        await self.notify_from_replication(states, stream_id)\n\n    def get_currently_syncing_users_for_replication(self) -> Iterable[str]:\n        return [\n            user_id\n            for user_id, count in self._user_to_num_current_syncs.items()\n            if count > 0\n        ]\n\n    async def set_state(self, target_user, state, ignore_status_msg=False):\n        \"\"\"Set the presence state of the user.\n        \"\"\"\n        presence = state[\"presence\"]\n\n        valid_presence = (\n            PresenceState.ONLINE,\n            PresenceState.UNAVAILABLE,\n            PresenceState.OFFLINE,\n        )\n        if presence not in valid_presence:\n            raise SynapseError(400, \"Invalid presence state\")\n\n        user_id = target_user.to_string()\n\n        # If presence is disabled, no-op\n        if not self.hs.config.use_presence:\n            return\n\n        # Proxy request to master\n        await self._set_state_client(\n            user_id=user_id, state=state, ignore_status_msg=ignore_status_msg\n        )\n\n    async def bump_presence_active_time(self, user):\n        \"\"\"We've seen the user do something that indicates they're interacting\n        with the app.\n        \"\"\"\n        # If presence is disabled, no-op\n        if not self.hs.config.use_presence:\n            return\n\n        # Proxy request to master\n        user_id = user.to_string()\n        await self._bump_active_client(user_id=user_id)", "target": 0}, {"function": "class GenericWorkerSlavedStore(\n    # FIXME(#3714): We need to add UserDirectoryStore as we write directly\n    # rather than going via the correct worker.\n    UserDirectoryStore,\n    StatsStore,\n    UIAuthWorkerStore,\n    SlavedDeviceInboxStore,\n    SlavedDeviceStore,\n    SlavedReceiptsStore,\n    SlavedPushRuleStore,\n    SlavedGroupServerStore,\n    SlavedAccountDataStore,\n    SlavedPusherStore,\n    CensorEventsStore,\n    ClientIpWorkerStore,\n    SlavedEventStore,\n    SlavedKeyStore,\n    RoomStore,\n    DirectoryStore,\n    SlavedApplicationServiceStore,\n    SlavedRegistrationStore,\n    SlavedTransactionStore,\n    SlavedProfileStore,\n    SlavedClientIpStore,\n    SlavedPresenceStore,\n    SlavedFilteringStore,\n    MonthlyActiveUsersWorkerStore,\n    MediaRepositoryStore,\n    ServerMetricsStore,\n    SearchWorkerStore,\n    TransactionWorkerStore,\n    BaseSlavedStore,\n):\n    pass", "target": 0}, {"function": "class GenericWorkerServer(HomeServer):\n    DATASTORE_CLASS = GenericWorkerSlavedStore\n\n    def _listen_http(self, listener_config: ListenerConfig):\n        port = listener_config.port\n        bind_addresses = listener_config.bind_addresses\n\n        assert listener_config.http_options is not None\n\n        site_tag = listener_config.http_options.tag\n        if site_tag is None:\n            site_tag = port\n\n        # We always include a health resource.\n        resources = {\"/health\": HealthResource()}\n\n        for res in listener_config.http_options.resources:\n            for name in res.names:\n                if name == \"metrics\":\n                    resources[METRICS_PREFIX] = MetricsResource(RegistryProxy)\n                elif name == \"client\":\n                    resource = JsonResource(self, canonical_json=False)\n\n                    PublicRoomListRestServlet(self).register(resource)\n                    RoomMemberListRestServlet(self).register(resource)\n                    JoinedRoomMemberListRestServlet(self).register(resource)\n                    RoomStateRestServlet(self).register(resource)\n                    RoomEventContextServlet(self).register(resource)\n                    RoomMessageListRestServlet(self).register(resource)\n                    RegisterRestServlet(self).register(resource)\n                    LoginRestServlet(self).register(resource)\n                    ThreepidRestServlet(self).register(resource)\n                    KeyQueryServlet(self).register(resource)\n                    KeyChangesServlet(self).register(resource)\n                    VoipRestServlet(self).register(resource)\n                    PushRuleRestServlet(self).register(resource)\n                    VersionsRestServlet(self).register(resource)\n                    RoomSendEventRestServlet(self).register(resource)\n                    RoomMembershipRestServlet(self).register(resource)\n                    RoomStateEventRestServlet(self).register(resource)\n                    JoinRoomAliasServlet(self).register(resource)\n                    ProfileAvatarURLRestServlet(self).register(resource)\n                    ProfileDisplaynameRestServlet(self).register(resource)\n                    ProfileRestServlet(self).register(resource)\n                    KeyUploadServlet(self).register(resource)\n                    AccountDataServlet(self).register(resource)\n                    RoomAccountDataServlet(self).register(resource)\n                    RoomTypingRestServlet(self).register(resource)\n\n                    sync.register_servlets(self, resource)\n                    events.register_servlets(self, resource)\n                    InitialSyncRestServlet(self).register(resource)\n                    RoomInitialSyncRestServlet(self).register(resource)\n\n                    user_directory.register_servlets(self, resource)\n\n                    # If presence is disabled, use the stub servlet that does\n                    # not allow sending presence\n                    if not self.config.use_presence:\n                        PresenceStatusStubServlet(self).register(resource)\n\n                    groups.register_servlets(self, resource)\n\n                    resources.update({CLIENT_API_PREFIX: resource})\n                elif name == \"federation\":\n                    resources.update({FEDERATION_PREFIX: TransportLayerServer(self)})\n                elif name == \"media\":\n                    if self.config.can_load_media_repo:\n                        media_repo = self.get_media_repository_resource()\n\n                        # We need to serve the admin servlets for media on the\n                        # worker.\n                        admin_resource = JsonResource(self, canonical_json=False)\n                        register_servlets_for_media_repo(self, admin_resource)\n\n                        resources.update(\n                            {\n                                MEDIA_PREFIX: media_repo,\n                                LEGACY_MEDIA_PREFIX: media_repo,\n                                \"/_synapse/admin\": admin_resource,\n                            }\n                        )\n                    else:\n                        logger.warning(\n                            \"A 'media' listener is configured but the media\"\n                            \" repository is disabled. Ignoring.\"\n                        )\n\n                if name == \"openid\" and \"federation\" not in res.names:\n                    # Only load the openid resource separately if federation resource\n                    # is not specified since federation resource includes openid\n                    # resource.\n                    resources.update(\n                        {\n                            FEDERATION_PREFIX: TransportLayerServer(\n                                self, servlet_groups=[\"openid\"]\n                            )\n                        }\n                    )\n\n                if name in [\"keys\", \"federation\"]:\n                    resources[SERVER_KEY_V2_PREFIX] = KeyApiV2Resource(self)\n\n                if name == \"replication\":\n                    resources[REPLICATION_PREFIX] = ReplicationRestResource(self)\n\n        root_resource = create_resource_tree(resources, OptionsResource())\n\n        _base.listen_tcp(\n            bind_addresses,\n            port,\n            SynapseSite(\n                \"synapse.access.http.%s\" % (site_tag,),\n                site_tag,\n                listener_config,\n                root_resource,\n                self.version_string,\n            ),\n            reactor=self.get_reactor(),\n        )\n\n        logger.info(\"Synapse worker now listening on port %d\", port)\n\n    def start_listening(self, listeners: Iterable[ListenerConfig]):\n        for listener in listeners:\n            if listener.type == \"http\":\n                self._listen_http(listener)\n            elif listener.type == \"manhole\":\n                _base.listen_tcp(\n                    listener.bind_addresses,\n                    listener.port,\n                    manhole(\n                        username=\"matrix\", password=\"rabbithole\", globals={\"hs\": self}\n                    ),\n                )\n            elif listener.type == \"metrics\":\n                if not self.get_config().enable_metrics:\n                    logger.warning(\n                        (\n                            \"Metrics listener configured, but \"\n                            \"enable_metrics is not True!\"\n                        )\n                    )\n                else:\n                    _base.listen_metrics(listener.bind_addresses, listener.port)\n            else:\n                logger.warning(\"Unsupported listener type: %s\", listener.type)\n\n        self.get_tcp_replication().start_replication(self)\n\n    async def remove_pusher(self, app_id, push_key, user_id):\n        self.get_tcp_replication().send_remove_pusher(app_id, push_key, user_id)\n\n    @cache_in_self\n    def get_replication_data_handler(self):\n        return GenericWorkerReplicationHandler(self)\n\n    @cache_in_self\n    def get_presence_handler(self):\n        return GenericWorkerPresence(self)", "target": 0}, {"function": "class GenericWorkerReplicationHandler(ReplicationDataHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        self.store = hs.get_datastore()\n        self.presence_handler = hs.get_presence_handler()  # type: GenericWorkerPresence\n        self.notifier = hs.get_notifier()\n\n        self.notify_pushers = hs.config.start_pushers\n        self.pusher_pool = hs.get_pusherpool()\n\n        self.send_handler = None  # type: Optional[FederationSenderHandler]\n        if hs.config.send_federation:\n            self.send_handler = FederationSenderHandler(hs)\n\n    async def on_rdata(self, stream_name, instance_name, token, rows):\n        await super().on_rdata(stream_name, instance_name, token, rows)\n        await self._process_and_notify(stream_name, instance_name, token, rows)\n\n    async def _process_and_notify(self, stream_name, instance_name, token, rows):\n        try:\n            if self.send_handler:\n                await self.send_handler.process_replication_rows(\n                    stream_name, token, rows\n                )\n\n            if stream_name == PushRulesStream.NAME:\n                self.notifier.on_new_event(\n                    \"push_rules_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name in (AccountDataStream.NAME, TagAccountDataStream.NAME):\n                self.notifier.on_new_event(\n                    \"account_data_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name == ReceiptsStream.NAME:\n                self.notifier.on_new_event(\n                    \"receipt_key\", token, rooms=[row.room_id for row in rows]\n                )\n                await self.pusher_pool.on_new_receipts(\n                    token, token, {row.room_id for row in rows}\n                )\n            elif stream_name == ToDeviceStream.NAME:\n                entities = [row.entity for row in rows if row.entity.startswith(\"@\")]\n                if entities:\n                    self.notifier.on_new_event(\"to_device_key\", token, users=entities)\n            elif stream_name == DeviceListsStream.NAME:\n                all_room_ids = set()  # type: Set[str]\n                for row in rows:\n                    if row.entity.startswith(\"@\"):\n                        room_ids = await self.store.get_rooms_for_user(row.entity)\n                        all_room_ids.update(room_ids)\n                self.notifier.on_new_event(\"device_list_key\", token, rooms=all_room_ids)\n            elif stream_name == PresenceStream.NAME:\n                await self.presence_handler.process_replication_rows(token, rows)\n            elif stream_name == GroupServerStream.NAME:\n                self.notifier.on_new_event(\n                    \"groups_key\", token, users=[row.user_id for row in rows]\n                )\n            elif stream_name == PushersStream.NAME:\n                for row in rows:\n                    if row.deleted:\n                        self.stop_pusher(row.user_id, row.app_id, row.pushkey)\n                    else:\n                        await self.start_pusher(row.user_id, row.app_id, row.pushkey)\n        except Exception:\n            logger.exception(\"Error processing replication\")\n\n    async def on_position(self, stream_name: str, instance_name: str, token: int):\n        await super().on_position(stream_name, instance_name, token)\n        # Also call on_rdata to ensure that stream positions are properly reset.\n        await self.on_rdata(stream_name, instance_name, token, [])\n\n    def stop_pusher(self, user_id, app_id, pushkey):\n        if not self.notify_pushers:\n            return\n\n        key = \"%s:%s\" % (app_id, pushkey)\n        pushers_for_user = self.pusher_pool.pushers.get(user_id, {})\n        pusher = pushers_for_user.pop(key, None)\n        if pusher is None:\n            return\n        logger.info(\"Stopping pusher %r / %r\", user_id, key)\n        pusher.on_stop()\n\n    async def start_pusher(self, user_id, app_id, pushkey):\n        if not self.notify_pushers:\n            return\n\n        key = \"%s:%s\" % (app_id, pushkey)\n        logger.info(\"Starting pusher %r / %r\", user_id, key)\n        return await self.pusher_pool.start_pusher_by_id(app_id, pushkey, user_id)\n\n    def on_remote_server_up(self, server: str):\n        \"\"\"Called when get a new REMOTE_SERVER_UP command.\"\"\"\n\n        # Let's wake up the transaction queue for the server in case we have\n        # pending stuff to send to it.\n        if self.send_handler:\n            self.send_handler.wake_destination(server)", "target": 0}, {"function": "class FederationSenderHandler:\n    \"\"\"Processes the fedration replication stream\n\n    This class is only instantiate on the worker responsible for sending outbound\n    federation transactions. It receives rows from the replication stream and forwards\n    the appropriate entries to the FederationSender class.\n    \"\"\"\n\n    def __init__(self, hs: GenericWorkerServer):\n        self.store = hs.get_datastore()\n        self._is_mine_id = hs.is_mine_id\n        self.federation_sender = hs.get_federation_sender()\n        self._hs = hs\n\n        # Stores the latest position in the federation stream we've gotten up\n        # to. This is always set before we use it.\n        self.federation_position = None\n\n        self._fed_position_linearizer = Linearizer(name=\"_fed_position_linearizer\")\n\n    def on_start(self):\n        # There may be some events that are persisted but haven't been sent,\n        # so send them now.\n        self.federation_sender.notify_new_events(\n            self.store.get_room_max_stream_ordering()\n        )\n\n    def wake_destination(self, server: str):\n        self.federation_sender.wake_destination(server)\n\n    async def process_replication_rows(self, stream_name, token, rows):\n        # The federation stream contains things that we want to send out, e.g.\n        # presence, typing, etc.\n        if stream_name == \"federation\":\n            send_queue.process_rows_for_federation(self.federation_sender, rows)\n            await self.update_token(token)\n\n        # ... and when new receipts happen\n        elif stream_name == ReceiptsStream.NAME:\n            await self._on_new_receipts(rows)\n\n        # ... as well as device updates and messages\n        elif stream_name == DeviceListsStream.NAME:\n            # The entities are either user IDs (starting with '@') whose devices\n            # have changed, or remote servers that we need to tell about\n            # changes.\n            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}\n            for host in hosts:\n                self.federation_sender.send_device_messages(host)\n\n        elif stream_name == ToDeviceStream.NAME:\n            # The to_device stream includes stuff to be pushed to both local\n            # clients and remote servers, so we ignore entities that start with\n            # '@' (since they'll be local users rather than destinations).\n            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}\n            for host in hosts:\n                self.federation_sender.send_device_messages(host)\n\n    async def _on_new_receipts(self, rows):\n        \"\"\"\n        Args:\n            rows (Iterable[synapse.replication.tcp.streams.ReceiptsStream.ReceiptsStreamRow]):\n                new receipts to be processed\n        \"\"\"\n        for receipt in rows:\n            # we only want to send on receipts for our own users\n            if not self._is_mine_id(receipt.user_id):\n                continue\n            receipt_info = ReadReceipt(\n                receipt.room_id,\n                receipt.receipt_type,\n                receipt.user_id,\n                [receipt.event_id],\n                receipt.data,\n            )\n            await self.federation_sender.send_read_receipt(receipt_info)\n\n    async def update_token(self, token):\n        \"\"\"Update the record of where we have processed to in the federation stream.\n\n        Called after we have processed a an update received over replication. Sends\n        a FEDERATION_ACK back to the master, and stores the token that we have processed\n         in `federation_stream_position` so that we can restart where we left off.\n        \"\"\"\n        self.federation_position = token\n\n        # We save and send the ACK to master asynchronously, so we don't block\n        # processing on persistence. We don't need to do this operation for\n        # every single RDATA we receive, we just need to do it periodically.\n\n        if self._fed_position_linearizer.is_queued(None):\n            # There is already a task queued up to save and send the token, so\n            # no need to queue up another task.\n            return\n\n        run_as_background_process(\"_save_and_send_ack\", self._save_and_send_ack)\n\n    async def _save_and_send_ack(self):\n        \"\"\"Save the current federation position in the database and send an ACK\n        to master with where we're up to.\n        \"\"\"\n        try:\n            # We linearize here to ensure we don't have races updating the token\n            #\n            # XXX this appears to be redundant, since the ReplicationCommandHandler\n            # has a linearizer which ensures that we only process one line of\n            # replication data at a time. Should we remove it, or is it doing useful\n            # service for robustness? Or could we replace it with an assertion that\n            # we're not being re-entered?\n\n            with (await self._fed_position_linearizer.queue(None)):\n                # We persist and ack the same position, so we take a copy of it\n                # here as otherwise it can get modified from underneath us.\n                current_position = self.federation_position\n\n                await self.store.update_federation_out_pos(\n                    \"federation\", current_position\n                )\n\n                # We ACK this token over replication so that the master can drop\n                # its in memory queues\n                self._hs.get_tcp_replication().send_federation_ack(current_position)\n        except Exception:\n            logger.exception(\"Error updating federation stream position\")", "target": 0}, {"function": "def start(config_options):\n    try:\n        config = HomeServerConfig.load_config(\"Synapse worker\", config_options)\n    except ConfigError as e:\n        sys.stderr.write(\"\\n\" + str(e) + \"\\n\")\n        sys.exit(1)\n\n    # For backwards compatibility let any of the old app names.\n    assert config.worker_app in (\n        \"synapse.app.appservice\",\n        \"synapse.app.client_reader\",\n        \"synapse.app.event_creator\",\n        \"synapse.app.federation_reader\",\n        \"synapse.app.federation_sender\",\n        \"synapse.app.frontend_proxy\",\n        \"synapse.app.generic_worker\",\n        \"synapse.app.media_repository\",\n        \"synapse.app.pusher\",\n        \"synapse.app.synchrotron\",\n        \"synapse.app.user_dir\",\n    )\n\n    if config.worker_app == \"synapse.app.appservice\":\n        if config.appservice.notify_appservices:\n            sys.stderr.write(\n                \"\\nThe appservices must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``notify_appservices: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the appservice to start since they will be disabled in the main config\n        config.appservice.notify_appservices = True\n    else:\n        # For other worker types we force this to off.\n        config.appservice.notify_appservices = False\n\n    if config.worker_app == \"synapse.app.pusher\":\n        if config.server.start_pushers:\n            sys.stderr.write(\n                \"\\nThe pushers must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``start_pushers: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.server.start_pushers = True\n    else:\n        # For other worker types we force this to off.\n        config.server.start_pushers = False\n\n    if config.worker_app == \"synapse.app.user_dir\":\n        if config.server.update_user_directory:\n            sys.stderr.write(\n                \"\\nThe update_user_directory must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``update_user_directory: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.server.update_user_directory = True\n    else:\n        # For other worker types we force this to off.\n        config.server.update_user_directory = False\n\n    if config.worker_app == \"synapse.app.federation_sender\":\n        if config.worker.send_federation:\n            sys.stderr.write(\n                \"\\nThe send_federation must be disabled in the main synapse process\"\n                \"\\nbefore they can be run in a separate worker.\"\n                \"\\nPlease add ``send_federation: false`` to the main config\"\n                \"\\n\"\n            )\n            sys.exit(1)\n\n        # Force the pushers to start since they will be disabled in the main config\n        config.worker.send_federation = True\n    else:\n        # For other worker types we force this to off.\n        config.worker.send_federation = False\n\n    synapse.events.USE_FROZEN_DICTS = config.use_frozen_dicts\n\n    hs = GenericWorkerServer(\n        config.server_name,\n        config=config,\n        version_string=\"Synapse/\" + get_version_string(synapse),\n    )\n\n    setup_logging(hs, config, use_worker_options=True)\n\n    hs.setup()\n\n    # Ensure the replication streamer is always started in case we write to any\n    # streams. Will no-op if no streams can be written to by this worker.\n    hs.get_replication_streamer()\n\n    reactor.addSystemEventTrigger(\n        \"before\", \"startup\", _base.start, hs, config.worker_listeners\n    )\n\n    _base.start_worker_reactor(\"synapse-generic-worker\", config)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Fconfig%2Ffederation.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nfrom netaddr import IPSet\n\nfrom synapse.config._base import Config, ConfigError\nfrom synapse.config._util import validate_config\n\n\nclass FederationConfig(Config):\n    section = \"federation\"\n\n    def read_config(self, config, **kwargs):\n        # FIXME: federation_domain_whitelist needs sytests\n        self.federation_domain_whitelist = None  # type: Optional[dict]\n        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)\n\n        if federation_domain_whitelist is not None:\n            # turn the whitelist into a hash for speed of lookup\n            self.federation_domain_whitelist = {}\n\n            for domain in federation_domain_whitelist:\n                self.federation_domain_whitelist[domain] = True\n\n        ip_range_blacklist = config.get(\"ip_range_blacklist\", [])\n\n        # Attempt to create an IPSet from the given ranges\n        try:\n            self.ip_range_blacklist = IPSet(ip_range_blacklist)\n        except Exception as e:\n            raise ConfigError(\"Invalid range(s) provided in ip_range_blacklist: %s\" % e)\n        # Always blacklist 0.0.0.0, ::\n        self.ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n\n        # The federation_ip_range_blacklist is used for backwards-compatibility\n        # and only applies to federation and identity servers. If it is not given,\n        # default to ip_range_blacklist.\n        federation_ip_range_blacklist = config.get(\n            \"federation_ip_range_blacklist\", ip_range_blacklist\n        )\n        try:\n            self.federation_ip_range_blacklist = IPSet(federation_ip_range_blacklist)\n        except Exception as e:\n            raise ConfigError(\n                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e\n            )\n        # Always blacklist 0.0.0.0, ::\n        self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n\n        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []\n        validate_config(\n            _METRICS_FOR_DOMAINS_SCHEMA,\n            federation_metrics_domains,\n            (\"federation_metrics_domains\",),\n        )\n        self.federation_metrics_domains = set(federation_metrics_domains)\n\n    def generate_config_section(self, config_dir_path, server_name, **kwargs):\n        return \"\"\"\\\n        ## Federation ##\n\n        # Restrict federation to the following whitelist of domains.\n        # N.B. we recommend also firewalling your federation listener to limit\n        # inbound federation traffic as early as possible, rather than relying\n        # purely on this application-layer restriction.  If not specified, the\n        # default is to whitelist everything.\n        #\n        #federation_domain_whitelist:\n        #  - lon.example.com\n        #  - nyc.example.com\n        #  - syd.example.com\n\n        # Prevent outgoing requests from being sent to the following blacklisted IP address\n        # CIDR ranges. If this option is not specified, or specified with an empty list,\n        # no IP range blacklist will be enforced.\n        #\n        # The blacklist applies to the outbound requests for federation, identity servers,\n        # push servers, and for checking key validitity for third-party invite events.\n        #\n        # (0.0.0.0 and :: are always blacklisted, whether or not they are explicitly\n        # listed here, since they correspond to unroutable addresses.)\n        #\n        # This option replaces federation_ip_range_blacklist in Synapse v1.24.0.\n        #\n        ip_range_blacklist:\n          - '127.0.0.0/8'\n          - '10.0.0.0/8'\n          - '172.16.0.0/12'\n          - '192.168.0.0/16'\n          - '100.64.0.0/10'\n          - '169.254.0.0/16'\n          - '::1/128'\n          - 'fe80::/64'\n          - 'fc00::/7'\n\n        # Report prometheus metrics on the age of PDUs being sent to and received from\n        # the following domains. This can be used to give an idea of \"delay\" on inbound\n        # and outbound federation, though be aware that any delay can be due to problems\n        # at either end or with the intermediate network.\n        #\n        # By default, no domains are monitored in this way.\n        #\n        #federation_metrics_domains:\n        #  - matrix.org\n        #  - example.com\n        \"\"\"\n\n\n_METRICS_FOR_DOMAINS_SCHEMA = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nfrom netaddr import IPSet\n\nfrom synapse.config._base import Config, ConfigError\nfrom synapse.config._util import validate_config\n\n\nclass FederationConfig(Config):\n    section = \"federation\"\n\n    def read_config(self, config, **kwargs):\n        # FIXME: federation_domain_whitelist needs sytests\n        self.federation_domain_whitelist = None  # type: Optional[dict]\n        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)\n\n        if federation_domain_whitelist is not None:\n            # turn the whitelist into a hash for speed of lookup\n            self.federation_domain_whitelist = {}\n\n            for domain in federation_domain_whitelist:\n                self.federation_domain_whitelist[domain] = True\n\n        self.federation_ip_range_blacklist = config.get(\n            \"federation_ip_range_blacklist\", []\n        )\n\n        # Attempt to create an IPSet from the given ranges\n        try:\n            self.federation_ip_range_blacklist = IPSet(\n                self.federation_ip_range_blacklist\n            )\n\n            # Always blacklist 0.0.0.0, ::\n            self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n        except Exception as e:\n            raise ConfigError(\n                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e\n            )\n\n        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []\n        validate_config(\n            _METRICS_FOR_DOMAINS_SCHEMA,\n            federation_metrics_domains,\n            (\"federation_metrics_domains\",),\n        )\n        self.federation_metrics_domains = set(federation_metrics_domains)\n\n    def generate_config_section(self, config_dir_path, server_name, **kwargs):\n        return \"\"\"\\\n        ## Federation ##\n\n        # Restrict federation to the following whitelist of domains.\n        # N.B. we recommend also firewalling your federation listener to limit\n        # inbound federation traffic as early as possible, rather than relying\n        # purely on this application-layer restriction.  If not specified, the\n        # default is to whitelist everything.\n        #\n        #federation_domain_whitelist:\n        #  - lon.example.com\n        #  - nyc.example.com\n        #  - syd.example.com\n\n        # Prevent federation requests from being sent to the following\n        # blacklist IP address CIDR ranges. If this option is not specified, or\n        # specified with an empty list, no ip range blacklist will be enforced.\n        #\n        # As of Synapse v1.4.0 this option also affects any outbound requests to identity\n        # servers provided by user input.\n        #\n        # (0.0.0.0 and :: are always blacklisted, whether or not they are explicitly\n        # listed here, since they correspond to unroutable addresses.)\n        #\n        federation_ip_range_blacklist:\n          - '127.0.0.0/8'\n          - '10.0.0.0/8'\n          - '172.16.0.0/12'\n          - '192.168.0.0/16'\n          - '100.64.0.0/10'\n          - '169.254.0.0/16'\n          - '::1/128'\n          - 'fe80::/64'\n          - 'fc00::/7'\n\n        # Report prometheus metrics on the age of PDUs being sent to and received from\n        # the following domains. This can be used to give an idea of \"delay\" on inbound\n        # and outbound federation, though be aware that any delay can be due to problems\n        # at either end or with the intermediate network.\n        #\n        # By default, no domains are monitored in this way.\n        #\n        #federation_metrics_domains:\n        #  - matrix.org\n        #  - example.com\n        \"\"\"\n\n\n_METRICS_FOR_DOMAINS_SCHEMA = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n", "patch": "@@ -36,22 +36,30 @@ def read_config(self, config, **kwargs):\n             for domain in federation_domain_whitelist:\n                 self.federation_domain_whitelist[domain] = True\n \n-        self.federation_ip_range_blacklist = config.get(\n-            \"federation_ip_range_blacklist\", []\n-        )\n+        ip_range_blacklist = config.get(\"ip_range_blacklist\", [])\n \n         # Attempt to create an IPSet from the given ranges\n         try:\n-            self.federation_ip_range_blacklist = IPSet(\n-                self.federation_ip_range_blacklist\n-            )\n-\n-            # Always blacklist 0.0.0.0, ::\n-            self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n+            self.ip_range_blacklist = IPSet(ip_range_blacklist)\n+        except Exception as e:\n+            raise ConfigError(\"Invalid range(s) provided in ip_range_blacklist: %s\" % e)\n+        # Always blacklist 0.0.0.0, ::\n+        self.ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n+\n+        # The federation_ip_range_blacklist is used for backwards-compatibility\n+        # and only applies to federation and identity servers. If it is not given,\n+        # default to ip_range_blacklist.\n+        federation_ip_range_blacklist = config.get(\n+            \"federation_ip_range_blacklist\", ip_range_blacklist\n+        )\n+        try:\n+            self.federation_ip_range_blacklist = IPSet(federation_ip_range_blacklist)\n         except Exception as e:\n             raise ConfigError(\n                 \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e\n             )\n+        # Always blacklist 0.0.0.0, ::\n+        self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n \n         federation_metrics_domains = config.get(\"federation_metrics_domains\") or []\n         validate_config(\n@@ -76,17 +84,19 @@ def generate_config_section(self, config_dir_path, server_name, **kwargs):\n         #  - nyc.example.com\n         #  - syd.example.com\n \n-        # Prevent federation requests from being sent to the following\n-        # blacklist IP address CIDR ranges. If this option is not specified, or\n-        # specified with an empty list, no ip range blacklist will be enforced.\n+        # Prevent outgoing requests from being sent to the following blacklisted IP address\n+        # CIDR ranges. If this option is not specified, or specified with an empty list,\n+        # no IP range blacklist will be enforced.\n         #\n-        # As of Synapse v1.4.0 this option also affects any outbound requests to identity\n-        # servers provided by user input.\n+        # The blacklist applies to the outbound requests for federation, identity servers,\n+        # push servers, and for checking key validitity for third-party invite events.\n         #\n         # (0.0.0.0 and :: are always blacklisted, whether or not they are explicitly\n         # listed here, since they correspond to unroutable addresses.)\n         #\n-        federation_ip_range_blacklist:\n+        # This option replaces federation_ip_range_blacklist in Synapse v1.24.0.\n+        #\n+        ip_range_blacklist:\n           - '127.0.0.0/8'\n           - '10.0.0.0/8'\n           - '172.16.0.0/12'", "file_path": "files/2021_2/18", "file_language": "py", "file_name": "synapse/config/federation.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Fcrypto%2Fkeyring.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2017, 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport urllib\nfrom collections import defaultdict\n\nimport attr\nfrom signedjson.key import (\n    decode_verify_key_bytes,\n    encode_verify_key_base64,\n    is_signing_algorithm_supported,\n)\nfrom signedjson.sign import (\n    SignatureVerifyException,\n    encode_canonical_json,\n    signature_ids,\n    verify_signed_json,\n)\nfrom unpaddedbase64 import decode_base64\n\nfrom twisted.internet import defer\n\nfrom synapse.api.errors import (\n    Codes,\n    HttpResponseException,\n    RequestSendFailed,\n    SynapseError,\n)\nfrom synapse.logging.context import (\n    PreserveLoggingContext,\n    make_deferred_yieldable,\n    preserve_fn,\n    run_in_background,\n)\nfrom synapse.storage.keys import FetchKeyResult\nfrom synapse.util import unwrapFirstError\nfrom synapse.util.async_helpers import yieldable_gather_results\nfrom synapse.util.metrics import Measure\nfrom synapse.util.retryutils import NotRetryingDestination\n\nlogger = logging.getLogger(__name__)\n\n\n@attr.s(slots=True, cmp=False)\nclass VerifyJsonRequest:\n    \"\"\"\n    A request to verify a JSON object.\n\n    Attributes:\n        server_name(str): The name of the server to verify against.\n\n        key_ids(set[str]): The set of key_ids to that could be used to verify the\n            JSON object\n\n        json_object(dict): The JSON object to verify.\n\n        minimum_valid_until_ts (int): time at which we require the signing key to\n            be valid. (0 implies we don't care)\n\n        key_ready (Deferred[str, str, nacl.signing.VerifyKey]):\n            A deferred (server_name, key_id, verify_key) tuple that resolves when\n            a verify key has been fetched. The deferreds' callbacks are run with no\n            logcontext.\n\n            If we are unable to find a key which satisfies the request, the deferred\n            errbacks with an M_UNAUTHORIZED SynapseError.\n    \"\"\"\n\n    server_name = attr.ib()\n    json_object = attr.ib()\n    minimum_valid_until_ts = attr.ib()\n    request_name = attr.ib()\n    key_ids = attr.ib(init=False)\n    key_ready = attr.ib(default=attr.Factory(defer.Deferred))\n\n    def __attrs_post_init__(self):\n        self.key_ids = signature_ids(self.json_object, self.server_name)\n\n\nclass KeyLookupError(ValueError):\n    pass\n\n\nclass Keyring:\n    def __init__(self, hs, key_fetchers=None):\n        self.clock = hs.get_clock()\n\n        if key_fetchers is None:\n            key_fetchers = (\n                StoreKeyFetcher(hs),\n                PerspectivesKeyFetcher(hs),\n                ServerKeyFetcher(hs),\n            )\n        self._key_fetchers = key_fetchers\n\n        # map from server name to Deferred. Has an entry for each server with\n        # an ongoing key download; the Deferred completes once the download\n        # completes.\n        #\n        # These are regular, logcontext-agnostic Deferreds.\n        self.key_downloads = {}\n\n    def verify_json_for_server(\n        self, server_name, json_object, validity_time, request_name\n    ):\n        \"\"\"Verify that a JSON object has been signed by a given server\n\n        Args:\n            server_name (str): name of the server which must have signed this object\n\n            json_object (dict): object to be checked\n\n            validity_time (int): timestamp at which we require the signing key to\n                be valid. (0 implies we don't care)\n\n            request_name (str): an identifier for this json object (eg, an event id)\n                for logging.\n\n        Returns:\n            Deferred[None]: completes if the the object was correctly signed, otherwise\n                errbacks with an error\n        \"\"\"\n        req = VerifyJsonRequest(server_name, json_object, validity_time, request_name)\n        requests = (req,)\n        return make_deferred_yieldable(self._verify_objects(requests)[0])\n\n    def verify_json_objects_for_server(self, server_and_json):\n        \"\"\"Bulk verifies signatures of json objects, bulk fetching keys as\n        necessary.\n\n        Args:\n            server_and_json (iterable[Tuple[str, dict, int, str]):\n                Iterable of (server_name, json_object, validity_time, request_name)\n                tuples.\n\n                validity_time is a timestamp at which the signing key must be\n                valid.\n\n                request_name is an identifier for this json object (eg, an event id)\n                for logging.\n\n        Returns:\n            List<Deferred[None]>: for each input triplet, a deferred indicating success\n                or failure to verify each json object's signature for the given\n                server_name. The deferreds run their callbacks in the sentinel\n                logcontext.\n        \"\"\"\n        return self._verify_objects(\n            VerifyJsonRequest(server_name, json_object, validity_time, request_name)\n            for server_name, json_object, validity_time, request_name in server_and_json\n        )\n\n    def _verify_objects(self, verify_requests):\n        \"\"\"Does the work of verify_json_[objects_]for_server\n\n\n        Args:\n            verify_requests (iterable[VerifyJsonRequest]):\n                Iterable of verification requests.\n\n        Returns:\n            List<Deferred[None]>: for each input item, a deferred indicating success\n                or failure to verify each json object's signature for the given\n                server_name. The deferreds run their callbacks in the sentinel\n                logcontext.\n        \"\"\"\n        # a list of VerifyJsonRequests which are awaiting a key lookup\n        key_lookups = []\n        handle = preserve_fn(_handle_key_deferred)\n\n        def process(verify_request):\n            \"\"\"Process an entry in the request list\n\n            Adds a key request to key_lookups, and returns a deferred which\n            will complete or fail (in the sentinel context) when verification completes.\n            \"\"\"\n            if not verify_request.key_ids:\n                return defer.fail(\n                    SynapseError(\n                        400,\n                        \"Not signed by %s\" % (verify_request.server_name,),\n                        Codes.UNAUTHORIZED,\n                    )\n                )\n\n            logger.debug(\n                \"Verifying %s for %s with key_ids %s, min_validity %i\",\n                verify_request.request_name,\n                verify_request.server_name,\n                verify_request.key_ids,\n                verify_request.minimum_valid_until_ts,\n            )\n\n            # add the key request to the queue, but don't start it off yet.\n            key_lookups.append(verify_request)\n\n            # now run _handle_key_deferred, which will wait for the key request\n            # to complete and then do the verification.\n            #\n            # We want _handle_key_request to log to the right context, so we\n            # wrap it with preserve_fn (aka run_in_background)\n            return handle(verify_request)\n\n        results = [process(r) for r in verify_requests]\n\n        if key_lookups:\n            run_in_background(self._start_key_lookups, key_lookups)\n\n        return results\n\n    async def _start_key_lookups(self, verify_requests):\n        \"\"\"Sets off the key fetches for each verify request\n\n        Once each fetch completes, verify_request.key_ready will be resolved.\n\n        Args:\n            verify_requests (List[VerifyJsonRequest]):\n        \"\"\"\n\n        try:\n            # map from server name to a set of outstanding request ids\n            server_to_request_ids = {}\n\n            for verify_request in verify_requests:\n                server_name = verify_request.server_name\n                request_id = id(verify_request)\n                server_to_request_ids.setdefault(server_name, set()).add(request_id)\n\n            # Wait for any previous lookups to complete before proceeding.\n            await self.wait_for_previous_lookups(server_to_request_ids.keys())\n\n            # take out a lock on each of the servers by sticking a Deferred in\n            # key_downloads\n            for server_name in server_to_request_ids.keys():\n                self.key_downloads[server_name] = defer.Deferred()\n                logger.debug(\"Got key lookup lock on %s\", server_name)\n\n            # When we've finished fetching all the keys for a given server_name,\n            # drop the lock by resolving the deferred in key_downloads.\n            def drop_server_lock(server_name):\n                d = self.key_downloads.pop(server_name)\n                d.callback(None)\n\n            def lookup_done(res, verify_request):\n                server_name = verify_request.server_name\n                server_requests = server_to_request_ids[server_name]\n                server_requests.remove(id(verify_request))\n\n                # if there are no more requests for this server, we can drop the lock.\n                if not server_requests:\n                    logger.debug(\"Releasing key lookup lock on %s\", server_name)\n                    drop_server_lock(server_name)\n\n                return res\n\n            for verify_request in verify_requests:\n                verify_request.key_ready.addBoth(lookup_done, verify_request)\n\n            # Actually start fetching keys.\n            self._get_server_verify_keys(verify_requests)\n        except Exception:\n            logger.exception(\"Error starting key lookups\")\n\n    async def wait_for_previous_lookups(self, server_names) -> None:\n        \"\"\"Waits for any previous key lookups for the given servers to finish.\n\n        Args:\n            server_names (Iterable[str]): list of servers which we want to look up\n\n        Returns:\n            Resolves once all key lookups for the given servers have\n                completed. Follows the synapse rules of logcontext preservation.\n        \"\"\"\n        loop_count = 1\n        while True:\n            wait_on = [\n                (server_name, self.key_downloads[server_name])\n                for server_name in server_names\n                if server_name in self.key_downloads\n            ]\n            if not wait_on:\n                break\n            logger.info(\n                \"Waiting for existing lookups for %s to complete [loop %i]\",\n                [w[0] for w in wait_on],\n                loop_count,\n            )\n            with PreserveLoggingContext():\n                await defer.DeferredList((w[1] for w in wait_on))\n\n            loop_count += 1\n\n    def _get_server_verify_keys(self, verify_requests):\n        \"\"\"Tries to find at least one key for each verify request\n\n        For each verify_request, verify_request.key_ready is called back with\n        params (server_name, key_id, VerifyKey) if a key is found, or errbacked\n        with a SynapseError if none of the keys are found.\n\n        Args:\n            verify_requests (list[VerifyJsonRequest]): list of verify requests\n        \"\"\"\n\n        remaining_requests = {rq for rq in verify_requests if not rq.key_ready.called}\n\n        async def do_iterations():\n            try:\n                with Measure(self.clock, \"get_server_verify_keys\"):\n                    for f in self._key_fetchers:\n                        if not remaining_requests:\n                            return\n                        await self._attempt_key_fetches_with_fetcher(\n                            f, remaining_requests\n                        )\n\n                    # look for any requests which weren't satisfied\n                    while remaining_requests:\n                        verify_request = remaining_requests.pop()\n                        rq_str = (\n                            \"VerifyJsonRequest(server=%s, key_ids=%s, min_valid=%i)\"\n                            % (\n                                verify_request.server_name,\n                                verify_request.key_ids,\n                                verify_request.minimum_valid_until_ts,\n                            )\n                        )\n\n                        # If we run the errback immediately, it may cancel our\n                        # loggingcontext while we are still in it, so instead we\n                        # schedule it for the next time round the reactor.\n                        #\n                        # (this also ensures that we don't get a stack overflow if we\n                        # has a massive queue of lookups waiting for this server).\n                        self.clock.call_later(\n                            0,\n                            verify_request.key_ready.errback,\n                            SynapseError(\n                                401,\n                                \"Failed to find any key to satisfy %s\" % (rq_str,),\n                                Codes.UNAUTHORIZED,\n                            ),\n                        )\n            except Exception as err:\n                # we don't really expect to get here, because any errors should already\n                # have been caught and logged. But if we do, let's log the error and make\n                # sure that all of the deferreds are resolved.\n                logger.error(\"Unexpected error in _get_server_verify_keys: %s\", err)\n                with PreserveLoggingContext():\n                    for verify_request in remaining_requests:\n                        if not verify_request.key_ready.called:\n                            verify_request.key_ready.errback(err)\n\n        run_in_background(do_iterations)\n\n    async def _attempt_key_fetches_with_fetcher(self, fetcher, remaining_requests):\n        \"\"\"Use a key fetcher to attempt to satisfy some key requests\n\n        Args:\n            fetcher (KeyFetcher): fetcher to use to fetch the keys\n            remaining_requests (set[VerifyJsonRequest]): outstanding key requests.\n                Any successfully-completed requests will be removed from the list.\n        \"\"\"\n        # dict[str, dict[str, int]]: keys to fetch.\n        # server_name -> key_id -> min_valid_ts\n        missing_keys = defaultdict(dict)\n\n        for verify_request in remaining_requests:\n            # any completed requests should already have been removed\n            assert not verify_request.key_ready.called\n            keys_for_server = missing_keys[verify_request.server_name]\n\n            for key_id in verify_request.key_ids:\n                # If we have several requests for the same key, then we only need to\n                # request that key once, but we should do so with the greatest\n                # min_valid_until_ts of the requests, so that we can satisfy all of\n                # the requests.\n                keys_for_server[key_id] = max(\n                    keys_for_server.get(key_id, -1),\n                    verify_request.minimum_valid_until_ts,\n                )\n\n        results = await fetcher.get_keys(missing_keys)\n\n        completed = []\n        for verify_request in remaining_requests:\n            server_name = verify_request.server_name\n\n            # see if any of the keys we got this time are sufficient to\n            # complete this VerifyJsonRequest.\n            result_keys = results.get(server_name, {})\n            for key_id in verify_request.key_ids:\n                fetch_key_result = result_keys.get(key_id)\n                if not fetch_key_result:\n                    # we didn't get a result for this key\n                    continue\n\n                if (\n                    fetch_key_result.valid_until_ts\n                    < verify_request.minimum_valid_until_ts\n                ):\n                    # key was not valid at this point\n                    continue\n\n                # we have a valid key for this request. If we run the callback\n                # immediately, it may cancel our loggingcontext while we are still in\n                # it, so instead we schedule it for the next time round the reactor.\n                #\n                # (this also ensures that we don't get a stack overflow if we had\n                # a massive queue of lookups waiting for this server).\n                logger.debug(\n                    \"Found key %s:%s for %s\",\n                    server_name,\n                    key_id,\n                    verify_request.request_name,\n                )\n                self.clock.call_later(\n                    0,\n                    verify_request.key_ready.callback,\n                    (server_name, key_id, fetch_key_result.verify_key),\n                )\n                completed.append(verify_request)\n                break\n\n        remaining_requests.difference_update(completed)\n\n\nclass KeyFetcher:\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, dict[str, int]]):\n                the keys to be fetched. server_name -> key_id -> min_valid_ts\n\n        Returns:\n            Deferred[dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]]:\n                map from server_name -> key_id -> FetchKeyResult\n        \"\"\"\n        raise NotImplementedError\n\n\nclass StoreKeyFetcher(KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from our data store\"\"\"\n\n    def __init__(self, hs):\n        self.store = hs.get_datastore()\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"see KeyFetcher.get_keys\"\"\"\n\n        keys_to_fetch = (\n            (server_name, key_id)\n            for server_name, keys_for_server in keys_to_fetch.items()\n            for key_id in keys_for_server.keys()\n        )\n\n        res = await self.store.get_server_verify_keys(keys_to_fetch)\n        keys = {}\n        for (server_name, key_id), key in res.items():\n            keys.setdefault(server_name, {})[key_id] = key\n        return keys\n\n\nclass BaseV2KeyFetcher:\n    def __init__(self, hs):\n        self.store = hs.get_datastore()\n        self.config = hs.get_config()\n\n    async def process_v2_response(self, from_server, response_json, time_added_ms):\n        \"\"\"Parse a 'Server Keys' structure from the result of a /key request\n\n        This is used to parse either the entirety of the response from\n        GET /_matrix/key/v2/server, or a single entry from the list returned by\n        POST /_matrix/key/v2/query.\n\n        Checks that each signature in the response that claims to come from the origin\n        server is valid, and that there is at least one such signature.\n\n        Stores the json in server_keys_json so that it can be used for future responses\n        to /_matrix/key/v2/query.\n\n        Args:\n            from_server (str): the name of the server producing this result: either\n                the origin server for a /_matrix/key/v2/server request, or the notary\n                for a /_matrix/key/v2/query.\n\n            response_json (dict): the json-decoded Server Keys response object\n\n            time_added_ms (int): the timestamp to record in server_keys_json\n\n        Returns:\n            Deferred[dict[str, FetchKeyResult]]: map from key_id to result object\n        \"\"\"\n        ts_valid_until_ms = response_json[\"valid_until_ts\"]\n\n        # start by extracting the keys from the response, since they may be required\n        # to validate the signature on the response.\n        verify_keys = {}\n        for key_id, key_data in response_json[\"verify_keys\"].items():\n            if is_signing_algorithm_supported(key_id):\n                key_base64 = key_data[\"key\"]\n                key_bytes = decode_base64(key_base64)\n                verify_key = decode_verify_key_bytes(key_id, key_bytes)\n                verify_keys[key_id] = FetchKeyResult(\n                    verify_key=verify_key, valid_until_ts=ts_valid_until_ms\n                )\n\n        server_name = response_json[\"server_name\"]\n        verified = False\n        for key_id in response_json[\"signatures\"].get(server_name, {}):\n            key = verify_keys.get(key_id)\n            if not key:\n                # the key may not be present in verify_keys if:\n                #  * we got the key from the notary server, and:\n                #  * the key belongs to the notary server, and:\n                #  * the notary server is using a different key to sign notary\n                #    responses.\n                continue\n\n            verify_signed_json(response_json, server_name, key.verify_key)\n            verified = True\n            break\n\n        if not verified:\n            raise KeyLookupError(\n                \"Key response for %s is not signed by the origin server\"\n                % (server_name,)\n            )\n\n        for key_id, key_data in response_json[\"old_verify_keys\"].items():\n            if is_signing_algorithm_supported(key_id):\n                key_base64 = key_data[\"key\"]\n                key_bytes = decode_base64(key_base64)\n                verify_key = decode_verify_key_bytes(key_id, key_bytes)\n                verify_keys[key_id] = FetchKeyResult(\n                    verify_key=verify_key, valid_until_ts=key_data[\"expired_ts\"]\n                )\n\n        key_json_bytes = encode_canonical_json(response_json)\n\n        await make_deferred_yieldable(\n            defer.gatherResults(\n                [\n                    run_in_background(\n                        self.store.store_server_keys_json,\n                        server_name=server_name,\n                        key_id=key_id,\n                        from_server=from_server,\n                        ts_now_ms=time_added_ms,\n                        ts_expires_ms=ts_valid_until_ms,\n                        key_json_bytes=key_json_bytes,\n                    )\n                    for key_id in verify_keys\n                ],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        return verify_keys\n\n\nclass PerspectivesKeyFetcher(BaseV2KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from the \"perspectives\" servers\"\"\"\n\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_federation_http_client()\n        self.key_servers = self.config.key_servers\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"see KeyFetcher.get_keys\"\"\"\n\n        async def get_key(key_server):\n            try:\n                result = await self.get_server_verify_key_v2_indirect(\n                    keys_to_fetch, key_server\n                )\n                return result\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Key lookup failed from %r: %s\", key_server.server_name, e\n                )\n            except Exception as e:\n                logger.exception(\n                    \"Unable to get key from %r: %s %s\",\n                    key_server.server_name,\n                    type(e).__name__,\n                    str(e),\n                )\n\n            return {}\n\n        results = await make_deferred_yieldable(\n            defer.gatherResults(\n                [run_in_background(get_key, server) for server in self.key_servers],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        union_of_keys = {}\n        for result in results:\n            for server_name, keys in result.items():\n                union_of_keys.setdefault(server_name, {}).update(keys)\n\n        return union_of_keys\n\n    async def get_server_verify_key_v2_indirect(self, keys_to_fetch, key_server):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, dict[str, int]]):\n                the keys to be fetched. server_name -> key_id -> min_valid_ts\n\n            key_server (synapse.config.key.TrustedKeyServer): notary server to query for\n                the keys\n\n        Returns:\n            dict[str, dict[str, synapse.storage.keys.FetchKeyResult]]: map\n                from server_name -> key_id -> FetchKeyResult\n\n        Raises:\n            KeyLookupError if there was an error processing the entire response from\n                the server\n        \"\"\"\n        perspective_name = key_server.server_name\n        logger.info(\n            \"Requesting keys %s from notary server %s\",\n            keys_to_fetch.items(),\n            perspective_name,\n        )\n\n        try:\n            query_response = await self.client.post_json(\n                destination=perspective_name,\n                path=\"/_matrix/key/v2/query\",\n                data={\n                    \"server_keys\": {\n                        server_name: {\n                            key_id: {\"minimum_valid_until_ts\": min_valid_ts}\n                            for key_id, min_valid_ts in server_keys.items()\n                        }\n                        for server_name, server_keys in keys_to_fetch.items()\n                    }\n                },\n            )\n        except (NotRetryingDestination, RequestSendFailed) as e:\n            # these both have str() representations which we can't really improve upon\n            raise KeyLookupError(str(e))\n        except HttpResponseException as e:\n            raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))\n\n        keys = {}\n        added_keys = []\n\n        time_now_ms = self.clock.time_msec()\n\n        for response in query_response[\"server_keys\"]:\n            # do this first, so that we can give useful errors thereafter\n            server_name = response.get(\"server_name\")\n            if not isinstance(server_name, str):\n                raise KeyLookupError(\n                    \"Malformed response from key notary server %s: invalid server_name\"\n                    % (perspective_name,)\n                )\n\n            try:\n                self._validate_perspectives_response(key_server, response)\n\n                processed_response = await self.process_v2_response(\n                    perspective_name, response, time_added_ms=time_now_ms\n                )\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Error processing response from key notary server %s for origin \"\n                    \"server %s: %s\",\n                    perspective_name,\n                    server_name,\n                    e,\n                )\n                # we continue to process the rest of the response\n                continue\n\n            added_keys.extend(\n                (server_name, key_id, key) for key_id, key in processed_response.items()\n            )\n            keys.setdefault(server_name, {}).update(processed_response)\n\n        await self.store.store_server_verify_keys(\n            perspective_name, time_now_ms, added_keys\n        )\n\n        return keys\n\n    def _validate_perspectives_response(self, key_server, response):\n        \"\"\"Optionally check the signature on the result of a /key/query request\n\n        Args:\n            key_server (synapse.config.key.TrustedKeyServer): the notary server that\n                produced this result\n\n            response (dict): the json-decoded Server Keys response object\n        \"\"\"\n        perspective_name = key_server.server_name\n        perspective_keys = key_server.verify_keys\n\n        if perspective_keys is None:\n            # signature checking is disabled on this server\n            return\n\n        if (\n            \"signatures\" not in response\n            or perspective_name not in response[\"signatures\"]\n        ):\n            raise KeyLookupError(\"Response not signed by the notary server\")\n\n        verified = False\n        for key_id in response[\"signatures\"][perspective_name]:\n            if key_id in perspective_keys:\n                verify_signed_json(response, perspective_name, perspective_keys[key_id])\n                verified = True\n\n        if not verified:\n            raise KeyLookupError(\n                \"Response not signed with a known key: signed with: %r, known keys: %r\"\n                % (\n                    list(response[\"signatures\"][perspective_name].keys()),\n                    list(perspective_keys.keys()),\n                )\n            )\n\n\nclass ServerKeyFetcher(BaseV2KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from the origin servers\"\"\"\n\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_federation_http_client()\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, iterable[str]]):\n                the keys to be fetched. server_name -> key_ids\n\n        Returns:\n            dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]:\n                map from server_name -> key_id -> FetchKeyResult\n        \"\"\"\n\n        results = {}\n\n        async def get_key(key_to_fetch_item):\n            server_name, key_ids = key_to_fetch_item\n            try:\n                keys = await self.get_server_verify_key_v2_direct(server_name, key_ids)\n                results[server_name] = keys\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Error looking up keys %s from %s: %s\", key_ids, server_name, e\n                )\n            except Exception:\n                logger.exception(\"Error getting keys %s from %s\", key_ids, server_name)\n\n        await yieldable_gather_results(get_key, keys_to_fetch.items())\n        return results\n\n    async def get_server_verify_key_v2_direct(self, server_name, key_ids):\n        \"\"\"\n\n        Args:\n            server_name (str):\n            key_ids (iterable[str]):\n\n        Returns:\n            dict[str, FetchKeyResult]: map from key ID to lookup result\n\n        Raises:\n            KeyLookupError if there was a problem making the lookup\n        \"\"\"\n        keys = {}  # type: dict[str, FetchKeyResult]\n\n        for requested_key_id in key_ids:\n            # we may have found this key as a side-effect of asking for another.\n            if requested_key_id in keys:\n                continue\n\n            time_now_ms = self.clock.time_msec()\n            try:\n                response = await self.client.get_json(\n                    destination=server_name,\n                    path=\"/_matrix/key/v2/server/\"\n                    + urllib.parse.quote(requested_key_id),\n                    ignore_backoff=True,\n                    # we only give the remote server 10s to respond. It should be an\n                    # easy request to handle, so if it doesn't reply within 10s, it's\n                    # probably not going to.\n                    #\n                    # Furthermore, when we are acting as a notary server, we cannot\n                    # wait all day for all of the origin servers, as the requesting\n                    # server will otherwise time out before we can respond.\n                    #\n                    # (Note that get_json may make 4 attempts, so this can still take\n                    # almost 45 seconds to fetch the headers, plus up to another 60s to\n                    # read the response).\n                    timeout=10000,\n                )\n            except (NotRetryingDestination, RequestSendFailed) as e:\n                # these both have str() representations which we can't really improve\n                # upon\n                raise KeyLookupError(str(e))\n            except HttpResponseException as e:\n                raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))\n\n            if response[\"server_name\"] != server_name:\n                raise KeyLookupError(\n                    \"Expected a response for server %r not %r\"\n                    % (server_name, response[\"server_name\"])\n                )\n\n            response_keys = await self.process_v2_response(\n                from_server=server_name,\n                response_json=response,\n                time_added_ms=time_now_ms,\n            )\n            await self.store.store_server_verify_keys(\n                server_name,\n                time_now_ms,\n                ((server_name, key_id, key) for key_id, key in response_keys.items()),\n            )\n            keys.update(response_keys)\n\n        return keys\n\n\nasync def _handle_key_deferred(verify_request) -> None:\n    \"\"\"Waits for the key to become available, and then performs a verification\n\n    Args:\n        verify_request (VerifyJsonRequest):\n\n    Raises:\n        SynapseError if there was a problem performing the verification\n    \"\"\"\n    server_name = verify_request.server_name\n    with PreserveLoggingContext():\n        _, key_id, verify_key = await verify_request.key_ready\n\n    json_object = verify_request.json_object\n\n    try:\n        verify_signed_json(json_object, server_name, verify_key)\n    except SignatureVerifyException as e:\n        logger.debug(\n            \"Error verifying signature for %s:%s:%s with key %s: %s\",\n            server_name,\n            verify_key.alg,\n            verify_key.version,\n            encode_verify_key_base64(verify_key),\n            str(e),\n        )\n        raise SynapseError(\n            401,\n            \"Invalid signature for server %s with key %s:%s: %s\"\n            % (server_name, verify_key.alg, verify_key.version, str(e)),\n            Codes.UNAUTHORIZED,\n        )\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2017, 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport urllib\nfrom collections import defaultdict\n\nimport attr\nfrom signedjson.key import (\n    decode_verify_key_bytes,\n    encode_verify_key_base64,\n    is_signing_algorithm_supported,\n)\nfrom signedjson.sign import (\n    SignatureVerifyException,\n    encode_canonical_json,\n    signature_ids,\n    verify_signed_json,\n)\nfrom unpaddedbase64 import decode_base64\n\nfrom twisted.internet import defer\n\nfrom synapse.api.errors import (\n    Codes,\n    HttpResponseException,\n    RequestSendFailed,\n    SynapseError,\n)\nfrom synapse.logging.context import (\n    PreserveLoggingContext,\n    make_deferred_yieldable,\n    preserve_fn,\n    run_in_background,\n)\nfrom synapse.storage.keys import FetchKeyResult\nfrom synapse.util import unwrapFirstError\nfrom synapse.util.async_helpers import yieldable_gather_results\nfrom synapse.util.metrics import Measure\nfrom synapse.util.retryutils import NotRetryingDestination\n\nlogger = logging.getLogger(__name__)\n\n\n@attr.s(slots=True, cmp=False)\nclass VerifyJsonRequest:\n    \"\"\"\n    A request to verify a JSON object.\n\n    Attributes:\n        server_name(str): The name of the server to verify against.\n\n        key_ids(set[str]): The set of key_ids to that could be used to verify the\n            JSON object\n\n        json_object(dict): The JSON object to verify.\n\n        minimum_valid_until_ts (int): time at which we require the signing key to\n            be valid. (0 implies we don't care)\n\n        key_ready (Deferred[str, str, nacl.signing.VerifyKey]):\n            A deferred (server_name, key_id, verify_key) tuple that resolves when\n            a verify key has been fetched. The deferreds' callbacks are run with no\n            logcontext.\n\n            If we are unable to find a key which satisfies the request, the deferred\n            errbacks with an M_UNAUTHORIZED SynapseError.\n    \"\"\"\n\n    server_name = attr.ib()\n    json_object = attr.ib()\n    minimum_valid_until_ts = attr.ib()\n    request_name = attr.ib()\n    key_ids = attr.ib(init=False)\n    key_ready = attr.ib(default=attr.Factory(defer.Deferred))\n\n    def __attrs_post_init__(self):\n        self.key_ids = signature_ids(self.json_object, self.server_name)\n\n\nclass KeyLookupError(ValueError):\n    pass\n\n\nclass Keyring:\n    def __init__(self, hs, key_fetchers=None):\n        self.clock = hs.get_clock()\n\n        if key_fetchers is None:\n            key_fetchers = (\n                StoreKeyFetcher(hs),\n                PerspectivesKeyFetcher(hs),\n                ServerKeyFetcher(hs),\n            )\n        self._key_fetchers = key_fetchers\n\n        # map from server name to Deferred. Has an entry for each server with\n        # an ongoing key download; the Deferred completes once the download\n        # completes.\n        #\n        # These are regular, logcontext-agnostic Deferreds.\n        self.key_downloads = {}\n\n    def verify_json_for_server(\n        self, server_name, json_object, validity_time, request_name\n    ):\n        \"\"\"Verify that a JSON object has been signed by a given server\n\n        Args:\n            server_name (str): name of the server which must have signed this object\n\n            json_object (dict): object to be checked\n\n            validity_time (int): timestamp at which we require the signing key to\n                be valid. (0 implies we don't care)\n\n            request_name (str): an identifier for this json object (eg, an event id)\n                for logging.\n\n        Returns:\n            Deferred[None]: completes if the the object was correctly signed, otherwise\n                errbacks with an error\n        \"\"\"\n        req = VerifyJsonRequest(server_name, json_object, validity_time, request_name)\n        requests = (req,)\n        return make_deferred_yieldable(self._verify_objects(requests)[0])\n\n    def verify_json_objects_for_server(self, server_and_json):\n        \"\"\"Bulk verifies signatures of json objects, bulk fetching keys as\n        necessary.\n\n        Args:\n            server_and_json (iterable[Tuple[str, dict, int, str]):\n                Iterable of (server_name, json_object, validity_time, request_name)\n                tuples.\n\n                validity_time is a timestamp at which the signing key must be\n                valid.\n\n                request_name is an identifier for this json object (eg, an event id)\n                for logging.\n\n        Returns:\n            List<Deferred[None]>: for each input triplet, a deferred indicating success\n                or failure to verify each json object's signature for the given\n                server_name. The deferreds run their callbacks in the sentinel\n                logcontext.\n        \"\"\"\n        return self._verify_objects(\n            VerifyJsonRequest(server_name, json_object, validity_time, request_name)\n            for server_name, json_object, validity_time, request_name in server_and_json\n        )\n\n    def _verify_objects(self, verify_requests):\n        \"\"\"Does the work of verify_json_[objects_]for_server\n\n\n        Args:\n            verify_requests (iterable[VerifyJsonRequest]):\n                Iterable of verification requests.\n\n        Returns:\n            List<Deferred[None]>: for each input item, a deferred indicating success\n                or failure to verify each json object's signature for the given\n                server_name. The deferreds run their callbacks in the sentinel\n                logcontext.\n        \"\"\"\n        # a list of VerifyJsonRequests which are awaiting a key lookup\n        key_lookups = []\n        handle = preserve_fn(_handle_key_deferred)\n\n        def process(verify_request):\n            \"\"\"Process an entry in the request list\n\n            Adds a key request to key_lookups, and returns a deferred which\n            will complete or fail (in the sentinel context) when verification completes.\n            \"\"\"\n            if not verify_request.key_ids:\n                return defer.fail(\n                    SynapseError(\n                        400,\n                        \"Not signed by %s\" % (verify_request.server_name,),\n                        Codes.UNAUTHORIZED,\n                    )\n                )\n\n            logger.debug(\n                \"Verifying %s for %s with key_ids %s, min_validity %i\",\n                verify_request.request_name,\n                verify_request.server_name,\n                verify_request.key_ids,\n                verify_request.minimum_valid_until_ts,\n            )\n\n            # add the key request to the queue, but don't start it off yet.\n            key_lookups.append(verify_request)\n\n            # now run _handle_key_deferred, which will wait for the key request\n            # to complete and then do the verification.\n            #\n            # We want _handle_key_request to log to the right context, so we\n            # wrap it with preserve_fn (aka run_in_background)\n            return handle(verify_request)\n\n        results = [process(r) for r in verify_requests]\n\n        if key_lookups:\n            run_in_background(self._start_key_lookups, key_lookups)\n\n        return results\n\n    async def _start_key_lookups(self, verify_requests):\n        \"\"\"Sets off the key fetches for each verify request\n\n        Once each fetch completes, verify_request.key_ready will be resolved.\n\n        Args:\n            verify_requests (List[VerifyJsonRequest]):\n        \"\"\"\n\n        try:\n            # map from server name to a set of outstanding request ids\n            server_to_request_ids = {}\n\n            for verify_request in verify_requests:\n                server_name = verify_request.server_name\n                request_id = id(verify_request)\n                server_to_request_ids.setdefault(server_name, set()).add(request_id)\n\n            # Wait for any previous lookups to complete before proceeding.\n            await self.wait_for_previous_lookups(server_to_request_ids.keys())\n\n            # take out a lock on each of the servers by sticking a Deferred in\n            # key_downloads\n            for server_name in server_to_request_ids.keys():\n                self.key_downloads[server_name] = defer.Deferred()\n                logger.debug(\"Got key lookup lock on %s\", server_name)\n\n            # When we've finished fetching all the keys for a given server_name,\n            # drop the lock by resolving the deferred in key_downloads.\n            def drop_server_lock(server_name):\n                d = self.key_downloads.pop(server_name)\n                d.callback(None)\n\n            def lookup_done(res, verify_request):\n                server_name = verify_request.server_name\n                server_requests = server_to_request_ids[server_name]\n                server_requests.remove(id(verify_request))\n\n                # if there are no more requests for this server, we can drop the lock.\n                if not server_requests:\n                    logger.debug(\"Releasing key lookup lock on %s\", server_name)\n                    drop_server_lock(server_name)\n\n                return res\n\n            for verify_request in verify_requests:\n                verify_request.key_ready.addBoth(lookup_done, verify_request)\n\n            # Actually start fetching keys.\n            self._get_server_verify_keys(verify_requests)\n        except Exception:\n            logger.exception(\"Error starting key lookups\")\n\n    async def wait_for_previous_lookups(self, server_names) -> None:\n        \"\"\"Waits for any previous key lookups for the given servers to finish.\n\n        Args:\n            server_names (Iterable[str]): list of servers which we want to look up\n\n        Returns:\n            Resolves once all key lookups for the given servers have\n                completed. Follows the synapse rules of logcontext preservation.\n        \"\"\"\n        loop_count = 1\n        while True:\n            wait_on = [\n                (server_name, self.key_downloads[server_name])\n                for server_name in server_names\n                if server_name in self.key_downloads\n            ]\n            if not wait_on:\n                break\n            logger.info(\n                \"Waiting for existing lookups for %s to complete [loop %i]\",\n                [w[0] for w in wait_on],\n                loop_count,\n            )\n            with PreserveLoggingContext():\n                await defer.DeferredList((w[1] for w in wait_on))\n\n            loop_count += 1\n\n    def _get_server_verify_keys(self, verify_requests):\n        \"\"\"Tries to find at least one key for each verify request\n\n        For each verify_request, verify_request.key_ready is called back with\n        params (server_name, key_id, VerifyKey) if a key is found, or errbacked\n        with a SynapseError if none of the keys are found.\n\n        Args:\n            verify_requests (list[VerifyJsonRequest]): list of verify requests\n        \"\"\"\n\n        remaining_requests = {rq for rq in verify_requests if not rq.key_ready.called}\n\n        async def do_iterations():\n            try:\n                with Measure(self.clock, \"get_server_verify_keys\"):\n                    for f in self._key_fetchers:\n                        if not remaining_requests:\n                            return\n                        await self._attempt_key_fetches_with_fetcher(\n                            f, remaining_requests\n                        )\n\n                    # look for any requests which weren't satisfied\n                    while remaining_requests:\n                        verify_request = remaining_requests.pop()\n                        rq_str = (\n                            \"VerifyJsonRequest(server=%s, key_ids=%s, min_valid=%i)\"\n                            % (\n                                verify_request.server_name,\n                                verify_request.key_ids,\n                                verify_request.minimum_valid_until_ts,\n                            )\n                        )\n\n                        # If we run the errback immediately, it may cancel our\n                        # loggingcontext while we are still in it, so instead we\n                        # schedule it for the next time round the reactor.\n                        #\n                        # (this also ensures that we don't get a stack overflow if we\n                        # has a massive queue of lookups waiting for this server).\n                        self.clock.call_later(\n                            0,\n                            verify_request.key_ready.errback,\n                            SynapseError(\n                                401,\n                                \"Failed to find any key to satisfy %s\" % (rq_str,),\n                                Codes.UNAUTHORIZED,\n                            ),\n                        )\n            except Exception as err:\n                # we don't really expect to get here, because any errors should already\n                # have been caught and logged. But if we do, let's log the error and make\n                # sure that all of the deferreds are resolved.\n                logger.error(\"Unexpected error in _get_server_verify_keys: %s\", err)\n                with PreserveLoggingContext():\n                    for verify_request in remaining_requests:\n                        if not verify_request.key_ready.called:\n                            verify_request.key_ready.errback(err)\n\n        run_in_background(do_iterations)\n\n    async def _attempt_key_fetches_with_fetcher(self, fetcher, remaining_requests):\n        \"\"\"Use a key fetcher to attempt to satisfy some key requests\n\n        Args:\n            fetcher (KeyFetcher): fetcher to use to fetch the keys\n            remaining_requests (set[VerifyJsonRequest]): outstanding key requests.\n                Any successfully-completed requests will be removed from the list.\n        \"\"\"\n        # dict[str, dict[str, int]]: keys to fetch.\n        # server_name -> key_id -> min_valid_ts\n        missing_keys = defaultdict(dict)\n\n        for verify_request in remaining_requests:\n            # any completed requests should already have been removed\n            assert not verify_request.key_ready.called\n            keys_for_server = missing_keys[verify_request.server_name]\n\n            for key_id in verify_request.key_ids:\n                # If we have several requests for the same key, then we only need to\n                # request that key once, but we should do so with the greatest\n                # min_valid_until_ts of the requests, so that we can satisfy all of\n                # the requests.\n                keys_for_server[key_id] = max(\n                    keys_for_server.get(key_id, -1),\n                    verify_request.minimum_valid_until_ts,\n                )\n\n        results = await fetcher.get_keys(missing_keys)\n\n        completed = []\n        for verify_request in remaining_requests:\n            server_name = verify_request.server_name\n\n            # see if any of the keys we got this time are sufficient to\n            # complete this VerifyJsonRequest.\n            result_keys = results.get(server_name, {})\n            for key_id in verify_request.key_ids:\n                fetch_key_result = result_keys.get(key_id)\n                if not fetch_key_result:\n                    # we didn't get a result for this key\n                    continue\n\n                if (\n                    fetch_key_result.valid_until_ts\n                    < verify_request.minimum_valid_until_ts\n                ):\n                    # key was not valid at this point\n                    continue\n\n                # we have a valid key for this request. If we run the callback\n                # immediately, it may cancel our loggingcontext while we are still in\n                # it, so instead we schedule it for the next time round the reactor.\n                #\n                # (this also ensures that we don't get a stack overflow if we had\n                # a massive queue of lookups waiting for this server).\n                logger.debug(\n                    \"Found key %s:%s for %s\",\n                    server_name,\n                    key_id,\n                    verify_request.request_name,\n                )\n                self.clock.call_later(\n                    0,\n                    verify_request.key_ready.callback,\n                    (server_name, key_id, fetch_key_result.verify_key),\n                )\n                completed.append(verify_request)\n                break\n\n        remaining_requests.difference_update(completed)\n\n\nclass KeyFetcher:\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, dict[str, int]]):\n                the keys to be fetched. server_name -> key_id -> min_valid_ts\n\n        Returns:\n            Deferred[dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]]:\n                map from server_name -> key_id -> FetchKeyResult\n        \"\"\"\n        raise NotImplementedError\n\n\nclass StoreKeyFetcher(KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from our data store\"\"\"\n\n    def __init__(self, hs):\n        self.store = hs.get_datastore()\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"see KeyFetcher.get_keys\"\"\"\n\n        keys_to_fetch = (\n            (server_name, key_id)\n            for server_name, keys_for_server in keys_to_fetch.items()\n            for key_id in keys_for_server.keys()\n        )\n\n        res = await self.store.get_server_verify_keys(keys_to_fetch)\n        keys = {}\n        for (server_name, key_id), key in res.items():\n            keys.setdefault(server_name, {})[key_id] = key\n        return keys\n\n\nclass BaseV2KeyFetcher:\n    def __init__(self, hs):\n        self.store = hs.get_datastore()\n        self.config = hs.get_config()\n\n    async def process_v2_response(self, from_server, response_json, time_added_ms):\n        \"\"\"Parse a 'Server Keys' structure from the result of a /key request\n\n        This is used to parse either the entirety of the response from\n        GET /_matrix/key/v2/server, or a single entry from the list returned by\n        POST /_matrix/key/v2/query.\n\n        Checks that each signature in the response that claims to come from the origin\n        server is valid, and that there is at least one such signature.\n\n        Stores the json in server_keys_json so that it can be used for future responses\n        to /_matrix/key/v2/query.\n\n        Args:\n            from_server (str): the name of the server producing this result: either\n                the origin server for a /_matrix/key/v2/server request, or the notary\n                for a /_matrix/key/v2/query.\n\n            response_json (dict): the json-decoded Server Keys response object\n\n            time_added_ms (int): the timestamp to record in server_keys_json\n\n        Returns:\n            Deferred[dict[str, FetchKeyResult]]: map from key_id to result object\n        \"\"\"\n        ts_valid_until_ms = response_json[\"valid_until_ts\"]\n\n        # start by extracting the keys from the response, since they may be required\n        # to validate the signature on the response.\n        verify_keys = {}\n        for key_id, key_data in response_json[\"verify_keys\"].items():\n            if is_signing_algorithm_supported(key_id):\n                key_base64 = key_data[\"key\"]\n                key_bytes = decode_base64(key_base64)\n                verify_key = decode_verify_key_bytes(key_id, key_bytes)\n                verify_keys[key_id] = FetchKeyResult(\n                    verify_key=verify_key, valid_until_ts=ts_valid_until_ms\n                )\n\n        server_name = response_json[\"server_name\"]\n        verified = False\n        for key_id in response_json[\"signatures\"].get(server_name, {}):\n            key = verify_keys.get(key_id)\n            if not key:\n                # the key may not be present in verify_keys if:\n                #  * we got the key from the notary server, and:\n                #  * the key belongs to the notary server, and:\n                #  * the notary server is using a different key to sign notary\n                #    responses.\n                continue\n\n            verify_signed_json(response_json, server_name, key.verify_key)\n            verified = True\n            break\n\n        if not verified:\n            raise KeyLookupError(\n                \"Key response for %s is not signed by the origin server\"\n                % (server_name,)\n            )\n\n        for key_id, key_data in response_json[\"old_verify_keys\"].items():\n            if is_signing_algorithm_supported(key_id):\n                key_base64 = key_data[\"key\"]\n                key_bytes = decode_base64(key_base64)\n                verify_key = decode_verify_key_bytes(key_id, key_bytes)\n                verify_keys[key_id] = FetchKeyResult(\n                    verify_key=verify_key, valid_until_ts=key_data[\"expired_ts\"]\n                )\n\n        key_json_bytes = encode_canonical_json(response_json)\n\n        await make_deferred_yieldable(\n            defer.gatherResults(\n                [\n                    run_in_background(\n                        self.store.store_server_keys_json,\n                        server_name=server_name,\n                        key_id=key_id,\n                        from_server=from_server,\n                        ts_now_ms=time_added_ms,\n                        ts_expires_ms=ts_valid_until_ms,\n                        key_json_bytes=key_json_bytes,\n                    )\n                    for key_id in verify_keys\n                ],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        return verify_keys\n\n\nclass PerspectivesKeyFetcher(BaseV2KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from the \"perspectives\" servers\"\"\"\n\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_http_client()\n        self.key_servers = self.config.key_servers\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"see KeyFetcher.get_keys\"\"\"\n\n        async def get_key(key_server):\n            try:\n                result = await self.get_server_verify_key_v2_indirect(\n                    keys_to_fetch, key_server\n                )\n                return result\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Key lookup failed from %r: %s\", key_server.server_name, e\n                )\n            except Exception as e:\n                logger.exception(\n                    \"Unable to get key from %r: %s %s\",\n                    key_server.server_name,\n                    type(e).__name__,\n                    str(e),\n                )\n\n            return {}\n\n        results = await make_deferred_yieldable(\n            defer.gatherResults(\n                [run_in_background(get_key, server) for server in self.key_servers],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        union_of_keys = {}\n        for result in results:\n            for server_name, keys in result.items():\n                union_of_keys.setdefault(server_name, {}).update(keys)\n\n        return union_of_keys\n\n    async def get_server_verify_key_v2_indirect(self, keys_to_fetch, key_server):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, dict[str, int]]):\n                the keys to be fetched. server_name -> key_id -> min_valid_ts\n\n            key_server (synapse.config.key.TrustedKeyServer): notary server to query for\n                the keys\n\n        Returns:\n            dict[str, dict[str, synapse.storage.keys.FetchKeyResult]]: map\n                from server_name -> key_id -> FetchKeyResult\n\n        Raises:\n            KeyLookupError if there was an error processing the entire response from\n                the server\n        \"\"\"\n        perspective_name = key_server.server_name\n        logger.info(\n            \"Requesting keys %s from notary server %s\",\n            keys_to_fetch.items(),\n            perspective_name,\n        )\n\n        try:\n            query_response = await self.client.post_json(\n                destination=perspective_name,\n                path=\"/_matrix/key/v2/query\",\n                data={\n                    \"server_keys\": {\n                        server_name: {\n                            key_id: {\"minimum_valid_until_ts\": min_valid_ts}\n                            for key_id, min_valid_ts in server_keys.items()\n                        }\n                        for server_name, server_keys in keys_to_fetch.items()\n                    }\n                },\n            )\n        except (NotRetryingDestination, RequestSendFailed) as e:\n            # these both have str() representations which we can't really improve upon\n            raise KeyLookupError(str(e))\n        except HttpResponseException as e:\n            raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))\n\n        keys = {}\n        added_keys = []\n\n        time_now_ms = self.clock.time_msec()\n\n        for response in query_response[\"server_keys\"]:\n            # do this first, so that we can give useful errors thereafter\n            server_name = response.get(\"server_name\")\n            if not isinstance(server_name, str):\n                raise KeyLookupError(\n                    \"Malformed response from key notary server %s: invalid server_name\"\n                    % (perspective_name,)\n                )\n\n            try:\n                self._validate_perspectives_response(key_server, response)\n\n                processed_response = await self.process_v2_response(\n                    perspective_name, response, time_added_ms=time_now_ms\n                )\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Error processing response from key notary server %s for origin \"\n                    \"server %s: %s\",\n                    perspective_name,\n                    server_name,\n                    e,\n                )\n                # we continue to process the rest of the response\n                continue\n\n            added_keys.extend(\n                (server_name, key_id, key) for key_id, key in processed_response.items()\n            )\n            keys.setdefault(server_name, {}).update(processed_response)\n\n        await self.store.store_server_verify_keys(\n            perspective_name, time_now_ms, added_keys\n        )\n\n        return keys\n\n    def _validate_perspectives_response(self, key_server, response):\n        \"\"\"Optionally check the signature on the result of a /key/query request\n\n        Args:\n            key_server (synapse.config.key.TrustedKeyServer): the notary server that\n                produced this result\n\n            response (dict): the json-decoded Server Keys response object\n        \"\"\"\n        perspective_name = key_server.server_name\n        perspective_keys = key_server.verify_keys\n\n        if perspective_keys is None:\n            # signature checking is disabled on this server\n            return\n\n        if (\n            \"signatures\" not in response\n            or perspective_name not in response[\"signatures\"]\n        ):\n            raise KeyLookupError(\"Response not signed by the notary server\")\n\n        verified = False\n        for key_id in response[\"signatures\"][perspective_name]:\n            if key_id in perspective_keys:\n                verify_signed_json(response, perspective_name, perspective_keys[key_id])\n                verified = True\n\n        if not verified:\n            raise KeyLookupError(\n                \"Response not signed with a known key: signed with: %r, known keys: %r\"\n                % (\n                    list(response[\"signatures\"][perspective_name].keys()),\n                    list(perspective_keys.keys()),\n                )\n            )\n\n\nclass ServerKeyFetcher(BaseV2KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from the origin servers\"\"\"\n\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_http_client()\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, iterable[str]]):\n                the keys to be fetched. server_name -> key_ids\n\n        Returns:\n            dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]:\n                map from server_name -> key_id -> FetchKeyResult\n        \"\"\"\n\n        results = {}\n\n        async def get_key(key_to_fetch_item):\n            server_name, key_ids = key_to_fetch_item\n            try:\n                keys = await self.get_server_verify_key_v2_direct(server_name, key_ids)\n                results[server_name] = keys\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Error looking up keys %s from %s: %s\", key_ids, server_name, e\n                )\n            except Exception:\n                logger.exception(\"Error getting keys %s from %s\", key_ids, server_name)\n\n        await yieldable_gather_results(get_key, keys_to_fetch.items())\n        return results\n\n    async def get_server_verify_key_v2_direct(self, server_name, key_ids):\n        \"\"\"\n\n        Args:\n            server_name (str):\n            key_ids (iterable[str]):\n\n        Returns:\n            dict[str, FetchKeyResult]: map from key ID to lookup result\n\n        Raises:\n            KeyLookupError if there was a problem making the lookup\n        \"\"\"\n        keys = {}  # type: dict[str, FetchKeyResult]\n\n        for requested_key_id in key_ids:\n            # we may have found this key as a side-effect of asking for another.\n            if requested_key_id in keys:\n                continue\n\n            time_now_ms = self.clock.time_msec()\n            try:\n                response = await self.client.get_json(\n                    destination=server_name,\n                    path=\"/_matrix/key/v2/server/\"\n                    + urllib.parse.quote(requested_key_id),\n                    ignore_backoff=True,\n                    # we only give the remote server 10s to respond. It should be an\n                    # easy request to handle, so if it doesn't reply within 10s, it's\n                    # probably not going to.\n                    #\n                    # Furthermore, when we are acting as a notary server, we cannot\n                    # wait all day for all of the origin servers, as the requesting\n                    # server will otherwise time out before we can respond.\n                    #\n                    # (Note that get_json may make 4 attempts, so this can still take\n                    # almost 45 seconds to fetch the headers, plus up to another 60s to\n                    # read the response).\n                    timeout=10000,\n                )\n            except (NotRetryingDestination, RequestSendFailed) as e:\n                # these both have str() representations which we can't really improve\n                # upon\n                raise KeyLookupError(str(e))\n            except HttpResponseException as e:\n                raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))\n\n            if response[\"server_name\"] != server_name:\n                raise KeyLookupError(\n                    \"Expected a response for server %r not %r\"\n                    % (server_name, response[\"server_name\"])\n                )\n\n            response_keys = await self.process_v2_response(\n                from_server=server_name,\n                response_json=response,\n                time_added_ms=time_now_ms,\n            )\n            await self.store.store_server_verify_keys(\n                server_name,\n                time_now_ms,\n                ((server_name, key_id, key) for key_id, key in response_keys.items()),\n            )\n            keys.update(response_keys)\n\n        return keys\n\n\nasync def _handle_key_deferred(verify_request) -> None:\n    \"\"\"Waits for the key to become available, and then performs a verification\n\n    Args:\n        verify_request (VerifyJsonRequest):\n\n    Raises:\n        SynapseError if there was a problem performing the verification\n    \"\"\"\n    server_name = verify_request.server_name\n    with PreserveLoggingContext():\n        _, key_id, verify_key = await verify_request.key_ready\n\n    json_object = verify_request.json_object\n\n    try:\n        verify_signed_json(json_object, server_name, verify_key)\n    except SignatureVerifyException as e:\n        logger.debug(\n            \"Error verifying signature for %s:%s:%s with key %s: %s\",\n            server_name,\n            verify_key.alg,\n            verify_key.version,\n            encode_verify_key_base64(verify_key),\n            str(e),\n        )\n        raise SynapseError(\n            401,\n            \"Invalid signature for server %s with key %s:%s: %s\"\n            % (server_name, verify_key.alg, verify_key.version, str(e)),\n            Codes.UNAUTHORIZED,\n        )\n", "patch": "@@ -578,7 +578,7 @@ class PerspectivesKeyFetcher(BaseV2KeyFetcher):\n     def __init__(self, hs):\n         super().__init__(hs)\n         self.clock = hs.get_clock()\n-        self.client = hs.get_http_client()\n+        self.client = hs.get_federation_http_client()\n         self.key_servers = self.config.key_servers\n \n     async def get_keys(self, keys_to_fetch):\n@@ -748,7 +748,7 @@ class ServerKeyFetcher(BaseV2KeyFetcher):\n     def __init__(self, hs):\n         super().__init__(hs)\n         self.clock = hs.get_clock()\n-        self.client = hs.get_http_client()\n+        self.client = hs.get_federation_http_client()\n \n     async def get_keys(self, keys_to_fetch):\n         \"\"\"", "file_path": "files/2021_2/19", "file_language": "py", "file_name": "synapse/crypto/keyring.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class KeyLookupError(ValueError):\n    pass", "target": 0}, {"function": "class Keyring:\n    def __init__(self, hs, key_fetchers=None):\n        self.clock = hs.get_clock()\n\n        if key_fetchers is None:\n            key_fetchers = (\n                StoreKeyFetcher(hs),\n                PerspectivesKeyFetcher(hs),\n                ServerKeyFetcher(hs),\n            )\n        self._key_fetchers = key_fetchers\n\n        # map from server name to Deferred. Has an entry for each server with\n        # an ongoing key download; the Deferred completes once the download\n        # completes.\n        #\n        # These are regular, logcontext-agnostic Deferreds.\n        self.key_downloads = {}\n\n    def verify_json_for_server(\n        self, server_name, json_object, validity_time, request_name\n    ):\n        \"\"\"Verify that a JSON object has been signed by a given server\n\n        Args:\n            server_name (str): name of the server which must have signed this object\n\n            json_object (dict): object to be checked\n\n            validity_time (int): timestamp at which we require the signing key to\n                be valid. (0 implies we don't care)\n\n            request_name (str): an identifier for this json object (eg, an event id)\n                for logging.\n\n        Returns:\n            Deferred[None]: completes if the the object was correctly signed, otherwise\n                errbacks with an error\n        \"\"\"\n        req = VerifyJsonRequest(server_name, json_object, validity_time, request_name)\n        requests = (req,)\n        return make_deferred_yieldable(self._verify_objects(requests)[0])\n\n    def verify_json_objects_for_server(self, server_and_json):\n        \"\"\"Bulk verifies signatures of json objects, bulk fetching keys as\n        necessary.\n\n        Args:\n            server_and_json (iterable[Tuple[str, dict, int, str]):\n                Iterable of (server_name, json_object, validity_time, request_name)\n                tuples.\n\n                validity_time is a timestamp at which the signing key must be\n                valid.\n\n                request_name is an identifier for this json object (eg, an event id)\n                for logging.\n\n        Returns:\n            List<Deferred[None]>: for each input triplet, a deferred indicating success\n                or failure to verify each json object's signature for the given\n                server_name. The deferreds run their callbacks in the sentinel\n                logcontext.\n        \"\"\"\n        return self._verify_objects(\n            VerifyJsonRequest(server_name, json_object, validity_time, request_name)\n            for server_name, json_object, validity_time, request_name in server_and_json\n        )\n\n    def _verify_objects(self, verify_requests):\n        \"\"\"Does the work of verify_json_[objects_]for_server\n\n\n        Args:\n            verify_requests (iterable[VerifyJsonRequest]):\n                Iterable of verification requests.\n\n        Returns:\n            List<Deferred[None]>: for each input item, a deferred indicating success\n                or failure to verify each json object's signature for the given\n                server_name. The deferreds run their callbacks in the sentinel\n                logcontext.\n        \"\"\"\n        # a list of VerifyJsonRequests which are awaiting a key lookup\n        key_lookups = []\n        handle = preserve_fn(_handle_key_deferred)\n\n        def process(verify_request):\n            \"\"\"Process an entry in the request list\n\n            Adds a key request to key_lookups, and returns a deferred which\n            will complete or fail (in the sentinel context) when verification completes.\n            \"\"\"\n            if not verify_request.key_ids:\n                return defer.fail(\n                    SynapseError(\n                        400,\n                        \"Not signed by %s\" % (verify_request.server_name,),\n                        Codes.UNAUTHORIZED,\n                    )\n                )\n\n            logger.debug(\n                \"Verifying %s for %s with key_ids %s, min_validity %i\",\n                verify_request.request_name,\n                verify_request.server_name,\n                verify_request.key_ids,\n                verify_request.minimum_valid_until_ts,\n            )\n\n            # add the key request to the queue, but don't start it off yet.\n            key_lookups.append(verify_request)\n\n            # now run _handle_key_deferred, which will wait for the key request\n            # to complete and then do the verification.\n            #\n            # We want _handle_key_request to log to the right context, so we\n            # wrap it with preserve_fn (aka run_in_background)\n            return handle(verify_request)\n\n        results = [process(r) for r in verify_requests]\n\n        if key_lookups:\n            run_in_background(self._start_key_lookups, key_lookups)\n\n        return results\n\n    async def _start_key_lookups(self, verify_requests):\n        \"\"\"Sets off the key fetches for each verify request\n\n        Once each fetch completes, verify_request.key_ready will be resolved.\n\n        Args:\n            verify_requests (List[VerifyJsonRequest]):\n        \"\"\"\n\n        try:\n            # map from server name to a set of outstanding request ids\n            server_to_request_ids = {}\n\n            for verify_request in verify_requests:\n                server_name = verify_request.server_name\n                request_id = id(verify_request)\n                server_to_request_ids.setdefault(server_name, set()).add(request_id)\n\n            # Wait for any previous lookups to complete before proceeding.\n            await self.wait_for_previous_lookups(server_to_request_ids.keys())\n\n            # take out a lock on each of the servers by sticking a Deferred in\n            # key_downloads\n            for server_name in server_to_request_ids.keys():\n                self.key_downloads[server_name] = defer.Deferred()\n                logger.debug(\"Got key lookup lock on %s\", server_name)\n\n            # When we've finished fetching all the keys for a given server_name,\n            # drop the lock by resolving the deferred in key_downloads.\n            def drop_server_lock(server_name):\n                d = self.key_downloads.pop(server_name)\n                d.callback(None)\n\n            def lookup_done(res, verify_request):\n                server_name = verify_request.server_name\n                server_requests = server_to_request_ids[server_name]\n                server_requests.remove(id(verify_request))\n\n                # if there are no more requests for this server, we can drop the lock.\n                if not server_requests:\n                    logger.debug(\"Releasing key lookup lock on %s\", server_name)\n                    drop_server_lock(server_name)\n\n                return res\n\n            for verify_request in verify_requests:\n                verify_request.key_ready.addBoth(lookup_done, verify_request)\n\n            # Actually start fetching keys.\n            self._get_server_verify_keys(verify_requests)\n        except Exception:\n            logger.exception(\"Error starting key lookups\")\n\n    async def wait_for_previous_lookups(self, server_names) -> None:\n        \"\"\"Waits for any previous key lookups for the given servers to finish.\n\n        Args:\n            server_names (Iterable[str]): list of servers which we want to look up\n\n        Returns:\n            Resolves once all key lookups for the given servers have\n                completed. Follows the synapse rules of logcontext preservation.\n        \"\"\"\n        loop_count = 1\n        while True:\n            wait_on = [\n                (server_name, self.key_downloads[server_name])\n                for server_name in server_names\n                if server_name in self.key_downloads\n            ]\n            if not wait_on:\n                break\n            logger.info(\n                \"Waiting for existing lookups for %s to complete [loop %i]\",\n                [w[0] for w in wait_on],\n                loop_count,\n            )\n            with PreserveLoggingContext():\n                await defer.DeferredList((w[1] for w in wait_on))\n\n            loop_count += 1\n\n    def _get_server_verify_keys(self, verify_requests):\n        \"\"\"Tries to find at least one key for each verify request\n\n        For each verify_request, verify_request.key_ready is called back with\n        params (server_name, key_id, VerifyKey) if a key is found, or errbacked\n        with a SynapseError if none of the keys are found.\n\n        Args:\n            verify_requests (list[VerifyJsonRequest]): list of verify requests\n        \"\"\"\n\n        remaining_requests = {rq for rq in verify_requests if not rq.key_ready.called}\n\n        async def do_iterations():\n            try:\n                with Measure(self.clock, \"get_server_verify_keys\"):\n                    for f in self._key_fetchers:\n                        if not remaining_requests:\n                            return\n                        await self._attempt_key_fetches_with_fetcher(\n                            f, remaining_requests\n                        )\n\n                    # look for any requests which weren't satisfied\n                    while remaining_requests:\n                        verify_request = remaining_requests.pop()\n                        rq_str = (\n                            \"VerifyJsonRequest(server=%s, key_ids=%s, min_valid=%i)\"\n                            % (\n                                verify_request.server_name,\n                                verify_request.key_ids,\n                                verify_request.minimum_valid_until_ts,\n                            )\n                        )\n\n                        # If we run the errback immediately, it may cancel our\n                        # loggingcontext while we are still in it, so instead we\n                        # schedule it for the next time round the reactor.\n                        #\n                        # (this also ensures that we don't get a stack overflow if we\n                        # has a massive queue of lookups waiting for this server).\n                        self.clock.call_later(\n                            0,\n                            verify_request.key_ready.errback,\n                            SynapseError(\n                                401,\n                                \"Failed to find any key to satisfy %s\" % (rq_str,),\n                                Codes.UNAUTHORIZED,\n                            ),\n                        )\n            except Exception as err:\n                # we don't really expect to get here, because any errors should already\n                # have been caught and logged. But if we do, let's log the error and make\n                # sure that all of the deferreds are resolved.\n                logger.error(\"Unexpected error in _get_server_verify_keys: %s\", err)\n                with PreserveLoggingContext():\n                    for verify_request in remaining_requests:\n                        if not verify_request.key_ready.called:\n                            verify_request.key_ready.errback(err)\n\n        run_in_background(do_iterations)\n\n    async def _attempt_key_fetches_with_fetcher(self, fetcher, remaining_requests):\n        \"\"\"Use a key fetcher to attempt to satisfy some key requests\n\n        Args:\n            fetcher (KeyFetcher): fetcher to use to fetch the keys\n            remaining_requests (set[VerifyJsonRequest]): outstanding key requests.\n                Any successfully-completed requests will be removed from the list.\n        \"\"\"\n        # dict[str, dict[str, int]]: keys to fetch.\n        # server_name -> key_id -> min_valid_ts\n        missing_keys = defaultdict(dict)\n\n        for verify_request in remaining_requests:\n            # any completed requests should already have been removed\n            assert not verify_request.key_ready.called\n            keys_for_server = missing_keys[verify_request.server_name]\n\n            for key_id in verify_request.key_ids:\n                # If we have several requests for the same key, then we only need to\n                # request that key once, but we should do so with the greatest\n                # min_valid_until_ts of the requests, so that we can satisfy all of\n                # the requests.\n                keys_for_server[key_id] = max(\n                    keys_for_server.get(key_id, -1),\n                    verify_request.minimum_valid_until_ts,\n                )\n\n        results = await fetcher.get_keys(missing_keys)\n\n        completed = []\n        for verify_request in remaining_requests:\n            server_name = verify_request.server_name\n\n            # see if any of the keys we got this time are sufficient to\n            # complete this VerifyJsonRequest.\n            result_keys = results.get(server_name, {})\n            for key_id in verify_request.key_ids:\n                fetch_key_result = result_keys.get(key_id)\n                if not fetch_key_result:\n                    # we didn't get a result for this key\n                    continue\n\n                if (\n                    fetch_key_result.valid_until_ts\n                    < verify_request.minimum_valid_until_ts\n                ):\n                    # key was not valid at this point\n                    continue\n\n                # we have a valid key for this request. If we run the callback\n                # immediately, it may cancel our loggingcontext while we are still in\n                # it, so instead we schedule it for the next time round the reactor.\n                #\n                # (this also ensures that we don't get a stack overflow if we had\n                # a massive queue of lookups waiting for this server).\n                logger.debug(\n                    \"Found key %s:%s for %s\",\n                    server_name,\n                    key_id,\n                    verify_request.request_name,\n                )\n                self.clock.call_later(\n                    0,\n                    verify_request.key_ready.callback,\n                    (server_name, key_id, fetch_key_result.verify_key),\n                )\n                completed.append(verify_request)\n                break\n\n        remaining_requests.difference_update(completed)", "target": 0}, {"function": "class KeyFetcher:\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, dict[str, int]]):\n                the keys to be fetched. server_name -> key_id -> min_valid_ts\n\n        Returns:\n            Deferred[dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]]:\n                map from server_name -> key_id -> FetchKeyResult\n        \"\"\"\n        raise NotImplementedError", "target": 0}, {"function": "class StoreKeyFetcher(KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from our data store\"\"\"\n\n    def __init__(self, hs):\n        self.store = hs.get_datastore()\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"see KeyFetcher.get_keys\"\"\"\n\n        keys_to_fetch = (\n            (server_name, key_id)\n            for server_name, keys_for_server in keys_to_fetch.items()\n            for key_id in keys_for_server.keys()\n        )\n\n        res = await self.store.get_server_verify_keys(keys_to_fetch)\n        keys = {}\n        for (server_name, key_id), key in res.items():\n            keys.setdefault(server_name, {})[key_id] = key\n        return keys", "target": 0}, {"function": "class BaseV2KeyFetcher:\n    def __init__(self, hs):\n        self.store = hs.get_datastore()\n        self.config = hs.get_config()\n\n    async def process_v2_response(self, from_server, response_json, time_added_ms):\n        \"\"\"Parse a 'Server Keys' structure from the result of a /key request\n\n        This is used to parse either the entirety of the response from\n        GET /_matrix/key/v2/server, or a single entry from the list returned by\n        POST /_matrix/key/v2/query.\n\n        Checks that each signature in the response that claims to come from the origin\n        server is valid, and that there is at least one such signature.\n\n        Stores the json in server_keys_json so that it can be used for future responses\n        to /_matrix/key/v2/query.\n\n        Args:\n            from_server (str): the name of the server producing this result: either\n                the origin server for a /_matrix/key/v2/server request, or the notary\n                for a /_matrix/key/v2/query.\n\n            response_json (dict): the json-decoded Server Keys response object\n\n            time_added_ms (int): the timestamp to record in server_keys_json\n\n        Returns:\n            Deferred[dict[str, FetchKeyResult]]: map from key_id to result object\n        \"\"\"\n        ts_valid_until_ms = response_json[\"valid_until_ts\"]\n\n        # start by extracting the keys from the response, since they may be required\n        # to validate the signature on the response.\n        verify_keys = {}\n        for key_id, key_data in response_json[\"verify_keys\"].items():\n            if is_signing_algorithm_supported(key_id):\n                key_base64 = key_data[\"key\"]\n                key_bytes = decode_base64(key_base64)\n                verify_key = decode_verify_key_bytes(key_id, key_bytes)\n                verify_keys[key_id] = FetchKeyResult(\n                    verify_key=verify_key, valid_until_ts=ts_valid_until_ms\n                )\n\n        server_name = response_json[\"server_name\"]\n        verified = False\n        for key_id in response_json[\"signatures\"].get(server_name, {}):\n            key = verify_keys.get(key_id)\n            if not key:\n                # the key may not be present in verify_keys if:\n                #  * we got the key from the notary server, and:\n                #  * the key belongs to the notary server, and:\n                #  * the notary server is using a different key to sign notary\n                #    responses.\n                continue\n\n            verify_signed_json(response_json, server_name, key.verify_key)\n            verified = True\n            break\n\n        if not verified:\n            raise KeyLookupError(\n                \"Key response for %s is not signed by the origin server\"\n                % (server_name,)\n            )\n\n        for key_id, key_data in response_json[\"old_verify_keys\"].items():\n            if is_signing_algorithm_supported(key_id):\n                key_base64 = key_data[\"key\"]\n                key_bytes = decode_base64(key_base64)\n                verify_key = decode_verify_key_bytes(key_id, key_bytes)\n                verify_keys[key_id] = FetchKeyResult(\n                    verify_key=verify_key, valid_until_ts=key_data[\"expired_ts\"]\n                )\n\n        key_json_bytes = encode_canonical_json(response_json)\n\n        await make_deferred_yieldable(\n            defer.gatherResults(\n                [\n                    run_in_background(\n                        self.store.store_server_keys_json,\n                        server_name=server_name,\n                        key_id=key_id,\n                        from_server=from_server,\n                        ts_now_ms=time_added_ms,\n                        ts_expires_ms=ts_valid_until_ms,\n                        key_json_bytes=key_json_bytes,\n                    )\n                    for key_id in verify_keys\n                ],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        return verify_keys", "target": 0}, {"function": "class PerspectivesKeyFetcher(BaseV2KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from the \"perspectives\" servers\"\"\"\n\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_http_client()\n        self.key_servers = self.config.key_servers\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"see KeyFetcher.get_keys\"\"\"\n\n        async def get_key(key_server):\n            try:\n                result = await self.get_server_verify_key_v2_indirect(\n                    keys_to_fetch, key_server\n                )\n                return result\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Key lookup failed from %r: %s\", key_server.server_name, e\n                )\n            except Exception as e:\n                logger.exception(\n                    \"Unable to get key from %r: %s %s\",\n                    key_server.server_name,\n                    type(e).__name__,\n                    str(e),\n                )\n\n            return {}\n\n        results = await make_deferred_yieldable(\n            defer.gatherResults(\n                [run_in_background(get_key, server) for server in self.key_servers],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        union_of_keys = {}\n        for result in results:\n            for server_name, keys in result.items():\n                union_of_keys.setdefault(server_name, {}).update(keys)\n\n        return union_of_keys\n\n    async def get_server_verify_key_v2_indirect(self, keys_to_fetch, key_server):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, dict[str, int]]):\n                the keys to be fetched. server_name -> key_id -> min_valid_ts\n\n            key_server (synapse.config.key.TrustedKeyServer): notary server to query for\n                the keys\n\n        Returns:\n            dict[str, dict[str, synapse.storage.keys.FetchKeyResult]]: map\n                from server_name -> key_id -> FetchKeyResult\n\n        Raises:\n            KeyLookupError if there was an error processing the entire response from\n                the server\n        \"\"\"\n        perspective_name = key_server.server_name\n        logger.info(\n            \"Requesting keys %s from notary server %s\",\n            keys_to_fetch.items(),\n            perspective_name,\n        )\n\n        try:\n            query_response = await self.client.post_json(\n                destination=perspective_name,\n                path=\"/_matrix/key/v2/query\",\n                data={\n                    \"server_keys\": {\n                        server_name: {\n                            key_id: {\"minimum_valid_until_ts\": min_valid_ts}\n                            for key_id, min_valid_ts in server_keys.items()\n                        }\n                        for server_name, server_keys in keys_to_fetch.items()\n                    }\n                },\n            )\n        except (NotRetryingDestination, RequestSendFailed) as e:\n            # these both have str() representations which we can't really improve upon\n            raise KeyLookupError(str(e))\n        except HttpResponseException as e:\n            raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))\n\n        keys = {}\n        added_keys = []\n\n        time_now_ms = self.clock.time_msec()\n\n        for response in query_response[\"server_keys\"]:\n            # do this first, so that we can give useful errors thereafter\n            server_name = response.get(\"server_name\")\n            if not isinstance(server_name, str):\n                raise KeyLookupError(\n                    \"Malformed response from key notary server %s: invalid server_name\"\n                    % (perspective_name,)\n                )\n\n            try:\n                self._validate_perspectives_response(key_server, response)\n\n                processed_response = await self.process_v2_response(\n                    perspective_name, response, time_added_ms=time_now_ms\n                )\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Error processing response from key notary server %s for origin \"\n                    \"server %s: %s\",\n                    perspective_name,\n                    server_name,\n                    e,\n                )\n                # we continue to process the rest of the response\n                continue\n\n            added_keys.extend(\n                (server_name, key_id, key) for key_id, key in processed_response.items()\n            )\n            keys.setdefault(server_name, {}).update(processed_response)\n\n        await self.store.store_server_verify_keys(\n            perspective_name, time_now_ms, added_keys\n        )\n\n        return keys\n\n    def _validate_perspectives_response(self, key_server, response):\n        \"\"\"Optionally check the signature on the result of a /key/query request\n\n        Args:\n            key_server (synapse.config.key.TrustedKeyServer): the notary server that\n                produced this result\n\n            response (dict): the json-decoded Server Keys response object\n        \"\"\"\n        perspective_name = key_server.server_name\n        perspective_keys = key_server.verify_keys\n\n        if perspective_keys is None:\n            # signature checking is disabled on this server\n            return\n\n        if (\n            \"signatures\" not in response\n            or perspective_name not in response[\"signatures\"]\n        ):\n            raise KeyLookupError(\"Response not signed by the notary server\")\n\n        verified = False\n        for key_id in response[\"signatures\"][perspective_name]:\n            if key_id in perspective_keys:\n                verify_signed_json(response, perspective_name, perspective_keys[key_id])\n                verified = True\n\n        if not verified:\n            raise KeyLookupError(\n                \"Response not signed with a known key: signed with: %r, known keys: %r\"\n                % (\n                    list(response[\"signatures\"][perspective_name].keys()),\n                    list(perspective_keys.keys()),\n                )\n            )", "target": 0}, {"function": "class ServerKeyFetcher(BaseV2KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from the origin servers\"\"\"\n\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_http_client()\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, iterable[str]]):\n                the keys to be fetched. server_name -> key_ids\n\n        Returns:\n            dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]:\n                map from server_name -> key_id -> FetchKeyResult\n        \"\"\"\n\n        results = {}\n\n        async def get_key(key_to_fetch_item):\n            server_name, key_ids = key_to_fetch_item\n            try:\n                keys = await self.get_server_verify_key_v2_direct(server_name, key_ids)\n                results[server_name] = keys\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Error looking up keys %s from %s: %s\", key_ids, server_name, e\n                )\n            except Exception:\n                logger.exception(\"Error getting keys %s from %s\", key_ids, server_name)\n\n        await yieldable_gather_results(get_key, keys_to_fetch.items())\n        return results\n\n    async def get_server_verify_key_v2_direct(self, server_name, key_ids):\n        \"\"\"\n\n        Args:\n            server_name (str):\n            key_ids (iterable[str]):\n\n        Returns:\n            dict[str, FetchKeyResult]: map from key ID to lookup result\n\n        Raises:\n            KeyLookupError if there was a problem making the lookup\n        \"\"\"\n        keys = {}  # type: dict[str, FetchKeyResult]\n\n        for requested_key_id in key_ids:\n            # we may have found this key as a side-effect of asking for another.\n            if requested_key_id in keys:\n                continue\n\n            time_now_ms = self.clock.time_msec()\n            try:\n                response = await self.client.get_json(\n                    destination=server_name,\n                    path=\"/_matrix/key/v2/server/\"\n                    + urllib.parse.quote(requested_key_id),\n                    ignore_backoff=True,\n                    # we only give the remote server 10s to respond. It should be an\n                    # easy request to handle, so if it doesn't reply within 10s, it's\n                    # probably not going to.\n                    #\n                    # Furthermore, when we are acting as a notary server, we cannot\n                    # wait all day for all of the origin servers, as the requesting\n                    # server will otherwise time out before we can respond.\n                    #\n                    # (Note that get_json may make 4 attempts, so this can still take\n                    # almost 45 seconds to fetch the headers, plus up to another 60s to\n                    # read the response).\n                    timeout=10000,\n                )\n            except (NotRetryingDestination, RequestSendFailed) as e:\n                # these both have str() representations which we can't really improve\n                # upon\n                raise KeyLookupError(str(e))\n            except HttpResponseException as e:\n                raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))\n\n            if response[\"server_name\"] != server_name:\n                raise KeyLookupError(\n                    \"Expected a response for server %r not %r\"\n                    % (server_name, response[\"server_name\"])\n                )\n\n            response_keys = await self.process_v2_response(\n                from_server=server_name,\n                response_json=response,\n                time_added_ms=time_now_ms,\n            )\n            await self.store.store_server_verify_keys(\n                server_name,\n                time_now_ms,\n                ((server_name, key_id, key) for key_id, key in response_keys.items()),\n            )\n            keys.update(response_keys)\n\n        return keys", "target": 0}, {"function": "async def _handle_key_deferred(verify_request) -> None:\n    \"\"\"Waits for the key to become available, and then performs a verification\n\n    Args:\n        verify_request (VerifyJsonRequest):\n\n    Raises:\n        SynapseError if there was a problem performing the verification\n    \"\"\"\n    server_name = verify_request.server_name\n    with PreserveLoggingContext():\n        _, key_id, verify_key = await verify_request.key_ready\n\n    json_object = verify_request.json_object\n\n    try:\n        verify_signed_json(json_object, server_name, verify_key)\n    except SignatureVerifyException as e:\n        logger.debug(\n            \"Error verifying signature for %s:%s:%s with key %s: %s\",\n            server_name,\n            verify_key.alg,\n            verify_key.version,\n            encode_verify_key_base64(verify_key),\n            str(e),\n        )\n        raise SynapseError(\n            401,\n            \"Invalid signature for server %s with key %s:%s: %s\"\n            % (server_name, verify_key.alg, verify_key.version, str(e)),\n            Codes.UNAUTHORIZED,\n        )", "target": 0}], "function_after": [{"function": "class KeyLookupError(ValueError):\n    pass", "target": 0}, {"function": "class Keyring:\n    def __init__(self, hs, key_fetchers=None):\n        self.clock = hs.get_clock()\n\n        if key_fetchers is None:\n            key_fetchers = (\n                StoreKeyFetcher(hs),\n                PerspectivesKeyFetcher(hs),\n                ServerKeyFetcher(hs),\n            )\n        self._key_fetchers = key_fetchers\n\n        # map from server name to Deferred. Has an entry for each server with\n        # an ongoing key download; the Deferred completes once the download\n        # completes.\n        #\n        # These are regular, logcontext-agnostic Deferreds.\n        self.key_downloads = {}\n\n    def verify_json_for_server(\n        self, server_name, json_object, validity_time, request_name\n    ):\n        \"\"\"Verify that a JSON object has been signed by a given server\n\n        Args:\n            server_name (str): name of the server which must have signed this object\n\n            json_object (dict): object to be checked\n\n            validity_time (int): timestamp at which we require the signing key to\n                be valid. (0 implies we don't care)\n\n            request_name (str): an identifier for this json object (eg, an event id)\n                for logging.\n\n        Returns:\n            Deferred[None]: completes if the the object was correctly signed, otherwise\n                errbacks with an error\n        \"\"\"\n        req = VerifyJsonRequest(server_name, json_object, validity_time, request_name)\n        requests = (req,)\n        return make_deferred_yieldable(self._verify_objects(requests)[0])\n\n    def verify_json_objects_for_server(self, server_and_json):\n        \"\"\"Bulk verifies signatures of json objects, bulk fetching keys as\n        necessary.\n\n        Args:\n            server_and_json (iterable[Tuple[str, dict, int, str]):\n                Iterable of (server_name, json_object, validity_time, request_name)\n                tuples.\n\n                validity_time is a timestamp at which the signing key must be\n                valid.\n\n                request_name is an identifier for this json object (eg, an event id)\n                for logging.\n\n        Returns:\n            List<Deferred[None]>: for each input triplet, a deferred indicating success\n                or failure to verify each json object's signature for the given\n                server_name. The deferreds run their callbacks in the sentinel\n                logcontext.\n        \"\"\"\n        return self._verify_objects(\n            VerifyJsonRequest(server_name, json_object, validity_time, request_name)\n            for server_name, json_object, validity_time, request_name in server_and_json\n        )\n\n    def _verify_objects(self, verify_requests):\n        \"\"\"Does the work of verify_json_[objects_]for_server\n\n\n        Args:\n            verify_requests (iterable[VerifyJsonRequest]):\n                Iterable of verification requests.\n\n        Returns:\n            List<Deferred[None]>: for each input item, a deferred indicating success\n                or failure to verify each json object's signature for the given\n                server_name. The deferreds run their callbacks in the sentinel\n                logcontext.\n        \"\"\"\n        # a list of VerifyJsonRequests which are awaiting a key lookup\n        key_lookups = []\n        handle = preserve_fn(_handle_key_deferred)\n\n        def process(verify_request):\n            \"\"\"Process an entry in the request list\n\n            Adds a key request to key_lookups, and returns a deferred which\n            will complete or fail (in the sentinel context) when verification completes.\n            \"\"\"\n            if not verify_request.key_ids:\n                return defer.fail(\n                    SynapseError(\n                        400,\n                        \"Not signed by %s\" % (verify_request.server_name,),\n                        Codes.UNAUTHORIZED,\n                    )\n                )\n\n            logger.debug(\n                \"Verifying %s for %s with key_ids %s, min_validity %i\",\n                verify_request.request_name,\n                verify_request.server_name,\n                verify_request.key_ids,\n                verify_request.minimum_valid_until_ts,\n            )\n\n            # add the key request to the queue, but don't start it off yet.\n            key_lookups.append(verify_request)\n\n            # now run _handle_key_deferred, which will wait for the key request\n            # to complete and then do the verification.\n            #\n            # We want _handle_key_request to log to the right context, so we\n            # wrap it with preserve_fn (aka run_in_background)\n            return handle(verify_request)\n\n        results = [process(r) for r in verify_requests]\n\n        if key_lookups:\n            run_in_background(self._start_key_lookups, key_lookups)\n\n        return results\n\n    async def _start_key_lookups(self, verify_requests):\n        \"\"\"Sets off the key fetches for each verify request\n\n        Once each fetch completes, verify_request.key_ready will be resolved.\n\n        Args:\n            verify_requests (List[VerifyJsonRequest]):\n        \"\"\"\n\n        try:\n            # map from server name to a set of outstanding request ids\n            server_to_request_ids = {}\n\n            for verify_request in verify_requests:\n                server_name = verify_request.server_name\n                request_id = id(verify_request)\n                server_to_request_ids.setdefault(server_name, set()).add(request_id)\n\n            # Wait for any previous lookups to complete before proceeding.\n            await self.wait_for_previous_lookups(server_to_request_ids.keys())\n\n            # take out a lock on each of the servers by sticking a Deferred in\n            # key_downloads\n            for server_name in server_to_request_ids.keys():\n                self.key_downloads[server_name] = defer.Deferred()\n                logger.debug(\"Got key lookup lock on %s\", server_name)\n\n            # When we've finished fetching all the keys for a given server_name,\n            # drop the lock by resolving the deferred in key_downloads.\n            def drop_server_lock(server_name):\n                d = self.key_downloads.pop(server_name)\n                d.callback(None)\n\n            def lookup_done(res, verify_request):\n                server_name = verify_request.server_name\n                server_requests = server_to_request_ids[server_name]\n                server_requests.remove(id(verify_request))\n\n                # if there are no more requests for this server, we can drop the lock.\n                if not server_requests:\n                    logger.debug(\"Releasing key lookup lock on %s\", server_name)\n                    drop_server_lock(server_name)\n\n                return res\n\n            for verify_request in verify_requests:\n                verify_request.key_ready.addBoth(lookup_done, verify_request)\n\n            # Actually start fetching keys.\n            self._get_server_verify_keys(verify_requests)\n        except Exception:\n            logger.exception(\"Error starting key lookups\")\n\n    async def wait_for_previous_lookups(self, server_names) -> None:\n        \"\"\"Waits for any previous key lookups for the given servers to finish.\n\n        Args:\n            server_names (Iterable[str]): list of servers which we want to look up\n\n        Returns:\n            Resolves once all key lookups for the given servers have\n                completed. Follows the synapse rules of logcontext preservation.\n        \"\"\"\n        loop_count = 1\n        while True:\n            wait_on = [\n                (server_name, self.key_downloads[server_name])\n                for server_name in server_names\n                if server_name in self.key_downloads\n            ]\n            if not wait_on:\n                break\n            logger.info(\n                \"Waiting for existing lookups for %s to complete [loop %i]\",\n                [w[0] for w in wait_on],\n                loop_count,\n            )\n            with PreserveLoggingContext():\n                await defer.DeferredList((w[1] for w in wait_on))\n\n            loop_count += 1\n\n    def _get_server_verify_keys(self, verify_requests):\n        \"\"\"Tries to find at least one key for each verify request\n\n        For each verify_request, verify_request.key_ready is called back with\n        params (server_name, key_id, VerifyKey) if a key is found, or errbacked\n        with a SynapseError if none of the keys are found.\n\n        Args:\n            verify_requests (list[VerifyJsonRequest]): list of verify requests\n        \"\"\"\n\n        remaining_requests = {rq for rq in verify_requests if not rq.key_ready.called}\n\n        async def do_iterations():\n            try:\n                with Measure(self.clock, \"get_server_verify_keys\"):\n                    for f in self._key_fetchers:\n                        if not remaining_requests:\n                            return\n                        await self._attempt_key_fetches_with_fetcher(\n                            f, remaining_requests\n                        )\n\n                    # look for any requests which weren't satisfied\n                    while remaining_requests:\n                        verify_request = remaining_requests.pop()\n                        rq_str = (\n                            \"VerifyJsonRequest(server=%s, key_ids=%s, min_valid=%i)\"\n                            % (\n                                verify_request.server_name,\n                                verify_request.key_ids,\n                                verify_request.minimum_valid_until_ts,\n                            )\n                        )\n\n                        # If we run the errback immediately, it may cancel our\n                        # loggingcontext while we are still in it, so instead we\n                        # schedule it for the next time round the reactor.\n                        #\n                        # (this also ensures that we don't get a stack overflow if we\n                        # has a massive queue of lookups waiting for this server).\n                        self.clock.call_later(\n                            0,\n                            verify_request.key_ready.errback,\n                            SynapseError(\n                                401,\n                                \"Failed to find any key to satisfy %s\" % (rq_str,),\n                                Codes.UNAUTHORIZED,\n                            ),\n                        )\n            except Exception as err:\n                # we don't really expect to get here, because any errors should already\n                # have been caught and logged. But if we do, let's log the error and make\n                # sure that all of the deferreds are resolved.\n                logger.error(\"Unexpected error in _get_server_verify_keys: %s\", err)\n                with PreserveLoggingContext():\n                    for verify_request in remaining_requests:\n                        if not verify_request.key_ready.called:\n                            verify_request.key_ready.errback(err)\n\n        run_in_background(do_iterations)\n\n    async def _attempt_key_fetches_with_fetcher(self, fetcher, remaining_requests):\n        \"\"\"Use a key fetcher to attempt to satisfy some key requests\n\n        Args:\n            fetcher (KeyFetcher): fetcher to use to fetch the keys\n            remaining_requests (set[VerifyJsonRequest]): outstanding key requests.\n                Any successfully-completed requests will be removed from the list.\n        \"\"\"\n        # dict[str, dict[str, int]]: keys to fetch.\n        # server_name -> key_id -> min_valid_ts\n        missing_keys = defaultdict(dict)\n\n        for verify_request in remaining_requests:\n            # any completed requests should already have been removed\n            assert not verify_request.key_ready.called\n            keys_for_server = missing_keys[verify_request.server_name]\n\n            for key_id in verify_request.key_ids:\n                # If we have several requests for the same key, then we only need to\n                # request that key once, but we should do so with the greatest\n                # min_valid_until_ts of the requests, so that we can satisfy all of\n                # the requests.\n                keys_for_server[key_id] = max(\n                    keys_for_server.get(key_id, -1),\n                    verify_request.minimum_valid_until_ts,\n                )\n\n        results = await fetcher.get_keys(missing_keys)\n\n        completed = []\n        for verify_request in remaining_requests:\n            server_name = verify_request.server_name\n\n            # see if any of the keys we got this time are sufficient to\n            # complete this VerifyJsonRequest.\n            result_keys = results.get(server_name, {})\n            for key_id in verify_request.key_ids:\n                fetch_key_result = result_keys.get(key_id)\n                if not fetch_key_result:\n                    # we didn't get a result for this key\n                    continue\n\n                if (\n                    fetch_key_result.valid_until_ts\n                    < verify_request.minimum_valid_until_ts\n                ):\n                    # key was not valid at this point\n                    continue\n\n                # we have a valid key for this request. If we run the callback\n                # immediately, it may cancel our loggingcontext while we are still in\n                # it, so instead we schedule it for the next time round the reactor.\n                #\n                # (this also ensures that we don't get a stack overflow if we had\n                # a massive queue of lookups waiting for this server).\n                logger.debug(\n                    \"Found key %s:%s for %s\",\n                    server_name,\n                    key_id,\n                    verify_request.request_name,\n                )\n                self.clock.call_later(\n                    0,\n                    verify_request.key_ready.callback,\n                    (server_name, key_id, fetch_key_result.verify_key),\n                )\n                completed.append(verify_request)\n                break\n\n        remaining_requests.difference_update(completed)", "target": 0}, {"function": "class KeyFetcher:\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, dict[str, int]]):\n                the keys to be fetched. server_name -> key_id -> min_valid_ts\n\n        Returns:\n            Deferred[dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]]:\n                map from server_name -> key_id -> FetchKeyResult\n        \"\"\"\n        raise NotImplementedError", "target": 0}, {"function": "class StoreKeyFetcher(KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from our data store\"\"\"\n\n    def __init__(self, hs):\n        self.store = hs.get_datastore()\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"see KeyFetcher.get_keys\"\"\"\n\n        keys_to_fetch = (\n            (server_name, key_id)\n            for server_name, keys_for_server in keys_to_fetch.items()\n            for key_id in keys_for_server.keys()\n        )\n\n        res = await self.store.get_server_verify_keys(keys_to_fetch)\n        keys = {}\n        for (server_name, key_id), key in res.items():\n            keys.setdefault(server_name, {})[key_id] = key\n        return keys", "target": 0}, {"function": "class BaseV2KeyFetcher:\n    def __init__(self, hs):\n        self.store = hs.get_datastore()\n        self.config = hs.get_config()\n\n    async def process_v2_response(self, from_server, response_json, time_added_ms):\n        \"\"\"Parse a 'Server Keys' structure from the result of a /key request\n\n        This is used to parse either the entirety of the response from\n        GET /_matrix/key/v2/server, or a single entry from the list returned by\n        POST /_matrix/key/v2/query.\n\n        Checks that each signature in the response that claims to come from the origin\n        server is valid, and that there is at least one such signature.\n\n        Stores the json in server_keys_json so that it can be used for future responses\n        to /_matrix/key/v2/query.\n\n        Args:\n            from_server (str): the name of the server producing this result: either\n                the origin server for a /_matrix/key/v2/server request, or the notary\n                for a /_matrix/key/v2/query.\n\n            response_json (dict): the json-decoded Server Keys response object\n\n            time_added_ms (int): the timestamp to record in server_keys_json\n\n        Returns:\n            Deferred[dict[str, FetchKeyResult]]: map from key_id to result object\n        \"\"\"\n        ts_valid_until_ms = response_json[\"valid_until_ts\"]\n\n        # start by extracting the keys from the response, since they may be required\n        # to validate the signature on the response.\n        verify_keys = {}\n        for key_id, key_data in response_json[\"verify_keys\"].items():\n            if is_signing_algorithm_supported(key_id):\n                key_base64 = key_data[\"key\"]\n                key_bytes = decode_base64(key_base64)\n                verify_key = decode_verify_key_bytes(key_id, key_bytes)\n                verify_keys[key_id] = FetchKeyResult(\n                    verify_key=verify_key, valid_until_ts=ts_valid_until_ms\n                )\n\n        server_name = response_json[\"server_name\"]\n        verified = False\n        for key_id in response_json[\"signatures\"].get(server_name, {}):\n            key = verify_keys.get(key_id)\n            if not key:\n                # the key may not be present in verify_keys if:\n                #  * we got the key from the notary server, and:\n                #  * the key belongs to the notary server, and:\n                #  * the notary server is using a different key to sign notary\n                #    responses.\n                continue\n\n            verify_signed_json(response_json, server_name, key.verify_key)\n            verified = True\n            break\n\n        if not verified:\n            raise KeyLookupError(\n                \"Key response for %s is not signed by the origin server\"\n                % (server_name,)\n            )\n\n        for key_id, key_data in response_json[\"old_verify_keys\"].items():\n            if is_signing_algorithm_supported(key_id):\n                key_base64 = key_data[\"key\"]\n                key_bytes = decode_base64(key_base64)\n                verify_key = decode_verify_key_bytes(key_id, key_bytes)\n                verify_keys[key_id] = FetchKeyResult(\n                    verify_key=verify_key, valid_until_ts=key_data[\"expired_ts\"]\n                )\n\n        key_json_bytes = encode_canonical_json(response_json)\n\n        await make_deferred_yieldable(\n            defer.gatherResults(\n                [\n                    run_in_background(\n                        self.store.store_server_keys_json,\n                        server_name=server_name,\n                        key_id=key_id,\n                        from_server=from_server,\n                        ts_now_ms=time_added_ms,\n                        ts_expires_ms=ts_valid_until_ms,\n                        key_json_bytes=key_json_bytes,\n                    )\n                    for key_id in verify_keys\n                ],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        return verify_keys", "target": 0}, {"function": "class PerspectivesKeyFetcher(BaseV2KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from the \"perspectives\" servers\"\"\"\n\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_federation_http_client()\n        self.key_servers = self.config.key_servers\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"see KeyFetcher.get_keys\"\"\"\n\n        async def get_key(key_server):\n            try:\n                result = await self.get_server_verify_key_v2_indirect(\n                    keys_to_fetch, key_server\n                )\n                return result\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Key lookup failed from %r: %s\", key_server.server_name, e\n                )\n            except Exception as e:\n                logger.exception(\n                    \"Unable to get key from %r: %s %s\",\n                    key_server.server_name,\n                    type(e).__name__,\n                    str(e),\n                )\n\n            return {}\n\n        results = await make_deferred_yieldable(\n            defer.gatherResults(\n                [run_in_background(get_key, server) for server in self.key_servers],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        union_of_keys = {}\n        for result in results:\n            for server_name, keys in result.items():\n                union_of_keys.setdefault(server_name, {}).update(keys)\n\n        return union_of_keys\n\n    async def get_server_verify_key_v2_indirect(self, keys_to_fetch, key_server):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, dict[str, int]]):\n                the keys to be fetched. server_name -> key_id -> min_valid_ts\n\n            key_server (synapse.config.key.TrustedKeyServer): notary server to query for\n                the keys\n\n        Returns:\n            dict[str, dict[str, synapse.storage.keys.FetchKeyResult]]: map\n                from server_name -> key_id -> FetchKeyResult\n\n        Raises:\n            KeyLookupError if there was an error processing the entire response from\n                the server\n        \"\"\"\n        perspective_name = key_server.server_name\n        logger.info(\n            \"Requesting keys %s from notary server %s\",\n            keys_to_fetch.items(),\n            perspective_name,\n        )\n\n        try:\n            query_response = await self.client.post_json(\n                destination=perspective_name,\n                path=\"/_matrix/key/v2/query\",\n                data={\n                    \"server_keys\": {\n                        server_name: {\n                            key_id: {\"minimum_valid_until_ts\": min_valid_ts}\n                            for key_id, min_valid_ts in server_keys.items()\n                        }\n                        for server_name, server_keys in keys_to_fetch.items()\n                    }\n                },\n            )\n        except (NotRetryingDestination, RequestSendFailed) as e:\n            # these both have str() representations which we can't really improve upon\n            raise KeyLookupError(str(e))\n        except HttpResponseException as e:\n            raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))\n\n        keys = {}\n        added_keys = []\n\n        time_now_ms = self.clock.time_msec()\n\n        for response in query_response[\"server_keys\"]:\n            # do this first, so that we can give useful errors thereafter\n            server_name = response.get(\"server_name\")\n            if not isinstance(server_name, str):\n                raise KeyLookupError(\n                    \"Malformed response from key notary server %s: invalid server_name\"\n                    % (perspective_name,)\n                )\n\n            try:\n                self._validate_perspectives_response(key_server, response)\n\n                processed_response = await self.process_v2_response(\n                    perspective_name, response, time_added_ms=time_now_ms\n                )\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Error processing response from key notary server %s for origin \"\n                    \"server %s: %s\",\n                    perspective_name,\n                    server_name,\n                    e,\n                )\n                # we continue to process the rest of the response\n                continue\n\n            added_keys.extend(\n                (server_name, key_id, key) for key_id, key in processed_response.items()\n            )\n            keys.setdefault(server_name, {}).update(processed_response)\n\n        await self.store.store_server_verify_keys(\n            perspective_name, time_now_ms, added_keys\n        )\n\n        return keys\n\n    def _validate_perspectives_response(self, key_server, response):\n        \"\"\"Optionally check the signature on the result of a /key/query request\n\n        Args:\n            key_server (synapse.config.key.TrustedKeyServer): the notary server that\n                produced this result\n\n            response (dict): the json-decoded Server Keys response object\n        \"\"\"\n        perspective_name = key_server.server_name\n        perspective_keys = key_server.verify_keys\n\n        if perspective_keys is None:\n            # signature checking is disabled on this server\n            return\n\n        if (\n            \"signatures\" not in response\n            or perspective_name not in response[\"signatures\"]\n        ):\n            raise KeyLookupError(\"Response not signed by the notary server\")\n\n        verified = False\n        for key_id in response[\"signatures\"][perspective_name]:\n            if key_id in perspective_keys:\n                verify_signed_json(response, perspective_name, perspective_keys[key_id])\n                verified = True\n\n        if not verified:\n            raise KeyLookupError(\n                \"Response not signed with a known key: signed with: %r, known keys: %r\"\n                % (\n                    list(response[\"signatures\"][perspective_name].keys()),\n                    list(perspective_keys.keys()),\n                )\n            )", "target": 0}, {"function": "class ServerKeyFetcher(BaseV2KeyFetcher):\n    \"\"\"KeyFetcher impl which fetches keys from the origin servers\"\"\"\n\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_federation_http_client()\n\n    async def get_keys(self, keys_to_fetch):\n        \"\"\"\n        Args:\n            keys_to_fetch (dict[str, iterable[str]]):\n                the keys to be fetched. server_name -> key_ids\n\n        Returns:\n            dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]:\n                map from server_name -> key_id -> FetchKeyResult\n        \"\"\"\n\n        results = {}\n\n        async def get_key(key_to_fetch_item):\n            server_name, key_ids = key_to_fetch_item\n            try:\n                keys = await self.get_server_verify_key_v2_direct(server_name, key_ids)\n                results[server_name] = keys\n            except KeyLookupError as e:\n                logger.warning(\n                    \"Error looking up keys %s from %s: %s\", key_ids, server_name, e\n                )\n            except Exception:\n                logger.exception(\"Error getting keys %s from %s\", key_ids, server_name)\n\n        await yieldable_gather_results(get_key, keys_to_fetch.items())\n        return results\n\n    async def get_server_verify_key_v2_direct(self, server_name, key_ids):\n        \"\"\"\n\n        Args:\n            server_name (str):\n            key_ids (iterable[str]):\n\n        Returns:\n            dict[str, FetchKeyResult]: map from key ID to lookup result\n\n        Raises:\n            KeyLookupError if there was a problem making the lookup\n        \"\"\"\n        keys = {}  # type: dict[str, FetchKeyResult]\n\n        for requested_key_id in key_ids:\n            # we may have found this key as a side-effect of asking for another.\n            if requested_key_id in keys:\n                continue\n\n            time_now_ms = self.clock.time_msec()\n            try:\n                response = await self.client.get_json(\n                    destination=server_name,\n                    path=\"/_matrix/key/v2/server/\"\n                    + urllib.parse.quote(requested_key_id),\n                    ignore_backoff=True,\n                    # we only give the remote server 10s to respond. It should be an\n                    # easy request to handle, so if it doesn't reply within 10s, it's\n                    # probably not going to.\n                    #\n                    # Furthermore, when we are acting as a notary server, we cannot\n                    # wait all day for all of the origin servers, as the requesting\n                    # server will otherwise time out before we can respond.\n                    #\n                    # (Note that get_json may make 4 attempts, so this can still take\n                    # almost 45 seconds to fetch the headers, plus up to another 60s to\n                    # read the response).\n                    timeout=10000,\n                )\n            except (NotRetryingDestination, RequestSendFailed) as e:\n                # these both have str() representations which we can't really improve\n                # upon\n                raise KeyLookupError(str(e))\n            except HttpResponseException as e:\n                raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))\n\n            if response[\"server_name\"] != server_name:\n                raise KeyLookupError(\n                    \"Expected a response for server %r not %r\"\n                    % (server_name, response[\"server_name\"])\n                )\n\n            response_keys = await self.process_v2_response(\n                from_server=server_name,\n                response_json=response,\n                time_added_ms=time_now_ms,\n            )\n            await self.store.store_server_verify_keys(\n                server_name,\n                time_now_ms,\n                ((server_name, key_id, key) for key_id, key in response_keys.items()),\n            )\n            keys.update(response_keys)\n\n        return keys", "target": 0}, {"function": "async def _handle_key_deferred(verify_request) -> None:\n    \"\"\"Waits for the key to become available, and then performs a verification\n\n    Args:\n        verify_request (VerifyJsonRequest):\n\n    Raises:\n        SynapseError if there was a problem performing the verification\n    \"\"\"\n    server_name = verify_request.server_name\n    with PreserveLoggingContext():\n        _, key_id, verify_key = await verify_request.key_ready\n\n    json_object = verify_request.json_object\n\n    try:\n        verify_signed_json(json_object, server_name, verify_key)\n    except SignatureVerifyException as e:\n        logger.debug(\n            \"Error verifying signature for %s:%s:%s with key %s: %s\",\n            server_name,\n            verify_key.alg,\n            verify_key.version,\n            encode_verify_key_base64(verify_key),\n            str(e),\n        )\n        raise SynapseError(\n            401,\n            \"Invalid signature for server %s with key %s:%s: %s\"\n            % (server_name, verify_key.alg, verify_key.version, str(e)),\n            Codes.UNAUTHORIZED,\n        )", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Ffederation%2Ffederation_server.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2015, 2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n# Copyright 2019 Matrix.org Federation C.I.C\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Tuple,\n    Union,\n)\n\nfrom prometheus_client import Counter, Gauge, Histogram\n\nfrom twisted.internet import defer\nfrom twisted.internet.abstract import isIPAddress\nfrom twisted.python import failure\n\nfrom synapse.api.constants import EventTypes, Membership\nfrom synapse.api.errors import (\n    AuthError,\n    Codes,\n    FederationError,\n    IncompatibleRoomVersionError,\n    NotFoundError,\n    SynapseError,\n    UnsupportedRoomVersionError,\n)\nfrom synapse.api.room_versions import KNOWN_ROOM_VERSIONS\nfrom synapse.events import EventBase\nfrom synapse.federation.federation_base import FederationBase, event_from_pdu_json\nfrom synapse.federation.persistence import TransactionActions\nfrom synapse.federation.units import Edu, Transaction\nfrom synapse.http.endpoint import parse_server_name\nfrom synapse.http.servlet import assert_params_in_dict\nfrom synapse.logging.context import (\n    make_deferred_yieldable,\n    nested_logging_context,\n    run_in_background,\n)\nfrom synapse.logging.opentracing import log_kv, start_active_span_from_edu, trace\nfrom synapse.logging.utils import log_function\nfrom synapse.replication.http.federation import (\n    ReplicationFederationSendEduRestServlet,\n    ReplicationGetQueryRestServlet,\n)\nfrom synapse.types import JsonDict, get_domain_from_id\nfrom synapse.util import glob_to_regex, json_decoder, unwrapFirstError\nfrom synapse.util.async_helpers import Linearizer, concurrently_execute\nfrom synapse.util.caches.response_cache import ResponseCache\n\nif TYPE_CHECKING:\n    from synapse.server import HomeServer\n\n# when processing incoming transactions, we try to handle multiple rooms in\n# parallel, up to this limit.\nTRANSACTION_CONCURRENCY_LIMIT = 10\n\nlogger = logging.getLogger(__name__)\n\nreceived_pdus_counter = Counter(\"synapse_federation_server_received_pdus\", \"\")\n\nreceived_edus_counter = Counter(\"synapse_federation_server_received_edus\", \"\")\n\nreceived_queries_counter = Counter(\n    \"synapse_federation_server_received_queries\", \"\", [\"type\"]\n)\n\npdu_process_time = Histogram(\n    \"synapse_federation_server_pdu_process_time\", \"Time taken to process an event\",\n)\n\n\nlast_pdu_age_metric = Gauge(\n    \"synapse_federation_last_received_pdu_age\",\n    \"The age (in seconds) of the last PDU successfully received from the given domain\",\n    labelnames=(\"server_name\",),\n)\n\n\nclass FederationServer(FederationBase):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        self.auth = hs.get_auth()\n        self.handler = hs.get_federation_handler()\n        self.state = hs.get_state_handler()\n\n        self.device_handler = hs.get_device_handler()\n\n        # Ensure the following handlers are loaded since they register callbacks\n        # with FederationHandlerRegistry.\n        hs.get_directory_handler()\n\n        self._federation_ratelimiter = hs.get_federation_ratelimiter()\n\n        self._server_linearizer = Linearizer(\"fed_server\")\n        self._transaction_linearizer = Linearizer(\"fed_txn_handler\")\n\n        # We cache results for transaction with the same ID\n        self._transaction_resp_cache = ResponseCache(\n            hs, \"fed_txn_handler\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n\n        self.transaction_actions = TransactionActions(self.store)\n\n        self.registry = hs.get_federation_registry()\n\n        # We cache responses to state queries, as they take a while and often\n        # come in waves.\n        self._state_resp_cache = ResponseCache(\n            hs, \"state_resp\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n        self._state_ids_resp_cache = ResponseCache(\n            hs, \"state_ids_resp\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n\n        self._federation_metrics_domains = (\n            hs.get_config().federation.federation_metrics_domains\n        )\n\n    async def on_backfill_request(\n        self, origin: str, room_id: str, versions: List[str], limit: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            pdus = await self.handler.on_backfill_request(\n                origin, room_id, versions, limit\n            )\n\n            res = self._transaction_from_pdus(pdus).get_dict()\n\n        return 200, res\n\n    async def on_incoming_transaction(\n        self, origin: str, transaction_data: JsonDict\n    ) -> Tuple[int, Dict[str, Any]]:\n        # keep this as early as possible to make the calculated origin ts as\n        # accurate as possible.\n        request_time = self._clock.time_msec()\n\n        transaction = Transaction(**transaction_data)\n        transaction_id = transaction.transaction_id  # type: ignore\n\n        if not transaction_id:\n            raise Exception(\"Transaction missing transaction_id\")\n\n        logger.debug(\"[%s] Got transaction\", transaction_id)\n\n        # We wrap in a ResponseCache so that we de-duplicate retried\n        # transactions.\n        return await self._transaction_resp_cache.wrap(\n            (origin, transaction_id),\n            self._on_incoming_transaction_inner,\n            origin,\n            transaction,\n            request_time,\n        )\n\n    async def _on_incoming_transaction_inner(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        # Use a linearizer to ensure that transactions from a remote are\n        # processed in order.\n        with await self._transaction_linearizer.queue(origin):\n            # We rate limit here *after* we've queued up the incoming requests,\n            # so that we don't fill up the ratelimiter with blocked requests.\n            #\n            # This is important as the ratelimiter allows N concurrent requests\n            # at a time, and only starts ratelimiting if there are more requests\n            # than that being processed at a time. If we queued up requests in\n            # the linearizer/response cache *after* the ratelimiting then those\n            # queued up requests would count as part of the allowed limit of N\n            # concurrent requests.\n            with self._federation_ratelimiter.ratelimit(origin) as d:\n                await d\n\n                result = await self._handle_incoming_transaction(\n                    origin, transaction, request_time\n                )\n\n        return result\n\n    async def _handle_incoming_transaction(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        \"\"\" Process an incoming transaction and return the HTTP response\n\n        Args:\n            origin: the server making the request\n            transaction: incoming transaction\n            request_time: timestamp that the HTTP request arrived at\n\n        Returns:\n            HTTP response code and body\n        \"\"\"\n        response = await self.transaction_actions.have_responded(origin, transaction)\n\n        if response:\n            logger.debug(\n                \"[%s] We've already responded to this request\",\n                transaction.transaction_id,  # type: ignore\n            )\n            return response\n\n        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)  # type: ignore\n\n        # Reject if PDU count > 50 or EDU count > 100\n        if len(transaction.pdus) > 50 or (  # type: ignore\n            hasattr(transaction, \"edus\") and len(transaction.edus) > 100  # type: ignore\n        ):\n\n            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")\n\n            response = {}\n            await self.transaction_actions.set_response(\n                origin, transaction, 400, response\n            )\n            return 400, response\n\n        # We process PDUs and EDUs in parallel. This is important as we don't\n        # want to block things like to device messages from reaching clients\n        # behind the potentially expensive handling of PDUs.\n        pdu_results, _ = await make_deferred_yieldable(\n            defer.gatherResults(\n                [\n                    run_in_background(\n                        self._handle_pdus_in_txn, origin, transaction, request_time\n                    ),\n                    run_in_background(self._handle_edus_in_txn, origin, transaction),\n                ],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        response = {\"pdus\": pdu_results}\n\n        logger.debug(\"Returning: %s\", str(response))\n\n        await self.transaction_actions.set_response(origin, transaction, 200, response)\n        return 200, response\n\n    async def _handle_pdus_in_txn(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Dict[str, dict]:\n        \"\"\"Process the PDUs in a received transaction.\n\n        Args:\n            origin: the server making the request\n            transaction: incoming transaction\n            request_time: timestamp that the HTTP request arrived at\n\n        Returns:\n            A map from event ID of a processed PDU to any errors we should\n            report back to the sending server.\n        \"\"\"\n\n        received_pdus_counter.inc(len(transaction.pdus))  # type: ignore\n\n        origin_host, _ = parse_server_name(origin)\n\n        pdus_by_room = {}  # type: Dict[str, List[EventBase]]\n\n        newest_pdu_ts = 0\n\n        for p in transaction.pdus:  # type: ignore\n            # FIXME (richardv): I don't think this works:\n            #  https://github.com/matrix-org/synapse/issues/8429\n            if \"unsigned\" in p:\n                unsigned = p[\"unsigned\"]\n                if \"age\" in unsigned:\n                    p[\"age\"] = unsigned[\"age\"]\n            if \"age\" in p:\n                p[\"age_ts\"] = request_time - int(p[\"age\"])\n                del p[\"age\"]\n\n            # We try and pull out an event ID so that if later checks fail we\n            # can log something sensible. We don't mandate an event ID here in\n            # case future event formats get rid of the key.\n            possible_event_id = p.get(\"event_id\", \"<Unknown>\")\n\n            # Now we get the room ID so that we can check that we know the\n            # version of the room.\n            room_id = p.get(\"room_id\")\n            if not room_id:\n                logger.info(\n                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",\n                    possible_event_id,\n                )\n                continue\n\n            try:\n                room_version = await self.store.get_room_version(room_id)\n            except NotFoundError:\n                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)\n                continue\n            except UnsupportedRoomVersionError as e:\n                # this can happen if support for a given room version is withdrawn,\n                # so that we still get events for said room.\n                logger.info(\"Ignoring PDU: %s\", e)\n                continue\n\n            event = event_from_pdu_json(p, room_version)\n            pdus_by_room.setdefault(room_id, []).append(event)\n\n            if event.origin_server_ts > newest_pdu_ts:\n                newest_pdu_ts = event.origin_server_ts\n\n        pdu_results = {}\n\n        # we can process different rooms in parallel (which is useful if they\n        # require callouts to other servers to fetch missing events), but\n        # impose a limit to avoid going too crazy with ram/cpu.\n\n        async def process_pdus_for_room(room_id: str):\n            logger.debug(\"Processing PDUs for %s\", room_id)\n            try:\n                await self.check_server_matches_acl(origin_host, room_id)\n            except AuthError as e:\n                logger.warning(\"Ignoring PDUs for room %s from banned server\", room_id)\n                for pdu in pdus_by_room[room_id]:\n                    event_id = pdu.event_id\n                    pdu_results[event_id] = e.error_dict()\n                return\n\n            for pdu in pdus_by_room[room_id]:\n                event_id = pdu.event_id\n                with pdu_process_time.time():\n                    with nested_logging_context(event_id):\n                        try:\n                            await self._handle_received_pdu(origin, pdu)\n                            pdu_results[event_id] = {}\n                        except FederationError as e:\n                            logger.warning(\"Error handling PDU %s: %s\", event_id, e)\n                            pdu_results[event_id] = {\"error\": str(e)}\n                        except Exception as e:\n                            f = failure.Failure()\n                            pdu_results[event_id] = {\"error\": str(e)}\n                            logger.error(\n                                \"Failed to handle PDU %s\",\n                                event_id,\n                                exc_info=(f.type, f.value, f.getTracebackObject()),\n                            )\n\n        await concurrently_execute(\n            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT\n        )\n\n        if newest_pdu_ts and origin in self._federation_metrics_domains:\n            newest_pdu_age = self._clock.time_msec() - newest_pdu_ts\n            last_pdu_age_metric.labels(server_name=origin).set(newest_pdu_age / 1000)\n\n        return pdu_results\n\n    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction):\n        \"\"\"Process the EDUs in a received transaction.\n        \"\"\"\n\n        async def _process_edu(edu_dict):\n            received_edus_counter.inc()\n\n            edu = Edu(\n                origin=origin,\n                destination=self.server_name,\n                edu_type=edu_dict[\"edu_type\"],\n                content=edu_dict[\"content\"],\n            )\n            await self.registry.on_edu(edu.edu_type, origin, edu.content)\n\n        await concurrently_execute(\n            _process_edu,\n            getattr(transaction, \"edus\", []),\n            TRANSACTION_CONCURRENCY_LIMIT,\n        )\n\n    async def on_room_state_request(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # we grab the linearizer to protect ourselves from servers which hammer\n        # us. In theory we might already have the response to this query\n        # in the cache so we could return it without waiting for the linearizer\n        # - but that's non-trivial to get right, and anyway somewhat defeats\n        # the point of the linearizer.\n        with (await self._server_linearizer.queue((origin, room_id))):\n            resp = dict(\n                await self._state_resp_cache.wrap(\n                    (room_id, event_id),\n                    self._on_context_state_request_compute,\n                    room_id,\n                    event_id,\n                )\n            )\n\n        room_version = await self.store.get_room_version_id(room_id)\n        resp[\"room_version\"] = room_version\n\n        return 200, resp\n\n    async def on_state_ids_request(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        if not event_id:\n            raise NotImplementedError(\"Specify an event\")\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        resp = await self._state_ids_resp_cache.wrap(\n            (room_id, event_id), self._on_state_ids_request_compute, room_id, event_id,\n        )\n\n        return 200, resp\n\n    async def _on_state_ids_request_compute(self, room_id, event_id):\n        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)\n        auth_chain_ids = await self.store.get_auth_chain_ids(state_ids)\n        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": auth_chain_ids}\n\n    async def _on_context_state_request_compute(\n        self, room_id: str, event_id: str\n    ) -> Dict[str, list]:\n        if event_id:\n            pdus = await self.handler.get_state_for_pdu(room_id, event_id)\n        else:\n            pdus = (await self.state.get_current_state(room_id)).values()\n\n        auth_chain = await self.store.get_auth_chain([pdu.event_id for pdu in pdus])\n\n        return {\n            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],\n            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],\n        }\n\n    async def on_pdu_request(\n        self, origin: str, event_id: str\n    ) -> Tuple[int, Union[JsonDict, str]]:\n        pdu = await self.handler.get_persisted_pdu(origin, event_id)\n\n        if pdu:\n            return 200, self._transaction_from_pdus([pdu]).get_dict()\n        else:\n            return 404, \"\"\n\n    async def on_query_request(\n        self, query_type: str, args: Dict[str, str]\n    ) -> Tuple[int, Dict[str, Any]]:\n        received_queries_counter.labels(query_type).inc()\n        resp = await self.registry.on_query(query_type, args)\n        return 200, resp\n\n    async def on_make_join_request(\n        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]\n    ) -> Dict[str, Any]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        room_version = await self.store.get_room_version_id(room_id)\n        if room_version not in supported_versions:\n            logger.warning(\n                \"Room version %s not in %s\", room_version, supported_versions\n            )\n            raise IncompatibleRoomVersionError(room_version=room_version)\n\n        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)\n        time_now = self._clock.time_msec()\n        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}\n\n    async def on_invite_request(\n        self, origin: str, content: JsonDict, room_version_id: str\n    ) -> Dict[str, Any]:\n        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)\n        if not room_version:\n            raise SynapseError(\n                400,\n                \"Homeserver does not support this room version\",\n                Codes.UNSUPPORTED_ROOM_VERSION,\n            )\n\n        pdu = event_from_pdu_json(content, room_version)\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)\n        time_now = self._clock.time_msec()\n        return {\"event\": ret_pdu.get_pdu_json(time_now)}\n\n    async def on_send_join_request(\n        self, origin: str, content: JsonDict\n    ) -> Dict[str, Any]:\n        logger.debug(\"on_send_join_request: content: %s\", content)\n\n        assert_params_in_dict(content, [\"room_id\"])\n        room_version = await self.store.get_room_version(content[\"room_id\"])\n        pdu = event_from_pdu_json(content, room_version)\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_join_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n\n        res_pdus = await self.handler.on_send_join_request(origin, pdu)\n        time_now = self._clock.time_msec()\n        return {\n            \"state\": [p.get_pdu_json(time_now) for p in res_pdus[\"state\"]],\n            \"auth_chain\": [p.get_pdu_json(time_now) for p in res_pdus[\"auth_chain\"]],\n        }\n\n    async def on_make_leave_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> Dict[str, Any]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)\n\n        room_version = await self.store.get_room_version_id(room_id)\n\n        time_now = self._clock.time_msec()\n        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}\n\n    async def on_send_leave_request(self, origin: str, content: JsonDict) -> dict:\n        logger.debug(\"on_send_leave_request: content: %s\", content)\n\n        assert_params_in_dict(content, [\"room_id\"])\n        room_version = await self.store.get_room_version(content[\"room_id\"])\n        pdu = event_from_pdu_json(content, room_version)\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n\n        await self.handler.on_send_leave_request(origin, pdu)\n        return {}\n\n    async def on_event_auth(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            time_now = self._clock.time_msec()\n            auth_pdus = await self.handler.on_event_auth(event_id)\n            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}\n        return 200, res\n\n    @log_function\n    async def on_query_client_keys(\n        self, origin: str, content: Dict[str, str]\n    ) -> Tuple[int, Dict[str, Any]]:\n        return await self.on_query_request(\"client_keys\", content)\n\n    async def on_query_user_devices(\n        self, origin: str, user_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        keys = await self.device_handler.on_federation_query_user_devices(user_id)\n        return 200, keys\n\n    @trace\n    async def on_claim_client_keys(\n        self, origin: str, content: JsonDict\n    ) -> Dict[str, Any]:\n        query = []\n        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():\n            for device_id, algorithm in device_keys.items():\n                query.append((user_id, device_id, algorithm))\n\n        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})\n        results = await self.store.claim_e2e_one_time_keys(query)\n\n        json_result = {}  # type: Dict[str, Dict[str, dict]]\n        for user_id, device_keys in results.items():\n            for device_id, keys in device_keys.items():\n                for key_id, json_str in keys.items():\n                    json_result.setdefault(user_id, {})[device_id] = {\n                        key_id: json_decoder.decode(json_str)\n                    }\n\n        logger.info(\n            \"Claimed one-time-keys: %s\",\n            \",\".join(\n                (\n                    \"%s for %s:%s\" % (key_id, user_id, device_id)\n                    for user_id, user_keys in json_result.items()\n                    for device_id, device_keys in user_keys.items()\n                    for key_id, _ in device_keys.items()\n                )\n            ),\n        )\n\n        return {\"one_time_keys\": json_result}\n\n    async def on_get_missing_events(\n        self,\n        origin: str,\n        room_id: str,\n        earliest_events: List[str],\n        latest_events: List[str],\n        limit: int,\n    ) -> Dict[str, list]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            logger.debug(\n                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"\n                \" limit: %d\",\n                earliest_events,\n                latest_events,\n                limit,\n            )\n\n            missing_events = await self.handler.on_get_missing_events(\n                origin, room_id, earliest_events, latest_events, limit\n            )\n\n            if len(missing_events) < 5:\n                logger.debug(\n                    \"Returning %d events: %r\", len(missing_events), missing_events\n                )\n            else:\n                logger.debug(\"Returning %d events\", len(missing_events))\n\n            time_now = self._clock.time_msec()\n\n        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}\n\n    @log_function\n    async def on_openid_userinfo(self, token: str) -> Optional[str]:\n        ts_now_ms = self._clock.time_msec()\n        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)\n\n    def _transaction_from_pdus(self, pdu_list: List[EventBase]) -> Transaction:\n        \"\"\"Returns a new Transaction containing the given PDUs suitable for\n        transmission.\n        \"\"\"\n        time_now = self._clock.time_msec()\n        pdus = [p.get_pdu_json(time_now) for p in pdu_list]\n        return Transaction(\n            origin=self.server_name,\n            pdus=pdus,\n            origin_server_ts=int(time_now),\n            destination=None,\n        )\n\n    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:\n        \"\"\" Process a PDU received in a federation /send/ transaction.\n\n        If the event is invalid, then this method throws a FederationError.\n        (The error will then be logged and sent back to the sender (which\n        probably won't do anything with it), and other events in the\n        transaction will be processed as normal).\n\n        It is likely that we'll then receive other events which refer to\n        this rejected_event in their prev_events, etc.  When that happens,\n        we'll attempt to fetch the rejected event again, which will presumably\n        fail, so those second-generation events will also get rejected.\n\n        Eventually, we get to the point where there are more than 10 events\n        between any new events and the original rejected event. Since we\n        only try to backfill 10 events deep on received pdu, we then accept the\n        new event, possibly introducing a discontinuity in the DAG, with new\n        forward extremities, so normal service is approximately returned,\n        until we try to backfill across the discontinuity.\n\n        Args:\n            origin: server which sent the pdu\n            pdu: received pdu\n\n        Raises: FederationError if the signatures / hash do not match, or\n            if the event was unacceptable for any other reason (eg, too large,\n            too many prev_events, couldn't find the prev_events)\n        \"\"\"\n        # check that it's actually being sent from a valid destination to\n        # workaround bug #1753 in 0.18.5 and 0.18.6\n        if origin != get_domain_from_id(pdu.sender):\n            # We continue to accept join events from any server; this is\n            # necessary for the federation join dance to work correctly.\n            # (When we join over federation, the \"helper\" server is\n            # responsible for sending out the join event, rather than the\n            # origin. See bug #1893. This is also true for some third party\n            # invites).\n            if not (\n                pdu.type == \"m.room.member\"\n                and pdu.content\n                and pdu.content.get(\"membership\", None)\n                in (Membership.JOIN, Membership.INVITE)\n            ):\n                logger.info(\n                    \"Discarding PDU %s from invalid origin %s\", pdu.event_id, origin\n                )\n                return\n            else:\n                logger.info(\"Accepting join PDU %s from %s\", pdu.event_id, origin)\n\n        # We've already checked that we know the room version by this point\n        room_version = await self.store.get_room_version(pdu.room_id)\n\n        # Check signature.\n        try:\n            pdu = await self._check_sigs_and_hash(room_version, pdu)\n        except SynapseError as e:\n            raise FederationError(\"ERROR\", e.code, e.msg, affected=pdu.event_id)\n\n        await self.handler.on_receive_pdu(origin, pdu, sent_to_us_directly=True)\n\n    def __str__(self):\n        return \"<ReplicationLayer(%s)>\" % self.server_name\n\n    async def exchange_third_party_invite(\n        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict\n    ):\n        ret = await self.handler.exchange_third_party_invite(\n            sender_user_id, target_user_id, room_id, signed\n        )\n        return ret\n\n    async def on_exchange_third_party_invite_request(self, event_dict: Dict):\n        ret = await self.handler.on_exchange_third_party_invite_request(event_dict)\n        return ret\n\n    async def check_server_matches_acl(self, server_name: str, room_id: str):\n        \"\"\"Check if the given server is allowed by the server ACLs in the room\n\n        Args:\n            server_name: name of server, *without any port part*\n            room_id: ID of the room to check\n\n        Raises:\n            AuthError if the server does not match the ACL\n        \"\"\"\n        state_ids = await self.store.get_current_state_ids(room_id)\n        acl_event_id = state_ids.get((EventTypes.ServerACL, \"\"))\n\n        if not acl_event_id:\n            return\n\n        acl_event = await self.store.get_event(acl_event_id)\n        if server_matches_acl_event(server_name, acl_event):\n            return\n\n        raise AuthError(code=403, msg=\"Server is banned from room\")\n\n\ndef server_matches_acl_event(server_name: str, acl_event: EventBase) -> bool:\n    \"\"\"Check if the given server is allowed by the ACL event\n\n    Args:\n        server_name: name of server, without any port part\n        acl_event: m.room.server_acl event\n\n    Returns:\n        True if this server is allowed by the ACLs\n    \"\"\"\n    logger.debug(\"Checking %s against acl %s\", server_name, acl_event.content)\n\n    # first of all, check if literal IPs are blocked, and if so, whether the\n    # server name is a literal IP\n    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)\n    if not isinstance(allow_ip_literals, bool):\n        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")\n        allow_ip_literals = True\n    if not allow_ip_literals:\n        # check for ipv6 literals. These start with '['.\n        if server_name[0] == \"[\":\n            return False\n\n        # check for ipv4 literals. We can just lift the routine from twisted.\n        if isIPAddress(server_name):\n            return False\n\n    # next,  check the deny list\n    deny = acl_event.content.get(\"deny\", [])\n    if not isinstance(deny, (list, tuple)):\n        logger.warning(\"Ignoring non-list deny ACL %s\", deny)\n        deny = []\n    for e in deny:\n        if _acl_entry_matches(server_name, e):\n            # logger.info(\"%s matched deny rule %s\", server_name, e)\n            return False\n\n    # then the allow list.\n    allow = acl_event.content.get(\"allow\", [])\n    if not isinstance(allow, (list, tuple)):\n        logger.warning(\"Ignoring non-list allow ACL %s\", allow)\n        allow = []\n    for e in allow:\n        if _acl_entry_matches(server_name, e):\n            # logger.info(\"%s matched allow rule %s\", server_name, e)\n            return True\n\n    # everything else should be rejected.\n    # logger.info(\"%s fell through\", server_name)\n    return False\n\n\ndef _acl_entry_matches(server_name: str, acl_entry: Any) -> bool:\n    if not isinstance(acl_entry, str):\n        logger.warning(\n            \"Ignoring non-str ACL entry '%s' (is %s)\", acl_entry, type(acl_entry)\n        )\n        return False\n    regex = glob_to_regex(acl_entry)\n    return bool(regex.match(server_name))\n\n\nclass FederationHandlerRegistry:\n    \"\"\"Allows classes to register themselves as handlers for a given EDU or\n    query type for incoming federation traffic.\n    \"\"\"\n\n    def __init__(self, hs: \"HomeServer\"):\n        self.config = hs.config\n        self.clock = hs.get_clock()\n        self._instance_name = hs.get_instance_name()\n\n        # These are safe to load in monolith mode, but will explode if we try\n        # and use them. However we have guards before we use them to ensure that\n        # we don't route to ourselves, and in monolith mode that will always be\n        # the case.\n        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)\n        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)\n\n        self.edu_handlers = (\n            {}\n        )  # type: Dict[str, Callable[[str, dict], Awaitable[None]]]\n        self.query_handlers = {}  # type: Dict[str, Callable[[dict], Awaitable[None]]]\n\n        # Map from type to instance name that we should route EDU handling to.\n        self._edu_type_to_instance = {}  # type: Dict[str, str]\n\n    def register_edu_handler(\n        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]\n    ):\n        \"\"\"Sets the handler callable that will be used to handle an incoming\n        federation EDU of the given type.\n\n        Args:\n            edu_type: The type of the incoming EDU to register handler for\n            handler: A callable invoked on incoming EDU\n                of the given type. The arguments are the origin server name and\n                the EDU contents.\n        \"\"\"\n        if edu_type in self.edu_handlers:\n            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))\n\n        logger.info(\"Registering federation EDU handler for %r\", edu_type)\n\n        self.edu_handlers[edu_type] = handler\n\n    def register_query_handler(\n        self, query_type: str, handler: Callable[[dict], defer.Deferred]\n    ):\n        \"\"\"Sets the handler callable that will be used to handle an incoming\n        federation query of the given type.\n\n        Args:\n            query_type: Category name of the query, which should match\n                the string used by make_query.\n            handler: Invoked to handle\n                incoming queries of this type. The return will be yielded\n                on and the result used as the response to the query request.\n        \"\"\"\n        if query_type in self.query_handlers:\n            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))\n\n        logger.info(\"Registering federation query handler for %r\", query_type)\n\n        self.query_handlers[query_type] = handler\n\n    def register_instance_for_edu(self, edu_type: str, instance_name: str):\n        \"\"\"Register that the EDU handler is on a different instance than master.\n        \"\"\"\n        self._edu_type_to_instance[edu_type] = instance_name\n\n    async def on_edu(self, edu_type: str, origin: str, content: dict):\n        if not self.config.use_presence and edu_type == \"m.presence\":\n            return\n\n        # Check if we have a handler on this instance\n        handler = self.edu_handlers.get(edu_type)\n        if handler:\n            with start_active_span_from_edu(content, \"handle_edu\"):\n                try:\n                    await handler(origin, content)\n                except SynapseError as e:\n                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)\n                except Exception:\n                    logger.exception(\"Failed to handle edu %r\", edu_type)\n            return\n\n        # Check if we can route it somewhere else that isn't us\n        route_to = self._edu_type_to_instance.get(edu_type, \"master\")\n        if route_to != self._instance_name:\n            try:\n                await self._send_edu(\n                    instance_name=route_to,\n                    edu_type=edu_type,\n                    origin=origin,\n                    content=content,\n                )\n            except SynapseError as e:\n                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)\n            except Exception:\n                logger.exception(\"Failed to handle edu %r\", edu_type)\n            return\n\n        # Oh well, let's just log and move on.\n        logger.warning(\"No handler registered for EDU type %s\", edu_type)\n\n    async def on_query(self, query_type: str, args: dict):\n        handler = self.query_handlers.get(query_type)\n        if handler:\n            return await handler(args)\n\n        # Check if we can route it somewhere else that isn't us\n        if self._instance_name == \"master\":\n            return await self._get_query_client(query_type=query_type, args=args)\n\n        # Uh oh, no handler! Let's raise an exception so the request returns an\n        # error.\n        logger.warning(\"No handler registered for query type %s\", query_type)\n        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2015, 2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n# Copyright 2019 Matrix.org Federation C.I.C\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Tuple,\n    Union,\n)\n\nfrom prometheus_client import Counter, Gauge, Histogram\n\nfrom twisted.internet import defer\nfrom twisted.internet.abstract import isIPAddress\nfrom twisted.python import failure\n\nfrom synapse.api.constants import EventTypes, Membership\nfrom synapse.api.errors import (\n    AuthError,\n    Codes,\n    FederationError,\n    IncompatibleRoomVersionError,\n    NotFoundError,\n    SynapseError,\n    UnsupportedRoomVersionError,\n)\nfrom synapse.api.room_versions import KNOWN_ROOM_VERSIONS\nfrom synapse.events import EventBase\nfrom synapse.federation.federation_base import FederationBase, event_from_pdu_json\nfrom synapse.federation.persistence import TransactionActions\nfrom synapse.federation.units import Edu, Transaction\nfrom synapse.http.endpoint import parse_server_name\nfrom synapse.http.servlet import assert_params_in_dict\nfrom synapse.logging.context import (\n    make_deferred_yieldable,\n    nested_logging_context,\n    run_in_background,\n)\nfrom synapse.logging.opentracing import log_kv, start_active_span_from_edu, trace\nfrom synapse.logging.utils import log_function\nfrom synapse.replication.http.federation import (\n    ReplicationFederationSendEduRestServlet,\n    ReplicationGetQueryRestServlet,\n)\nfrom synapse.types import JsonDict, get_domain_from_id\nfrom synapse.util import glob_to_regex, json_decoder, unwrapFirstError\nfrom synapse.util.async_helpers import Linearizer, concurrently_execute\nfrom synapse.util.caches.response_cache import ResponseCache\n\nif TYPE_CHECKING:\n    from synapse.server import HomeServer\n\n# when processing incoming transactions, we try to handle multiple rooms in\n# parallel, up to this limit.\nTRANSACTION_CONCURRENCY_LIMIT = 10\n\nlogger = logging.getLogger(__name__)\n\nreceived_pdus_counter = Counter(\"synapse_federation_server_received_pdus\", \"\")\n\nreceived_edus_counter = Counter(\"synapse_federation_server_received_edus\", \"\")\n\nreceived_queries_counter = Counter(\n    \"synapse_federation_server_received_queries\", \"\", [\"type\"]\n)\n\npdu_process_time = Histogram(\n    \"synapse_federation_server_pdu_process_time\", \"Time taken to process an event\",\n)\n\n\nlast_pdu_age_metric = Gauge(\n    \"synapse_federation_last_received_pdu_age\",\n    \"The age (in seconds) of the last PDU successfully received from the given domain\",\n    labelnames=(\"server_name\",),\n)\n\n\nclass FederationServer(FederationBase):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        self.auth = hs.get_auth()\n        self.handler = hs.get_federation_handler()\n        self.state = hs.get_state_handler()\n\n        self.device_handler = hs.get_device_handler()\n\n        # Ensure the following handlers are loaded since they register callbacks\n        # with FederationHandlerRegistry.\n        hs.get_directory_handler()\n\n        self._federation_ratelimiter = hs.get_federation_ratelimiter()\n\n        self._server_linearizer = Linearizer(\"fed_server\")\n        self._transaction_linearizer = Linearizer(\"fed_txn_handler\")\n\n        # We cache results for transaction with the same ID\n        self._transaction_resp_cache = ResponseCache(\n            hs, \"fed_txn_handler\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n\n        self.transaction_actions = TransactionActions(self.store)\n\n        self.registry = hs.get_federation_registry()\n\n        # We cache responses to state queries, as they take a while and often\n        # come in waves.\n        self._state_resp_cache = ResponseCache(\n            hs, \"state_resp\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n        self._state_ids_resp_cache = ResponseCache(\n            hs, \"state_ids_resp\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n\n        self._federation_metrics_domains = (\n            hs.get_config().federation.federation_metrics_domains\n        )\n\n    async def on_backfill_request(\n        self, origin: str, room_id: str, versions: List[str], limit: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            pdus = await self.handler.on_backfill_request(\n                origin, room_id, versions, limit\n            )\n\n            res = self._transaction_from_pdus(pdus).get_dict()\n\n        return 200, res\n\n    async def on_incoming_transaction(\n        self, origin: str, transaction_data: JsonDict\n    ) -> Tuple[int, Dict[str, Any]]:\n        # keep this as early as possible to make the calculated origin ts as\n        # accurate as possible.\n        request_time = self._clock.time_msec()\n\n        transaction = Transaction(**transaction_data)\n        transaction_id = transaction.transaction_id  # type: ignore\n\n        if not transaction_id:\n            raise Exception(\"Transaction missing transaction_id\")\n\n        logger.debug(\"[%s] Got transaction\", transaction_id)\n\n        # We wrap in a ResponseCache so that we de-duplicate retried\n        # transactions.\n        return await self._transaction_resp_cache.wrap(\n            (origin, transaction_id),\n            self._on_incoming_transaction_inner,\n            origin,\n            transaction,\n            request_time,\n        )\n\n    async def _on_incoming_transaction_inner(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        # Use a linearizer to ensure that transactions from a remote are\n        # processed in order.\n        with await self._transaction_linearizer.queue(origin):\n            # We rate limit here *after* we've queued up the incoming requests,\n            # so that we don't fill up the ratelimiter with blocked requests.\n            #\n            # This is important as the ratelimiter allows N concurrent requests\n            # at a time, and only starts ratelimiting if there are more requests\n            # than that being processed at a time. If we queued up requests in\n            # the linearizer/response cache *after* the ratelimiting then those\n            # queued up requests would count as part of the allowed limit of N\n            # concurrent requests.\n            with self._federation_ratelimiter.ratelimit(origin) as d:\n                await d\n\n                result = await self._handle_incoming_transaction(\n                    origin, transaction, request_time\n                )\n\n        return result\n\n    async def _handle_incoming_transaction(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        \"\"\" Process an incoming transaction and return the HTTP response\n\n        Args:\n            origin: the server making the request\n            transaction: incoming transaction\n            request_time: timestamp that the HTTP request arrived at\n\n        Returns:\n            HTTP response code and body\n        \"\"\"\n        response = await self.transaction_actions.have_responded(origin, transaction)\n\n        if response:\n            logger.debug(\n                \"[%s] We've already responded to this request\",\n                transaction.transaction_id,  # type: ignore\n            )\n            return response\n\n        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)  # type: ignore\n\n        # Reject if PDU count > 50 or EDU count > 100\n        if len(transaction.pdus) > 50 or (  # type: ignore\n            hasattr(transaction, \"edus\") and len(transaction.edus) > 100  # type: ignore\n        ):\n\n            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")\n\n            response = {}\n            await self.transaction_actions.set_response(\n                origin, transaction, 400, response\n            )\n            return 400, response\n\n        # We process PDUs and EDUs in parallel. This is important as we don't\n        # want to block things like to device messages from reaching clients\n        # behind the potentially expensive handling of PDUs.\n        pdu_results, _ = await make_deferred_yieldable(\n            defer.gatherResults(\n                [\n                    run_in_background(\n                        self._handle_pdus_in_txn, origin, transaction, request_time\n                    ),\n                    run_in_background(self._handle_edus_in_txn, origin, transaction),\n                ],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        response = {\"pdus\": pdu_results}\n\n        logger.debug(\"Returning: %s\", str(response))\n\n        await self.transaction_actions.set_response(origin, transaction, 200, response)\n        return 200, response\n\n    async def _handle_pdus_in_txn(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Dict[str, dict]:\n        \"\"\"Process the PDUs in a received transaction.\n\n        Args:\n            origin: the server making the request\n            transaction: incoming transaction\n            request_time: timestamp that the HTTP request arrived at\n\n        Returns:\n            A map from event ID of a processed PDU to any errors we should\n            report back to the sending server.\n        \"\"\"\n\n        received_pdus_counter.inc(len(transaction.pdus))  # type: ignore\n\n        origin_host, _ = parse_server_name(origin)\n\n        pdus_by_room = {}  # type: Dict[str, List[EventBase]]\n\n        newest_pdu_ts = 0\n\n        for p in transaction.pdus:  # type: ignore\n            # FIXME (richardv): I don't think this works:\n            #  https://github.com/matrix-org/synapse/issues/8429\n            if \"unsigned\" in p:\n                unsigned = p[\"unsigned\"]\n                if \"age\" in unsigned:\n                    p[\"age\"] = unsigned[\"age\"]\n            if \"age\" in p:\n                p[\"age_ts\"] = request_time - int(p[\"age\"])\n                del p[\"age\"]\n\n            # We try and pull out an event ID so that if later checks fail we\n            # can log something sensible. We don't mandate an event ID here in\n            # case future event formats get rid of the key.\n            possible_event_id = p.get(\"event_id\", \"<Unknown>\")\n\n            # Now we get the room ID so that we can check that we know the\n            # version of the room.\n            room_id = p.get(\"room_id\")\n            if not room_id:\n                logger.info(\n                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",\n                    possible_event_id,\n                )\n                continue\n\n            try:\n                room_version = await self.store.get_room_version(room_id)\n            except NotFoundError:\n                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)\n                continue\n            except UnsupportedRoomVersionError as e:\n                # this can happen if support for a given room version is withdrawn,\n                # so that we still get events for said room.\n                logger.info(\"Ignoring PDU: %s\", e)\n                continue\n\n            event = event_from_pdu_json(p, room_version)\n            pdus_by_room.setdefault(room_id, []).append(event)\n\n            if event.origin_server_ts > newest_pdu_ts:\n                newest_pdu_ts = event.origin_server_ts\n\n        pdu_results = {}\n\n        # we can process different rooms in parallel (which is useful if they\n        # require callouts to other servers to fetch missing events), but\n        # impose a limit to avoid going too crazy with ram/cpu.\n\n        async def process_pdus_for_room(room_id: str):\n            logger.debug(\"Processing PDUs for %s\", room_id)\n            try:\n                await self.check_server_matches_acl(origin_host, room_id)\n            except AuthError as e:\n                logger.warning(\"Ignoring PDUs for room %s from banned server\", room_id)\n                for pdu in pdus_by_room[room_id]:\n                    event_id = pdu.event_id\n                    pdu_results[event_id] = e.error_dict()\n                return\n\n            for pdu in pdus_by_room[room_id]:\n                event_id = pdu.event_id\n                with pdu_process_time.time():\n                    with nested_logging_context(event_id):\n                        try:\n                            await self._handle_received_pdu(origin, pdu)\n                            pdu_results[event_id] = {}\n                        except FederationError as e:\n                            logger.warning(\"Error handling PDU %s: %s\", event_id, e)\n                            pdu_results[event_id] = {\"error\": str(e)}\n                        except Exception as e:\n                            f = failure.Failure()\n                            pdu_results[event_id] = {\"error\": str(e)}\n                            logger.error(\n                                \"Failed to handle PDU %s\",\n                                event_id,\n                                exc_info=(f.type, f.value, f.getTracebackObject()),\n                            )\n\n        await concurrently_execute(\n            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT\n        )\n\n        if newest_pdu_ts and origin in self._federation_metrics_domains:\n            newest_pdu_age = self._clock.time_msec() - newest_pdu_ts\n            last_pdu_age_metric.labels(server_name=origin).set(newest_pdu_age / 1000)\n\n        return pdu_results\n\n    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction):\n        \"\"\"Process the EDUs in a received transaction.\n        \"\"\"\n\n        async def _process_edu(edu_dict):\n            received_edus_counter.inc()\n\n            edu = Edu(\n                origin=origin,\n                destination=self.server_name,\n                edu_type=edu_dict[\"edu_type\"],\n                content=edu_dict[\"content\"],\n            )\n            await self.registry.on_edu(edu.edu_type, origin, edu.content)\n\n        await concurrently_execute(\n            _process_edu,\n            getattr(transaction, \"edus\", []),\n            TRANSACTION_CONCURRENCY_LIMIT,\n        )\n\n    async def on_room_state_request(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # we grab the linearizer to protect ourselves from servers which hammer\n        # us. In theory we might already have the response to this query\n        # in the cache so we could return it without waiting for the linearizer\n        # - but that's non-trivial to get right, and anyway somewhat defeats\n        # the point of the linearizer.\n        with (await self._server_linearizer.queue((origin, room_id))):\n            resp = dict(\n                await self._state_resp_cache.wrap(\n                    (room_id, event_id),\n                    self._on_context_state_request_compute,\n                    room_id,\n                    event_id,\n                )\n            )\n\n        room_version = await self.store.get_room_version_id(room_id)\n        resp[\"room_version\"] = room_version\n\n        return 200, resp\n\n    async def on_state_ids_request(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        if not event_id:\n            raise NotImplementedError(\"Specify an event\")\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        resp = await self._state_ids_resp_cache.wrap(\n            (room_id, event_id), self._on_state_ids_request_compute, room_id, event_id,\n        )\n\n        return 200, resp\n\n    async def _on_state_ids_request_compute(self, room_id, event_id):\n        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)\n        auth_chain_ids = await self.store.get_auth_chain_ids(state_ids)\n        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": auth_chain_ids}\n\n    async def _on_context_state_request_compute(\n        self, room_id: str, event_id: str\n    ) -> Dict[str, list]:\n        if event_id:\n            pdus = await self.handler.get_state_for_pdu(room_id, event_id)\n        else:\n            pdus = (await self.state.get_current_state(room_id)).values()\n\n        auth_chain = await self.store.get_auth_chain([pdu.event_id for pdu in pdus])\n\n        return {\n            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],\n            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],\n        }\n\n    async def on_pdu_request(\n        self, origin: str, event_id: str\n    ) -> Tuple[int, Union[JsonDict, str]]:\n        pdu = await self.handler.get_persisted_pdu(origin, event_id)\n\n        if pdu:\n            return 200, self._transaction_from_pdus([pdu]).get_dict()\n        else:\n            return 404, \"\"\n\n    async def on_query_request(\n        self, query_type: str, args: Dict[str, str]\n    ) -> Tuple[int, Dict[str, Any]]:\n        received_queries_counter.labels(query_type).inc()\n        resp = await self.registry.on_query(query_type, args)\n        return 200, resp\n\n    async def on_make_join_request(\n        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]\n    ) -> Dict[str, Any]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        room_version = await self.store.get_room_version_id(room_id)\n        if room_version not in supported_versions:\n            logger.warning(\n                \"Room version %s not in %s\", room_version, supported_versions\n            )\n            raise IncompatibleRoomVersionError(room_version=room_version)\n\n        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)\n        time_now = self._clock.time_msec()\n        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}\n\n    async def on_invite_request(\n        self, origin: str, content: JsonDict, room_version_id: str\n    ) -> Dict[str, Any]:\n        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)\n        if not room_version:\n            raise SynapseError(\n                400,\n                \"Homeserver does not support this room version\",\n                Codes.UNSUPPORTED_ROOM_VERSION,\n            )\n\n        pdu = event_from_pdu_json(content, room_version)\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)\n        time_now = self._clock.time_msec()\n        return {\"event\": ret_pdu.get_pdu_json(time_now)}\n\n    async def on_send_join_request(\n        self, origin: str, content: JsonDict\n    ) -> Dict[str, Any]:\n        logger.debug(\"on_send_join_request: content: %s\", content)\n\n        assert_params_in_dict(content, [\"room_id\"])\n        room_version = await self.store.get_room_version(content[\"room_id\"])\n        pdu = event_from_pdu_json(content, room_version)\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_join_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n\n        res_pdus = await self.handler.on_send_join_request(origin, pdu)\n        time_now = self._clock.time_msec()\n        return {\n            \"state\": [p.get_pdu_json(time_now) for p in res_pdus[\"state\"]],\n            \"auth_chain\": [p.get_pdu_json(time_now) for p in res_pdus[\"auth_chain\"]],\n        }\n\n    async def on_make_leave_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> Dict[str, Any]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)\n\n        room_version = await self.store.get_room_version_id(room_id)\n\n        time_now = self._clock.time_msec()\n        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}\n\n    async def on_send_leave_request(self, origin: str, content: JsonDict) -> dict:\n        logger.debug(\"on_send_leave_request: content: %s\", content)\n\n        assert_params_in_dict(content, [\"room_id\"])\n        room_version = await self.store.get_room_version(content[\"room_id\"])\n        pdu = event_from_pdu_json(content, room_version)\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n\n        await self.handler.on_send_leave_request(origin, pdu)\n        return {}\n\n    async def on_event_auth(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            time_now = self._clock.time_msec()\n            auth_pdus = await self.handler.on_event_auth(event_id)\n            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}\n        return 200, res\n\n    @log_function\n    async def on_query_client_keys(\n        self, origin: str, content: Dict[str, str]\n    ) -> Tuple[int, Dict[str, Any]]:\n        return await self.on_query_request(\"client_keys\", content)\n\n    async def on_query_user_devices(\n        self, origin: str, user_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        keys = await self.device_handler.on_federation_query_user_devices(user_id)\n        return 200, keys\n\n    @trace\n    async def on_claim_client_keys(\n        self, origin: str, content: JsonDict\n    ) -> Dict[str, Any]:\n        query = []\n        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():\n            for device_id, algorithm in device_keys.items():\n                query.append((user_id, device_id, algorithm))\n\n        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})\n        results = await self.store.claim_e2e_one_time_keys(query)\n\n        json_result = {}  # type: Dict[str, Dict[str, dict]]\n        for user_id, device_keys in results.items():\n            for device_id, keys in device_keys.items():\n                for key_id, json_str in keys.items():\n                    json_result.setdefault(user_id, {})[device_id] = {\n                        key_id: json_decoder.decode(json_str)\n                    }\n\n        logger.info(\n            \"Claimed one-time-keys: %s\",\n            \",\".join(\n                (\n                    \"%s for %s:%s\" % (key_id, user_id, device_id)\n                    for user_id, user_keys in json_result.items()\n                    for device_id, device_keys in user_keys.items()\n                    for key_id, _ in device_keys.items()\n                )\n            ),\n        )\n\n        return {\"one_time_keys\": json_result}\n\n    async def on_get_missing_events(\n        self,\n        origin: str,\n        room_id: str,\n        earliest_events: List[str],\n        latest_events: List[str],\n        limit: int,\n    ) -> Dict[str, list]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            logger.debug(\n                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"\n                \" limit: %d\",\n                earliest_events,\n                latest_events,\n                limit,\n            )\n\n            missing_events = await self.handler.on_get_missing_events(\n                origin, room_id, earliest_events, latest_events, limit\n            )\n\n            if len(missing_events) < 5:\n                logger.debug(\n                    \"Returning %d events: %r\", len(missing_events), missing_events\n                )\n            else:\n                logger.debug(\"Returning %d events\", len(missing_events))\n\n            time_now = self._clock.time_msec()\n\n        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}\n\n    @log_function\n    async def on_openid_userinfo(self, token: str) -> Optional[str]:\n        ts_now_ms = self._clock.time_msec()\n        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)\n\n    def _transaction_from_pdus(self, pdu_list: List[EventBase]) -> Transaction:\n        \"\"\"Returns a new Transaction containing the given PDUs suitable for\n        transmission.\n        \"\"\"\n        time_now = self._clock.time_msec()\n        pdus = [p.get_pdu_json(time_now) for p in pdu_list]\n        return Transaction(\n            origin=self.server_name,\n            pdus=pdus,\n            origin_server_ts=int(time_now),\n            destination=None,\n        )\n\n    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:\n        \"\"\" Process a PDU received in a federation /send/ transaction.\n\n        If the event is invalid, then this method throws a FederationError.\n        (The error will then be logged and sent back to the sender (which\n        probably won't do anything with it), and other events in the\n        transaction will be processed as normal).\n\n        It is likely that we'll then receive other events which refer to\n        this rejected_event in their prev_events, etc.  When that happens,\n        we'll attempt to fetch the rejected event again, which will presumably\n        fail, so those second-generation events will also get rejected.\n\n        Eventually, we get to the point where there are more than 10 events\n        between any new events and the original rejected event. Since we\n        only try to backfill 10 events deep on received pdu, we then accept the\n        new event, possibly introducing a discontinuity in the DAG, with new\n        forward extremities, so normal service is approximately returned,\n        until we try to backfill across the discontinuity.\n\n        Args:\n            origin: server which sent the pdu\n            pdu: received pdu\n\n        Raises: FederationError if the signatures / hash do not match, or\n            if the event was unacceptable for any other reason (eg, too large,\n            too many prev_events, couldn't find the prev_events)\n        \"\"\"\n        # check that it's actually being sent from a valid destination to\n        # workaround bug #1753 in 0.18.5 and 0.18.6\n        if origin != get_domain_from_id(pdu.sender):\n            # We continue to accept join events from any server; this is\n            # necessary for the federation join dance to work correctly.\n            # (When we join over federation, the \"helper\" server is\n            # responsible for sending out the join event, rather than the\n            # origin. See bug #1893. This is also true for some third party\n            # invites).\n            if not (\n                pdu.type == \"m.room.member\"\n                and pdu.content\n                and pdu.content.get(\"membership\", None)\n                in (Membership.JOIN, Membership.INVITE)\n            ):\n                logger.info(\n                    \"Discarding PDU %s from invalid origin %s\", pdu.event_id, origin\n                )\n                return\n            else:\n                logger.info(\"Accepting join PDU %s from %s\", pdu.event_id, origin)\n\n        # We've already checked that we know the room version by this point\n        room_version = await self.store.get_room_version(pdu.room_id)\n\n        # Check signature.\n        try:\n            pdu = await self._check_sigs_and_hash(room_version, pdu)\n        except SynapseError as e:\n            raise FederationError(\"ERROR\", e.code, e.msg, affected=pdu.event_id)\n\n        await self.handler.on_receive_pdu(origin, pdu, sent_to_us_directly=True)\n\n    def __str__(self):\n        return \"<ReplicationLayer(%s)>\" % self.server_name\n\n    async def exchange_third_party_invite(\n        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict\n    ):\n        ret = await self.handler.exchange_third_party_invite(\n            sender_user_id, target_user_id, room_id, signed\n        )\n        return ret\n\n    async def on_exchange_third_party_invite_request(self, event_dict: Dict):\n        ret = await self.handler.on_exchange_third_party_invite_request(event_dict)\n        return ret\n\n    async def check_server_matches_acl(self, server_name: str, room_id: str):\n        \"\"\"Check if the given server is allowed by the server ACLs in the room\n\n        Args:\n            server_name: name of server, *without any port part*\n            room_id: ID of the room to check\n\n        Raises:\n            AuthError if the server does not match the ACL\n        \"\"\"\n        state_ids = await self.store.get_current_state_ids(room_id)\n        acl_event_id = state_ids.get((EventTypes.ServerACL, \"\"))\n\n        if not acl_event_id:\n            return\n\n        acl_event = await self.store.get_event(acl_event_id)\n        if server_matches_acl_event(server_name, acl_event):\n            return\n\n        raise AuthError(code=403, msg=\"Server is banned from room\")\n\n\ndef server_matches_acl_event(server_name: str, acl_event: EventBase) -> bool:\n    \"\"\"Check if the given server is allowed by the ACL event\n\n    Args:\n        server_name: name of server, without any port part\n        acl_event: m.room.server_acl event\n\n    Returns:\n        True if this server is allowed by the ACLs\n    \"\"\"\n    logger.debug(\"Checking %s against acl %s\", server_name, acl_event.content)\n\n    # first of all, check if literal IPs are blocked, and if so, whether the\n    # server name is a literal IP\n    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)\n    if not isinstance(allow_ip_literals, bool):\n        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")\n        allow_ip_literals = True\n    if not allow_ip_literals:\n        # check for ipv6 literals. These start with '['.\n        if server_name[0] == \"[\":\n            return False\n\n        # check for ipv4 literals. We can just lift the routine from twisted.\n        if isIPAddress(server_name):\n            return False\n\n    # next,  check the deny list\n    deny = acl_event.content.get(\"deny\", [])\n    if not isinstance(deny, (list, tuple)):\n        logger.warning(\"Ignoring non-list deny ACL %s\", deny)\n        deny = []\n    for e in deny:\n        if _acl_entry_matches(server_name, e):\n            # logger.info(\"%s matched deny rule %s\", server_name, e)\n            return False\n\n    # then the allow list.\n    allow = acl_event.content.get(\"allow\", [])\n    if not isinstance(allow, (list, tuple)):\n        logger.warning(\"Ignoring non-list allow ACL %s\", allow)\n        allow = []\n    for e in allow:\n        if _acl_entry_matches(server_name, e):\n            # logger.info(\"%s matched allow rule %s\", server_name, e)\n            return True\n\n    # everything else should be rejected.\n    # logger.info(\"%s fell through\", server_name)\n    return False\n\n\ndef _acl_entry_matches(server_name: str, acl_entry: Any) -> bool:\n    if not isinstance(acl_entry, str):\n        logger.warning(\n            \"Ignoring non-str ACL entry '%s' (is %s)\", acl_entry, type(acl_entry)\n        )\n        return False\n    regex = glob_to_regex(acl_entry)\n    return bool(regex.match(server_name))\n\n\nclass FederationHandlerRegistry:\n    \"\"\"Allows classes to register themselves as handlers for a given EDU or\n    query type for incoming federation traffic.\n    \"\"\"\n\n    def __init__(self, hs: \"HomeServer\"):\n        self.config = hs.config\n        self.http_client = hs.get_simple_http_client()\n        self.clock = hs.get_clock()\n        self._instance_name = hs.get_instance_name()\n\n        # These are safe to load in monolith mode, but will explode if we try\n        # and use them. However we have guards before we use them to ensure that\n        # we don't route to ourselves, and in monolith mode that will always be\n        # the case.\n        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)\n        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)\n\n        self.edu_handlers = (\n            {}\n        )  # type: Dict[str, Callable[[str, dict], Awaitable[None]]]\n        self.query_handlers = {}  # type: Dict[str, Callable[[dict], Awaitable[None]]]\n\n        # Map from type to instance name that we should route EDU handling to.\n        self._edu_type_to_instance = {}  # type: Dict[str, str]\n\n    def register_edu_handler(\n        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]\n    ):\n        \"\"\"Sets the handler callable that will be used to handle an incoming\n        federation EDU of the given type.\n\n        Args:\n            edu_type: The type of the incoming EDU to register handler for\n            handler: A callable invoked on incoming EDU\n                of the given type. The arguments are the origin server name and\n                the EDU contents.\n        \"\"\"\n        if edu_type in self.edu_handlers:\n            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))\n\n        logger.info(\"Registering federation EDU handler for %r\", edu_type)\n\n        self.edu_handlers[edu_type] = handler\n\n    def register_query_handler(\n        self, query_type: str, handler: Callable[[dict], defer.Deferred]\n    ):\n        \"\"\"Sets the handler callable that will be used to handle an incoming\n        federation query of the given type.\n\n        Args:\n            query_type: Category name of the query, which should match\n                the string used by make_query.\n            handler: Invoked to handle\n                incoming queries of this type. The return will be yielded\n                on and the result used as the response to the query request.\n        \"\"\"\n        if query_type in self.query_handlers:\n            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))\n\n        logger.info(\"Registering federation query handler for %r\", query_type)\n\n        self.query_handlers[query_type] = handler\n\n    def register_instance_for_edu(self, edu_type: str, instance_name: str):\n        \"\"\"Register that the EDU handler is on a different instance than master.\n        \"\"\"\n        self._edu_type_to_instance[edu_type] = instance_name\n\n    async def on_edu(self, edu_type: str, origin: str, content: dict):\n        if not self.config.use_presence and edu_type == \"m.presence\":\n            return\n\n        # Check if we have a handler on this instance\n        handler = self.edu_handlers.get(edu_type)\n        if handler:\n            with start_active_span_from_edu(content, \"handle_edu\"):\n                try:\n                    await handler(origin, content)\n                except SynapseError as e:\n                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)\n                except Exception:\n                    logger.exception(\"Failed to handle edu %r\", edu_type)\n            return\n\n        # Check if we can route it somewhere else that isn't us\n        route_to = self._edu_type_to_instance.get(edu_type, \"master\")\n        if route_to != self._instance_name:\n            try:\n                await self._send_edu(\n                    instance_name=route_to,\n                    edu_type=edu_type,\n                    origin=origin,\n                    content=content,\n                )\n            except SynapseError as e:\n                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)\n            except Exception:\n                logger.exception(\"Failed to handle edu %r\", edu_type)\n            return\n\n        # Oh well, let's just log and move on.\n        logger.warning(\"No handler registered for EDU type %s\", edu_type)\n\n    async def on_query(self, query_type: str, args: dict):\n        handler = self.query_handlers.get(query_type)\n        if handler:\n            return await handler(args)\n\n        # Check if we can route it somewhere else that isn't us\n        if self._instance_name == \"master\":\n            return await self._get_query_client(query_type=query_type, args=args)\n\n        # Uh oh, no handler! Let's raise an exception so the request returns an\n        # error.\n        logger.warning(\"No handler registered for query type %s\", query_type)\n        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))\n", "patch": "@@ -845,7 +845,6 @@ class FederationHandlerRegistry:\n \n     def __init__(self, hs: \"HomeServer\"):\n         self.config = hs.config\n-        self.http_client = hs.get_simple_http_client()\n         self.clock = hs.get_clock()\n         self._instance_name = hs.get_instance_name()\n ", "file_path": "files/2021_2/20", "file_language": "py", "file_name": "synapse/federation/federation_server.py", "outdated_file_modify": 1, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class FederationServer(FederationBase):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        self.auth = hs.get_auth()\n        self.handler = hs.get_federation_handler()\n        self.state = hs.get_state_handler()\n\n        self.device_handler = hs.get_device_handler()\n\n        # Ensure the following handlers are loaded since they register callbacks\n        # with FederationHandlerRegistry.\n        hs.get_directory_handler()\n\n        self._federation_ratelimiter = hs.get_federation_ratelimiter()\n\n        self._server_linearizer = Linearizer(\"fed_server\")\n        self._transaction_linearizer = Linearizer(\"fed_txn_handler\")\n\n        # We cache results for transaction with the same ID\n        self._transaction_resp_cache = ResponseCache(\n            hs, \"fed_txn_handler\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n\n        self.transaction_actions = TransactionActions(self.store)\n\n        self.registry = hs.get_federation_registry()\n\n        # We cache responses to state queries, as they take a while and often\n        # come in waves.\n        self._state_resp_cache = ResponseCache(\n            hs, \"state_resp\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n        self._state_ids_resp_cache = ResponseCache(\n            hs, \"state_ids_resp\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n\n        self._federation_metrics_domains = (\n            hs.get_config().federation.federation_metrics_domains\n        )\n\n    async def on_backfill_request(\n        self, origin: str, room_id: str, versions: List[str], limit: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            pdus = await self.handler.on_backfill_request(\n                origin, room_id, versions, limit\n            )\n\n            res = self._transaction_from_pdus(pdus).get_dict()\n\n        return 200, res\n\n    async def on_incoming_transaction(\n        self, origin: str, transaction_data: JsonDict\n    ) -> Tuple[int, Dict[str, Any]]:\n        # keep this as early as possible to make the calculated origin ts as\n        # accurate as possible.\n        request_time = self._clock.time_msec()\n\n        transaction = Transaction(**transaction_data)\n        transaction_id = transaction.transaction_id  # type: ignore\n\n        if not transaction_id:\n            raise Exception(\"Transaction missing transaction_id\")\n\n        logger.debug(\"[%s] Got transaction\", transaction_id)\n\n        # We wrap in a ResponseCache so that we de-duplicate retried\n        # transactions.\n        return await self._transaction_resp_cache.wrap(\n            (origin, transaction_id),\n            self._on_incoming_transaction_inner,\n            origin,\n            transaction,\n            request_time,\n        )\n\n    async def _on_incoming_transaction_inner(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        # Use a linearizer to ensure that transactions from a remote are\n        # processed in order.\n        with await self._transaction_linearizer.queue(origin):\n            # We rate limit here *after* we've queued up the incoming requests,\n            # so that we don't fill up the ratelimiter with blocked requests.\n            #\n            # This is important as the ratelimiter allows N concurrent requests\n            # at a time, and only starts ratelimiting if there are more requests\n            # than that being processed at a time. If we queued up requests in\n            # the linearizer/response cache *after* the ratelimiting then those\n            # queued up requests would count as part of the allowed limit of N\n            # concurrent requests.\n            with self._federation_ratelimiter.ratelimit(origin) as d:\n                await d\n\n                result = await self._handle_incoming_transaction(\n                    origin, transaction, request_time\n                )\n\n        return result\n\n    async def _handle_incoming_transaction(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        \"\"\" Process an incoming transaction and return the HTTP response\n\n        Args:\n            origin: the server making the request\n            transaction: incoming transaction\n            request_time: timestamp that the HTTP request arrived at\n\n        Returns:\n            HTTP response code and body\n        \"\"\"\n        response = await self.transaction_actions.have_responded(origin, transaction)\n\n        if response:\n            logger.debug(\n                \"[%s] We've already responded to this request\",\n                transaction.transaction_id,  # type: ignore\n            )\n            return response\n\n        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)  # type: ignore\n\n        # Reject if PDU count > 50 or EDU count > 100\n        if len(transaction.pdus) > 50 or (  # type: ignore\n            hasattr(transaction, \"edus\") and len(transaction.edus) > 100  # type: ignore\n        ):\n\n            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")\n\n            response = {}\n            await self.transaction_actions.set_response(\n                origin, transaction, 400, response\n            )\n            return 400, response\n\n        # We process PDUs and EDUs in parallel. This is important as we don't\n        # want to block things like to device messages from reaching clients\n        # behind the potentially expensive handling of PDUs.\n        pdu_results, _ = await make_deferred_yieldable(\n            defer.gatherResults(\n                [\n                    run_in_background(\n                        self._handle_pdus_in_txn, origin, transaction, request_time\n                    ),\n                    run_in_background(self._handle_edus_in_txn, origin, transaction),\n                ],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        response = {\"pdus\": pdu_results}\n\n        logger.debug(\"Returning: %s\", str(response))\n\n        await self.transaction_actions.set_response(origin, transaction, 200, response)\n        return 200, response\n\n    async def _handle_pdus_in_txn(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Dict[str, dict]:\n        \"\"\"Process the PDUs in a received transaction.\n\n        Args:\n            origin: the server making the request\n            transaction: incoming transaction\n            request_time: timestamp that the HTTP request arrived at\n\n        Returns:\n            A map from event ID of a processed PDU to any errors we should\n            report back to the sending server.\n        \"\"\"\n\n        received_pdus_counter.inc(len(transaction.pdus))  # type: ignore\n\n        origin_host, _ = parse_server_name(origin)\n\n        pdus_by_room = {}  # type: Dict[str, List[EventBase]]\n\n        newest_pdu_ts = 0\n\n        for p in transaction.pdus:  # type: ignore\n            # FIXME (richardv): I don't think this works:\n            #  https://github.com/matrix-org/synapse/issues/8429\n            if \"unsigned\" in p:\n                unsigned = p[\"unsigned\"]\n                if \"age\" in unsigned:\n                    p[\"age\"] = unsigned[\"age\"]\n            if \"age\" in p:\n                p[\"age_ts\"] = request_time - int(p[\"age\"])\n                del p[\"age\"]\n\n            # We try and pull out an event ID so that if later checks fail we\n            # can log something sensible. We don't mandate an event ID here in\n            # case future event formats get rid of the key.\n            possible_event_id = p.get(\"event_id\", \"<Unknown>\")\n\n            # Now we get the room ID so that we can check that we know the\n            # version of the room.\n            room_id = p.get(\"room_id\")\n            if not room_id:\n                logger.info(\n                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",\n                    possible_event_id,\n                )\n                continue\n\n            try:\n                room_version = await self.store.get_room_version(room_id)\n            except NotFoundError:\n                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)\n                continue\n            except UnsupportedRoomVersionError as e:\n                # this can happen if support for a given room version is withdrawn,\n                # so that we still get events for said room.\n                logger.info(\"Ignoring PDU: %s\", e)\n                continue\n\n            event = event_from_pdu_json(p, room_version)\n            pdus_by_room.setdefault(room_id, []).append(event)\n\n            if event.origin_server_ts > newest_pdu_ts:\n                newest_pdu_ts = event.origin_server_ts\n\n        pdu_results = {}\n\n        # we can process different rooms in parallel (which is useful if they\n        # require callouts to other servers to fetch missing events), but\n        # impose a limit to avoid going too crazy with ram/cpu.\n\n        async def process_pdus_for_room(room_id: str):\n            logger.debug(\"Processing PDUs for %s\", room_id)\n            try:\n                await self.check_server_matches_acl(origin_host, room_id)\n            except AuthError as e:\n                logger.warning(\"Ignoring PDUs for room %s from banned server\", room_id)\n                for pdu in pdus_by_room[room_id]:\n                    event_id = pdu.event_id\n                    pdu_results[event_id] = e.error_dict()\n                return\n\n            for pdu in pdus_by_room[room_id]:\n                event_id = pdu.event_id\n                with pdu_process_time.time():\n                    with nested_logging_context(event_id):\n                        try:\n                            await self._handle_received_pdu(origin, pdu)\n                            pdu_results[event_id] = {}\n                        except FederationError as e:\n                            logger.warning(\"Error handling PDU %s: %s\", event_id, e)\n                            pdu_results[event_id] = {\"error\": str(e)}\n                        except Exception as e:\n                            f = failure.Failure()\n                            pdu_results[event_id] = {\"error\": str(e)}\n                            logger.error(\n                                \"Failed to handle PDU %s\",\n                                event_id,\n                                exc_info=(f.type, f.value, f.getTracebackObject()),\n                            )\n\n        await concurrently_execute(\n            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT\n        )\n\n        if newest_pdu_ts and origin in self._federation_metrics_domains:\n            newest_pdu_age = self._clock.time_msec() - newest_pdu_ts\n            last_pdu_age_metric.labels(server_name=origin).set(newest_pdu_age / 1000)\n\n        return pdu_results\n\n    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction):\n        \"\"\"Process the EDUs in a received transaction.\n        \"\"\"\n\n        async def _process_edu(edu_dict):\n            received_edus_counter.inc()\n\n            edu = Edu(\n                origin=origin,\n                destination=self.server_name,\n                edu_type=edu_dict[\"edu_type\"],\n                content=edu_dict[\"content\"],\n            )\n            await self.registry.on_edu(edu.edu_type, origin, edu.content)\n\n        await concurrently_execute(\n            _process_edu,\n            getattr(transaction, \"edus\", []),\n            TRANSACTION_CONCURRENCY_LIMIT,\n        )\n\n    async def on_room_state_request(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # we grab the linearizer to protect ourselves from servers which hammer\n        # us. In theory we might already have the response to this query\n        # in the cache so we could return it without waiting for the linearizer\n        # - but that's non-trivial to get right, and anyway somewhat defeats\n        # the point of the linearizer.\n        with (await self._server_linearizer.queue((origin, room_id))):\n            resp = dict(\n                await self._state_resp_cache.wrap(\n                    (room_id, event_id),\n                    self._on_context_state_request_compute,\n                    room_id,\n                    event_id,\n                )\n            )\n\n        room_version = await self.store.get_room_version_id(room_id)\n        resp[\"room_version\"] = room_version\n\n        return 200, resp\n\n    async def on_state_ids_request(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        if not event_id:\n            raise NotImplementedError(\"Specify an event\")\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        resp = await self._state_ids_resp_cache.wrap(\n            (room_id, event_id), self._on_state_ids_request_compute, room_id, event_id,\n        )\n\n        return 200, resp\n\n    async def _on_state_ids_request_compute(self, room_id, event_id):\n        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)\n        auth_chain_ids = await self.store.get_auth_chain_ids(state_ids)\n        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": auth_chain_ids}\n\n    async def _on_context_state_request_compute(\n        self, room_id: str, event_id: str\n    ) -> Dict[str, list]:\n        if event_id:\n            pdus = await self.handler.get_state_for_pdu(room_id, event_id)\n        else:\n            pdus = (await self.state.get_current_state(room_id)).values()\n\n        auth_chain = await self.store.get_auth_chain([pdu.event_id for pdu in pdus])\n\n        return {\n            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],\n            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],\n        }\n\n    async def on_pdu_request(\n        self, origin: str, event_id: str\n    ) -> Tuple[int, Union[JsonDict, str]]:\n        pdu = await self.handler.get_persisted_pdu(origin, event_id)\n\n        if pdu:\n            return 200, self._transaction_from_pdus([pdu]).get_dict()\n        else:\n            return 404, \"\"\n\n    async def on_query_request(\n        self, query_type: str, args: Dict[str, str]\n    ) -> Tuple[int, Dict[str, Any]]:\n        received_queries_counter.labels(query_type).inc()\n        resp = await self.registry.on_query(query_type, args)\n        return 200, resp\n\n    async def on_make_join_request(\n        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]\n    ) -> Dict[str, Any]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        room_version = await self.store.get_room_version_id(room_id)\n        if room_version not in supported_versions:\n            logger.warning(\n                \"Room version %s not in %s\", room_version, supported_versions\n            )\n            raise IncompatibleRoomVersionError(room_version=room_version)\n\n        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)\n        time_now = self._clock.time_msec()\n        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}\n\n    async def on_invite_request(\n        self, origin: str, content: JsonDict, room_version_id: str\n    ) -> Dict[str, Any]:\n        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)\n        if not room_version:\n            raise SynapseError(\n                400,\n                \"Homeserver does not support this room version\",\n                Codes.UNSUPPORTED_ROOM_VERSION,\n            )\n\n        pdu = event_from_pdu_json(content, room_version)\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)\n        time_now = self._clock.time_msec()\n        return {\"event\": ret_pdu.get_pdu_json(time_now)}\n\n    async def on_send_join_request(\n        self, origin: str, content: JsonDict\n    ) -> Dict[str, Any]:\n        logger.debug(\"on_send_join_request: content: %s\", content)\n\n        assert_params_in_dict(content, [\"room_id\"])\n        room_version = await self.store.get_room_version(content[\"room_id\"])\n        pdu = event_from_pdu_json(content, room_version)\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_join_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n\n        res_pdus = await self.handler.on_send_join_request(origin, pdu)\n        time_now = self._clock.time_msec()\n        return {\n            \"state\": [p.get_pdu_json(time_now) for p in res_pdus[\"state\"]],\n            \"auth_chain\": [p.get_pdu_json(time_now) for p in res_pdus[\"auth_chain\"]],\n        }\n\n    async def on_make_leave_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> Dict[str, Any]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)\n\n        room_version = await self.store.get_room_version_id(room_id)\n\n        time_now = self._clock.time_msec()\n        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}\n\n    async def on_send_leave_request(self, origin: str, content: JsonDict) -> dict:\n        logger.debug(\"on_send_leave_request: content: %s\", content)\n\n        assert_params_in_dict(content, [\"room_id\"])\n        room_version = await self.store.get_room_version(content[\"room_id\"])\n        pdu = event_from_pdu_json(content, room_version)\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n\n        await self.handler.on_send_leave_request(origin, pdu)\n        return {}\n\n    async def on_event_auth(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            time_now = self._clock.time_msec()\n            auth_pdus = await self.handler.on_event_auth(event_id)\n            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}\n        return 200, res\n\n    @log_function\n    async def on_query_client_keys(\n        self, origin: str, content: Dict[str, str]\n    ) -> Tuple[int, Dict[str, Any]]:\n        return await self.on_query_request(\"client_keys\", content)\n\n    async def on_query_user_devices(\n        self, origin: str, user_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        keys = await self.device_handler.on_federation_query_user_devices(user_id)\n        return 200, keys\n\n    @trace\n    async def on_claim_client_keys(\n        self, origin: str, content: JsonDict\n    ) -> Dict[str, Any]:\n        query = []\n        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():\n            for device_id, algorithm in device_keys.items():\n                query.append((user_id, device_id, algorithm))\n\n        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})\n        results = await self.store.claim_e2e_one_time_keys(query)\n\n        json_result = {}  # type: Dict[str, Dict[str, dict]]\n        for user_id, device_keys in results.items():\n            for device_id, keys in device_keys.items():\n                for key_id, json_str in keys.items():\n                    json_result.setdefault(user_id, {})[device_id] = {\n                        key_id: json_decoder.decode(json_str)\n                    }\n\n        logger.info(\n            \"Claimed one-time-keys: %s\",\n            \",\".join(\n                (\n                    \"%s for %s:%s\" % (key_id, user_id, device_id)\n                    for user_id, user_keys in json_result.items()\n                    for device_id, device_keys in user_keys.items()\n                    for key_id, _ in device_keys.items()\n                )\n            ),\n        )\n\n        return {\"one_time_keys\": json_result}\n\n    async def on_get_missing_events(\n        self,\n        origin: str,\n        room_id: str,\n        earliest_events: List[str],\n        latest_events: List[str],\n        limit: int,\n    ) -> Dict[str, list]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            logger.debug(\n                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"\n                \" limit: %d\",\n                earliest_events,\n                latest_events,\n                limit,\n            )\n\n            missing_events = await self.handler.on_get_missing_events(\n                origin, room_id, earliest_events, latest_events, limit\n            )\n\n            if len(missing_events) < 5:\n                logger.debug(\n                    \"Returning %d events: %r\", len(missing_events), missing_events\n                )\n            else:\n                logger.debug(\"Returning %d events\", len(missing_events))\n\n            time_now = self._clock.time_msec()\n\n        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}\n\n    @log_function\n    async def on_openid_userinfo(self, token: str) -> Optional[str]:\n        ts_now_ms = self._clock.time_msec()\n        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)\n\n    def _transaction_from_pdus(self, pdu_list: List[EventBase]) -> Transaction:\n        \"\"\"Returns a new Transaction containing the given PDUs suitable for\n        transmission.\n        \"\"\"\n        time_now = self._clock.time_msec()\n        pdus = [p.get_pdu_json(time_now) for p in pdu_list]\n        return Transaction(\n            origin=self.server_name,\n            pdus=pdus,\n            origin_server_ts=int(time_now),\n            destination=None,\n        )\n\n    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:\n        \"\"\" Process a PDU received in a federation /send/ transaction.\n\n        If the event is invalid, then this method throws a FederationError.\n        (The error will then be logged and sent back to the sender (which\n        probably won't do anything with it), and other events in the\n        transaction will be processed as normal).\n\n        It is likely that we'll then receive other events which refer to\n        this rejected_event in their prev_events, etc.  When that happens,\n        we'll attempt to fetch the rejected event again, which will presumably\n        fail, so those second-generation events will also get rejected.\n\n        Eventually, we get to the point where there are more than 10 events\n        between any new events and the original rejected event. Since we\n        only try to backfill 10 events deep on received pdu, we then accept the\n        new event, possibly introducing a discontinuity in the DAG, with new\n        forward extremities, so normal service is approximately returned,\n        until we try to backfill across the discontinuity.\n\n        Args:\n            origin: server which sent the pdu\n            pdu: received pdu\n\n        Raises: FederationError if the signatures / hash do not match, or\n            if the event was unacceptable for any other reason (eg, too large,\n            too many prev_events, couldn't find the prev_events)\n        \"\"\"\n        # check that it's actually being sent from a valid destination to\n        # workaround bug #1753 in 0.18.5 and 0.18.6\n        if origin != get_domain_from_id(pdu.sender):\n            # We continue to accept join events from any server; this is\n            # necessary for the federation join dance to work correctly.\n            # (When we join over federation, the \"helper\" server is\n            # responsible for sending out the join event, rather than the\n            # origin. See bug #1893. This is also true for some third party\n            # invites).\n            if not (\n                pdu.type == \"m.room.member\"\n                and pdu.content\n                and pdu.content.get(\"membership\", None)\n                in (Membership.JOIN, Membership.INVITE)\n            ):\n                logger.info(\n                    \"Discarding PDU %s from invalid origin %s\", pdu.event_id, origin\n                )\n                return\n            else:\n                logger.info(\"Accepting join PDU %s from %s\", pdu.event_id, origin)\n\n        # We've already checked that we know the room version by this point\n        room_version = await self.store.get_room_version(pdu.room_id)\n\n        # Check signature.\n        try:\n            pdu = await self._check_sigs_and_hash(room_version, pdu)\n        except SynapseError as e:\n            raise FederationError(\"ERROR\", e.code, e.msg, affected=pdu.event_id)\n\n        await self.handler.on_receive_pdu(origin, pdu, sent_to_us_directly=True)\n\n    def __str__(self):\n        return \"<ReplicationLayer(%s)>\" % self.server_name\n\n    async def exchange_third_party_invite(\n        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict\n    ):\n        ret = await self.handler.exchange_third_party_invite(\n            sender_user_id, target_user_id, room_id, signed\n        )\n        return ret\n\n    async def on_exchange_third_party_invite_request(self, event_dict: Dict):\n        ret = await self.handler.on_exchange_third_party_invite_request(event_dict)\n        return ret\n\n    async def check_server_matches_acl(self, server_name: str, room_id: str):\n        \"\"\"Check if the given server is allowed by the server ACLs in the room\n\n        Args:\n            server_name: name of server, *without any port part*\n            room_id: ID of the room to check\n\n        Raises:\n            AuthError if the server does not match the ACL\n        \"\"\"\n        state_ids = await self.store.get_current_state_ids(room_id)\n        acl_event_id = state_ids.get((EventTypes.ServerACL, \"\"))\n\n        if not acl_event_id:\n            return\n\n        acl_event = await self.store.get_event(acl_event_id)\n        if server_matches_acl_event(server_name, acl_event):\n            return\n\n        raise AuthError(code=403, msg=\"Server is banned from room\")", "target": 0}, {"function": "def server_matches_acl_event(server_name: str, acl_event: EventBase) -> bool:\n    \"\"\"Check if the given server is allowed by the ACL event\n\n    Args:\n        server_name: name of server, without any port part\n        acl_event: m.room.server_acl event\n\n    Returns:\n        True if this server is allowed by the ACLs\n    \"\"\"\n    logger.debug(\"Checking %s against acl %s\", server_name, acl_event.content)\n\n    # first of all, check if literal IPs are blocked, and if so, whether the\n    # server name is a literal IP\n    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)\n    if not isinstance(allow_ip_literals, bool):\n        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")\n        allow_ip_literals = True\n    if not allow_ip_literals:\n        # check for ipv6 literals. These start with '['.\n        if server_name[0] == \"[\":\n            return False\n\n        # check for ipv4 literals. We can just lift the routine from twisted.\n        if isIPAddress(server_name):\n            return False\n\n    # next,  check the deny list\n    deny = acl_event.content.get(\"deny\", [])\n    if not isinstance(deny, (list, tuple)):\n        logger.warning(\"Ignoring non-list deny ACL %s\", deny)\n        deny = []\n    for e in deny:\n        if _acl_entry_matches(server_name, e):\n            # logger.info(\"%s matched deny rule %s\", server_name, e)\n            return False\n\n    # then the allow list.\n    allow = acl_event.content.get(\"allow\", [])\n    if not isinstance(allow, (list, tuple)):\n        logger.warning(\"Ignoring non-list allow ACL %s\", allow)\n        allow = []\n    for e in allow:\n        if _acl_entry_matches(server_name, e):\n            # logger.info(\"%s matched allow rule %s\", server_name, e)\n            return True\n\n    # everything else should be rejected.\n    # logger.info(\"%s fell through\", server_name)\n    return False", "target": 0}, {"function": "def _acl_entry_matches(server_name: str, acl_entry: Any) -> bool:\n    if not isinstance(acl_entry, str):\n        logger.warning(\n            \"Ignoring non-str ACL entry '%s' (is %s)\", acl_entry, type(acl_entry)\n        )\n        return False\n    regex = glob_to_regex(acl_entry)\n    return bool(regex.match(server_name))", "target": 0}, {"function": "class FederationHandlerRegistry:\n    \"\"\"Allows classes to register themselves as handlers for a given EDU or\n    query type for incoming federation traffic.\n    \"\"\"\n\n    def __init__(self, hs: \"HomeServer\"):\n        self.config = hs.config\n        self.http_client = hs.get_simple_http_client()\n        self.clock = hs.get_clock()\n        self._instance_name = hs.get_instance_name()\n\n        # These are safe to load in monolith mode, but will explode if we try\n        # and use them. However we have guards before we use them to ensure that\n        # we don't route to ourselves, and in monolith mode that will always be\n        # the case.\n        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)\n        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)\n\n        self.edu_handlers = (\n            {}\n        )  # type: Dict[str, Callable[[str, dict], Awaitable[None]]]\n        self.query_handlers = {}  # type: Dict[str, Callable[[dict], Awaitable[None]]]\n\n        # Map from type to instance name that we should route EDU handling to.\n        self._edu_type_to_instance = {}  # type: Dict[str, str]\n\n    def register_edu_handler(\n        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]\n    ):\n        \"\"\"Sets the handler callable that will be used to handle an incoming\n        federation EDU of the given type.\n\n        Args:\n            edu_type: The type of the incoming EDU to register handler for\n            handler: A callable invoked on incoming EDU\n                of the given type. The arguments are the origin server name and\n                the EDU contents.\n        \"\"\"\n        if edu_type in self.edu_handlers:\n            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))\n\n        logger.info(\"Registering federation EDU handler for %r\", edu_type)\n\n        self.edu_handlers[edu_type] = handler\n\n    def register_query_handler(\n        self, query_type: str, handler: Callable[[dict], defer.Deferred]\n    ):\n        \"\"\"Sets the handler callable that will be used to handle an incoming\n        federation query of the given type.\n\n        Args:\n            query_type: Category name of the query, which should match\n                the string used by make_query.\n            handler: Invoked to handle\n                incoming queries of this type. The return will be yielded\n                on and the result used as the response to the query request.\n        \"\"\"\n        if query_type in self.query_handlers:\n            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))\n\n        logger.info(\"Registering federation query handler for %r\", query_type)\n\n        self.query_handlers[query_type] = handler\n\n    def register_instance_for_edu(self, edu_type: str, instance_name: str):\n        \"\"\"Register that the EDU handler is on a different instance than master.\n        \"\"\"\n        self._edu_type_to_instance[edu_type] = instance_name\n\n    async def on_edu(self, edu_type: str, origin: str, content: dict):\n        if not self.config.use_presence and edu_type == \"m.presence\":\n            return\n\n        # Check if we have a handler on this instance\n        handler = self.edu_handlers.get(edu_type)\n        if handler:\n            with start_active_span_from_edu(content, \"handle_edu\"):\n                try:\n                    await handler(origin, content)\n                except SynapseError as e:\n                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)\n                except Exception:\n                    logger.exception(\"Failed to handle edu %r\", edu_type)\n            return\n\n        # Check if we can route it somewhere else that isn't us\n        route_to = self._edu_type_to_instance.get(edu_type, \"master\")\n        if route_to != self._instance_name:\n            try:\n                await self._send_edu(\n                    instance_name=route_to,\n                    edu_type=edu_type,\n                    origin=origin,\n                    content=content,\n                )\n            except SynapseError as e:\n                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)\n            except Exception:\n                logger.exception(\"Failed to handle edu %r\", edu_type)\n            return\n\n        # Oh well, let's just log and move on.\n        logger.warning(\"No handler registered for EDU type %s\", edu_type)\n\n    async def on_query(self, query_type: str, args: dict):\n        handler = self.query_handlers.get(query_type)\n        if handler:\n            return await handler(args)\n\n        # Check if we can route it somewhere else that isn't us\n        if self._instance_name == \"master\":\n            return await self._get_query_client(query_type=query_type, args=args)\n\n        # Uh oh, no handler! Let's raise an exception so the request returns an\n        # error.\n        logger.warning(\"No handler registered for query type %s\", query_type)\n        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))", "target": 0}], "function_after": [{"function": "class FederationServer(FederationBase):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        self.auth = hs.get_auth()\n        self.handler = hs.get_federation_handler()\n        self.state = hs.get_state_handler()\n\n        self.device_handler = hs.get_device_handler()\n\n        # Ensure the following handlers are loaded since they register callbacks\n        # with FederationHandlerRegistry.\n        hs.get_directory_handler()\n\n        self._federation_ratelimiter = hs.get_federation_ratelimiter()\n\n        self._server_linearizer = Linearizer(\"fed_server\")\n        self._transaction_linearizer = Linearizer(\"fed_txn_handler\")\n\n        # We cache results for transaction with the same ID\n        self._transaction_resp_cache = ResponseCache(\n            hs, \"fed_txn_handler\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n\n        self.transaction_actions = TransactionActions(self.store)\n\n        self.registry = hs.get_federation_registry()\n\n        # We cache responses to state queries, as they take a while and often\n        # come in waves.\n        self._state_resp_cache = ResponseCache(\n            hs, \"state_resp\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n        self._state_ids_resp_cache = ResponseCache(\n            hs, \"state_ids_resp\", timeout_ms=30000\n        )  # type: ResponseCache[Tuple[str, str]]\n\n        self._federation_metrics_domains = (\n            hs.get_config().federation.federation_metrics_domains\n        )\n\n    async def on_backfill_request(\n        self, origin: str, room_id: str, versions: List[str], limit: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            pdus = await self.handler.on_backfill_request(\n                origin, room_id, versions, limit\n            )\n\n            res = self._transaction_from_pdus(pdus).get_dict()\n\n        return 200, res\n\n    async def on_incoming_transaction(\n        self, origin: str, transaction_data: JsonDict\n    ) -> Tuple[int, Dict[str, Any]]:\n        # keep this as early as possible to make the calculated origin ts as\n        # accurate as possible.\n        request_time = self._clock.time_msec()\n\n        transaction = Transaction(**transaction_data)\n        transaction_id = transaction.transaction_id  # type: ignore\n\n        if not transaction_id:\n            raise Exception(\"Transaction missing transaction_id\")\n\n        logger.debug(\"[%s] Got transaction\", transaction_id)\n\n        # We wrap in a ResponseCache so that we de-duplicate retried\n        # transactions.\n        return await self._transaction_resp_cache.wrap(\n            (origin, transaction_id),\n            self._on_incoming_transaction_inner,\n            origin,\n            transaction,\n            request_time,\n        )\n\n    async def _on_incoming_transaction_inner(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        # Use a linearizer to ensure that transactions from a remote are\n        # processed in order.\n        with await self._transaction_linearizer.queue(origin):\n            # We rate limit here *after* we've queued up the incoming requests,\n            # so that we don't fill up the ratelimiter with blocked requests.\n            #\n            # This is important as the ratelimiter allows N concurrent requests\n            # at a time, and only starts ratelimiting if there are more requests\n            # than that being processed at a time. If we queued up requests in\n            # the linearizer/response cache *after* the ratelimiting then those\n            # queued up requests would count as part of the allowed limit of N\n            # concurrent requests.\n            with self._federation_ratelimiter.ratelimit(origin) as d:\n                await d\n\n                result = await self._handle_incoming_transaction(\n                    origin, transaction, request_time\n                )\n\n        return result\n\n    async def _handle_incoming_transaction(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Tuple[int, Dict[str, Any]]:\n        \"\"\" Process an incoming transaction and return the HTTP response\n\n        Args:\n            origin: the server making the request\n            transaction: incoming transaction\n            request_time: timestamp that the HTTP request arrived at\n\n        Returns:\n            HTTP response code and body\n        \"\"\"\n        response = await self.transaction_actions.have_responded(origin, transaction)\n\n        if response:\n            logger.debug(\n                \"[%s] We've already responded to this request\",\n                transaction.transaction_id,  # type: ignore\n            )\n            return response\n\n        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)  # type: ignore\n\n        # Reject if PDU count > 50 or EDU count > 100\n        if len(transaction.pdus) > 50 or (  # type: ignore\n            hasattr(transaction, \"edus\") and len(transaction.edus) > 100  # type: ignore\n        ):\n\n            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")\n\n            response = {}\n            await self.transaction_actions.set_response(\n                origin, transaction, 400, response\n            )\n            return 400, response\n\n        # We process PDUs and EDUs in parallel. This is important as we don't\n        # want to block things like to device messages from reaching clients\n        # behind the potentially expensive handling of PDUs.\n        pdu_results, _ = await make_deferred_yieldable(\n            defer.gatherResults(\n                [\n                    run_in_background(\n                        self._handle_pdus_in_txn, origin, transaction, request_time\n                    ),\n                    run_in_background(self._handle_edus_in_txn, origin, transaction),\n                ],\n                consumeErrors=True,\n            ).addErrback(unwrapFirstError)\n        )\n\n        response = {\"pdus\": pdu_results}\n\n        logger.debug(\"Returning: %s\", str(response))\n\n        await self.transaction_actions.set_response(origin, transaction, 200, response)\n        return 200, response\n\n    async def _handle_pdus_in_txn(\n        self, origin: str, transaction: Transaction, request_time: int\n    ) -> Dict[str, dict]:\n        \"\"\"Process the PDUs in a received transaction.\n\n        Args:\n            origin: the server making the request\n            transaction: incoming transaction\n            request_time: timestamp that the HTTP request arrived at\n\n        Returns:\n            A map from event ID of a processed PDU to any errors we should\n            report back to the sending server.\n        \"\"\"\n\n        received_pdus_counter.inc(len(transaction.pdus))  # type: ignore\n\n        origin_host, _ = parse_server_name(origin)\n\n        pdus_by_room = {}  # type: Dict[str, List[EventBase]]\n\n        newest_pdu_ts = 0\n\n        for p in transaction.pdus:  # type: ignore\n            # FIXME (richardv): I don't think this works:\n            #  https://github.com/matrix-org/synapse/issues/8429\n            if \"unsigned\" in p:\n                unsigned = p[\"unsigned\"]\n                if \"age\" in unsigned:\n                    p[\"age\"] = unsigned[\"age\"]\n            if \"age\" in p:\n                p[\"age_ts\"] = request_time - int(p[\"age\"])\n                del p[\"age\"]\n\n            # We try and pull out an event ID so that if later checks fail we\n            # can log something sensible. We don't mandate an event ID here in\n            # case future event formats get rid of the key.\n            possible_event_id = p.get(\"event_id\", \"<Unknown>\")\n\n            # Now we get the room ID so that we can check that we know the\n            # version of the room.\n            room_id = p.get(\"room_id\")\n            if not room_id:\n                logger.info(\n                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",\n                    possible_event_id,\n                )\n                continue\n\n            try:\n                room_version = await self.store.get_room_version(room_id)\n            except NotFoundError:\n                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)\n                continue\n            except UnsupportedRoomVersionError as e:\n                # this can happen if support for a given room version is withdrawn,\n                # so that we still get events for said room.\n                logger.info(\"Ignoring PDU: %s\", e)\n                continue\n\n            event = event_from_pdu_json(p, room_version)\n            pdus_by_room.setdefault(room_id, []).append(event)\n\n            if event.origin_server_ts > newest_pdu_ts:\n                newest_pdu_ts = event.origin_server_ts\n\n        pdu_results = {}\n\n        # we can process different rooms in parallel (which is useful if they\n        # require callouts to other servers to fetch missing events), but\n        # impose a limit to avoid going too crazy with ram/cpu.\n\n        async def process_pdus_for_room(room_id: str):\n            logger.debug(\"Processing PDUs for %s\", room_id)\n            try:\n                await self.check_server_matches_acl(origin_host, room_id)\n            except AuthError as e:\n                logger.warning(\"Ignoring PDUs for room %s from banned server\", room_id)\n                for pdu in pdus_by_room[room_id]:\n                    event_id = pdu.event_id\n                    pdu_results[event_id] = e.error_dict()\n                return\n\n            for pdu in pdus_by_room[room_id]:\n                event_id = pdu.event_id\n                with pdu_process_time.time():\n                    with nested_logging_context(event_id):\n                        try:\n                            await self._handle_received_pdu(origin, pdu)\n                            pdu_results[event_id] = {}\n                        except FederationError as e:\n                            logger.warning(\"Error handling PDU %s: %s\", event_id, e)\n                            pdu_results[event_id] = {\"error\": str(e)}\n                        except Exception as e:\n                            f = failure.Failure()\n                            pdu_results[event_id] = {\"error\": str(e)}\n                            logger.error(\n                                \"Failed to handle PDU %s\",\n                                event_id,\n                                exc_info=(f.type, f.value, f.getTracebackObject()),\n                            )\n\n        await concurrently_execute(\n            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT\n        )\n\n        if newest_pdu_ts and origin in self._federation_metrics_domains:\n            newest_pdu_age = self._clock.time_msec() - newest_pdu_ts\n            last_pdu_age_metric.labels(server_name=origin).set(newest_pdu_age / 1000)\n\n        return pdu_results\n\n    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction):\n        \"\"\"Process the EDUs in a received transaction.\n        \"\"\"\n\n        async def _process_edu(edu_dict):\n            received_edus_counter.inc()\n\n            edu = Edu(\n                origin=origin,\n                destination=self.server_name,\n                edu_type=edu_dict[\"edu_type\"],\n                content=edu_dict[\"content\"],\n            )\n            await self.registry.on_edu(edu.edu_type, origin, edu.content)\n\n        await concurrently_execute(\n            _process_edu,\n            getattr(transaction, \"edus\", []),\n            TRANSACTION_CONCURRENCY_LIMIT,\n        )\n\n    async def on_room_state_request(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # we grab the linearizer to protect ourselves from servers which hammer\n        # us. In theory we might already have the response to this query\n        # in the cache so we could return it without waiting for the linearizer\n        # - but that's non-trivial to get right, and anyway somewhat defeats\n        # the point of the linearizer.\n        with (await self._server_linearizer.queue((origin, room_id))):\n            resp = dict(\n                await self._state_resp_cache.wrap(\n                    (room_id, event_id),\n                    self._on_context_state_request_compute,\n                    room_id,\n                    event_id,\n                )\n            )\n\n        room_version = await self.store.get_room_version_id(room_id)\n        resp[\"room_version\"] = room_version\n\n        return 200, resp\n\n    async def on_state_ids_request(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        if not event_id:\n            raise NotImplementedError(\"Specify an event\")\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        resp = await self._state_ids_resp_cache.wrap(\n            (room_id, event_id), self._on_state_ids_request_compute, room_id, event_id,\n        )\n\n        return 200, resp\n\n    async def _on_state_ids_request_compute(self, room_id, event_id):\n        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)\n        auth_chain_ids = await self.store.get_auth_chain_ids(state_ids)\n        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": auth_chain_ids}\n\n    async def _on_context_state_request_compute(\n        self, room_id: str, event_id: str\n    ) -> Dict[str, list]:\n        if event_id:\n            pdus = await self.handler.get_state_for_pdu(room_id, event_id)\n        else:\n            pdus = (await self.state.get_current_state(room_id)).values()\n\n        auth_chain = await self.store.get_auth_chain([pdu.event_id for pdu in pdus])\n\n        return {\n            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],\n            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],\n        }\n\n    async def on_pdu_request(\n        self, origin: str, event_id: str\n    ) -> Tuple[int, Union[JsonDict, str]]:\n        pdu = await self.handler.get_persisted_pdu(origin, event_id)\n\n        if pdu:\n            return 200, self._transaction_from_pdus([pdu]).get_dict()\n        else:\n            return 404, \"\"\n\n    async def on_query_request(\n        self, query_type: str, args: Dict[str, str]\n    ) -> Tuple[int, Dict[str, Any]]:\n        received_queries_counter.labels(query_type).inc()\n        resp = await self.registry.on_query(query_type, args)\n        return 200, resp\n\n    async def on_make_join_request(\n        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]\n    ) -> Dict[str, Any]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n\n        room_version = await self.store.get_room_version_id(room_id)\n        if room_version not in supported_versions:\n            logger.warning(\n                \"Room version %s not in %s\", room_version, supported_versions\n            )\n            raise IncompatibleRoomVersionError(room_version=room_version)\n\n        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)\n        time_now = self._clock.time_msec()\n        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}\n\n    async def on_invite_request(\n        self, origin: str, content: JsonDict, room_version_id: str\n    ) -> Dict[str, Any]:\n        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)\n        if not room_version:\n            raise SynapseError(\n                400,\n                \"Homeserver does not support this room version\",\n                Codes.UNSUPPORTED_ROOM_VERSION,\n            )\n\n        pdu = event_from_pdu_json(content, room_version)\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)\n        time_now = self._clock.time_msec()\n        return {\"event\": ret_pdu.get_pdu_json(time_now)}\n\n    async def on_send_join_request(\n        self, origin: str, content: JsonDict\n    ) -> Dict[str, Any]:\n        logger.debug(\"on_send_join_request: content: %s\", content)\n\n        assert_params_in_dict(content, [\"room_id\"])\n        room_version = await self.store.get_room_version(content[\"room_id\"])\n        pdu = event_from_pdu_json(content, room_version)\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_join_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n\n        res_pdus = await self.handler.on_send_join_request(origin, pdu)\n        time_now = self._clock.time_msec()\n        return {\n            \"state\": [p.get_pdu_json(time_now) for p in res_pdus[\"state\"]],\n            \"auth_chain\": [p.get_pdu_json(time_now) for p in res_pdus[\"auth_chain\"]],\n        }\n\n    async def on_make_leave_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> Dict[str, Any]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)\n\n        room_version = await self.store.get_room_version_id(room_id)\n\n        time_now = self._clock.time_msec()\n        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}\n\n    async def on_send_leave_request(self, origin: str, content: JsonDict) -> dict:\n        logger.debug(\"on_send_leave_request: content: %s\", content)\n\n        assert_params_in_dict(content, [\"room_id\"])\n        room_version = await self.store.get_room_version(content[\"room_id\"])\n        pdu = event_from_pdu_json(content, room_version)\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n\n        await self.handler.on_send_leave_request(origin, pdu)\n        return {}\n\n    async def on_event_auth(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            time_now = self._clock.time_msec()\n            auth_pdus = await self.handler.on_event_auth(event_id)\n            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}\n        return 200, res\n\n    @log_function\n    async def on_query_client_keys(\n        self, origin: str, content: Dict[str, str]\n    ) -> Tuple[int, Dict[str, Any]]:\n        return await self.on_query_request(\"client_keys\", content)\n\n    async def on_query_user_devices(\n        self, origin: str, user_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        keys = await self.device_handler.on_federation_query_user_devices(user_id)\n        return 200, keys\n\n    @trace\n    async def on_claim_client_keys(\n        self, origin: str, content: JsonDict\n    ) -> Dict[str, Any]:\n        query = []\n        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():\n            for device_id, algorithm in device_keys.items():\n                query.append((user_id, device_id, algorithm))\n\n        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})\n        results = await self.store.claim_e2e_one_time_keys(query)\n\n        json_result = {}  # type: Dict[str, Dict[str, dict]]\n        for user_id, device_keys in results.items():\n            for device_id, keys in device_keys.items():\n                for key_id, json_str in keys.items():\n                    json_result.setdefault(user_id, {})[device_id] = {\n                        key_id: json_decoder.decode(json_str)\n                    }\n\n        logger.info(\n            \"Claimed one-time-keys: %s\",\n            \",\".join(\n                (\n                    \"%s for %s:%s\" % (key_id, user_id, device_id)\n                    for user_id, user_keys in json_result.items()\n                    for device_id, device_keys in user_keys.items()\n                    for key_id, _ in device_keys.items()\n                )\n            ),\n        )\n\n        return {\"one_time_keys\": json_result}\n\n    async def on_get_missing_events(\n        self,\n        origin: str,\n        room_id: str,\n        earliest_events: List[str],\n        latest_events: List[str],\n        limit: int,\n    ) -> Dict[str, list]:\n        with (await self._server_linearizer.queue((origin, room_id))):\n            origin_host, _ = parse_server_name(origin)\n            await self.check_server_matches_acl(origin_host, room_id)\n\n            logger.debug(\n                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"\n                \" limit: %d\",\n                earliest_events,\n                latest_events,\n                limit,\n            )\n\n            missing_events = await self.handler.on_get_missing_events(\n                origin, room_id, earliest_events, latest_events, limit\n            )\n\n            if len(missing_events) < 5:\n                logger.debug(\n                    \"Returning %d events: %r\", len(missing_events), missing_events\n                )\n            else:\n                logger.debug(\"Returning %d events\", len(missing_events))\n\n            time_now = self._clock.time_msec()\n\n        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}\n\n    @log_function\n    async def on_openid_userinfo(self, token: str) -> Optional[str]:\n        ts_now_ms = self._clock.time_msec()\n        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)\n\n    def _transaction_from_pdus(self, pdu_list: List[EventBase]) -> Transaction:\n        \"\"\"Returns a new Transaction containing the given PDUs suitable for\n        transmission.\n        \"\"\"\n        time_now = self._clock.time_msec()\n        pdus = [p.get_pdu_json(time_now) for p in pdu_list]\n        return Transaction(\n            origin=self.server_name,\n            pdus=pdus,\n            origin_server_ts=int(time_now),\n            destination=None,\n        )\n\n    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:\n        \"\"\" Process a PDU received in a federation /send/ transaction.\n\n        If the event is invalid, then this method throws a FederationError.\n        (The error will then be logged and sent back to the sender (which\n        probably won't do anything with it), and other events in the\n        transaction will be processed as normal).\n\n        It is likely that we'll then receive other events which refer to\n        this rejected_event in their prev_events, etc.  When that happens,\n        we'll attempt to fetch the rejected event again, which will presumably\n        fail, so those second-generation events will also get rejected.\n\n        Eventually, we get to the point where there are more than 10 events\n        between any new events and the original rejected event. Since we\n        only try to backfill 10 events deep on received pdu, we then accept the\n        new event, possibly introducing a discontinuity in the DAG, with new\n        forward extremities, so normal service is approximately returned,\n        until we try to backfill across the discontinuity.\n\n        Args:\n            origin: server which sent the pdu\n            pdu: received pdu\n\n        Raises: FederationError if the signatures / hash do not match, or\n            if the event was unacceptable for any other reason (eg, too large,\n            too many prev_events, couldn't find the prev_events)\n        \"\"\"\n        # check that it's actually being sent from a valid destination to\n        # workaround bug #1753 in 0.18.5 and 0.18.6\n        if origin != get_domain_from_id(pdu.sender):\n            # We continue to accept join events from any server; this is\n            # necessary for the federation join dance to work correctly.\n            # (When we join over federation, the \"helper\" server is\n            # responsible for sending out the join event, rather than the\n            # origin. See bug #1893. This is also true for some third party\n            # invites).\n            if not (\n                pdu.type == \"m.room.member\"\n                and pdu.content\n                and pdu.content.get(\"membership\", None)\n                in (Membership.JOIN, Membership.INVITE)\n            ):\n                logger.info(\n                    \"Discarding PDU %s from invalid origin %s\", pdu.event_id, origin\n                )\n                return\n            else:\n                logger.info(\"Accepting join PDU %s from %s\", pdu.event_id, origin)\n\n        # We've already checked that we know the room version by this point\n        room_version = await self.store.get_room_version(pdu.room_id)\n\n        # Check signature.\n        try:\n            pdu = await self._check_sigs_and_hash(room_version, pdu)\n        except SynapseError as e:\n            raise FederationError(\"ERROR\", e.code, e.msg, affected=pdu.event_id)\n\n        await self.handler.on_receive_pdu(origin, pdu, sent_to_us_directly=True)\n\n    def __str__(self):\n        return \"<ReplicationLayer(%s)>\" % self.server_name\n\n    async def exchange_third_party_invite(\n        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict\n    ):\n        ret = await self.handler.exchange_third_party_invite(\n            sender_user_id, target_user_id, room_id, signed\n        )\n        return ret\n\n    async def on_exchange_third_party_invite_request(self, event_dict: Dict):\n        ret = await self.handler.on_exchange_third_party_invite_request(event_dict)\n        return ret\n\n    async def check_server_matches_acl(self, server_name: str, room_id: str):\n        \"\"\"Check if the given server is allowed by the server ACLs in the room\n\n        Args:\n            server_name: name of server, *without any port part*\n            room_id: ID of the room to check\n\n        Raises:\n            AuthError if the server does not match the ACL\n        \"\"\"\n        state_ids = await self.store.get_current_state_ids(room_id)\n        acl_event_id = state_ids.get((EventTypes.ServerACL, \"\"))\n\n        if not acl_event_id:\n            return\n\n        acl_event = await self.store.get_event(acl_event_id)\n        if server_matches_acl_event(server_name, acl_event):\n            return\n\n        raise AuthError(code=403, msg=\"Server is banned from room\")", "target": 0}, {"function": "def server_matches_acl_event(server_name: str, acl_event: EventBase) -> bool:\n    \"\"\"Check if the given server is allowed by the ACL event\n\n    Args:\n        server_name: name of server, without any port part\n        acl_event: m.room.server_acl event\n\n    Returns:\n        True if this server is allowed by the ACLs\n    \"\"\"\n    logger.debug(\"Checking %s against acl %s\", server_name, acl_event.content)\n\n    # first of all, check if literal IPs are blocked, and if so, whether the\n    # server name is a literal IP\n    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)\n    if not isinstance(allow_ip_literals, bool):\n        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")\n        allow_ip_literals = True\n    if not allow_ip_literals:\n        # check for ipv6 literals. These start with '['.\n        if server_name[0] == \"[\":\n            return False\n\n        # check for ipv4 literals. We can just lift the routine from twisted.\n        if isIPAddress(server_name):\n            return False\n\n    # next,  check the deny list\n    deny = acl_event.content.get(\"deny\", [])\n    if not isinstance(deny, (list, tuple)):\n        logger.warning(\"Ignoring non-list deny ACL %s\", deny)\n        deny = []\n    for e in deny:\n        if _acl_entry_matches(server_name, e):\n            # logger.info(\"%s matched deny rule %s\", server_name, e)\n            return False\n\n    # then the allow list.\n    allow = acl_event.content.get(\"allow\", [])\n    if not isinstance(allow, (list, tuple)):\n        logger.warning(\"Ignoring non-list allow ACL %s\", allow)\n        allow = []\n    for e in allow:\n        if _acl_entry_matches(server_name, e):\n            # logger.info(\"%s matched allow rule %s\", server_name, e)\n            return True\n\n    # everything else should be rejected.\n    # logger.info(\"%s fell through\", server_name)\n    return False", "target": 0}, {"function": "def _acl_entry_matches(server_name: str, acl_entry: Any) -> bool:\n    if not isinstance(acl_entry, str):\n        logger.warning(\n            \"Ignoring non-str ACL entry '%s' (is %s)\", acl_entry, type(acl_entry)\n        )\n        return False\n    regex = glob_to_regex(acl_entry)\n    return bool(regex.match(server_name))", "target": 0}, {"function": "class FederationHandlerRegistry:\n    \"\"\"Allows classes to register themselves as handlers for a given EDU or\n    query type for incoming federation traffic.\n    \"\"\"\n\n    def __init__(self, hs: \"HomeServer\"):\n        self.config = hs.config\n        self.clock = hs.get_clock()\n        self._instance_name = hs.get_instance_name()\n\n        # These are safe to load in monolith mode, but will explode if we try\n        # and use them. However we have guards before we use them to ensure that\n        # we don't route to ourselves, and in monolith mode that will always be\n        # the case.\n        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)\n        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)\n\n        self.edu_handlers = (\n            {}\n        )  # type: Dict[str, Callable[[str, dict], Awaitable[None]]]\n        self.query_handlers = {}  # type: Dict[str, Callable[[dict], Awaitable[None]]]\n\n        # Map from type to instance name that we should route EDU handling to.\n        self._edu_type_to_instance = {}  # type: Dict[str, str]\n\n    def register_edu_handler(\n        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]\n    ):\n        \"\"\"Sets the handler callable that will be used to handle an incoming\n        federation EDU of the given type.\n\n        Args:\n            edu_type: The type of the incoming EDU to register handler for\n            handler: A callable invoked on incoming EDU\n                of the given type. The arguments are the origin server name and\n                the EDU contents.\n        \"\"\"\n        if edu_type in self.edu_handlers:\n            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))\n\n        logger.info(\"Registering federation EDU handler for %r\", edu_type)\n\n        self.edu_handlers[edu_type] = handler\n\n    def register_query_handler(\n        self, query_type: str, handler: Callable[[dict], defer.Deferred]\n    ):\n        \"\"\"Sets the handler callable that will be used to handle an incoming\n        federation query of the given type.\n\n        Args:\n            query_type: Category name of the query, which should match\n                the string used by make_query.\n            handler: Invoked to handle\n                incoming queries of this type. The return will be yielded\n                on and the result used as the response to the query request.\n        \"\"\"\n        if query_type in self.query_handlers:\n            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))\n\n        logger.info(\"Registering federation query handler for %r\", query_type)\n\n        self.query_handlers[query_type] = handler\n\n    def register_instance_for_edu(self, edu_type: str, instance_name: str):\n        \"\"\"Register that the EDU handler is on a different instance than master.\n        \"\"\"\n        self._edu_type_to_instance[edu_type] = instance_name\n\n    async def on_edu(self, edu_type: str, origin: str, content: dict):\n        if not self.config.use_presence and edu_type == \"m.presence\":\n            return\n\n        # Check if we have a handler on this instance\n        handler = self.edu_handlers.get(edu_type)\n        if handler:\n            with start_active_span_from_edu(content, \"handle_edu\"):\n                try:\n                    await handler(origin, content)\n                except SynapseError as e:\n                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)\n                except Exception:\n                    logger.exception(\"Failed to handle edu %r\", edu_type)\n            return\n\n        # Check if we can route it somewhere else that isn't us\n        route_to = self._edu_type_to_instance.get(edu_type, \"master\")\n        if route_to != self._instance_name:\n            try:\n                await self._send_edu(\n                    instance_name=route_to,\n                    edu_type=edu_type,\n                    origin=origin,\n                    content=content,\n                )\n            except SynapseError as e:\n                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)\n            except Exception:\n                logger.exception(\"Failed to handle edu %r\", edu_type)\n            return\n\n        # Oh well, let's just log and move on.\n        logger.warning(\"No handler registered for EDU type %s\", edu_type)\n\n    async def on_query(self, query_type: str, args: dict):\n        handler = self.query_handlers.get(query_type)\n        if handler:\n            return await handler(args)\n\n        # Check if we can route it somewhere else that isn't us\n        if self._instance_name == \"master\":\n            return await self._get_query_client(query_type=query_type, args=args)\n\n        # Uh oh, no handler! Let's raise an exception so the request returns an\n        # error.\n        logger.warning(\"No handler registered for query type %s\", query_type)\n        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Ffederation%2Ftransport%2Fclient.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport urllib\nfrom typing import Any, Dict, Optional\n\nfrom synapse.api.constants import Membership\nfrom synapse.api.errors import Codes, HttpResponseException, SynapseError\nfrom synapse.api.urls import (\n    FEDERATION_UNSTABLE_PREFIX,\n    FEDERATION_V1_PREFIX,\n    FEDERATION_V2_PREFIX,\n)\nfrom synapse.logging.utils import log_function\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransportLayerClient:\n    \"\"\"Sends federation HTTP requests to other servers\"\"\"\n\n    def __init__(self, hs):\n        self.server_name = hs.hostname\n        self.client = hs.get_federation_http_client()\n\n    @log_function\n    def get_room_state_ids(self, destination, room_id, event_id):\n        \"\"\" Requests all state for a given room from the given server at the\n        given event. Returns the state's event_id's\n\n        Args:\n            destination (str): The host name of the remote homeserver we want\n                to get the state from.\n            context (str): The name of the context we want the state of\n            event_id (str): The event we want the context at.\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\"get_room_state_ids dest=%s, room=%s\", destination, room_id)\n\n        path = _create_v1_path(\"/state_ids/%s\", room_id)\n        return self.client.get_json(\n            destination,\n            path=path,\n            args={\"event_id\": event_id},\n            try_trailing_slash_on_400=True,\n        )\n\n    @log_function\n    def get_event(self, destination, event_id, timeout=None):\n        \"\"\" Requests the pdu with give id and origin from the given server.\n\n        Args:\n            destination (str): The host name of the remote homeserver we want\n                to get the state from.\n            event_id (str): The id of the event being requested.\n            timeout (int): How long to try (in ms) the destination for before\n                giving up. None indicates no timeout.\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\"get_pdu dest=%s, event_id=%s\", destination, event_id)\n\n        path = _create_v1_path(\"/event/%s\", event_id)\n        return self.client.get_json(\n            destination, path=path, timeout=timeout, try_trailing_slash_on_400=True\n        )\n\n    @log_function\n    def backfill(self, destination, room_id, event_tuples, limit):\n        \"\"\" Requests `limit` previous PDUs in a given context before list of\n        PDUs.\n\n        Args:\n            dest (str)\n            room_id (str)\n            event_tuples (list)\n            limit (int)\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\n            \"backfill dest=%s, room_id=%s, event_tuples=%r, limit=%s\",\n            destination,\n            room_id,\n            event_tuples,\n            str(limit),\n        )\n\n        if not event_tuples:\n            # TODO: raise?\n            return\n\n        path = _create_v1_path(\"/backfill/%s\", room_id)\n\n        args = {\"v\": event_tuples, \"limit\": [str(limit)]}\n\n        return self.client.get_json(\n            destination, path=path, args=args, try_trailing_slash_on_400=True\n        )\n\n    @log_function\n    async def send_transaction(self, transaction, json_data_callback=None):\n        \"\"\" Sends the given Transaction to its destination\n\n        Args:\n            transaction (Transaction)\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The result\n            will be the decoded JSON body.\n\n            Fails with ``HTTPRequestException`` if we get an HTTP response\n            code >= 300.\n\n            Fails with ``NotRetryingDestination`` if we are not yet ready\n            to retry this server.\n\n            Fails with ``FederationDeniedError`` if this destination\n            is not on our federation whitelist\n        \"\"\"\n        logger.debug(\n            \"send_data dest=%s, txid=%s\",\n            transaction.destination,\n            transaction.transaction_id,\n        )\n\n        if transaction.destination == self.server_name:\n            raise RuntimeError(\"Transport layer cannot send to itself!\")\n\n        # FIXME: This is only used by the tests. The actual json sent is\n        # generated by the json_data_callback.\n        json_data = transaction.get_dict()\n\n        path = _create_v1_path(\"/send/%s\", transaction.transaction_id)\n\n        response = await self.client.put_json(\n            transaction.destination,\n            path=path,\n            data=json_data,\n            json_data_callback=json_data_callback,\n            long_retries=True,\n            backoff_on_404=True,  # If we get a 404 the other side has gone\n            try_trailing_slash_on_400=True,\n        )\n\n        return response\n\n    @log_function\n    async def make_query(\n        self, destination, query_type, args, retry_on_dns_fail, ignore_backoff=False\n    ):\n        path = _create_v1_path(\"/query/%s\", query_type)\n\n        content = await self.client.get_json(\n            destination=destination,\n            path=path,\n            args=args,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=10000,\n            ignore_backoff=ignore_backoff,\n        )\n\n        return content\n\n    @log_function\n    async def make_membership_event(\n        self, destination, room_id, user_id, membership, params\n    ):\n        \"\"\"Asks a remote server to build and sign us a membership event\n\n        Note that this does not append any events to any graphs.\n\n        Args:\n            destination (str): address of remote homeserver\n            room_id (str): room to join/leave\n            user_id (str): user to be joined/left\n            membership (str): one of join/leave\n            params (dict[str, str|Iterable[str]]): Query parameters to include in the\n                request.\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The result\n            will be the decoded JSON body (ie, the new event).\n\n            Fails with ``HTTPRequestException`` if we get an HTTP response\n            code >= 300.\n\n            Fails with ``NotRetryingDestination`` if we are not yet ready\n            to retry this server.\n\n            Fails with ``FederationDeniedError`` if the remote destination\n            is not in our federation whitelist\n        \"\"\"\n        valid_memberships = {Membership.JOIN, Membership.LEAVE}\n        if membership not in valid_memberships:\n            raise RuntimeError(\n                \"make_membership_event called with membership='%s', must be one of %s\"\n                % (membership, \",\".join(valid_memberships))\n            )\n        path = _create_v1_path(\"/make_%s/%s/%s\", membership, room_id, user_id)\n\n        ignore_backoff = False\n        retry_on_dns_fail = False\n\n        if membership == Membership.LEAVE:\n            # we particularly want to do our best to send leave events. The\n            # problem is that if it fails, we won't retry it later, so if the\n            # remote server was just having a momentary blip, the room will be\n            # out of sync.\n            ignore_backoff = True\n            retry_on_dns_fail = True\n\n        content = await self.client.get_json(\n            destination=destination,\n            path=path,\n            args=params,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=20000,\n            ignore_backoff=ignore_backoff,\n        )\n\n        return content\n\n    @log_function\n    async def send_join_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/send_join/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content\n        )\n\n        return response\n\n    @log_function\n    async def send_join_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/send_join/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content\n        )\n\n        return response\n\n    @log_function\n    async def send_leave_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/send_leave/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination,\n            path=path,\n            data=content,\n            # we want to do our best to send this through. The problem is\n            # that if it fails, we won't retry it later, so if the remote\n            # server was just having a momentary blip, the room will be out of\n            # sync.\n            ignore_backoff=True,\n        )\n\n        return response\n\n    @log_function\n    async def send_leave_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/send_leave/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination,\n            path=path,\n            data=content,\n            # we want to do our best to send this through. The problem is\n            # that if it fails, we won't retry it later, so if the remote\n            # server was just having a momentary blip, the room will be out of\n            # sync.\n            ignore_backoff=True,\n        )\n\n        return response\n\n    @log_function\n    async def send_invite_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/invite/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n        return response\n\n    @log_function\n    async def send_invite_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/invite/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n        return response\n\n    @log_function\n    async def get_public_rooms(\n        self,\n        remote_server: str,\n        limit: Optional[int] = None,\n        since_token: Optional[str] = None,\n        search_filter: Optional[Dict] = None,\n        include_all_networks: bool = False,\n        third_party_instance_id: Optional[str] = None,\n    ):\n        \"\"\"Get the list of public rooms from a remote homeserver\n\n        See synapse.federation.federation_client.FederationClient.get_public_rooms for\n        more information.\n        \"\"\"\n        if search_filter:\n            # this uses MSC2197 (Search Filtering over Federation)\n            path = _create_v1_path(\"/publicRooms\")\n\n            data = {\n                \"include_all_networks\": \"true\" if include_all_networks else \"false\"\n            }  # type: Dict[str, Any]\n            if third_party_instance_id:\n                data[\"third_party_instance_id\"] = third_party_instance_id\n            if limit:\n                data[\"limit\"] = str(limit)\n            if since_token:\n                data[\"since\"] = since_token\n\n            data[\"filter\"] = search_filter\n\n            try:\n                response = await self.client.post_json(\n                    destination=remote_server, path=path, data=data, ignore_backoff=True\n                )\n            except HttpResponseException as e:\n                if e.code == 403:\n                    raise SynapseError(\n                        403,\n                        \"You are not allowed to view the public rooms list of %s\"\n                        % (remote_server,),\n                        errcode=Codes.FORBIDDEN,\n                    )\n                raise\n        else:\n            path = _create_v1_path(\"/publicRooms\")\n\n            args = {\n                \"include_all_networks\": \"true\" if include_all_networks else \"false\"\n            }  # type: Dict[str, Any]\n            if third_party_instance_id:\n                args[\"third_party_instance_id\"] = (third_party_instance_id,)\n            if limit:\n                args[\"limit\"] = [str(limit)]\n            if since_token:\n                args[\"since\"] = [since_token]\n\n            try:\n                response = await self.client.get_json(\n                    destination=remote_server, path=path, args=args, ignore_backoff=True\n                )\n            except HttpResponseException as e:\n                if e.code == 403:\n                    raise SynapseError(\n                        403,\n                        \"You are not allowed to view the public rooms list of %s\"\n                        % (remote_server,),\n                        errcode=Codes.FORBIDDEN,\n                    )\n                raise\n\n        return response\n\n    @log_function\n    async def exchange_third_party_invite(self, destination, room_id, event_dict):\n        path = _create_v1_path(\"/exchange_third_party_invite/%s\", room_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=event_dict\n        )\n\n        return response\n\n    @log_function\n    async def get_event_auth(self, destination, room_id, event_id):\n        path = _create_v1_path(\"/event_auth/%s/%s\", room_id, event_id)\n\n        content = await self.client.get_json(destination=destination, path=path)\n\n        return content\n\n    @log_function\n    async def query_client_keys(self, destination, query_content, timeout):\n        \"\"\"Query the device keys for a list of user ids hosted on a remote\n        server.\n\n        Request:\n            {\n              \"device_keys\": {\n                \"<user_id>\": [\"<device_id>\"]\n              }\n            }\n\n        Response:\n            {\n              \"device_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": {...}\n                }\n              },\n              \"master_key\": {\n                \"<user_id>\": {...}\n                }\n              },\n              \"self_signing_key\": {\n                \"<user_id>\": {...}\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing device and cross-signing keys.\n        \"\"\"\n        path = _create_v1_path(\"/user/keys/query\")\n\n        content = await self.client.post_json(\n            destination=destination, path=path, data=query_content, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def query_user_devices(self, destination, user_id, timeout):\n        \"\"\"Query the devices for a user id hosted on a remote server.\n\n        Response:\n            {\n              \"stream_id\": \"...\",\n              \"devices\": [ { ... } ],\n              \"master_key\": {\n                \"user_id\": \"<user_id>\",\n                \"usage\": [...],\n                \"keys\": {...},\n                \"signatures\": {\n                  \"<user_id>\": {...}\n                }\n              },\n              \"self_signing_key\": {\n                \"user_id\": \"<user_id>\",\n                \"usage\": [...],\n                \"keys\": {...},\n                \"signatures\": {\n                  \"<user_id>\": {...}\n                }\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing device and cross-signing keys.\n        \"\"\"\n        path = _create_v1_path(\"/user/devices/%s\", user_id)\n\n        content = await self.client.get_json(\n            destination=destination, path=path, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def claim_client_keys(self, destination, query_content, timeout):\n        \"\"\"Claim one-time keys for a list of devices hosted on a remote server.\n\n        Request:\n            {\n              \"one_time_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": \"<algorithm>\"\n                }\n              }\n            }\n\n        Response:\n            {\n              \"device_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": {\n                    \"<algorithm>:<key_id>\": \"<key_base64>\"\n                  }\n                }\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing the one-time keys.\n        \"\"\"\n\n        path = _create_v1_path(\"/user/keys/claim\")\n\n        content = await self.client.post_json(\n            destination=destination, path=path, data=query_content, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def get_missing_events(\n        self,\n        destination,\n        room_id,\n        earliest_events,\n        latest_events,\n        limit,\n        min_depth,\n        timeout,\n    ):\n        path = _create_v1_path(\"/get_missing_events/%s\", room_id)\n\n        content = await self.client.post_json(\n            destination=destination,\n            path=path,\n            data={\n                \"limit\": int(limit),\n                \"min_depth\": int(min_depth),\n                \"earliest_events\": earliest_events,\n                \"latest_events\": latest_events,\n            },\n            timeout=timeout,\n        )\n\n        return content\n\n    @log_function\n    def get_group_profile(self, destination, group_id, requester_user_id):\n        \"\"\"Get a group profile\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/profile\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_profile(self, destination, group_id, requester_user_id, content):\n        \"\"\"Update a remote group profile\n\n        Args:\n            destination (str)\n            group_id (str)\n            requester_user_id (str)\n            content (dict): The new profile of the group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/profile\", group_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_summary(self, destination, group_id, requester_user_id):\n        \"\"\"Get a group summary\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/summary\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_rooms_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get all rooms in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/rooms\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    def add_room_to_group(\n        self, destination, group_id, requester_user_id, room_id, content\n    ):\n        \"\"\"Add a room to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    def update_room_in_group(\n        self, destination, group_id, requester_user_id, room_id, config_key, content\n    ):\n        \"\"\"Update room in group\n        \"\"\"\n        path = _create_v1_path(\n            \"/groups/%s/room/%s/config/%s\", group_id, room_id, config_key\n        )\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    def remove_room_from_group(self, destination, group_id, requester_user_id, room_id):\n        \"\"\"Remove a room from a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_users_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get users in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_invited_users_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get users that have been invited to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/invited_users\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def accept_group_invite(self, destination, group_id, user_id, content):\n        \"\"\"Accept a group invite\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/accept_invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def join_group(self, destination, group_id, user_id, content):\n        \"\"\"Attempts to join a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/join\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def invite_to_group(\n        self, destination, group_id, user_id, requester_user_id, content\n    ):\n        \"\"\"Invite a user to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def invite_to_group_notification(self, destination, group_id, user_id, content):\n        \"\"\"Sent by group server to inform a user's server that they have been\n        invited.\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/local/%s/users/%s/invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def remove_user_from_group(\n        self, destination, group_id, requester_user_id, user_id, content\n    ):\n        \"\"\"Remove a user from a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/remove\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def remove_user_from_group_notification(\n        self, destination, group_id, user_id, content\n    ):\n        \"\"\"Sent by group server to inform a user's server that they have been\n        kicked from the group.\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/local/%s/users/%s/remove\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def renew_group_attestation(self, destination, group_id, user_id, content):\n        \"\"\"Sent by either a group server or a user's server to periodically update\n        the attestations\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/%s/renew_attestation/%s\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def update_group_summary_room(\n        self, destination, group_id, user_id, room_id, category_id, content\n    ):\n        \"\"\"Update a room entry in a group summary\n        \"\"\"\n        if category_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/categories/%s/rooms/%s\",\n                group_id,\n                category_id,\n                room_id,\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_summary_room(\n        self, destination, group_id, user_id, room_id, category_id\n    ):\n        \"\"\"Delete a room entry in a group summary\n        \"\"\"\n        if category_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/categories/%s/rooms/%s\",\n                group_id,\n                category_id,\n                room_id,\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_categories(self, destination, group_id, requester_user_id):\n        \"\"\"Get all categories in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_category(self, destination, group_id, requester_user_id, category_id):\n        \"\"\"Get category info in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_category(\n        self, destination, group_id, requester_user_id, category_id, content\n    ):\n        \"\"\"Update a category in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_category(\n        self, destination, group_id, requester_user_id, category_id\n    ):\n        \"\"\"Delete a category in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_roles(self, destination, group_id, requester_user_id):\n        \"\"\"Get all roles in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_role(self, destination, group_id, requester_user_id, role_id):\n        \"\"\"Get a roles info\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_role(\n        self, destination, group_id, requester_user_id, role_id, content\n    ):\n        \"\"\"Update a role in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_role(self, destination, group_id, requester_user_id, role_id):\n        \"\"\"Delete a role in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_summary_user(\n        self, destination, group_id, requester_user_id, user_id, role_id, content\n    ):\n        \"\"\"Update a users entry in a group\n        \"\"\"\n        if role_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def set_group_join_policy(self, destination, group_id, requester_user_id, content):\n        \"\"\"Sets the join policy for a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/settings/m.join_policy\", group_id)\n\n        return self.client.put_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_summary_user(\n        self, destination, group_id, requester_user_id, user_id, role_id\n    ):\n        \"\"\"Delete a users entry in a group\n        \"\"\"\n        if role_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    def bulk_get_publicised_groups(self, destination, user_ids):\n        \"\"\"Get the groups a list of users are publicising\n        \"\"\"\n\n        path = _create_v1_path(\"/get_groups_publicised\")\n\n        content = {\"user_ids\": user_ids}\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    def get_room_complexity(self, destination, room_id):\n        \"\"\"\n        Args:\n            destination (str): The remote server\n            room_id (str): The room ID to ask about.\n        \"\"\"\n        path = _create_path(FEDERATION_UNSTABLE_PREFIX, \"/rooms/%s/complexity\", room_id)\n\n        return self.client.get_json(destination=destination, path=path)\n\n\ndef _create_path(federation_prefix, path, *args):\n    \"\"\"\n    Ensures that all args are url encoded.\n    \"\"\"\n    return federation_prefix + path % tuple(urllib.parse.quote(arg, \"\") for arg in args)\n\n\ndef _create_v1_path(path, *args):\n    \"\"\"Creates a path against V1 federation API from the path template and\n    args. Ensures that all args are url encoded.\n\n    Example:\n\n        _create_v1_path(\"/event/%s\", event_id)\n\n    Args:\n        path (str): String template for the path\n        args: ([str]): Args to insert into path. Each arg will be url encoded\n\n    Returns:\n        str\n    \"\"\"\n    return _create_path(FEDERATION_V1_PREFIX, path, *args)\n\n\ndef _create_v2_path(path, *args):\n    \"\"\"Creates a path against V2 federation API from the path template and\n    args. Ensures that all args are url encoded.\n\n    Example:\n\n        _create_v2_path(\"/event/%s\", event_id)\n\n    Args:\n        path (str): String template for the path\n        args: ([str]): Args to insert into path. Each arg will be url encoded\n\n    Returns:\n        str\n    \"\"\"\n    return _create_path(FEDERATION_V2_PREFIX, path, *args)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport urllib\nfrom typing import Any, Dict, Optional\n\nfrom synapse.api.constants import Membership\nfrom synapse.api.errors import Codes, HttpResponseException, SynapseError\nfrom synapse.api.urls import (\n    FEDERATION_UNSTABLE_PREFIX,\n    FEDERATION_V1_PREFIX,\n    FEDERATION_V2_PREFIX,\n)\nfrom synapse.logging.utils import log_function\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransportLayerClient:\n    \"\"\"Sends federation HTTP requests to other servers\"\"\"\n\n    def __init__(self, hs):\n        self.server_name = hs.hostname\n        self.client = hs.get_http_client()\n\n    @log_function\n    def get_room_state_ids(self, destination, room_id, event_id):\n        \"\"\" Requests all state for a given room from the given server at the\n        given event. Returns the state's event_id's\n\n        Args:\n            destination (str): The host name of the remote homeserver we want\n                to get the state from.\n            context (str): The name of the context we want the state of\n            event_id (str): The event we want the context at.\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\"get_room_state_ids dest=%s, room=%s\", destination, room_id)\n\n        path = _create_v1_path(\"/state_ids/%s\", room_id)\n        return self.client.get_json(\n            destination,\n            path=path,\n            args={\"event_id\": event_id},\n            try_trailing_slash_on_400=True,\n        )\n\n    @log_function\n    def get_event(self, destination, event_id, timeout=None):\n        \"\"\" Requests the pdu with give id and origin from the given server.\n\n        Args:\n            destination (str): The host name of the remote homeserver we want\n                to get the state from.\n            event_id (str): The id of the event being requested.\n            timeout (int): How long to try (in ms) the destination for before\n                giving up. None indicates no timeout.\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\"get_pdu dest=%s, event_id=%s\", destination, event_id)\n\n        path = _create_v1_path(\"/event/%s\", event_id)\n        return self.client.get_json(\n            destination, path=path, timeout=timeout, try_trailing_slash_on_400=True\n        )\n\n    @log_function\n    def backfill(self, destination, room_id, event_tuples, limit):\n        \"\"\" Requests `limit` previous PDUs in a given context before list of\n        PDUs.\n\n        Args:\n            dest (str)\n            room_id (str)\n            event_tuples (list)\n            limit (int)\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\n            \"backfill dest=%s, room_id=%s, event_tuples=%r, limit=%s\",\n            destination,\n            room_id,\n            event_tuples,\n            str(limit),\n        )\n\n        if not event_tuples:\n            # TODO: raise?\n            return\n\n        path = _create_v1_path(\"/backfill/%s\", room_id)\n\n        args = {\"v\": event_tuples, \"limit\": [str(limit)]}\n\n        return self.client.get_json(\n            destination, path=path, args=args, try_trailing_slash_on_400=True\n        )\n\n    @log_function\n    async def send_transaction(self, transaction, json_data_callback=None):\n        \"\"\" Sends the given Transaction to its destination\n\n        Args:\n            transaction (Transaction)\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The result\n            will be the decoded JSON body.\n\n            Fails with ``HTTPRequestException`` if we get an HTTP response\n            code >= 300.\n\n            Fails with ``NotRetryingDestination`` if we are not yet ready\n            to retry this server.\n\n            Fails with ``FederationDeniedError`` if this destination\n            is not on our federation whitelist\n        \"\"\"\n        logger.debug(\n            \"send_data dest=%s, txid=%s\",\n            transaction.destination,\n            transaction.transaction_id,\n        )\n\n        if transaction.destination == self.server_name:\n            raise RuntimeError(\"Transport layer cannot send to itself!\")\n\n        # FIXME: This is only used by the tests. The actual json sent is\n        # generated by the json_data_callback.\n        json_data = transaction.get_dict()\n\n        path = _create_v1_path(\"/send/%s\", transaction.transaction_id)\n\n        response = await self.client.put_json(\n            transaction.destination,\n            path=path,\n            data=json_data,\n            json_data_callback=json_data_callback,\n            long_retries=True,\n            backoff_on_404=True,  # If we get a 404 the other side has gone\n            try_trailing_slash_on_400=True,\n        )\n\n        return response\n\n    @log_function\n    async def make_query(\n        self, destination, query_type, args, retry_on_dns_fail, ignore_backoff=False\n    ):\n        path = _create_v1_path(\"/query/%s\", query_type)\n\n        content = await self.client.get_json(\n            destination=destination,\n            path=path,\n            args=args,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=10000,\n            ignore_backoff=ignore_backoff,\n        )\n\n        return content\n\n    @log_function\n    async def make_membership_event(\n        self, destination, room_id, user_id, membership, params\n    ):\n        \"\"\"Asks a remote server to build and sign us a membership event\n\n        Note that this does not append any events to any graphs.\n\n        Args:\n            destination (str): address of remote homeserver\n            room_id (str): room to join/leave\n            user_id (str): user to be joined/left\n            membership (str): one of join/leave\n            params (dict[str, str|Iterable[str]]): Query parameters to include in the\n                request.\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The result\n            will be the decoded JSON body (ie, the new event).\n\n            Fails with ``HTTPRequestException`` if we get an HTTP response\n            code >= 300.\n\n            Fails with ``NotRetryingDestination`` if we are not yet ready\n            to retry this server.\n\n            Fails with ``FederationDeniedError`` if the remote destination\n            is not in our federation whitelist\n        \"\"\"\n        valid_memberships = {Membership.JOIN, Membership.LEAVE}\n        if membership not in valid_memberships:\n            raise RuntimeError(\n                \"make_membership_event called with membership='%s', must be one of %s\"\n                % (membership, \",\".join(valid_memberships))\n            )\n        path = _create_v1_path(\"/make_%s/%s/%s\", membership, room_id, user_id)\n\n        ignore_backoff = False\n        retry_on_dns_fail = False\n\n        if membership == Membership.LEAVE:\n            # we particularly want to do our best to send leave events. The\n            # problem is that if it fails, we won't retry it later, so if the\n            # remote server was just having a momentary blip, the room will be\n            # out of sync.\n            ignore_backoff = True\n            retry_on_dns_fail = True\n\n        content = await self.client.get_json(\n            destination=destination,\n            path=path,\n            args=params,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=20000,\n            ignore_backoff=ignore_backoff,\n        )\n\n        return content\n\n    @log_function\n    async def send_join_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/send_join/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content\n        )\n\n        return response\n\n    @log_function\n    async def send_join_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/send_join/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content\n        )\n\n        return response\n\n    @log_function\n    async def send_leave_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/send_leave/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination,\n            path=path,\n            data=content,\n            # we want to do our best to send this through. The problem is\n            # that if it fails, we won't retry it later, so if the remote\n            # server was just having a momentary blip, the room will be out of\n            # sync.\n            ignore_backoff=True,\n        )\n\n        return response\n\n    @log_function\n    async def send_leave_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/send_leave/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination,\n            path=path,\n            data=content,\n            # we want to do our best to send this through. The problem is\n            # that if it fails, we won't retry it later, so if the remote\n            # server was just having a momentary blip, the room will be out of\n            # sync.\n            ignore_backoff=True,\n        )\n\n        return response\n\n    @log_function\n    async def send_invite_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/invite/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n        return response\n\n    @log_function\n    async def send_invite_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/invite/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n        return response\n\n    @log_function\n    async def get_public_rooms(\n        self,\n        remote_server: str,\n        limit: Optional[int] = None,\n        since_token: Optional[str] = None,\n        search_filter: Optional[Dict] = None,\n        include_all_networks: bool = False,\n        third_party_instance_id: Optional[str] = None,\n    ):\n        \"\"\"Get the list of public rooms from a remote homeserver\n\n        See synapse.federation.federation_client.FederationClient.get_public_rooms for\n        more information.\n        \"\"\"\n        if search_filter:\n            # this uses MSC2197 (Search Filtering over Federation)\n            path = _create_v1_path(\"/publicRooms\")\n\n            data = {\n                \"include_all_networks\": \"true\" if include_all_networks else \"false\"\n            }  # type: Dict[str, Any]\n            if third_party_instance_id:\n                data[\"third_party_instance_id\"] = third_party_instance_id\n            if limit:\n                data[\"limit\"] = str(limit)\n            if since_token:\n                data[\"since\"] = since_token\n\n            data[\"filter\"] = search_filter\n\n            try:\n                response = await self.client.post_json(\n                    destination=remote_server, path=path, data=data, ignore_backoff=True\n                )\n            except HttpResponseException as e:\n                if e.code == 403:\n                    raise SynapseError(\n                        403,\n                        \"You are not allowed to view the public rooms list of %s\"\n                        % (remote_server,),\n                        errcode=Codes.FORBIDDEN,\n                    )\n                raise\n        else:\n            path = _create_v1_path(\"/publicRooms\")\n\n            args = {\n                \"include_all_networks\": \"true\" if include_all_networks else \"false\"\n            }  # type: Dict[str, Any]\n            if third_party_instance_id:\n                args[\"third_party_instance_id\"] = (third_party_instance_id,)\n            if limit:\n                args[\"limit\"] = [str(limit)]\n            if since_token:\n                args[\"since\"] = [since_token]\n\n            try:\n                response = await self.client.get_json(\n                    destination=remote_server, path=path, args=args, ignore_backoff=True\n                )\n            except HttpResponseException as e:\n                if e.code == 403:\n                    raise SynapseError(\n                        403,\n                        \"You are not allowed to view the public rooms list of %s\"\n                        % (remote_server,),\n                        errcode=Codes.FORBIDDEN,\n                    )\n                raise\n\n        return response\n\n    @log_function\n    async def exchange_third_party_invite(self, destination, room_id, event_dict):\n        path = _create_v1_path(\"/exchange_third_party_invite/%s\", room_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=event_dict\n        )\n\n        return response\n\n    @log_function\n    async def get_event_auth(self, destination, room_id, event_id):\n        path = _create_v1_path(\"/event_auth/%s/%s\", room_id, event_id)\n\n        content = await self.client.get_json(destination=destination, path=path)\n\n        return content\n\n    @log_function\n    async def query_client_keys(self, destination, query_content, timeout):\n        \"\"\"Query the device keys for a list of user ids hosted on a remote\n        server.\n\n        Request:\n            {\n              \"device_keys\": {\n                \"<user_id>\": [\"<device_id>\"]\n              }\n            }\n\n        Response:\n            {\n              \"device_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": {...}\n                }\n              },\n              \"master_key\": {\n                \"<user_id>\": {...}\n                }\n              },\n              \"self_signing_key\": {\n                \"<user_id>\": {...}\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing device and cross-signing keys.\n        \"\"\"\n        path = _create_v1_path(\"/user/keys/query\")\n\n        content = await self.client.post_json(\n            destination=destination, path=path, data=query_content, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def query_user_devices(self, destination, user_id, timeout):\n        \"\"\"Query the devices for a user id hosted on a remote server.\n\n        Response:\n            {\n              \"stream_id\": \"...\",\n              \"devices\": [ { ... } ],\n              \"master_key\": {\n                \"user_id\": \"<user_id>\",\n                \"usage\": [...],\n                \"keys\": {...},\n                \"signatures\": {\n                  \"<user_id>\": {...}\n                }\n              },\n              \"self_signing_key\": {\n                \"user_id\": \"<user_id>\",\n                \"usage\": [...],\n                \"keys\": {...},\n                \"signatures\": {\n                  \"<user_id>\": {...}\n                }\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing device and cross-signing keys.\n        \"\"\"\n        path = _create_v1_path(\"/user/devices/%s\", user_id)\n\n        content = await self.client.get_json(\n            destination=destination, path=path, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def claim_client_keys(self, destination, query_content, timeout):\n        \"\"\"Claim one-time keys for a list of devices hosted on a remote server.\n\n        Request:\n            {\n              \"one_time_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": \"<algorithm>\"\n                }\n              }\n            }\n\n        Response:\n            {\n              \"device_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": {\n                    \"<algorithm>:<key_id>\": \"<key_base64>\"\n                  }\n                }\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing the one-time keys.\n        \"\"\"\n\n        path = _create_v1_path(\"/user/keys/claim\")\n\n        content = await self.client.post_json(\n            destination=destination, path=path, data=query_content, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def get_missing_events(\n        self,\n        destination,\n        room_id,\n        earliest_events,\n        latest_events,\n        limit,\n        min_depth,\n        timeout,\n    ):\n        path = _create_v1_path(\"/get_missing_events/%s\", room_id)\n\n        content = await self.client.post_json(\n            destination=destination,\n            path=path,\n            data={\n                \"limit\": int(limit),\n                \"min_depth\": int(min_depth),\n                \"earliest_events\": earliest_events,\n                \"latest_events\": latest_events,\n            },\n            timeout=timeout,\n        )\n\n        return content\n\n    @log_function\n    def get_group_profile(self, destination, group_id, requester_user_id):\n        \"\"\"Get a group profile\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/profile\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_profile(self, destination, group_id, requester_user_id, content):\n        \"\"\"Update a remote group profile\n\n        Args:\n            destination (str)\n            group_id (str)\n            requester_user_id (str)\n            content (dict): The new profile of the group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/profile\", group_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_summary(self, destination, group_id, requester_user_id):\n        \"\"\"Get a group summary\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/summary\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_rooms_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get all rooms in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/rooms\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    def add_room_to_group(\n        self, destination, group_id, requester_user_id, room_id, content\n    ):\n        \"\"\"Add a room to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    def update_room_in_group(\n        self, destination, group_id, requester_user_id, room_id, config_key, content\n    ):\n        \"\"\"Update room in group\n        \"\"\"\n        path = _create_v1_path(\n            \"/groups/%s/room/%s/config/%s\", group_id, room_id, config_key\n        )\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    def remove_room_from_group(self, destination, group_id, requester_user_id, room_id):\n        \"\"\"Remove a room from a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_users_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get users in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_invited_users_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get users that have been invited to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/invited_users\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def accept_group_invite(self, destination, group_id, user_id, content):\n        \"\"\"Accept a group invite\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/accept_invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def join_group(self, destination, group_id, user_id, content):\n        \"\"\"Attempts to join a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/join\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def invite_to_group(\n        self, destination, group_id, user_id, requester_user_id, content\n    ):\n        \"\"\"Invite a user to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def invite_to_group_notification(self, destination, group_id, user_id, content):\n        \"\"\"Sent by group server to inform a user's server that they have been\n        invited.\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/local/%s/users/%s/invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def remove_user_from_group(\n        self, destination, group_id, requester_user_id, user_id, content\n    ):\n        \"\"\"Remove a user from a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/remove\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def remove_user_from_group_notification(\n        self, destination, group_id, user_id, content\n    ):\n        \"\"\"Sent by group server to inform a user's server that they have been\n        kicked from the group.\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/local/%s/users/%s/remove\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def renew_group_attestation(self, destination, group_id, user_id, content):\n        \"\"\"Sent by either a group server or a user's server to periodically update\n        the attestations\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/%s/renew_attestation/%s\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def update_group_summary_room(\n        self, destination, group_id, user_id, room_id, category_id, content\n    ):\n        \"\"\"Update a room entry in a group summary\n        \"\"\"\n        if category_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/categories/%s/rooms/%s\",\n                group_id,\n                category_id,\n                room_id,\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_summary_room(\n        self, destination, group_id, user_id, room_id, category_id\n    ):\n        \"\"\"Delete a room entry in a group summary\n        \"\"\"\n        if category_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/categories/%s/rooms/%s\",\n                group_id,\n                category_id,\n                room_id,\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_categories(self, destination, group_id, requester_user_id):\n        \"\"\"Get all categories in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_category(self, destination, group_id, requester_user_id, category_id):\n        \"\"\"Get category info in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_category(\n        self, destination, group_id, requester_user_id, category_id, content\n    ):\n        \"\"\"Update a category in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_category(\n        self, destination, group_id, requester_user_id, category_id\n    ):\n        \"\"\"Delete a category in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_roles(self, destination, group_id, requester_user_id):\n        \"\"\"Get all roles in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_role(self, destination, group_id, requester_user_id, role_id):\n        \"\"\"Get a roles info\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_role(\n        self, destination, group_id, requester_user_id, role_id, content\n    ):\n        \"\"\"Update a role in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_role(self, destination, group_id, requester_user_id, role_id):\n        \"\"\"Delete a role in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_summary_user(\n        self, destination, group_id, requester_user_id, user_id, role_id, content\n    ):\n        \"\"\"Update a users entry in a group\n        \"\"\"\n        if role_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def set_group_join_policy(self, destination, group_id, requester_user_id, content):\n        \"\"\"Sets the join policy for a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/settings/m.join_policy\", group_id)\n\n        return self.client.put_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_summary_user(\n        self, destination, group_id, requester_user_id, user_id, role_id\n    ):\n        \"\"\"Delete a users entry in a group\n        \"\"\"\n        if role_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    def bulk_get_publicised_groups(self, destination, user_ids):\n        \"\"\"Get the groups a list of users are publicising\n        \"\"\"\n\n        path = _create_v1_path(\"/get_groups_publicised\")\n\n        content = {\"user_ids\": user_ids}\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    def get_room_complexity(self, destination, room_id):\n        \"\"\"\n        Args:\n            destination (str): The remote server\n            room_id (str): The room ID to ask about.\n        \"\"\"\n        path = _create_path(FEDERATION_UNSTABLE_PREFIX, \"/rooms/%s/complexity\", room_id)\n\n        return self.client.get_json(destination=destination, path=path)\n\n\ndef _create_path(federation_prefix, path, *args):\n    \"\"\"\n    Ensures that all args are url encoded.\n    \"\"\"\n    return federation_prefix + path % tuple(urllib.parse.quote(arg, \"\") for arg in args)\n\n\ndef _create_v1_path(path, *args):\n    \"\"\"Creates a path against V1 federation API from the path template and\n    args. Ensures that all args are url encoded.\n\n    Example:\n\n        _create_v1_path(\"/event/%s\", event_id)\n\n    Args:\n        path (str): String template for the path\n        args: ([str]): Args to insert into path. Each arg will be url encoded\n\n    Returns:\n        str\n    \"\"\"\n    return _create_path(FEDERATION_V1_PREFIX, path, *args)\n\n\ndef _create_v2_path(path, *args):\n    \"\"\"Creates a path against V2 federation API from the path template and\n    args. Ensures that all args are url encoded.\n\n    Example:\n\n        _create_v2_path(\"/event/%s\", event_id)\n\n    Args:\n        path (str): String template for the path\n        args: ([str]): Args to insert into path. Each arg will be url encoded\n\n    Returns:\n        str\n    \"\"\"\n    return _create_path(FEDERATION_V2_PREFIX, path, *args)\n", "patch": "@@ -35,7 +35,7 @@ class TransportLayerClient:\n \n     def __init__(self, hs):\n         self.server_name = hs.hostname\n-        self.client = hs.get_http_client()\n+        self.client = hs.get_federation_http_client()\n \n     @log_function\n     def get_room_state_ids(self, destination, room_id, event_id):", "file_path": "files/2021_2/21", "file_language": "py", "file_name": "synapse/federation/transport/client.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class TransportLayerClient:\n    \"\"\"Sends federation HTTP requests to other servers\"\"\"\n\n    def __init__(self, hs):\n        self.server_name = hs.hostname\n        self.client = hs.get_http_client()\n\n    @log_function\n    def get_room_state_ids(self, destination, room_id, event_id):\n        \"\"\" Requests all state for a given room from the given server at the\n        given event. Returns the state's event_id's\n\n        Args:\n            destination (str): The host name of the remote homeserver we want\n                to get the state from.\n            context (str): The name of the context we want the state of\n            event_id (str): The event we want the context at.\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\"get_room_state_ids dest=%s, room=%s\", destination, room_id)\n\n        path = _create_v1_path(\"/state_ids/%s\", room_id)\n        return self.client.get_json(\n            destination,\n            path=path,\n            args={\"event_id\": event_id},\n            try_trailing_slash_on_400=True,\n        )\n\n    @log_function\n    def get_event(self, destination, event_id, timeout=None):\n        \"\"\" Requests the pdu with give id and origin from the given server.\n\n        Args:\n            destination (str): The host name of the remote homeserver we want\n                to get the state from.\n            event_id (str): The id of the event being requested.\n            timeout (int): How long to try (in ms) the destination for before\n                giving up. None indicates no timeout.\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\"get_pdu dest=%s, event_id=%s\", destination, event_id)\n\n        path = _create_v1_path(\"/event/%s\", event_id)\n        return self.client.get_json(\n            destination, path=path, timeout=timeout, try_trailing_slash_on_400=True\n        )\n\n    @log_function\n    def backfill(self, destination, room_id, event_tuples, limit):\n        \"\"\" Requests `limit` previous PDUs in a given context before list of\n        PDUs.\n\n        Args:\n            dest (str)\n            room_id (str)\n            event_tuples (list)\n            limit (int)\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\n            \"backfill dest=%s, room_id=%s, event_tuples=%r, limit=%s\",\n            destination,\n            room_id,\n            event_tuples,\n            str(limit),\n        )\n\n        if not event_tuples:\n            # TODO: raise?\n            return\n\n        path = _create_v1_path(\"/backfill/%s\", room_id)\n\n        args = {\"v\": event_tuples, \"limit\": [str(limit)]}\n\n        return self.client.get_json(\n            destination, path=path, args=args, try_trailing_slash_on_400=True\n        )\n\n    @log_function\n    async def send_transaction(self, transaction, json_data_callback=None):\n        \"\"\" Sends the given Transaction to its destination\n\n        Args:\n            transaction (Transaction)\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The result\n            will be the decoded JSON body.\n\n            Fails with ``HTTPRequestException`` if we get an HTTP response\n            code >= 300.\n\n            Fails with ``NotRetryingDestination`` if we are not yet ready\n            to retry this server.\n\n            Fails with ``FederationDeniedError`` if this destination\n            is not on our federation whitelist\n        \"\"\"\n        logger.debug(\n            \"send_data dest=%s, txid=%s\",\n            transaction.destination,\n            transaction.transaction_id,\n        )\n\n        if transaction.destination == self.server_name:\n            raise RuntimeError(\"Transport layer cannot send to itself!\")\n\n        # FIXME: This is only used by the tests. The actual json sent is\n        # generated by the json_data_callback.\n        json_data = transaction.get_dict()\n\n        path = _create_v1_path(\"/send/%s\", transaction.transaction_id)\n\n        response = await self.client.put_json(\n            transaction.destination,\n            path=path,\n            data=json_data,\n            json_data_callback=json_data_callback,\n            long_retries=True,\n            backoff_on_404=True,  # If we get a 404 the other side has gone\n            try_trailing_slash_on_400=True,\n        )\n\n        return response\n\n    @log_function\n    async def make_query(\n        self, destination, query_type, args, retry_on_dns_fail, ignore_backoff=False\n    ):\n        path = _create_v1_path(\"/query/%s\", query_type)\n\n        content = await self.client.get_json(\n            destination=destination,\n            path=path,\n            args=args,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=10000,\n            ignore_backoff=ignore_backoff,\n        )\n\n        return content\n\n    @log_function\n    async def make_membership_event(\n        self, destination, room_id, user_id, membership, params\n    ):\n        \"\"\"Asks a remote server to build and sign us a membership event\n\n        Note that this does not append any events to any graphs.\n\n        Args:\n            destination (str): address of remote homeserver\n            room_id (str): room to join/leave\n            user_id (str): user to be joined/left\n            membership (str): one of join/leave\n            params (dict[str, str|Iterable[str]]): Query parameters to include in the\n                request.\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The result\n            will be the decoded JSON body (ie, the new event).\n\n            Fails with ``HTTPRequestException`` if we get an HTTP response\n            code >= 300.\n\n            Fails with ``NotRetryingDestination`` if we are not yet ready\n            to retry this server.\n\n            Fails with ``FederationDeniedError`` if the remote destination\n            is not in our federation whitelist\n        \"\"\"\n        valid_memberships = {Membership.JOIN, Membership.LEAVE}\n        if membership not in valid_memberships:\n            raise RuntimeError(\n                \"make_membership_event called with membership='%s', must be one of %s\"\n                % (membership, \",\".join(valid_memberships))\n            )\n        path = _create_v1_path(\"/make_%s/%s/%s\", membership, room_id, user_id)\n\n        ignore_backoff = False\n        retry_on_dns_fail = False\n\n        if membership == Membership.LEAVE:\n            # we particularly want to do our best to send leave events. The\n            # problem is that if it fails, we won't retry it later, so if the\n            # remote server was just having a momentary blip, the room will be\n            # out of sync.\n            ignore_backoff = True\n            retry_on_dns_fail = True\n\n        content = await self.client.get_json(\n            destination=destination,\n            path=path,\n            args=params,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=20000,\n            ignore_backoff=ignore_backoff,\n        )\n\n        return content\n\n    @log_function\n    async def send_join_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/send_join/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content\n        )\n\n        return response\n\n    @log_function\n    async def send_join_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/send_join/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content\n        )\n\n        return response\n\n    @log_function\n    async def send_leave_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/send_leave/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination,\n            path=path,\n            data=content,\n            # we want to do our best to send this through. The problem is\n            # that if it fails, we won't retry it later, so if the remote\n            # server was just having a momentary blip, the room will be out of\n            # sync.\n            ignore_backoff=True,\n        )\n\n        return response\n\n    @log_function\n    async def send_leave_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/send_leave/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination,\n            path=path,\n            data=content,\n            # we want to do our best to send this through. The problem is\n            # that if it fails, we won't retry it later, so if the remote\n            # server was just having a momentary blip, the room will be out of\n            # sync.\n            ignore_backoff=True,\n        )\n\n        return response\n\n    @log_function\n    async def send_invite_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/invite/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n        return response\n\n    @log_function\n    async def send_invite_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/invite/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n        return response\n\n    @log_function\n    async def get_public_rooms(\n        self,\n        remote_server: str,\n        limit: Optional[int] = None,\n        since_token: Optional[str] = None,\n        search_filter: Optional[Dict] = None,\n        include_all_networks: bool = False,\n        third_party_instance_id: Optional[str] = None,\n    ):\n        \"\"\"Get the list of public rooms from a remote homeserver\n\n        See synapse.federation.federation_client.FederationClient.get_public_rooms for\n        more information.\n        \"\"\"\n        if search_filter:\n            # this uses MSC2197 (Search Filtering over Federation)\n            path = _create_v1_path(\"/publicRooms\")\n\n            data = {\n                \"include_all_networks\": \"true\" if include_all_networks else \"false\"\n            }  # type: Dict[str, Any]\n            if third_party_instance_id:\n                data[\"third_party_instance_id\"] = third_party_instance_id\n            if limit:\n                data[\"limit\"] = str(limit)\n            if since_token:\n                data[\"since\"] = since_token\n\n            data[\"filter\"] = search_filter\n\n            try:\n                response = await self.client.post_json(\n                    destination=remote_server, path=path, data=data, ignore_backoff=True\n                )\n            except HttpResponseException as e:\n                if e.code == 403:\n                    raise SynapseError(\n                        403,\n                        \"You are not allowed to view the public rooms list of %s\"\n                        % (remote_server,),\n                        errcode=Codes.FORBIDDEN,\n                    )\n                raise\n        else:\n            path = _create_v1_path(\"/publicRooms\")\n\n            args = {\n                \"include_all_networks\": \"true\" if include_all_networks else \"false\"\n            }  # type: Dict[str, Any]\n            if third_party_instance_id:\n                args[\"third_party_instance_id\"] = (third_party_instance_id,)\n            if limit:\n                args[\"limit\"] = [str(limit)]\n            if since_token:\n                args[\"since\"] = [since_token]\n\n            try:\n                response = await self.client.get_json(\n                    destination=remote_server, path=path, args=args, ignore_backoff=True\n                )\n            except HttpResponseException as e:\n                if e.code == 403:\n                    raise SynapseError(\n                        403,\n                        \"You are not allowed to view the public rooms list of %s\"\n                        % (remote_server,),\n                        errcode=Codes.FORBIDDEN,\n                    )\n                raise\n\n        return response\n\n    @log_function\n    async def exchange_third_party_invite(self, destination, room_id, event_dict):\n        path = _create_v1_path(\"/exchange_third_party_invite/%s\", room_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=event_dict\n        )\n\n        return response\n\n    @log_function\n    async def get_event_auth(self, destination, room_id, event_id):\n        path = _create_v1_path(\"/event_auth/%s/%s\", room_id, event_id)\n\n        content = await self.client.get_json(destination=destination, path=path)\n\n        return content\n\n    @log_function\n    async def query_client_keys(self, destination, query_content, timeout):\n        \"\"\"Query the device keys for a list of user ids hosted on a remote\n        server.\n\n        Request:\n            {\n              \"device_keys\": {\n                \"<user_id>\": [\"<device_id>\"]\n              }\n            }\n\n        Response:\n            {\n              \"device_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": {...}\n                }\n              },\n              \"master_key\": {\n                \"<user_id>\": {...}\n                }\n              },\n              \"self_signing_key\": {\n                \"<user_id>\": {...}\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing device and cross-signing keys.\n        \"\"\"\n        path = _create_v1_path(\"/user/keys/query\")\n\n        content = await self.client.post_json(\n            destination=destination, path=path, data=query_content, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def query_user_devices(self, destination, user_id, timeout):\n        \"\"\"Query the devices for a user id hosted on a remote server.\n\n        Response:\n            {\n              \"stream_id\": \"...\",\n              \"devices\": [ { ... } ],\n              \"master_key\": {\n                \"user_id\": \"<user_id>\",\n                \"usage\": [...],\n                \"keys\": {...},\n                \"signatures\": {\n                  \"<user_id>\": {...}\n                }\n              },\n              \"self_signing_key\": {\n                \"user_id\": \"<user_id>\",\n                \"usage\": [...],\n                \"keys\": {...},\n                \"signatures\": {\n                  \"<user_id>\": {...}\n                }\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing device and cross-signing keys.\n        \"\"\"\n        path = _create_v1_path(\"/user/devices/%s\", user_id)\n\n        content = await self.client.get_json(\n            destination=destination, path=path, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def claim_client_keys(self, destination, query_content, timeout):\n        \"\"\"Claim one-time keys for a list of devices hosted on a remote server.\n\n        Request:\n            {\n              \"one_time_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": \"<algorithm>\"\n                }\n              }\n            }\n\n        Response:\n            {\n              \"device_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": {\n                    \"<algorithm>:<key_id>\": \"<key_base64>\"\n                  }\n                }\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing the one-time keys.\n        \"\"\"\n\n        path = _create_v1_path(\"/user/keys/claim\")\n\n        content = await self.client.post_json(\n            destination=destination, path=path, data=query_content, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def get_missing_events(\n        self,\n        destination,\n        room_id,\n        earliest_events,\n        latest_events,\n        limit,\n        min_depth,\n        timeout,\n    ):\n        path = _create_v1_path(\"/get_missing_events/%s\", room_id)\n\n        content = await self.client.post_json(\n            destination=destination,\n            path=path,\n            data={\n                \"limit\": int(limit),\n                \"min_depth\": int(min_depth),\n                \"earliest_events\": earliest_events,\n                \"latest_events\": latest_events,\n            },\n            timeout=timeout,\n        )\n\n        return content\n\n    @log_function\n    def get_group_profile(self, destination, group_id, requester_user_id):\n        \"\"\"Get a group profile\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/profile\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_profile(self, destination, group_id, requester_user_id, content):\n        \"\"\"Update a remote group profile\n\n        Args:\n            destination (str)\n            group_id (str)\n            requester_user_id (str)\n            content (dict): The new profile of the group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/profile\", group_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_summary(self, destination, group_id, requester_user_id):\n        \"\"\"Get a group summary\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/summary\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_rooms_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get all rooms in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/rooms\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    def add_room_to_group(\n        self, destination, group_id, requester_user_id, room_id, content\n    ):\n        \"\"\"Add a room to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    def update_room_in_group(\n        self, destination, group_id, requester_user_id, room_id, config_key, content\n    ):\n        \"\"\"Update room in group\n        \"\"\"\n        path = _create_v1_path(\n            \"/groups/%s/room/%s/config/%s\", group_id, room_id, config_key\n        )\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    def remove_room_from_group(self, destination, group_id, requester_user_id, room_id):\n        \"\"\"Remove a room from a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_users_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get users in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_invited_users_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get users that have been invited to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/invited_users\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def accept_group_invite(self, destination, group_id, user_id, content):\n        \"\"\"Accept a group invite\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/accept_invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def join_group(self, destination, group_id, user_id, content):\n        \"\"\"Attempts to join a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/join\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def invite_to_group(\n        self, destination, group_id, user_id, requester_user_id, content\n    ):\n        \"\"\"Invite a user to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def invite_to_group_notification(self, destination, group_id, user_id, content):\n        \"\"\"Sent by group server to inform a user's server that they have been\n        invited.\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/local/%s/users/%s/invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def remove_user_from_group(\n        self, destination, group_id, requester_user_id, user_id, content\n    ):\n        \"\"\"Remove a user from a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/remove\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def remove_user_from_group_notification(\n        self, destination, group_id, user_id, content\n    ):\n        \"\"\"Sent by group server to inform a user's server that they have been\n        kicked from the group.\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/local/%s/users/%s/remove\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def renew_group_attestation(self, destination, group_id, user_id, content):\n        \"\"\"Sent by either a group server or a user's server to periodically update\n        the attestations\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/%s/renew_attestation/%s\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def update_group_summary_room(\n        self, destination, group_id, user_id, room_id, category_id, content\n    ):\n        \"\"\"Update a room entry in a group summary\n        \"\"\"\n        if category_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/categories/%s/rooms/%s\",\n                group_id,\n                category_id,\n                room_id,\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_summary_room(\n        self, destination, group_id, user_id, room_id, category_id\n    ):\n        \"\"\"Delete a room entry in a group summary\n        \"\"\"\n        if category_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/categories/%s/rooms/%s\",\n                group_id,\n                category_id,\n                room_id,\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_categories(self, destination, group_id, requester_user_id):\n        \"\"\"Get all categories in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_category(self, destination, group_id, requester_user_id, category_id):\n        \"\"\"Get category info in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_category(\n        self, destination, group_id, requester_user_id, category_id, content\n    ):\n        \"\"\"Update a category in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_category(\n        self, destination, group_id, requester_user_id, category_id\n    ):\n        \"\"\"Delete a category in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_roles(self, destination, group_id, requester_user_id):\n        \"\"\"Get all roles in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_role(self, destination, group_id, requester_user_id, role_id):\n        \"\"\"Get a roles info\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_role(\n        self, destination, group_id, requester_user_id, role_id, content\n    ):\n        \"\"\"Update a role in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_role(self, destination, group_id, requester_user_id, role_id):\n        \"\"\"Delete a role in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_summary_user(\n        self, destination, group_id, requester_user_id, user_id, role_id, content\n    ):\n        \"\"\"Update a users entry in a group\n        \"\"\"\n        if role_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def set_group_join_policy(self, destination, group_id, requester_user_id, content):\n        \"\"\"Sets the join policy for a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/settings/m.join_policy\", group_id)\n\n        return self.client.put_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_summary_user(\n        self, destination, group_id, requester_user_id, user_id, role_id\n    ):\n        \"\"\"Delete a users entry in a group\n        \"\"\"\n        if role_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    def bulk_get_publicised_groups(self, destination, user_ids):\n        \"\"\"Get the groups a list of users are publicising\n        \"\"\"\n\n        path = _create_v1_path(\"/get_groups_publicised\")\n\n        content = {\"user_ids\": user_ids}\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    def get_room_complexity(self, destination, room_id):\n        \"\"\"\n        Args:\n            destination (str): The remote server\n            room_id (str): The room ID to ask about.\n        \"\"\"\n        path = _create_path(FEDERATION_UNSTABLE_PREFIX, \"/rooms/%s/complexity\", room_id)\n\n        return self.client.get_json(destination=destination, path=path)", "target": 0}, {"function": "def _create_path(federation_prefix, path, *args):\n    \"\"\"\n    Ensures that all args are url encoded.\n    \"\"\"\n    return federation_prefix + path % tuple(urllib.parse.quote(arg, \"\") for arg in args)", "target": 0}, {"function": "def _create_v1_path(path, *args):\n    \"\"\"Creates a path against V1 federation API from the path template and\n    args. Ensures that all args are url encoded.\n\n    Example:\n\n        _create_v1_path(\"/event/%s\", event_id)\n\n    Args:\n        path (str): String template for the path\n        args: ([str]): Args to insert into path. Each arg will be url encoded\n\n    Returns:\n        str\n    \"\"\"\n    return _create_path(FEDERATION_V1_PREFIX, path, *args)", "target": 0}, {"function": "def _create_v2_path(path, *args):\n    \"\"\"Creates a path against V2 federation API from the path template and\n    args. Ensures that all args are url encoded.\n\n    Example:\n\n        _create_v2_path(\"/event/%s\", event_id)\n\n    Args:\n        path (str): String template for the path\n        args: ([str]): Args to insert into path. Each arg will be url encoded\n\n    Returns:\n        str\n    \"\"\"\n    return _create_path(FEDERATION_V2_PREFIX, path, *args)", "target": 0}], "function_after": [{"function": "class TransportLayerClient:\n    \"\"\"Sends federation HTTP requests to other servers\"\"\"\n\n    def __init__(self, hs):\n        self.server_name = hs.hostname\n        self.client = hs.get_federation_http_client()\n\n    @log_function\n    def get_room_state_ids(self, destination, room_id, event_id):\n        \"\"\" Requests all state for a given room from the given server at the\n        given event. Returns the state's event_id's\n\n        Args:\n            destination (str): The host name of the remote homeserver we want\n                to get the state from.\n            context (str): The name of the context we want the state of\n            event_id (str): The event we want the context at.\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\"get_room_state_ids dest=%s, room=%s\", destination, room_id)\n\n        path = _create_v1_path(\"/state_ids/%s\", room_id)\n        return self.client.get_json(\n            destination,\n            path=path,\n            args={\"event_id\": event_id},\n            try_trailing_slash_on_400=True,\n        )\n\n    @log_function\n    def get_event(self, destination, event_id, timeout=None):\n        \"\"\" Requests the pdu with give id and origin from the given server.\n\n        Args:\n            destination (str): The host name of the remote homeserver we want\n                to get the state from.\n            event_id (str): The id of the event being requested.\n            timeout (int): How long to try (in ms) the destination for before\n                giving up. None indicates no timeout.\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\"get_pdu dest=%s, event_id=%s\", destination, event_id)\n\n        path = _create_v1_path(\"/event/%s\", event_id)\n        return self.client.get_json(\n            destination, path=path, timeout=timeout, try_trailing_slash_on_400=True\n        )\n\n    @log_function\n    def backfill(self, destination, room_id, event_tuples, limit):\n        \"\"\" Requests `limit` previous PDUs in a given context before list of\n        PDUs.\n\n        Args:\n            dest (str)\n            room_id (str)\n            event_tuples (list)\n            limit (int)\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\n            \"backfill dest=%s, room_id=%s, event_tuples=%r, limit=%s\",\n            destination,\n            room_id,\n            event_tuples,\n            str(limit),\n        )\n\n        if not event_tuples:\n            # TODO: raise?\n            return\n\n        path = _create_v1_path(\"/backfill/%s\", room_id)\n\n        args = {\"v\": event_tuples, \"limit\": [str(limit)]}\n\n        return self.client.get_json(\n            destination, path=path, args=args, try_trailing_slash_on_400=True\n        )\n\n    @log_function\n    async def send_transaction(self, transaction, json_data_callback=None):\n        \"\"\" Sends the given Transaction to its destination\n\n        Args:\n            transaction (Transaction)\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The result\n            will be the decoded JSON body.\n\n            Fails with ``HTTPRequestException`` if we get an HTTP response\n            code >= 300.\n\n            Fails with ``NotRetryingDestination`` if we are not yet ready\n            to retry this server.\n\n            Fails with ``FederationDeniedError`` if this destination\n            is not on our federation whitelist\n        \"\"\"\n        logger.debug(\n            \"send_data dest=%s, txid=%s\",\n            transaction.destination,\n            transaction.transaction_id,\n        )\n\n        if transaction.destination == self.server_name:\n            raise RuntimeError(\"Transport layer cannot send to itself!\")\n\n        # FIXME: This is only used by the tests. The actual json sent is\n        # generated by the json_data_callback.\n        json_data = transaction.get_dict()\n\n        path = _create_v1_path(\"/send/%s\", transaction.transaction_id)\n\n        response = await self.client.put_json(\n            transaction.destination,\n            path=path,\n            data=json_data,\n            json_data_callback=json_data_callback,\n            long_retries=True,\n            backoff_on_404=True,  # If we get a 404 the other side has gone\n            try_trailing_slash_on_400=True,\n        )\n\n        return response\n\n    @log_function\n    async def make_query(\n        self, destination, query_type, args, retry_on_dns_fail, ignore_backoff=False\n    ):\n        path = _create_v1_path(\"/query/%s\", query_type)\n\n        content = await self.client.get_json(\n            destination=destination,\n            path=path,\n            args=args,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=10000,\n            ignore_backoff=ignore_backoff,\n        )\n\n        return content\n\n    @log_function\n    async def make_membership_event(\n        self, destination, room_id, user_id, membership, params\n    ):\n        \"\"\"Asks a remote server to build and sign us a membership event\n\n        Note that this does not append any events to any graphs.\n\n        Args:\n            destination (str): address of remote homeserver\n            room_id (str): room to join/leave\n            user_id (str): user to be joined/left\n            membership (str): one of join/leave\n            params (dict[str, str|Iterable[str]]): Query parameters to include in the\n                request.\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The result\n            will be the decoded JSON body (ie, the new event).\n\n            Fails with ``HTTPRequestException`` if we get an HTTP response\n            code >= 300.\n\n            Fails with ``NotRetryingDestination`` if we are not yet ready\n            to retry this server.\n\n            Fails with ``FederationDeniedError`` if the remote destination\n            is not in our federation whitelist\n        \"\"\"\n        valid_memberships = {Membership.JOIN, Membership.LEAVE}\n        if membership not in valid_memberships:\n            raise RuntimeError(\n                \"make_membership_event called with membership='%s', must be one of %s\"\n                % (membership, \",\".join(valid_memberships))\n            )\n        path = _create_v1_path(\"/make_%s/%s/%s\", membership, room_id, user_id)\n\n        ignore_backoff = False\n        retry_on_dns_fail = False\n\n        if membership == Membership.LEAVE:\n            # we particularly want to do our best to send leave events. The\n            # problem is that if it fails, we won't retry it later, so if the\n            # remote server was just having a momentary blip, the room will be\n            # out of sync.\n            ignore_backoff = True\n            retry_on_dns_fail = True\n\n        content = await self.client.get_json(\n            destination=destination,\n            path=path,\n            args=params,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=20000,\n            ignore_backoff=ignore_backoff,\n        )\n\n        return content\n\n    @log_function\n    async def send_join_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/send_join/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content\n        )\n\n        return response\n\n    @log_function\n    async def send_join_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/send_join/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content\n        )\n\n        return response\n\n    @log_function\n    async def send_leave_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/send_leave/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination,\n            path=path,\n            data=content,\n            # we want to do our best to send this through. The problem is\n            # that if it fails, we won't retry it later, so if the remote\n            # server was just having a momentary blip, the room will be out of\n            # sync.\n            ignore_backoff=True,\n        )\n\n        return response\n\n    @log_function\n    async def send_leave_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/send_leave/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination,\n            path=path,\n            data=content,\n            # we want to do our best to send this through. The problem is\n            # that if it fails, we won't retry it later, so if the remote\n            # server was just having a momentary blip, the room will be out of\n            # sync.\n            ignore_backoff=True,\n        )\n\n        return response\n\n    @log_function\n    async def send_invite_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/invite/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n        return response\n\n    @log_function\n    async def send_invite_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/invite/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n        return response\n\n    @log_function\n    async def get_public_rooms(\n        self,\n        remote_server: str,\n        limit: Optional[int] = None,\n        since_token: Optional[str] = None,\n        search_filter: Optional[Dict] = None,\n        include_all_networks: bool = False,\n        third_party_instance_id: Optional[str] = None,\n    ):\n        \"\"\"Get the list of public rooms from a remote homeserver\n\n        See synapse.federation.federation_client.FederationClient.get_public_rooms for\n        more information.\n        \"\"\"\n        if search_filter:\n            # this uses MSC2197 (Search Filtering over Federation)\n            path = _create_v1_path(\"/publicRooms\")\n\n            data = {\n                \"include_all_networks\": \"true\" if include_all_networks else \"false\"\n            }  # type: Dict[str, Any]\n            if third_party_instance_id:\n                data[\"third_party_instance_id\"] = third_party_instance_id\n            if limit:\n                data[\"limit\"] = str(limit)\n            if since_token:\n                data[\"since\"] = since_token\n\n            data[\"filter\"] = search_filter\n\n            try:\n                response = await self.client.post_json(\n                    destination=remote_server, path=path, data=data, ignore_backoff=True\n                )\n            except HttpResponseException as e:\n                if e.code == 403:\n                    raise SynapseError(\n                        403,\n                        \"You are not allowed to view the public rooms list of %s\"\n                        % (remote_server,),\n                        errcode=Codes.FORBIDDEN,\n                    )\n                raise\n        else:\n            path = _create_v1_path(\"/publicRooms\")\n\n            args = {\n                \"include_all_networks\": \"true\" if include_all_networks else \"false\"\n            }  # type: Dict[str, Any]\n            if third_party_instance_id:\n                args[\"third_party_instance_id\"] = (third_party_instance_id,)\n            if limit:\n                args[\"limit\"] = [str(limit)]\n            if since_token:\n                args[\"since\"] = [since_token]\n\n            try:\n                response = await self.client.get_json(\n                    destination=remote_server, path=path, args=args, ignore_backoff=True\n                )\n            except HttpResponseException as e:\n                if e.code == 403:\n                    raise SynapseError(\n                        403,\n                        \"You are not allowed to view the public rooms list of %s\"\n                        % (remote_server,),\n                        errcode=Codes.FORBIDDEN,\n                    )\n                raise\n\n        return response\n\n    @log_function\n    async def exchange_third_party_invite(self, destination, room_id, event_dict):\n        path = _create_v1_path(\"/exchange_third_party_invite/%s\", room_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=event_dict\n        )\n\n        return response\n\n    @log_function\n    async def get_event_auth(self, destination, room_id, event_id):\n        path = _create_v1_path(\"/event_auth/%s/%s\", room_id, event_id)\n\n        content = await self.client.get_json(destination=destination, path=path)\n\n        return content\n\n    @log_function\n    async def query_client_keys(self, destination, query_content, timeout):\n        \"\"\"Query the device keys for a list of user ids hosted on a remote\n        server.\n\n        Request:\n            {\n              \"device_keys\": {\n                \"<user_id>\": [\"<device_id>\"]\n              }\n            }\n\n        Response:\n            {\n              \"device_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": {...}\n                }\n              },\n              \"master_key\": {\n                \"<user_id>\": {...}\n                }\n              },\n              \"self_signing_key\": {\n                \"<user_id>\": {...}\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing device and cross-signing keys.\n        \"\"\"\n        path = _create_v1_path(\"/user/keys/query\")\n\n        content = await self.client.post_json(\n            destination=destination, path=path, data=query_content, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def query_user_devices(self, destination, user_id, timeout):\n        \"\"\"Query the devices for a user id hosted on a remote server.\n\n        Response:\n            {\n              \"stream_id\": \"...\",\n              \"devices\": [ { ... } ],\n              \"master_key\": {\n                \"user_id\": \"<user_id>\",\n                \"usage\": [...],\n                \"keys\": {...},\n                \"signatures\": {\n                  \"<user_id>\": {...}\n                }\n              },\n              \"self_signing_key\": {\n                \"user_id\": \"<user_id>\",\n                \"usage\": [...],\n                \"keys\": {...},\n                \"signatures\": {\n                  \"<user_id>\": {...}\n                }\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing device and cross-signing keys.\n        \"\"\"\n        path = _create_v1_path(\"/user/devices/%s\", user_id)\n\n        content = await self.client.get_json(\n            destination=destination, path=path, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def claim_client_keys(self, destination, query_content, timeout):\n        \"\"\"Claim one-time keys for a list of devices hosted on a remote server.\n\n        Request:\n            {\n              \"one_time_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": \"<algorithm>\"\n                }\n              }\n            }\n\n        Response:\n            {\n              \"device_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": {\n                    \"<algorithm>:<key_id>\": \"<key_base64>\"\n                  }\n                }\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing the one-time keys.\n        \"\"\"\n\n        path = _create_v1_path(\"/user/keys/claim\")\n\n        content = await self.client.post_json(\n            destination=destination, path=path, data=query_content, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def get_missing_events(\n        self,\n        destination,\n        room_id,\n        earliest_events,\n        latest_events,\n        limit,\n        min_depth,\n        timeout,\n    ):\n        path = _create_v1_path(\"/get_missing_events/%s\", room_id)\n\n        content = await self.client.post_json(\n            destination=destination,\n            path=path,\n            data={\n                \"limit\": int(limit),\n                \"min_depth\": int(min_depth),\n                \"earliest_events\": earliest_events,\n                \"latest_events\": latest_events,\n            },\n            timeout=timeout,\n        )\n\n        return content\n\n    @log_function\n    def get_group_profile(self, destination, group_id, requester_user_id):\n        \"\"\"Get a group profile\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/profile\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_profile(self, destination, group_id, requester_user_id, content):\n        \"\"\"Update a remote group profile\n\n        Args:\n            destination (str)\n            group_id (str)\n            requester_user_id (str)\n            content (dict): The new profile of the group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/profile\", group_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_summary(self, destination, group_id, requester_user_id):\n        \"\"\"Get a group summary\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/summary\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_rooms_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get all rooms in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/rooms\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    def add_room_to_group(\n        self, destination, group_id, requester_user_id, room_id, content\n    ):\n        \"\"\"Add a room to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    def update_room_in_group(\n        self, destination, group_id, requester_user_id, room_id, config_key, content\n    ):\n        \"\"\"Update room in group\n        \"\"\"\n        path = _create_v1_path(\n            \"/groups/%s/room/%s/config/%s\", group_id, room_id, config_key\n        )\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    def remove_room_from_group(self, destination, group_id, requester_user_id, room_id):\n        \"\"\"Remove a room from a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_users_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get users in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_invited_users_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get users that have been invited to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/invited_users\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def accept_group_invite(self, destination, group_id, user_id, content):\n        \"\"\"Accept a group invite\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/accept_invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def join_group(self, destination, group_id, user_id, content):\n        \"\"\"Attempts to join a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/join\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def invite_to_group(\n        self, destination, group_id, user_id, requester_user_id, content\n    ):\n        \"\"\"Invite a user to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def invite_to_group_notification(self, destination, group_id, user_id, content):\n        \"\"\"Sent by group server to inform a user's server that they have been\n        invited.\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/local/%s/users/%s/invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def remove_user_from_group(\n        self, destination, group_id, requester_user_id, user_id, content\n    ):\n        \"\"\"Remove a user from a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/remove\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def remove_user_from_group_notification(\n        self, destination, group_id, user_id, content\n    ):\n        \"\"\"Sent by group server to inform a user's server that they have been\n        kicked from the group.\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/local/%s/users/%s/remove\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def renew_group_attestation(self, destination, group_id, user_id, content):\n        \"\"\"Sent by either a group server or a user's server to periodically update\n        the attestations\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/%s/renew_attestation/%s\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def update_group_summary_room(\n        self, destination, group_id, user_id, room_id, category_id, content\n    ):\n        \"\"\"Update a room entry in a group summary\n        \"\"\"\n        if category_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/categories/%s/rooms/%s\",\n                group_id,\n                category_id,\n                room_id,\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_summary_room(\n        self, destination, group_id, user_id, room_id, category_id\n    ):\n        \"\"\"Delete a room entry in a group summary\n        \"\"\"\n        if category_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/categories/%s/rooms/%s\",\n                group_id,\n                category_id,\n                room_id,\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_categories(self, destination, group_id, requester_user_id):\n        \"\"\"Get all categories in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_category(self, destination, group_id, requester_user_id, category_id):\n        \"\"\"Get category info in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_category(\n        self, destination, group_id, requester_user_id, category_id, content\n    ):\n        \"\"\"Update a category in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_category(\n        self, destination, group_id, requester_user_id, category_id\n    ):\n        \"\"\"Delete a category in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_roles(self, destination, group_id, requester_user_id):\n        \"\"\"Get all roles in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_role(self, destination, group_id, requester_user_id, role_id):\n        \"\"\"Get a roles info\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_role(\n        self, destination, group_id, requester_user_id, role_id, content\n    ):\n        \"\"\"Update a role in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_role(self, destination, group_id, requester_user_id, role_id):\n        \"\"\"Delete a role in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_summary_user(\n        self, destination, group_id, requester_user_id, user_id, role_id, content\n    ):\n        \"\"\"Update a users entry in a group\n        \"\"\"\n        if role_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def set_group_join_policy(self, destination, group_id, requester_user_id, content):\n        \"\"\"Sets the join policy for a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/settings/m.join_policy\", group_id)\n\n        return self.client.put_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_summary_user(\n        self, destination, group_id, requester_user_id, user_id, role_id\n    ):\n        \"\"\"Delete a users entry in a group\n        \"\"\"\n        if role_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    def bulk_get_publicised_groups(self, destination, user_ids):\n        \"\"\"Get the groups a list of users are publicising\n        \"\"\"\n\n        path = _create_v1_path(\"/get_groups_publicised\")\n\n        content = {\"user_ids\": user_ids}\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    def get_room_complexity(self, destination, room_id):\n        \"\"\"\n        Args:\n            destination (str): The remote server\n            room_id (str): The room ID to ask about.\n        \"\"\"\n        path = _create_path(FEDERATION_UNSTABLE_PREFIX, \"/rooms/%s/complexity\", room_id)\n\n        return self.client.get_json(destination=destination, path=path)", "target": 0}, {"function": "def _create_path(federation_prefix, path, *args):\n    \"\"\"\n    Ensures that all args are url encoded.\n    \"\"\"\n    return federation_prefix + path % tuple(urllib.parse.quote(arg, \"\") for arg in args)", "target": 0}, {"function": "def _create_v1_path(path, *args):\n    \"\"\"Creates a path against V1 federation API from the path template and\n    args. Ensures that all args are url encoded.\n\n    Example:\n\n        _create_v1_path(\"/event/%s\", event_id)\n\n    Args:\n        path (str): String template for the path\n        args: ([str]): Args to insert into path. Each arg will be url encoded\n\n    Returns:\n        str\n    \"\"\"\n    return _create_path(FEDERATION_V1_PREFIX, path, *args)", "target": 0}, {"function": "def _create_v2_path(path, *args):\n    \"\"\"Creates a path against V2 federation API from the path template and\n    args. Ensures that all args are url encoded.\n\n    Example:\n\n        _create_v2_path(\"/event/%s\", event_id)\n\n    Args:\n        path (str): String template for the path\n        args: ([str]): Args to insert into path. Each arg will be url encoded\n\n    Returns:\n        str\n    \"\"\"\n    return _create_path(FEDERATION_V2_PREFIX, path, *args)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Fhandlers%2Ffederation.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2017-2018 New Vector Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Contains handlers for federation events.\"\"\"\n\nimport itertools\nimport logging\nfrom collections.abc import Container\nfrom http import HTTPStatus\nfrom typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n\nimport attr\nfrom signedjson.key import decode_verify_key_bytes\nfrom signedjson.sign import verify_signed_json\nfrom unpaddedbase64 import decode_base64\n\nfrom twisted.internet import defer\n\nfrom synapse import event_auth\nfrom synapse.api.constants import (\n    EventTypes,\n    Membership,\n    RejectedReason,\n    RoomEncryptionAlgorithms,\n)\nfrom synapse.api.errors import (\n    AuthError,\n    CodeMessageException,\n    Codes,\n    FederationDeniedError,\n    FederationError,\n    HttpResponseException,\n    NotFoundError,\n    RequestSendFailed,\n    SynapseError,\n)\nfrom synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion, RoomVersions\nfrom synapse.crypto.event_signing import compute_event_signature\nfrom synapse.event_auth import auth_types_for_event\nfrom synapse.events import EventBase\nfrom synapse.events.snapshot import EventContext\nfrom synapse.events.validator import EventValidator\nfrom synapse.handlers._base import BaseHandler\nfrom synapse.http.servlet import assert_params_in_dict\nfrom synapse.logging.context import (\n    make_deferred_yieldable,\n    nested_logging_context,\n    preserve_fn,\n    run_in_background,\n)\nfrom synapse.logging.utils import log_function\nfrom synapse.metrics.background_process_metrics import run_as_background_process\nfrom synapse.replication.http.devices import ReplicationUserDevicesResyncRestServlet\nfrom synapse.replication.http.federation import (\n    ReplicationCleanRoomRestServlet,\n    ReplicationFederationSendEventsRestServlet,\n    ReplicationStoreRoomOnOutlierMembershipRestServlet,\n)\nfrom synapse.state import StateResolutionStore\nfrom synapse.storage.databases.main.events_worker import EventRedactBehaviour\nfrom synapse.types import (\n    JsonDict,\n    MutableStateMap,\n    PersistedEventPosition,\n    RoomStreamToken,\n    StateMap,\n    UserID,\n    get_domain_from_id,\n)\nfrom synapse.util.async_helpers import Linearizer, concurrently_execute\nfrom synapse.util.retryutils import NotRetryingDestination\nfrom synapse.util.stringutils import shortstr\nfrom synapse.visibility import filter_events_for_server\n\nif TYPE_CHECKING:\n    from synapse.server import HomeServer\n\nlogger = logging.getLogger(__name__)\n\n\n@attr.s(slots=True)\nclass _NewEventInfo:\n    \"\"\"Holds information about a received event, ready for passing to _handle_new_events\n\n    Attributes:\n        event: the received event\n\n        state: the state at that event\n\n        auth_events: the auth_event map for that event\n    \"\"\"\n\n    event = attr.ib(type=EventBase)\n    state = attr.ib(type=Optional[Sequence[EventBase]], default=None)\n    auth_events = attr.ib(type=Optional[MutableStateMap[EventBase]], default=None)\n\n\nclass FederationHandler(BaseHandler):\n    \"\"\"Handles events that originated from federation.\n        Responsible for:\n        a) handling received Pdus before handing them on as Events to the rest\n        of the homeserver (including auth and state conflict resolutions)\n        b) converting events that were produced by local clients that may need\n        to be sent to remote homeservers.\n        c) doing the necessary dances to invite remote users and join remote\n        rooms.\n    \"\"\"\n\n    def __init__(self, hs: \"HomeServer\"):\n        super().__init__(hs)\n\n        self.hs = hs\n\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.state_store = self.storage.state\n        self.federation_client = hs.get_federation_client()\n        self.state_handler = hs.get_state_handler()\n        self._state_resolution_handler = hs.get_state_resolution_handler()\n        self.server_name = hs.hostname\n        self.keyring = hs.get_keyring()\n        self.action_generator = hs.get_action_generator()\n        self.is_mine_id = hs.is_mine_id\n        self.spam_checker = hs.get_spam_checker()\n        self.event_creation_handler = hs.get_event_creation_handler()\n        self._message_handler = hs.get_message_handler()\n        self._server_notices_mxid = hs.config.server_notices_mxid\n        self.config = hs.config\n        self.http_client = hs.get_proxied_blacklisted_http_client()\n        self._instance_name = hs.get_instance_name()\n        self._replication = hs.get_replication_data_handler()\n\n        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)\n        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(\n            hs\n        )\n\n        if hs.config.worker_app:\n            self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(\n                hs\n            )\n            self._maybe_store_room_on_outlier_membership = ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(\n                hs\n            )\n        else:\n            self._device_list_updater = hs.get_device_handler().device_list_updater\n            self._maybe_store_room_on_outlier_membership = (\n                self.store.maybe_store_room_on_outlier_membership\n            )\n\n        # When joining a room we need to queue any events for that room up.\n        # For each room, a list of (pdu, origin) tuples.\n        self.room_queues = {}  # type: Dict[str, List[Tuple[EventBase, str]]]\n        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")\n\n        self.third_party_event_rules = hs.get_third_party_event_rules()\n\n        self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages\n\n    async def on_receive_pdu(self, origin, pdu, sent_to_us_directly=False) -> None:\n        \"\"\" Process a PDU received via a federation /send/ transaction, or\n        via backfill of missing prev_events\n\n        Args:\n            origin (str): server which initiated the /send/ transaction. Will\n                be used to fetch missing events or state.\n            pdu (FrozenEvent): received PDU\n            sent_to_us_directly (bool): True if this event was pushed to us; False if\n                we pulled it as the result of a missing prev_event.\n        \"\"\"\n\n        room_id = pdu.room_id\n        event_id = pdu.event_id\n\n        logger.info(\"handling received PDU: %s\", pdu)\n\n        # We reprocess pdus when we have seen them only as outliers\n        existing = await self.store.get_event(\n            event_id, allow_none=True, allow_rejected=True\n        )\n\n        # FIXME: Currently we fetch an event again when we already have it\n        # if it has been marked as an outlier.\n\n        already_seen = existing and (\n            not existing.internal_metadata.is_outlier()\n            or pdu.internal_metadata.is_outlier()\n        )\n        if already_seen:\n            logger.debug(\"[%s %s]: Already seen pdu\", room_id, event_id)\n            return\n\n        # do some initial sanity-checking of the event. In particular, make\n        # sure it doesn't have hundreds of prev_events or auth_events, which\n        # could cause a huge state resolution or cascade of event fetches.\n        try:\n            self._sanity_check_event(pdu)\n        except SynapseError as err:\n            logger.warning(\n                \"[%s %s] Received event failed sanity checks\", room_id, event_id\n            )\n            raise FederationError(\"ERROR\", err.code, err.msg, affected=pdu.event_id)\n\n        # If we are currently in the process of joining this room, then we\n        # queue up events for later processing.\n        if room_id in self.room_queues:\n            logger.info(\n                \"[%s %s] Queuing PDU from %s for now: join in progress\",\n                room_id,\n                event_id,\n                origin,\n            )\n            self.room_queues[room_id].append((pdu, origin))\n            return\n\n        # If we're not in the room just ditch the event entirely. This is\n        # probably an old server that has come back and thinks we're still in\n        # the room (or we've been rejoined to the room by a state reset).\n        #\n        # Note that if we were never in the room then we would have already\n        # dropped the event, since we wouldn't know the room version.\n        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)\n        if not is_in_room:\n            logger.info(\n                \"[%s %s] Ignoring PDU from %s as we're not in the room\",\n                room_id,\n                event_id,\n                origin,\n            )\n            return None\n\n        state = None\n\n        # Get missing pdus if necessary.\n        if not pdu.internal_metadata.is_outlier():\n            # We only backfill backwards to the min depth.\n            min_depth = await self.get_min_depth_for_context(pdu.room_id)\n\n            logger.debug(\"[%s %s] min_depth: %d\", room_id, event_id, min_depth)\n\n            prevs = set(pdu.prev_event_ids())\n            seen = await self.store.have_events_in_timeline(prevs)\n\n            if min_depth is not None and pdu.depth < min_depth:\n                # This is so that we don't notify the user about this\n                # message, to work around the fact that some events will\n                # reference really really old events we really don't want to\n                # send to the clients.\n                pdu.internal_metadata.outlier = True\n            elif min_depth is not None and pdu.depth > min_depth:\n                missing_prevs = prevs - seen\n                if sent_to_us_directly and missing_prevs:\n                    # If we're missing stuff, ensure we only fetch stuff one\n                    # at a time.\n                    logger.info(\n                        \"[%s %s] Acquiring room lock to fetch %d missing prev_events: %s\",\n                        room_id,\n                        event_id,\n                        len(missing_prevs),\n                        shortstr(missing_prevs),\n                    )\n                    with (await self._room_pdu_linearizer.queue(pdu.room_id)):\n                        logger.info(\n                            \"[%s %s] Acquired room lock to fetch %d missing prev_events\",\n                            room_id,\n                            event_id,\n                            len(missing_prevs),\n                        )\n\n                        try:\n                            await self._get_missing_events_for_pdu(\n                                origin, pdu, prevs, min_depth\n                            )\n                        except Exception as e:\n                            raise Exception(\n                                \"Error fetching missing prev_events for %s: %s\"\n                                % (event_id, e)\n                            ) from e\n\n                        # Update the set of things we've seen after trying to\n                        # fetch the missing stuff\n                        seen = await self.store.have_events_in_timeline(prevs)\n\n                        if not prevs - seen:\n                            logger.info(\n                                \"[%s %s] Found all missing prev_events\",\n                                room_id,\n                                event_id,\n                            )\n\n            if prevs - seen:\n                # We've still not been able to get all of the prev_events for this event.\n                #\n                # In this case, we need to fall back to asking another server in the\n                # federation for the state at this event. That's ok provided we then\n                # resolve the state against other bits of the DAG before using it (which\n                # will ensure that you can't just take over a room by sending an event,\n                # withholding its prev_events, and declaring yourself to be an admin in\n                # the subsequent state request).\n                #\n                # Now, if we're pulling this event as a missing prev_event, then clearly\n                # this event is not going to become the only forward-extremity and we are\n                # guaranteed to resolve its state against our existing forward\n                # extremities, so that should be fine.\n                #\n                # On the other hand, if this event was pushed to us, it is possible for\n                # it to become the only forward-extremity in the room, and we would then\n                # trust its state to be the state for the whole room. This is very bad.\n                # Further, if the event was pushed to us, there is no excuse for us not to\n                # have all the prev_events. We therefore reject any such events.\n                #\n                # XXX this really feels like it could/should be merged with the above,\n                # but there is an interaction with min_depth that I'm not really\n                # following.\n\n                if sent_to_us_directly:\n                    logger.warning(\n                        \"[%s %s] Rejecting: failed to fetch %d prev events: %s\",\n                        room_id,\n                        event_id,\n                        len(prevs - seen),\n                        shortstr(prevs - seen),\n                    )\n                    raise FederationError(\n                        \"ERROR\",\n                        403,\n                        (\n                            \"Your server isn't divulging details about prev_events \"\n                            \"referenced in this event.\"\n                        ),\n                        affected=pdu.event_id,\n                    )\n\n                logger.info(\n                    \"Event %s is missing prev_events: calculating state for a \"\n                    \"backwards extremity\",\n                    event_id,\n                )\n\n                # Calculate the state after each of the previous events, and\n                # resolve them to find the correct state at the current event.\n                event_map = {event_id: pdu}\n                try:\n                    # Get the state of the events we know about\n                    ours = await self.state_store.get_state_groups_ids(room_id, seen)\n\n                    # state_maps is a list of mappings from (type, state_key) to event_id\n                    state_maps = list(ours.values())  # type: List[StateMap[str]]\n\n                    # we don't need this any more, let's delete it.\n                    del ours\n\n                    # Ask the remote server for the states we don't\n                    # know about\n                    for p in prevs - seen:\n                        logger.info(\n                            \"Requesting state at missing prev_event %s\", event_id,\n                        )\n\n                        with nested_logging_context(p):\n                            # note that if any of the missing prevs share missing state or\n                            # auth events, the requests to fetch those events are deduped\n                            # by the get_pdu_cache in federation_client.\n                            (remote_state, _,) = await self._get_state_for_room(\n                                origin, room_id, p, include_event_in_state=True\n                            )\n\n                            remote_state_map = {\n                                (x.type, x.state_key): x.event_id for x in remote_state\n                            }\n                            state_maps.append(remote_state_map)\n\n                            for x in remote_state:\n                                event_map[x.event_id] = x\n\n                    room_version = await self.store.get_room_version_id(room_id)\n                    state_map = await self._state_resolution_handler.resolve_events_with_store(\n                        room_id,\n                        room_version,\n                        state_maps,\n                        event_map,\n                        state_res_store=StateResolutionStore(self.store),\n                    )\n\n                    # We need to give _process_received_pdu the actual state events\n                    # rather than event ids, so generate that now.\n\n                    # First though we need to fetch all the events that are in\n                    # state_map, so we can build up the state below.\n                    evs = await self.store.get_events(\n                        list(state_map.values()),\n                        get_prev_content=False,\n                        redact_behaviour=EventRedactBehaviour.AS_IS,\n                    )\n                    event_map.update(evs)\n\n                    state = [event_map[e] for e in state_map.values()]\n                except Exception:\n                    logger.warning(\n                        \"[%s %s] Error attempting to resolve state at missing \"\n                        \"prev_events\",\n                        room_id,\n                        event_id,\n                        exc_info=True,\n                    )\n                    raise FederationError(\n                        \"ERROR\",\n                        403,\n                        \"We can't get valid state history.\",\n                        affected=event_id,\n                    )\n\n        await self._process_received_pdu(origin, pdu, state=state)\n\n    async def _get_missing_events_for_pdu(self, origin, pdu, prevs, min_depth):\n        \"\"\"\n        Args:\n            origin (str): Origin of the pdu. Will be called to get the missing events\n            pdu: received pdu\n            prevs (set(str)): List of event ids which we are missing\n            min_depth (int): Minimum depth of events to return.\n        \"\"\"\n\n        room_id = pdu.room_id\n        event_id = pdu.event_id\n\n        seen = await self.store.have_events_in_timeline(prevs)\n\n        if not prevs - seen:\n            return\n\n        latest_list = await self.store.get_latest_event_ids_in_room(room_id)\n\n        # We add the prev events that we have seen to the latest\n        # list to ensure the remote server doesn't give them to us\n        latest = set(latest_list)\n        latest |= seen\n\n        logger.info(\n            \"[%s %s]: Requesting missing events between %s and %s\",\n            room_id,\n            event_id,\n            shortstr(latest),\n            event_id,\n        )\n\n        # XXX: we set timeout to 10s to help workaround\n        # https://github.com/matrix-org/synapse/issues/1733.\n        # The reason is to avoid holding the linearizer lock\n        # whilst processing inbound /send transactions, causing\n        # FDs to stack up and block other inbound transactions\n        # which empirically can currently take up to 30 minutes.\n        #\n        # N.B. this explicitly disables retry attempts.\n        #\n        # N.B. this also increases our chances of falling back to\n        # fetching fresh state for the room if the missing event\n        # can't be found, which slightly reduces our security.\n        # it may also increase our DAG extremity count for the room,\n        # causing additional state resolution?  See #1760.\n        # However, fetching state doesn't hold the linearizer lock\n        # apparently.\n        #\n        # see https://github.com/matrix-org/synapse/pull/1744\n        #\n        # ----\n        #\n        # Update richvdh 2018/09/18: There are a number of problems with timing this\n        # request out aggressively on the client side:\n        #\n        # - it plays badly with the server-side rate-limiter, which starts tarpitting you\n        #   if you send too many requests at once, so you end up with the server carefully\n        #   working through the backlog of your requests, which you have already timed\n        #   out.\n        #\n        # - for this request in particular, we now (as of\n        #   https://github.com/matrix-org/synapse/pull/3456) reject any PDUs where the\n        #   server can't produce a plausible-looking set of prev_events - so we becone\n        #   much more likely to reject the event.\n        #\n        # - contrary to what it says above, we do *not* fall back to fetching fresh state\n        #   for the room if get_missing_events times out. Rather, we give up processing\n        #   the PDU whose prevs we are missing, which then makes it much more likely that\n        #   we'll end up back here for the *next* PDU in the list, which exacerbates the\n        #   problem.\n        #\n        # - the aggressive 10s timeout was introduced to deal with incoming federation\n        #   requests taking 8 hours to process. It's not entirely clear why that was going\n        #   on; certainly there were other issues causing traffic storms which are now\n        #   resolved, and I think in any case we may be more sensible about our locking\n        #   now. We're *certainly* more sensible about our logging.\n        #\n        # All that said: Let's try increasing the timeout to 60s and see what happens.\n\n        try:\n            missing_events = await self.federation_client.get_missing_events(\n                origin,\n                room_id,\n                earliest_events_ids=list(latest),\n                latest_events=[pdu],\n                limit=10,\n                min_depth=min_depth,\n                timeout=60000,\n            )\n        except (RequestSendFailed, HttpResponseException, NotRetryingDestination) as e:\n            # We failed to get the missing events, but since we need to handle\n            # the case of `get_missing_events` not returning the necessary\n            # events anyway, it is safe to simply log the error and continue.\n            logger.warning(\n                \"[%s %s]: Failed to get prev_events: %s\", room_id, event_id, e\n            )\n            return\n\n        logger.info(\n            \"[%s %s]: Got %d prev_events: %s\",\n            room_id,\n            event_id,\n            len(missing_events),\n            shortstr(missing_events),\n        )\n\n        # We want to sort these by depth so we process them and\n        # tell clients about them in order.\n        missing_events.sort(key=lambda x: x.depth)\n\n        for ev in missing_events:\n            logger.info(\n                \"[%s %s] Handling received prev_event %s\",\n                room_id,\n                event_id,\n                ev.event_id,\n            )\n            with nested_logging_context(ev.event_id):\n                try:\n                    await self.on_receive_pdu(origin, ev, sent_to_us_directly=False)\n                except FederationError as e:\n                    if e.code == 403:\n                        logger.warning(\n                            \"[%s %s] Received prev_event %s failed history check.\",\n                            room_id,\n                            event_id,\n                            ev.event_id,\n                        )\n                    else:\n                        raise\n\n    async def _get_state_for_room(\n        self,\n        destination: str,\n        room_id: str,\n        event_id: str,\n        include_event_in_state: bool = False,\n    ) -> Tuple[List[EventBase], List[EventBase]]:\n        \"\"\"Requests all of the room state at a given event from a remote homeserver.\n\n        Args:\n            destination: The remote homeserver to query for the state.\n            room_id: The id of the room we're interested in.\n            event_id: The id of the event we want the state at.\n            include_event_in_state: if true, the event itself will be included in the\n                returned state event list.\n\n        Returns:\n            A list of events in the state, possibly including the event itself, and\n            a list of events in the auth chain for the given event.\n        \"\"\"\n        (\n            state_event_ids,\n            auth_event_ids,\n        ) = await self.federation_client.get_room_state_ids(\n            destination, room_id, event_id=event_id\n        )\n\n        desired_events = set(state_event_ids + auth_event_ids)\n\n        if include_event_in_state:\n            desired_events.add(event_id)\n\n        event_map = await self._get_events_from_store_or_dest(\n            destination, room_id, desired_events\n        )\n\n        failed_to_fetch = desired_events - event_map.keys()\n        if failed_to_fetch:\n            logger.warning(\n                \"Failed to fetch missing state/auth events for %s %s\",\n                event_id,\n                failed_to_fetch,\n            )\n\n        remote_state = [\n            event_map[e_id] for e_id in state_event_ids if e_id in event_map\n        ]\n\n        if include_event_in_state:\n            remote_event = event_map.get(event_id)\n            if not remote_event:\n                raise Exception(\"Unable to get missing prev_event %s\" % (event_id,))\n            if remote_event.is_state() and remote_event.rejected_reason is None:\n                remote_state.append(remote_event)\n\n        auth_chain = [event_map[e_id] for e_id in auth_event_ids if e_id in event_map]\n        auth_chain.sort(key=lambda e: e.depth)\n\n        return remote_state, auth_chain\n\n    async def _get_events_from_store_or_dest(\n        self, destination: str, room_id: str, event_ids: Iterable[str]\n    ) -> Dict[str, EventBase]:\n        \"\"\"Fetch events from a remote destination, checking if we already have them.\n\n        Persists any events we don't already have as outliers.\n\n        If we fail to fetch any of the events, a warning will be logged, and the event\n        will be omitted from the result. Likewise, any events which turn out not to\n        be in the given room.\n\n        This function *does not* automatically get missing auth events of the\n        newly fetched events. Callers must include the full auth chain of\n        of the missing events in the `event_ids` argument, to ensure that any\n        missing auth events are correctly fetched.\n\n        Returns:\n            map from event_id to event\n        \"\"\"\n        fetched_events = await self.store.get_events(event_ids, allow_rejected=True)\n\n        missing_events = set(event_ids) - fetched_events.keys()\n\n        if missing_events:\n            logger.debug(\n                \"Fetching unknown state/auth events %s for room %s\",\n                missing_events,\n                room_id,\n            )\n\n            await self._get_events_and_persist(\n                destination=destination, room_id=room_id, events=missing_events\n            )\n\n            # we need to make sure we re-load from the database to get the rejected\n            # state correct.\n            fetched_events.update(\n                (await self.store.get_events(missing_events, allow_rejected=True))\n            )\n\n        # check for events which were in the wrong room.\n        #\n        # this can happen if a remote server claims that the state or\n        # auth_events at an event in room A are actually events in room B\n\n        bad_events = [\n            (event_id, event.room_id)\n            for event_id, event in fetched_events.items()\n            if event.room_id != room_id\n        ]\n\n        for bad_event_id, bad_room_id in bad_events:\n            # This is a bogus situation, but since we may only discover it a long time\n            # after it happened, we try our best to carry on, by just omitting the\n            # bad events from the returned auth/state set.\n            logger.warning(\n                \"Remote server %s claims event %s in room %s is an auth/state \"\n                \"event in room %s\",\n                destination,\n                bad_event_id,\n                bad_room_id,\n                room_id,\n            )\n\n            del fetched_events[bad_event_id]\n\n        return fetched_events\n\n    async def _process_received_pdu(\n        self, origin: str, event: EventBase, state: Optional[Iterable[EventBase]],\n    ):\n        \"\"\" Called when we have a new pdu. We need to do auth checks and put it\n        through the StateHandler.\n\n        Args:\n            origin: server sending the event\n\n            event: event to be persisted\n\n            state: Normally None, but if we are handling a gap in the graph\n                (ie, we are missing one or more prev_events), the resolved state at the\n                event\n        \"\"\"\n        room_id = event.room_id\n        event_id = event.event_id\n\n        logger.debug(\"[%s %s] Processing event: %s\", room_id, event_id, event)\n\n        try:\n            await self._handle_new_event(origin, event, state=state)\n        except AuthError as e:\n            raise FederationError(\"ERROR\", e.code, e.msg, affected=event.event_id)\n\n        # For encrypted messages we check that we know about the sending device,\n        # if we don't then we mark the device cache for that user as stale.\n        if event.type == EventTypes.Encrypted:\n            device_id = event.content.get(\"device_id\")\n            sender_key = event.content.get(\"sender_key\")\n\n            cached_devices = await self.store.get_cached_devices_for_user(event.sender)\n\n            resync = False  # Whether we should resync device lists.\n\n            device = None\n            if device_id is not None:\n                device = cached_devices.get(device_id)\n                if device is None:\n                    logger.info(\n                        \"Received event from remote device not in our cache: %s %s\",\n                        event.sender,\n                        device_id,\n                    )\n                    resync = True\n\n            # We also check if the `sender_key` matches what we expect.\n            if sender_key is not None:\n                # Figure out what sender key we're expecting. If we know the\n                # device and recognize the algorithm then we can work out the\n                # exact key to expect. Otherwise check it matches any key we\n                # have for that device.\n\n                current_keys = []  # type: Container[str]\n\n                if device:\n                    keys = device.get(\"keys\", {}).get(\"keys\", {})\n\n                    if (\n                        event.content.get(\"algorithm\")\n                        == RoomEncryptionAlgorithms.MEGOLM_V1_AES_SHA2\n                    ):\n                        # For this algorithm we expect a curve25519 key.\n                        key_name = \"curve25519:%s\" % (device_id,)\n                        current_keys = [keys.get(key_name)]\n                    else:\n                        # We don't know understand the algorithm, so we just\n                        # check it matches a key for the device.\n                        current_keys = keys.values()\n                elif device_id:\n                    # We don't have any keys for the device ID.\n                    pass\n                else:\n                    # The event didn't include a device ID, so we just look for\n                    # keys across all devices.\n                    current_keys = [\n                        key\n                        for device in cached_devices.values()\n                        for key in device.get(\"keys\", {}).get(\"keys\", {}).values()\n                    ]\n\n                # We now check that the sender key matches (one of) the expected\n                # keys.\n                if sender_key not in current_keys:\n                    logger.info(\n                        \"Received event from remote device with unexpected sender key: %s %s: %s\",\n                        event.sender,\n                        device_id or \"<no device_id>\",\n                        sender_key,\n                    )\n                    resync = True\n\n            if resync:\n                run_as_background_process(\n                    \"resync_device_due_to_pdu\", self._resync_device, event.sender\n                )\n\n    async def _resync_device(self, sender: str) -> None:\n        \"\"\"We have detected that the device list for the given user may be out\n        of sync, so we try and resync them.\n        \"\"\"\n\n        try:\n            await self.store.mark_remote_user_device_cache_as_stale(sender)\n\n            # Immediately attempt a resync in the background\n            if self.config.worker_app:\n                await self._user_device_resync(user_id=sender)\n            else:\n                await self._device_list_updater.user_device_resync(sender)\n        except Exception:\n            logger.exception(\"Failed to resync device for %s\", sender)\n\n    @log_function\n    async def backfill(self, dest, room_id, limit, extremities):\n        \"\"\" Trigger a backfill request to `dest` for the given `room_id`\n\n        This will attempt to get more events from the remote. If the other side\n        has no new events to offer, this will return an empty list.\n\n        As the events are received, we check their signatures, and also do some\n        sanity-checking on them. If any of the backfilled events are invalid,\n        this method throws a SynapseError.\n\n        TODO: make this more useful to distinguish failures of the remote\n        server from invalid events (there is probably no point in trying to\n        re-fetch invalid events from every other HS in the room.)\n        \"\"\"\n        if dest == self.server_name:\n            raise SynapseError(400, \"Can't backfill from self.\")\n\n        events = await self.federation_client.backfill(\n            dest, room_id, limit=limit, extremities=extremities\n        )\n\n        if not events:\n            return []\n\n        # ideally we'd sanity check the events here for excess prev_events etc,\n        # but it's hard to reject events at this point without completely\n        # breaking backfill in the same way that it is currently broken by\n        # events whose signature we cannot verify (#3121).\n        #\n        # So for now we accept the events anyway. #3124 tracks this.\n        #\n        # for ev in events:\n        #     self._sanity_check_event(ev)\n\n        # Don't bother processing events we already have.\n        seen_events = await self.store.have_events_in_timeline(\n            {e.event_id for e in events}\n        )\n\n        events = [e for e in events if e.event_id not in seen_events]\n\n        if not events:\n            return []\n\n        event_map = {e.event_id: e for e in events}\n\n        event_ids = {e.event_id for e in events}\n\n        # build a list of events whose prev_events weren't in the batch.\n        # (XXX: this will include events whose prev_events we already have; that doesn't\n        # sound right?)\n        edges = [ev.event_id for ev in events if set(ev.prev_event_ids()) - event_ids]\n\n        logger.info(\"backfill: Got %d events with %d edges\", len(events), len(edges))\n\n        # For each edge get the current state.\n\n        auth_events = {}\n        state_events = {}\n        events_to_state = {}\n        for e_id in edges:\n            state, auth = await self._get_state_for_room(\n                destination=dest,\n                room_id=room_id,\n                event_id=e_id,\n                include_event_in_state=False,\n            )\n            auth_events.update({a.event_id: a for a in auth})\n            auth_events.update({s.event_id: s for s in state})\n            state_events.update({s.event_id: s for s in state})\n            events_to_state[e_id] = state\n\n        required_auth = {\n            a_id\n            for event in events\n            + list(state_events.values())\n            + list(auth_events.values())\n            for a_id in event.auth_event_ids()\n        }\n        auth_events.update(\n            {e_id: event_map[e_id] for e_id in required_auth if e_id in event_map}\n        )\n\n        ev_infos = []\n\n        # Step 1: persist the events in the chunk we fetched state for (i.e.\n        # the backwards extremities), with custom auth events and state\n        for e_id in events_to_state:\n            # For paranoia we ensure that these events are marked as\n            # non-outliers\n            ev = event_map[e_id]\n            assert not ev.internal_metadata.is_outlier()\n\n            ev_infos.append(\n                _NewEventInfo(\n                    event=ev,\n                    state=events_to_state[e_id],\n                    auth_events={\n                        (\n                            auth_events[a_id].type,\n                            auth_events[a_id].state_key,\n                        ): auth_events[a_id]\n                        for a_id in ev.auth_event_ids()\n                        if a_id in auth_events\n                    },\n                )\n            )\n\n        if ev_infos:\n            await self._handle_new_events(dest, room_id, ev_infos, backfilled=True)\n\n        # Step 2: Persist the rest of the events in the chunk one by one\n        events.sort(key=lambda e: e.depth)\n\n        for event in events:\n            if event in events_to_state:\n                continue\n\n            # For paranoia we ensure that these events are marked as\n            # non-outliers\n            assert not event.internal_metadata.is_outlier()\n\n            # We store these one at a time since each event depends on the\n            # previous to work out the state.\n            # TODO: We can probably do something more clever here.\n            await self._handle_new_event(dest, event, backfilled=True)\n\n        return events\n\n    async def maybe_backfill(\n        self, room_id: str, current_depth: int, limit: int\n    ) -> bool:\n        \"\"\"Checks the database to see if we should backfill before paginating,\n        and if so do.\n\n        Args:\n            room_id\n            current_depth: The depth from which we're paginating from. This is\n                used to decide if we should backfill and what extremities to\n                use.\n            limit: The number of events that the pagination request will\n                return. This is used as part of the heuristic to decide if we\n                should back paginate.\n        \"\"\"\n        extremities = await self.store.get_oldest_events_with_depth_in_room(room_id)\n\n        if not extremities:\n            logger.debug(\"Not backfilling as no extremeties found.\")\n            return False\n\n        # We only want to paginate if we can actually see the events we'll get,\n        # as otherwise we'll just spend a lot of resources to get redacted\n        # events.\n        #\n        # We do this by filtering all the backwards extremities and seeing if\n        # any remain. Given we don't have the extremity events themselves, we\n        # need to actually check the events that reference them.\n        #\n        # *Note*: the spec wants us to keep backfilling until we reach the start\n        # of the room in case we are allowed to see some of the history. However\n        # in practice that causes more issues than its worth, as a) its\n        # relatively rare for there to be any visible history and b) even when\n        # there is its often sufficiently long ago that clients would stop\n        # attempting to paginate before backfill reached the visible history.\n        #\n        # TODO: If we do do a backfill then we should filter the backwards\n        #   extremities to only include those that point to visible portions of\n        #   history.\n        #\n        # TODO: Correctly handle the case where we are allowed to see the\n        #   forward event but not the backward extremity, e.g. in the case of\n        #   initial join of the server where we are allowed to see the join\n        #   event but not anything before it. This would require looking at the\n        #   state *before* the event, ignoring the special casing certain event\n        #   types have.\n\n        forward_events = await self.store.get_successor_events(list(extremities))\n\n        extremities_events = await self.store.get_events(\n            forward_events,\n            redact_behaviour=EventRedactBehaviour.AS_IS,\n            get_prev_content=False,\n        )\n\n        # We set `check_history_visibility_only` as we might otherwise get false\n        # positives from users having been erased.\n        filtered_extremities = await filter_events_for_server(\n            self.storage,\n            self.server_name,\n            list(extremities_events.values()),\n            redact=False,\n            check_history_visibility_only=True,\n        )\n\n        if not filtered_extremities:\n            return False\n\n        # Check if we reached a point where we should start backfilling.\n        sorted_extremeties_tuple = sorted(extremities.items(), key=lambda e: -int(e[1]))\n        max_depth = sorted_extremeties_tuple[0][1]\n\n        # If we're approaching an extremity we trigger a backfill, otherwise we\n        # no-op.\n        #\n        # We chose twice the limit here as then clients paginating backwards\n        # will send pagination requests that trigger backfill at least twice\n        # using the most recent extremity before it gets removed (see below). We\n        # chose more than one times the limit in case of failure, but choosing a\n        # much larger factor will result in triggering a backfill request much\n        # earlier than necessary.\n        if current_depth - 2 * limit > max_depth:\n            logger.debug(\n                \"Not backfilling as we don't need to. %d < %d - 2 * %d\",\n                max_depth,\n                current_depth,\n                limit,\n            )\n            return False\n\n        logger.debug(\n            \"room_id: %s, backfill: current_depth: %s, max_depth: %s, extrems: %s\",\n            room_id,\n            current_depth,\n            max_depth,\n            sorted_extremeties_tuple,\n        )\n\n        # We ignore extremities that have a greater depth than our current depth\n        # as:\n        #    1. we don't really care about getting events that have happened\n        #       before our current position; and\n        #    2. we have likely previously tried and failed to backfill from that\n        #       extremity, so to avoid getting \"stuck\" requesting the same\n        #       backfill repeatedly we drop those extremities.\n        filtered_sorted_extremeties_tuple = [\n            t for t in sorted_extremeties_tuple if int(t[1]) <= current_depth\n        ]\n\n        # However, we need to check that the filtered extremities are non-empty.\n        # If they are empty then either we can a) bail or b) still attempt to\n        # backill. We opt to try backfilling anyway just in case we do get\n        # relevant events.\n        if filtered_sorted_extremeties_tuple:\n            sorted_extremeties_tuple = filtered_sorted_extremeties_tuple\n\n        # We don't want to specify too many extremities as it causes the backfill\n        # request URI to be too long.\n        extremities = dict(sorted_extremeties_tuple[:5])\n\n        # Now we need to decide which hosts to hit first.\n\n        # First we try hosts that are already in the room\n        # TODO: HEURISTIC ALERT.\n\n        curr_state = await self.state_handler.get_current_state(room_id)\n\n        def get_domains_from_state(state):\n            \"\"\"Get joined domains from state\n\n            Args:\n                state (dict[tuple, FrozenEvent]): State map from type/state\n                    key to event.\n\n            Returns:\n                list[tuple[str, int]]: Returns a list of servers with the\n                lowest depth of their joins. Sorted by lowest depth first.\n            \"\"\"\n            joined_users = [\n                (state_key, int(event.depth))\n                for (e_type, state_key), event in state.items()\n                if e_type == EventTypes.Member and event.membership == Membership.JOIN\n            ]\n\n            joined_domains = {}  # type: Dict[str, int]\n            for u, d in joined_users:\n                try:\n                    dom = get_domain_from_id(u)\n                    old_d = joined_domains.get(dom)\n                    if old_d:\n                        joined_domains[dom] = min(d, old_d)\n                    else:\n                        joined_domains[dom] = d\n                except Exception:\n                    pass\n\n            return sorted(joined_domains.items(), key=lambda d: d[1])\n\n        curr_domains = get_domains_from_state(curr_state)\n\n        likely_domains = [\n            domain for domain, depth in curr_domains if domain != self.server_name\n        ]\n\n        async def try_backfill(domains):\n            # TODO: Should we try multiple of these at a time?\n            for dom in domains:\n                try:\n                    await self.backfill(\n                        dom, room_id, limit=100, extremities=extremities\n                    )\n                    # If this succeeded then we probably already have the\n                    # appropriate stuff.\n                    # TODO: We can probably do something more intelligent here.\n                    return True\n                except SynapseError as e:\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except HttpResponseException as e:\n                    if 400 <= e.code < 500:\n                        raise e.to_synapse_error()\n\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except CodeMessageException as e:\n                    if 400 <= e.code < 500:\n                        raise\n\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except NotRetryingDestination as e:\n                    logger.info(str(e))\n                    continue\n                except RequestSendFailed as e:\n                    logger.info(\"Failed to get backfill from %s because %s\", dom, e)\n                    continue\n                except FederationDeniedError as e:\n                    logger.info(e)\n                    continue\n                except Exception as e:\n                    logger.exception(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n\n            return False\n\n        success = await try_backfill(likely_domains)\n        if success:\n            return True\n\n        # Huh, well *those* domains didn't work out. Lets try some domains\n        # from the time.\n\n        tried_domains = set(likely_domains)\n        tried_domains.add(self.server_name)\n\n        event_ids = list(extremities.keys())\n\n        logger.debug(\"calling resolve_state_groups in _maybe_backfill\")\n        resolve = preserve_fn(self.state_handler.resolve_state_groups_for_events)\n        states = await make_deferred_yieldable(\n            defer.gatherResults(\n                [resolve(room_id, [e]) for e in event_ids], consumeErrors=True\n            )\n        )\n\n        # dict[str, dict[tuple, str]], a map from event_id to state map of\n        # event_ids.\n        states = dict(zip(event_ids, [s.state for s in states]))\n\n        state_map = await self.store.get_events(\n            [e_id for ids in states.values() for e_id in ids.values()],\n            get_prev_content=False,\n        )\n        states = {\n            key: {\n                k: state_map[e_id]\n                for k, e_id in state_dict.items()\n                if e_id in state_map\n            }\n            for key, state_dict in states.items()\n        }\n\n        for e_id, _ in sorted_extremeties_tuple:\n            likely_domains = get_domains_from_state(states[e_id])\n\n            success = await try_backfill(\n                [dom for dom, _ in likely_domains if dom not in tried_domains]\n            )\n            if success:\n                return True\n\n            tried_domains.update(dom for dom, _ in likely_domains)\n\n        return False\n\n    async def _get_events_and_persist(\n        self, destination: str, room_id: str, events: Iterable[str]\n    ):\n        \"\"\"Fetch the given events from a server, and persist them as outliers.\n\n        This function *does not* recursively get missing auth events of the\n        newly fetched events. Callers must include in the `events` argument\n        any missing events from the auth chain.\n\n        Logs a warning if we can't find the given event.\n        \"\"\"\n\n        room_version = await self.store.get_room_version(room_id)\n\n        event_map = {}  # type: Dict[str, EventBase]\n\n        async def get_event(event_id: str):\n            with nested_logging_context(event_id):\n                try:\n                    event = await self.federation_client.get_pdu(\n                        [destination], event_id, room_version, outlier=True,\n                    )\n                    if event is None:\n                        logger.warning(\n                            \"Server %s didn't return event %s\", destination, event_id,\n                        )\n                        return\n\n                    event_map[event.event_id] = event\n\n                except Exception as e:\n                    logger.warning(\n                        \"Error fetching missing state/auth event %s: %s %s\",\n                        event_id,\n                        type(e),\n                        e,\n                    )\n\n        await concurrently_execute(get_event, events, 5)\n\n        # Make a map of auth events for each event. We do this after fetching\n        # all the events as some of the events' auth events will be in the list\n        # of requested events.\n\n        auth_events = [\n            aid\n            for event in event_map.values()\n            for aid in event.auth_event_ids()\n            if aid not in event_map\n        ]\n        persisted_events = await self.store.get_events(\n            auth_events, allow_rejected=True,\n        )\n\n        event_infos = []\n        for event in event_map.values():\n            auth = {}\n            for auth_event_id in event.auth_event_ids():\n                ae = persisted_events.get(auth_event_id) or event_map.get(auth_event_id)\n                if ae:\n                    auth[(ae.type, ae.state_key)] = ae\n                else:\n                    logger.info(\"Missing auth event %s\", auth_event_id)\n\n            event_infos.append(_NewEventInfo(event, None, auth))\n\n        await self._handle_new_events(\n            destination, room_id, event_infos,\n        )\n\n    def _sanity_check_event(self, ev):\n        \"\"\"\n        Do some early sanity checks of a received event\n\n        In particular, checks it doesn't have an excessive number of\n        prev_events or auth_events, which could cause a huge state resolution\n        or cascade of event fetches.\n\n        Args:\n            ev (synapse.events.EventBase): event to be checked\n\n        Returns: None\n\n        Raises:\n            SynapseError if the event does not pass muster\n        \"\"\"\n        if len(ev.prev_event_ids()) > 20:\n            logger.warning(\n                \"Rejecting event %s which has %i prev_events\",\n                ev.event_id,\n                len(ev.prev_event_ids()),\n            )\n            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many prev_events\")\n\n        if len(ev.auth_event_ids()) > 10:\n            logger.warning(\n                \"Rejecting event %s which has %i auth_events\",\n                ev.event_id,\n                len(ev.auth_event_ids()),\n            )\n            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many auth_events\")\n\n    async def send_invite(self, target_host, event):\n        \"\"\" Sends the invite to the remote server for signing.\n\n        Invites must be signed by the invitee's server before distribution.\n        \"\"\"\n        pdu = await self.federation_client.send_invite(\n            destination=target_host,\n            room_id=event.room_id,\n            event_id=event.event_id,\n            pdu=event,\n        )\n\n        return pdu\n\n    async def on_event_auth(self, event_id: str) -> List[EventBase]:\n        event = await self.store.get_event(event_id)\n        auth = await self.store.get_auth_chain(\n            list(event.auth_event_ids()), include_given=True\n        )\n        return list(auth)\n\n    async def do_invite_join(\n        self, target_hosts: Iterable[str], room_id: str, joinee: str, content: JsonDict\n    ) -> Tuple[str, int]:\n        \"\"\" Attempts to join the `joinee` to the room `room_id` via the\n        servers contained in `target_hosts`.\n\n        This first triggers a /make_join/ request that returns a partial\n        event that we can fill out and sign. This is then sent to the\n        remote server via /send_join/ which responds with the state at that\n        event and the auth_chains.\n\n        We suspend processing of any received events from this room until we\n        have finished processing the join.\n\n        Args:\n            target_hosts: List of servers to attempt to join the room with.\n\n            room_id: The ID of the room to join.\n\n            joinee: The User ID of the joining user.\n\n            content: The event content to use for the join event.\n        \"\"\"\n        # TODO: We should be able to call this on workers, but the upgrading of\n        # room stuff after join currently doesn't work on workers.\n        assert self.config.worker.worker_app is None\n\n        logger.debug(\"Joining %s to %s\", joinee, room_id)\n\n        origin, event, room_version_obj = await self._make_and_verify_event(\n            target_hosts,\n            room_id,\n            joinee,\n            \"join\",\n            content,\n            params={\"ver\": KNOWN_ROOM_VERSIONS},\n        )\n\n        # This shouldn't happen, because the RoomMemberHandler has a\n        # linearizer lock which only allows one operation per user per room\n        # at a time - so this is just paranoia.\n        assert room_id not in self.room_queues\n\n        self.room_queues[room_id] = []\n\n        await self._clean_room_for_join(room_id)\n\n        handled_events = set()\n\n        try:\n            # Try the host we successfully got a response to /make_join/\n            # request first.\n            host_list = list(target_hosts)\n            try:\n                host_list.remove(origin)\n                host_list.insert(0, origin)\n            except ValueError:\n                pass\n\n            ret = await self.federation_client.send_join(\n                host_list, event, room_version_obj\n            )\n\n            origin = ret[\"origin\"]\n            state = ret[\"state\"]\n            auth_chain = ret[\"auth_chain\"]\n            auth_chain.sort(key=lambda e: e.depth)\n\n            handled_events.update([s.event_id for s in state])\n            handled_events.update([a.event_id for a in auth_chain])\n            handled_events.add(event.event_id)\n\n            logger.debug(\"do_invite_join auth_chain: %s\", auth_chain)\n            logger.debug(\"do_invite_join state: %s\", state)\n\n            logger.debug(\"do_invite_join event: %s\", event)\n\n            # if this is the first time we've joined this room, it's time to add\n            # a row to `rooms` with the correct room version. If there's already a\n            # row there, we should override it, since it may have been populated\n            # based on an invite request which lied about the room version.\n            #\n            # federation_client.send_join has already checked that the room\n            # version in the received create event is the same as room_version_obj,\n            # so we can rely on it now.\n            #\n            await self.store.upsert_room_on_join(\n                room_id=room_id, room_version=room_version_obj,\n            )\n\n            max_stream_id = await self._persist_auth_tree(\n                origin, room_id, auth_chain, state, event, room_version_obj\n            )\n\n            # We wait here until this instance has seen the events come down\n            # replication (if we're using replication) as the below uses caches.\n            await self._replication.wait_for_stream_position(\n                self.config.worker.events_shard_config.get_instance(room_id),\n                \"events\",\n                max_stream_id,\n            )\n\n            # Check whether this room is the result of an upgrade of a room we already know\n            # about. If so, migrate over user information\n            predecessor = await self.store.get_room_predecessor(room_id)\n            if not predecessor or not isinstance(predecessor.get(\"room_id\"), str):\n                return event.event_id, max_stream_id\n            old_room_id = predecessor[\"room_id\"]\n            logger.debug(\n                \"Found predecessor for %s during remote join: %s\", room_id, old_room_id\n            )\n\n            # We retrieve the room member handler here as to not cause a cyclic dependency\n            member_handler = self.hs.get_room_member_handler()\n            await member_handler.transfer_room_state_on_room_upgrade(\n                old_room_id, room_id\n            )\n\n            logger.debug(\"Finished joining %s to %s\", joinee, room_id)\n            return event.event_id, max_stream_id\n        finally:\n            room_queue = self.room_queues[room_id]\n            del self.room_queues[room_id]\n\n            # we don't need to wait for the queued events to be processed -\n            # it's just a best-effort thing at this point. We do want to do\n            # them roughly in order, though, otherwise we'll end up making\n            # lots of requests for missing prev_events which we do actually\n            # have. Hence we fire off the background task, but don't wait for it.\n\n            run_in_background(self._handle_queued_pdus, room_queue)\n\n    async def _handle_queued_pdus(self, room_queue):\n        \"\"\"Process PDUs which got queued up while we were busy send_joining.\n\n        Args:\n            room_queue (list[FrozenEvent, str]): list of PDUs to be processed\n                and the servers that sent them\n        \"\"\"\n        for p, origin in room_queue:\n            try:\n                logger.info(\n                    \"Processing queued PDU %s which was received \"\n                    \"while we were joining %s\",\n                    p.event_id,\n                    p.room_id,\n                )\n                with nested_logging_context(p.event_id):\n                    await self.on_receive_pdu(origin, p, sent_to_us_directly=True)\n            except Exception as e:\n                logger.warning(\n                    \"Error handling queued PDU %s from %s: %s\", p.event_id, origin, e\n                )\n\n    async def on_make_join_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> EventBase:\n        \"\"\" We've received a /make_join/ request, so we create a partial\n        join event for the room and return that. We do *not* persist or\n        process it until the other server has signed it and sent it back.\n\n        Args:\n            origin: The (verified) server name of the requesting server.\n            room_id: Room to create join event in\n            user_id: The user to create the join for\n        \"\"\"\n        if get_domain_from_id(user_id) != origin:\n            logger.info(\n                \"Got /make_join request for user %r from different origin %s, ignoring\",\n                user_id,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        # checking the room version will check that we've actually heard of the room\n        # (and return a 404 otherwise)\n        room_version = await self.store.get_room_version_id(room_id)\n\n        # now check that we are *still* in the room\n        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)\n        if not is_in_room:\n            logger.info(\n                \"Got /make_join request for room %s we are no longer in\", room_id,\n            )\n            raise NotFoundError(\"Not an active room on this server\")\n\n        event_content = {\"membership\": Membership.JOIN}\n\n        builder = self.event_builder_factory.new(\n            room_version,\n            {\n                \"type\": EventTypes.Member,\n                \"content\": event_content,\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": user_id,\n            },\n        )\n\n        try:\n            event, context = await self.event_creation_handler.create_new_client_event(\n                builder=builder\n            )\n        except SynapseError as e:\n            logger.warning(\"Failed to create join to %s because %s\", room_id, e)\n            raise\n\n        # The remote hasn't signed it yet, obviously. We'll do the full checks\n        # when we get the event back in `on_send_join_request`\n        await self.auth.check_from_context(\n            room_version, event, context, do_sig_check=False\n        )\n\n        return event\n\n    async def on_send_join_request(self, origin, pdu):\n        \"\"\" We have received a join event for a room. Fully process it and\n        respond with the current state and auth chains.\n        \"\"\"\n        event = pdu\n\n        logger.debug(\n            \"on_send_join_request from %s: Got event: %s, signatures: %s\",\n            origin,\n            event.event_id,\n            event.signatures,\n        )\n\n        if get_domain_from_id(event.sender) != origin:\n            logger.info(\n                \"Got /send_join request for user %r from different origin %s\",\n                event.sender,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        event.internal_metadata.outlier = False\n        # Send this event on behalf of the origin server.\n        #\n        # The reasons we have the destination server rather than the origin\n        # server send it are slightly mysterious: the origin server should have\n        # all the necessary state once it gets the response to the send_join,\n        # so it could send the event itself if it wanted to. It may be that\n        # doing it this way reduces failure modes, or avoids certain attacks\n        # where a new server selectively tells a subset of the federation that\n        # it has joined.\n        #\n        # The fact is that, as of the current writing, Synapse doesn't send out\n        # the join event over federation after joining, and changing it now\n        # would introduce the danger of backwards-compatibility problems.\n        event.internal_metadata.send_on_behalf_of = origin\n\n        context = await self._handle_new_event(origin, event)\n\n        logger.debug(\n            \"on_send_join_request: After _handle_new_event: %s, sigs: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        prev_state_ids = await context.get_prev_state_ids()\n\n        state_ids = list(prev_state_ids.values())\n        auth_chain = await self.store.get_auth_chain(state_ids)\n\n        state = await self.store.get_events(list(prev_state_ids.values()))\n\n        return {\"state\": list(state.values()), \"auth_chain\": auth_chain}\n\n    async def on_invite_request(\n        self, origin: str, event: EventBase, room_version: RoomVersion\n    ):\n        \"\"\" We've got an invite event. Process and persist it. Sign it.\n\n        Respond with the now signed event.\n        \"\"\"\n        if event.state_key is None:\n            raise SynapseError(400, \"The invite event did not have a state key\")\n\n        is_blocked = await self.store.is_room_blocked(event.room_id)\n        if is_blocked:\n            raise SynapseError(403, \"This room has been blocked on this server\")\n\n        if self.hs.config.block_non_admin_invites:\n            raise SynapseError(403, \"This server does not accept room invites\")\n\n        if not self.spam_checker.user_may_invite(\n            event.sender, event.state_key, event.room_id\n        ):\n            raise SynapseError(\n                403, \"This user is not permitted to send invites to this server/user\"\n            )\n\n        membership = event.content.get(\"membership\")\n        if event.type != EventTypes.Member or membership != Membership.INVITE:\n            raise SynapseError(400, \"The event was not an m.room.member invite event\")\n\n        sender_domain = get_domain_from_id(event.sender)\n        if sender_domain != origin:\n            raise SynapseError(\n                400, \"The invite event was not from the server sending it\"\n            )\n\n        if not self.is_mine_id(event.state_key):\n            raise SynapseError(400, \"The invite event must be for this server\")\n\n        # block any attempts to invite the server notices mxid\n        if event.state_key == self._server_notices_mxid:\n            raise SynapseError(HTTPStatus.FORBIDDEN, \"Cannot invite this user\")\n\n        # keep a record of the room version, if we don't yet know it.\n        # (this may get overwritten if we later get a different room version in a\n        # join dance).\n        await self._maybe_store_room_on_outlier_membership(\n            room_id=event.room_id, room_version=room_version\n        )\n\n        event.internal_metadata.outlier = True\n        event.internal_metadata.out_of_band_membership = True\n\n        event.signatures.update(\n            compute_event_signature(\n                room_version,\n                event.get_pdu_json(),\n                self.hs.hostname,\n                self.hs.signing_key,\n            )\n        )\n\n        context = await self.state_handler.compute_event_context(event)\n        await self.persist_events_and_notify(event.room_id, [(event, context)])\n\n        return event\n\n    async def do_remotely_reject_invite(\n        self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict\n    ) -> Tuple[EventBase, int]:\n        origin, event, room_version = await self._make_and_verify_event(\n            target_hosts, room_id, user_id, \"leave\", content=content\n        )\n        # Mark as outlier as we don't have any state for this event; we're not\n        # even in the room.\n        event.internal_metadata.outlier = True\n        event.internal_metadata.out_of_band_membership = True\n\n        # Try the host that we successfully called /make_leave/ on first for\n        # the /send_leave/ request.\n        host_list = list(target_hosts)\n        try:\n            host_list.remove(origin)\n            host_list.insert(0, origin)\n        except ValueError:\n            pass\n\n        await self.federation_client.send_leave(host_list, event)\n\n        context = await self.state_handler.compute_event_context(event)\n        stream_id = await self.persist_events_and_notify(\n            event.room_id, [(event, context)]\n        )\n\n        return event, stream_id\n\n    async def _make_and_verify_event(\n        self,\n        target_hosts: Iterable[str],\n        room_id: str,\n        user_id: str,\n        membership: str,\n        content: JsonDict = {},\n        params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,\n    ) -> Tuple[str, EventBase, RoomVersion]:\n        (\n            origin,\n            event,\n            room_version,\n        ) = await self.federation_client.make_membership_event(\n            target_hosts, room_id, user_id, membership, content, params=params\n        )\n\n        logger.debug(\"Got response to make_%s: %s\", membership, event)\n\n        # We should assert some things.\n        # FIXME: Do this in a nicer way\n        assert event.type == EventTypes.Member\n        assert event.user_id == user_id\n        assert event.state_key == user_id\n        assert event.room_id == room_id\n        return origin, event, room_version\n\n    async def on_make_leave_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> EventBase:\n        \"\"\" We've received a /make_leave/ request, so we create a partial\n        leave event for the room and return that. We do *not* persist or\n        process it until the other server has signed it and sent it back.\n\n        Args:\n            origin: The (verified) server name of the requesting server.\n            room_id: Room to create leave event in\n            user_id: The user to create the leave for\n        \"\"\"\n        if get_domain_from_id(user_id) != origin:\n            logger.info(\n                \"Got /make_leave request for user %r from different origin %s, ignoring\",\n                user_id,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        room_version = await self.store.get_room_version_id(room_id)\n        builder = self.event_builder_factory.new(\n            room_version,\n            {\n                \"type\": EventTypes.Member,\n                \"content\": {\"membership\": Membership.LEAVE},\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": user_id,\n            },\n        )\n\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n\n        try:\n            # The remote hasn't signed it yet, obviously. We'll do the full checks\n            # when we get the event back in `on_send_leave_request`\n            await self.auth.check_from_context(\n                room_version, event, context, do_sig_check=False\n            )\n        except AuthError as e:\n            logger.warning(\"Failed to create new leave %r because %s\", event, e)\n            raise e\n\n        return event\n\n    async def on_send_leave_request(self, origin, pdu):\n        \"\"\" We have received a leave event for a room. Fully process it.\"\"\"\n        event = pdu\n\n        logger.debug(\n            \"on_send_leave_request: Got event: %s, signatures: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        if get_domain_from_id(event.sender) != origin:\n            logger.info(\n                \"Got /send_leave request for user %r from different origin %s\",\n                event.sender,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        event.internal_metadata.outlier = False\n\n        await self._handle_new_event(origin, event)\n\n        logger.debug(\n            \"on_send_leave_request: After _handle_new_event: %s, sigs: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        return None\n\n    async def get_state_for_pdu(self, room_id: str, event_id: str) -> List[EventBase]:\n        \"\"\"Returns the state at the event. i.e. not including said event.\n        \"\"\"\n\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        state_groups = await self.state_store.get_state_groups(room_id, [event_id])\n\n        if state_groups:\n            _, state = list(state_groups.items()).pop()\n            results = {(e.type, e.state_key): e for e in state}\n\n            if event.is_state():\n                # Get previous state\n                if \"replaces_state\" in event.unsigned:\n                    prev_id = event.unsigned[\"replaces_state\"]\n                    if prev_id != event.event_id:\n                        prev_event = await self.store.get_event(prev_id)\n                        results[(event.type, event.state_key)] = prev_event\n                else:\n                    del results[(event.type, event.state_key)]\n\n            res = list(results.values())\n            return res\n        else:\n            return []\n\n    async def get_state_ids_for_pdu(self, room_id: str, event_id: str) -> List[str]:\n        \"\"\"Returns the state at the event. i.e. not including said event.\n        \"\"\"\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        state_groups = await self.state_store.get_state_groups_ids(room_id, [event_id])\n\n        if state_groups:\n            _, state = list(state_groups.items()).pop()\n            results = state\n\n            if event.is_state():\n                # Get previous state\n                if \"replaces_state\" in event.unsigned:\n                    prev_id = event.unsigned[\"replaces_state\"]\n                    if prev_id != event.event_id:\n                        results[(event.type, event.state_key)] = prev_id\n                else:\n                    results.pop((event.type, event.state_key), None)\n\n            return list(results.values())\n        else:\n            return []\n\n    @log_function\n    async def on_backfill_request(\n        self, origin: str, room_id: str, pdu_list: List[str], limit: int\n    ) -> List[EventBase]:\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # Synapse asks for 100 events per backfill request. Do not allow more.\n        limit = min(limit, 100)\n\n        events = await self.store.get_backfill_events(room_id, pdu_list, limit)\n\n        events = await filter_events_for_server(self.storage, origin, events)\n\n        return events\n\n    @log_function\n    async def get_persisted_pdu(\n        self, origin: str, event_id: str\n    ) -> Optional[EventBase]:\n        \"\"\"Get an event from the database for the given server.\n\n        Args:\n            origin: hostname of server which is requesting the event; we\n               will check that the server is allowed to see it.\n            event_id: id of the event being requested\n\n        Returns:\n            None if we know nothing about the event; otherwise the (possibly-redacted) event.\n\n        Raises:\n            AuthError if the server is not currently in the room\n        \"\"\"\n        event = await self.store.get_event(\n            event_id, allow_none=True, allow_rejected=True\n        )\n\n        if event:\n            in_room = await self.auth.check_host_in_room(event.room_id, origin)\n            if not in_room:\n                raise AuthError(403, \"Host not in room.\")\n\n            events = await filter_events_for_server(self.storage, origin, [event])\n            event = events[0]\n            return event\n        else:\n            return None\n\n    async def get_min_depth_for_context(self, context):\n        return await self.store.get_min_depth(context)\n\n    async def _handle_new_event(\n        self, origin, event, state=None, auth_events=None, backfilled=False\n    ):\n        context = await self._prep_event(\n            origin, event, state=state, auth_events=auth_events, backfilled=backfilled\n        )\n\n        try:\n            if (\n                not event.internal_metadata.is_outlier()\n                and not backfilled\n                and not context.rejected\n            ):\n                await self.action_generator.handle_push_actions_for_event(\n                    event, context\n                )\n\n            await self.persist_events_and_notify(\n                event.room_id, [(event, context)], backfilled=backfilled\n            )\n        except Exception:\n            run_in_background(\n                self.store.remove_push_actions_from_staging, event.event_id\n            )\n            raise\n\n        return context\n\n    async def _handle_new_events(\n        self,\n        origin: str,\n        room_id: str,\n        event_infos: Iterable[_NewEventInfo],\n        backfilled: bool = False,\n    ) -> None:\n        \"\"\"Creates the appropriate contexts and persists events. The events\n        should not depend on one another, e.g. this should be used to persist\n        a bunch of outliers, but not a chunk of individual events that depend\n        on each other for state calculations.\n\n        Notifies about the events where appropriate.\n        \"\"\"\n\n        async def prep(ev_info: _NewEventInfo):\n            event = ev_info.event\n            with nested_logging_context(suffix=event.event_id):\n                res = await self._prep_event(\n                    origin,\n                    event,\n                    state=ev_info.state,\n                    auth_events=ev_info.auth_events,\n                    backfilled=backfilled,\n                )\n            return res\n\n        contexts = await make_deferred_yieldable(\n            defer.gatherResults(\n                [run_in_background(prep, ev_info) for ev_info in event_infos],\n                consumeErrors=True,\n            )\n        )\n\n        await self.persist_events_and_notify(\n            room_id,\n            [\n                (ev_info.event, context)\n                for ev_info, context in zip(event_infos, contexts)\n            ],\n            backfilled=backfilled,\n        )\n\n    async def _persist_auth_tree(\n        self,\n        origin: str,\n        room_id: str,\n        auth_events: List[EventBase],\n        state: List[EventBase],\n        event: EventBase,\n        room_version: RoomVersion,\n    ) -> int:\n        \"\"\"Checks the auth chain is valid (and passes auth checks) for the\n        state and event. Then persists the auth chain and state atomically.\n        Persists the event separately. Notifies about the persisted events\n        where appropriate.\n\n        Will attempt to fetch missing auth events.\n\n        Args:\n            origin: Where the events came from\n            room_id,\n            auth_events\n            state\n            event\n            room_version: The room version we expect this room to have, and\n                will raise if it doesn't match the version in the create event.\n        \"\"\"\n        events_to_context = {}\n        for e in itertools.chain(auth_events, state):\n            e.internal_metadata.outlier = True\n            ctx = await self.state_handler.compute_event_context(e)\n            events_to_context[e.event_id] = ctx\n\n        event_map = {\n            e.event_id: e for e in itertools.chain(auth_events, state, [event])\n        }\n\n        create_event = None\n        for e in auth_events:\n            if (e.type, e.state_key) == (EventTypes.Create, \"\"):\n                create_event = e\n                break\n\n        if create_event is None:\n            # If the state doesn't have a create event then the room is\n            # invalid, and it would fail auth checks anyway.\n            raise SynapseError(400, \"No create event in state\")\n\n        room_version_id = create_event.content.get(\n            \"room_version\", RoomVersions.V1.identifier\n        )\n\n        if room_version.identifier != room_version_id:\n            raise SynapseError(400, \"Room version mismatch\")\n\n        missing_auth_events = set()\n        for e in itertools.chain(auth_events, state, [event]):\n            for e_id in e.auth_event_ids():\n                if e_id not in event_map:\n                    missing_auth_events.add(e_id)\n\n        for e_id in missing_auth_events:\n            m_ev = await self.federation_client.get_pdu(\n                [origin], e_id, room_version=room_version, outlier=True, timeout=10000,\n            )\n            if m_ev and m_ev.event_id == e_id:\n                event_map[e_id] = m_ev\n            else:\n                logger.info(\"Failed to find auth event %r\", e_id)\n\n        for e in itertools.chain(auth_events, state, [event]):\n            auth_for_e = {\n                (event_map[e_id].type, event_map[e_id].state_key): event_map[e_id]\n                for e_id in e.auth_event_ids()\n                if e_id in event_map\n            }\n            if create_event:\n                auth_for_e[(EventTypes.Create, \"\")] = create_event\n\n            try:\n                event_auth.check(room_version, e, auth_events=auth_for_e)\n            except SynapseError as err:\n                # we may get SynapseErrors here as well as AuthErrors. For\n                # instance, there are a couple of (ancient) events in some\n                # rooms whose senders do not have the correct sigil; these\n                # cause SynapseErrors in auth.check. We don't want to give up\n                # the attempt to federate altogether in such cases.\n\n                logger.warning(\"Rejecting %s because %s\", e.event_id, err.msg)\n\n                if e == event:\n                    raise\n                events_to_context[e.event_id].rejected = RejectedReason.AUTH_ERROR\n\n        await self.persist_events_and_notify(\n            room_id,\n            [\n                (e, events_to_context[e.event_id])\n                for e in itertools.chain(auth_events, state)\n            ],\n        )\n\n        new_event_context = await self.state_handler.compute_event_context(\n            event, old_state=state\n        )\n\n        return await self.persist_events_and_notify(\n            room_id, [(event, new_event_context)]\n        )\n\n    async def _prep_event(\n        self,\n        origin: str,\n        event: EventBase,\n        state: Optional[Iterable[EventBase]],\n        auth_events: Optional[MutableStateMap[EventBase]],\n        backfilled: bool,\n    ) -> EventContext:\n        context = await self.state_handler.compute_event_context(event, old_state=state)\n\n        if not auth_events:\n            prev_state_ids = await context.get_prev_state_ids()\n            auth_events_ids = self.auth.compute_auth_events(\n                event, prev_state_ids, for_verification=True\n            )\n            auth_events_x = await self.store.get_events(auth_events_ids)\n            auth_events = {(e.type, e.state_key): e for e in auth_events_x.values()}\n\n        # This is a hack to fix some old rooms where the initial join event\n        # didn't reference the create event in its auth events.\n        if event.type == EventTypes.Member and not event.auth_event_ids():\n            if len(event.prev_event_ids()) == 1 and event.depth < 5:\n                c = await self.store.get_event(\n                    event.prev_event_ids()[0], allow_none=True\n                )\n                if c and c.type == EventTypes.Create:\n                    auth_events[(c.type, c.state_key)] = c\n\n        context = await self.do_auth(origin, event, context, auth_events=auth_events)\n\n        if not context.rejected:\n            await self._check_for_soft_fail(event, state, backfilled)\n\n        if event.type == EventTypes.GuestAccess and not context.rejected:\n            await self.maybe_kick_guest_users(event)\n\n        return context\n\n    async def _check_for_soft_fail(\n        self, event: EventBase, state: Optional[Iterable[EventBase]], backfilled: bool\n    ) -> None:\n        \"\"\"Checks if we should soft fail the event; if so, marks the event as\n        such.\n\n        Args:\n            event\n            state: The state at the event if we don't have all the event's prev events\n            backfilled: Whether the event is from backfill\n        \"\"\"\n        # For new (non-backfilled and non-outlier) events we check if the event\n        # passes auth based on the current state. If it doesn't then we\n        # \"soft-fail\" the event.\n        if backfilled or event.internal_metadata.is_outlier():\n            return\n\n        extrem_ids_list = await self.store.get_latest_event_ids_in_room(event.room_id)\n        extrem_ids = set(extrem_ids_list)\n        prev_event_ids = set(event.prev_event_ids())\n\n        if extrem_ids == prev_event_ids:\n            # If they're the same then the current state is the same as the\n            # state at the event, so no point rechecking auth for soft fail.\n            return\n\n        room_version = await self.store.get_room_version_id(event.room_id)\n        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]\n\n        # Calculate the \"current state\".\n        if state is not None:\n            # If we're explicitly given the state then we won't have all the\n            # prev events, and so we have a gap in the graph. In this case\n            # we want to be a little careful as we might have been down for\n            # a while and have an incorrect view of the current state,\n            # however we still want to do checks as gaps are easy to\n            # maliciously manufacture.\n            #\n            # So we use a \"current state\" that is actually a state\n            # resolution across the current forward extremities and the\n            # given state at the event. This should correctly handle cases\n            # like bans, especially with state res v2.\n\n            state_sets_d = await self.state_store.get_state_groups(\n                event.room_id, extrem_ids\n            )\n            state_sets = list(state_sets_d.values())  # type: List[Iterable[EventBase]]\n            state_sets.append(state)\n            current_states = await self.state_handler.resolve_events(\n                room_version, state_sets, event\n            )\n            current_state_ids = {\n                k: e.event_id for k, e in current_states.items()\n            }  # type: StateMap[str]\n        else:\n            current_state_ids = await self.state_handler.get_current_state_ids(\n                event.room_id, latest_event_ids=extrem_ids\n            )\n\n        logger.debug(\n            \"Doing soft-fail check for %s: state %s\", event.event_id, current_state_ids,\n        )\n\n        # Now check if event pass auth against said current state\n        auth_types = auth_types_for_event(event)\n        current_state_ids_list = [\n            e for k, e in current_state_ids.items() if k in auth_types\n        ]\n\n        auth_events_map = await self.store.get_events(current_state_ids_list)\n        current_auth_events = {\n            (e.type, e.state_key): e for e in auth_events_map.values()\n        }\n\n        try:\n            event_auth.check(room_version_obj, event, auth_events=current_auth_events)\n        except AuthError as e:\n            logger.warning(\"Soft-failing %r because %s\", event, e)\n            event.internal_metadata.soft_failed = True\n\n    async def on_query_auth(\n        self, origin, event_id, room_id, remote_auth_chain, rejects, missing\n    ):\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        # Just go through and process each event in `remote_auth_chain`. We\n        # don't want to fall into the trap of `missing` being wrong.\n        for e in remote_auth_chain:\n            try:\n                await self._handle_new_event(origin, e)\n            except AuthError:\n                pass\n\n        # Now get the current auth_chain for the event.\n        local_auth_chain = await self.store.get_auth_chain(\n            list(event.auth_event_ids()), include_given=True\n        )\n\n        # TODO: Check if we would now reject event_id. If so we need to tell\n        # everyone.\n\n        ret = await self.construct_auth_difference(local_auth_chain, remote_auth_chain)\n\n        logger.debug(\"on_query_auth returning: %s\", ret)\n\n        return ret\n\n    async def on_get_missing_events(\n        self, origin, room_id, earliest_events, latest_events, limit\n    ):\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # Only allow up to 20 events to be retrieved per request.\n        limit = min(limit, 20)\n\n        missing_events = await self.store.get_missing_events(\n            room_id=room_id,\n            earliest_events=earliest_events,\n            latest_events=latest_events,\n            limit=limit,\n        )\n\n        missing_events = await filter_events_for_server(\n            self.storage, origin, missing_events\n        )\n\n        return missing_events\n\n    async def do_auth(\n        self,\n        origin: str,\n        event: EventBase,\n        context: EventContext,\n        auth_events: MutableStateMap[EventBase],\n    ) -> EventContext:\n        \"\"\"\n\n        Args:\n            origin:\n            event:\n            context:\n            auth_events:\n                Map from (event_type, state_key) to event\n\n                Normally, our calculated auth_events based on the state of the room\n                at the event's position in the DAG, though occasionally (eg if the\n                event is an outlier), may be the auth events claimed by the remote\n                server.\n\n                Also NB that this function adds entries to it.\n        Returns:\n            updated context object\n        \"\"\"\n        room_version = await self.store.get_room_version_id(event.room_id)\n        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]\n\n        try:\n            context = await self._update_auth_events_and_context_for_auth(\n                origin, event, context, auth_events\n            )\n        except Exception:\n            # We don't really mind if the above fails, so lets not fail\n            # processing if it does. However, it really shouldn't fail so\n            # let's still log as an exception since we'll still want to fix\n            # any bugs.\n            logger.exception(\n                \"Failed to double check auth events for %s with remote. \"\n                \"Ignoring failure and continuing processing of event.\",\n                event.event_id,\n            )\n\n        try:\n            event_auth.check(room_version_obj, event, auth_events=auth_events)\n        except AuthError as e:\n            logger.warning(\"Failed auth resolution for %r because %s\", event, e)\n            context.rejected = RejectedReason.AUTH_ERROR\n\n        return context\n\n    async def _update_auth_events_and_context_for_auth(\n        self,\n        origin: str,\n        event: EventBase,\n        context: EventContext,\n        auth_events: MutableStateMap[EventBase],\n    ) -> EventContext:\n        \"\"\"Helper for do_auth. See there for docs.\n\n        Checks whether a given event has the expected auth events. If it\n        doesn't then we talk to the remote server to compare state to see if\n        we can come to a consensus (e.g. if one server missed some valid\n        state).\n\n        This attempts to resolve any potential divergence of state between\n        servers, but is not essential and so failures should not block further\n        processing of the event.\n\n        Args:\n            origin:\n            event:\n            context:\n\n            auth_events:\n                Map from (event_type, state_key) to event\n\n                Normally, our calculated auth_events based on the state of the room\n                at the event's position in the DAG, though occasionally (eg if the\n                event is an outlier), may be the auth events claimed by the remote\n                server.\n\n                Also NB that this function adds entries to it.\n\n        Returns:\n            updated context\n        \"\"\"\n        event_auth_events = set(event.auth_event_ids())\n\n        # missing_auth is the set of the event's auth_events which we don't yet have\n        # in auth_events.\n        missing_auth = event_auth_events.difference(\n            e.event_id for e in auth_events.values()\n        )\n\n        # if we have missing events, we need to fetch those events from somewhere.\n        #\n        # we start by checking if they are in the store, and then try calling /event_auth/.\n        if missing_auth:\n            have_events = await self.store.have_seen_events(missing_auth)\n            logger.debug(\"Events %s are in the store\", have_events)\n            missing_auth.difference_update(have_events)\n\n        if missing_auth:\n            # If we don't have all the auth events, we need to get them.\n            logger.info(\"auth_events contains unknown events: %s\", missing_auth)\n            try:\n                try:\n                    remote_auth_chain = await self.federation_client.get_event_auth(\n                        origin, event.room_id, event.event_id\n                    )\n                except RequestSendFailed as e1:\n                    # The other side isn't around or doesn't implement the\n                    # endpoint, so lets just bail out.\n                    logger.info(\"Failed to get event auth from remote: %s\", e1)\n                    return context\n\n                seen_remotes = await self.store.have_seen_events(\n                    [e.event_id for e in remote_auth_chain]\n                )\n\n                for e in remote_auth_chain:\n                    if e.event_id in seen_remotes:\n                        continue\n\n                    if e.event_id == event.event_id:\n                        continue\n\n                    try:\n                        auth_ids = e.auth_event_ids()\n                        auth = {\n                            (e.type, e.state_key): e\n                            for e in remote_auth_chain\n                            if e.event_id in auth_ids or e.type == EventTypes.Create\n                        }\n                        e.internal_metadata.outlier = True\n\n                        logger.debug(\n                            \"do_auth %s missing_auth: %s\", event.event_id, e.event_id\n                        )\n                        await self._handle_new_event(origin, e, auth_events=auth)\n\n                        if e.event_id in event_auth_events:\n                            auth_events[(e.type, e.state_key)] = e\n                    except AuthError:\n                        pass\n\n            except Exception:\n                logger.exception(\"Failed to get auth chain\")\n\n        if event.internal_metadata.is_outlier():\n            # XXX: given that, for an outlier, we'll be working with the\n            # event's *claimed* auth events rather than those we calculated:\n            # (a) is there any point in this test, since different_auth below will\n            # obviously be empty\n            # (b) alternatively, why don't we do it earlier?\n            logger.info(\"Skipping auth_event fetch for outlier\")\n            return context\n\n        different_auth = event_auth_events.difference(\n            e.event_id for e in auth_events.values()\n        )\n\n        if not different_auth:\n            return context\n\n        logger.info(\n            \"auth_events refers to events which are not in our calculated auth \"\n            \"chain: %s\",\n            different_auth,\n        )\n\n        # XXX: currently this checks for redactions but I'm not convinced that is\n        # necessary?\n        different_events = await self.store.get_events_as_list(different_auth)\n\n        for d in different_events:\n            if d.room_id != event.room_id:\n                logger.warning(\n                    \"Event %s refers to auth_event %s which is in a different room\",\n                    event.event_id,\n                    d.event_id,\n                )\n\n                # don't attempt to resolve the claimed auth events against our own\n                # in this case: just use our own auth events.\n                #\n                # XXX: should we reject the event in this case? It feels like we should,\n                # but then shouldn't we also do so if we've failed to fetch any of the\n                # auth events?\n                return context\n\n        # now we state-resolve between our own idea of the auth events, and the remote's\n        # idea of them.\n\n        local_state = auth_events.values()\n        remote_auth_events = dict(auth_events)\n        remote_auth_events.update({(d.type, d.state_key): d for d in different_events})\n        remote_state = remote_auth_events.values()\n\n        room_version = await self.store.get_room_version_id(event.room_id)\n        new_state = await self.state_handler.resolve_events(\n            room_version, (local_state, remote_state), event\n        )\n\n        logger.info(\n            \"After state res: updating auth_events with new state %s\",\n            {\n                (d.type, d.state_key): d.event_id\n                for d in new_state.values()\n                if auth_events.get((d.type, d.state_key)) != d\n            },\n        )\n\n        auth_events.update(new_state)\n\n        context = await self._update_context_for_auth_events(\n            event, context, auth_events\n        )\n\n        return context\n\n    async def _update_context_for_auth_events(\n        self, event: EventBase, context: EventContext, auth_events: StateMap[EventBase]\n    ) -> EventContext:\n        \"\"\"Update the state_ids in an event context after auth event resolution,\n        storing the changes as a new state group.\n\n        Args:\n            event: The event we're handling the context for\n\n            context: initial event context\n\n            auth_events: Events to update in the event context.\n\n        Returns:\n            new event context\n        \"\"\"\n        # exclude the state key of the new event from the current_state in the context.\n        if event.is_state():\n            event_key = (event.type, event.state_key)  # type: Optional[Tuple[str, str]]\n        else:\n            event_key = None\n        state_updates = {\n            k: a.event_id for k, a in auth_events.items() if k != event_key\n        }\n\n        current_state_ids = await context.get_current_state_ids()\n        current_state_ids = dict(current_state_ids)  # type: ignore\n\n        current_state_ids.update(state_updates)\n\n        prev_state_ids = await context.get_prev_state_ids()\n        prev_state_ids = dict(prev_state_ids)\n\n        prev_state_ids.update({k: a.event_id for k, a in auth_events.items()})\n\n        # create a new state group as a delta from the existing one.\n        prev_group = context.state_group\n        state_group = await self.state_store.store_state_group(\n            event.event_id,\n            event.room_id,\n            prev_group=prev_group,\n            delta_ids=state_updates,\n            current_state_ids=current_state_ids,\n        )\n\n        return EventContext.with_state(\n            state_group=state_group,\n            state_group_before_event=context.state_group_before_event,\n            current_state_ids=current_state_ids,\n            prev_state_ids=prev_state_ids,\n            prev_group=prev_group,\n            delta_ids=state_updates,\n        )\n\n    async def construct_auth_difference(\n        self, local_auth: Iterable[EventBase], remote_auth: Iterable[EventBase]\n    ) -> Dict:\n        \"\"\" Given a local and remote auth chain, find the differences. This\n        assumes that we have already processed all events in remote_auth\n\n        Params:\n            local_auth (list)\n            remote_auth (list)\n\n        Returns:\n            dict\n        \"\"\"\n\n        logger.debug(\"construct_auth_difference Start!\")\n\n        # TODO: Make sure we are OK with local_auth or remote_auth having more\n        # auth events in them than strictly necessary.\n\n        def sort_fun(ev):\n            return ev.depth, ev.event_id\n\n        logger.debug(\"construct_auth_difference after sort_fun!\")\n\n        # We find the differences by starting at the \"bottom\" of each list\n        # and iterating up on both lists. The lists are ordered by depth and\n        # then event_id, we iterate up both lists until we find the event ids\n        # don't match. Then we look at depth/event_id to see which side is\n        # missing that event, and iterate only up that list. Repeat.\n\n        remote_list = list(remote_auth)\n        remote_list.sort(key=sort_fun)\n\n        local_list = list(local_auth)\n        local_list.sort(key=sort_fun)\n\n        local_iter = iter(local_list)\n        remote_iter = iter(remote_list)\n\n        logger.debug(\"construct_auth_difference before get_next!\")\n\n        def get_next(it, opt=None):\n            try:\n                return next(it)\n            except Exception:\n                return opt\n\n        current_local = get_next(local_iter)\n        current_remote = get_next(remote_iter)\n\n        logger.debug(\"construct_auth_difference before while\")\n\n        missing_remotes = []\n        missing_locals = []\n        while current_local or current_remote:\n            if current_remote is None:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n                continue\n\n            if current_local is None:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n            if current_local.event_id == current_remote.event_id:\n                current_local = get_next(local_iter)\n                current_remote = get_next(remote_iter)\n                continue\n\n            if current_local.depth < current_remote.depth:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n                continue\n\n            if current_local.depth > current_remote.depth:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n            # They have the same depth, so we fall back to the event_id order\n            if current_local.event_id < current_remote.event_id:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n\n            if current_local.event_id > current_remote.event_id:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n        logger.debug(\"construct_auth_difference after while\")\n\n        # missing locals should be sent to the server\n        # We should find why we are missing remotes, as they will have been\n        # rejected.\n\n        # Remove events from missing_remotes if they are referencing a missing\n        # remote. We only care about the \"root\" rejected ones.\n        missing_remote_ids = [e.event_id for e in missing_remotes]\n        base_remote_rejected = list(missing_remotes)\n        for e in missing_remotes:\n            for e_id in e.auth_event_ids():\n                if e_id in missing_remote_ids:\n                    try:\n                        base_remote_rejected.remove(e)\n                    except ValueError:\n                        pass\n\n        reason_map = {}\n\n        for e in base_remote_rejected:\n            reason = await self.store.get_rejection_reason(e.event_id)\n            if reason is None:\n                # TODO: e is not in the current state, so we should\n                # construct some proof of that.\n                continue\n\n            reason_map[e.event_id] = reason\n\n        logger.debug(\"construct_auth_difference returning\")\n\n        return {\n            \"auth_chain\": local_auth,\n            \"rejects\": {\n                e.event_id: {\"reason\": reason_map[e.event_id], \"proof\": None}\n                for e in base_remote_rejected\n            },\n            \"missing\": [e.event_id for e in missing_locals],\n        }\n\n    @log_function\n    async def exchange_third_party_invite(\n        self, sender_user_id, target_user_id, room_id, signed\n    ):\n        third_party_invite = {\"signed\": signed}\n\n        event_dict = {\n            \"type\": EventTypes.Member,\n            \"content\": {\n                \"membership\": Membership.INVITE,\n                \"third_party_invite\": third_party_invite,\n            },\n            \"room_id\": room_id,\n            \"sender\": sender_user_id,\n            \"state_key\": target_user_id,\n        }\n\n        if await self.auth.check_host_in_room(room_id, self.hs.hostname):\n            room_version = await self.store.get_room_version_id(room_id)\n            builder = self.event_builder_factory.new(room_version, event_dict)\n\n            EventValidator().validate_builder(builder)\n            event, context = await self.event_creation_handler.create_new_client_event(\n                builder=builder\n            )\n\n            event, context = await self.add_display_name_to_third_party_invite(\n                room_version, event_dict, event, context\n            )\n\n            EventValidator().validate_new(event, self.config)\n\n            # We need to tell the transaction queue to send this out, even\n            # though the sender isn't a local user.\n            event.internal_metadata.send_on_behalf_of = self.hs.hostname\n\n            try:\n                await self.auth.check_from_context(room_version, event, context)\n            except AuthError as e:\n                logger.warning(\"Denying new third party invite %r because %s\", event, e)\n                raise e\n\n            await self._check_signature(event, context)\n\n            # We retrieve the room member handler here as to not cause a cyclic dependency\n            member_handler = self.hs.get_room_member_handler()\n            await member_handler.send_membership_event(None, event, context)\n        else:\n            destinations = {x.split(\":\", 1)[-1] for x in (sender_user_id, room_id)}\n            await self.federation_client.forward_third_party_invite(\n                destinations, room_id, event_dict\n            )\n\n    async def on_exchange_third_party_invite_request(\n        self, event_dict: JsonDict\n    ) -> None:\n        \"\"\"Handle an exchange_third_party_invite request from a remote server\n\n        The remote server will call this when it wants to turn a 3pid invite\n        into a normal m.room.member invite.\n\n        Args:\n            event_dict: Dictionary containing the event body.\n\n        \"\"\"\n        assert_params_in_dict(event_dict, [\"room_id\"])\n        room_version = await self.store.get_room_version_id(event_dict[\"room_id\"])\n\n        # NB: event_dict has a particular specced format we might need to fudge\n        # if we change event formats too much.\n        builder = self.event_builder_factory.new(room_version, event_dict)\n\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n        event, context = await self.add_display_name_to_third_party_invite(\n            room_version, event_dict, event, context\n        )\n\n        try:\n            await self.auth.check_from_context(room_version, event, context)\n        except AuthError as e:\n            logger.warning(\"Denying third party invite %r because %s\", event, e)\n            raise e\n        await self._check_signature(event, context)\n\n        # We need to tell the transaction queue to send this out, even\n        # though the sender isn't a local user.\n        event.internal_metadata.send_on_behalf_of = get_domain_from_id(event.sender)\n\n        # We retrieve the room member handler here as to not cause a cyclic dependency\n        member_handler = self.hs.get_room_member_handler()\n        await member_handler.send_membership_event(None, event, context)\n\n    async def add_display_name_to_third_party_invite(\n        self, room_version, event_dict, event, context\n    ):\n        key = (\n            EventTypes.ThirdPartyInvite,\n            event.content[\"third_party_invite\"][\"signed\"][\"token\"],\n        )\n        original_invite = None\n        prev_state_ids = await context.get_prev_state_ids()\n        original_invite_id = prev_state_ids.get(key)\n        if original_invite_id:\n            original_invite = await self.store.get_event(\n                original_invite_id, allow_none=True\n            )\n        if original_invite:\n            # If the m.room.third_party_invite event's content is empty, it means the\n            # invite has been revoked. In this case, we don't have to raise an error here\n            # because the auth check will fail on the invite (because it's not able to\n            # fetch public keys from the m.room.third_party_invite event's content, which\n            # is empty).\n            display_name = original_invite.content.get(\"display_name\")\n            event_dict[\"content\"][\"third_party_invite\"][\"display_name\"] = display_name\n        else:\n            logger.info(\n                \"Could not find invite event for third_party_invite: %r\", event_dict\n            )\n            # We don't discard here as this is not the appropriate place to do\n            # auth checks. If we need the invite and don't have it then the\n            # auth check code will explode appropriately.\n\n        builder = self.event_builder_factory.new(room_version, event_dict)\n        EventValidator().validate_builder(builder)\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n        EventValidator().validate_new(event, self.config)\n        return (event, context)\n\n    async def _check_signature(self, event, context):\n        \"\"\"\n        Checks that the signature in the event is consistent with its invite.\n\n        Args:\n            event (Event): The m.room.member event to check\n            context (EventContext):\n\n        Raises:\n            AuthError: if signature didn't match any keys, or key has been\n                revoked,\n            SynapseError: if a transient error meant a key couldn't be checked\n                for revocation.\n        \"\"\"\n        signed = event.content[\"third_party_invite\"][\"signed\"]\n        token = signed[\"token\"]\n\n        prev_state_ids = await context.get_prev_state_ids()\n        invite_event_id = prev_state_ids.get((EventTypes.ThirdPartyInvite, token))\n\n        invite_event = None\n        if invite_event_id:\n            invite_event = await self.store.get_event(invite_event_id, allow_none=True)\n\n        if not invite_event:\n            raise AuthError(403, \"Could not find invite\")\n\n        logger.debug(\"Checking auth on event %r\", event.content)\n\n        last_exception = None  # type: Optional[Exception]\n\n        # for each public key in the 3pid invite event\n        for public_key_object in self.hs.get_auth().get_public_keys(invite_event):\n            try:\n                # for each sig on the third_party_invite block of the actual invite\n                for server, signature_block in signed[\"signatures\"].items():\n                    for key_name, encoded_signature in signature_block.items():\n                        if not key_name.startswith(\"ed25519:\"):\n                            continue\n\n                        logger.debug(\n                            \"Attempting to verify sig with key %s from %r \"\n                            \"against pubkey %r\",\n                            key_name,\n                            server,\n                            public_key_object,\n                        )\n\n                        try:\n                            public_key = public_key_object[\"public_key\"]\n                            verify_key = decode_verify_key_bytes(\n                                key_name, decode_base64(public_key)\n                            )\n                            verify_signed_json(signed, server, verify_key)\n                            logger.debug(\n                                \"Successfully verified sig with key %s from %r \"\n                                \"against pubkey %r\",\n                                key_name,\n                                server,\n                                public_key_object,\n                            )\n                        except Exception:\n                            logger.info(\n                                \"Failed to verify sig with key %s from %r \"\n                                \"against pubkey %r\",\n                                key_name,\n                                server,\n                                public_key_object,\n                            )\n                            raise\n                        try:\n                            if \"key_validity_url\" in public_key_object:\n                                await self._check_key_revocation(\n                                    public_key, public_key_object[\"key_validity_url\"]\n                                )\n                        except Exception:\n                            logger.info(\n                                \"Failed to query key_validity_url %s\",\n                                public_key_object[\"key_validity_url\"],\n                            )\n                            raise\n                        return\n            except Exception as e:\n                last_exception = e\n\n        if last_exception is None:\n            # we can only get here if get_public_keys() returned an empty list\n            # TODO: make this better\n            raise RuntimeError(\"no public key in invite event\")\n\n        raise last_exception\n\n    async def _check_key_revocation(self, public_key, url):\n        \"\"\"\n        Checks whether public_key has been revoked.\n\n        Args:\n            public_key (str): base-64 encoded public key.\n            url (str): Key revocation URL.\n\n        Raises:\n            AuthError: if they key has been revoked.\n            SynapseError: if a transient error meant a key couldn't be checked\n                for revocation.\n        \"\"\"\n        try:\n            response = await self.http_client.get_json(url, {\"public_key\": public_key})\n        except Exception:\n            raise SynapseError(502, \"Third party certificate could not be checked\")\n        if \"valid\" not in response or not response[\"valid\"]:\n            raise AuthError(403, \"Third party certificate was invalid\")\n\n    async def persist_events_and_notify(\n        self,\n        room_id: str,\n        event_and_contexts: Sequence[Tuple[EventBase, EventContext]],\n        backfilled: bool = False,\n    ) -> int:\n        \"\"\"Persists events and tells the notifier/pushers about them, if\n        necessary.\n\n        Args:\n            room_id: The room ID of events being persisted.\n            event_and_contexts: Sequence of events with their associated\n                context that should be persisted. All events must belong to\n                the same room.\n            backfilled: Whether these events are a result of\n                backfilling or not\n        \"\"\"\n        instance = self.config.worker.events_shard_config.get_instance(room_id)\n        if instance != self._instance_name:\n            result = await self._send_events(\n                instance_name=instance,\n                store=self.store,\n                room_id=room_id,\n                event_and_contexts=event_and_contexts,\n                backfilled=backfilled,\n            )\n            return result[\"max_stream_id\"]\n        else:\n            assert self.storage.persistence\n\n            # Note that this returns the events that were persisted, which may not be\n            # the same as were passed in if some were deduplicated due to transaction IDs.\n            events, max_stream_token = await self.storage.persistence.persist_events(\n                event_and_contexts, backfilled=backfilled\n            )\n\n            if self._ephemeral_messages_enabled:\n                for event in events:\n                    # If there's an expiry timestamp on the event, schedule its expiry.\n                    self._message_handler.maybe_schedule_expiry(event)\n\n            if not backfilled:  # Never notify for backfilled events\n                for event in events:\n                    await self._notify_persisted_event(event, max_stream_token)\n\n            return max_stream_token.stream\n\n    async def _notify_persisted_event(\n        self, event: EventBase, max_stream_token: RoomStreamToken\n    ) -> None:\n        \"\"\"Checks to see if notifier/pushers should be notified about the\n        event or not.\n\n        Args:\n            event:\n            max_stream_id: The max_stream_id returned by persist_events\n        \"\"\"\n\n        extra_users = []\n        if event.type == EventTypes.Member:\n            target_user_id = event.state_key\n\n            # We notify for memberships if its an invite for one of our\n            # users\n            if event.internal_metadata.is_outlier():\n                if event.membership != Membership.INVITE:\n                    if not self.is_mine_id(target_user_id):\n                        return\n\n            target_user = UserID.from_string(target_user_id)\n            extra_users.append(target_user)\n        elif event.internal_metadata.is_outlier():\n            return\n\n        # the event has been persisted so it should have a stream ordering.\n        assert event.internal_metadata.stream_ordering\n\n        event_pos = PersistedEventPosition(\n            self._instance_name, event.internal_metadata.stream_ordering\n        )\n        self.notifier.on_new_room_event(\n            event, event_pos, max_stream_token, extra_users=extra_users\n        )\n\n    async def _clean_room_for_join(self, room_id: str) -> None:\n        \"\"\"Called to clean up any data in DB for a given room, ready for the\n        server to join the room.\n\n        Args:\n            room_id\n        \"\"\"\n        if self.config.worker_app:\n            await self._clean_room_for_join_client(room_id)\n        else:\n            await self.store.clean_room_for_join(room_id)\n\n    async def get_room_complexity(\n        self, remote_room_hosts: List[str], room_id: str\n    ) -> Optional[dict]:\n        \"\"\"\n        Fetch the complexity of a remote room over federation.\n\n        Args:\n            remote_room_hosts (list[str]): The remote servers to ask.\n            room_id (str): The room ID to ask about.\n\n        Returns:\n            Dict contains the complexity\n            metric versions, while None means we could not fetch the complexity.\n        \"\"\"\n\n        for host in remote_room_hosts:\n            res = await self.federation_client.get_room_complexity(host, room_id)\n\n            # We got a result, return it.\n            if res:\n                return res\n\n        # We fell off the bottom, couldn't get the complexity from anyone. Oh\n        # well.\n        return None\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2017-2018 New Vector Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Contains handlers for federation events.\"\"\"\n\nimport itertools\nimport logging\nfrom collections.abc import Container\nfrom http import HTTPStatus\nfrom typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n\nimport attr\nfrom signedjson.key import decode_verify_key_bytes\nfrom signedjson.sign import verify_signed_json\nfrom unpaddedbase64 import decode_base64\n\nfrom twisted.internet import defer\n\nfrom synapse import event_auth\nfrom synapse.api.constants import (\n    EventTypes,\n    Membership,\n    RejectedReason,\n    RoomEncryptionAlgorithms,\n)\nfrom synapse.api.errors import (\n    AuthError,\n    CodeMessageException,\n    Codes,\n    FederationDeniedError,\n    FederationError,\n    HttpResponseException,\n    NotFoundError,\n    RequestSendFailed,\n    SynapseError,\n)\nfrom synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion, RoomVersions\nfrom synapse.crypto.event_signing import compute_event_signature\nfrom synapse.event_auth import auth_types_for_event\nfrom synapse.events import EventBase\nfrom synapse.events.snapshot import EventContext\nfrom synapse.events.validator import EventValidator\nfrom synapse.handlers._base import BaseHandler\nfrom synapse.http.servlet import assert_params_in_dict\nfrom synapse.logging.context import (\n    make_deferred_yieldable,\n    nested_logging_context,\n    preserve_fn,\n    run_in_background,\n)\nfrom synapse.logging.utils import log_function\nfrom synapse.metrics.background_process_metrics import run_as_background_process\nfrom synapse.replication.http.devices import ReplicationUserDevicesResyncRestServlet\nfrom synapse.replication.http.federation import (\n    ReplicationCleanRoomRestServlet,\n    ReplicationFederationSendEventsRestServlet,\n    ReplicationStoreRoomOnOutlierMembershipRestServlet,\n)\nfrom synapse.state import StateResolutionStore\nfrom synapse.storage.databases.main.events_worker import EventRedactBehaviour\nfrom synapse.types import (\n    JsonDict,\n    MutableStateMap,\n    PersistedEventPosition,\n    RoomStreamToken,\n    StateMap,\n    UserID,\n    get_domain_from_id,\n)\nfrom synapse.util.async_helpers import Linearizer, concurrently_execute\nfrom synapse.util.retryutils import NotRetryingDestination\nfrom synapse.util.stringutils import shortstr\nfrom synapse.visibility import filter_events_for_server\n\nif TYPE_CHECKING:\n    from synapse.server import HomeServer\n\nlogger = logging.getLogger(__name__)\n\n\n@attr.s(slots=True)\nclass _NewEventInfo:\n    \"\"\"Holds information about a received event, ready for passing to _handle_new_events\n\n    Attributes:\n        event: the received event\n\n        state: the state at that event\n\n        auth_events: the auth_event map for that event\n    \"\"\"\n\n    event = attr.ib(type=EventBase)\n    state = attr.ib(type=Optional[Sequence[EventBase]], default=None)\n    auth_events = attr.ib(type=Optional[MutableStateMap[EventBase]], default=None)\n\n\nclass FederationHandler(BaseHandler):\n    \"\"\"Handles events that originated from federation.\n        Responsible for:\n        a) handling received Pdus before handing them on as Events to the rest\n        of the homeserver (including auth and state conflict resolutions)\n        b) converting events that were produced by local clients that may need\n        to be sent to remote homeservers.\n        c) doing the necessary dances to invite remote users and join remote\n        rooms.\n    \"\"\"\n\n    def __init__(self, hs: \"HomeServer\"):\n        super().__init__(hs)\n\n        self.hs = hs\n\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.state_store = self.storage.state\n        self.federation_client = hs.get_federation_client()\n        self.state_handler = hs.get_state_handler()\n        self._state_resolution_handler = hs.get_state_resolution_handler()\n        self.server_name = hs.hostname\n        self.keyring = hs.get_keyring()\n        self.action_generator = hs.get_action_generator()\n        self.is_mine_id = hs.is_mine_id\n        self.spam_checker = hs.get_spam_checker()\n        self.event_creation_handler = hs.get_event_creation_handler()\n        self._message_handler = hs.get_message_handler()\n        self._server_notices_mxid = hs.config.server_notices_mxid\n        self.config = hs.config\n        self.http_client = hs.get_simple_http_client()\n        self._instance_name = hs.get_instance_name()\n        self._replication = hs.get_replication_data_handler()\n\n        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)\n        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(\n            hs\n        )\n\n        if hs.config.worker_app:\n            self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(\n                hs\n            )\n            self._maybe_store_room_on_outlier_membership = ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(\n                hs\n            )\n        else:\n            self._device_list_updater = hs.get_device_handler().device_list_updater\n            self._maybe_store_room_on_outlier_membership = (\n                self.store.maybe_store_room_on_outlier_membership\n            )\n\n        # When joining a room we need to queue any events for that room up.\n        # For each room, a list of (pdu, origin) tuples.\n        self.room_queues = {}  # type: Dict[str, List[Tuple[EventBase, str]]]\n        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")\n\n        self.third_party_event_rules = hs.get_third_party_event_rules()\n\n        self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages\n\n    async def on_receive_pdu(self, origin, pdu, sent_to_us_directly=False) -> None:\n        \"\"\" Process a PDU received via a federation /send/ transaction, or\n        via backfill of missing prev_events\n\n        Args:\n            origin (str): server which initiated the /send/ transaction. Will\n                be used to fetch missing events or state.\n            pdu (FrozenEvent): received PDU\n            sent_to_us_directly (bool): True if this event was pushed to us; False if\n                we pulled it as the result of a missing prev_event.\n        \"\"\"\n\n        room_id = pdu.room_id\n        event_id = pdu.event_id\n\n        logger.info(\"handling received PDU: %s\", pdu)\n\n        # We reprocess pdus when we have seen them only as outliers\n        existing = await self.store.get_event(\n            event_id, allow_none=True, allow_rejected=True\n        )\n\n        # FIXME: Currently we fetch an event again when we already have it\n        # if it has been marked as an outlier.\n\n        already_seen = existing and (\n            not existing.internal_metadata.is_outlier()\n            or pdu.internal_metadata.is_outlier()\n        )\n        if already_seen:\n            logger.debug(\"[%s %s]: Already seen pdu\", room_id, event_id)\n            return\n\n        # do some initial sanity-checking of the event. In particular, make\n        # sure it doesn't have hundreds of prev_events or auth_events, which\n        # could cause a huge state resolution or cascade of event fetches.\n        try:\n            self._sanity_check_event(pdu)\n        except SynapseError as err:\n            logger.warning(\n                \"[%s %s] Received event failed sanity checks\", room_id, event_id\n            )\n            raise FederationError(\"ERROR\", err.code, err.msg, affected=pdu.event_id)\n\n        # If we are currently in the process of joining this room, then we\n        # queue up events for later processing.\n        if room_id in self.room_queues:\n            logger.info(\n                \"[%s %s] Queuing PDU from %s for now: join in progress\",\n                room_id,\n                event_id,\n                origin,\n            )\n            self.room_queues[room_id].append((pdu, origin))\n            return\n\n        # If we're not in the room just ditch the event entirely. This is\n        # probably an old server that has come back and thinks we're still in\n        # the room (or we've been rejoined to the room by a state reset).\n        #\n        # Note that if we were never in the room then we would have already\n        # dropped the event, since we wouldn't know the room version.\n        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)\n        if not is_in_room:\n            logger.info(\n                \"[%s %s] Ignoring PDU from %s as we're not in the room\",\n                room_id,\n                event_id,\n                origin,\n            )\n            return None\n\n        state = None\n\n        # Get missing pdus if necessary.\n        if not pdu.internal_metadata.is_outlier():\n            # We only backfill backwards to the min depth.\n            min_depth = await self.get_min_depth_for_context(pdu.room_id)\n\n            logger.debug(\"[%s %s] min_depth: %d\", room_id, event_id, min_depth)\n\n            prevs = set(pdu.prev_event_ids())\n            seen = await self.store.have_events_in_timeline(prevs)\n\n            if min_depth is not None and pdu.depth < min_depth:\n                # This is so that we don't notify the user about this\n                # message, to work around the fact that some events will\n                # reference really really old events we really don't want to\n                # send to the clients.\n                pdu.internal_metadata.outlier = True\n            elif min_depth is not None and pdu.depth > min_depth:\n                missing_prevs = prevs - seen\n                if sent_to_us_directly and missing_prevs:\n                    # If we're missing stuff, ensure we only fetch stuff one\n                    # at a time.\n                    logger.info(\n                        \"[%s %s] Acquiring room lock to fetch %d missing prev_events: %s\",\n                        room_id,\n                        event_id,\n                        len(missing_prevs),\n                        shortstr(missing_prevs),\n                    )\n                    with (await self._room_pdu_linearizer.queue(pdu.room_id)):\n                        logger.info(\n                            \"[%s %s] Acquired room lock to fetch %d missing prev_events\",\n                            room_id,\n                            event_id,\n                            len(missing_prevs),\n                        )\n\n                        try:\n                            await self._get_missing_events_for_pdu(\n                                origin, pdu, prevs, min_depth\n                            )\n                        except Exception as e:\n                            raise Exception(\n                                \"Error fetching missing prev_events for %s: %s\"\n                                % (event_id, e)\n                            ) from e\n\n                        # Update the set of things we've seen after trying to\n                        # fetch the missing stuff\n                        seen = await self.store.have_events_in_timeline(prevs)\n\n                        if not prevs - seen:\n                            logger.info(\n                                \"[%s %s] Found all missing prev_events\",\n                                room_id,\n                                event_id,\n                            )\n\n            if prevs - seen:\n                # We've still not been able to get all of the prev_events for this event.\n                #\n                # In this case, we need to fall back to asking another server in the\n                # federation for the state at this event. That's ok provided we then\n                # resolve the state against other bits of the DAG before using it (which\n                # will ensure that you can't just take over a room by sending an event,\n                # withholding its prev_events, and declaring yourself to be an admin in\n                # the subsequent state request).\n                #\n                # Now, if we're pulling this event as a missing prev_event, then clearly\n                # this event is not going to become the only forward-extremity and we are\n                # guaranteed to resolve its state against our existing forward\n                # extremities, so that should be fine.\n                #\n                # On the other hand, if this event was pushed to us, it is possible for\n                # it to become the only forward-extremity in the room, and we would then\n                # trust its state to be the state for the whole room. This is very bad.\n                # Further, if the event was pushed to us, there is no excuse for us not to\n                # have all the prev_events. We therefore reject any such events.\n                #\n                # XXX this really feels like it could/should be merged with the above,\n                # but there is an interaction with min_depth that I'm not really\n                # following.\n\n                if sent_to_us_directly:\n                    logger.warning(\n                        \"[%s %s] Rejecting: failed to fetch %d prev events: %s\",\n                        room_id,\n                        event_id,\n                        len(prevs - seen),\n                        shortstr(prevs - seen),\n                    )\n                    raise FederationError(\n                        \"ERROR\",\n                        403,\n                        (\n                            \"Your server isn't divulging details about prev_events \"\n                            \"referenced in this event.\"\n                        ),\n                        affected=pdu.event_id,\n                    )\n\n                logger.info(\n                    \"Event %s is missing prev_events: calculating state for a \"\n                    \"backwards extremity\",\n                    event_id,\n                )\n\n                # Calculate the state after each of the previous events, and\n                # resolve them to find the correct state at the current event.\n                event_map = {event_id: pdu}\n                try:\n                    # Get the state of the events we know about\n                    ours = await self.state_store.get_state_groups_ids(room_id, seen)\n\n                    # state_maps is a list of mappings from (type, state_key) to event_id\n                    state_maps = list(ours.values())  # type: List[StateMap[str]]\n\n                    # we don't need this any more, let's delete it.\n                    del ours\n\n                    # Ask the remote server for the states we don't\n                    # know about\n                    for p in prevs - seen:\n                        logger.info(\n                            \"Requesting state at missing prev_event %s\", event_id,\n                        )\n\n                        with nested_logging_context(p):\n                            # note that if any of the missing prevs share missing state or\n                            # auth events, the requests to fetch those events are deduped\n                            # by the get_pdu_cache in federation_client.\n                            (remote_state, _,) = await self._get_state_for_room(\n                                origin, room_id, p, include_event_in_state=True\n                            )\n\n                            remote_state_map = {\n                                (x.type, x.state_key): x.event_id for x in remote_state\n                            }\n                            state_maps.append(remote_state_map)\n\n                            for x in remote_state:\n                                event_map[x.event_id] = x\n\n                    room_version = await self.store.get_room_version_id(room_id)\n                    state_map = await self._state_resolution_handler.resolve_events_with_store(\n                        room_id,\n                        room_version,\n                        state_maps,\n                        event_map,\n                        state_res_store=StateResolutionStore(self.store),\n                    )\n\n                    # We need to give _process_received_pdu the actual state events\n                    # rather than event ids, so generate that now.\n\n                    # First though we need to fetch all the events that are in\n                    # state_map, so we can build up the state below.\n                    evs = await self.store.get_events(\n                        list(state_map.values()),\n                        get_prev_content=False,\n                        redact_behaviour=EventRedactBehaviour.AS_IS,\n                    )\n                    event_map.update(evs)\n\n                    state = [event_map[e] for e in state_map.values()]\n                except Exception:\n                    logger.warning(\n                        \"[%s %s] Error attempting to resolve state at missing \"\n                        \"prev_events\",\n                        room_id,\n                        event_id,\n                        exc_info=True,\n                    )\n                    raise FederationError(\n                        \"ERROR\",\n                        403,\n                        \"We can't get valid state history.\",\n                        affected=event_id,\n                    )\n\n        await self._process_received_pdu(origin, pdu, state=state)\n\n    async def _get_missing_events_for_pdu(self, origin, pdu, prevs, min_depth):\n        \"\"\"\n        Args:\n            origin (str): Origin of the pdu. Will be called to get the missing events\n            pdu: received pdu\n            prevs (set(str)): List of event ids which we are missing\n            min_depth (int): Minimum depth of events to return.\n        \"\"\"\n\n        room_id = pdu.room_id\n        event_id = pdu.event_id\n\n        seen = await self.store.have_events_in_timeline(prevs)\n\n        if not prevs - seen:\n            return\n\n        latest_list = await self.store.get_latest_event_ids_in_room(room_id)\n\n        # We add the prev events that we have seen to the latest\n        # list to ensure the remote server doesn't give them to us\n        latest = set(latest_list)\n        latest |= seen\n\n        logger.info(\n            \"[%s %s]: Requesting missing events between %s and %s\",\n            room_id,\n            event_id,\n            shortstr(latest),\n            event_id,\n        )\n\n        # XXX: we set timeout to 10s to help workaround\n        # https://github.com/matrix-org/synapse/issues/1733.\n        # The reason is to avoid holding the linearizer lock\n        # whilst processing inbound /send transactions, causing\n        # FDs to stack up and block other inbound transactions\n        # which empirically can currently take up to 30 minutes.\n        #\n        # N.B. this explicitly disables retry attempts.\n        #\n        # N.B. this also increases our chances of falling back to\n        # fetching fresh state for the room if the missing event\n        # can't be found, which slightly reduces our security.\n        # it may also increase our DAG extremity count for the room,\n        # causing additional state resolution?  See #1760.\n        # However, fetching state doesn't hold the linearizer lock\n        # apparently.\n        #\n        # see https://github.com/matrix-org/synapse/pull/1744\n        #\n        # ----\n        #\n        # Update richvdh 2018/09/18: There are a number of problems with timing this\n        # request out aggressively on the client side:\n        #\n        # - it plays badly with the server-side rate-limiter, which starts tarpitting you\n        #   if you send too many requests at once, so you end up with the server carefully\n        #   working through the backlog of your requests, which you have already timed\n        #   out.\n        #\n        # - for this request in particular, we now (as of\n        #   https://github.com/matrix-org/synapse/pull/3456) reject any PDUs where the\n        #   server can't produce a plausible-looking set of prev_events - so we becone\n        #   much more likely to reject the event.\n        #\n        # - contrary to what it says above, we do *not* fall back to fetching fresh state\n        #   for the room if get_missing_events times out. Rather, we give up processing\n        #   the PDU whose prevs we are missing, which then makes it much more likely that\n        #   we'll end up back here for the *next* PDU in the list, which exacerbates the\n        #   problem.\n        #\n        # - the aggressive 10s timeout was introduced to deal with incoming federation\n        #   requests taking 8 hours to process. It's not entirely clear why that was going\n        #   on; certainly there were other issues causing traffic storms which are now\n        #   resolved, and I think in any case we may be more sensible about our locking\n        #   now. We're *certainly* more sensible about our logging.\n        #\n        # All that said: Let's try increasing the timeout to 60s and see what happens.\n\n        try:\n            missing_events = await self.federation_client.get_missing_events(\n                origin,\n                room_id,\n                earliest_events_ids=list(latest),\n                latest_events=[pdu],\n                limit=10,\n                min_depth=min_depth,\n                timeout=60000,\n            )\n        except (RequestSendFailed, HttpResponseException, NotRetryingDestination) as e:\n            # We failed to get the missing events, but since we need to handle\n            # the case of `get_missing_events` not returning the necessary\n            # events anyway, it is safe to simply log the error and continue.\n            logger.warning(\n                \"[%s %s]: Failed to get prev_events: %s\", room_id, event_id, e\n            )\n            return\n\n        logger.info(\n            \"[%s %s]: Got %d prev_events: %s\",\n            room_id,\n            event_id,\n            len(missing_events),\n            shortstr(missing_events),\n        )\n\n        # We want to sort these by depth so we process them and\n        # tell clients about them in order.\n        missing_events.sort(key=lambda x: x.depth)\n\n        for ev in missing_events:\n            logger.info(\n                \"[%s %s] Handling received prev_event %s\",\n                room_id,\n                event_id,\n                ev.event_id,\n            )\n            with nested_logging_context(ev.event_id):\n                try:\n                    await self.on_receive_pdu(origin, ev, sent_to_us_directly=False)\n                except FederationError as e:\n                    if e.code == 403:\n                        logger.warning(\n                            \"[%s %s] Received prev_event %s failed history check.\",\n                            room_id,\n                            event_id,\n                            ev.event_id,\n                        )\n                    else:\n                        raise\n\n    async def _get_state_for_room(\n        self,\n        destination: str,\n        room_id: str,\n        event_id: str,\n        include_event_in_state: bool = False,\n    ) -> Tuple[List[EventBase], List[EventBase]]:\n        \"\"\"Requests all of the room state at a given event from a remote homeserver.\n\n        Args:\n            destination: The remote homeserver to query for the state.\n            room_id: The id of the room we're interested in.\n            event_id: The id of the event we want the state at.\n            include_event_in_state: if true, the event itself will be included in the\n                returned state event list.\n\n        Returns:\n            A list of events in the state, possibly including the event itself, and\n            a list of events in the auth chain for the given event.\n        \"\"\"\n        (\n            state_event_ids,\n            auth_event_ids,\n        ) = await self.federation_client.get_room_state_ids(\n            destination, room_id, event_id=event_id\n        )\n\n        desired_events = set(state_event_ids + auth_event_ids)\n\n        if include_event_in_state:\n            desired_events.add(event_id)\n\n        event_map = await self._get_events_from_store_or_dest(\n            destination, room_id, desired_events\n        )\n\n        failed_to_fetch = desired_events - event_map.keys()\n        if failed_to_fetch:\n            logger.warning(\n                \"Failed to fetch missing state/auth events for %s %s\",\n                event_id,\n                failed_to_fetch,\n            )\n\n        remote_state = [\n            event_map[e_id] for e_id in state_event_ids if e_id in event_map\n        ]\n\n        if include_event_in_state:\n            remote_event = event_map.get(event_id)\n            if not remote_event:\n                raise Exception(\"Unable to get missing prev_event %s\" % (event_id,))\n            if remote_event.is_state() and remote_event.rejected_reason is None:\n                remote_state.append(remote_event)\n\n        auth_chain = [event_map[e_id] for e_id in auth_event_ids if e_id in event_map]\n        auth_chain.sort(key=lambda e: e.depth)\n\n        return remote_state, auth_chain\n\n    async def _get_events_from_store_or_dest(\n        self, destination: str, room_id: str, event_ids: Iterable[str]\n    ) -> Dict[str, EventBase]:\n        \"\"\"Fetch events from a remote destination, checking if we already have them.\n\n        Persists any events we don't already have as outliers.\n\n        If we fail to fetch any of the events, a warning will be logged, and the event\n        will be omitted from the result. Likewise, any events which turn out not to\n        be in the given room.\n\n        This function *does not* automatically get missing auth events of the\n        newly fetched events. Callers must include the full auth chain of\n        of the missing events in the `event_ids` argument, to ensure that any\n        missing auth events are correctly fetched.\n\n        Returns:\n            map from event_id to event\n        \"\"\"\n        fetched_events = await self.store.get_events(event_ids, allow_rejected=True)\n\n        missing_events = set(event_ids) - fetched_events.keys()\n\n        if missing_events:\n            logger.debug(\n                \"Fetching unknown state/auth events %s for room %s\",\n                missing_events,\n                room_id,\n            )\n\n            await self._get_events_and_persist(\n                destination=destination, room_id=room_id, events=missing_events\n            )\n\n            # we need to make sure we re-load from the database to get the rejected\n            # state correct.\n            fetched_events.update(\n                (await self.store.get_events(missing_events, allow_rejected=True))\n            )\n\n        # check for events which were in the wrong room.\n        #\n        # this can happen if a remote server claims that the state or\n        # auth_events at an event in room A are actually events in room B\n\n        bad_events = [\n            (event_id, event.room_id)\n            for event_id, event in fetched_events.items()\n            if event.room_id != room_id\n        ]\n\n        for bad_event_id, bad_room_id in bad_events:\n            # This is a bogus situation, but since we may only discover it a long time\n            # after it happened, we try our best to carry on, by just omitting the\n            # bad events from the returned auth/state set.\n            logger.warning(\n                \"Remote server %s claims event %s in room %s is an auth/state \"\n                \"event in room %s\",\n                destination,\n                bad_event_id,\n                bad_room_id,\n                room_id,\n            )\n\n            del fetched_events[bad_event_id]\n\n        return fetched_events\n\n    async def _process_received_pdu(\n        self, origin: str, event: EventBase, state: Optional[Iterable[EventBase]],\n    ):\n        \"\"\" Called when we have a new pdu. We need to do auth checks and put it\n        through the StateHandler.\n\n        Args:\n            origin: server sending the event\n\n            event: event to be persisted\n\n            state: Normally None, but if we are handling a gap in the graph\n                (ie, we are missing one or more prev_events), the resolved state at the\n                event\n        \"\"\"\n        room_id = event.room_id\n        event_id = event.event_id\n\n        logger.debug(\"[%s %s] Processing event: %s\", room_id, event_id, event)\n\n        try:\n            await self._handle_new_event(origin, event, state=state)\n        except AuthError as e:\n            raise FederationError(\"ERROR\", e.code, e.msg, affected=event.event_id)\n\n        # For encrypted messages we check that we know about the sending device,\n        # if we don't then we mark the device cache for that user as stale.\n        if event.type == EventTypes.Encrypted:\n            device_id = event.content.get(\"device_id\")\n            sender_key = event.content.get(\"sender_key\")\n\n            cached_devices = await self.store.get_cached_devices_for_user(event.sender)\n\n            resync = False  # Whether we should resync device lists.\n\n            device = None\n            if device_id is not None:\n                device = cached_devices.get(device_id)\n                if device is None:\n                    logger.info(\n                        \"Received event from remote device not in our cache: %s %s\",\n                        event.sender,\n                        device_id,\n                    )\n                    resync = True\n\n            # We also check if the `sender_key` matches what we expect.\n            if sender_key is not None:\n                # Figure out what sender key we're expecting. If we know the\n                # device and recognize the algorithm then we can work out the\n                # exact key to expect. Otherwise check it matches any key we\n                # have for that device.\n\n                current_keys = []  # type: Container[str]\n\n                if device:\n                    keys = device.get(\"keys\", {}).get(\"keys\", {})\n\n                    if (\n                        event.content.get(\"algorithm\")\n                        == RoomEncryptionAlgorithms.MEGOLM_V1_AES_SHA2\n                    ):\n                        # For this algorithm we expect a curve25519 key.\n                        key_name = \"curve25519:%s\" % (device_id,)\n                        current_keys = [keys.get(key_name)]\n                    else:\n                        # We don't know understand the algorithm, so we just\n                        # check it matches a key for the device.\n                        current_keys = keys.values()\n                elif device_id:\n                    # We don't have any keys for the device ID.\n                    pass\n                else:\n                    # The event didn't include a device ID, so we just look for\n                    # keys across all devices.\n                    current_keys = [\n                        key\n                        for device in cached_devices.values()\n                        for key in device.get(\"keys\", {}).get(\"keys\", {}).values()\n                    ]\n\n                # We now check that the sender key matches (one of) the expected\n                # keys.\n                if sender_key not in current_keys:\n                    logger.info(\n                        \"Received event from remote device with unexpected sender key: %s %s: %s\",\n                        event.sender,\n                        device_id or \"<no device_id>\",\n                        sender_key,\n                    )\n                    resync = True\n\n            if resync:\n                run_as_background_process(\n                    \"resync_device_due_to_pdu\", self._resync_device, event.sender\n                )\n\n    async def _resync_device(self, sender: str) -> None:\n        \"\"\"We have detected that the device list for the given user may be out\n        of sync, so we try and resync them.\n        \"\"\"\n\n        try:\n            await self.store.mark_remote_user_device_cache_as_stale(sender)\n\n            # Immediately attempt a resync in the background\n            if self.config.worker_app:\n                await self._user_device_resync(user_id=sender)\n            else:\n                await self._device_list_updater.user_device_resync(sender)\n        except Exception:\n            logger.exception(\"Failed to resync device for %s\", sender)\n\n    @log_function\n    async def backfill(self, dest, room_id, limit, extremities):\n        \"\"\" Trigger a backfill request to `dest` for the given `room_id`\n\n        This will attempt to get more events from the remote. If the other side\n        has no new events to offer, this will return an empty list.\n\n        As the events are received, we check their signatures, and also do some\n        sanity-checking on them. If any of the backfilled events are invalid,\n        this method throws a SynapseError.\n\n        TODO: make this more useful to distinguish failures of the remote\n        server from invalid events (there is probably no point in trying to\n        re-fetch invalid events from every other HS in the room.)\n        \"\"\"\n        if dest == self.server_name:\n            raise SynapseError(400, \"Can't backfill from self.\")\n\n        events = await self.federation_client.backfill(\n            dest, room_id, limit=limit, extremities=extremities\n        )\n\n        if not events:\n            return []\n\n        # ideally we'd sanity check the events here for excess prev_events etc,\n        # but it's hard to reject events at this point without completely\n        # breaking backfill in the same way that it is currently broken by\n        # events whose signature we cannot verify (#3121).\n        #\n        # So for now we accept the events anyway. #3124 tracks this.\n        #\n        # for ev in events:\n        #     self._sanity_check_event(ev)\n\n        # Don't bother processing events we already have.\n        seen_events = await self.store.have_events_in_timeline(\n            {e.event_id for e in events}\n        )\n\n        events = [e for e in events if e.event_id not in seen_events]\n\n        if not events:\n            return []\n\n        event_map = {e.event_id: e for e in events}\n\n        event_ids = {e.event_id for e in events}\n\n        # build a list of events whose prev_events weren't in the batch.\n        # (XXX: this will include events whose prev_events we already have; that doesn't\n        # sound right?)\n        edges = [ev.event_id for ev in events if set(ev.prev_event_ids()) - event_ids]\n\n        logger.info(\"backfill: Got %d events with %d edges\", len(events), len(edges))\n\n        # For each edge get the current state.\n\n        auth_events = {}\n        state_events = {}\n        events_to_state = {}\n        for e_id in edges:\n            state, auth = await self._get_state_for_room(\n                destination=dest,\n                room_id=room_id,\n                event_id=e_id,\n                include_event_in_state=False,\n            )\n            auth_events.update({a.event_id: a for a in auth})\n            auth_events.update({s.event_id: s for s in state})\n            state_events.update({s.event_id: s for s in state})\n            events_to_state[e_id] = state\n\n        required_auth = {\n            a_id\n            for event in events\n            + list(state_events.values())\n            + list(auth_events.values())\n            for a_id in event.auth_event_ids()\n        }\n        auth_events.update(\n            {e_id: event_map[e_id] for e_id in required_auth if e_id in event_map}\n        )\n\n        ev_infos = []\n\n        # Step 1: persist the events in the chunk we fetched state for (i.e.\n        # the backwards extremities), with custom auth events and state\n        for e_id in events_to_state:\n            # For paranoia we ensure that these events are marked as\n            # non-outliers\n            ev = event_map[e_id]\n            assert not ev.internal_metadata.is_outlier()\n\n            ev_infos.append(\n                _NewEventInfo(\n                    event=ev,\n                    state=events_to_state[e_id],\n                    auth_events={\n                        (\n                            auth_events[a_id].type,\n                            auth_events[a_id].state_key,\n                        ): auth_events[a_id]\n                        for a_id in ev.auth_event_ids()\n                        if a_id in auth_events\n                    },\n                )\n            )\n\n        if ev_infos:\n            await self._handle_new_events(dest, room_id, ev_infos, backfilled=True)\n\n        # Step 2: Persist the rest of the events in the chunk one by one\n        events.sort(key=lambda e: e.depth)\n\n        for event in events:\n            if event in events_to_state:\n                continue\n\n            # For paranoia we ensure that these events are marked as\n            # non-outliers\n            assert not event.internal_metadata.is_outlier()\n\n            # We store these one at a time since each event depends on the\n            # previous to work out the state.\n            # TODO: We can probably do something more clever here.\n            await self._handle_new_event(dest, event, backfilled=True)\n\n        return events\n\n    async def maybe_backfill(\n        self, room_id: str, current_depth: int, limit: int\n    ) -> bool:\n        \"\"\"Checks the database to see if we should backfill before paginating,\n        and if so do.\n\n        Args:\n            room_id\n            current_depth: The depth from which we're paginating from. This is\n                used to decide if we should backfill and what extremities to\n                use.\n            limit: The number of events that the pagination request will\n                return. This is used as part of the heuristic to decide if we\n                should back paginate.\n        \"\"\"\n        extremities = await self.store.get_oldest_events_with_depth_in_room(room_id)\n\n        if not extremities:\n            logger.debug(\"Not backfilling as no extremeties found.\")\n            return False\n\n        # We only want to paginate if we can actually see the events we'll get,\n        # as otherwise we'll just spend a lot of resources to get redacted\n        # events.\n        #\n        # We do this by filtering all the backwards extremities and seeing if\n        # any remain. Given we don't have the extremity events themselves, we\n        # need to actually check the events that reference them.\n        #\n        # *Note*: the spec wants us to keep backfilling until we reach the start\n        # of the room in case we are allowed to see some of the history. However\n        # in practice that causes more issues than its worth, as a) its\n        # relatively rare for there to be any visible history and b) even when\n        # there is its often sufficiently long ago that clients would stop\n        # attempting to paginate before backfill reached the visible history.\n        #\n        # TODO: If we do do a backfill then we should filter the backwards\n        #   extremities to only include those that point to visible portions of\n        #   history.\n        #\n        # TODO: Correctly handle the case where we are allowed to see the\n        #   forward event but not the backward extremity, e.g. in the case of\n        #   initial join of the server where we are allowed to see the join\n        #   event but not anything before it. This would require looking at the\n        #   state *before* the event, ignoring the special casing certain event\n        #   types have.\n\n        forward_events = await self.store.get_successor_events(list(extremities))\n\n        extremities_events = await self.store.get_events(\n            forward_events,\n            redact_behaviour=EventRedactBehaviour.AS_IS,\n            get_prev_content=False,\n        )\n\n        # We set `check_history_visibility_only` as we might otherwise get false\n        # positives from users having been erased.\n        filtered_extremities = await filter_events_for_server(\n            self.storage,\n            self.server_name,\n            list(extremities_events.values()),\n            redact=False,\n            check_history_visibility_only=True,\n        )\n\n        if not filtered_extremities:\n            return False\n\n        # Check if we reached a point where we should start backfilling.\n        sorted_extremeties_tuple = sorted(extremities.items(), key=lambda e: -int(e[1]))\n        max_depth = sorted_extremeties_tuple[0][1]\n\n        # If we're approaching an extremity we trigger a backfill, otherwise we\n        # no-op.\n        #\n        # We chose twice the limit here as then clients paginating backwards\n        # will send pagination requests that trigger backfill at least twice\n        # using the most recent extremity before it gets removed (see below). We\n        # chose more than one times the limit in case of failure, but choosing a\n        # much larger factor will result in triggering a backfill request much\n        # earlier than necessary.\n        if current_depth - 2 * limit > max_depth:\n            logger.debug(\n                \"Not backfilling as we don't need to. %d < %d - 2 * %d\",\n                max_depth,\n                current_depth,\n                limit,\n            )\n            return False\n\n        logger.debug(\n            \"room_id: %s, backfill: current_depth: %s, max_depth: %s, extrems: %s\",\n            room_id,\n            current_depth,\n            max_depth,\n            sorted_extremeties_tuple,\n        )\n\n        # We ignore extremities that have a greater depth than our current depth\n        # as:\n        #    1. we don't really care about getting events that have happened\n        #       before our current position; and\n        #    2. we have likely previously tried and failed to backfill from that\n        #       extremity, so to avoid getting \"stuck\" requesting the same\n        #       backfill repeatedly we drop those extremities.\n        filtered_sorted_extremeties_tuple = [\n            t for t in sorted_extremeties_tuple if int(t[1]) <= current_depth\n        ]\n\n        # However, we need to check that the filtered extremities are non-empty.\n        # If they are empty then either we can a) bail or b) still attempt to\n        # backill. We opt to try backfilling anyway just in case we do get\n        # relevant events.\n        if filtered_sorted_extremeties_tuple:\n            sorted_extremeties_tuple = filtered_sorted_extremeties_tuple\n\n        # We don't want to specify too many extremities as it causes the backfill\n        # request URI to be too long.\n        extremities = dict(sorted_extremeties_tuple[:5])\n\n        # Now we need to decide which hosts to hit first.\n\n        # First we try hosts that are already in the room\n        # TODO: HEURISTIC ALERT.\n\n        curr_state = await self.state_handler.get_current_state(room_id)\n\n        def get_domains_from_state(state):\n            \"\"\"Get joined domains from state\n\n            Args:\n                state (dict[tuple, FrozenEvent]): State map from type/state\n                    key to event.\n\n            Returns:\n                list[tuple[str, int]]: Returns a list of servers with the\n                lowest depth of their joins. Sorted by lowest depth first.\n            \"\"\"\n            joined_users = [\n                (state_key, int(event.depth))\n                for (e_type, state_key), event in state.items()\n                if e_type == EventTypes.Member and event.membership == Membership.JOIN\n            ]\n\n            joined_domains = {}  # type: Dict[str, int]\n            for u, d in joined_users:\n                try:\n                    dom = get_domain_from_id(u)\n                    old_d = joined_domains.get(dom)\n                    if old_d:\n                        joined_domains[dom] = min(d, old_d)\n                    else:\n                        joined_domains[dom] = d\n                except Exception:\n                    pass\n\n            return sorted(joined_domains.items(), key=lambda d: d[1])\n\n        curr_domains = get_domains_from_state(curr_state)\n\n        likely_domains = [\n            domain for domain, depth in curr_domains if domain != self.server_name\n        ]\n\n        async def try_backfill(domains):\n            # TODO: Should we try multiple of these at a time?\n            for dom in domains:\n                try:\n                    await self.backfill(\n                        dom, room_id, limit=100, extremities=extremities\n                    )\n                    # If this succeeded then we probably already have the\n                    # appropriate stuff.\n                    # TODO: We can probably do something more intelligent here.\n                    return True\n                except SynapseError as e:\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except HttpResponseException as e:\n                    if 400 <= e.code < 500:\n                        raise e.to_synapse_error()\n\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except CodeMessageException as e:\n                    if 400 <= e.code < 500:\n                        raise\n\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except NotRetryingDestination as e:\n                    logger.info(str(e))\n                    continue\n                except RequestSendFailed as e:\n                    logger.info(\"Failed to get backfill from %s because %s\", dom, e)\n                    continue\n                except FederationDeniedError as e:\n                    logger.info(e)\n                    continue\n                except Exception as e:\n                    logger.exception(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n\n            return False\n\n        success = await try_backfill(likely_domains)\n        if success:\n            return True\n\n        # Huh, well *those* domains didn't work out. Lets try some domains\n        # from the time.\n\n        tried_domains = set(likely_domains)\n        tried_domains.add(self.server_name)\n\n        event_ids = list(extremities.keys())\n\n        logger.debug(\"calling resolve_state_groups in _maybe_backfill\")\n        resolve = preserve_fn(self.state_handler.resolve_state_groups_for_events)\n        states = await make_deferred_yieldable(\n            defer.gatherResults(\n                [resolve(room_id, [e]) for e in event_ids], consumeErrors=True\n            )\n        )\n\n        # dict[str, dict[tuple, str]], a map from event_id to state map of\n        # event_ids.\n        states = dict(zip(event_ids, [s.state for s in states]))\n\n        state_map = await self.store.get_events(\n            [e_id for ids in states.values() for e_id in ids.values()],\n            get_prev_content=False,\n        )\n        states = {\n            key: {\n                k: state_map[e_id]\n                for k, e_id in state_dict.items()\n                if e_id in state_map\n            }\n            for key, state_dict in states.items()\n        }\n\n        for e_id, _ in sorted_extremeties_tuple:\n            likely_domains = get_domains_from_state(states[e_id])\n\n            success = await try_backfill(\n                [dom for dom, _ in likely_domains if dom not in tried_domains]\n            )\n            if success:\n                return True\n\n            tried_domains.update(dom for dom, _ in likely_domains)\n\n        return False\n\n    async def _get_events_and_persist(\n        self, destination: str, room_id: str, events: Iterable[str]\n    ):\n        \"\"\"Fetch the given events from a server, and persist them as outliers.\n\n        This function *does not* recursively get missing auth events of the\n        newly fetched events. Callers must include in the `events` argument\n        any missing events from the auth chain.\n\n        Logs a warning if we can't find the given event.\n        \"\"\"\n\n        room_version = await self.store.get_room_version(room_id)\n\n        event_map = {}  # type: Dict[str, EventBase]\n\n        async def get_event(event_id: str):\n            with nested_logging_context(event_id):\n                try:\n                    event = await self.federation_client.get_pdu(\n                        [destination], event_id, room_version, outlier=True,\n                    )\n                    if event is None:\n                        logger.warning(\n                            \"Server %s didn't return event %s\", destination, event_id,\n                        )\n                        return\n\n                    event_map[event.event_id] = event\n\n                except Exception as e:\n                    logger.warning(\n                        \"Error fetching missing state/auth event %s: %s %s\",\n                        event_id,\n                        type(e),\n                        e,\n                    )\n\n        await concurrently_execute(get_event, events, 5)\n\n        # Make a map of auth events for each event. We do this after fetching\n        # all the events as some of the events' auth events will be in the list\n        # of requested events.\n\n        auth_events = [\n            aid\n            for event in event_map.values()\n            for aid in event.auth_event_ids()\n            if aid not in event_map\n        ]\n        persisted_events = await self.store.get_events(\n            auth_events, allow_rejected=True,\n        )\n\n        event_infos = []\n        for event in event_map.values():\n            auth = {}\n            for auth_event_id in event.auth_event_ids():\n                ae = persisted_events.get(auth_event_id) or event_map.get(auth_event_id)\n                if ae:\n                    auth[(ae.type, ae.state_key)] = ae\n                else:\n                    logger.info(\"Missing auth event %s\", auth_event_id)\n\n            event_infos.append(_NewEventInfo(event, None, auth))\n\n        await self._handle_new_events(\n            destination, room_id, event_infos,\n        )\n\n    def _sanity_check_event(self, ev):\n        \"\"\"\n        Do some early sanity checks of a received event\n\n        In particular, checks it doesn't have an excessive number of\n        prev_events or auth_events, which could cause a huge state resolution\n        or cascade of event fetches.\n\n        Args:\n            ev (synapse.events.EventBase): event to be checked\n\n        Returns: None\n\n        Raises:\n            SynapseError if the event does not pass muster\n        \"\"\"\n        if len(ev.prev_event_ids()) > 20:\n            logger.warning(\n                \"Rejecting event %s which has %i prev_events\",\n                ev.event_id,\n                len(ev.prev_event_ids()),\n            )\n            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many prev_events\")\n\n        if len(ev.auth_event_ids()) > 10:\n            logger.warning(\n                \"Rejecting event %s which has %i auth_events\",\n                ev.event_id,\n                len(ev.auth_event_ids()),\n            )\n            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many auth_events\")\n\n    async def send_invite(self, target_host, event):\n        \"\"\" Sends the invite to the remote server for signing.\n\n        Invites must be signed by the invitee's server before distribution.\n        \"\"\"\n        pdu = await self.federation_client.send_invite(\n            destination=target_host,\n            room_id=event.room_id,\n            event_id=event.event_id,\n            pdu=event,\n        )\n\n        return pdu\n\n    async def on_event_auth(self, event_id: str) -> List[EventBase]:\n        event = await self.store.get_event(event_id)\n        auth = await self.store.get_auth_chain(\n            list(event.auth_event_ids()), include_given=True\n        )\n        return list(auth)\n\n    async def do_invite_join(\n        self, target_hosts: Iterable[str], room_id: str, joinee: str, content: JsonDict\n    ) -> Tuple[str, int]:\n        \"\"\" Attempts to join the `joinee` to the room `room_id` via the\n        servers contained in `target_hosts`.\n\n        This first triggers a /make_join/ request that returns a partial\n        event that we can fill out and sign. This is then sent to the\n        remote server via /send_join/ which responds with the state at that\n        event and the auth_chains.\n\n        We suspend processing of any received events from this room until we\n        have finished processing the join.\n\n        Args:\n            target_hosts: List of servers to attempt to join the room with.\n\n            room_id: The ID of the room to join.\n\n            joinee: The User ID of the joining user.\n\n            content: The event content to use for the join event.\n        \"\"\"\n        # TODO: We should be able to call this on workers, but the upgrading of\n        # room stuff after join currently doesn't work on workers.\n        assert self.config.worker.worker_app is None\n\n        logger.debug(\"Joining %s to %s\", joinee, room_id)\n\n        origin, event, room_version_obj = await self._make_and_verify_event(\n            target_hosts,\n            room_id,\n            joinee,\n            \"join\",\n            content,\n            params={\"ver\": KNOWN_ROOM_VERSIONS},\n        )\n\n        # This shouldn't happen, because the RoomMemberHandler has a\n        # linearizer lock which only allows one operation per user per room\n        # at a time - so this is just paranoia.\n        assert room_id not in self.room_queues\n\n        self.room_queues[room_id] = []\n\n        await self._clean_room_for_join(room_id)\n\n        handled_events = set()\n\n        try:\n            # Try the host we successfully got a response to /make_join/\n            # request first.\n            host_list = list(target_hosts)\n            try:\n                host_list.remove(origin)\n                host_list.insert(0, origin)\n            except ValueError:\n                pass\n\n            ret = await self.federation_client.send_join(\n                host_list, event, room_version_obj\n            )\n\n            origin = ret[\"origin\"]\n            state = ret[\"state\"]\n            auth_chain = ret[\"auth_chain\"]\n            auth_chain.sort(key=lambda e: e.depth)\n\n            handled_events.update([s.event_id for s in state])\n            handled_events.update([a.event_id for a in auth_chain])\n            handled_events.add(event.event_id)\n\n            logger.debug(\"do_invite_join auth_chain: %s\", auth_chain)\n            logger.debug(\"do_invite_join state: %s\", state)\n\n            logger.debug(\"do_invite_join event: %s\", event)\n\n            # if this is the first time we've joined this room, it's time to add\n            # a row to `rooms` with the correct room version. If there's already a\n            # row there, we should override it, since it may have been populated\n            # based on an invite request which lied about the room version.\n            #\n            # federation_client.send_join has already checked that the room\n            # version in the received create event is the same as room_version_obj,\n            # so we can rely on it now.\n            #\n            await self.store.upsert_room_on_join(\n                room_id=room_id, room_version=room_version_obj,\n            )\n\n            max_stream_id = await self._persist_auth_tree(\n                origin, room_id, auth_chain, state, event, room_version_obj\n            )\n\n            # We wait here until this instance has seen the events come down\n            # replication (if we're using replication) as the below uses caches.\n            await self._replication.wait_for_stream_position(\n                self.config.worker.events_shard_config.get_instance(room_id),\n                \"events\",\n                max_stream_id,\n            )\n\n            # Check whether this room is the result of an upgrade of a room we already know\n            # about. If so, migrate over user information\n            predecessor = await self.store.get_room_predecessor(room_id)\n            if not predecessor or not isinstance(predecessor.get(\"room_id\"), str):\n                return event.event_id, max_stream_id\n            old_room_id = predecessor[\"room_id\"]\n            logger.debug(\n                \"Found predecessor for %s during remote join: %s\", room_id, old_room_id\n            )\n\n            # We retrieve the room member handler here as to not cause a cyclic dependency\n            member_handler = self.hs.get_room_member_handler()\n            await member_handler.transfer_room_state_on_room_upgrade(\n                old_room_id, room_id\n            )\n\n            logger.debug(\"Finished joining %s to %s\", joinee, room_id)\n            return event.event_id, max_stream_id\n        finally:\n            room_queue = self.room_queues[room_id]\n            del self.room_queues[room_id]\n\n            # we don't need to wait for the queued events to be processed -\n            # it's just a best-effort thing at this point. We do want to do\n            # them roughly in order, though, otherwise we'll end up making\n            # lots of requests for missing prev_events which we do actually\n            # have. Hence we fire off the background task, but don't wait for it.\n\n            run_in_background(self._handle_queued_pdus, room_queue)\n\n    async def _handle_queued_pdus(self, room_queue):\n        \"\"\"Process PDUs which got queued up while we were busy send_joining.\n\n        Args:\n            room_queue (list[FrozenEvent, str]): list of PDUs to be processed\n                and the servers that sent them\n        \"\"\"\n        for p, origin in room_queue:\n            try:\n                logger.info(\n                    \"Processing queued PDU %s which was received \"\n                    \"while we were joining %s\",\n                    p.event_id,\n                    p.room_id,\n                )\n                with nested_logging_context(p.event_id):\n                    await self.on_receive_pdu(origin, p, sent_to_us_directly=True)\n            except Exception as e:\n                logger.warning(\n                    \"Error handling queued PDU %s from %s: %s\", p.event_id, origin, e\n                )\n\n    async def on_make_join_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> EventBase:\n        \"\"\" We've received a /make_join/ request, so we create a partial\n        join event for the room and return that. We do *not* persist or\n        process it until the other server has signed it and sent it back.\n\n        Args:\n            origin: The (verified) server name of the requesting server.\n            room_id: Room to create join event in\n            user_id: The user to create the join for\n        \"\"\"\n        if get_domain_from_id(user_id) != origin:\n            logger.info(\n                \"Got /make_join request for user %r from different origin %s, ignoring\",\n                user_id,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        # checking the room version will check that we've actually heard of the room\n        # (and return a 404 otherwise)\n        room_version = await self.store.get_room_version_id(room_id)\n\n        # now check that we are *still* in the room\n        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)\n        if not is_in_room:\n            logger.info(\n                \"Got /make_join request for room %s we are no longer in\", room_id,\n            )\n            raise NotFoundError(\"Not an active room on this server\")\n\n        event_content = {\"membership\": Membership.JOIN}\n\n        builder = self.event_builder_factory.new(\n            room_version,\n            {\n                \"type\": EventTypes.Member,\n                \"content\": event_content,\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": user_id,\n            },\n        )\n\n        try:\n            event, context = await self.event_creation_handler.create_new_client_event(\n                builder=builder\n            )\n        except SynapseError as e:\n            logger.warning(\"Failed to create join to %s because %s\", room_id, e)\n            raise\n\n        # The remote hasn't signed it yet, obviously. We'll do the full checks\n        # when we get the event back in `on_send_join_request`\n        await self.auth.check_from_context(\n            room_version, event, context, do_sig_check=False\n        )\n\n        return event\n\n    async def on_send_join_request(self, origin, pdu):\n        \"\"\" We have received a join event for a room. Fully process it and\n        respond with the current state and auth chains.\n        \"\"\"\n        event = pdu\n\n        logger.debug(\n            \"on_send_join_request from %s: Got event: %s, signatures: %s\",\n            origin,\n            event.event_id,\n            event.signatures,\n        )\n\n        if get_domain_from_id(event.sender) != origin:\n            logger.info(\n                \"Got /send_join request for user %r from different origin %s\",\n                event.sender,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        event.internal_metadata.outlier = False\n        # Send this event on behalf of the origin server.\n        #\n        # The reasons we have the destination server rather than the origin\n        # server send it are slightly mysterious: the origin server should have\n        # all the necessary state once it gets the response to the send_join,\n        # so it could send the event itself if it wanted to. It may be that\n        # doing it this way reduces failure modes, or avoids certain attacks\n        # where a new server selectively tells a subset of the federation that\n        # it has joined.\n        #\n        # The fact is that, as of the current writing, Synapse doesn't send out\n        # the join event over federation after joining, and changing it now\n        # would introduce the danger of backwards-compatibility problems.\n        event.internal_metadata.send_on_behalf_of = origin\n\n        context = await self._handle_new_event(origin, event)\n\n        logger.debug(\n            \"on_send_join_request: After _handle_new_event: %s, sigs: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        prev_state_ids = await context.get_prev_state_ids()\n\n        state_ids = list(prev_state_ids.values())\n        auth_chain = await self.store.get_auth_chain(state_ids)\n\n        state = await self.store.get_events(list(prev_state_ids.values()))\n\n        return {\"state\": list(state.values()), \"auth_chain\": auth_chain}\n\n    async def on_invite_request(\n        self, origin: str, event: EventBase, room_version: RoomVersion\n    ):\n        \"\"\" We've got an invite event. Process and persist it. Sign it.\n\n        Respond with the now signed event.\n        \"\"\"\n        if event.state_key is None:\n            raise SynapseError(400, \"The invite event did not have a state key\")\n\n        is_blocked = await self.store.is_room_blocked(event.room_id)\n        if is_blocked:\n            raise SynapseError(403, \"This room has been blocked on this server\")\n\n        if self.hs.config.block_non_admin_invites:\n            raise SynapseError(403, \"This server does not accept room invites\")\n\n        if not self.spam_checker.user_may_invite(\n            event.sender, event.state_key, event.room_id\n        ):\n            raise SynapseError(\n                403, \"This user is not permitted to send invites to this server/user\"\n            )\n\n        membership = event.content.get(\"membership\")\n        if event.type != EventTypes.Member or membership != Membership.INVITE:\n            raise SynapseError(400, \"The event was not an m.room.member invite event\")\n\n        sender_domain = get_domain_from_id(event.sender)\n        if sender_domain != origin:\n            raise SynapseError(\n                400, \"The invite event was not from the server sending it\"\n            )\n\n        if not self.is_mine_id(event.state_key):\n            raise SynapseError(400, \"The invite event must be for this server\")\n\n        # block any attempts to invite the server notices mxid\n        if event.state_key == self._server_notices_mxid:\n            raise SynapseError(HTTPStatus.FORBIDDEN, \"Cannot invite this user\")\n\n        # keep a record of the room version, if we don't yet know it.\n        # (this may get overwritten if we later get a different room version in a\n        # join dance).\n        await self._maybe_store_room_on_outlier_membership(\n            room_id=event.room_id, room_version=room_version\n        )\n\n        event.internal_metadata.outlier = True\n        event.internal_metadata.out_of_band_membership = True\n\n        event.signatures.update(\n            compute_event_signature(\n                room_version,\n                event.get_pdu_json(),\n                self.hs.hostname,\n                self.hs.signing_key,\n            )\n        )\n\n        context = await self.state_handler.compute_event_context(event)\n        await self.persist_events_and_notify(event.room_id, [(event, context)])\n\n        return event\n\n    async def do_remotely_reject_invite(\n        self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict\n    ) -> Tuple[EventBase, int]:\n        origin, event, room_version = await self._make_and_verify_event(\n            target_hosts, room_id, user_id, \"leave\", content=content\n        )\n        # Mark as outlier as we don't have any state for this event; we're not\n        # even in the room.\n        event.internal_metadata.outlier = True\n        event.internal_metadata.out_of_band_membership = True\n\n        # Try the host that we successfully called /make_leave/ on first for\n        # the /send_leave/ request.\n        host_list = list(target_hosts)\n        try:\n            host_list.remove(origin)\n            host_list.insert(0, origin)\n        except ValueError:\n            pass\n\n        await self.federation_client.send_leave(host_list, event)\n\n        context = await self.state_handler.compute_event_context(event)\n        stream_id = await self.persist_events_and_notify(\n            event.room_id, [(event, context)]\n        )\n\n        return event, stream_id\n\n    async def _make_and_verify_event(\n        self,\n        target_hosts: Iterable[str],\n        room_id: str,\n        user_id: str,\n        membership: str,\n        content: JsonDict = {},\n        params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,\n    ) -> Tuple[str, EventBase, RoomVersion]:\n        (\n            origin,\n            event,\n            room_version,\n        ) = await self.federation_client.make_membership_event(\n            target_hosts, room_id, user_id, membership, content, params=params\n        )\n\n        logger.debug(\"Got response to make_%s: %s\", membership, event)\n\n        # We should assert some things.\n        # FIXME: Do this in a nicer way\n        assert event.type == EventTypes.Member\n        assert event.user_id == user_id\n        assert event.state_key == user_id\n        assert event.room_id == room_id\n        return origin, event, room_version\n\n    async def on_make_leave_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> EventBase:\n        \"\"\" We've received a /make_leave/ request, so we create a partial\n        leave event for the room and return that. We do *not* persist or\n        process it until the other server has signed it and sent it back.\n\n        Args:\n            origin: The (verified) server name of the requesting server.\n            room_id: Room to create leave event in\n            user_id: The user to create the leave for\n        \"\"\"\n        if get_domain_from_id(user_id) != origin:\n            logger.info(\n                \"Got /make_leave request for user %r from different origin %s, ignoring\",\n                user_id,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        room_version = await self.store.get_room_version_id(room_id)\n        builder = self.event_builder_factory.new(\n            room_version,\n            {\n                \"type\": EventTypes.Member,\n                \"content\": {\"membership\": Membership.LEAVE},\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": user_id,\n            },\n        )\n\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n\n        try:\n            # The remote hasn't signed it yet, obviously. We'll do the full checks\n            # when we get the event back in `on_send_leave_request`\n            await self.auth.check_from_context(\n                room_version, event, context, do_sig_check=False\n            )\n        except AuthError as e:\n            logger.warning(\"Failed to create new leave %r because %s\", event, e)\n            raise e\n\n        return event\n\n    async def on_send_leave_request(self, origin, pdu):\n        \"\"\" We have received a leave event for a room. Fully process it.\"\"\"\n        event = pdu\n\n        logger.debug(\n            \"on_send_leave_request: Got event: %s, signatures: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        if get_domain_from_id(event.sender) != origin:\n            logger.info(\n                \"Got /send_leave request for user %r from different origin %s\",\n                event.sender,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        event.internal_metadata.outlier = False\n\n        await self._handle_new_event(origin, event)\n\n        logger.debug(\n            \"on_send_leave_request: After _handle_new_event: %s, sigs: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        return None\n\n    async def get_state_for_pdu(self, room_id: str, event_id: str) -> List[EventBase]:\n        \"\"\"Returns the state at the event. i.e. not including said event.\n        \"\"\"\n\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        state_groups = await self.state_store.get_state_groups(room_id, [event_id])\n\n        if state_groups:\n            _, state = list(state_groups.items()).pop()\n            results = {(e.type, e.state_key): e for e in state}\n\n            if event.is_state():\n                # Get previous state\n                if \"replaces_state\" in event.unsigned:\n                    prev_id = event.unsigned[\"replaces_state\"]\n                    if prev_id != event.event_id:\n                        prev_event = await self.store.get_event(prev_id)\n                        results[(event.type, event.state_key)] = prev_event\n                else:\n                    del results[(event.type, event.state_key)]\n\n            res = list(results.values())\n            return res\n        else:\n            return []\n\n    async def get_state_ids_for_pdu(self, room_id: str, event_id: str) -> List[str]:\n        \"\"\"Returns the state at the event. i.e. not including said event.\n        \"\"\"\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        state_groups = await self.state_store.get_state_groups_ids(room_id, [event_id])\n\n        if state_groups:\n            _, state = list(state_groups.items()).pop()\n            results = state\n\n            if event.is_state():\n                # Get previous state\n                if \"replaces_state\" in event.unsigned:\n                    prev_id = event.unsigned[\"replaces_state\"]\n                    if prev_id != event.event_id:\n                        results[(event.type, event.state_key)] = prev_id\n                else:\n                    results.pop((event.type, event.state_key), None)\n\n            return list(results.values())\n        else:\n            return []\n\n    @log_function\n    async def on_backfill_request(\n        self, origin: str, room_id: str, pdu_list: List[str], limit: int\n    ) -> List[EventBase]:\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # Synapse asks for 100 events per backfill request. Do not allow more.\n        limit = min(limit, 100)\n\n        events = await self.store.get_backfill_events(room_id, pdu_list, limit)\n\n        events = await filter_events_for_server(self.storage, origin, events)\n\n        return events\n\n    @log_function\n    async def get_persisted_pdu(\n        self, origin: str, event_id: str\n    ) -> Optional[EventBase]:\n        \"\"\"Get an event from the database for the given server.\n\n        Args:\n            origin: hostname of server which is requesting the event; we\n               will check that the server is allowed to see it.\n            event_id: id of the event being requested\n\n        Returns:\n            None if we know nothing about the event; otherwise the (possibly-redacted) event.\n\n        Raises:\n            AuthError if the server is not currently in the room\n        \"\"\"\n        event = await self.store.get_event(\n            event_id, allow_none=True, allow_rejected=True\n        )\n\n        if event:\n            in_room = await self.auth.check_host_in_room(event.room_id, origin)\n            if not in_room:\n                raise AuthError(403, \"Host not in room.\")\n\n            events = await filter_events_for_server(self.storage, origin, [event])\n            event = events[0]\n            return event\n        else:\n            return None\n\n    async def get_min_depth_for_context(self, context):\n        return await self.store.get_min_depth(context)\n\n    async def _handle_new_event(\n        self, origin, event, state=None, auth_events=None, backfilled=False\n    ):\n        context = await self._prep_event(\n            origin, event, state=state, auth_events=auth_events, backfilled=backfilled\n        )\n\n        try:\n            if (\n                not event.internal_metadata.is_outlier()\n                and not backfilled\n                and not context.rejected\n            ):\n                await self.action_generator.handle_push_actions_for_event(\n                    event, context\n                )\n\n            await self.persist_events_and_notify(\n                event.room_id, [(event, context)], backfilled=backfilled\n            )\n        except Exception:\n            run_in_background(\n                self.store.remove_push_actions_from_staging, event.event_id\n            )\n            raise\n\n        return context\n\n    async def _handle_new_events(\n        self,\n        origin: str,\n        room_id: str,\n        event_infos: Iterable[_NewEventInfo],\n        backfilled: bool = False,\n    ) -> None:\n        \"\"\"Creates the appropriate contexts and persists events. The events\n        should not depend on one another, e.g. this should be used to persist\n        a bunch of outliers, but not a chunk of individual events that depend\n        on each other for state calculations.\n\n        Notifies about the events where appropriate.\n        \"\"\"\n\n        async def prep(ev_info: _NewEventInfo):\n            event = ev_info.event\n            with nested_logging_context(suffix=event.event_id):\n                res = await self._prep_event(\n                    origin,\n                    event,\n                    state=ev_info.state,\n                    auth_events=ev_info.auth_events,\n                    backfilled=backfilled,\n                )\n            return res\n\n        contexts = await make_deferred_yieldable(\n            defer.gatherResults(\n                [run_in_background(prep, ev_info) for ev_info in event_infos],\n                consumeErrors=True,\n            )\n        )\n\n        await self.persist_events_and_notify(\n            room_id,\n            [\n                (ev_info.event, context)\n                for ev_info, context in zip(event_infos, contexts)\n            ],\n            backfilled=backfilled,\n        )\n\n    async def _persist_auth_tree(\n        self,\n        origin: str,\n        room_id: str,\n        auth_events: List[EventBase],\n        state: List[EventBase],\n        event: EventBase,\n        room_version: RoomVersion,\n    ) -> int:\n        \"\"\"Checks the auth chain is valid (and passes auth checks) for the\n        state and event. Then persists the auth chain and state atomically.\n        Persists the event separately. Notifies about the persisted events\n        where appropriate.\n\n        Will attempt to fetch missing auth events.\n\n        Args:\n            origin: Where the events came from\n            room_id,\n            auth_events\n            state\n            event\n            room_version: The room version we expect this room to have, and\n                will raise if it doesn't match the version in the create event.\n        \"\"\"\n        events_to_context = {}\n        for e in itertools.chain(auth_events, state):\n            e.internal_metadata.outlier = True\n            ctx = await self.state_handler.compute_event_context(e)\n            events_to_context[e.event_id] = ctx\n\n        event_map = {\n            e.event_id: e for e in itertools.chain(auth_events, state, [event])\n        }\n\n        create_event = None\n        for e in auth_events:\n            if (e.type, e.state_key) == (EventTypes.Create, \"\"):\n                create_event = e\n                break\n\n        if create_event is None:\n            # If the state doesn't have a create event then the room is\n            # invalid, and it would fail auth checks anyway.\n            raise SynapseError(400, \"No create event in state\")\n\n        room_version_id = create_event.content.get(\n            \"room_version\", RoomVersions.V1.identifier\n        )\n\n        if room_version.identifier != room_version_id:\n            raise SynapseError(400, \"Room version mismatch\")\n\n        missing_auth_events = set()\n        for e in itertools.chain(auth_events, state, [event]):\n            for e_id in e.auth_event_ids():\n                if e_id not in event_map:\n                    missing_auth_events.add(e_id)\n\n        for e_id in missing_auth_events:\n            m_ev = await self.federation_client.get_pdu(\n                [origin], e_id, room_version=room_version, outlier=True, timeout=10000,\n            )\n            if m_ev and m_ev.event_id == e_id:\n                event_map[e_id] = m_ev\n            else:\n                logger.info(\"Failed to find auth event %r\", e_id)\n\n        for e in itertools.chain(auth_events, state, [event]):\n            auth_for_e = {\n                (event_map[e_id].type, event_map[e_id].state_key): event_map[e_id]\n                for e_id in e.auth_event_ids()\n                if e_id in event_map\n            }\n            if create_event:\n                auth_for_e[(EventTypes.Create, \"\")] = create_event\n\n            try:\n                event_auth.check(room_version, e, auth_events=auth_for_e)\n            except SynapseError as err:\n                # we may get SynapseErrors here as well as AuthErrors. For\n                # instance, there are a couple of (ancient) events in some\n                # rooms whose senders do not have the correct sigil; these\n                # cause SynapseErrors in auth.check. We don't want to give up\n                # the attempt to federate altogether in such cases.\n\n                logger.warning(\"Rejecting %s because %s\", e.event_id, err.msg)\n\n                if e == event:\n                    raise\n                events_to_context[e.event_id].rejected = RejectedReason.AUTH_ERROR\n\n        await self.persist_events_and_notify(\n            room_id,\n            [\n                (e, events_to_context[e.event_id])\n                for e in itertools.chain(auth_events, state)\n            ],\n        )\n\n        new_event_context = await self.state_handler.compute_event_context(\n            event, old_state=state\n        )\n\n        return await self.persist_events_and_notify(\n            room_id, [(event, new_event_context)]\n        )\n\n    async def _prep_event(\n        self,\n        origin: str,\n        event: EventBase,\n        state: Optional[Iterable[EventBase]],\n        auth_events: Optional[MutableStateMap[EventBase]],\n        backfilled: bool,\n    ) -> EventContext:\n        context = await self.state_handler.compute_event_context(event, old_state=state)\n\n        if not auth_events:\n            prev_state_ids = await context.get_prev_state_ids()\n            auth_events_ids = self.auth.compute_auth_events(\n                event, prev_state_ids, for_verification=True\n            )\n            auth_events_x = await self.store.get_events(auth_events_ids)\n            auth_events = {(e.type, e.state_key): e for e in auth_events_x.values()}\n\n        # This is a hack to fix some old rooms where the initial join event\n        # didn't reference the create event in its auth events.\n        if event.type == EventTypes.Member and not event.auth_event_ids():\n            if len(event.prev_event_ids()) == 1 and event.depth < 5:\n                c = await self.store.get_event(\n                    event.prev_event_ids()[0], allow_none=True\n                )\n                if c and c.type == EventTypes.Create:\n                    auth_events[(c.type, c.state_key)] = c\n\n        context = await self.do_auth(origin, event, context, auth_events=auth_events)\n\n        if not context.rejected:\n            await self._check_for_soft_fail(event, state, backfilled)\n\n        if event.type == EventTypes.GuestAccess and not context.rejected:\n            await self.maybe_kick_guest_users(event)\n\n        return context\n\n    async def _check_for_soft_fail(\n        self, event: EventBase, state: Optional[Iterable[EventBase]], backfilled: bool\n    ) -> None:\n        \"\"\"Checks if we should soft fail the event; if so, marks the event as\n        such.\n\n        Args:\n            event\n            state: The state at the event if we don't have all the event's prev events\n            backfilled: Whether the event is from backfill\n        \"\"\"\n        # For new (non-backfilled and non-outlier) events we check if the event\n        # passes auth based on the current state. If it doesn't then we\n        # \"soft-fail\" the event.\n        if backfilled or event.internal_metadata.is_outlier():\n            return\n\n        extrem_ids_list = await self.store.get_latest_event_ids_in_room(event.room_id)\n        extrem_ids = set(extrem_ids_list)\n        prev_event_ids = set(event.prev_event_ids())\n\n        if extrem_ids == prev_event_ids:\n            # If they're the same then the current state is the same as the\n            # state at the event, so no point rechecking auth for soft fail.\n            return\n\n        room_version = await self.store.get_room_version_id(event.room_id)\n        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]\n\n        # Calculate the \"current state\".\n        if state is not None:\n            # If we're explicitly given the state then we won't have all the\n            # prev events, and so we have a gap in the graph. In this case\n            # we want to be a little careful as we might have been down for\n            # a while and have an incorrect view of the current state,\n            # however we still want to do checks as gaps are easy to\n            # maliciously manufacture.\n            #\n            # So we use a \"current state\" that is actually a state\n            # resolution across the current forward extremities and the\n            # given state at the event. This should correctly handle cases\n            # like bans, especially with state res v2.\n\n            state_sets_d = await self.state_store.get_state_groups(\n                event.room_id, extrem_ids\n            )\n            state_sets = list(state_sets_d.values())  # type: List[Iterable[EventBase]]\n            state_sets.append(state)\n            current_states = await self.state_handler.resolve_events(\n                room_version, state_sets, event\n            )\n            current_state_ids = {\n                k: e.event_id for k, e in current_states.items()\n            }  # type: StateMap[str]\n        else:\n            current_state_ids = await self.state_handler.get_current_state_ids(\n                event.room_id, latest_event_ids=extrem_ids\n            )\n\n        logger.debug(\n            \"Doing soft-fail check for %s: state %s\", event.event_id, current_state_ids,\n        )\n\n        # Now check if event pass auth against said current state\n        auth_types = auth_types_for_event(event)\n        current_state_ids_list = [\n            e for k, e in current_state_ids.items() if k in auth_types\n        ]\n\n        auth_events_map = await self.store.get_events(current_state_ids_list)\n        current_auth_events = {\n            (e.type, e.state_key): e for e in auth_events_map.values()\n        }\n\n        try:\n            event_auth.check(room_version_obj, event, auth_events=current_auth_events)\n        except AuthError as e:\n            logger.warning(\"Soft-failing %r because %s\", event, e)\n            event.internal_metadata.soft_failed = True\n\n    async def on_query_auth(\n        self, origin, event_id, room_id, remote_auth_chain, rejects, missing\n    ):\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        # Just go through and process each event in `remote_auth_chain`. We\n        # don't want to fall into the trap of `missing` being wrong.\n        for e in remote_auth_chain:\n            try:\n                await self._handle_new_event(origin, e)\n            except AuthError:\n                pass\n\n        # Now get the current auth_chain for the event.\n        local_auth_chain = await self.store.get_auth_chain(\n            list(event.auth_event_ids()), include_given=True\n        )\n\n        # TODO: Check if we would now reject event_id. If so we need to tell\n        # everyone.\n\n        ret = await self.construct_auth_difference(local_auth_chain, remote_auth_chain)\n\n        logger.debug(\"on_query_auth returning: %s\", ret)\n\n        return ret\n\n    async def on_get_missing_events(\n        self, origin, room_id, earliest_events, latest_events, limit\n    ):\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # Only allow up to 20 events to be retrieved per request.\n        limit = min(limit, 20)\n\n        missing_events = await self.store.get_missing_events(\n            room_id=room_id,\n            earliest_events=earliest_events,\n            latest_events=latest_events,\n            limit=limit,\n        )\n\n        missing_events = await filter_events_for_server(\n            self.storage, origin, missing_events\n        )\n\n        return missing_events\n\n    async def do_auth(\n        self,\n        origin: str,\n        event: EventBase,\n        context: EventContext,\n        auth_events: MutableStateMap[EventBase],\n    ) -> EventContext:\n        \"\"\"\n\n        Args:\n            origin:\n            event:\n            context:\n            auth_events:\n                Map from (event_type, state_key) to event\n\n                Normally, our calculated auth_events based on the state of the room\n                at the event's position in the DAG, though occasionally (eg if the\n                event is an outlier), may be the auth events claimed by the remote\n                server.\n\n                Also NB that this function adds entries to it.\n        Returns:\n            updated context object\n        \"\"\"\n        room_version = await self.store.get_room_version_id(event.room_id)\n        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]\n\n        try:\n            context = await self._update_auth_events_and_context_for_auth(\n                origin, event, context, auth_events\n            )\n        except Exception:\n            # We don't really mind if the above fails, so lets not fail\n            # processing if it does. However, it really shouldn't fail so\n            # let's still log as an exception since we'll still want to fix\n            # any bugs.\n            logger.exception(\n                \"Failed to double check auth events for %s with remote. \"\n                \"Ignoring failure and continuing processing of event.\",\n                event.event_id,\n            )\n\n        try:\n            event_auth.check(room_version_obj, event, auth_events=auth_events)\n        except AuthError as e:\n            logger.warning(\"Failed auth resolution for %r because %s\", event, e)\n            context.rejected = RejectedReason.AUTH_ERROR\n\n        return context\n\n    async def _update_auth_events_and_context_for_auth(\n        self,\n        origin: str,\n        event: EventBase,\n        context: EventContext,\n        auth_events: MutableStateMap[EventBase],\n    ) -> EventContext:\n        \"\"\"Helper for do_auth. See there for docs.\n\n        Checks whether a given event has the expected auth events. If it\n        doesn't then we talk to the remote server to compare state to see if\n        we can come to a consensus (e.g. if one server missed some valid\n        state).\n\n        This attempts to resolve any potential divergence of state between\n        servers, but is not essential and so failures should not block further\n        processing of the event.\n\n        Args:\n            origin:\n            event:\n            context:\n\n            auth_events:\n                Map from (event_type, state_key) to event\n\n                Normally, our calculated auth_events based on the state of the room\n                at the event's position in the DAG, though occasionally (eg if the\n                event is an outlier), may be the auth events claimed by the remote\n                server.\n\n                Also NB that this function adds entries to it.\n\n        Returns:\n            updated context\n        \"\"\"\n        event_auth_events = set(event.auth_event_ids())\n\n        # missing_auth is the set of the event's auth_events which we don't yet have\n        # in auth_events.\n        missing_auth = event_auth_events.difference(\n            e.event_id for e in auth_events.values()\n        )\n\n        # if we have missing events, we need to fetch those events from somewhere.\n        #\n        # we start by checking if they are in the store, and then try calling /event_auth/.\n        if missing_auth:\n            have_events = await self.store.have_seen_events(missing_auth)\n            logger.debug(\"Events %s are in the store\", have_events)\n            missing_auth.difference_update(have_events)\n\n        if missing_auth:\n            # If we don't have all the auth events, we need to get them.\n            logger.info(\"auth_events contains unknown events: %s\", missing_auth)\n            try:\n                try:\n                    remote_auth_chain = await self.federation_client.get_event_auth(\n                        origin, event.room_id, event.event_id\n                    )\n                except RequestSendFailed as e1:\n                    # The other side isn't around or doesn't implement the\n                    # endpoint, so lets just bail out.\n                    logger.info(\"Failed to get event auth from remote: %s\", e1)\n                    return context\n\n                seen_remotes = await self.store.have_seen_events(\n                    [e.event_id for e in remote_auth_chain]\n                )\n\n                for e in remote_auth_chain:\n                    if e.event_id in seen_remotes:\n                        continue\n\n                    if e.event_id == event.event_id:\n                        continue\n\n                    try:\n                        auth_ids = e.auth_event_ids()\n                        auth = {\n                            (e.type, e.state_key): e\n                            for e in remote_auth_chain\n                            if e.event_id in auth_ids or e.type == EventTypes.Create\n                        }\n                        e.internal_metadata.outlier = True\n\n                        logger.debug(\n                            \"do_auth %s missing_auth: %s\", event.event_id, e.event_id\n                        )\n                        await self._handle_new_event(origin, e, auth_events=auth)\n\n                        if e.event_id in event_auth_events:\n                            auth_events[(e.type, e.state_key)] = e\n                    except AuthError:\n                        pass\n\n            except Exception:\n                logger.exception(\"Failed to get auth chain\")\n\n        if event.internal_metadata.is_outlier():\n            # XXX: given that, for an outlier, we'll be working with the\n            # event's *claimed* auth events rather than those we calculated:\n            # (a) is there any point in this test, since different_auth below will\n            # obviously be empty\n            # (b) alternatively, why don't we do it earlier?\n            logger.info(\"Skipping auth_event fetch for outlier\")\n            return context\n\n        different_auth = event_auth_events.difference(\n            e.event_id for e in auth_events.values()\n        )\n\n        if not different_auth:\n            return context\n\n        logger.info(\n            \"auth_events refers to events which are not in our calculated auth \"\n            \"chain: %s\",\n            different_auth,\n        )\n\n        # XXX: currently this checks for redactions but I'm not convinced that is\n        # necessary?\n        different_events = await self.store.get_events_as_list(different_auth)\n\n        for d in different_events:\n            if d.room_id != event.room_id:\n                logger.warning(\n                    \"Event %s refers to auth_event %s which is in a different room\",\n                    event.event_id,\n                    d.event_id,\n                )\n\n                # don't attempt to resolve the claimed auth events against our own\n                # in this case: just use our own auth events.\n                #\n                # XXX: should we reject the event in this case? It feels like we should,\n                # but then shouldn't we also do so if we've failed to fetch any of the\n                # auth events?\n                return context\n\n        # now we state-resolve between our own idea of the auth events, and the remote's\n        # idea of them.\n\n        local_state = auth_events.values()\n        remote_auth_events = dict(auth_events)\n        remote_auth_events.update({(d.type, d.state_key): d for d in different_events})\n        remote_state = remote_auth_events.values()\n\n        room_version = await self.store.get_room_version_id(event.room_id)\n        new_state = await self.state_handler.resolve_events(\n            room_version, (local_state, remote_state), event\n        )\n\n        logger.info(\n            \"After state res: updating auth_events with new state %s\",\n            {\n                (d.type, d.state_key): d.event_id\n                for d in new_state.values()\n                if auth_events.get((d.type, d.state_key)) != d\n            },\n        )\n\n        auth_events.update(new_state)\n\n        context = await self._update_context_for_auth_events(\n            event, context, auth_events\n        )\n\n        return context\n\n    async def _update_context_for_auth_events(\n        self, event: EventBase, context: EventContext, auth_events: StateMap[EventBase]\n    ) -> EventContext:\n        \"\"\"Update the state_ids in an event context after auth event resolution,\n        storing the changes as a new state group.\n\n        Args:\n            event: The event we're handling the context for\n\n            context: initial event context\n\n            auth_events: Events to update in the event context.\n\n        Returns:\n            new event context\n        \"\"\"\n        # exclude the state key of the new event from the current_state in the context.\n        if event.is_state():\n            event_key = (event.type, event.state_key)  # type: Optional[Tuple[str, str]]\n        else:\n            event_key = None\n        state_updates = {\n            k: a.event_id for k, a in auth_events.items() if k != event_key\n        }\n\n        current_state_ids = await context.get_current_state_ids()\n        current_state_ids = dict(current_state_ids)  # type: ignore\n\n        current_state_ids.update(state_updates)\n\n        prev_state_ids = await context.get_prev_state_ids()\n        prev_state_ids = dict(prev_state_ids)\n\n        prev_state_ids.update({k: a.event_id for k, a in auth_events.items()})\n\n        # create a new state group as a delta from the existing one.\n        prev_group = context.state_group\n        state_group = await self.state_store.store_state_group(\n            event.event_id,\n            event.room_id,\n            prev_group=prev_group,\n            delta_ids=state_updates,\n            current_state_ids=current_state_ids,\n        )\n\n        return EventContext.with_state(\n            state_group=state_group,\n            state_group_before_event=context.state_group_before_event,\n            current_state_ids=current_state_ids,\n            prev_state_ids=prev_state_ids,\n            prev_group=prev_group,\n            delta_ids=state_updates,\n        )\n\n    async def construct_auth_difference(\n        self, local_auth: Iterable[EventBase], remote_auth: Iterable[EventBase]\n    ) -> Dict:\n        \"\"\" Given a local and remote auth chain, find the differences. This\n        assumes that we have already processed all events in remote_auth\n\n        Params:\n            local_auth (list)\n            remote_auth (list)\n\n        Returns:\n            dict\n        \"\"\"\n\n        logger.debug(\"construct_auth_difference Start!\")\n\n        # TODO: Make sure we are OK with local_auth or remote_auth having more\n        # auth events in them than strictly necessary.\n\n        def sort_fun(ev):\n            return ev.depth, ev.event_id\n\n        logger.debug(\"construct_auth_difference after sort_fun!\")\n\n        # We find the differences by starting at the \"bottom\" of each list\n        # and iterating up on both lists. The lists are ordered by depth and\n        # then event_id, we iterate up both lists until we find the event ids\n        # don't match. Then we look at depth/event_id to see which side is\n        # missing that event, and iterate only up that list. Repeat.\n\n        remote_list = list(remote_auth)\n        remote_list.sort(key=sort_fun)\n\n        local_list = list(local_auth)\n        local_list.sort(key=sort_fun)\n\n        local_iter = iter(local_list)\n        remote_iter = iter(remote_list)\n\n        logger.debug(\"construct_auth_difference before get_next!\")\n\n        def get_next(it, opt=None):\n            try:\n                return next(it)\n            except Exception:\n                return opt\n\n        current_local = get_next(local_iter)\n        current_remote = get_next(remote_iter)\n\n        logger.debug(\"construct_auth_difference before while\")\n\n        missing_remotes = []\n        missing_locals = []\n        while current_local or current_remote:\n            if current_remote is None:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n                continue\n\n            if current_local is None:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n            if current_local.event_id == current_remote.event_id:\n                current_local = get_next(local_iter)\n                current_remote = get_next(remote_iter)\n                continue\n\n            if current_local.depth < current_remote.depth:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n                continue\n\n            if current_local.depth > current_remote.depth:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n            # They have the same depth, so we fall back to the event_id order\n            if current_local.event_id < current_remote.event_id:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n\n            if current_local.event_id > current_remote.event_id:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n        logger.debug(\"construct_auth_difference after while\")\n\n        # missing locals should be sent to the server\n        # We should find why we are missing remotes, as they will have been\n        # rejected.\n\n        # Remove events from missing_remotes if they are referencing a missing\n        # remote. We only care about the \"root\" rejected ones.\n        missing_remote_ids = [e.event_id for e in missing_remotes]\n        base_remote_rejected = list(missing_remotes)\n        for e in missing_remotes:\n            for e_id in e.auth_event_ids():\n                if e_id in missing_remote_ids:\n                    try:\n                        base_remote_rejected.remove(e)\n                    except ValueError:\n                        pass\n\n        reason_map = {}\n\n        for e in base_remote_rejected:\n            reason = await self.store.get_rejection_reason(e.event_id)\n            if reason is None:\n                # TODO: e is not in the current state, so we should\n                # construct some proof of that.\n                continue\n\n            reason_map[e.event_id] = reason\n\n        logger.debug(\"construct_auth_difference returning\")\n\n        return {\n            \"auth_chain\": local_auth,\n            \"rejects\": {\n                e.event_id: {\"reason\": reason_map[e.event_id], \"proof\": None}\n                for e in base_remote_rejected\n            },\n            \"missing\": [e.event_id for e in missing_locals],\n        }\n\n    @log_function\n    async def exchange_third_party_invite(\n        self, sender_user_id, target_user_id, room_id, signed\n    ):\n        third_party_invite = {\"signed\": signed}\n\n        event_dict = {\n            \"type\": EventTypes.Member,\n            \"content\": {\n                \"membership\": Membership.INVITE,\n                \"third_party_invite\": third_party_invite,\n            },\n            \"room_id\": room_id,\n            \"sender\": sender_user_id,\n            \"state_key\": target_user_id,\n        }\n\n        if await self.auth.check_host_in_room(room_id, self.hs.hostname):\n            room_version = await self.store.get_room_version_id(room_id)\n            builder = self.event_builder_factory.new(room_version, event_dict)\n\n            EventValidator().validate_builder(builder)\n            event, context = await self.event_creation_handler.create_new_client_event(\n                builder=builder\n            )\n\n            event, context = await self.add_display_name_to_third_party_invite(\n                room_version, event_dict, event, context\n            )\n\n            EventValidator().validate_new(event, self.config)\n\n            # We need to tell the transaction queue to send this out, even\n            # though the sender isn't a local user.\n            event.internal_metadata.send_on_behalf_of = self.hs.hostname\n\n            try:\n                await self.auth.check_from_context(room_version, event, context)\n            except AuthError as e:\n                logger.warning(\"Denying new third party invite %r because %s\", event, e)\n                raise e\n\n            await self._check_signature(event, context)\n\n            # We retrieve the room member handler here as to not cause a cyclic dependency\n            member_handler = self.hs.get_room_member_handler()\n            await member_handler.send_membership_event(None, event, context)\n        else:\n            destinations = {x.split(\":\", 1)[-1] for x in (sender_user_id, room_id)}\n            await self.federation_client.forward_third_party_invite(\n                destinations, room_id, event_dict\n            )\n\n    async def on_exchange_third_party_invite_request(\n        self, event_dict: JsonDict\n    ) -> None:\n        \"\"\"Handle an exchange_third_party_invite request from a remote server\n\n        The remote server will call this when it wants to turn a 3pid invite\n        into a normal m.room.member invite.\n\n        Args:\n            event_dict: Dictionary containing the event body.\n\n        \"\"\"\n        assert_params_in_dict(event_dict, [\"room_id\"])\n        room_version = await self.store.get_room_version_id(event_dict[\"room_id\"])\n\n        # NB: event_dict has a particular specced format we might need to fudge\n        # if we change event formats too much.\n        builder = self.event_builder_factory.new(room_version, event_dict)\n\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n        event, context = await self.add_display_name_to_third_party_invite(\n            room_version, event_dict, event, context\n        )\n\n        try:\n            await self.auth.check_from_context(room_version, event, context)\n        except AuthError as e:\n            logger.warning(\"Denying third party invite %r because %s\", event, e)\n            raise e\n        await self._check_signature(event, context)\n\n        # We need to tell the transaction queue to send this out, even\n        # though the sender isn't a local user.\n        event.internal_metadata.send_on_behalf_of = get_domain_from_id(event.sender)\n\n        # We retrieve the room member handler here as to not cause a cyclic dependency\n        member_handler = self.hs.get_room_member_handler()\n        await member_handler.send_membership_event(None, event, context)\n\n    async def add_display_name_to_third_party_invite(\n        self, room_version, event_dict, event, context\n    ):\n        key = (\n            EventTypes.ThirdPartyInvite,\n            event.content[\"third_party_invite\"][\"signed\"][\"token\"],\n        )\n        original_invite = None\n        prev_state_ids = await context.get_prev_state_ids()\n        original_invite_id = prev_state_ids.get(key)\n        if original_invite_id:\n            original_invite = await self.store.get_event(\n                original_invite_id, allow_none=True\n            )\n        if original_invite:\n            # If the m.room.third_party_invite event's content is empty, it means the\n            # invite has been revoked. In this case, we don't have to raise an error here\n            # because the auth check will fail on the invite (because it's not able to\n            # fetch public keys from the m.room.third_party_invite event's content, which\n            # is empty).\n            display_name = original_invite.content.get(\"display_name\")\n            event_dict[\"content\"][\"third_party_invite\"][\"display_name\"] = display_name\n        else:\n            logger.info(\n                \"Could not find invite event for third_party_invite: %r\", event_dict\n            )\n            # We don't discard here as this is not the appropriate place to do\n            # auth checks. If we need the invite and don't have it then the\n            # auth check code will explode appropriately.\n\n        builder = self.event_builder_factory.new(room_version, event_dict)\n        EventValidator().validate_builder(builder)\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n        EventValidator().validate_new(event, self.config)\n        return (event, context)\n\n    async def _check_signature(self, event, context):\n        \"\"\"\n        Checks that the signature in the event is consistent with its invite.\n\n        Args:\n            event (Event): The m.room.member event to check\n            context (EventContext):\n\n        Raises:\n            AuthError: if signature didn't match any keys, or key has been\n                revoked,\n            SynapseError: if a transient error meant a key couldn't be checked\n                for revocation.\n        \"\"\"\n        signed = event.content[\"third_party_invite\"][\"signed\"]\n        token = signed[\"token\"]\n\n        prev_state_ids = await context.get_prev_state_ids()\n        invite_event_id = prev_state_ids.get((EventTypes.ThirdPartyInvite, token))\n\n        invite_event = None\n        if invite_event_id:\n            invite_event = await self.store.get_event(invite_event_id, allow_none=True)\n\n        if not invite_event:\n            raise AuthError(403, \"Could not find invite\")\n\n        logger.debug(\"Checking auth on event %r\", event.content)\n\n        last_exception = None  # type: Optional[Exception]\n\n        # for each public key in the 3pid invite event\n        for public_key_object in self.hs.get_auth().get_public_keys(invite_event):\n            try:\n                # for each sig on the third_party_invite block of the actual invite\n                for server, signature_block in signed[\"signatures\"].items():\n                    for key_name, encoded_signature in signature_block.items():\n                        if not key_name.startswith(\"ed25519:\"):\n                            continue\n\n                        logger.debug(\n                            \"Attempting to verify sig with key %s from %r \"\n                            \"against pubkey %r\",\n                            key_name,\n                            server,\n                            public_key_object,\n                        )\n\n                        try:\n                            public_key = public_key_object[\"public_key\"]\n                            verify_key = decode_verify_key_bytes(\n                                key_name, decode_base64(public_key)\n                            )\n                            verify_signed_json(signed, server, verify_key)\n                            logger.debug(\n                                \"Successfully verified sig with key %s from %r \"\n                                \"against pubkey %r\",\n                                key_name,\n                                server,\n                                public_key_object,\n                            )\n                        except Exception:\n                            logger.info(\n                                \"Failed to verify sig with key %s from %r \"\n                                \"against pubkey %r\",\n                                key_name,\n                                server,\n                                public_key_object,\n                            )\n                            raise\n                        try:\n                            if \"key_validity_url\" in public_key_object:\n                                await self._check_key_revocation(\n                                    public_key, public_key_object[\"key_validity_url\"]\n                                )\n                        except Exception:\n                            logger.info(\n                                \"Failed to query key_validity_url %s\",\n                                public_key_object[\"key_validity_url\"],\n                            )\n                            raise\n                        return\n            except Exception as e:\n                last_exception = e\n\n        if last_exception is None:\n            # we can only get here if get_public_keys() returned an empty list\n            # TODO: make this better\n            raise RuntimeError(\"no public key in invite event\")\n\n        raise last_exception\n\n    async def _check_key_revocation(self, public_key, url):\n        \"\"\"\n        Checks whether public_key has been revoked.\n\n        Args:\n            public_key (str): base-64 encoded public key.\n            url (str): Key revocation URL.\n\n        Raises:\n            AuthError: if they key has been revoked.\n            SynapseError: if a transient error meant a key couldn't be checked\n                for revocation.\n        \"\"\"\n        try:\n            response = await self.http_client.get_json(url, {\"public_key\": public_key})\n        except Exception:\n            raise SynapseError(502, \"Third party certificate could not be checked\")\n        if \"valid\" not in response or not response[\"valid\"]:\n            raise AuthError(403, \"Third party certificate was invalid\")\n\n    async def persist_events_and_notify(\n        self,\n        room_id: str,\n        event_and_contexts: Sequence[Tuple[EventBase, EventContext]],\n        backfilled: bool = False,\n    ) -> int:\n        \"\"\"Persists events and tells the notifier/pushers about them, if\n        necessary.\n\n        Args:\n            room_id: The room ID of events being persisted.\n            event_and_contexts: Sequence of events with their associated\n                context that should be persisted. All events must belong to\n                the same room.\n            backfilled: Whether these events are a result of\n                backfilling or not\n        \"\"\"\n        instance = self.config.worker.events_shard_config.get_instance(room_id)\n        if instance != self._instance_name:\n            result = await self._send_events(\n                instance_name=instance,\n                store=self.store,\n                room_id=room_id,\n                event_and_contexts=event_and_contexts,\n                backfilled=backfilled,\n            )\n            return result[\"max_stream_id\"]\n        else:\n            assert self.storage.persistence\n\n            # Note that this returns the events that were persisted, which may not be\n            # the same as were passed in if some were deduplicated due to transaction IDs.\n            events, max_stream_token = await self.storage.persistence.persist_events(\n                event_and_contexts, backfilled=backfilled\n            )\n\n            if self._ephemeral_messages_enabled:\n                for event in events:\n                    # If there's an expiry timestamp on the event, schedule its expiry.\n                    self._message_handler.maybe_schedule_expiry(event)\n\n            if not backfilled:  # Never notify for backfilled events\n                for event in events:\n                    await self._notify_persisted_event(event, max_stream_token)\n\n            return max_stream_token.stream\n\n    async def _notify_persisted_event(\n        self, event: EventBase, max_stream_token: RoomStreamToken\n    ) -> None:\n        \"\"\"Checks to see if notifier/pushers should be notified about the\n        event or not.\n\n        Args:\n            event:\n            max_stream_id: The max_stream_id returned by persist_events\n        \"\"\"\n\n        extra_users = []\n        if event.type == EventTypes.Member:\n            target_user_id = event.state_key\n\n            # We notify for memberships if its an invite for one of our\n            # users\n            if event.internal_metadata.is_outlier():\n                if event.membership != Membership.INVITE:\n                    if not self.is_mine_id(target_user_id):\n                        return\n\n            target_user = UserID.from_string(target_user_id)\n            extra_users.append(target_user)\n        elif event.internal_metadata.is_outlier():\n            return\n\n        # the event has been persisted so it should have a stream ordering.\n        assert event.internal_metadata.stream_ordering\n\n        event_pos = PersistedEventPosition(\n            self._instance_name, event.internal_metadata.stream_ordering\n        )\n        self.notifier.on_new_room_event(\n            event, event_pos, max_stream_token, extra_users=extra_users\n        )\n\n    async def _clean_room_for_join(self, room_id: str) -> None:\n        \"\"\"Called to clean up any data in DB for a given room, ready for the\n        server to join the room.\n\n        Args:\n            room_id\n        \"\"\"\n        if self.config.worker_app:\n            await self._clean_room_for_join_client(room_id)\n        else:\n            await self.store.clean_room_for_join(room_id)\n\n    async def get_room_complexity(\n        self, remote_room_hosts: List[str], room_id: str\n    ) -> Optional[dict]:\n        \"\"\"\n        Fetch the complexity of a remote room over federation.\n\n        Args:\n            remote_room_hosts (list[str]): The remote servers to ask.\n            room_id (str): The room ID to ask about.\n\n        Returns:\n            Dict contains the complexity\n            metric versions, while None means we could not fetch the complexity.\n        \"\"\"\n\n        for host in remote_room_hosts:\n            res = await self.federation_client.get_room_complexity(host, room_id)\n\n            # We got a result, return it.\n            if res:\n                return res\n\n        # We fell off the bottom, couldn't get the complexity from anyone. Oh\n        # well.\n        return None\n", "patch": "@@ -140,7 +140,7 @@ def __init__(self, hs: \"HomeServer\"):\n         self._message_handler = hs.get_message_handler()\n         self._server_notices_mxid = hs.config.server_notices_mxid\n         self.config = hs.config\n-        self.http_client = hs.get_simple_http_client()\n+        self.http_client = hs.get_proxied_blacklisted_http_client()\n         self._instance_name = hs.get_instance_name()\n         self._replication = hs.get_replication_data_handler()\n ", "file_path": "files/2021_2/22", "file_language": "py", "file_name": "synapse/handlers/federation.py", "outdated_file_modify": 1, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class FederationHandler(BaseHandler):\n    \"\"\"Handles events that originated from federation.\n        Responsible for:\n        a) handling received Pdus before handing them on as Events to the rest\n        of the homeserver (including auth and state conflict resolutions)\n        b) converting events that were produced by local clients that may need\n        to be sent to remote homeservers.\n        c) doing the necessary dances to invite remote users and join remote\n        rooms.\n    \"\"\"\n\n    def __init__(self, hs: \"HomeServer\"):\n        super().__init__(hs)\n\n        self.hs = hs\n\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.state_store = self.storage.state\n        self.federation_client = hs.get_federation_client()\n        self.state_handler = hs.get_state_handler()\n        self._state_resolution_handler = hs.get_state_resolution_handler()\n        self.server_name = hs.hostname\n        self.keyring = hs.get_keyring()\n        self.action_generator = hs.get_action_generator()\n        self.is_mine_id = hs.is_mine_id\n        self.spam_checker = hs.get_spam_checker()\n        self.event_creation_handler = hs.get_event_creation_handler()\n        self._message_handler = hs.get_message_handler()\n        self._server_notices_mxid = hs.config.server_notices_mxid\n        self.config = hs.config\n        self.http_client = hs.get_simple_http_client()\n        self._instance_name = hs.get_instance_name()\n        self._replication = hs.get_replication_data_handler()\n\n        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)\n        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(\n            hs\n        )\n\n        if hs.config.worker_app:\n            self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(\n                hs\n            )\n            self._maybe_store_room_on_outlier_membership = ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(\n                hs\n            )\n        else:\n            self._device_list_updater = hs.get_device_handler().device_list_updater\n            self._maybe_store_room_on_outlier_membership = (\n                self.store.maybe_store_room_on_outlier_membership\n            )\n\n        # When joining a room we need to queue any events for that room up.\n        # For each room, a list of (pdu, origin) tuples.\n        self.room_queues = {}  # type: Dict[str, List[Tuple[EventBase, str]]]\n        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")\n\n        self.third_party_event_rules = hs.get_third_party_event_rules()\n\n        self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages\n\n    async def on_receive_pdu(self, origin, pdu, sent_to_us_directly=False) -> None:\n        \"\"\" Process a PDU received via a federation /send/ transaction, or\n        via backfill of missing prev_events\n\n        Args:\n            origin (str): server which initiated the /send/ transaction. Will\n                be used to fetch missing events or state.\n            pdu (FrozenEvent): received PDU\n            sent_to_us_directly (bool): True if this event was pushed to us; False if\n                we pulled it as the result of a missing prev_event.\n        \"\"\"\n\n        room_id = pdu.room_id\n        event_id = pdu.event_id\n\n        logger.info(\"handling received PDU: %s\", pdu)\n\n        # We reprocess pdus when we have seen them only as outliers\n        existing = await self.store.get_event(\n            event_id, allow_none=True, allow_rejected=True\n        )\n\n        # FIXME: Currently we fetch an event again when we already have it\n        # if it has been marked as an outlier.\n\n        already_seen = existing and (\n            not existing.internal_metadata.is_outlier()\n            or pdu.internal_metadata.is_outlier()\n        )\n        if already_seen:\n            logger.debug(\"[%s %s]: Already seen pdu\", room_id, event_id)\n            return\n\n        # do some initial sanity-checking of the event. In particular, make\n        # sure it doesn't have hundreds of prev_events or auth_events, which\n        # could cause a huge state resolution or cascade of event fetches.\n        try:\n            self._sanity_check_event(pdu)\n        except SynapseError as err:\n            logger.warning(\n                \"[%s %s] Received event failed sanity checks\", room_id, event_id\n            )\n            raise FederationError(\"ERROR\", err.code, err.msg, affected=pdu.event_id)\n\n        # If we are currently in the process of joining this room, then we\n        # queue up events for later processing.\n        if room_id in self.room_queues:\n            logger.info(\n                \"[%s %s] Queuing PDU from %s for now: join in progress\",\n                room_id,\n                event_id,\n                origin,\n            )\n            self.room_queues[room_id].append((pdu, origin))\n            return\n\n        # If we're not in the room just ditch the event entirely. This is\n        # probably an old server that has come back and thinks we're still in\n        # the room (or we've been rejoined to the room by a state reset).\n        #\n        # Note that if we were never in the room then we would have already\n        # dropped the event, since we wouldn't know the room version.\n        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)\n        if not is_in_room:\n            logger.info(\n                \"[%s %s] Ignoring PDU from %s as we're not in the room\",\n                room_id,\n                event_id,\n                origin,\n            )\n            return None\n\n        state = None\n\n        # Get missing pdus if necessary.\n        if not pdu.internal_metadata.is_outlier():\n            # We only backfill backwards to the min depth.\n            min_depth = await self.get_min_depth_for_context(pdu.room_id)\n\n            logger.debug(\"[%s %s] min_depth: %d\", room_id, event_id, min_depth)\n\n            prevs = set(pdu.prev_event_ids())\n            seen = await self.store.have_events_in_timeline(prevs)\n\n            if min_depth is not None and pdu.depth < min_depth:\n                # This is so that we don't notify the user about this\n                # message, to work around the fact that some events will\n                # reference really really old events we really don't want to\n                # send to the clients.\n                pdu.internal_metadata.outlier = True\n            elif min_depth is not None and pdu.depth > min_depth:\n                missing_prevs = prevs - seen\n                if sent_to_us_directly and missing_prevs:\n                    # If we're missing stuff, ensure we only fetch stuff one\n                    # at a time.\n                    logger.info(\n                        \"[%s %s] Acquiring room lock to fetch %d missing prev_events: %s\",\n                        room_id,\n                        event_id,\n                        len(missing_prevs),\n                        shortstr(missing_prevs),\n                    )\n                    with (await self._room_pdu_linearizer.queue(pdu.room_id)):\n                        logger.info(\n                            \"[%s %s] Acquired room lock to fetch %d missing prev_events\",\n                            room_id,\n                            event_id,\n                            len(missing_prevs),\n                        )\n\n                        try:\n                            await self._get_missing_events_for_pdu(\n                                origin, pdu, prevs, min_depth\n                            )\n                        except Exception as e:\n                            raise Exception(\n                                \"Error fetching missing prev_events for %s: %s\"\n                                % (event_id, e)\n                            ) from e\n\n                        # Update the set of things we've seen after trying to\n                        # fetch the missing stuff\n                        seen = await self.store.have_events_in_timeline(prevs)\n\n                        if not prevs - seen:\n                            logger.info(\n                                \"[%s %s] Found all missing prev_events\",\n                                room_id,\n                                event_id,\n                            )\n\n            if prevs - seen:\n                # We've still not been able to get all of the prev_events for this event.\n                #\n                # In this case, we need to fall back to asking another server in the\n                # federation for the state at this event. That's ok provided we then\n                # resolve the state against other bits of the DAG before using it (which\n                # will ensure that you can't just take over a room by sending an event,\n                # withholding its prev_events, and declaring yourself to be an admin in\n                # the subsequent state request).\n                #\n                # Now, if we're pulling this event as a missing prev_event, then clearly\n                # this event is not going to become the only forward-extremity and we are\n                # guaranteed to resolve its state against our existing forward\n                # extremities, so that should be fine.\n                #\n                # On the other hand, if this event was pushed to us, it is possible for\n                # it to become the only forward-extremity in the room, and we would then\n                # trust its state to be the state for the whole room. This is very bad.\n                # Further, if the event was pushed to us, there is no excuse for us not to\n                # have all the prev_events. We therefore reject any such events.\n                #\n                # XXX this really feels like it could/should be merged with the above,\n                # but there is an interaction with min_depth that I'm not really\n                # following.\n\n                if sent_to_us_directly:\n                    logger.warning(\n                        \"[%s %s] Rejecting: failed to fetch %d prev events: %s\",\n                        room_id,\n                        event_id,\n                        len(prevs - seen),\n                        shortstr(prevs - seen),\n                    )\n                    raise FederationError(\n                        \"ERROR\",\n                        403,\n                        (\n                            \"Your server isn't divulging details about prev_events \"\n                            \"referenced in this event.\"\n                        ),\n                        affected=pdu.event_id,\n                    )\n\n                logger.info(\n                    \"Event %s is missing prev_events: calculating state for a \"\n                    \"backwards extremity\",\n                    event_id,\n                )\n\n                # Calculate the state after each of the previous events, and\n                # resolve them to find the correct state at the current event.\n                event_map = {event_id: pdu}\n                try:\n                    # Get the state of the events we know about\n                    ours = await self.state_store.get_state_groups_ids(room_id, seen)\n\n                    # state_maps is a list of mappings from (type, state_key) to event_id\n                    state_maps = list(ours.values())  # type: List[StateMap[str]]\n\n                    # we don't need this any more, let's delete it.\n                    del ours\n\n                    # Ask the remote server for the states we don't\n                    # know about\n                    for p in prevs - seen:\n                        logger.info(\n                            \"Requesting state at missing prev_event %s\", event_id,\n                        )\n\n                        with nested_logging_context(p):\n                            # note that if any of the missing prevs share missing state or\n                            # auth events, the requests to fetch those events are deduped\n                            # by the get_pdu_cache in federation_client.\n                            (remote_state, _,) = await self._get_state_for_room(\n                                origin, room_id, p, include_event_in_state=True\n                            )\n\n                            remote_state_map = {\n                                (x.type, x.state_key): x.event_id for x in remote_state\n                            }\n                            state_maps.append(remote_state_map)\n\n                            for x in remote_state:\n                                event_map[x.event_id] = x\n\n                    room_version = await self.store.get_room_version_id(room_id)\n                    state_map = await self._state_resolution_handler.resolve_events_with_store(\n                        room_id,\n                        room_version,\n                        state_maps,\n                        event_map,\n                        state_res_store=StateResolutionStore(self.store),\n                    )\n\n                    # We need to give _process_received_pdu the actual state events\n                    # rather than event ids, so generate that now.\n\n                    # First though we need to fetch all the events that are in\n                    # state_map, so we can build up the state below.\n                    evs = await self.store.get_events(\n                        list(state_map.values()),\n                        get_prev_content=False,\n                        redact_behaviour=EventRedactBehaviour.AS_IS,\n                    )\n                    event_map.update(evs)\n\n                    state = [event_map[e] for e in state_map.values()]\n                except Exception:\n                    logger.warning(\n                        \"[%s %s] Error attempting to resolve state at missing \"\n                        \"prev_events\",\n                        room_id,\n                        event_id,\n                        exc_info=True,\n                    )\n                    raise FederationError(\n                        \"ERROR\",\n                        403,\n                        \"We can't get valid state history.\",\n                        affected=event_id,\n                    )\n\n        await self._process_received_pdu(origin, pdu, state=state)\n\n    async def _get_missing_events_for_pdu(self, origin, pdu, prevs, min_depth):\n        \"\"\"\n        Args:\n            origin (str): Origin of the pdu. Will be called to get the missing events\n            pdu: received pdu\n            prevs (set(str)): List of event ids which we are missing\n            min_depth (int): Minimum depth of events to return.\n        \"\"\"\n\n        room_id = pdu.room_id\n        event_id = pdu.event_id\n\n        seen = await self.store.have_events_in_timeline(prevs)\n\n        if not prevs - seen:\n            return\n\n        latest_list = await self.store.get_latest_event_ids_in_room(room_id)\n\n        # We add the prev events that we have seen to the latest\n        # list to ensure the remote server doesn't give them to us\n        latest = set(latest_list)\n        latest |= seen\n\n        logger.info(\n            \"[%s %s]: Requesting missing events between %s and %s\",\n            room_id,\n            event_id,\n            shortstr(latest),\n            event_id,\n        )\n\n        # XXX: we set timeout to 10s to help workaround\n        # https://github.com/matrix-org/synapse/issues/1733.\n        # The reason is to avoid holding the linearizer lock\n        # whilst processing inbound /send transactions, causing\n        # FDs to stack up and block other inbound transactions\n        # which empirically can currently take up to 30 minutes.\n        #\n        # N.B. this explicitly disables retry attempts.\n        #\n        # N.B. this also increases our chances of falling back to\n        # fetching fresh state for the room if the missing event\n        # can't be found, which slightly reduces our security.\n        # it may also increase our DAG extremity count for the room,\n        # causing additional state resolution?  See #1760.\n        # However, fetching state doesn't hold the linearizer lock\n        # apparently.\n        #\n        # see https://github.com/matrix-org/synapse/pull/1744\n        #\n        # ----\n        #\n        # Update richvdh 2018/09/18: There are a number of problems with timing this\n        # request out aggressively on the client side:\n        #\n        # - it plays badly with the server-side rate-limiter, which starts tarpitting you\n        #   if you send too many requests at once, so you end up with the server carefully\n        #   working through the backlog of your requests, which you have already timed\n        #   out.\n        #\n        # - for this request in particular, we now (as of\n        #   https://github.com/matrix-org/synapse/pull/3456) reject any PDUs where the\n        #   server can't produce a plausible-looking set of prev_events - so we becone\n        #   much more likely to reject the event.\n        #\n        # - contrary to what it says above, we do *not* fall back to fetching fresh state\n        #   for the room if get_missing_events times out. Rather, we give up processing\n        #   the PDU whose prevs we are missing, which then makes it much more likely that\n        #   we'll end up back here for the *next* PDU in the list, which exacerbates the\n        #   problem.\n        #\n        # - the aggressive 10s timeout was introduced to deal with incoming federation\n        #   requests taking 8 hours to process. It's not entirely clear why that was going\n        #   on; certainly there were other issues causing traffic storms which are now\n        #   resolved, and I think in any case we may be more sensible about our locking\n        #   now. We're *certainly* more sensible about our logging.\n        #\n        # All that said: Let's try increasing the timeout to 60s and see what happens.\n\n        try:\n            missing_events = await self.federation_client.get_missing_events(\n                origin,\n                room_id,\n                earliest_events_ids=list(latest),\n                latest_events=[pdu],\n                limit=10,\n                min_depth=min_depth,\n                timeout=60000,\n            )\n        except (RequestSendFailed, HttpResponseException, NotRetryingDestination) as e:\n            # We failed to get the missing events, but since we need to handle\n            # the case of `get_missing_events` not returning the necessary\n            # events anyway, it is safe to simply log the error and continue.\n            logger.warning(\n                \"[%s %s]: Failed to get prev_events: %s\", room_id, event_id, e\n            )\n            return\n\n        logger.info(\n            \"[%s %s]: Got %d prev_events: %s\",\n            room_id,\n            event_id,\n            len(missing_events),\n            shortstr(missing_events),\n        )\n\n        # We want to sort these by depth so we process them and\n        # tell clients about them in order.\n        missing_events.sort(key=lambda x: x.depth)\n\n        for ev in missing_events:\n            logger.info(\n                \"[%s %s] Handling received prev_event %s\",\n                room_id,\n                event_id,\n                ev.event_id,\n            )\n            with nested_logging_context(ev.event_id):\n                try:\n                    await self.on_receive_pdu(origin, ev, sent_to_us_directly=False)\n                except FederationError as e:\n                    if e.code == 403:\n                        logger.warning(\n                            \"[%s %s] Received prev_event %s failed history check.\",\n                            room_id,\n                            event_id,\n                            ev.event_id,\n                        )\n                    else:\n                        raise\n\n    async def _get_state_for_room(\n        self,\n        destination: str,\n        room_id: str,\n        event_id: str,\n        include_event_in_state: bool = False,\n    ) -> Tuple[List[EventBase], List[EventBase]]:\n        \"\"\"Requests all of the room state at a given event from a remote homeserver.\n\n        Args:\n            destination: The remote homeserver to query for the state.\n            room_id: The id of the room we're interested in.\n            event_id: The id of the event we want the state at.\n            include_event_in_state: if true, the event itself will be included in the\n                returned state event list.\n\n        Returns:\n            A list of events in the state, possibly including the event itself, and\n            a list of events in the auth chain for the given event.\n        \"\"\"\n        (\n            state_event_ids,\n            auth_event_ids,\n        ) = await self.federation_client.get_room_state_ids(\n            destination, room_id, event_id=event_id\n        )\n\n        desired_events = set(state_event_ids + auth_event_ids)\n\n        if include_event_in_state:\n            desired_events.add(event_id)\n\n        event_map = await self._get_events_from_store_or_dest(\n            destination, room_id, desired_events\n        )\n\n        failed_to_fetch = desired_events - event_map.keys()\n        if failed_to_fetch:\n            logger.warning(\n                \"Failed to fetch missing state/auth events for %s %s\",\n                event_id,\n                failed_to_fetch,\n            )\n\n        remote_state = [\n            event_map[e_id] for e_id in state_event_ids if e_id in event_map\n        ]\n\n        if include_event_in_state:\n            remote_event = event_map.get(event_id)\n            if not remote_event:\n                raise Exception(\"Unable to get missing prev_event %s\" % (event_id,))\n            if remote_event.is_state() and remote_event.rejected_reason is None:\n                remote_state.append(remote_event)\n\n        auth_chain = [event_map[e_id] for e_id in auth_event_ids if e_id in event_map]\n        auth_chain.sort(key=lambda e: e.depth)\n\n        return remote_state, auth_chain\n\n    async def _get_events_from_store_or_dest(\n        self, destination: str, room_id: str, event_ids: Iterable[str]\n    ) -> Dict[str, EventBase]:\n        \"\"\"Fetch events from a remote destination, checking if we already have them.\n\n        Persists any events we don't already have as outliers.\n\n        If we fail to fetch any of the events, a warning will be logged, and the event\n        will be omitted from the result. Likewise, any events which turn out not to\n        be in the given room.\n\n        This function *does not* automatically get missing auth events of the\n        newly fetched events. Callers must include the full auth chain of\n        of the missing events in the `event_ids` argument, to ensure that any\n        missing auth events are correctly fetched.\n\n        Returns:\n            map from event_id to event\n        \"\"\"\n        fetched_events = await self.store.get_events(event_ids, allow_rejected=True)\n\n        missing_events = set(event_ids) - fetched_events.keys()\n\n        if missing_events:\n            logger.debug(\n                \"Fetching unknown state/auth events %s for room %s\",\n                missing_events,\n                room_id,\n            )\n\n            await self._get_events_and_persist(\n                destination=destination, room_id=room_id, events=missing_events\n            )\n\n            # we need to make sure we re-load from the database to get the rejected\n            # state correct.\n            fetched_events.update(\n                (await self.store.get_events(missing_events, allow_rejected=True))\n            )\n\n        # check for events which were in the wrong room.\n        #\n        # this can happen if a remote server claims that the state or\n        # auth_events at an event in room A are actually events in room B\n\n        bad_events = [\n            (event_id, event.room_id)\n            for event_id, event in fetched_events.items()\n            if event.room_id != room_id\n        ]\n\n        for bad_event_id, bad_room_id in bad_events:\n            # This is a bogus situation, but since we may only discover it a long time\n            # after it happened, we try our best to carry on, by just omitting the\n            # bad events from the returned auth/state set.\n            logger.warning(\n                \"Remote server %s claims event %s in room %s is an auth/state \"\n                \"event in room %s\",\n                destination,\n                bad_event_id,\n                bad_room_id,\n                room_id,\n            )\n\n            del fetched_events[bad_event_id]\n\n        return fetched_events\n\n    async def _process_received_pdu(\n        self, origin: str, event: EventBase, state: Optional[Iterable[EventBase]],\n    ):\n        \"\"\" Called when we have a new pdu. We need to do auth checks and put it\n        through the StateHandler.\n\n        Args:\n            origin: server sending the event\n\n            event: event to be persisted\n\n            state: Normally None, but if we are handling a gap in the graph\n                (ie, we are missing one or more prev_events), the resolved state at the\n                event\n        \"\"\"\n        room_id = event.room_id\n        event_id = event.event_id\n\n        logger.debug(\"[%s %s] Processing event: %s\", room_id, event_id, event)\n\n        try:\n            await self._handle_new_event(origin, event, state=state)\n        except AuthError as e:\n            raise FederationError(\"ERROR\", e.code, e.msg, affected=event.event_id)\n\n        # For encrypted messages we check that we know about the sending device,\n        # if we don't then we mark the device cache for that user as stale.\n        if event.type == EventTypes.Encrypted:\n            device_id = event.content.get(\"device_id\")\n            sender_key = event.content.get(\"sender_key\")\n\n            cached_devices = await self.store.get_cached_devices_for_user(event.sender)\n\n            resync = False  # Whether we should resync device lists.\n\n            device = None\n            if device_id is not None:\n                device = cached_devices.get(device_id)\n                if device is None:\n                    logger.info(\n                        \"Received event from remote device not in our cache: %s %s\",\n                        event.sender,\n                        device_id,\n                    )\n                    resync = True\n\n            # We also check if the `sender_key` matches what we expect.\n            if sender_key is not None:\n                # Figure out what sender key we're expecting. If we know the\n                # device and recognize the algorithm then we can work out the\n                # exact key to expect. Otherwise check it matches any key we\n                # have for that device.\n\n                current_keys = []  # type: Container[str]\n\n                if device:\n                    keys = device.get(\"keys\", {}).get(\"keys\", {})\n\n                    if (\n                        event.content.get(\"algorithm\")\n                        == RoomEncryptionAlgorithms.MEGOLM_V1_AES_SHA2\n                    ):\n                        # For this algorithm we expect a curve25519 key.\n                        key_name = \"curve25519:%s\" % (device_id,)\n                        current_keys = [keys.get(key_name)]\n                    else:\n                        # We don't know understand the algorithm, so we just\n                        # check it matches a key for the device.\n                        current_keys = keys.values()\n                elif device_id:\n                    # We don't have any keys for the device ID.\n                    pass\n                else:\n                    # The event didn't include a device ID, so we just look for\n                    # keys across all devices.\n                    current_keys = [\n                        key\n                        for device in cached_devices.values()\n                        for key in device.get(\"keys\", {}).get(\"keys\", {}).values()\n                    ]\n\n                # We now check that the sender key matches (one of) the expected\n                # keys.\n                if sender_key not in current_keys:\n                    logger.info(\n                        \"Received event from remote device with unexpected sender key: %s %s: %s\",\n                        event.sender,\n                        device_id or \"<no device_id>\",\n                        sender_key,\n                    )\n                    resync = True\n\n            if resync:\n                run_as_background_process(\n                    \"resync_device_due_to_pdu\", self._resync_device, event.sender\n                )\n\n    async def _resync_device(self, sender: str) -> None:\n        \"\"\"We have detected that the device list for the given user may be out\n        of sync, so we try and resync them.\n        \"\"\"\n\n        try:\n            await self.store.mark_remote_user_device_cache_as_stale(sender)\n\n            # Immediately attempt a resync in the background\n            if self.config.worker_app:\n                await self._user_device_resync(user_id=sender)\n            else:\n                await self._device_list_updater.user_device_resync(sender)\n        except Exception:\n            logger.exception(\"Failed to resync device for %s\", sender)\n\n    @log_function\n    async def backfill(self, dest, room_id, limit, extremities):\n        \"\"\" Trigger a backfill request to `dest` for the given `room_id`\n\n        This will attempt to get more events from the remote. If the other side\n        has no new events to offer, this will return an empty list.\n\n        As the events are received, we check their signatures, and also do some\n        sanity-checking on them. If any of the backfilled events are invalid,\n        this method throws a SynapseError.\n\n        TODO: make this more useful to distinguish failures of the remote\n        server from invalid events (there is probably no point in trying to\n        re-fetch invalid events from every other HS in the room.)\n        \"\"\"\n        if dest == self.server_name:\n            raise SynapseError(400, \"Can't backfill from self.\")\n\n        events = await self.federation_client.backfill(\n            dest, room_id, limit=limit, extremities=extremities\n        )\n\n        if not events:\n            return []\n\n        # ideally we'd sanity check the events here for excess prev_events etc,\n        # but it's hard to reject events at this point without completely\n        # breaking backfill in the same way that it is currently broken by\n        # events whose signature we cannot verify (#3121).\n        #\n        # So for now we accept the events anyway. #3124 tracks this.\n        #\n        # for ev in events:\n        #     self._sanity_check_event(ev)\n\n        # Don't bother processing events we already have.\n        seen_events = await self.store.have_events_in_timeline(\n            {e.event_id for e in events}\n        )\n\n        events = [e for e in events if e.event_id not in seen_events]\n\n        if not events:\n            return []\n\n        event_map = {e.event_id: e for e in events}\n\n        event_ids = {e.event_id for e in events}\n\n        # build a list of events whose prev_events weren't in the batch.\n        # (XXX: this will include events whose prev_events we already have; that doesn't\n        # sound right?)\n        edges = [ev.event_id for ev in events if set(ev.prev_event_ids()) - event_ids]\n\n        logger.info(\"backfill: Got %d events with %d edges\", len(events), len(edges))\n\n        # For each edge get the current state.\n\n        auth_events = {}\n        state_events = {}\n        events_to_state = {}\n        for e_id in edges:\n            state, auth = await self._get_state_for_room(\n                destination=dest,\n                room_id=room_id,\n                event_id=e_id,\n                include_event_in_state=False,\n            )\n            auth_events.update({a.event_id: a for a in auth})\n            auth_events.update({s.event_id: s for s in state})\n            state_events.update({s.event_id: s for s in state})\n            events_to_state[e_id] = state\n\n        required_auth = {\n            a_id\n            for event in events\n            + list(state_events.values())\n            + list(auth_events.values())\n            for a_id in event.auth_event_ids()\n        }\n        auth_events.update(\n            {e_id: event_map[e_id] for e_id in required_auth if e_id in event_map}\n        )\n\n        ev_infos = []\n\n        # Step 1: persist the events in the chunk we fetched state for (i.e.\n        # the backwards extremities), with custom auth events and state\n        for e_id in events_to_state:\n            # For paranoia we ensure that these events are marked as\n            # non-outliers\n            ev = event_map[e_id]\n            assert not ev.internal_metadata.is_outlier()\n\n            ev_infos.append(\n                _NewEventInfo(\n                    event=ev,\n                    state=events_to_state[e_id],\n                    auth_events={\n                        (\n                            auth_events[a_id].type,\n                            auth_events[a_id].state_key,\n                        ): auth_events[a_id]\n                        for a_id in ev.auth_event_ids()\n                        if a_id in auth_events\n                    },\n                )\n            )\n\n        if ev_infos:\n            await self._handle_new_events(dest, room_id, ev_infos, backfilled=True)\n\n        # Step 2: Persist the rest of the events in the chunk one by one\n        events.sort(key=lambda e: e.depth)\n\n        for event in events:\n            if event in events_to_state:\n                continue\n\n            # For paranoia we ensure that these events are marked as\n            # non-outliers\n            assert not event.internal_metadata.is_outlier()\n\n            # We store these one at a time since each event depends on the\n            # previous to work out the state.\n            # TODO: We can probably do something more clever here.\n            await self._handle_new_event(dest, event, backfilled=True)\n\n        return events\n\n    async def maybe_backfill(\n        self, room_id: str, current_depth: int, limit: int\n    ) -> bool:\n        \"\"\"Checks the database to see if we should backfill before paginating,\n        and if so do.\n\n        Args:\n            room_id\n            current_depth: The depth from which we're paginating from. This is\n                used to decide if we should backfill and what extremities to\n                use.\n            limit: The number of events that the pagination request will\n                return. This is used as part of the heuristic to decide if we\n                should back paginate.\n        \"\"\"\n        extremities = await self.store.get_oldest_events_with_depth_in_room(room_id)\n\n        if not extremities:\n            logger.debug(\"Not backfilling as no extremeties found.\")\n            return False\n\n        # We only want to paginate if we can actually see the events we'll get,\n        # as otherwise we'll just spend a lot of resources to get redacted\n        # events.\n        #\n        # We do this by filtering all the backwards extremities and seeing if\n        # any remain. Given we don't have the extremity events themselves, we\n        # need to actually check the events that reference them.\n        #\n        # *Note*: the spec wants us to keep backfilling until we reach the start\n        # of the room in case we are allowed to see some of the history. However\n        # in practice that causes more issues than its worth, as a) its\n        # relatively rare for there to be any visible history and b) even when\n        # there is its often sufficiently long ago that clients would stop\n        # attempting to paginate before backfill reached the visible history.\n        #\n        # TODO: If we do do a backfill then we should filter the backwards\n        #   extremities to only include those that point to visible portions of\n        #   history.\n        #\n        # TODO: Correctly handle the case where we are allowed to see the\n        #   forward event but not the backward extremity, e.g. in the case of\n        #   initial join of the server where we are allowed to see the join\n        #   event but not anything before it. This would require looking at the\n        #   state *before* the event, ignoring the special casing certain event\n        #   types have.\n\n        forward_events = await self.store.get_successor_events(list(extremities))\n\n        extremities_events = await self.store.get_events(\n            forward_events,\n            redact_behaviour=EventRedactBehaviour.AS_IS,\n            get_prev_content=False,\n        )\n\n        # We set `check_history_visibility_only` as we might otherwise get false\n        # positives from users having been erased.\n        filtered_extremities = await filter_events_for_server(\n            self.storage,\n            self.server_name,\n            list(extremities_events.values()),\n            redact=False,\n            check_history_visibility_only=True,\n        )\n\n        if not filtered_extremities:\n            return False\n\n        # Check if we reached a point where we should start backfilling.\n        sorted_extremeties_tuple = sorted(extremities.items(), key=lambda e: -int(e[1]))\n        max_depth = sorted_extremeties_tuple[0][1]\n\n        # If we're approaching an extremity we trigger a backfill, otherwise we\n        # no-op.\n        #\n        # We chose twice the limit here as then clients paginating backwards\n        # will send pagination requests that trigger backfill at least twice\n        # using the most recent extremity before it gets removed (see below). We\n        # chose more than one times the limit in case of failure, but choosing a\n        # much larger factor will result in triggering a backfill request much\n        # earlier than necessary.\n        if current_depth - 2 * limit > max_depth:\n            logger.debug(\n                \"Not backfilling as we don't need to. %d < %d - 2 * %d\",\n                max_depth,\n                current_depth,\n                limit,\n            )\n            return False\n\n        logger.debug(\n            \"room_id: %s, backfill: current_depth: %s, max_depth: %s, extrems: %s\",\n            room_id,\n            current_depth,\n            max_depth,\n            sorted_extremeties_tuple,\n        )\n\n        # We ignore extremities that have a greater depth than our current depth\n        # as:\n        #    1. we don't really care about getting events that have happened\n        #       before our current position; and\n        #    2. we have likely previously tried and failed to backfill from that\n        #       extremity, so to avoid getting \"stuck\" requesting the same\n        #       backfill repeatedly we drop those extremities.\n        filtered_sorted_extremeties_tuple = [\n            t for t in sorted_extremeties_tuple if int(t[1]) <= current_depth\n        ]\n\n        # However, we need to check that the filtered extremities are non-empty.\n        # If they are empty then either we can a) bail or b) still attempt to\n        # backill. We opt to try backfilling anyway just in case we do get\n        # relevant events.\n        if filtered_sorted_extremeties_tuple:\n            sorted_extremeties_tuple = filtered_sorted_extremeties_tuple\n\n        # We don't want to specify too many extremities as it causes the backfill\n        # request URI to be too long.\n        extremities = dict(sorted_extremeties_tuple[:5])\n\n        # Now we need to decide which hosts to hit first.\n\n        # First we try hosts that are already in the room\n        # TODO: HEURISTIC ALERT.\n\n        curr_state = await self.state_handler.get_current_state(room_id)\n\n        def get_domains_from_state(state):\n            \"\"\"Get joined domains from state\n\n            Args:\n                state (dict[tuple, FrozenEvent]): State map from type/state\n                    key to event.\n\n            Returns:\n                list[tuple[str, int]]: Returns a list of servers with the\n                lowest depth of their joins. Sorted by lowest depth first.\n            \"\"\"\n            joined_users = [\n                (state_key, int(event.depth))\n                for (e_type, state_key), event in state.items()\n                if e_type == EventTypes.Member and event.membership == Membership.JOIN\n            ]\n\n            joined_domains = {}  # type: Dict[str, int]\n            for u, d in joined_users:\n                try:\n                    dom = get_domain_from_id(u)\n                    old_d = joined_domains.get(dom)\n                    if old_d:\n                        joined_domains[dom] = min(d, old_d)\n                    else:\n                        joined_domains[dom] = d\n                except Exception:\n                    pass\n\n            return sorted(joined_domains.items(), key=lambda d: d[1])\n\n        curr_domains = get_domains_from_state(curr_state)\n\n        likely_domains = [\n            domain for domain, depth in curr_domains if domain != self.server_name\n        ]\n\n        async def try_backfill(domains):\n            # TODO: Should we try multiple of these at a time?\n            for dom in domains:\n                try:\n                    await self.backfill(\n                        dom, room_id, limit=100, extremities=extremities\n                    )\n                    # If this succeeded then we probably already have the\n                    # appropriate stuff.\n                    # TODO: We can probably do something more intelligent here.\n                    return True\n                except SynapseError as e:\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except HttpResponseException as e:\n                    if 400 <= e.code < 500:\n                        raise e.to_synapse_error()\n\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except CodeMessageException as e:\n                    if 400 <= e.code < 500:\n                        raise\n\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except NotRetryingDestination as e:\n                    logger.info(str(e))\n                    continue\n                except RequestSendFailed as e:\n                    logger.info(\"Failed to get backfill from %s because %s\", dom, e)\n                    continue\n                except FederationDeniedError as e:\n                    logger.info(e)\n                    continue\n                except Exception as e:\n                    logger.exception(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n\n            return False\n\n        success = await try_backfill(likely_domains)\n        if success:\n            return True\n\n        # Huh, well *those* domains didn't work out. Lets try some domains\n        # from the time.\n\n        tried_domains = set(likely_domains)\n        tried_domains.add(self.server_name)\n\n        event_ids = list(extremities.keys())\n\n        logger.debug(\"calling resolve_state_groups in _maybe_backfill\")\n        resolve = preserve_fn(self.state_handler.resolve_state_groups_for_events)\n        states = await make_deferred_yieldable(\n            defer.gatherResults(\n                [resolve(room_id, [e]) for e in event_ids], consumeErrors=True\n            )\n        )\n\n        # dict[str, dict[tuple, str]], a map from event_id to state map of\n        # event_ids.\n        states = dict(zip(event_ids, [s.state for s in states]))\n\n        state_map = await self.store.get_events(\n            [e_id for ids in states.values() for e_id in ids.values()],\n            get_prev_content=False,\n        )\n        states = {\n            key: {\n                k: state_map[e_id]\n                for k, e_id in state_dict.items()\n                if e_id in state_map\n            }\n            for key, state_dict in states.items()\n        }\n\n        for e_id, _ in sorted_extremeties_tuple:\n            likely_domains = get_domains_from_state(states[e_id])\n\n            success = await try_backfill(\n                [dom for dom, _ in likely_domains if dom not in tried_domains]\n            )\n            if success:\n                return True\n\n            tried_domains.update(dom for dom, _ in likely_domains)\n\n        return False\n\n    async def _get_events_and_persist(\n        self, destination: str, room_id: str, events: Iterable[str]\n    ):\n        \"\"\"Fetch the given events from a server, and persist them as outliers.\n\n        This function *does not* recursively get missing auth events of the\n        newly fetched events. Callers must include in the `events` argument\n        any missing events from the auth chain.\n\n        Logs a warning if we can't find the given event.\n        \"\"\"\n\n        room_version = await self.store.get_room_version(room_id)\n\n        event_map = {}  # type: Dict[str, EventBase]\n\n        async def get_event(event_id: str):\n            with nested_logging_context(event_id):\n                try:\n                    event = await self.federation_client.get_pdu(\n                        [destination], event_id, room_version, outlier=True,\n                    )\n                    if event is None:\n                        logger.warning(\n                            \"Server %s didn't return event %s\", destination, event_id,\n                        )\n                        return\n\n                    event_map[event.event_id] = event\n\n                except Exception as e:\n                    logger.warning(\n                        \"Error fetching missing state/auth event %s: %s %s\",\n                        event_id,\n                        type(e),\n                        e,\n                    )\n\n        await concurrently_execute(get_event, events, 5)\n\n        # Make a map of auth events for each event. We do this after fetching\n        # all the events as some of the events' auth events will be in the list\n        # of requested events.\n\n        auth_events = [\n            aid\n            for event in event_map.values()\n            for aid in event.auth_event_ids()\n            if aid not in event_map\n        ]\n        persisted_events = await self.store.get_events(\n            auth_events, allow_rejected=True,\n        )\n\n        event_infos = []\n        for event in event_map.values():\n            auth = {}\n            for auth_event_id in event.auth_event_ids():\n                ae = persisted_events.get(auth_event_id) or event_map.get(auth_event_id)\n                if ae:\n                    auth[(ae.type, ae.state_key)] = ae\n                else:\n                    logger.info(\"Missing auth event %s\", auth_event_id)\n\n            event_infos.append(_NewEventInfo(event, None, auth))\n\n        await self._handle_new_events(\n            destination, room_id, event_infos,\n        )\n\n    def _sanity_check_event(self, ev):\n        \"\"\"\n        Do some early sanity checks of a received event\n\n        In particular, checks it doesn't have an excessive number of\n        prev_events or auth_events, which could cause a huge state resolution\n        or cascade of event fetches.\n\n        Args:\n            ev (synapse.events.EventBase): event to be checked\n\n        Returns: None\n\n        Raises:\n            SynapseError if the event does not pass muster\n        \"\"\"\n        if len(ev.prev_event_ids()) > 20:\n            logger.warning(\n                \"Rejecting event %s which has %i prev_events\",\n                ev.event_id,\n                len(ev.prev_event_ids()),\n            )\n            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many prev_events\")\n\n        if len(ev.auth_event_ids()) > 10:\n            logger.warning(\n                \"Rejecting event %s which has %i auth_events\",\n                ev.event_id,\n                len(ev.auth_event_ids()),\n            )\n            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many auth_events\")\n\n    async def send_invite(self, target_host, event):\n        \"\"\" Sends the invite to the remote server for signing.\n\n        Invites must be signed by the invitee's server before distribution.\n        \"\"\"\n        pdu = await self.federation_client.send_invite(\n            destination=target_host,\n            room_id=event.room_id,\n            event_id=event.event_id,\n            pdu=event,\n        )\n\n        return pdu\n\n    async def on_event_auth(self, event_id: str) -> List[EventBase]:\n        event = await self.store.get_event(event_id)\n        auth = await self.store.get_auth_chain(\n            list(event.auth_event_ids()), include_given=True\n        )\n        return list(auth)\n\n    async def do_invite_join(\n        self, target_hosts: Iterable[str], room_id: str, joinee: str, content: JsonDict\n    ) -> Tuple[str, int]:\n        \"\"\" Attempts to join the `joinee` to the room `room_id` via the\n        servers contained in `target_hosts`.\n\n        This first triggers a /make_join/ request that returns a partial\n        event that we can fill out and sign. This is then sent to the\n        remote server via /send_join/ which responds with the state at that\n        event and the auth_chains.\n\n        We suspend processing of any received events from this room until we\n        have finished processing the join.\n\n        Args:\n            target_hosts: List of servers to attempt to join the room with.\n\n            room_id: The ID of the room to join.\n\n            joinee: The User ID of the joining user.\n\n            content: The event content to use for the join event.\n        \"\"\"\n        # TODO: We should be able to call this on workers, but the upgrading of\n        # room stuff after join currently doesn't work on workers.\n        assert self.config.worker.worker_app is None\n\n        logger.debug(\"Joining %s to %s\", joinee, room_id)\n\n        origin, event, room_version_obj = await self._make_and_verify_event(\n            target_hosts,\n            room_id,\n            joinee,\n            \"join\",\n            content,\n            params={\"ver\": KNOWN_ROOM_VERSIONS},\n        )\n\n        # This shouldn't happen, because the RoomMemberHandler has a\n        # linearizer lock which only allows one operation per user per room\n        # at a time - so this is just paranoia.\n        assert room_id not in self.room_queues\n\n        self.room_queues[room_id] = []\n\n        await self._clean_room_for_join(room_id)\n\n        handled_events = set()\n\n        try:\n            # Try the host we successfully got a response to /make_join/\n            # request first.\n            host_list = list(target_hosts)\n            try:\n                host_list.remove(origin)\n                host_list.insert(0, origin)\n            except ValueError:\n                pass\n\n            ret = await self.federation_client.send_join(\n                host_list, event, room_version_obj\n            )\n\n            origin = ret[\"origin\"]\n            state = ret[\"state\"]\n            auth_chain = ret[\"auth_chain\"]\n            auth_chain.sort(key=lambda e: e.depth)\n\n            handled_events.update([s.event_id for s in state])\n            handled_events.update([a.event_id for a in auth_chain])\n            handled_events.add(event.event_id)\n\n            logger.debug(\"do_invite_join auth_chain: %s\", auth_chain)\n            logger.debug(\"do_invite_join state: %s\", state)\n\n            logger.debug(\"do_invite_join event: %s\", event)\n\n            # if this is the first time we've joined this room, it's time to add\n            # a row to `rooms` with the correct room version. If there's already a\n            # row there, we should override it, since it may have been populated\n            # based on an invite request which lied about the room version.\n            #\n            # federation_client.send_join has already checked that the room\n            # version in the received create event is the same as room_version_obj,\n            # so we can rely on it now.\n            #\n            await self.store.upsert_room_on_join(\n                room_id=room_id, room_version=room_version_obj,\n            )\n\n            max_stream_id = await self._persist_auth_tree(\n                origin, room_id, auth_chain, state, event, room_version_obj\n            )\n\n            # We wait here until this instance has seen the events come down\n            # replication (if we're using replication) as the below uses caches.\n            await self._replication.wait_for_stream_position(\n                self.config.worker.events_shard_config.get_instance(room_id),\n                \"events\",\n                max_stream_id,\n            )\n\n            # Check whether this room is the result of an upgrade of a room we already know\n            # about. If so, migrate over user information\n            predecessor = await self.store.get_room_predecessor(room_id)\n            if not predecessor or not isinstance(predecessor.get(\"room_id\"), str):\n                return event.event_id, max_stream_id\n            old_room_id = predecessor[\"room_id\"]\n            logger.debug(\n                \"Found predecessor for %s during remote join: %s\", room_id, old_room_id\n            )\n\n            # We retrieve the room member handler here as to not cause a cyclic dependency\n            member_handler = self.hs.get_room_member_handler()\n            await member_handler.transfer_room_state_on_room_upgrade(\n                old_room_id, room_id\n            )\n\n            logger.debug(\"Finished joining %s to %s\", joinee, room_id)\n            return event.event_id, max_stream_id\n        finally:\n            room_queue = self.room_queues[room_id]\n            del self.room_queues[room_id]\n\n            # we don't need to wait for the queued events to be processed -\n            # it's just a best-effort thing at this point. We do want to do\n            # them roughly in order, though, otherwise we'll end up making\n            # lots of requests for missing prev_events which we do actually\n            # have. Hence we fire off the background task, but don't wait for it.\n\n            run_in_background(self._handle_queued_pdus, room_queue)\n\n    async def _handle_queued_pdus(self, room_queue):\n        \"\"\"Process PDUs which got queued up while we were busy send_joining.\n\n        Args:\n            room_queue (list[FrozenEvent, str]): list of PDUs to be processed\n                and the servers that sent them\n        \"\"\"\n        for p, origin in room_queue:\n            try:\n                logger.info(\n                    \"Processing queued PDU %s which was received \"\n                    \"while we were joining %s\",\n                    p.event_id,\n                    p.room_id,\n                )\n                with nested_logging_context(p.event_id):\n                    await self.on_receive_pdu(origin, p, sent_to_us_directly=True)\n            except Exception as e:\n                logger.warning(\n                    \"Error handling queued PDU %s from %s: %s\", p.event_id, origin, e\n                )\n\n    async def on_make_join_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> EventBase:\n        \"\"\" We've received a /make_join/ request, so we create a partial\n        join event for the room and return that. We do *not* persist or\n        process it until the other server has signed it and sent it back.\n\n        Args:\n            origin: The (verified) server name of the requesting server.\n            room_id: Room to create join event in\n            user_id: The user to create the join for\n        \"\"\"\n        if get_domain_from_id(user_id) != origin:\n            logger.info(\n                \"Got /make_join request for user %r from different origin %s, ignoring\",\n                user_id,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        # checking the room version will check that we've actually heard of the room\n        # (and return a 404 otherwise)\n        room_version = await self.store.get_room_version_id(room_id)\n\n        # now check that we are *still* in the room\n        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)\n        if not is_in_room:\n            logger.info(\n                \"Got /make_join request for room %s we are no longer in\", room_id,\n            )\n            raise NotFoundError(\"Not an active room on this server\")\n\n        event_content = {\"membership\": Membership.JOIN}\n\n        builder = self.event_builder_factory.new(\n            room_version,\n            {\n                \"type\": EventTypes.Member,\n                \"content\": event_content,\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": user_id,\n            },\n        )\n\n        try:\n            event, context = await self.event_creation_handler.create_new_client_event(\n                builder=builder\n            )\n        except SynapseError as e:\n            logger.warning(\"Failed to create join to %s because %s\", room_id, e)\n            raise\n\n        # The remote hasn't signed it yet, obviously. We'll do the full checks\n        # when we get the event back in `on_send_join_request`\n        await self.auth.check_from_context(\n            room_version, event, context, do_sig_check=False\n        )\n\n        return event\n\n    async def on_send_join_request(self, origin, pdu):\n        \"\"\" We have received a join event for a room. Fully process it and\n        respond with the current state and auth chains.\n        \"\"\"\n        event = pdu\n\n        logger.debug(\n            \"on_send_join_request from %s: Got event: %s, signatures: %s\",\n            origin,\n            event.event_id,\n            event.signatures,\n        )\n\n        if get_domain_from_id(event.sender) != origin:\n            logger.info(\n                \"Got /send_join request for user %r from different origin %s\",\n                event.sender,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        event.internal_metadata.outlier = False\n        # Send this event on behalf of the origin server.\n        #\n        # The reasons we have the destination server rather than the origin\n        # server send it are slightly mysterious: the origin server should have\n        # all the necessary state once it gets the response to the send_join,\n        # so it could send the event itself if it wanted to. It may be that\n        # doing it this way reduces failure modes, or avoids certain attacks\n        # where a new server selectively tells a subset of the federation that\n        # it has joined.\n        #\n        # The fact is that, as of the current writing, Synapse doesn't send out\n        # the join event over federation after joining, and changing it now\n        # would introduce the danger of backwards-compatibility problems.\n        event.internal_metadata.send_on_behalf_of = origin\n\n        context = await self._handle_new_event(origin, event)\n\n        logger.debug(\n            \"on_send_join_request: After _handle_new_event: %s, sigs: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        prev_state_ids = await context.get_prev_state_ids()\n\n        state_ids = list(prev_state_ids.values())\n        auth_chain = await self.store.get_auth_chain(state_ids)\n\n        state = await self.store.get_events(list(prev_state_ids.values()))\n\n        return {\"state\": list(state.values()), \"auth_chain\": auth_chain}\n\n    async def on_invite_request(\n        self, origin: str, event: EventBase, room_version: RoomVersion\n    ):\n        \"\"\" We've got an invite event. Process and persist it. Sign it.\n\n        Respond with the now signed event.\n        \"\"\"\n        if event.state_key is None:\n            raise SynapseError(400, \"The invite event did not have a state key\")\n\n        is_blocked = await self.store.is_room_blocked(event.room_id)\n        if is_blocked:\n            raise SynapseError(403, \"This room has been blocked on this server\")\n\n        if self.hs.config.block_non_admin_invites:\n            raise SynapseError(403, \"This server does not accept room invites\")\n\n        if not self.spam_checker.user_may_invite(\n            event.sender, event.state_key, event.room_id\n        ):\n            raise SynapseError(\n                403, \"This user is not permitted to send invites to this server/user\"\n            )\n\n        membership = event.content.get(\"membership\")\n        if event.type != EventTypes.Member or membership != Membership.INVITE:\n            raise SynapseError(400, \"The event was not an m.room.member invite event\")\n\n        sender_domain = get_domain_from_id(event.sender)\n        if sender_domain != origin:\n            raise SynapseError(\n                400, \"The invite event was not from the server sending it\"\n            )\n\n        if not self.is_mine_id(event.state_key):\n            raise SynapseError(400, \"The invite event must be for this server\")\n\n        # block any attempts to invite the server notices mxid\n        if event.state_key == self._server_notices_mxid:\n            raise SynapseError(HTTPStatus.FORBIDDEN, \"Cannot invite this user\")\n\n        # keep a record of the room version, if we don't yet know it.\n        # (this may get overwritten if we later get a different room version in a\n        # join dance).\n        await self._maybe_store_room_on_outlier_membership(\n            room_id=event.room_id, room_version=room_version\n        )\n\n        event.internal_metadata.outlier = True\n        event.internal_metadata.out_of_band_membership = True\n\n        event.signatures.update(\n            compute_event_signature(\n                room_version,\n                event.get_pdu_json(),\n                self.hs.hostname,\n                self.hs.signing_key,\n            )\n        )\n\n        context = await self.state_handler.compute_event_context(event)\n        await self.persist_events_and_notify(event.room_id, [(event, context)])\n\n        return event\n\n    async def do_remotely_reject_invite(\n        self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict\n    ) -> Tuple[EventBase, int]:\n        origin, event, room_version = await self._make_and_verify_event(\n            target_hosts, room_id, user_id, \"leave\", content=content\n        )\n        # Mark as outlier as we don't have any state for this event; we're not\n        # even in the room.\n        event.internal_metadata.outlier = True\n        event.internal_metadata.out_of_band_membership = True\n\n        # Try the host that we successfully called /make_leave/ on first for\n        # the /send_leave/ request.\n        host_list = list(target_hosts)\n        try:\n            host_list.remove(origin)\n            host_list.insert(0, origin)\n        except ValueError:\n            pass\n\n        await self.federation_client.send_leave(host_list, event)\n\n        context = await self.state_handler.compute_event_context(event)\n        stream_id = await self.persist_events_and_notify(\n            event.room_id, [(event, context)]\n        )\n\n        return event, stream_id\n\n    async def _make_and_verify_event(\n        self,\n        target_hosts: Iterable[str],\n        room_id: str,\n        user_id: str,\n        membership: str,\n        content: JsonDict = {},\n        params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,\n    ) -> Tuple[str, EventBase, RoomVersion]:\n        (\n            origin,\n            event,\n            room_version,\n        ) = await self.federation_client.make_membership_event(\n            target_hosts, room_id, user_id, membership, content, params=params\n        )\n\n        logger.debug(\"Got response to make_%s: %s\", membership, event)\n\n        # We should assert some things.\n        # FIXME: Do this in a nicer way\n        assert event.type == EventTypes.Member\n        assert event.user_id == user_id\n        assert event.state_key == user_id\n        assert event.room_id == room_id\n        return origin, event, room_version\n\n    async def on_make_leave_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> EventBase:\n        \"\"\" We've received a /make_leave/ request, so we create a partial\n        leave event for the room and return that. We do *not* persist or\n        process it until the other server has signed it and sent it back.\n\n        Args:\n            origin: The (verified) server name of the requesting server.\n            room_id: Room to create leave event in\n            user_id: The user to create the leave for\n        \"\"\"\n        if get_domain_from_id(user_id) != origin:\n            logger.info(\n                \"Got /make_leave request for user %r from different origin %s, ignoring\",\n                user_id,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        room_version = await self.store.get_room_version_id(room_id)\n        builder = self.event_builder_factory.new(\n            room_version,\n            {\n                \"type\": EventTypes.Member,\n                \"content\": {\"membership\": Membership.LEAVE},\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": user_id,\n            },\n        )\n\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n\n        try:\n            # The remote hasn't signed it yet, obviously. We'll do the full checks\n            # when we get the event back in `on_send_leave_request`\n            await self.auth.check_from_context(\n                room_version, event, context, do_sig_check=False\n            )\n        except AuthError as e:\n            logger.warning(\"Failed to create new leave %r because %s\", event, e)\n            raise e\n\n        return event\n\n    async def on_send_leave_request(self, origin, pdu):\n        \"\"\" We have received a leave event for a room. Fully process it.\"\"\"\n        event = pdu\n\n        logger.debug(\n            \"on_send_leave_request: Got event: %s, signatures: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        if get_domain_from_id(event.sender) != origin:\n            logger.info(\n                \"Got /send_leave request for user %r from different origin %s\",\n                event.sender,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        event.internal_metadata.outlier = False\n\n        await self._handle_new_event(origin, event)\n\n        logger.debug(\n            \"on_send_leave_request: After _handle_new_event: %s, sigs: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        return None\n\n    async def get_state_for_pdu(self, room_id: str, event_id: str) -> List[EventBase]:\n        \"\"\"Returns the state at the event. i.e. not including said event.\n        \"\"\"\n\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        state_groups = await self.state_store.get_state_groups(room_id, [event_id])\n\n        if state_groups:\n            _, state = list(state_groups.items()).pop()\n            results = {(e.type, e.state_key): e for e in state}\n\n            if event.is_state():\n                # Get previous state\n                if \"replaces_state\" in event.unsigned:\n                    prev_id = event.unsigned[\"replaces_state\"]\n                    if prev_id != event.event_id:\n                        prev_event = await self.store.get_event(prev_id)\n                        results[(event.type, event.state_key)] = prev_event\n                else:\n                    del results[(event.type, event.state_key)]\n\n            res = list(results.values())\n            return res\n        else:\n            return []\n\n    async def get_state_ids_for_pdu(self, room_id: str, event_id: str) -> List[str]:\n        \"\"\"Returns the state at the event. i.e. not including said event.\n        \"\"\"\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        state_groups = await self.state_store.get_state_groups_ids(room_id, [event_id])\n\n        if state_groups:\n            _, state = list(state_groups.items()).pop()\n            results = state\n\n            if event.is_state():\n                # Get previous state\n                if \"replaces_state\" in event.unsigned:\n                    prev_id = event.unsigned[\"replaces_state\"]\n                    if prev_id != event.event_id:\n                        results[(event.type, event.state_key)] = prev_id\n                else:\n                    results.pop((event.type, event.state_key), None)\n\n            return list(results.values())\n        else:\n            return []\n\n    @log_function\n    async def on_backfill_request(\n        self, origin: str, room_id: str, pdu_list: List[str], limit: int\n    ) -> List[EventBase]:\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # Synapse asks for 100 events per backfill request. Do not allow more.\n        limit = min(limit, 100)\n\n        events = await self.store.get_backfill_events(room_id, pdu_list, limit)\n\n        events = await filter_events_for_server(self.storage, origin, events)\n\n        return events\n\n    @log_function\n    async def get_persisted_pdu(\n        self, origin: str, event_id: str\n    ) -> Optional[EventBase]:\n        \"\"\"Get an event from the database for the given server.\n\n        Args:\n            origin: hostname of server which is requesting the event; we\n               will check that the server is allowed to see it.\n            event_id: id of the event being requested\n\n        Returns:\n            None if we know nothing about the event; otherwise the (possibly-redacted) event.\n\n        Raises:\n            AuthError if the server is not currently in the room\n        \"\"\"\n        event = await self.store.get_event(\n            event_id, allow_none=True, allow_rejected=True\n        )\n\n        if event:\n            in_room = await self.auth.check_host_in_room(event.room_id, origin)\n            if not in_room:\n                raise AuthError(403, \"Host not in room.\")\n\n            events = await filter_events_for_server(self.storage, origin, [event])\n            event = events[0]\n            return event\n        else:\n            return None\n\n    async def get_min_depth_for_context(self, context):\n        return await self.store.get_min_depth(context)\n\n    async def _handle_new_event(\n        self, origin, event, state=None, auth_events=None, backfilled=False\n    ):\n        context = await self._prep_event(\n            origin, event, state=state, auth_events=auth_events, backfilled=backfilled\n        )\n\n        try:\n            if (\n                not event.internal_metadata.is_outlier()\n                and not backfilled\n                and not context.rejected\n            ):\n                await self.action_generator.handle_push_actions_for_event(\n                    event, context\n                )\n\n            await self.persist_events_and_notify(\n                event.room_id, [(event, context)], backfilled=backfilled\n            )\n        except Exception:\n            run_in_background(\n                self.store.remove_push_actions_from_staging, event.event_id\n            )\n            raise\n\n        return context\n\n    async def _handle_new_events(\n        self,\n        origin: str,\n        room_id: str,\n        event_infos: Iterable[_NewEventInfo],\n        backfilled: bool = False,\n    ) -> None:\n        \"\"\"Creates the appropriate contexts and persists events. The events\n        should not depend on one another, e.g. this should be used to persist\n        a bunch of outliers, but not a chunk of individual events that depend\n        on each other for state calculations.\n\n        Notifies about the events where appropriate.\n        \"\"\"\n\n        async def prep(ev_info: _NewEventInfo):\n            event = ev_info.event\n            with nested_logging_context(suffix=event.event_id):\n                res = await self._prep_event(\n                    origin,\n                    event,\n                    state=ev_info.state,\n                    auth_events=ev_info.auth_events,\n                    backfilled=backfilled,\n                )\n            return res\n\n        contexts = await make_deferred_yieldable(\n            defer.gatherResults(\n                [run_in_background(prep, ev_info) for ev_info in event_infos],\n                consumeErrors=True,\n            )\n        )\n\n        await self.persist_events_and_notify(\n            room_id,\n            [\n                (ev_info.event, context)\n                for ev_info, context in zip(event_infos, contexts)\n            ],\n            backfilled=backfilled,\n        )\n\n    async def _persist_auth_tree(\n        self,\n        origin: str,\n        room_id: str,\n        auth_events: List[EventBase],\n        state: List[EventBase],\n        event: EventBase,\n        room_version: RoomVersion,\n    ) -> int:\n        \"\"\"Checks the auth chain is valid (and passes auth checks) for the\n        state and event. Then persists the auth chain and state atomically.\n        Persists the event separately. Notifies about the persisted events\n        where appropriate.\n\n        Will attempt to fetch missing auth events.\n\n        Args:\n            origin: Where the events came from\n            room_id,\n            auth_events\n            state\n            event\n            room_version: The room version we expect this room to have, and\n                will raise if it doesn't match the version in the create event.\n        \"\"\"\n        events_to_context = {}\n        for e in itertools.chain(auth_events, state):\n            e.internal_metadata.outlier = True\n            ctx = await self.state_handler.compute_event_context(e)\n            events_to_context[e.event_id] = ctx\n\n        event_map = {\n            e.event_id: e for e in itertools.chain(auth_events, state, [event])\n        }\n\n        create_event = None\n        for e in auth_events:\n            if (e.type, e.state_key) == (EventTypes.Create, \"\"):\n                create_event = e\n                break\n\n        if create_event is None:\n            # If the state doesn't have a create event then the room is\n            # invalid, and it would fail auth checks anyway.\n            raise SynapseError(400, \"No create event in state\")\n\n        room_version_id = create_event.content.get(\n            \"room_version\", RoomVersions.V1.identifier\n        )\n\n        if room_version.identifier != room_version_id:\n            raise SynapseError(400, \"Room version mismatch\")\n\n        missing_auth_events = set()\n        for e in itertools.chain(auth_events, state, [event]):\n            for e_id in e.auth_event_ids():\n                if e_id not in event_map:\n                    missing_auth_events.add(e_id)\n\n        for e_id in missing_auth_events:\n            m_ev = await self.federation_client.get_pdu(\n                [origin], e_id, room_version=room_version, outlier=True, timeout=10000,\n            )\n            if m_ev and m_ev.event_id == e_id:\n                event_map[e_id] = m_ev\n            else:\n                logger.info(\"Failed to find auth event %r\", e_id)\n\n        for e in itertools.chain(auth_events, state, [event]):\n            auth_for_e = {\n                (event_map[e_id].type, event_map[e_id].state_key): event_map[e_id]\n                for e_id in e.auth_event_ids()\n                if e_id in event_map\n            }\n            if create_event:\n                auth_for_e[(EventTypes.Create, \"\")] = create_event\n\n            try:\n                event_auth.check(room_version, e, auth_events=auth_for_e)\n            except SynapseError as err:\n                # we may get SynapseErrors here as well as AuthErrors. For\n                # instance, there are a couple of (ancient) events in some\n                # rooms whose senders do not have the correct sigil; these\n                # cause SynapseErrors in auth.check. We don't want to give up\n                # the attempt to federate altogether in such cases.\n\n                logger.warning(\"Rejecting %s because %s\", e.event_id, err.msg)\n\n                if e == event:\n                    raise\n                events_to_context[e.event_id].rejected = RejectedReason.AUTH_ERROR\n\n        await self.persist_events_and_notify(\n            room_id,\n            [\n                (e, events_to_context[e.event_id])\n                for e in itertools.chain(auth_events, state)\n            ],\n        )\n\n        new_event_context = await self.state_handler.compute_event_context(\n            event, old_state=state\n        )\n\n        return await self.persist_events_and_notify(\n            room_id, [(event, new_event_context)]\n        )\n\n    async def _prep_event(\n        self,\n        origin: str,\n        event: EventBase,\n        state: Optional[Iterable[EventBase]],\n        auth_events: Optional[MutableStateMap[EventBase]],\n        backfilled: bool,\n    ) -> EventContext:\n        context = await self.state_handler.compute_event_context(event, old_state=state)\n\n        if not auth_events:\n            prev_state_ids = await context.get_prev_state_ids()\n            auth_events_ids = self.auth.compute_auth_events(\n                event, prev_state_ids, for_verification=True\n            )\n            auth_events_x = await self.store.get_events(auth_events_ids)\n            auth_events = {(e.type, e.state_key): e for e in auth_events_x.values()}\n\n        # This is a hack to fix some old rooms where the initial join event\n        # didn't reference the create event in its auth events.\n        if event.type == EventTypes.Member and not event.auth_event_ids():\n            if len(event.prev_event_ids()) == 1 and event.depth < 5:\n                c = await self.store.get_event(\n                    event.prev_event_ids()[0], allow_none=True\n                )\n                if c and c.type == EventTypes.Create:\n                    auth_events[(c.type, c.state_key)] = c\n\n        context = await self.do_auth(origin, event, context, auth_events=auth_events)\n\n        if not context.rejected:\n            await self._check_for_soft_fail(event, state, backfilled)\n\n        if event.type == EventTypes.GuestAccess and not context.rejected:\n            await self.maybe_kick_guest_users(event)\n\n        return context\n\n    async def _check_for_soft_fail(\n        self, event: EventBase, state: Optional[Iterable[EventBase]], backfilled: bool\n    ) -> None:\n        \"\"\"Checks if we should soft fail the event; if so, marks the event as\n        such.\n\n        Args:\n            event\n            state: The state at the event if we don't have all the event's prev events\n            backfilled: Whether the event is from backfill\n        \"\"\"\n        # For new (non-backfilled and non-outlier) events we check if the event\n        # passes auth based on the current state. If it doesn't then we\n        # \"soft-fail\" the event.\n        if backfilled or event.internal_metadata.is_outlier():\n            return\n\n        extrem_ids_list = await self.store.get_latest_event_ids_in_room(event.room_id)\n        extrem_ids = set(extrem_ids_list)\n        prev_event_ids = set(event.prev_event_ids())\n\n        if extrem_ids == prev_event_ids:\n            # If they're the same then the current state is the same as the\n            # state at the event, so no point rechecking auth for soft fail.\n            return\n\n        room_version = await self.store.get_room_version_id(event.room_id)\n        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]\n\n        # Calculate the \"current state\".\n        if state is not None:\n            # If we're explicitly given the state then we won't have all the\n            # prev events, and so we have a gap in the graph. In this case\n            # we want to be a little careful as we might have been down for\n            # a while and have an incorrect view of the current state,\n            # however we still want to do checks as gaps are easy to\n            # maliciously manufacture.\n            #\n            # So we use a \"current state\" that is actually a state\n            # resolution across the current forward extremities and the\n            # given state at the event. This should correctly handle cases\n            # like bans, especially with state res v2.\n\n            state_sets_d = await self.state_store.get_state_groups(\n                event.room_id, extrem_ids\n            )\n            state_sets = list(state_sets_d.values())  # type: List[Iterable[EventBase]]\n            state_sets.append(state)\n            current_states = await self.state_handler.resolve_events(\n                room_version, state_sets, event\n            )\n            current_state_ids = {\n                k: e.event_id for k, e in current_states.items()\n            }  # type: StateMap[str]\n        else:\n            current_state_ids = await self.state_handler.get_current_state_ids(\n                event.room_id, latest_event_ids=extrem_ids\n            )\n\n        logger.debug(\n            \"Doing soft-fail check for %s: state %s\", event.event_id, current_state_ids,\n        )\n\n        # Now check if event pass auth against said current state\n        auth_types = auth_types_for_event(event)\n        current_state_ids_list = [\n            e for k, e in current_state_ids.items() if k in auth_types\n        ]\n\n        auth_events_map = await self.store.get_events(current_state_ids_list)\n        current_auth_events = {\n            (e.type, e.state_key): e for e in auth_events_map.values()\n        }\n\n        try:\n            event_auth.check(room_version_obj, event, auth_events=current_auth_events)\n        except AuthError as e:\n            logger.warning(\"Soft-failing %r because %s\", event, e)\n            event.internal_metadata.soft_failed = True\n\n    async def on_query_auth(\n        self, origin, event_id, room_id, remote_auth_chain, rejects, missing\n    ):\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        # Just go through and process each event in `remote_auth_chain`. We\n        # don't want to fall into the trap of `missing` being wrong.\n        for e in remote_auth_chain:\n            try:\n                await self._handle_new_event(origin, e)\n            except AuthError:\n                pass\n\n        # Now get the current auth_chain for the event.\n        local_auth_chain = await self.store.get_auth_chain(\n            list(event.auth_event_ids()), include_given=True\n        )\n\n        # TODO: Check if we would now reject event_id. If so we need to tell\n        # everyone.\n\n        ret = await self.construct_auth_difference(local_auth_chain, remote_auth_chain)\n\n        logger.debug(\"on_query_auth returning: %s\", ret)\n\n        return ret\n\n    async def on_get_missing_events(\n        self, origin, room_id, earliest_events, latest_events, limit\n    ):\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # Only allow up to 20 events to be retrieved per request.\n        limit = min(limit, 20)\n\n        missing_events = await self.store.get_missing_events(\n            room_id=room_id,\n            earliest_events=earliest_events,\n            latest_events=latest_events,\n            limit=limit,\n        )\n\n        missing_events = await filter_events_for_server(\n            self.storage, origin, missing_events\n        )\n\n        return missing_events\n\n    async def do_auth(\n        self,\n        origin: str,\n        event: EventBase,\n        context: EventContext,\n        auth_events: MutableStateMap[EventBase],\n    ) -> EventContext:\n        \"\"\"\n\n        Args:\n            origin:\n            event:\n            context:\n            auth_events:\n                Map from (event_type, state_key) to event\n\n                Normally, our calculated auth_events based on the state of the room\n                at the event's position in the DAG, though occasionally (eg if the\n                event is an outlier), may be the auth events claimed by the remote\n                server.\n\n                Also NB that this function adds entries to it.\n        Returns:\n            updated context object\n        \"\"\"\n        room_version = await self.store.get_room_version_id(event.room_id)\n        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]\n\n        try:\n            context = await self._update_auth_events_and_context_for_auth(\n                origin, event, context, auth_events\n            )\n        except Exception:\n            # We don't really mind if the above fails, so lets not fail\n            # processing if it does. However, it really shouldn't fail so\n            # let's still log as an exception since we'll still want to fix\n            # any bugs.\n            logger.exception(\n                \"Failed to double check auth events for %s with remote. \"\n                \"Ignoring failure and continuing processing of event.\",\n                event.event_id,\n            )\n\n        try:\n            event_auth.check(room_version_obj, event, auth_events=auth_events)\n        except AuthError as e:\n            logger.warning(\"Failed auth resolution for %r because %s\", event, e)\n            context.rejected = RejectedReason.AUTH_ERROR\n\n        return context\n\n    async def _update_auth_events_and_context_for_auth(\n        self,\n        origin: str,\n        event: EventBase,\n        context: EventContext,\n        auth_events: MutableStateMap[EventBase],\n    ) -> EventContext:\n        \"\"\"Helper for do_auth. See there for docs.\n\n        Checks whether a given event has the expected auth events. If it\n        doesn't then we talk to the remote server to compare state to see if\n        we can come to a consensus (e.g. if one server missed some valid\n        state).\n\n        This attempts to resolve any potential divergence of state between\n        servers, but is not essential and so failures should not block further\n        processing of the event.\n\n        Args:\n            origin:\n            event:\n            context:\n\n            auth_events:\n                Map from (event_type, state_key) to event\n\n                Normally, our calculated auth_events based on the state of the room\n                at the event's position in the DAG, though occasionally (eg if the\n                event is an outlier), may be the auth events claimed by the remote\n                server.\n\n                Also NB that this function adds entries to it.\n\n        Returns:\n            updated context\n        \"\"\"\n        event_auth_events = set(event.auth_event_ids())\n\n        # missing_auth is the set of the event's auth_events which we don't yet have\n        # in auth_events.\n        missing_auth = event_auth_events.difference(\n            e.event_id for e in auth_events.values()\n        )\n\n        # if we have missing events, we need to fetch those events from somewhere.\n        #\n        # we start by checking if they are in the store, and then try calling /event_auth/.\n        if missing_auth:\n            have_events = await self.store.have_seen_events(missing_auth)\n            logger.debug(\"Events %s are in the store\", have_events)\n            missing_auth.difference_update(have_events)\n\n        if missing_auth:\n            # If we don't have all the auth events, we need to get them.\n            logger.info(\"auth_events contains unknown events: %s\", missing_auth)\n            try:\n                try:\n                    remote_auth_chain = await self.federation_client.get_event_auth(\n                        origin, event.room_id, event.event_id\n                    )\n                except RequestSendFailed as e1:\n                    # The other side isn't around or doesn't implement the\n                    # endpoint, so lets just bail out.\n                    logger.info(\"Failed to get event auth from remote: %s\", e1)\n                    return context\n\n                seen_remotes = await self.store.have_seen_events(\n                    [e.event_id for e in remote_auth_chain]\n                )\n\n                for e in remote_auth_chain:\n                    if e.event_id in seen_remotes:\n                        continue\n\n                    if e.event_id == event.event_id:\n                        continue\n\n                    try:\n                        auth_ids = e.auth_event_ids()\n                        auth = {\n                            (e.type, e.state_key): e\n                            for e in remote_auth_chain\n                            if e.event_id in auth_ids or e.type == EventTypes.Create\n                        }\n                        e.internal_metadata.outlier = True\n\n                        logger.debug(\n                            \"do_auth %s missing_auth: %s\", event.event_id, e.event_id\n                        )\n                        await self._handle_new_event(origin, e, auth_events=auth)\n\n                        if e.event_id in event_auth_events:\n                            auth_events[(e.type, e.state_key)] = e\n                    except AuthError:\n                        pass\n\n            except Exception:\n                logger.exception(\"Failed to get auth chain\")\n\n        if event.internal_metadata.is_outlier():\n            # XXX: given that, for an outlier, we'll be working with the\n            # event's *claimed* auth events rather than those we calculated:\n            # (a) is there any point in this test, since different_auth below will\n            # obviously be empty\n            # (b) alternatively, why don't we do it earlier?\n            logger.info(\"Skipping auth_event fetch for outlier\")\n            return context\n\n        different_auth = event_auth_events.difference(\n            e.event_id for e in auth_events.values()\n        )\n\n        if not different_auth:\n            return context\n\n        logger.info(\n            \"auth_events refers to events which are not in our calculated auth \"\n            \"chain: %s\",\n            different_auth,\n        )\n\n        # XXX: currently this checks for redactions but I'm not convinced that is\n        # necessary?\n        different_events = await self.store.get_events_as_list(different_auth)\n\n        for d in different_events:\n            if d.room_id != event.room_id:\n                logger.warning(\n                    \"Event %s refers to auth_event %s which is in a different room\",\n                    event.event_id,\n                    d.event_id,\n                )\n\n                # don't attempt to resolve the claimed auth events against our own\n                # in this case: just use our own auth events.\n                #\n                # XXX: should we reject the event in this case? It feels like we should,\n                # but then shouldn't we also do so if we've failed to fetch any of the\n                # auth events?\n                return context\n\n        # now we state-resolve between our own idea of the auth events, and the remote's\n        # idea of them.\n\n        local_state = auth_events.values()\n        remote_auth_events = dict(auth_events)\n        remote_auth_events.update({(d.type, d.state_key): d for d in different_events})\n        remote_state = remote_auth_events.values()\n\n        room_version = await self.store.get_room_version_id(event.room_id)\n        new_state = await self.state_handler.resolve_events(\n            room_version, (local_state, remote_state), event\n        )\n\n        logger.info(\n            \"After state res: updating auth_events with new state %s\",\n            {\n                (d.type, d.state_key): d.event_id\n                for d in new_state.values()\n                if auth_events.get((d.type, d.state_key)) != d\n            },\n        )\n\n        auth_events.update(new_state)\n\n        context = await self._update_context_for_auth_events(\n            event, context, auth_events\n        )\n\n        return context\n\n    async def _update_context_for_auth_events(\n        self, event: EventBase, context: EventContext, auth_events: StateMap[EventBase]\n    ) -> EventContext:\n        \"\"\"Update the state_ids in an event context after auth event resolution,\n        storing the changes as a new state group.\n\n        Args:\n            event: The event we're handling the context for\n\n            context: initial event context\n\n            auth_events: Events to update in the event context.\n\n        Returns:\n            new event context\n        \"\"\"\n        # exclude the state key of the new event from the current_state in the context.\n        if event.is_state():\n            event_key = (event.type, event.state_key)  # type: Optional[Tuple[str, str]]\n        else:\n            event_key = None\n        state_updates = {\n            k: a.event_id for k, a in auth_events.items() if k != event_key\n        }\n\n        current_state_ids = await context.get_current_state_ids()\n        current_state_ids = dict(current_state_ids)  # type: ignore\n\n        current_state_ids.update(state_updates)\n\n        prev_state_ids = await context.get_prev_state_ids()\n        prev_state_ids = dict(prev_state_ids)\n\n        prev_state_ids.update({k: a.event_id for k, a in auth_events.items()})\n\n        # create a new state group as a delta from the existing one.\n        prev_group = context.state_group\n        state_group = await self.state_store.store_state_group(\n            event.event_id,\n            event.room_id,\n            prev_group=prev_group,\n            delta_ids=state_updates,\n            current_state_ids=current_state_ids,\n        )\n\n        return EventContext.with_state(\n            state_group=state_group,\n            state_group_before_event=context.state_group_before_event,\n            current_state_ids=current_state_ids,\n            prev_state_ids=prev_state_ids,\n            prev_group=prev_group,\n            delta_ids=state_updates,\n        )\n\n    async def construct_auth_difference(\n        self, local_auth: Iterable[EventBase], remote_auth: Iterable[EventBase]\n    ) -> Dict:\n        \"\"\" Given a local and remote auth chain, find the differences. This\n        assumes that we have already processed all events in remote_auth\n\n        Params:\n            local_auth (list)\n            remote_auth (list)\n\n        Returns:\n            dict\n        \"\"\"\n\n        logger.debug(\"construct_auth_difference Start!\")\n\n        # TODO: Make sure we are OK with local_auth or remote_auth having more\n        # auth events in them than strictly necessary.\n\n        def sort_fun(ev):\n            return ev.depth, ev.event_id\n\n        logger.debug(\"construct_auth_difference after sort_fun!\")\n\n        # We find the differences by starting at the \"bottom\" of each list\n        # and iterating up on both lists. The lists are ordered by depth and\n        # then event_id, we iterate up both lists until we find the event ids\n        # don't match. Then we look at depth/event_id to see which side is\n        # missing that event, and iterate only up that list. Repeat.\n\n        remote_list = list(remote_auth)\n        remote_list.sort(key=sort_fun)\n\n        local_list = list(local_auth)\n        local_list.sort(key=sort_fun)\n\n        local_iter = iter(local_list)\n        remote_iter = iter(remote_list)\n\n        logger.debug(\"construct_auth_difference before get_next!\")\n\n        def get_next(it, opt=None):\n            try:\n                return next(it)\n            except Exception:\n                return opt\n\n        current_local = get_next(local_iter)\n        current_remote = get_next(remote_iter)\n\n        logger.debug(\"construct_auth_difference before while\")\n\n        missing_remotes = []\n        missing_locals = []\n        while current_local or current_remote:\n            if current_remote is None:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n                continue\n\n            if current_local is None:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n            if current_local.event_id == current_remote.event_id:\n                current_local = get_next(local_iter)\n                current_remote = get_next(remote_iter)\n                continue\n\n            if current_local.depth < current_remote.depth:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n                continue\n\n            if current_local.depth > current_remote.depth:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n            # They have the same depth, so we fall back to the event_id order\n            if current_local.event_id < current_remote.event_id:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n\n            if current_local.event_id > current_remote.event_id:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n        logger.debug(\"construct_auth_difference after while\")\n\n        # missing locals should be sent to the server\n        # We should find why we are missing remotes, as they will have been\n        # rejected.\n\n        # Remove events from missing_remotes if they are referencing a missing\n        # remote. We only care about the \"root\" rejected ones.\n        missing_remote_ids = [e.event_id for e in missing_remotes]\n        base_remote_rejected = list(missing_remotes)\n        for e in missing_remotes:\n            for e_id in e.auth_event_ids():\n                if e_id in missing_remote_ids:\n                    try:\n                        base_remote_rejected.remove(e)\n                    except ValueError:\n                        pass\n\n        reason_map = {}\n\n        for e in base_remote_rejected:\n            reason = await self.store.get_rejection_reason(e.event_id)\n            if reason is None:\n                # TODO: e is not in the current state, so we should\n                # construct some proof of that.\n                continue\n\n            reason_map[e.event_id] = reason\n\n        logger.debug(\"construct_auth_difference returning\")\n\n        return {\n            \"auth_chain\": local_auth,\n            \"rejects\": {\n                e.event_id: {\"reason\": reason_map[e.event_id], \"proof\": None}\n                for e in base_remote_rejected\n            },\n            \"missing\": [e.event_id for e in missing_locals],\n        }\n\n    @log_function\n    async def exchange_third_party_invite(\n        self, sender_user_id, target_user_id, room_id, signed\n    ):\n        third_party_invite = {\"signed\": signed}\n\n        event_dict = {\n            \"type\": EventTypes.Member,\n            \"content\": {\n                \"membership\": Membership.INVITE,\n                \"third_party_invite\": third_party_invite,\n            },\n            \"room_id\": room_id,\n            \"sender\": sender_user_id,\n            \"state_key\": target_user_id,\n        }\n\n        if await self.auth.check_host_in_room(room_id, self.hs.hostname):\n            room_version = await self.store.get_room_version_id(room_id)\n            builder = self.event_builder_factory.new(room_version, event_dict)\n\n            EventValidator().validate_builder(builder)\n            event, context = await self.event_creation_handler.create_new_client_event(\n                builder=builder\n            )\n\n            event, context = await self.add_display_name_to_third_party_invite(\n                room_version, event_dict, event, context\n            )\n\n            EventValidator().validate_new(event, self.config)\n\n            # We need to tell the transaction queue to send this out, even\n            # though the sender isn't a local user.\n            event.internal_metadata.send_on_behalf_of = self.hs.hostname\n\n            try:\n                await self.auth.check_from_context(room_version, event, context)\n            except AuthError as e:\n                logger.warning(\"Denying new third party invite %r because %s\", event, e)\n                raise e\n\n            await self._check_signature(event, context)\n\n            # We retrieve the room member handler here as to not cause a cyclic dependency\n            member_handler = self.hs.get_room_member_handler()\n            await member_handler.send_membership_event(None, event, context)\n        else:\n            destinations = {x.split(\":\", 1)[-1] for x in (sender_user_id, room_id)}\n            await self.federation_client.forward_third_party_invite(\n                destinations, room_id, event_dict\n            )\n\n    async def on_exchange_third_party_invite_request(\n        self, event_dict: JsonDict\n    ) -> None:\n        \"\"\"Handle an exchange_third_party_invite request from a remote server\n\n        The remote server will call this when it wants to turn a 3pid invite\n        into a normal m.room.member invite.\n\n        Args:\n            event_dict: Dictionary containing the event body.\n\n        \"\"\"\n        assert_params_in_dict(event_dict, [\"room_id\"])\n        room_version = await self.store.get_room_version_id(event_dict[\"room_id\"])\n\n        # NB: event_dict has a particular specced format we might need to fudge\n        # if we change event formats too much.\n        builder = self.event_builder_factory.new(room_version, event_dict)\n\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n        event, context = await self.add_display_name_to_third_party_invite(\n            room_version, event_dict, event, context\n        )\n\n        try:\n            await self.auth.check_from_context(room_version, event, context)\n        except AuthError as e:\n            logger.warning(\"Denying third party invite %r because %s\", event, e)\n            raise e\n        await self._check_signature(event, context)\n\n        # We need to tell the transaction queue to send this out, even\n        # though the sender isn't a local user.\n        event.internal_metadata.send_on_behalf_of = get_domain_from_id(event.sender)\n\n        # We retrieve the room member handler here as to not cause a cyclic dependency\n        member_handler = self.hs.get_room_member_handler()\n        await member_handler.send_membership_event(None, event, context)\n\n    async def add_display_name_to_third_party_invite(\n        self, room_version, event_dict, event, context\n    ):\n        key = (\n            EventTypes.ThirdPartyInvite,\n            event.content[\"third_party_invite\"][\"signed\"][\"token\"],\n        )\n        original_invite = None\n        prev_state_ids = await context.get_prev_state_ids()\n        original_invite_id = prev_state_ids.get(key)\n        if original_invite_id:\n            original_invite = await self.store.get_event(\n                original_invite_id, allow_none=True\n            )\n        if original_invite:\n            # If the m.room.third_party_invite event's content is empty, it means the\n            # invite has been revoked. In this case, we don't have to raise an error here\n            # because the auth check will fail on the invite (because it's not able to\n            # fetch public keys from the m.room.third_party_invite event's content, which\n            # is empty).\n            display_name = original_invite.content.get(\"display_name\")\n            event_dict[\"content\"][\"third_party_invite\"][\"display_name\"] = display_name\n        else:\n            logger.info(\n                \"Could not find invite event for third_party_invite: %r\", event_dict\n            )\n            # We don't discard here as this is not the appropriate place to do\n            # auth checks. If we need the invite and don't have it then the\n            # auth check code will explode appropriately.\n\n        builder = self.event_builder_factory.new(room_version, event_dict)\n        EventValidator().validate_builder(builder)\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n        EventValidator().validate_new(event, self.config)\n        return (event, context)\n\n    async def _check_signature(self, event, context):\n        \"\"\"\n        Checks that the signature in the event is consistent with its invite.\n\n        Args:\n            event (Event): The m.room.member event to check\n            context (EventContext):\n\n        Raises:\n            AuthError: if signature didn't match any keys, or key has been\n                revoked,\n            SynapseError: if a transient error meant a key couldn't be checked\n                for revocation.\n        \"\"\"\n        signed = event.content[\"third_party_invite\"][\"signed\"]\n        token = signed[\"token\"]\n\n        prev_state_ids = await context.get_prev_state_ids()\n        invite_event_id = prev_state_ids.get((EventTypes.ThirdPartyInvite, token))\n\n        invite_event = None\n        if invite_event_id:\n            invite_event = await self.store.get_event(invite_event_id, allow_none=True)\n\n        if not invite_event:\n            raise AuthError(403, \"Could not find invite\")\n\n        logger.debug(\"Checking auth on event %r\", event.content)\n\n        last_exception = None  # type: Optional[Exception]\n\n        # for each public key in the 3pid invite event\n        for public_key_object in self.hs.get_auth().get_public_keys(invite_event):\n            try:\n                # for each sig on the third_party_invite block of the actual invite\n                for server, signature_block in signed[\"signatures\"].items():\n                    for key_name, encoded_signature in signature_block.items():\n                        if not key_name.startswith(\"ed25519:\"):\n                            continue\n\n                        logger.debug(\n                            \"Attempting to verify sig with key %s from %r \"\n                            \"against pubkey %r\",\n                            key_name,\n                            server,\n                            public_key_object,\n                        )\n\n                        try:\n                            public_key = public_key_object[\"public_key\"]\n                            verify_key = decode_verify_key_bytes(\n                                key_name, decode_base64(public_key)\n                            )\n                            verify_signed_json(signed, server, verify_key)\n                            logger.debug(\n                                \"Successfully verified sig with key %s from %r \"\n                                \"against pubkey %r\",\n                                key_name,\n                                server,\n                                public_key_object,\n                            )\n                        except Exception:\n                            logger.info(\n                                \"Failed to verify sig with key %s from %r \"\n                                \"against pubkey %r\",\n                                key_name,\n                                server,\n                                public_key_object,\n                            )\n                            raise\n                        try:\n                            if \"key_validity_url\" in public_key_object:\n                                await self._check_key_revocation(\n                                    public_key, public_key_object[\"key_validity_url\"]\n                                )\n                        except Exception:\n                            logger.info(\n                                \"Failed to query key_validity_url %s\",\n                                public_key_object[\"key_validity_url\"],\n                            )\n                            raise\n                        return\n            except Exception as e:\n                last_exception = e\n\n        if last_exception is None:\n            # we can only get here if get_public_keys() returned an empty list\n            # TODO: make this better\n            raise RuntimeError(\"no public key in invite event\")\n\n        raise last_exception\n\n    async def _check_key_revocation(self, public_key, url):\n        \"\"\"\n        Checks whether public_key has been revoked.\n\n        Args:\n            public_key (str): base-64 encoded public key.\n            url (str): Key revocation URL.\n\n        Raises:\n            AuthError: if they key has been revoked.\n            SynapseError: if a transient error meant a key couldn't be checked\n                for revocation.\n        \"\"\"\n        try:\n            response = await self.http_client.get_json(url, {\"public_key\": public_key})\n        except Exception:\n            raise SynapseError(502, \"Third party certificate could not be checked\")\n        if \"valid\" not in response or not response[\"valid\"]:\n            raise AuthError(403, \"Third party certificate was invalid\")\n\n    async def persist_events_and_notify(\n        self,\n        room_id: str,\n        event_and_contexts: Sequence[Tuple[EventBase, EventContext]],\n        backfilled: bool = False,\n    ) -> int:\n        \"\"\"Persists events and tells the notifier/pushers about them, if\n        necessary.\n\n        Args:\n            room_id: The room ID of events being persisted.\n            event_and_contexts: Sequence of events with their associated\n                context that should be persisted. All events must belong to\n                the same room.\n            backfilled: Whether these events are a result of\n                backfilling or not\n        \"\"\"\n        instance = self.config.worker.events_shard_config.get_instance(room_id)\n        if instance != self._instance_name:\n            result = await self._send_events(\n                instance_name=instance,\n                store=self.store,\n                room_id=room_id,\n                event_and_contexts=event_and_contexts,\n                backfilled=backfilled,\n            )\n            return result[\"max_stream_id\"]\n        else:\n            assert self.storage.persistence\n\n            # Note that this returns the events that were persisted, which may not be\n            # the same as were passed in if some were deduplicated due to transaction IDs.\n            events, max_stream_token = await self.storage.persistence.persist_events(\n                event_and_contexts, backfilled=backfilled\n            )\n\n            if self._ephemeral_messages_enabled:\n                for event in events:\n                    # If there's an expiry timestamp on the event, schedule its expiry.\n                    self._message_handler.maybe_schedule_expiry(event)\n\n            if not backfilled:  # Never notify for backfilled events\n                for event in events:\n                    await self._notify_persisted_event(event, max_stream_token)\n\n            return max_stream_token.stream\n\n    async def _notify_persisted_event(\n        self, event: EventBase, max_stream_token: RoomStreamToken\n    ) -> None:\n        \"\"\"Checks to see if notifier/pushers should be notified about the\n        event or not.\n\n        Args:\n            event:\n            max_stream_id: The max_stream_id returned by persist_events\n        \"\"\"\n\n        extra_users = []\n        if event.type == EventTypes.Member:\n            target_user_id = event.state_key\n\n            # We notify for memberships if its an invite for one of our\n            # users\n            if event.internal_metadata.is_outlier():\n                if event.membership != Membership.INVITE:\n                    if not self.is_mine_id(target_user_id):\n                        return\n\n            target_user = UserID.from_string(target_user_id)\n            extra_users.append(target_user)\n        elif event.internal_metadata.is_outlier():\n            return\n\n        # the event has been persisted so it should have a stream ordering.\n        assert event.internal_metadata.stream_ordering\n\n        event_pos = PersistedEventPosition(\n            self._instance_name, event.internal_metadata.stream_ordering\n        )\n        self.notifier.on_new_room_event(\n            event, event_pos, max_stream_token, extra_users=extra_users\n        )\n\n    async def _clean_room_for_join(self, room_id: str) -> None:\n        \"\"\"Called to clean up any data in DB for a given room, ready for the\n        server to join the room.\n\n        Args:\n            room_id\n        \"\"\"\n        if self.config.worker_app:\n            await self._clean_room_for_join_client(room_id)\n        else:\n            await self.store.clean_room_for_join(room_id)\n\n    async def get_room_complexity(\n        self, remote_room_hosts: List[str], room_id: str\n    ) -> Optional[dict]:\n        \"\"\"\n        Fetch the complexity of a remote room over federation.\n\n        Args:\n            remote_room_hosts (list[str]): The remote servers to ask.\n            room_id (str): The room ID to ask about.\n\n        Returns:\n            Dict contains the complexity\n            metric versions, while None means we could not fetch the complexity.\n        \"\"\"\n\n        for host in remote_room_hosts:\n            res = await self.federation_client.get_room_complexity(host, room_id)\n\n            # We got a result, return it.\n            if res:\n                return res\n\n        # We fell off the bottom, couldn't get the complexity from anyone. Oh\n        # well.\n        return None", "target": 0}], "function_after": [{"function": "class FederationHandler(BaseHandler):\n    \"\"\"Handles events that originated from federation.\n        Responsible for:\n        a) handling received Pdus before handing them on as Events to the rest\n        of the homeserver (including auth and state conflict resolutions)\n        b) converting events that were produced by local clients that may need\n        to be sent to remote homeservers.\n        c) doing the necessary dances to invite remote users and join remote\n        rooms.\n    \"\"\"\n\n    def __init__(self, hs: \"HomeServer\"):\n        super().__init__(hs)\n\n        self.hs = hs\n\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.state_store = self.storage.state\n        self.federation_client = hs.get_federation_client()\n        self.state_handler = hs.get_state_handler()\n        self._state_resolution_handler = hs.get_state_resolution_handler()\n        self.server_name = hs.hostname\n        self.keyring = hs.get_keyring()\n        self.action_generator = hs.get_action_generator()\n        self.is_mine_id = hs.is_mine_id\n        self.spam_checker = hs.get_spam_checker()\n        self.event_creation_handler = hs.get_event_creation_handler()\n        self._message_handler = hs.get_message_handler()\n        self._server_notices_mxid = hs.config.server_notices_mxid\n        self.config = hs.config\n        self.http_client = hs.get_proxied_blacklisted_http_client()\n        self._instance_name = hs.get_instance_name()\n        self._replication = hs.get_replication_data_handler()\n\n        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)\n        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(\n            hs\n        )\n\n        if hs.config.worker_app:\n            self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(\n                hs\n            )\n            self._maybe_store_room_on_outlier_membership = ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(\n                hs\n            )\n        else:\n            self._device_list_updater = hs.get_device_handler().device_list_updater\n            self._maybe_store_room_on_outlier_membership = (\n                self.store.maybe_store_room_on_outlier_membership\n            )\n\n        # When joining a room we need to queue any events for that room up.\n        # For each room, a list of (pdu, origin) tuples.\n        self.room_queues = {}  # type: Dict[str, List[Tuple[EventBase, str]]]\n        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")\n\n        self.third_party_event_rules = hs.get_third_party_event_rules()\n\n        self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages\n\n    async def on_receive_pdu(self, origin, pdu, sent_to_us_directly=False) -> None:\n        \"\"\" Process a PDU received via a federation /send/ transaction, or\n        via backfill of missing prev_events\n\n        Args:\n            origin (str): server which initiated the /send/ transaction. Will\n                be used to fetch missing events or state.\n            pdu (FrozenEvent): received PDU\n            sent_to_us_directly (bool): True if this event was pushed to us; False if\n                we pulled it as the result of a missing prev_event.\n        \"\"\"\n\n        room_id = pdu.room_id\n        event_id = pdu.event_id\n\n        logger.info(\"handling received PDU: %s\", pdu)\n\n        # We reprocess pdus when we have seen them only as outliers\n        existing = await self.store.get_event(\n            event_id, allow_none=True, allow_rejected=True\n        )\n\n        # FIXME: Currently we fetch an event again when we already have it\n        # if it has been marked as an outlier.\n\n        already_seen = existing and (\n            not existing.internal_metadata.is_outlier()\n            or pdu.internal_metadata.is_outlier()\n        )\n        if already_seen:\n            logger.debug(\"[%s %s]: Already seen pdu\", room_id, event_id)\n            return\n\n        # do some initial sanity-checking of the event. In particular, make\n        # sure it doesn't have hundreds of prev_events or auth_events, which\n        # could cause a huge state resolution or cascade of event fetches.\n        try:\n            self._sanity_check_event(pdu)\n        except SynapseError as err:\n            logger.warning(\n                \"[%s %s] Received event failed sanity checks\", room_id, event_id\n            )\n            raise FederationError(\"ERROR\", err.code, err.msg, affected=pdu.event_id)\n\n        # If we are currently in the process of joining this room, then we\n        # queue up events for later processing.\n        if room_id in self.room_queues:\n            logger.info(\n                \"[%s %s] Queuing PDU from %s for now: join in progress\",\n                room_id,\n                event_id,\n                origin,\n            )\n            self.room_queues[room_id].append((pdu, origin))\n            return\n\n        # If we're not in the room just ditch the event entirely. This is\n        # probably an old server that has come back and thinks we're still in\n        # the room (or we've been rejoined to the room by a state reset).\n        #\n        # Note that if we were never in the room then we would have already\n        # dropped the event, since we wouldn't know the room version.\n        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)\n        if not is_in_room:\n            logger.info(\n                \"[%s %s] Ignoring PDU from %s as we're not in the room\",\n                room_id,\n                event_id,\n                origin,\n            )\n            return None\n\n        state = None\n\n        # Get missing pdus if necessary.\n        if not pdu.internal_metadata.is_outlier():\n            # We only backfill backwards to the min depth.\n            min_depth = await self.get_min_depth_for_context(pdu.room_id)\n\n            logger.debug(\"[%s %s] min_depth: %d\", room_id, event_id, min_depth)\n\n            prevs = set(pdu.prev_event_ids())\n            seen = await self.store.have_events_in_timeline(prevs)\n\n            if min_depth is not None and pdu.depth < min_depth:\n                # This is so that we don't notify the user about this\n                # message, to work around the fact that some events will\n                # reference really really old events we really don't want to\n                # send to the clients.\n                pdu.internal_metadata.outlier = True\n            elif min_depth is not None and pdu.depth > min_depth:\n                missing_prevs = prevs - seen\n                if sent_to_us_directly and missing_prevs:\n                    # If we're missing stuff, ensure we only fetch stuff one\n                    # at a time.\n                    logger.info(\n                        \"[%s %s] Acquiring room lock to fetch %d missing prev_events: %s\",\n                        room_id,\n                        event_id,\n                        len(missing_prevs),\n                        shortstr(missing_prevs),\n                    )\n                    with (await self._room_pdu_linearizer.queue(pdu.room_id)):\n                        logger.info(\n                            \"[%s %s] Acquired room lock to fetch %d missing prev_events\",\n                            room_id,\n                            event_id,\n                            len(missing_prevs),\n                        )\n\n                        try:\n                            await self._get_missing_events_for_pdu(\n                                origin, pdu, prevs, min_depth\n                            )\n                        except Exception as e:\n                            raise Exception(\n                                \"Error fetching missing prev_events for %s: %s\"\n                                % (event_id, e)\n                            ) from e\n\n                        # Update the set of things we've seen after trying to\n                        # fetch the missing stuff\n                        seen = await self.store.have_events_in_timeline(prevs)\n\n                        if not prevs - seen:\n                            logger.info(\n                                \"[%s %s] Found all missing prev_events\",\n                                room_id,\n                                event_id,\n                            )\n\n            if prevs - seen:\n                # We've still not been able to get all of the prev_events for this event.\n                #\n                # In this case, we need to fall back to asking another server in the\n                # federation for the state at this event. That's ok provided we then\n                # resolve the state against other bits of the DAG before using it (which\n                # will ensure that you can't just take over a room by sending an event,\n                # withholding its prev_events, and declaring yourself to be an admin in\n                # the subsequent state request).\n                #\n                # Now, if we're pulling this event as a missing prev_event, then clearly\n                # this event is not going to become the only forward-extremity and we are\n                # guaranteed to resolve its state against our existing forward\n                # extremities, so that should be fine.\n                #\n                # On the other hand, if this event was pushed to us, it is possible for\n                # it to become the only forward-extremity in the room, and we would then\n                # trust its state to be the state for the whole room. This is very bad.\n                # Further, if the event was pushed to us, there is no excuse for us not to\n                # have all the prev_events. We therefore reject any such events.\n                #\n                # XXX this really feels like it could/should be merged with the above,\n                # but there is an interaction with min_depth that I'm not really\n                # following.\n\n                if sent_to_us_directly:\n                    logger.warning(\n                        \"[%s %s] Rejecting: failed to fetch %d prev events: %s\",\n                        room_id,\n                        event_id,\n                        len(prevs - seen),\n                        shortstr(prevs - seen),\n                    )\n                    raise FederationError(\n                        \"ERROR\",\n                        403,\n                        (\n                            \"Your server isn't divulging details about prev_events \"\n                            \"referenced in this event.\"\n                        ),\n                        affected=pdu.event_id,\n                    )\n\n                logger.info(\n                    \"Event %s is missing prev_events: calculating state for a \"\n                    \"backwards extremity\",\n                    event_id,\n                )\n\n                # Calculate the state after each of the previous events, and\n                # resolve them to find the correct state at the current event.\n                event_map = {event_id: pdu}\n                try:\n                    # Get the state of the events we know about\n                    ours = await self.state_store.get_state_groups_ids(room_id, seen)\n\n                    # state_maps is a list of mappings from (type, state_key) to event_id\n                    state_maps = list(ours.values())  # type: List[StateMap[str]]\n\n                    # we don't need this any more, let's delete it.\n                    del ours\n\n                    # Ask the remote server for the states we don't\n                    # know about\n                    for p in prevs - seen:\n                        logger.info(\n                            \"Requesting state at missing prev_event %s\", event_id,\n                        )\n\n                        with nested_logging_context(p):\n                            # note that if any of the missing prevs share missing state or\n                            # auth events, the requests to fetch those events are deduped\n                            # by the get_pdu_cache in federation_client.\n                            (remote_state, _,) = await self._get_state_for_room(\n                                origin, room_id, p, include_event_in_state=True\n                            )\n\n                            remote_state_map = {\n                                (x.type, x.state_key): x.event_id for x in remote_state\n                            }\n                            state_maps.append(remote_state_map)\n\n                            for x in remote_state:\n                                event_map[x.event_id] = x\n\n                    room_version = await self.store.get_room_version_id(room_id)\n                    state_map = await self._state_resolution_handler.resolve_events_with_store(\n                        room_id,\n                        room_version,\n                        state_maps,\n                        event_map,\n                        state_res_store=StateResolutionStore(self.store),\n                    )\n\n                    # We need to give _process_received_pdu the actual state events\n                    # rather than event ids, so generate that now.\n\n                    # First though we need to fetch all the events that are in\n                    # state_map, so we can build up the state below.\n                    evs = await self.store.get_events(\n                        list(state_map.values()),\n                        get_prev_content=False,\n                        redact_behaviour=EventRedactBehaviour.AS_IS,\n                    )\n                    event_map.update(evs)\n\n                    state = [event_map[e] for e in state_map.values()]\n                except Exception:\n                    logger.warning(\n                        \"[%s %s] Error attempting to resolve state at missing \"\n                        \"prev_events\",\n                        room_id,\n                        event_id,\n                        exc_info=True,\n                    )\n                    raise FederationError(\n                        \"ERROR\",\n                        403,\n                        \"We can't get valid state history.\",\n                        affected=event_id,\n                    )\n\n        await self._process_received_pdu(origin, pdu, state=state)\n\n    async def _get_missing_events_for_pdu(self, origin, pdu, prevs, min_depth):\n        \"\"\"\n        Args:\n            origin (str): Origin of the pdu. Will be called to get the missing events\n            pdu: received pdu\n            prevs (set(str)): List of event ids which we are missing\n            min_depth (int): Minimum depth of events to return.\n        \"\"\"\n\n        room_id = pdu.room_id\n        event_id = pdu.event_id\n\n        seen = await self.store.have_events_in_timeline(prevs)\n\n        if not prevs - seen:\n            return\n\n        latest_list = await self.store.get_latest_event_ids_in_room(room_id)\n\n        # We add the prev events that we have seen to the latest\n        # list to ensure the remote server doesn't give them to us\n        latest = set(latest_list)\n        latest |= seen\n\n        logger.info(\n            \"[%s %s]: Requesting missing events between %s and %s\",\n            room_id,\n            event_id,\n            shortstr(latest),\n            event_id,\n        )\n\n        # XXX: we set timeout to 10s to help workaround\n        # https://github.com/matrix-org/synapse/issues/1733.\n        # The reason is to avoid holding the linearizer lock\n        # whilst processing inbound /send transactions, causing\n        # FDs to stack up and block other inbound transactions\n        # which empirically can currently take up to 30 minutes.\n        #\n        # N.B. this explicitly disables retry attempts.\n        #\n        # N.B. this also increases our chances of falling back to\n        # fetching fresh state for the room if the missing event\n        # can't be found, which slightly reduces our security.\n        # it may also increase our DAG extremity count for the room,\n        # causing additional state resolution?  See #1760.\n        # However, fetching state doesn't hold the linearizer lock\n        # apparently.\n        #\n        # see https://github.com/matrix-org/synapse/pull/1744\n        #\n        # ----\n        #\n        # Update richvdh 2018/09/18: There are a number of problems with timing this\n        # request out aggressively on the client side:\n        #\n        # - it plays badly with the server-side rate-limiter, which starts tarpitting you\n        #   if you send too many requests at once, so you end up with the server carefully\n        #   working through the backlog of your requests, which you have already timed\n        #   out.\n        #\n        # - for this request in particular, we now (as of\n        #   https://github.com/matrix-org/synapse/pull/3456) reject any PDUs where the\n        #   server can't produce a plausible-looking set of prev_events - so we becone\n        #   much more likely to reject the event.\n        #\n        # - contrary to what it says above, we do *not* fall back to fetching fresh state\n        #   for the room if get_missing_events times out. Rather, we give up processing\n        #   the PDU whose prevs we are missing, which then makes it much more likely that\n        #   we'll end up back here for the *next* PDU in the list, which exacerbates the\n        #   problem.\n        #\n        # - the aggressive 10s timeout was introduced to deal with incoming federation\n        #   requests taking 8 hours to process. It's not entirely clear why that was going\n        #   on; certainly there were other issues causing traffic storms which are now\n        #   resolved, and I think in any case we may be more sensible about our locking\n        #   now. We're *certainly* more sensible about our logging.\n        #\n        # All that said: Let's try increasing the timeout to 60s and see what happens.\n\n        try:\n            missing_events = await self.federation_client.get_missing_events(\n                origin,\n                room_id,\n                earliest_events_ids=list(latest),\n                latest_events=[pdu],\n                limit=10,\n                min_depth=min_depth,\n                timeout=60000,\n            )\n        except (RequestSendFailed, HttpResponseException, NotRetryingDestination) as e:\n            # We failed to get the missing events, but since we need to handle\n            # the case of `get_missing_events` not returning the necessary\n            # events anyway, it is safe to simply log the error and continue.\n            logger.warning(\n                \"[%s %s]: Failed to get prev_events: %s\", room_id, event_id, e\n            )\n            return\n\n        logger.info(\n            \"[%s %s]: Got %d prev_events: %s\",\n            room_id,\n            event_id,\n            len(missing_events),\n            shortstr(missing_events),\n        )\n\n        # We want to sort these by depth so we process them and\n        # tell clients about them in order.\n        missing_events.sort(key=lambda x: x.depth)\n\n        for ev in missing_events:\n            logger.info(\n                \"[%s %s] Handling received prev_event %s\",\n                room_id,\n                event_id,\n                ev.event_id,\n            )\n            with nested_logging_context(ev.event_id):\n                try:\n                    await self.on_receive_pdu(origin, ev, sent_to_us_directly=False)\n                except FederationError as e:\n                    if e.code == 403:\n                        logger.warning(\n                            \"[%s %s] Received prev_event %s failed history check.\",\n                            room_id,\n                            event_id,\n                            ev.event_id,\n                        )\n                    else:\n                        raise\n\n    async def _get_state_for_room(\n        self,\n        destination: str,\n        room_id: str,\n        event_id: str,\n        include_event_in_state: bool = False,\n    ) -> Tuple[List[EventBase], List[EventBase]]:\n        \"\"\"Requests all of the room state at a given event from a remote homeserver.\n\n        Args:\n            destination: The remote homeserver to query for the state.\n            room_id: The id of the room we're interested in.\n            event_id: The id of the event we want the state at.\n            include_event_in_state: if true, the event itself will be included in the\n                returned state event list.\n\n        Returns:\n            A list of events in the state, possibly including the event itself, and\n            a list of events in the auth chain for the given event.\n        \"\"\"\n        (\n            state_event_ids,\n            auth_event_ids,\n        ) = await self.federation_client.get_room_state_ids(\n            destination, room_id, event_id=event_id\n        )\n\n        desired_events = set(state_event_ids + auth_event_ids)\n\n        if include_event_in_state:\n            desired_events.add(event_id)\n\n        event_map = await self._get_events_from_store_or_dest(\n            destination, room_id, desired_events\n        )\n\n        failed_to_fetch = desired_events - event_map.keys()\n        if failed_to_fetch:\n            logger.warning(\n                \"Failed to fetch missing state/auth events for %s %s\",\n                event_id,\n                failed_to_fetch,\n            )\n\n        remote_state = [\n            event_map[e_id] for e_id in state_event_ids if e_id in event_map\n        ]\n\n        if include_event_in_state:\n            remote_event = event_map.get(event_id)\n            if not remote_event:\n                raise Exception(\"Unable to get missing prev_event %s\" % (event_id,))\n            if remote_event.is_state() and remote_event.rejected_reason is None:\n                remote_state.append(remote_event)\n\n        auth_chain = [event_map[e_id] for e_id in auth_event_ids if e_id in event_map]\n        auth_chain.sort(key=lambda e: e.depth)\n\n        return remote_state, auth_chain\n\n    async def _get_events_from_store_or_dest(\n        self, destination: str, room_id: str, event_ids: Iterable[str]\n    ) -> Dict[str, EventBase]:\n        \"\"\"Fetch events from a remote destination, checking if we already have them.\n\n        Persists any events we don't already have as outliers.\n\n        If we fail to fetch any of the events, a warning will be logged, and the event\n        will be omitted from the result. Likewise, any events which turn out not to\n        be in the given room.\n\n        This function *does not* automatically get missing auth events of the\n        newly fetched events. Callers must include the full auth chain of\n        of the missing events in the `event_ids` argument, to ensure that any\n        missing auth events are correctly fetched.\n\n        Returns:\n            map from event_id to event\n        \"\"\"\n        fetched_events = await self.store.get_events(event_ids, allow_rejected=True)\n\n        missing_events = set(event_ids) - fetched_events.keys()\n\n        if missing_events:\n            logger.debug(\n                \"Fetching unknown state/auth events %s for room %s\",\n                missing_events,\n                room_id,\n            )\n\n            await self._get_events_and_persist(\n                destination=destination, room_id=room_id, events=missing_events\n            )\n\n            # we need to make sure we re-load from the database to get the rejected\n            # state correct.\n            fetched_events.update(\n                (await self.store.get_events(missing_events, allow_rejected=True))\n            )\n\n        # check for events which were in the wrong room.\n        #\n        # this can happen if a remote server claims that the state or\n        # auth_events at an event in room A are actually events in room B\n\n        bad_events = [\n            (event_id, event.room_id)\n            for event_id, event in fetched_events.items()\n            if event.room_id != room_id\n        ]\n\n        for bad_event_id, bad_room_id in bad_events:\n            # This is a bogus situation, but since we may only discover it a long time\n            # after it happened, we try our best to carry on, by just omitting the\n            # bad events from the returned auth/state set.\n            logger.warning(\n                \"Remote server %s claims event %s in room %s is an auth/state \"\n                \"event in room %s\",\n                destination,\n                bad_event_id,\n                bad_room_id,\n                room_id,\n            )\n\n            del fetched_events[bad_event_id]\n\n        return fetched_events\n\n    async def _process_received_pdu(\n        self, origin: str, event: EventBase, state: Optional[Iterable[EventBase]],\n    ):\n        \"\"\" Called when we have a new pdu. We need to do auth checks and put it\n        through the StateHandler.\n\n        Args:\n            origin: server sending the event\n\n            event: event to be persisted\n\n            state: Normally None, but if we are handling a gap in the graph\n                (ie, we are missing one or more prev_events), the resolved state at the\n                event\n        \"\"\"\n        room_id = event.room_id\n        event_id = event.event_id\n\n        logger.debug(\"[%s %s] Processing event: %s\", room_id, event_id, event)\n\n        try:\n            await self._handle_new_event(origin, event, state=state)\n        except AuthError as e:\n            raise FederationError(\"ERROR\", e.code, e.msg, affected=event.event_id)\n\n        # For encrypted messages we check that we know about the sending device,\n        # if we don't then we mark the device cache for that user as stale.\n        if event.type == EventTypes.Encrypted:\n            device_id = event.content.get(\"device_id\")\n            sender_key = event.content.get(\"sender_key\")\n\n            cached_devices = await self.store.get_cached_devices_for_user(event.sender)\n\n            resync = False  # Whether we should resync device lists.\n\n            device = None\n            if device_id is not None:\n                device = cached_devices.get(device_id)\n                if device is None:\n                    logger.info(\n                        \"Received event from remote device not in our cache: %s %s\",\n                        event.sender,\n                        device_id,\n                    )\n                    resync = True\n\n            # We also check if the `sender_key` matches what we expect.\n            if sender_key is not None:\n                # Figure out what sender key we're expecting. If we know the\n                # device and recognize the algorithm then we can work out the\n                # exact key to expect. Otherwise check it matches any key we\n                # have for that device.\n\n                current_keys = []  # type: Container[str]\n\n                if device:\n                    keys = device.get(\"keys\", {}).get(\"keys\", {})\n\n                    if (\n                        event.content.get(\"algorithm\")\n                        == RoomEncryptionAlgorithms.MEGOLM_V1_AES_SHA2\n                    ):\n                        # For this algorithm we expect a curve25519 key.\n                        key_name = \"curve25519:%s\" % (device_id,)\n                        current_keys = [keys.get(key_name)]\n                    else:\n                        # We don't know understand the algorithm, so we just\n                        # check it matches a key for the device.\n                        current_keys = keys.values()\n                elif device_id:\n                    # We don't have any keys for the device ID.\n                    pass\n                else:\n                    # The event didn't include a device ID, so we just look for\n                    # keys across all devices.\n                    current_keys = [\n                        key\n                        for device in cached_devices.values()\n                        for key in device.get(\"keys\", {}).get(\"keys\", {}).values()\n                    ]\n\n                # We now check that the sender key matches (one of) the expected\n                # keys.\n                if sender_key not in current_keys:\n                    logger.info(\n                        \"Received event from remote device with unexpected sender key: %s %s: %s\",\n                        event.sender,\n                        device_id or \"<no device_id>\",\n                        sender_key,\n                    )\n                    resync = True\n\n            if resync:\n                run_as_background_process(\n                    \"resync_device_due_to_pdu\", self._resync_device, event.sender\n                )\n\n    async def _resync_device(self, sender: str) -> None:\n        \"\"\"We have detected that the device list for the given user may be out\n        of sync, so we try and resync them.\n        \"\"\"\n\n        try:\n            await self.store.mark_remote_user_device_cache_as_stale(sender)\n\n            # Immediately attempt a resync in the background\n            if self.config.worker_app:\n                await self._user_device_resync(user_id=sender)\n            else:\n                await self._device_list_updater.user_device_resync(sender)\n        except Exception:\n            logger.exception(\"Failed to resync device for %s\", sender)\n\n    @log_function\n    async def backfill(self, dest, room_id, limit, extremities):\n        \"\"\" Trigger a backfill request to `dest` for the given `room_id`\n\n        This will attempt to get more events from the remote. If the other side\n        has no new events to offer, this will return an empty list.\n\n        As the events are received, we check their signatures, and also do some\n        sanity-checking on them. If any of the backfilled events are invalid,\n        this method throws a SynapseError.\n\n        TODO: make this more useful to distinguish failures of the remote\n        server from invalid events (there is probably no point in trying to\n        re-fetch invalid events from every other HS in the room.)\n        \"\"\"\n        if dest == self.server_name:\n            raise SynapseError(400, \"Can't backfill from self.\")\n\n        events = await self.federation_client.backfill(\n            dest, room_id, limit=limit, extremities=extremities\n        )\n\n        if not events:\n            return []\n\n        # ideally we'd sanity check the events here for excess prev_events etc,\n        # but it's hard to reject events at this point without completely\n        # breaking backfill in the same way that it is currently broken by\n        # events whose signature we cannot verify (#3121).\n        #\n        # So for now we accept the events anyway. #3124 tracks this.\n        #\n        # for ev in events:\n        #     self._sanity_check_event(ev)\n\n        # Don't bother processing events we already have.\n        seen_events = await self.store.have_events_in_timeline(\n            {e.event_id for e in events}\n        )\n\n        events = [e for e in events if e.event_id not in seen_events]\n\n        if not events:\n            return []\n\n        event_map = {e.event_id: e for e in events}\n\n        event_ids = {e.event_id for e in events}\n\n        # build a list of events whose prev_events weren't in the batch.\n        # (XXX: this will include events whose prev_events we already have; that doesn't\n        # sound right?)\n        edges = [ev.event_id for ev in events if set(ev.prev_event_ids()) - event_ids]\n\n        logger.info(\"backfill: Got %d events with %d edges\", len(events), len(edges))\n\n        # For each edge get the current state.\n\n        auth_events = {}\n        state_events = {}\n        events_to_state = {}\n        for e_id in edges:\n            state, auth = await self._get_state_for_room(\n                destination=dest,\n                room_id=room_id,\n                event_id=e_id,\n                include_event_in_state=False,\n            )\n            auth_events.update({a.event_id: a for a in auth})\n            auth_events.update({s.event_id: s for s in state})\n            state_events.update({s.event_id: s for s in state})\n            events_to_state[e_id] = state\n\n        required_auth = {\n            a_id\n            for event in events\n            + list(state_events.values())\n            + list(auth_events.values())\n            for a_id in event.auth_event_ids()\n        }\n        auth_events.update(\n            {e_id: event_map[e_id] for e_id in required_auth if e_id in event_map}\n        )\n\n        ev_infos = []\n\n        # Step 1: persist the events in the chunk we fetched state for (i.e.\n        # the backwards extremities), with custom auth events and state\n        for e_id in events_to_state:\n            # For paranoia we ensure that these events are marked as\n            # non-outliers\n            ev = event_map[e_id]\n            assert not ev.internal_metadata.is_outlier()\n\n            ev_infos.append(\n                _NewEventInfo(\n                    event=ev,\n                    state=events_to_state[e_id],\n                    auth_events={\n                        (\n                            auth_events[a_id].type,\n                            auth_events[a_id].state_key,\n                        ): auth_events[a_id]\n                        for a_id in ev.auth_event_ids()\n                        if a_id in auth_events\n                    },\n                )\n            )\n\n        if ev_infos:\n            await self._handle_new_events(dest, room_id, ev_infos, backfilled=True)\n\n        # Step 2: Persist the rest of the events in the chunk one by one\n        events.sort(key=lambda e: e.depth)\n\n        for event in events:\n            if event in events_to_state:\n                continue\n\n            # For paranoia we ensure that these events are marked as\n            # non-outliers\n            assert not event.internal_metadata.is_outlier()\n\n            # We store these one at a time since each event depends on the\n            # previous to work out the state.\n            # TODO: We can probably do something more clever here.\n            await self._handle_new_event(dest, event, backfilled=True)\n\n        return events\n\n    async def maybe_backfill(\n        self, room_id: str, current_depth: int, limit: int\n    ) -> bool:\n        \"\"\"Checks the database to see if we should backfill before paginating,\n        and if so do.\n\n        Args:\n            room_id\n            current_depth: The depth from which we're paginating from. This is\n                used to decide if we should backfill and what extremities to\n                use.\n            limit: The number of events that the pagination request will\n                return. This is used as part of the heuristic to decide if we\n                should back paginate.\n        \"\"\"\n        extremities = await self.store.get_oldest_events_with_depth_in_room(room_id)\n\n        if not extremities:\n            logger.debug(\"Not backfilling as no extremeties found.\")\n            return False\n\n        # We only want to paginate if we can actually see the events we'll get,\n        # as otherwise we'll just spend a lot of resources to get redacted\n        # events.\n        #\n        # We do this by filtering all the backwards extremities and seeing if\n        # any remain. Given we don't have the extremity events themselves, we\n        # need to actually check the events that reference them.\n        #\n        # *Note*: the spec wants us to keep backfilling until we reach the start\n        # of the room in case we are allowed to see some of the history. However\n        # in practice that causes more issues than its worth, as a) its\n        # relatively rare for there to be any visible history and b) even when\n        # there is its often sufficiently long ago that clients would stop\n        # attempting to paginate before backfill reached the visible history.\n        #\n        # TODO: If we do do a backfill then we should filter the backwards\n        #   extremities to only include those that point to visible portions of\n        #   history.\n        #\n        # TODO: Correctly handle the case where we are allowed to see the\n        #   forward event but not the backward extremity, e.g. in the case of\n        #   initial join of the server where we are allowed to see the join\n        #   event but not anything before it. This would require looking at the\n        #   state *before* the event, ignoring the special casing certain event\n        #   types have.\n\n        forward_events = await self.store.get_successor_events(list(extremities))\n\n        extremities_events = await self.store.get_events(\n            forward_events,\n            redact_behaviour=EventRedactBehaviour.AS_IS,\n            get_prev_content=False,\n        )\n\n        # We set `check_history_visibility_only` as we might otherwise get false\n        # positives from users having been erased.\n        filtered_extremities = await filter_events_for_server(\n            self.storage,\n            self.server_name,\n            list(extremities_events.values()),\n            redact=False,\n            check_history_visibility_only=True,\n        )\n\n        if not filtered_extremities:\n            return False\n\n        # Check if we reached a point where we should start backfilling.\n        sorted_extremeties_tuple = sorted(extremities.items(), key=lambda e: -int(e[1]))\n        max_depth = sorted_extremeties_tuple[0][1]\n\n        # If we're approaching an extremity we trigger a backfill, otherwise we\n        # no-op.\n        #\n        # We chose twice the limit here as then clients paginating backwards\n        # will send pagination requests that trigger backfill at least twice\n        # using the most recent extremity before it gets removed (see below). We\n        # chose more than one times the limit in case of failure, but choosing a\n        # much larger factor will result in triggering a backfill request much\n        # earlier than necessary.\n        if current_depth - 2 * limit > max_depth:\n            logger.debug(\n                \"Not backfilling as we don't need to. %d < %d - 2 * %d\",\n                max_depth,\n                current_depth,\n                limit,\n            )\n            return False\n\n        logger.debug(\n            \"room_id: %s, backfill: current_depth: %s, max_depth: %s, extrems: %s\",\n            room_id,\n            current_depth,\n            max_depth,\n            sorted_extremeties_tuple,\n        )\n\n        # We ignore extremities that have a greater depth than our current depth\n        # as:\n        #    1. we don't really care about getting events that have happened\n        #       before our current position; and\n        #    2. we have likely previously tried and failed to backfill from that\n        #       extremity, so to avoid getting \"stuck\" requesting the same\n        #       backfill repeatedly we drop those extremities.\n        filtered_sorted_extremeties_tuple = [\n            t for t in sorted_extremeties_tuple if int(t[1]) <= current_depth\n        ]\n\n        # However, we need to check that the filtered extremities are non-empty.\n        # If they are empty then either we can a) bail or b) still attempt to\n        # backill. We opt to try backfilling anyway just in case we do get\n        # relevant events.\n        if filtered_sorted_extremeties_tuple:\n            sorted_extremeties_tuple = filtered_sorted_extremeties_tuple\n\n        # We don't want to specify too many extremities as it causes the backfill\n        # request URI to be too long.\n        extremities = dict(sorted_extremeties_tuple[:5])\n\n        # Now we need to decide which hosts to hit first.\n\n        # First we try hosts that are already in the room\n        # TODO: HEURISTIC ALERT.\n\n        curr_state = await self.state_handler.get_current_state(room_id)\n\n        def get_domains_from_state(state):\n            \"\"\"Get joined domains from state\n\n            Args:\n                state (dict[tuple, FrozenEvent]): State map from type/state\n                    key to event.\n\n            Returns:\n                list[tuple[str, int]]: Returns a list of servers with the\n                lowest depth of their joins. Sorted by lowest depth first.\n            \"\"\"\n            joined_users = [\n                (state_key, int(event.depth))\n                for (e_type, state_key), event in state.items()\n                if e_type == EventTypes.Member and event.membership == Membership.JOIN\n            ]\n\n            joined_domains = {}  # type: Dict[str, int]\n            for u, d in joined_users:\n                try:\n                    dom = get_domain_from_id(u)\n                    old_d = joined_domains.get(dom)\n                    if old_d:\n                        joined_domains[dom] = min(d, old_d)\n                    else:\n                        joined_domains[dom] = d\n                except Exception:\n                    pass\n\n            return sorted(joined_domains.items(), key=lambda d: d[1])\n\n        curr_domains = get_domains_from_state(curr_state)\n\n        likely_domains = [\n            domain for domain, depth in curr_domains if domain != self.server_name\n        ]\n\n        async def try_backfill(domains):\n            # TODO: Should we try multiple of these at a time?\n            for dom in domains:\n                try:\n                    await self.backfill(\n                        dom, room_id, limit=100, extremities=extremities\n                    )\n                    # If this succeeded then we probably already have the\n                    # appropriate stuff.\n                    # TODO: We can probably do something more intelligent here.\n                    return True\n                except SynapseError as e:\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except HttpResponseException as e:\n                    if 400 <= e.code < 500:\n                        raise e.to_synapse_error()\n\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except CodeMessageException as e:\n                    if 400 <= e.code < 500:\n                        raise\n\n                    logger.info(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n                except NotRetryingDestination as e:\n                    logger.info(str(e))\n                    continue\n                except RequestSendFailed as e:\n                    logger.info(\"Failed to get backfill from %s because %s\", dom, e)\n                    continue\n                except FederationDeniedError as e:\n                    logger.info(e)\n                    continue\n                except Exception as e:\n                    logger.exception(\"Failed to backfill from %s because %s\", dom, e)\n                    continue\n\n            return False\n\n        success = await try_backfill(likely_domains)\n        if success:\n            return True\n\n        # Huh, well *those* domains didn't work out. Lets try some domains\n        # from the time.\n\n        tried_domains = set(likely_domains)\n        tried_domains.add(self.server_name)\n\n        event_ids = list(extremities.keys())\n\n        logger.debug(\"calling resolve_state_groups in _maybe_backfill\")\n        resolve = preserve_fn(self.state_handler.resolve_state_groups_for_events)\n        states = await make_deferred_yieldable(\n            defer.gatherResults(\n                [resolve(room_id, [e]) for e in event_ids], consumeErrors=True\n            )\n        )\n\n        # dict[str, dict[tuple, str]], a map from event_id to state map of\n        # event_ids.\n        states = dict(zip(event_ids, [s.state for s in states]))\n\n        state_map = await self.store.get_events(\n            [e_id for ids in states.values() for e_id in ids.values()],\n            get_prev_content=False,\n        )\n        states = {\n            key: {\n                k: state_map[e_id]\n                for k, e_id in state_dict.items()\n                if e_id in state_map\n            }\n            for key, state_dict in states.items()\n        }\n\n        for e_id, _ in sorted_extremeties_tuple:\n            likely_domains = get_domains_from_state(states[e_id])\n\n            success = await try_backfill(\n                [dom for dom, _ in likely_domains if dom not in tried_domains]\n            )\n            if success:\n                return True\n\n            tried_domains.update(dom for dom, _ in likely_domains)\n\n        return False\n\n    async def _get_events_and_persist(\n        self, destination: str, room_id: str, events: Iterable[str]\n    ):\n        \"\"\"Fetch the given events from a server, and persist them as outliers.\n\n        This function *does not* recursively get missing auth events of the\n        newly fetched events. Callers must include in the `events` argument\n        any missing events from the auth chain.\n\n        Logs a warning if we can't find the given event.\n        \"\"\"\n\n        room_version = await self.store.get_room_version(room_id)\n\n        event_map = {}  # type: Dict[str, EventBase]\n\n        async def get_event(event_id: str):\n            with nested_logging_context(event_id):\n                try:\n                    event = await self.federation_client.get_pdu(\n                        [destination], event_id, room_version, outlier=True,\n                    )\n                    if event is None:\n                        logger.warning(\n                            \"Server %s didn't return event %s\", destination, event_id,\n                        )\n                        return\n\n                    event_map[event.event_id] = event\n\n                except Exception as e:\n                    logger.warning(\n                        \"Error fetching missing state/auth event %s: %s %s\",\n                        event_id,\n                        type(e),\n                        e,\n                    )\n\n        await concurrently_execute(get_event, events, 5)\n\n        # Make a map of auth events for each event. We do this after fetching\n        # all the events as some of the events' auth events will be in the list\n        # of requested events.\n\n        auth_events = [\n            aid\n            for event in event_map.values()\n            for aid in event.auth_event_ids()\n            if aid not in event_map\n        ]\n        persisted_events = await self.store.get_events(\n            auth_events, allow_rejected=True,\n        )\n\n        event_infos = []\n        for event in event_map.values():\n            auth = {}\n            for auth_event_id in event.auth_event_ids():\n                ae = persisted_events.get(auth_event_id) or event_map.get(auth_event_id)\n                if ae:\n                    auth[(ae.type, ae.state_key)] = ae\n                else:\n                    logger.info(\"Missing auth event %s\", auth_event_id)\n\n            event_infos.append(_NewEventInfo(event, None, auth))\n\n        await self._handle_new_events(\n            destination, room_id, event_infos,\n        )\n\n    def _sanity_check_event(self, ev):\n        \"\"\"\n        Do some early sanity checks of a received event\n\n        In particular, checks it doesn't have an excessive number of\n        prev_events or auth_events, which could cause a huge state resolution\n        or cascade of event fetches.\n\n        Args:\n            ev (synapse.events.EventBase): event to be checked\n\n        Returns: None\n\n        Raises:\n            SynapseError if the event does not pass muster\n        \"\"\"\n        if len(ev.prev_event_ids()) > 20:\n            logger.warning(\n                \"Rejecting event %s which has %i prev_events\",\n                ev.event_id,\n                len(ev.prev_event_ids()),\n            )\n            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many prev_events\")\n\n        if len(ev.auth_event_ids()) > 10:\n            logger.warning(\n                \"Rejecting event %s which has %i auth_events\",\n                ev.event_id,\n                len(ev.auth_event_ids()),\n            )\n            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many auth_events\")\n\n    async def send_invite(self, target_host, event):\n        \"\"\" Sends the invite to the remote server for signing.\n\n        Invites must be signed by the invitee's server before distribution.\n        \"\"\"\n        pdu = await self.federation_client.send_invite(\n            destination=target_host,\n            room_id=event.room_id,\n            event_id=event.event_id,\n            pdu=event,\n        )\n\n        return pdu\n\n    async def on_event_auth(self, event_id: str) -> List[EventBase]:\n        event = await self.store.get_event(event_id)\n        auth = await self.store.get_auth_chain(\n            list(event.auth_event_ids()), include_given=True\n        )\n        return list(auth)\n\n    async def do_invite_join(\n        self, target_hosts: Iterable[str], room_id: str, joinee: str, content: JsonDict\n    ) -> Tuple[str, int]:\n        \"\"\" Attempts to join the `joinee` to the room `room_id` via the\n        servers contained in `target_hosts`.\n\n        This first triggers a /make_join/ request that returns a partial\n        event that we can fill out and sign. This is then sent to the\n        remote server via /send_join/ which responds with the state at that\n        event and the auth_chains.\n\n        We suspend processing of any received events from this room until we\n        have finished processing the join.\n\n        Args:\n            target_hosts: List of servers to attempt to join the room with.\n\n            room_id: The ID of the room to join.\n\n            joinee: The User ID of the joining user.\n\n            content: The event content to use for the join event.\n        \"\"\"\n        # TODO: We should be able to call this on workers, but the upgrading of\n        # room stuff after join currently doesn't work on workers.\n        assert self.config.worker.worker_app is None\n\n        logger.debug(\"Joining %s to %s\", joinee, room_id)\n\n        origin, event, room_version_obj = await self._make_and_verify_event(\n            target_hosts,\n            room_id,\n            joinee,\n            \"join\",\n            content,\n            params={\"ver\": KNOWN_ROOM_VERSIONS},\n        )\n\n        # This shouldn't happen, because the RoomMemberHandler has a\n        # linearizer lock which only allows one operation per user per room\n        # at a time - so this is just paranoia.\n        assert room_id not in self.room_queues\n\n        self.room_queues[room_id] = []\n\n        await self._clean_room_for_join(room_id)\n\n        handled_events = set()\n\n        try:\n            # Try the host we successfully got a response to /make_join/\n            # request first.\n            host_list = list(target_hosts)\n            try:\n                host_list.remove(origin)\n                host_list.insert(0, origin)\n            except ValueError:\n                pass\n\n            ret = await self.federation_client.send_join(\n                host_list, event, room_version_obj\n            )\n\n            origin = ret[\"origin\"]\n            state = ret[\"state\"]\n            auth_chain = ret[\"auth_chain\"]\n            auth_chain.sort(key=lambda e: e.depth)\n\n            handled_events.update([s.event_id for s in state])\n            handled_events.update([a.event_id for a in auth_chain])\n            handled_events.add(event.event_id)\n\n            logger.debug(\"do_invite_join auth_chain: %s\", auth_chain)\n            logger.debug(\"do_invite_join state: %s\", state)\n\n            logger.debug(\"do_invite_join event: %s\", event)\n\n            # if this is the first time we've joined this room, it's time to add\n            # a row to `rooms` with the correct room version. If there's already a\n            # row there, we should override it, since it may have been populated\n            # based on an invite request which lied about the room version.\n            #\n            # federation_client.send_join has already checked that the room\n            # version in the received create event is the same as room_version_obj,\n            # so we can rely on it now.\n            #\n            await self.store.upsert_room_on_join(\n                room_id=room_id, room_version=room_version_obj,\n            )\n\n            max_stream_id = await self._persist_auth_tree(\n                origin, room_id, auth_chain, state, event, room_version_obj\n            )\n\n            # We wait here until this instance has seen the events come down\n            # replication (if we're using replication) as the below uses caches.\n            await self._replication.wait_for_stream_position(\n                self.config.worker.events_shard_config.get_instance(room_id),\n                \"events\",\n                max_stream_id,\n            )\n\n            # Check whether this room is the result of an upgrade of a room we already know\n            # about. If so, migrate over user information\n            predecessor = await self.store.get_room_predecessor(room_id)\n            if not predecessor or not isinstance(predecessor.get(\"room_id\"), str):\n                return event.event_id, max_stream_id\n            old_room_id = predecessor[\"room_id\"]\n            logger.debug(\n                \"Found predecessor for %s during remote join: %s\", room_id, old_room_id\n            )\n\n            # We retrieve the room member handler here as to not cause a cyclic dependency\n            member_handler = self.hs.get_room_member_handler()\n            await member_handler.transfer_room_state_on_room_upgrade(\n                old_room_id, room_id\n            )\n\n            logger.debug(\"Finished joining %s to %s\", joinee, room_id)\n            return event.event_id, max_stream_id\n        finally:\n            room_queue = self.room_queues[room_id]\n            del self.room_queues[room_id]\n\n            # we don't need to wait for the queued events to be processed -\n            # it's just a best-effort thing at this point. We do want to do\n            # them roughly in order, though, otherwise we'll end up making\n            # lots of requests for missing prev_events which we do actually\n            # have. Hence we fire off the background task, but don't wait for it.\n\n            run_in_background(self._handle_queued_pdus, room_queue)\n\n    async def _handle_queued_pdus(self, room_queue):\n        \"\"\"Process PDUs which got queued up while we were busy send_joining.\n\n        Args:\n            room_queue (list[FrozenEvent, str]): list of PDUs to be processed\n                and the servers that sent them\n        \"\"\"\n        for p, origin in room_queue:\n            try:\n                logger.info(\n                    \"Processing queued PDU %s which was received \"\n                    \"while we were joining %s\",\n                    p.event_id,\n                    p.room_id,\n                )\n                with nested_logging_context(p.event_id):\n                    await self.on_receive_pdu(origin, p, sent_to_us_directly=True)\n            except Exception as e:\n                logger.warning(\n                    \"Error handling queued PDU %s from %s: %s\", p.event_id, origin, e\n                )\n\n    async def on_make_join_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> EventBase:\n        \"\"\" We've received a /make_join/ request, so we create a partial\n        join event for the room and return that. We do *not* persist or\n        process it until the other server has signed it and sent it back.\n\n        Args:\n            origin: The (verified) server name of the requesting server.\n            room_id: Room to create join event in\n            user_id: The user to create the join for\n        \"\"\"\n        if get_domain_from_id(user_id) != origin:\n            logger.info(\n                \"Got /make_join request for user %r from different origin %s, ignoring\",\n                user_id,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        # checking the room version will check that we've actually heard of the room\n        # (and return a 404 otherwise)\n        room_version = await self.store.get_room_version_id(room_id)\n\n        # now check that we are *still* in the room\n        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)\n        if not is_in_room:\n            logger.info(\n                \"Got /make_join request for room %s we are no longer in\", room_id,\n            )\n            raise NotFoundError(\"Not an active room on this server\")\n\n        event_content = {\"membership\": Membership.JOIN}\n\n        builder = self.event_builder_factory.new(\n            room_version,\n            {\n                \"type\": EventTypes.Member,\n                \"content\": event_content,\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": user_id,\n            },\n        )\n\n        try:\n            event, context = await self.event_creation_handler.create_new_client_event(\n                builder=builder\n            )\n        except SynapseError as e:\n            logger.warning(\"Failed to create join to %s because %s\", room_id, e)\n            raise\n\n        # The remote hasn't signed it yet, obviously. We'll do the full checks\n        # when we get the event back in `on_send_join_request`\n        await self.auth.check_from_context(\n            room_version, event, context, do_sig_check=False\n        )\n\n        return event\n\n    async def on_send_join_request(self, origin, pdu):\n        \"\"\" We have received a join event for a room. Fully process it and\n        respond with the current state and auth chains.\n        \"\"\"\n        event = pdu\n\n        logger.debug(\n            \"on_send_join_request from %s: Got event: %s, signatures: %s\",\n            origin,\n            event.event_id,\n            event.signatures,\n        )\n\n        if get_domain_from_id(event.sender) != origin:\n            logger.info(\n                \"Got /send_join request for user %r from different origin %s\",\n                event.sender,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        event.internal_metadata.outlier = False\n        # Send this event on behalf of the origin server.\n        #\n        # The reasons we have the destination server rather than the origin\n        # server send it are slightly mysterious: the origin server should have\n        # all the necessary state once it gets the response to the send_join,\n        # so it could send the event itself if it wanted to. It may be that\n        # doing it this way reduces failure modes, or avoids certain attacks\n        # where a new server selectively tells a subset of the federation that\n        # it has joined.\n        #\n        # The fact is that, as of the current writing, Synapse doesn't send out\n        # the join event over federation after joining, and changing it now\n        # would introduce the danger of backwards-compatibility problems.\n        event.internal_metadata.send_on_behalf_of = origin\n\n        context = await self._handle_new_event(origin, event)\n\n        logger.debug(\n            \"on_send_join_request: After _handle_new_event: %s, sigs: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        prev_state_ids = await context.get_prev_state_ids()\n\n        state_ids = list(prev_state_ids.values())\n        auth_chain = await self.store.get_auth_chain(state_ids)\n\n        state = await self.store.get_events(list(prev_state_ids.values()))\n\n        return {\"state\": list(state.values()), \"auth_chain\": auth_chain}\n\n    async def on_invite_request(\n        self, origin: str, event: EventBase, room_version: RoomVersion\n    ):\n        \"\"\" We've got an invite event. Process and persist it. Sign it.\n\n        Respond with the now signed event.\n        \"\"\"\n        if event.state_key is None:\n            raise SynapseError(400, \"The invite event did not have a state key\")\n\n        is_blocked = await self.store.is_room_blocked(event.room_id)\n        if is_blocked:\n            raise SynapseError(403, \"This room has been blocked on this server\")\n\n        if self.hs.config.block_non_admin_invites:\n            raise SynapseError(403, \"This server does not accept room invites\")\n\n        if not self.spam_checker.user_may_invite(\n            event.sender, event.state_key, event.room_id\n        ):\n            raise SynapseError(\n                403, \"This user is not permitted to send invites to this server/user\"\n            )\n\n        membership = event.content.get(\"membership\")\n        if event.type != EventTypes.Member or membership != Membership.INVITE:\n            raise SynapseError(400, \"The event was not an m.room.member invite event\")\n\n        sender_domain = get_domain_from_id(event.sender)\n        if sender_domain != origin:\n            raise SynapseError(\n                400, \"The invite event was not from the server sending it\"\n            )\n\n        if not self.is_mine_id(event.state_key):\n            raise SynapseError(400, \"The invite event must be for this server\")\n\n        # block any attempts to invite the server notices mxid\n        if event.state_key == self._server_notices_mxid:\n            raise SynapseError(HTTPStatus.FORBIDDEN, \"Cannot invite this user\")\n\n        # keep a record of the room version, if we don't yet know it.\n        # (this may get overwritten if we later get a different room version in a\n        # join dance).\n        await self._maybe_store_room_on_outlier_membership(\n            room_id=event.room_id, room_version=room_version\n        )\n\n        event.internal_metadata.outlier = True\n        event.internal_metadata.out_of_band_membership = True\n\n        event.signatures.update(\n            compute_event_signature(\n                room_version,\n                event.get_pdu_json(),\n                self.hs.hostname,\n                self.hs.signing_key,\n            )\n        )\n\n        context = await self.state_handler.compute_event_context(event)\n        await self.persist_events_and_notify(event.room_id, [(event, context)])\n\n        return event\n\n    async def do_remotely_reject_invite(\n        self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict\n    ) -> Tuple[EventBase, int]:\n        origin, event, room_version = await self._make_and_verify_event(\n            target_hosts, room_id, user_id, \"leave\", content=content\n        )\n        # Mark as outlier as we don't have any state for this event; we're not\n        # even in the room.\n        event.internal_metadata.outlier = True\n        event.internal_metadata.out_of_band_membership = True\n\n        # Try the host that we successfully called /make_leave/ on first for\n        # the /send_leave/ request.\n        host_list = list(target_hosts)\n        try:\n            host_list.remove(origin)\n            host_list.insert(0, origin)\n        except ValueError:\n            pass\n\n        await self.federation_client.send_leave(host_list, event)\n\n        context = await self.state_handler.compute_event_context(event)\n        stream_id = await self.persist_events_and_notify(\n            event.room_id, [(event, context)]\n        )\n\n        return event, stream_id\n\n    async def _make_and_verify_event(\n        self,\n        target_hosts: Iterable[str],\n        room_id: str,\n        user_id: str,\n        membership: str,\n        content: JsonDict = {},\n        params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,\n    ) -> Tuple[str, EventBase, RoomVersion]:\n        (\n            origin,\n            event,\n            room_version,\n        ) = await self.federation_client.make_membership_event(\n            target_hosts, room_id, user_id, membership, content, params=params\n        )\n\n        logger.debug(\"Got response to make_%s: %s\", membership, event)\n\n        # We should assert some things.\n        # FIXME: Do this in a nicer way\n        assert event.type == EventTypes.Member\n        assert event.user_id == user_id\n        assert event.state_key == user_id\n        assert event.room_id == room_id\n        return origin, event, room_version\n\n    async def on_make_leave_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> EventBase:\n        \"\"\" We've received a /make_leave/ request, so we create a partial\n        leave event for the room and return that. We do *not* persist or\n        process it until the other server has signed it and sent it back.\n\n        Args:\n            origin: The (verified) server name of the requesting server.\n            room_id: Room to create leave event in\n            user_id: The user to create the leave for\n        \"\"\"\n        if get_domain_from_id(user_id) != origin:\n            logger.info(\n                \"Got /make_leave request for user %r from different origin %s, ignoring\",\n                user_id,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        room_version = await self.store.get_room_version_id(room_id)\n        builder = self.event_builder_factory.new(\n            room_version,\n            {\n                \"type\": EventTypes.Member,\n                \"content\": {\"membership\": Membership.LEAVE},\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": user_id,\n            },\n        )\n\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n\n        try:\n            # The remote hasn't signed it yet, obviously. We'll do the full checks\n            # when we get the event back in `on_send_leave_request`\n            await self.auth.check_from_context(\n                room_version, event, context, do_sig_check=False\n            )\n        except AuthError as e:\n            logger.warning(\"Failed to create new leave %r because %s\", event, e)\n            raise e\n\n        return event\n\n    async def on_send_leave_request(self, origin, pdu):\n        \"\"\" We have received a leave event for a room. Fully process it.\"\"\"\n        event = pdu\n\n        logger.debug(\n            \"on_send_leave_request: Got event: %s, signatures: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        if get_domain_from_id(event.sender) != origin:\n            logger.info(\n                \"Got /send_leave request for user %r from different origin %s\",\n                event.sender,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        event.internal_metadata.outlier = False\n\n        await self._handle_new_event(origin, event)\n\n        logger.debug(\n            \"on_send_leave_request: After _handle_new_event: %s, sigs: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        return None\n\n    async def get_state_for_pdu(self, room_id: str, event_id: str) -> List[EventBase]:\n        \"\"\"Returns the state at the event. i.e. not including said event.\n        \"\"\"\n\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        state_groups = await self.state_store.get_state_groups(room_id, [event_id])\n\n        if state_groups:\n            _, state = list(state_groups.items()).pop()\n            results = {(e.type, e.state_key): e for e in state}\n\n            if event.is_state():\n                # Get previous state\n                if \"replaces_state\" in event.unsigned:\n                    prev_id = event.unsigned[\"replaces_state\"]\n                    if prev_id != event.event_id:\n                        prev_event = await self.store.get_event(prev_id)\n                        results[(event.type, event.state_key)] = prev_event\n                else:\n                    del results[(event.type, event.state_key)]\n\n            res = list(results.values())\n            return res\n        else:\n            return []\n\n    async def get_state_ids_for_pdu(self, room_id: str, event_id: str) -> List[str]:\n        \"\"\"Returns the state at the event. i.e. not including said event.\n        \"\"\"\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        state_groups = await self.state_store.get_state_groups_ids(room_id, [event_id])\n\n        if state_groups:\n            _, state = list(state_groups.items()).pop()\n            results = state\n\n            if event.is_state():\n                # Get previous state\n                if \"replaces_state\" in event.unsigned:\n                    prev_id = event.unsigned[\"replaces_state\"]\n                    if prev_id != event.event_id:\n                        results[(event.type, event.state_key)] = prev_id\n                else:\n                    results.pop((event.type, event.state_key), None)\n\n            return list(results.values())\n        else:\n            return []\n\n    @log_function\n    async def on_backfill_request(\n        self, origin: str, room_id: str, pdu_list: List[str], limit: int\n    ) -> List[EventBase]:\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # Synapse asks for 100 events per backfill request. Do not allow more.\n        limit = min(limit, 100)\n\n        events = await self.store.get_backfill_events(room_id, pdu_list, limit)\n\n        events = await filter_events_for_server(self.storage, origin, events)\n\n        return events\n\n    @log_function\n    async def get_persisted_pdu(\n        self, origin: str, event_id: str\n    ) -> Optional[EventBase]:\n        \"\"\"Get an event from the database for the given server.\n\n        Args:\n            origin: hostname of server which is requesting the event; we\n               will check that the server is allowed to see it.\n            event_id: id of the event being requested\n\n        Returns:\n            None if we know nothing about the event; otherwise the (possibly-redacted) event.\n\n        Raises:\n            AuthError if the server is not currently in the room\n        \"\"\"\n        event = await self.store.get_event(\n            event_id, allow_none=True, allow_rejected=True\n        )\n\n        if event:\n            in_room = await self.auth.check_host_in_room(event.room_id, origin)\n            if not in_room:\n                raise AuthError(403, \"Host not in room.\")\n\n            events = await filter_events_for_server(self.storage, origin, [event])\n            event = events[0]\n            return event\n        else:\n            return None\n\n    async def get_min_depth_for_context(self, context):\n        return await self.store.get_min_depth(context)\n\n    async def _handle_new_event(\n        self, origin, event, state=None, auth_events=None, backfilled=False\n    ):\n        context = await self._prep_event(\n            origin, event, state=state, auth_events=auth_events, backfilled=backfilled\n        )\n\n        try:\n            if (\n                not event.internal_metadata.is_outlier()\n                and not backfilled\n                and not context.rejected\n            ):\n                await self.action_generator.handle_push_actions_for_event(\n                    event, context\n                )\n\n            await self.persist_events_and_notify(\n                event.room_id, [(event, context)], backfilled=backfilled\n            )\n        except Exception:\n            run_in_background(\n                self.store.remove_push_actions_from_staging, event.event_id\n            )\n            raise\n\n        return context\n\n    async def _handle_new_events(\n        self,\n        origin: str,\n        room_id: str,\n        event_infos: Iterable[_NewEventInfo],\n        backfilled: bool = False,\n    ) -> None:\n        \"\"\"Creates the appropriate contexts and persists events. The events\n        should not depend on one another, e.g. this should be used to persist\n        a bunch of outliers, but not a chunk of individual events that depend\n        on each other for state calculations.\n\n        Notifies about the events where appropriate.\n        \"\"\"\n\n        async def prep(ev_info: _NewEventInfo):\n            event = ev_info.event\n            with nested_logging_context(suffix=event.event_id):\n                res = await self._prep_event(\n                    origin,\n                    event,\n                    state=ev_info.state,\n                    auth_events=ev_info.auth_events,\n                    backfilled=backfilled,\n                )\n            return res\n\n        contexts = await make_deferred_yieldable(\n            defer.gatherResults(\n                [run_in_background(prep, ev_info) for ev_info in event_infos],\n                consumeErrors=True,\n            )\n        )\n\n        await self.persist_events_and_notify(\n            room_id,\n            [\n                (ev_info.event, context)\n                for ev_info, context in zip(event_infos, contexts)\n            ],\n            backfilled=backfilled,\n        )\n\n    async def _persist_auth_tree(\n        self,\n        origin: str,\n        room_id: str,\n        auth_events: List[EventBase],\n        state: List[EventBase],\n        event: EventBase,\n        room_version: RoomVersion,\n    ) -> int:\n        \"\"\"Checks the auth chain is valid (and passes auth checks) for the\n        state and event. Then persists the auth chain and state atomically.\n        Persists the event separately. Notifies about the persisted events\n        where appropriate.\n\n        Will attempt to fetch missing auth events.\n\n        Args:\n            origin: Where the events came from\n            room_id,\n            auth_events\n            state\n            event\n            room_version: The room version we expect this room to have, and\n                will raise if it doesn't match the version in the create event.\n        \"\"\"\n        events_to_context = {}\n        for e in itertools.chain(auth_events, state):\n            e.internal_metadata.outlier = True\n            ctx = await self.state_handler.compute_event_context(e)\n            events_to_context[e.event_id] = ctx\n\n        event_map = {\n            e.event_id: e for e in itertools.chain(auth_events, state, [event])\n        }\n\n        create_event = None\n        for e in auth_events:\n            if (e.type, e.state_key) == (EventTypes.Create, \"\"):\n                create_event = e\n                break\n\n        if create_event is None:\n            # If the state doesn't have a create event then the room is\n            # invalid, and it would fail auth checks anyway.\n            raise SynapseError(400, \"No create event in state\")\n\n        room_version_id = create_event.content.get(\n            \"room_version\", RoomVersions.V1.identifier\n        )\n\n        if room_version.identifier != room_version_id:\n            raise SynapseError(400, \"Room version mismatch\")\n\n        missing_auth_events = set()\n        for e in itertools.chain(auth_events, state, [event]):\n            for e_id in e.auth_event_ids():\n                if e_id not in event_map:\n                    missing_auth_events.add(e_id)\n\n        for e_id in missing_auth_events:\n            m_ev = await self.federation_client.get_pdu(\n                [origin], e_id, room_version=room_version, outlier=True, timeout=10000,\n            )\n            if m_ev and m_ev.event_id == e_id:\n                event_map[e_id] = m_ev\n            else:\n                logger.info(\"Failed to find auth event %r\", e_id)\n\n        for e in itertools.chain(auth_events, state, [event]):\n            auth_for_e = {\n                (event_map[e_id].type, event_map[e_id].state_key): event_map[e_id]\n                for e_id in e.auth_event_ids()\n                if e_id in event_map\n            }\n            if create_event:\n                auth_for_e[(EventTypes.Create, \"\")] = create_event\n\n            try:\n                event_auth.check(room_version, e, auth_events=auth_for_e)\n            except SynapseError as err:\n                # we may get SynapseErrors here as well as AuthErrors. For\n                # instance, there are a couple of (ancient) events in some\n                # rooms whose senders do not have the correct sigil; these\n                # cause SynapseErrors in auth.check. We don't want to give up\n                # the attempt to federate altogether in such cases.\n\n                logger.warning(\"Rejecting %s because %s\", e.event_id, err.msg)\n\n                if e == event:\n                    raise\n                events_to_context[e.event_id].rejected = RejectedReason.AUTH_ERROR\n\n        await self.persist_events_and_notify(\n            room_id,\n            [\n                (e, events_to_context[e.event_id])\n                for e in itertools.chain(auth_events, state)\n            ],\n        )\n\n        new_event_context = await self.state_handler.compute_event_context(\n            event, old_state=state\n        )\n\n        return await self.persist_events_and_notify(\n            room_id, [(event, new_event_context)]\n        )\n\n    async def _prep_event(\n        self,\n        origin: str,\n        event: EventBase,\n        state: Optional[Iterable[EventBase]],\n        auth_events: Optional[MutableStateMap[EventBase]],\n        backfilled: bool,\n    ) -> EventContext:\n        context = await self.state_handler.compute_event_context(event, old_state=state)\n\n        if not auth_events:\n            prev_state_ids = await context.get_prev_state_ids()\n            auth_events_ids = self.auth.compute_auth_events(\n                event, prev_state_ids, for_verification=True\n            )\n            auth_events_x = await self.store.get_events(auth_events_ids)\n            auth_events = {(e.type, e.state_key): e for e in auth_events_x.values()}\n\n        # This is a hack to fix some old rooms where the initial join event\n        # didn't reference the create event in its auth events.\n        if event.type == EventTypes.Member and not event.auth_event_ids():\n            if len(event.prev_event_ids()) == 1 and event.depth < 5:\n                c = await self.store.get_event(\n                    event.prev_event_ids()[0], allow_none=True\n                )\n                if c and c.type == EventTypes.Create:\n                    auth_events[(c.type, c.state_key)] = c\n\n        context = await self.do_auth(origin, event, context, auth_events=auth_events)\n\n        if not context.rejected:\n            await self._check_for_soft_fail(event, state, backfilled)\n\n        if event.type == EventTypes.GuestAccess and not context.rejected:\n            await self.maybe_kick_guest_users(event)\n\n        return context\n\n    async def _check_for_soft_fail(\n        self, event: EventBase, state: Optional[Iterable[EventBase]], backfilled: bool\n    ) -> None:\n        \"\"\"Checks if we should soft fail the event; if so, marks the event as\n        such.\n\n        Args:\n            event\n            state: The state at the event if we don't have all the event's prev events\n            backfilled: Whether the event is from backfill\n        \"\"\"\n        # For new (non-backfilled and non-outlier) events we check if the event\n        # passes auth based on the current state. If it doesn't then we\n        # \"soft-fail\" the event.\n        if backfilled or event.internal_metadata.is_outlier():\n            return\n\n        extrem_ids_list = await self.store.get_latest_event_ids_in_room(event.room_id)\n        extrem_ids = set(extrem_ids_list)\n        prev_event_ids = set(event.prev_event_ids())\n\n        if extrem_ids == prev_event_ids:\n            # If they're the same then the current state is the same as the\n            # state at the event, so no point rechecking auth for soft fail.\n            return\n\n        room_version = await self.store.get_room_version_id(event.room_id)\n        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]\n\n        # Calculate the \"current state\".\n        if state is not None:\n            # If we're explicitly given the state then we won't have all the\n            # prev events, and so we have a gap in the graph. In this case\n            # we want to be a little careful as we might have been down for\n            # a while and have an incorrect view of the current state,\n            # however we still want to do checks as gaps are easy to\n            # maliciously manufacture.\n            #\n            # So we use a \"current state\" that is actually a state\n            # resolution across the current forward extremities and the\n            # given state at the event. This should correctly handle cases\n            # like bans, especially with state res v2.\n\n            state_sets_d = await self.state_store.get_state_groups(\n                event.room_id, extrem_ids\n            )\n            state_sets = list(state_sets_d.values())  # type: List[Iterable[EventBase]]\n            state_sets.append(state)\n            current_states = await self.state_handler.resolve_events(\n                room_version, state_sets, event\n            )\n            current_state_ids = {\n                k: e.event_id for k, e in current_states.items()\n            }  # type: StateMap[str]\n        else:\n            current_state_ids = await self.state_handler.get_current_state_ids(\n                event.room_id, latest_event_ids=extrem_ids\n            )\n\n        logger.debug(\n            \"Doing soft-fail check for %s: state %s\", event.event_id, current_state_ids,\n        )\n\n        # Now check if event pass auth against said current state\n        auth_types = auth_types_for_event(event)\n        current_state_ids_list = [\n            e for k, e in current_state_ids.items() if k in auth_types\n        ]\n\n        auth_events_map = await self.store.get_events(current_state_ids_list)\n        current_auth_events = {\n            (e.type, e.state_key): e for e in auth_events_map.values()\n        }\n\n        try:\n            event_auth.check(room_version_obj, event, auth_events=current_auth_events)\n        except AuthError as e:\n            logger.warning(\"Soft-failing %r because %s\", event, e)\n            event.internal_metadata.soft_failed = True\n\n    async def on_query_auth(\n        self, origin, event_id, room_id, remote_auth_chain, rejects, missing\n    ):\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        event = await self.store.get_event(event_id, check_room_id=room_id)\n\n        # Just go through and process each event in `remote_auth_chain`. We\n        # don't want to fall into the trap of `missing` being wrong.\n        for e in remote_auth_chain:\n            try:\n                await self._handle_new_event(origin, e)\n            except AuthError:\n                pass\n\n        # Now get the current auth_chain for the event.\n        local_auth_chain = await self.store.get_auth_chain(\n            list(event.auth_event_ids()), include_given=True\n        )\n\n        # TODO: Check if we would now reject event_id. If so we need to tell\n        # everyone.\n\n        ret = await self.construct_auth_difference(local_auth_chain, remote_auth_chain)\n\n        logger.debug(\"on_query_auth returning: %s\", ret)\n\n        return ret\n\n    async def on_get_missing_events(\n        self, origin, room_id, earliest_events, latest_events, limit\n    ):\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n\n        # Only allow up to 20 events to be retrieved per request.\n        limit = min(limit, 20)\n\n        missing_events = await self.store.get_missing_events(\n            room_id=room_id,\n            earliest_events=earliest_events,\n            latest_events=latest_events,\n            limit=limit,\n        )\n\n        missing_events = await filter_events_for_server(\n            self.storage, origin, missing_events\n        )\n\n        return missing_events\n\n    async def do_auth(\n        self,\n        origin: str,\n        event: EventBase,\n        context: EventContext,\n        auth_events: MutableStateMap[EventBase],\n    ) -> EventContext:\n        \"\"\"\n\n        Args:\n            origin:\n            event:\n            context:\n            auth_events:\n                Map from (event_type, state_key) to event\n\n                Normally, our calculated auth_events based on the state of the room\n                at the event's position in the DAG, though occasionally (eg if the\n                event is an outlier), may be the auth events claimed by the remote\n                server.\n\n                Also NB that this function adds entries to it.\n        Returns:\n            updated context object\n        \"\"\"\n        room_version = await self.store.get_room_version_id(event.room_id)\n        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]\n\n        try:\n            context = await self._update_auth_events_and_context_for_auth(\n                origin, event, context, auth_events\n            )\n        except Exception:\n            # We don't really mind if the above fails, so lets not fail\n            # processing if it does. However, it really shouldn't fail so\n            # let's still log as an exception since we'll still want to fix\n            # any bugs.\n            logger.exception(\n                \"Failed to double check auth events for %s with remote. \"\n                \"Ignoring failure and continuing processing of event.\",\n                event.event_id,\n            )\n\n        try:\n            event_auth.check(room_version_obj, event, auth_events=auth_events)\n        except AuthError as e:\n            logger.warning(\"Failed auth resolution for %r because %s\", event, e)\n            context.rejected = RejectedReason.AUTH_ERROR\n\n        return context\n\n    async def _update_auth_events_and_context_for_auth(\n        self,\n        origin: str,\n        event: EventBase,\n        context: EventContext,\n        auth_events: MutableStateMap[EventBase],\n    ) -> EventContext:\n        \"\"\"Helper for do_auth. See there for docs.\n\n        Checks whether a given event has the expected auth events. If it\n        doesn't then we talk to the remote server to compare state to see if\n        we can come to a consensus (e.g. if one server missed some valid\n        state).\n\n        This attempts to resolve any potential divergence of state between\n        servers, but is not essential and so failures should not block further\n        processing of the event.\n\n        Args:\n            origin:\n            event:\n            context:\n\n            auth_events:\n                Map from (event_type, state_key) to event\n\n                Normally, our calculated auth_events based on the state of the room\n                at the event's position in the DAG, though occasionally (eg if the\n                event is an outlier), may be the auth events claimed by the remote\n                server.\n\n                Also NB that this function adds entries to it.\n\n        Returns:\n            updated context\n        \"\"\"\n        event_auth_events = set(event.auth_event_ids())\n\n        # missing_auth is the set of the event's auth_events which we don't yet have\n        # in auth_events.\n        missing_auth = event_auth_events.difference(\n            e.event_id for e in auth_events.values()\n        )\n\n        # if we have missing events, we need to fetch those events from somewhere.\n        #\n        # we start by checking if they are in the store, and then try calling /event_auth/.\n        if missing_auth:\n            have_events = await self.store.have_seen_events(missing_auth)\n            logger.debug(\"Events %s are in the store\", have_events)\n            missing_auth.difference_update(have_events)\n\n        if missing_auth:\n            # If we don't have all the auth events, we need to get them.\n            logger.info(\"auth_events contains unknown events: %s\", missing_auth)\n            try:\n                try:\n                    remote_auth_chain = await self.federation_client.get_event_auth(\n                        origin, event.room_id, event.event_id\n                    )\n                except RequestSendFailed as e1:\n                    # The other side isn't around or doesn't implement the\n                    # endpoint, so lets just bail out.\n                    logger.info(\"Failed to get event auth from remote: %s\", e1)\n                    return context\n\n                seen_remotes = await self.store.have_seen_events(\n                    [e.event_id for e in remote_auth_chain]\n                )\n\n                for e in remote_auth_chain:\n                    if e.event_id in seen_remotes:\n                        continue\n\n                    if e.event_id == event.event_id:\n                        continue\n\n                    try:\n                        auth_ids = e.auth_event_ids()\n                        auth = {\n                            (e.type, e.state_key): e\n                            for e in remote_auth_chain\n                            if e.event_id in auth_ids or e.type == EventTypes.Create\n                        }\n                        e.internal_metadata.outlier = True\n\n                        logger.debug(\n                            \"do_auth %s missing_auth: %s\", event.event_id, e.event_id\n                        )\n                        await self._handle_new_event(origin, e, auth_events=auth)\n\n                        if e.event_id in event_auth_events:\n                            auth_events[(e.type, e.state_key)] = e\n                    except AuthError:\n                        pass\n\n            except Exception:\n                logger.exception(\"Failed to get auth chain\")\n\n        if event.internal_metadata.is_outlier():\n            # XXX: given that, for an outlier, we'll be working with the\n            # event's *claimed* auth events rather than those we calculated:\n            # (a) is there any point in this test, since different_auth below will\n            # obviously be empty\n            # (b) alternatively, why don't we do it earlier?\n            logger.info(\"Skipping auth_event fetch for outlier\")\n            return context\n\n        different_auth = event_auth_events.difference(\n            e.event_id for e in auth_events.values()\n        )\n\n        if not different_auth:\n            return context\n\n        logger.info(\n            \"auth_events refers to events which are not in our calculated auth \"\n            \"chain: %s\",\n            different_auth,\n        )\n\n        # XXX: currently this checks for redactions but I'm not convinced that is\n        # necessary?\n        different_events = await self.store.get_events_as_list(different_auth)\n\n        for d in different_events:\n            if d.room_id != event.room_id:\n                logger.warning(\n                    \"Event %s refers to auth_event %s which is in a different room\",\n                    event.event_id,\n                    d.event_id,\n                )\n\n                # don't attempt to resolve the claimed auth events against our own\n                # in this case: just use our own auth events.\n                #\n                # XXX: should we reject the event in this case? It feels like we should,\n                # but then shouldn't we also do so if we've failed to fetch any of the\n                # auth events?\n                return context\n\n        # now we state-resolve between our own idea of the auth events, and the remote's\n        # idea of them.\n\n        local_state = auth_events.values()\n        remote_auth_events = dict(auth_events)\n        remote_auth_events.update({(d.type, d.state_key): d for d in different_events})\n        remote_state = remote_auth_events.values()\n\n        room_version = await self.store.get_room_version_id(event.room_id)\n        new_state = await self.state_handler.resolve_events(\n            room_version, (local_state, remote_state), event\n        )\n\n        logger.info(\n            \"After state res: updating auth_events with new state %s\",\n            {\n                (d.type, d.state_key): d.event_id\n                for d in new_state.values()\n                if auth_events.get((d.type, d.state_key)) != d\n            },\n        )\n\n        auth_events.update(new_state)\n\n        context = await self._update_context_for_auth_events(\n            event, context, auth_events\n        )\n\n        return context\n\n    async def _update_context_for_auth_events(\n        self, event: EventBase, context: EventContext, auth_events: StateMap[EventBase]\n    ) -> EventContext:\n        \"\"\"Update the state_ids in an event context after auth event resolution,\n        storing the changes as a new state group.\n\n        Args:\n            event: The event we're handling the context for\n\n            context: initial event context\n\n            auth_events: Events to update in the event context.\n\n        Returns:\n            new event context\n        \"\"\"\n        # exclude the state key of the new event from the current_state in the context.\n        if event.is_state():\n            event_key = (event.type, event.state_key)  # type: Optional[Tuple[str, str]]\n        else:\n            event_key = None\n        state_updates = {\n            k: a.event_id for k, a in auth_events.items() if k != event_key\n        }\n\n        current_state_ids = await context.get_current_state_ids()\n        current_state_ids = dict(current_state_ids)  # type: ignore\n\n        current_state_ids.update(state_updates)\n\n        prev_state_ids = await context.get_prev_state_ids()\n        prev_state_ids = dict(prev_state_ids)\n\n        prev_state_ids.update({k: a.event_id for k, a in auth_events.items()})\n\n        # create a new state group as a delta from the existing one.\n        prev_group = context.state_group\n        state_group = await self.state_store.store_state_group(\n            event.event_id,\n            event.room_id,\n            prev_group=prev_group,\n            delta_ids=state_updates,\n            current_state_ids=current_state_ids,\n        )\n\n        return EventContext.with_state(\n            state_group=state_group,\n            state_group_before_event=context.state_group_before_event,\n            current_state_ids=current_state_ids,\n            prev_state_ids=prev_state_ids,\n            prev_group=prev_group,\n            delta_ids=state_updates,\n        )\n\n    async def construct_auth_difference(\n        self, local_auth: Iterable[EventBase], remote_auth: Iterable[EventBase]\n    ) -> Dict:\n        \"\"\" Given a local and remote auth chain, find the differences. This\n        assumes that we have already processed all events in remote_auth\n\n        Params:\n            local_auth (list)\n            remote_auth (list)\n\n        Returns:\n            dict\n        \"\"\"\n\n        logger.debug(\"construct_auth_difference Start!\")\n\n        # TODO: Make sure we are OK with local_auth or remote_auth having more\n        # auth events in them than strictly necessary.\n\n        def sort_fun(ev):\n            return ev.depth, ev.event_id\n\n        logger.debug(\"construct_auth_difference after sort_fun!\")\n\n        # We find the differences by starting at the \"bottom\" of each list\n        # and iterating up on both lists. The lists are ordered by depth and\n        # then event_id, we iterate up both lists until we find the event ids\n        # don't match. Then we look at depth/event_id to see which side is\n        # missing that event, and iterate only up that list. Repeat.\n\n        remote_list = list(remote_auth)\n        remote_list.sort(key=sort_fun)\n\n        local_list = list(local_auth)\n        local_list.sort(key=sort_fun)\n\n        local_iter = iter(local_list)\n        remote_iter = iter(remote_list)\n\n        logger.debug(\"construct_auth_difference before get_next!\")\n\n        def get_next(it, opt=None):\n            try:\n                return next(it)\n            except Exception:\n                return opt\n\n        current_local = get_next(local_iter)\n        current_remote = get_next(remote_iter)\n\n        logger.debug(\"construct_auth_difference before while\")\n\n        missing_remotes = []\n        missing_locals = []\n        while current_local or current_remote:\n            if current_remote is None:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n                continue\n\n            if current_local is None:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n            if current_local.event_id == current_remote.event_id:\n                current_local = get_next(local_iter)\n                current_remote = get_next(remote_iter)\n                continue\n\n            if current_local.depth < current_remote.depth:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n                continue\n\n            if current_local.depth > current_remote.depth:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n            # They have the same depth, so we fall back to the event_id order\n            if current_local.event_id < current_remote.event_id:\n                missing_locals.append(current_local)\n                current_local = get_next(local_iter)\n\n            if current_local.event_id > current_remote.event_id:\n                missing_remotes.append(current_remote)\n                current_remote = get_next(remote_iter)\n                continue\n\n        logger.debug(\"construct_auth_difference after while\")\n\n        # missing locals should be sent to the server\n        # We should find why we are missing remotes, as they will have been\n        # rejected.\n\n        # Remove events from missing_remotes if they are referencing a missing\n        # remote. We only care about the \"root\" rejected ones.\n        missing_remote_ids = [e.event_id for e in missing_remotes]\n        base_remote_rejected = list(missing_remotes)\n        for e in missing_remotes:\n            for e_id in e.auth_event_ids():\n                if e_id in missing_remote_ids:\n                    try:\n                        base_remote_rejected.remove(e)\n                    except ValueError:\n                        pass\n\n        reason_map = {}\n\n        for e in base_remote_rejected:\n            reason = await self.store.get_rejection_reason(e.event_id)\n            if reason is None:\n                # TODO: e is not in the current state, so we should\n                # construct some proof of that.\n                continue\n\n            reason_map[e.event_id] = reason\n\n        logger.debug(\"construct_auth_difference returning\")\n\n        return {\n            \"auth_chain\": local_auth,\n            \"rejects\": {\n                e.event_id: {\"reason\": reason_map[e.event_id], \"proof\": None}\n                for e in base_remote_rejected\n            },\n            \"missing\": [e.event_id for e in missing_locals],\n        }\n\n    @log_function\n    async def exchange_third_party_invite(\n        self, sender_user_id, target_user_id, room_id, signed\n    ):\n        third_party_invite = {\"signed\": signed}\n\n        event_dict = {\n            \"type\": EventTypes.Member,\n            \"content\": {\n                \"membership\": Membership.INVITE,\n                \"third_party_invite\": third_party_invite,\n            },\n            \"room_id\": room_id,\n            \"sender\": sender_user_id,\n            \"state_key\": target_user_id,\n        }\n\n        if await self.auth.check_host_in_room(room_id, self.hs.hostname):\n            room_version = await self.store.get_room_version_id(room_id)\n            builder = self.event_builder_factory.new(room_version, event_dict)\n\n            EventValidator().validate_builder(builder)\n            event, context = await self.event_creation_handler.create_new_client_event(\n                builder=builder\n            )\n\n            event, context = await self.add_display_name_to_third_party_invite(\n                room_version, event_dict, event, context\n            )\n\n            EventValidator().validate_new(event, self.config)\n\n            # We need to tell the transaction queue to send this out, even\n            # though the sender isn't a local user.\n            event.internal_metadata.send_on_behalf_of = self.hs.hostname\n\n            try:\n                await self.auth.check_from_context(room_version, event, context)\n            except AuthError as e:\n                logger.warning(\"Denying new third party invite %r because %s\", event, e)\n                raise e\n\n            await self._check_signature(event, context)\n\n            # We retrieve the room member handler here as to not cause a cyclic dependency\n            member_handler = self.hs.get_room_member_handler()\n            await member_handler.send_membership_event(None, event, context)\n        else:\n            destinations = {x.split(\":\", 1)[-1] for x in (sender_user_id, room_id)}\n            await self.federation_client.forward_third_party_invite(\n                destinations, room_id, event_dict\n            )\n\n    async def on_exchange_third_party_invite_request(\n        self, event_dict: JsonDict\n    ) -> None:\n        \"\"\"Handle an exchange_third_party_invite request from a remote server\n\n        The remote server will call this when it wants to turn a 3pid invite\n        into a normal m.room.member invite.\n\n        Args:\n            event_dict: Dictionary containing the event body.\n\n        \"\"\"\n        assert_params_in_dict(event_dict, [\"room_id\"])\n        room_version = await self.store.get_room_version_id(event_dict[\"room_id\"])\n\n        # NB: event_dict has a particular specced format we might need to fudge\n        # if we change event formats too much.\n        builder = self.event_builder_factory.new(room_version, event_dict)\n\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n        event, context = await self.add_display_name_to_third_party_invite(\n            room_version, event_dict, event, context\n        )\n\n        try:\n            await self.auth.check_from_context(room_version, event, context)\n        except AuthError as e:\n            logger.warning(\"Denying third party invite %r because %s\", event, e)\n            raise e\n        await self._check_signature(event, context)\n\n        # We need to tell the transaction queue to send this out, even\n        # though the sender isn't a local user.\n        event.internal_metadata.send_on_behalf_of = get_domain_from_id(event.sender)\n\n        # We retrieve the room member handler here as to not cause a cyclic dependency\n        member_handler = self.hs.get_room_member_handler()\n        await member_handler.send_membership_event(None, event, context)\n\n    async def add_display_name_to_third_party_invite(\n        self, room_version, event_dict, event, context\n    ):\n        key = (\n            EventTypes.ThirdPartyInvite,\n            event.content[\"third_party_invite\"][\"signed\"][\"token\"],\n        )\n        original_invite = None\n        prev_state_ids = await context.get_prev_state_ids()\n        original_invite_id = prev_state_ids.get(key)\n        if original_invite_id:\n            original_invite = await self.store.get_event(\n                original_invite_id, allow_none=True\n            )\n        if original_invite:\n            # If the m.room.third_party_invite event's content is empty, it means the\n            # invite has been revoked. In this case, we don't have to raise an error here\n            # because the auth check will fail on the invite (because it's not able to\n            # fetch public keys from the m.room.third_party_invite event's content, which\n            # is empty).\n            display_name = original_invite.content.get(\"display_name\")\n            event_dict[\"content\"][\"third_party_invite\"][\"display_name\"] = display_name\n        else:\n            logger.info(\n                \"Could not find invite event for third_party_invite: %r\", event_dict\n            )\n            # We don't discard here as this is not the appropriate place to do\n            # auth checks. If we need the invite and don't have it then the\n            # auth check code will explode appropriately.\n\n        builder = self.event_builder_factory.new(room_version, event_dict)\n        EventValidator().validate_builder(builder)\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n        EventValidator().validate_new(event, self.config)\n        return (event, context)\n\n    async def _check_signature(self, event, context):\n        \"\"\"\n        Checks that the signature in the event is consistent with its invite.\n\n        Args:\n            event (Event): The m.room.member event to check\n            context (EventContext):\n\n        Raises:\n            AuthError: if signature didn't match any keys, or key has been\n                revoked,\n            SynapseError: if a transient error meant a key couldn't be checked\n                for revocation.\n        \"\"\"\n        signed = event.content[\"third_party_invite\"][\"signed\"]\n        token = signed[\"token\"]\n\n        prev_state_ids = await context.get_prev_state_ids()\n        invite_event_id = prev_state_ids.get((EventTypes.ThirdPartyInvite, token))\n\n        invite_event = None\n        if invite_event_id:\n            invite_event = await self.store.get_event(invite_event_id, allow_none=True)\n\n        if not invite_event:\n            raise AuthError(403, \"Could not find invite\")\n\n        logger.debug(\"Checking auth on event %r\", event.content)\n\n        last_exception = None  # type: Optional[Exception]\n\n        # for each public key in the 3pid invite event\n        for public_key_object in self.hs.get_auth().get_public_keys(invite_event):\n            try:\n                # for each sig on the third_party_invite block of the actual invite\n                for server, signature_block in signed[\"signatures\"].items():\n                    for key_name, encoded_signature in signature_block.items():\n                        if not key_name.startswith(\"ed25519:\"):\n                            continue\n\n                        logger.debug(\n                            \"Attempting to verify sig with key %s from %r \"\n                            \"against pubkey %r\",\n                            key_name,\n                            server,\n                            public_key_object,\n                        )\n\n                        try:\n                            public_key = public_key_object[\"public_key\"]\n                            verify_key = decode_verify_key_bytes(\n                                key_name, decode_base64(public_key)\n                            )\n                            verify_signed_json(signed, server, verify_key)\n                            logger.debug(\n                                \"Successfully verified sig with key %s from %r \"\n                                \"against pubkey %r\",\n                                key_name,\n                                server,\n                                public_key_object,\n                            )\n                        except Exception:\n                            logger.info(\n                                \"Failed to verify sig with key %s from %r \"\n                                \"against pubkey %r\",\n                                key_name,\n                                server,\n                                public_key_object,\n                            )\n                            raise\n                        try:\n                            if \"key_validity_url\" in public_key_object:\n                                await self._check_key_revocation(\n                                    public_key, public_key_object[\"key_validity_url\"]\n                                )\n                        except Exception:\n                            logger.info(\n                                \"Failed to query key_validity_url %s\",\n                                public_key_object[\"key_validity_url\"],\n                            )\n                            raise\n                        return\n            except Exception as e:\n                last_exception = e\n\n        if last_exception is None:\n            # we can only get here if get_public_keys() returned an empty list\n            # TODO: make this better\n            raise RuntimeError(\"no public key in invite event\")\n\n        raise last_exception\n\n    async def _check_key_revocation(self, public_key, url):\n        \"\"\"\n        Checks whether public_key has been revoked.\n\n        Args:\n            public_key (str): base-64 encoded public key.\n            url (str): Key revocation URL.\n\n        Raises:\n            AuthError: if they key has been revoked.\n            SynapseError: if a transient error meant a key couldn't be checked\n                for revocation.\n        \"\"\"\n        try:\n            response = await self.http_client.get_json(url, {\"public_key\": public_key})\n        except Exception:\n            raise SynapseError(502, \"Third party certificate could not be checked\")\n        if \"valid\" not in response or not response[\"valid\"]:\n            raise AuthError(403, \"Third party certificate was invalid\")\n\n    async def persist_events_and_notify(\n        self,\n        room_id: str,\n        event_and_contexts: Sequence[Tuple[EventBase, EventContext]],\n        backfilled: bool = False,\n    ) -> int:\n        \"\"\"Persists events and tells the notifier/pushers about them, if\n        necessary.\n\n        Args:\n            room_id: The room ID of events being persisted.\n            event_and_contexts: Sequence of events with their associated\n                context that should be persisted. All events must belong to\n                the same room.\n            backfilled: Whether these events are a result of\n                backfilling or not\n        \"\"\"\n        instance = self.config.worker.events_shard_config.get_instance(room_id)\n        if instance != self._instance_name:\n            result = await self._send_events(\n                instance_name=instance,\n                store=self.store,\n                room_id=room_id,\n                event_and_contexts=event_and_contexts,\n                backfilled=backfilled,\n            )\n            return result[\"max_stream_id\"]\n        else:\n            assert self.storage.persistence\n\n            # Note that this returns the events that were persisted, which may not be\n            # the same as were passed in if some were deduplicated due to transaction IDs.\n            events, max_stream_token = await self.storage.persistence.persist_events(\n                event_and_contexts, backfilled=backfilled\n            )\n\n            if self._ephemeral_messages_enabled:\n                for event in events:\n                    # If there's an expiry timestamp on the event, schedule its expiry.\n                    self._message_handler.maybe_schedule_expiry(event)\n\n            if not backfilled:  # Never notify for backfilled events\n                for event in events:\n                    await self._notify_persisted_event(event, max_stream_token)\n\n            return max_stream_token.stream\n\n    async def _notify_persisted_event(\n        self, event: EventBase, max_stream_token: RoomStreamToken\n    ) -> None:\n        \"\"\"Checks to see if notifier/pushers should be notified about the\n        event or not.\n\n        Args:\n            event:\n            max_stream_id: The max_stream_id returned by persist_events\n        \"\"\"\n\n        extra_users = []\n        if event.type == EventTypes.Member:\n            target_user_id = event.state_key\n\n            # We notify for memberships if its an invite for one of our\n            # users\n            if event.internal_metadata.is_outlier():\n                if event.membership != Membership.INVITE:\n                    if not self.is_mine_id(target_user_id):\n                        return\n\n            target_user = UserID.from_string(target_user_id)\n            extra_users.append(target_user)\n        elif event.internal_metadata.is_outlier():\n            return\n\n        # the event has been persisted so it should have a stream ordering.\n        assert event.internal_metadata.stream_ordering\n\n        event_pos = PersistedEventPosition(\n            self._instance_name, event.internal_metadata.stream_ordering\n        )\n        self.notifier.on_new_room_event(\n            event, event_pos, max_stream_token, extra_users=extra_users\n        )\n\n    async def _clean_room_for_join(self, room_id: str) -> None:\n        \"\"\"Called to clean up any data in DB for a given room, ready for the\n        server to join the room.\n\n        Args:\n            room_id\n        \"\"\"\n        if self.config.worker_app:\n            await self._clean_room_for_join_client(room_id)\n        else:\n            await self.store.clean_room_for_join(room_id)\n\n    async def get_room_complexity(\n        self, remote_room_hosts: List[str], room_id: str\n    ) -> Optional[dict]:\n        \"\"\"\n        Fetch the complexity of a remote room over federation.\n\n        Args:\n            remote_room_hosts (list[str]): The remote servers to ask.\n            room_id (str): The room ID to ask about.\n\n        Returns:\n            Dict contains the complexity\n            metric versions, while None means we could not fetch the complexity.\n        \"\"\"\n\n        for host in remote_room_hosts:\n            res = await self.federation_client.get_room_complexity(host, room_id)\n\n            # We got a result, return it.\n            if res:\n                return res\n\n        # We fell off the bottom, couldn't get the complexity from anyone. Oh\n        # well.\n        return None", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Fhandlers%2Fidentity.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2015, 2016 OpenMarket Ltd\n# Copyright 2017 Vector Creations Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for interacting with Identity Servers\"\"\"\n\nimport logging\nimport urllib.parse\nfrom typing import Awaitable, Callable, Dict, List, Optional, Tuple\n\nfrom synapse.api.errors import (\n    CodeMessageException,\n    Codes,\n    HttpResponseException,\n    SynapseError,\n)\nfrom synapse.config.emailconfig import ThreepidBehaviour\nfrom synapse.http import RequestTimedOutError\nfrom synapse.http.client import SimpleHttpClient\nfrom synapse.types import JsonDict, Requester\nfrom synapse.util import json_decoder\nfrom synapse.util.hash import sha256_and_url_safe_base64\nfrom synapse.util.stringutils import assert_valid_client_secret, random_string\n\nfrom ._base import BaseHandler\n\nlogger = logging.getLogger(__name__)\n\nid_server_scheme = \"https://\"\n\n\nclass IdentityHandler(BaseHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        # An HTTP client for contacting trusted URLs.\n        self.http_client = SimpleHttpClient(hs)\n        # An HTTP client for contacting identity servers specified by clients.\n        self.blacklisting_http_client = SimpleHttpClient(\n            hs, ip_blacklist=hs.config.federation_ip_range_blacklist\n        )\n        self.federation_http_client = hs.get_federation_http_client()\n        self.hs = hs\n\n    async def threepid_from_creds(\n        self, id_server: str, creds: Dict[str, str]\n    ) -> Optional[JsonDict]:\n        \"\"\"\n        Retrieve and validate a threepid identifier from a \"credentials\" dictionary against a\n        given identity server\n\n        Args:\n            id_server: The identity server to validate 3PIDs against. Must be a\n                complete URL including the protocol (http(s)://)\n            creds: Dictionary containing the following keys:\n                * client_secret|clientSecret: A unique secret str provided by the client\n                * sid: The ID of the validation session\n\n        Returns:\n            A dictionary consisting of response params to the /getValidated3pid\n            endpoint of the Identity Service API, or None if the threepid was not found\n        \"\"\"\n        client_secret = creds.get(\"client_secret\") or creds.get(\"clientSecret\")\n        if not client_secret:\n            raise SynapseError(\n                400, \"Missing param client_secret in creds\", errcode=Codes.MISSING_PARAM\n            )\n        assert_valid_client_secret(client_secret)\n\n        session_id = creds.get(\"sid\")\n        if not session_id:\n            raise SynapseError(\n                400, \"Missing param session_id in creds\", errcode=Codes.MISSING_PARAM\n            )\n\n        query_params = {\"sid\": session_id, \"client_secret\": client_secret}\n\n        url = id_server + \"/_matrix/identity/api/v1/3pid/getValidated3pid\"\n\n        try:\n            data = await self.http_client.get_json(url, query_params)\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except HttpResponseException as e:\n            logger.info(\n                \"%s returned %i for threepid validation for: %s\",\n                id_server,\n                e.code,\n                creds,\n            )\n            return None\n\n        # Old versions of Sydent return a 200 http code even on a failed validation\n        # check. Thus, in addition to the HttpResponseException check above (which\n        # checks for non-200 errors), we need to make sure validation_session isn't\n        # actually an error, identified by the absence of a \"medium\" key\n        # See https://github.com/matrix-org/sydent/issues/215 for details\n        if \"medium\" in data:\n            return data\n\n        logger.info(\"%s reported non-validated threepid: %s\", id_server, creds)\n        return None\n\n    async def bind_threepid(\n        self,\n        client_secret: str,\n        sid: str,\n        mxid: str,\n        id_server: str,\n        id_access_token: Optional[str] = None,\n        use_v2: bool = True,\n    ) -> JsonDict:\n        \"\"\"Bind a 3PID to an identity server\n\n        Args:\n            client_secret: A unique secret provided by the client\n            sid: The ID of the validation session\n            mxid: The MXID to bind the 3PID to\n            id_server: The domain of the identity server to query\n            id_access_token: The access token to authenticate to the identity\n                server with, if necessary. Required if use_v2 is true\n            use_v2: Whether to use v2 Identity Service API endpoints. Defaults to True\n\n        Returns:\n            The response from the identity server\n        \"\"\"\n        logger.debug(\"Proxying threepid bind request for %s to %s\", mxid, id_server)\n\n        # If an id_access_token is not supplied, force usage of v1\n        if id_access_token is None:\n            use_v2 = False\n\n        # Decide which API endpoint URLs to use\n        headers = {}\n        bind_data = {\"sid\": sid, \"client_secret\": client_secret, \"mxid\": mxid}\n        if use_v2:\n            bind_url = \"https://%s/_matrix/identity/v2/3pid/bind\" % (id_server,)\n            headers[\"Authorization\"] = create_id_access_token_header(id_access_token)  # type: ignore\n        else:\n            bind_url = \"https://%s/_matrix/identity/api/v1/3pid/bind\" % (id_server,)\n\n        try:\n            # Use the blacklisting http client as this call is only to identity servers\n            # provided by a client\n            data = await self.blacklisting_http_client.post_json_get_json(\n                bind_url, bind_data, headers=headers\n            )\n\n            # Remember where we bound the threepid\n            await self.store.add_user_bound_threepid(\n                user_id=mxid,\n                medium=data[\"medium\"],\n                address=data[\"address\"],\n                id_server=id_server,\n            )\n\n            return data\n        except HttpResponseException as e:\n            if e.code != 404 or not use_v2:\n                logger.error(\"3PID bind failed with Matrix error: %r\", e)\n                raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except CodeMessageException as e:\n            data = json_decoder.decode(e.msg)  # XXX WAT?\n            return data\n\n        logger.info(\"Got 404 when POSTing JSON %s, falling back to v1 URL\", bind_url)\n        res = await self.bind_threepid(\n            client_secret, sid, mxid, id_server, id_access_token, use_v2=False\n        )\n        return res\n\n    async def try_unbind_threepid(self, mxid: str, threepid: dict) -> bool:\n        \"\"\"Attempt to remove a 3PID from an identity server, or if one is not provided, all\n        identity servers we're aware the binding is present on\n\n        Args:\n            mxid: Matrix user ID of binding to be removed\n            threepid: Dict with medium & address of binding to be\n                removed, and an optional id_server.\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            True on success, otherwise False if the identity\n            server doesn't support unbinding (or no identity server found to\n            contact).\n        \"\"\"\n        if threepid.get(\"id_server\"):\n            id_servers = [threepid[\"id_server\"]]\n        else:\n            id_servers = await self.store.get_id_servers_user_bound(\n                user_id=mxid, medium=threepid[\"medium\"], address=threepid[\"address\"]\n            )\n\n        # We don't know where to unbind, so we don't have a choice but to return\n        if not id_servers:\n            return False\n\n        changed = True\n        for id_server in id_servers:\n            changed &= await self.try_unbind_threepid_with_id_server(\n                mxid, threepid, id_server\n            )\n\n        return changed\n\n    async def try_unbind_threepid_with_id_server(\n        self, mxid: str, threepid: dict, id_server: str\n    ) -> bool:\n        \"\"\"Removes a binding from an identity server\n\n        Args:\n            mxid: Matrix user ID of binding to be removed\n            threepid: Dict with medium & address of binding to be removed\n            id_server: Identity server to unbind from\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            True on success, otherwise False if the identity\n            server doesn't support unbinding\n        \"\"\"\n        url = \"https://%s/_matrix/identity/api/v1/3pid/unbind\" % (id_server,)\n        url_bytes = \"/_matrix/identity/api/v1/3pid/unbind\".encode(\"ascii\")\n\n        content = {\n            \"mxid\": mxid,\n            \"threepid\": {\"medium\": threepid[\"medium\"], \"address\": threepid[\"address\"]},\n        }\n\n        # we abuse the federation http client to sign the request, but we have to send it\n        # using the normal http client since we don't want the SRV lookup and want normal\n        # 'browser-like' HTTPS.\n        auth_headers = self.federation_http_client.build_auth_headers(\n            destination=None,\n            method=b\"POST\",\n            url_bytes=url_bytes,\n            content=content,\n            destination_is=id_server.encode(\"ascii\"),\n        )\n        headers = {b\"Authorization\": auth_headers}\n\n        try:\n            # Use the blacklisting http client as this call is only to identity servers\n            # provided by a client\n            await self.blacklisting_http_client.post_json_get_json(\n                url, content, headers\n            )\n            changed = True\n        except HttpResponseException as e:\n            changed = False\n            if e.code in (400, 404, 501):\n                # The remote server probably doesn't support unbinding (yet)\n                logger.warning(\"Received %d response while unbinding threepid\", e.code)\n            else:\n                logger.error(\"Failed to unbind threepid on identity server: %s\", e)\n                raise SynapseError(500, \"Failed to contact identity server\")\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        await self.store.remove_user_bound_threepid(\n            user_id=mxid,\n            medium=threepid[\"medium\"],\n            address=threepid[\"address\"],\n            id_server=id_server,\n        )\n\n        return changed\n\n    async def send_threepid_validation(\n        self,\n        email_address: str,\n        client_secret: str,\n        send_attempt: int,\n        send_email_func: Callable[[str, str, str, str], Awaitable],\n        next_link: Optional[str] = None,\n    ) -> str:\n        \"\"\"Send a threepid validation email for password reset or\n        registration purposes\n\n        Args:\n            email_address: The user's email address\n            client_secret: The provided client secret\n            send_attempt: Which send attempt this is\n            send_email_func: A function that takes an email address, token,\n                             client_secret and session_id, sends an email\n                             and returns an Awaitable.\n            next_link: The URL to redirect the user to after validation\n\n        Returns:\n            The new session_id upon success\n\n        Raises:\n            SynapseError is an error occurred when sending the email\n        \"\"\"\n        # Check that this email/client_secret/send_attempt combo is new or\n        # greater than what we've seen previously\n        session = await self.store.get_threepid_validation_session(\n            \"email\", client_secret, address=email_address, validated=False\n        )\n\n        # Check to see if a session already exists and that it is not yet\n        # marked as validated\n        if session and session.get(\"validated_at\") is None:\n            session_id = session[\"session_id\"]\n            last_send_attempt = session[\"last_send_attempt\"]\n\n            # Check that the send_attempt is higher than previous attempts\n            if send_attempt <= last_send_attempt:\n                # If not, just return a success without sending an email\n                return session_id\n        else:\n            # An non-validated session does not exist yet.\n            # Generate a session id\n            session_id = random_string(16)\n\n        if next_link:\n            # Manipulate the next_link to add the sid, because the caller won't get\n            # it until we send a response, by which time we've sent the mail.\n            if \"?\" in next_link:\n                next_link += \"&\"\n            else:\n                next_link += \"?\"\n            next_link += \"sid=\" + urllib.parse.quote(session_id)\n\n        # Generate a new validation token\n        token = random_string(32)\n\n        # Send the mail with the link containing the token, client_secret\n        # and session_id\n        try:\n            await send_email_func(email_address, token, client_secret, session_id)\n        except Exception:\n            logger.exception(\n                \"Error sending threepid validation email to %s\", email_address\n            )\n            raise SynapseError(500, \"An error was encountered when sending the email\")\n\n        token_expires = (\n            self.hs.get_clock().time_msec()\n            + self.hs.config.email_validation_token_lifetime\n        )\n\n        await self.store.start_or_continue_validation_session(\n            \"email\",\n            email_address,\n            session_id,\n            client_secret,\n            send_attempt,\n            next_link,\n            token,\n            token_expires,\n        )\n\n        return session_id\n\n    async def requestEmailToken(\n        self,\n        id_server: str,\n        email: str,\n        client_secret: str,\n        send_attempt: int,\n        next_link: Optional[str] = None,\n    ) -> JsonDict:\n        \"\"\"\n        Request an external server send an email on our behalf for the purposes of threepid\n        validation.\n\n        Args:\n            id_server: The identity server to proxy to\n            email: The email to send the message to\n            client_secret: The unique client_secret sends by the user\n            send_attempt: Which attempt this is\n            next_link: A link to redirect the user to once they submit the token\n\n        Returns:\n            The json response body from the server\n        \"\"\"\n        params = {\n            \"email\": email,\n            \"client_secret\": client_secret,\n            \"send_attempt\": send_attempt,\n        }\n        if next_link:\n            params[\"next_link\"] = next_link\n\n        if self.hs.config.using_identity_server_from_trusted_list:\n            # Warn that a deprecated config option is in use\n            logger.warning(\n                'The config option \"trust_identity_server_for_password_resets\" '\n                'has been replaced by \"account_threepid_delegate\". '\n                \"Please consult the sample config at docs/sample_config.yaml for \"\n                \"details and update your config file.\"\n            )\n\n        try:\n            data = await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/email/requestToken\",\n                params,\n            )\n            return data\n        except HttpResponseException as e:\n            logger.info(\"Proxied requestToken failed: %r\", e)\n            raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n    async def requestMsisdnToken(\n        self,\n        id_server: str,\n        country: str,\n        phone_number: str,\n        client_secret: str,\n        send_attempt: int,\n        next_link: Optional[str] = None,\n    ) -> JsonDict:\n        \"\"\"\n        Request an external server send an SMS message on our behalf for the purposes of\n        threepid validation.\n        Args:\n            id_server: The identity server to proxy to\n            country: The country code of the phone number\n            phone_number: The number to send the message to\n            client_secret: The unique client_secret sends by the user\n            send_attempt: Which attempt this is\n            next_link: A link to redirect the user to once they submit the token\n\n        Returns:\n            The json response body from the server\n        \"\"\"\n        params = {\n            \"country\": country,\n            \"phone_number\": phone_number,\n            \"client_secret\": client_secret,\n            \"send_attempt\": send_attempt,\n        }\n        if next_link:\n            params[\"next_link\"] = next_link\n\n        if self.hs.config.using_identity_server_from_trusted_list:\n            # Warn that a deprecated config option is in use\n            logger.warning(\n                'The config option \"trust_identity_server_for_password_resets\" '\n                'has been replaced by \"account_threepid_delegate\". '\n                \"Please consult the sample config at docs/sample_config.yaml for \"\n                \"details and update your config file.\"\n            )\n\n        try:\n            data = await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/msisdn/requestToken\",\n                params,\n            )\n        except HttpResponseException as e:\n            logger.info(\"Proxied requestToken failed: %r\", e)\n            raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        assert self.hs.config.public_baseurl\n\n        # we need to tell the client to send the token back to us, since it doesn't\n        # otherwise know where to send it, so add submit_url response parameter\n        # (see also MSC2078)\n        data[\"submit_url\"] = (\n            self.hs.config.public_baseurl\n            + \"_matrix/client/unstable/add_threepid/msisdn/submit_token\"\n        )\n        return data\n\n    async def validate_threepid_session(\n        self, client_secret: str, sid: str\n    ) -> Optional[JsonDict]:\n        \"\"\"Validates a threepid session with only the client secret and session ID\n        Tries validating against any configured account_threepid_delegates as well as locally.\n\n        Args:\n            client_secret: A secret provided by the client\n            sid: The ID of the session\n\n        Returns:\n            The json response if validation was successful, otherwise None\n        \"\"\"\n        # XXX: We shouldn't need to keep wrapping and unwrapping this value\n        threepid_creds = {\"client_secret\": client_secret, \"sid\": sid}\n\n        # We don't actually know which medium this 3PID is. Thus we first assume it's email,\n        # and if validation fails we try msisdn\n        validation_session = None\n\n        # Try to validate as email\n        if self.hs.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:\n            # Ask our delegated email identity server\n            validation_session = await self.threepid_from_creds(\n                self.hs.config.account_threepid_delegate_email, threepid_creds\n            )\n        elif self.hs.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:\n            # Get a validated session matching these details\n            validation_session = await self.store.get_threepid_validation_session(\n                \"email\", client_secret, sid=sid, validated=True\n            )\n\n        if validation_session:\n            return validation_session\n\n        # Try to validate as msisdn\n        if self.hs.config.account_threepid_delegate_msisdn:\n            # Ask our delegated msisdn identity server\n            validation_session = await self.threepid_from_creds(\n                self.hs.config.account_threepid_delegate_msisdn, threepid_creds\n            )\n\n        return validation_session\n\n    async def proxy_msisdn_submit_token(\n        self, id_server: str, client_secret: str, sid: str, token: str\n    ) -> JsonDict:\n        \"\"\"Proxy a POST submitToken request to an identity server for verification purposes\n\n        Args:\n            id_server: The identity server URL to contact\n            client_secret: Secret provided by the client\n            sid: The ID of the session\n            token: The verification token\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            The response dict from the identity server\n        \"\"\"\n        body = {\"client_secret\": client_secret, \"sid\": sid, \"token\": token}\n\n        try:\n            return await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/msisdn/submitToken\",\n                body,\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except HttpResponseException as e:\n            logger.warning(\"Error contacting msisdn account_threepid_delegate: %s\", e)\n            raise SynapseError(400, \"Error contacting the identity server\")\n\n    async def lookup_3pid(\n        self,\n        id_server: str,\n        medium: str,\n        address: str,\n        id_access_token: Optional[str] = None,\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n            id_access_token: The access token to authenticate to the identity\n                server with\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognized.\n        \"\"\"\n        if id_access_token is not None:\n            try:\n                results = await self._lookup_3pid_v2(\n                    id_server, id_access_token, medium, address\n                )\n                return results\n\n            except Exception as e:\n                # Catch HttpResponseExcept for a non-200 response code\n                # Check if this identity server does not know about v2 lookups\n                if isinstance(e, HttpResponseException) and e.code == 404:\n                    # This is an old identity server that does not yet support v2 lookups\n                    logger.warning(\n                        \"Attempted v2 lookup on v1 identity server %s. Falling \"\n                        \"back to v1\",\n                        id_server,\n                    )\n                else:\n                    logger.warning(\"Error when looking up hashing details: %s\", e)\n                    return None\n\n        return await self._lookup_3pid_v1(id_server, medium, address)\n\n    async def _lookup_3pid_v1(\n        self, id_server: str, medium: str, address: str\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server using v1 lookup.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognized.\n        \"\"\"\n        try:\n            data = await self.blacklisting_http_client.get_json(\n                \"%s%s/_matrix/identity/api/v1/lookup\" % (id_server_scheme, id_server),\n                {\"medium\": medium, \"address\": address},\n            )\n\n            if \"mxid\" in data:\n                # note: we used to verify the identity server's signature here, but no longer\n                # require or validate it. See the following for context:\n                # https://github.com/matrix-org/synapse/issues/5253#issuecomment-666246950\n                return data[\"mxid\"]\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except IOError as e:\n            logger.warning(\"Error from v1 identity server lookup: %s\" % (e,))\n\n        return None\n\n    async def _lookup_3pid_v2(\n        self, id_server: str, id_access_token: str, medium: str, address: str\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server using v2 lookup.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            id_access_token: The access token to authenticate to the identity server with\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognised.\n        \"\"\"\n        # Check what hashing details are supported by this identity server\n        try:\n            hash_details = await self.blacklisting_http_client.get_json(\n                \"%s%s/_matrix/identity/v2/hash_details\" % (id_server_scheme, id_server),\n                {\"access_token\": id_access_token},\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        if not isinstance(hash_details, dict):\n            logger.warning(\n                \"Got non-dict object when checking hash details of %s%s: %s\",\n                id_server_scheme,\n                id_server,\n                hash_details,\n            )\n            raise SynapseError(\n                400,\n                \"Non-dict object from %s%s during v2 hash_details request: %s\"\n                % (id_server_scheme, id_server, hash_details),\n            )\n\n        # Extract information from hash_details\n        supported_lookup_algorithms = hash_details.get(\"algorithms\")\n        lookup_pepper = hash_details.get(\"lookup_pepper\")\n        if (\n            not supported_lookup_algorithms\n            or not isinstance(supported_lookup_algorithms, list)\n            or not lookup_pepper\n            or not isinstance(lookup_pepper, str)\n        ):\n            raise SynapseError(\n                400,\n                \"Invalid hash details received from identity server %s%s: %s\"\n                % (id_server_scheme, id_server, hash_details),\n            )\n\n        # Check if any of the supported lookup algorithms are present\n        if LookupAlgorithm.SHA256 in supported_lookup_algorithms:\n            # Perform a hashed lookup\n            lookup_algorithm = LookupAlgorithm.SHA256\n\n            # Hash address, medium and the pepper with sha256\n            to_hash = \"%s %s %s\" % (address, medium, lookup_pepper)\n            lookup_value = sha256_and_url_safe_base64(to_hash)\n\n        elif LookupAlgorithm.NONE in supported_lookup_algorithms:\n            # Perform a non-hashed lookup\n            lookup_algorithm = LookupAlgorithm.NONE\n\n            # Combine together plaintext address and medium\n            lookup_value = \"%s %s\" % (address, medium)\n\n        else:\n            logger.warning(\n                \"None of the provided lookup algorithms of %s are supported: %s\",\n                id_server,\n                supported_lookup_algorithms,\n            )\n            raise SynapseError(\n                400,\n                \"Provided identity server does not support any v2 lookup \"\n                \"algorithms that this homeserver supports.\",\n            )\n\n        # Authenticate with identity server given the access token from the client\n        headers = {\"Authorization\": create_id_access_token_header(id_access_token)}\n\n        try:\n            lookup_results = await self.blacklisting_http_client.post_json_get_json(\n                \"%s%s/_matrix/identity/v2/lookup\" % (id_server_scheme, id_server),\n                {\n                    \"addresses\": [lookup_value],\n                    \"algorithm\": lookup_algorithm,\n                    \"pepper\": lookup_pepper,\n                },\n                headers=headers,\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except Exception as e:\n            logger.warning(\"Error when performing a v2 3pid lookup: %s\", e)\n            raise SynapseError(\n                500, \"Unknown error occurred during identity server lookup\"\n            )\n\n        # Check for a mapping from what we looked up to an MXID\n        if \"mappings\" not in lookup_results or not isinstance(\n            lookup_results[\"mappings\"], dict\n        ):\n            logger.warning(\"No results from 3pid lookup\")\n            return None\n\n        # Return the MXID if it's available, or None otherwise\n        mxid = lookup_results[\"mappings\"].get(lookup_value)\n        return mxid\n\n    async def ask_id_server_for_third_party_invite(\n        self,\n        requester: Requester,\n        id_server: str,\n        medium: str,\n        address: str,\n        room_id: str,\n        inviter_user_id: str,\n        room_alias: str,\n        room_avatar_url: str,\n        room_join_rules: str,\n        room_name: str,\n        inviter_display_name: str,\n        inviter_avatar_url: str,\n        id_access_token: Optional[str] = None,\n    ) -> Tuple[str, List[Dict[str, str]], Dict[str, str], str]:\n        \"\"\"\n        Asks an identity server for a third party invite.\n\n        Args:\n            requester\n            id_server: hostname + optional port for the identity server.\n            medium: The literal string \"email\".\n            address: The third party address being invited.\n            room_id: The ID of the room to which the user is invited.\n            inviter_user_id: The user ID of the inviter.\n            room_alias: An alias for the room, for cosmetic notifications.\n            room_avatar_url: The URL of the room's avatar, for cosmetic\n                notifications.\n            room_join_rules: The join rules of the email (e.g. \"public\").\n            room_name: The m.room.name of the room.\n            inviter_display_name: The current display name of the\n                inviter.\n            inviter_avatar_url: The URL of the inviter's avatar.\n            id_access_token (str|None): The access token to authenticate to the identity\n                server with\n\n        Returns:\n            A tuple containing:\n                token: The token which must be signed to prove authenticity.\n                public_keys ([{\"public_key\": str, \"key_validity_url\": str}]):\n                    public_key is a base64-encoded ed25519 public key.\n                fallback_public_key: One element from public_keys.\n                display_name: A user-friendly name to represent the invited user.\n        \"\"\"\n        invite_config = {\n            \"medium\": medium,\n            \"address\": address,\n            \"room_id\": room_id,\n            \"room_alias\": room_alias,\n            \"room_avatar_url\": room_avatar_url,\n            \"room_join_rules\": room_join_rules,\n            \"room_name\": room_name,\n            \"sender\": inviter_user_id,\n            \"sender_display_name\": inviter_display_name,\n            \"sender_avatar_url\": inviter_avatar_url,\n        }\n\n        # Add the identity service access token to the JSON body and use the v2\n        # Identity Service endpoints if id_access_token is present\n        data = None\n        base_url = \"%s%s/_matrix/identity\" % (id_server_scheme, id_server)\n\n        if id_access_token:\n            key_validity_url = \"%s%s/_matrix/identity/v2/pubkey/isvalid\" % (\n                id_server_scheme,\n                id_server,\n            )\n\n            # Attempt a v2 lookup\n            url = base_url + \"/v2/store-invite\"\n            try:\n                data = await self.blacklisting_http_client.post_json_get_json(\n                    url,\n                    invite_config,\n                    {\"Authorization\": create_id_access_token_header(id_access_token)},\n                )\n            except RequestTimedOutError:\n                raise SynapseError(500, \"Timed out contacting identity server\")\n            except HttpResponseException as e:\n                if e.code != 404:\n                    logger.info(\"Failed to POST %s with JSON: %s\", url, e)\n                    raise e\n\n        if data is None:\n            key_validity_url = \"%s%s/_matrix/identity/api/v1/pubkey/isvalid\" % (\n                id_server_scheme,\n                id_server,\n            )\n            url = base_url + \"/api/v1/store-invite\"\n\n            try:\n                data = await self.blacklisting_http_client.post_json_get_json(\n                    url, invite_config\n                )\n            except RequestTimedOutError:\n                raise SynapseError(500, \"Timed out contacting identity server\")\n            except HttpResponseException as e:\n                logger.warning(\n                    \"Error trying to call /store-invite on %s%s: %s\",\n                    id_server_scheme,\n                    id_server,\n                    e,\n                )\n\n            if data is None:\n                # Some identity servers may only support application/x-www-form-urlencoded\n                # types. This is especially true with old instances of Sydent, see\n                # https://github.com/matrix-org/sydent/pull/170\n                try:\n                    data = await self.blacklisting_http_client.post_urlencoded_get_json(\n                        url, invite_config\n                    )\n                except HttpResponseException as e:\n                    logger.warning(\n                        \"Error calling /store-invite on %s%s with fallback \"\n                        \"encoding: %s\",\n                        id_server_scheme,\n                        id_server,\n                        e,\n                    )\n                    raise e\n\n        # TODO: Check for success\n        token = data[\"token\"]\n        public_keys = data.get(\"public_keys\", [])\n        if \"public_key\" in data:\n            fallback_public_key = {\n                \"public_key\": data[\"public_key\"],\n                \"key_validity_url\": key_validity_url,\n            }\n        else:\n            fallback_public_key = public_keys[0]\n\n        if not public_keys:\n            public_keys.append(fallback_public_key)\n        display_name = data[\"display_name\"]\n        return token, public_keys, fallback_public_key, display_name\n\n\ndef create_id_access_token_header(id_access_token: str) -> List[str]:\n    \"\"\"Create an Authorization header for passing to SimpleHttpClient as the header value\n    of an HTTP request.\n\n    Args:\n        id_access_token: An identity server access token.\n\n    Returns:\n        The ascii-encoded bearer token encased in a list.\n    \"\"\"\n    # Prefix with Bearer\n    bearer_token = \"Bearer %s\" % id_access_token\n\n    # Encode headers to standard ascii\n    bearer_token.encode(\"ascii\")\n\n    # Return as a list as that's how SimpleHttpClient takes header values\n    return [bearer_token]\n\n\nclass LookupAlgorithm:\n    \"\"\"\n    Supported hashing algorithms when performing a 3PID lookup.\n\n    SHA256 - Hashing an (address, medium, pepper) combo with sha256, then url-safe base64\n        encoding\n    NONE - Not performing any hashing. Simply sending an (address, medium) combo in plaintext\n    \"\"\"\n\n    SHA256 = \"sha256\"\n    NONE = \"none\"\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2015, 2016 OpenMarket Ltd\n# Copyright 2017 Vector Creations Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for interacting with Identity Servers\"\"\"\n\nimport logging\nimport urllib.parse\nfrom typing import Awaitable, Callable, Dict, List, Optional, Tuple\n\nfrom synapse.api.errors import (\n    CodeMessageException,\n    Codes,\n    HttpResponseException,\n    SynapseError,\n)\nfrom synapse.config.emailconfig import ThreepidBehaviour\nfrom synapse.http import RequestTimedOutError\nfrom synapse.http.client import SimpleHttpClient\nfrom synapse.types import JsonDict, Requester\nfrom synapse.util import json_decoder\nfrom synapse.util.hash import sha256_and_url_safe_base64\nfrom synapse.util.stringutils import assert_valid_client_secret, random_string\n\nfrom ._base import BaseHandler\n\nlogger = logging.getLogger(__name__)\n\nid_server_scheme = \"https://\"\n\n\nclass IdentityHandler(BaseHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        self.http_client = SimpleHttpClient(hs)\n        # We create a blacklisting instance of SimpleHttpClient for contacting identity\n        # servers specified by clients\n        self.blacklisting_http_client = SimpleHttpClient(\n            hs, ip_blacklist=hs.config.federation_ip_range_blacklist\n        )\n        self.federation_http_client = hs.get_http_client()\n        self.hs = hs\n\n    async def threepid_from_creds(\n        self, id_server: str, creds: Dict[str, str]\n    ) -> Optional[JsonDict]:\n        \"\"\"\n        Retrieve and validate a threepid identifier from a \"credentials\" dictionary against a\n        given identity server\n\n        Args:\n            id_server: The identity server to validate 3PIDs against. Must be a\n                complete URL including the protocol (http(s)://)\n            creds: Dictionary containing the following keys:\n                * client_secret|clientSecret: A unique secret str provided by the client\n                * sid: The ID of the validation session\n\n        Returns:\n            A dictionary consisting of response params to the /getValidated3pid\n            endpoint of the Identity Service API, or None if the threepid was not found\n        \"\"\"\n        client_secret = creds.get(\"client_secret\") or creds.get(\"clientSecret\")\n        if not client_secret:\n            raise SynapseError(\n                400, \"Missing param client_secret in creds\", errcode=Codes.MISSING_PARAM\n            )\n        assert_valid_client_secret(client_secret)\n\n        session_id = creds.get(\"sid\")\n        if not session_id:\n            raise SynapseError(\n                400, \"Missing param session_id in creds\", errcode=Codes.MISSING_PARAM\n            )\n\n        query_params = {\"sid\": session_id, \"client_secret\": client_secret}\n\n        url = id_server + \"/_matrix/identity/api/v1/3pid/getValidated3pid\"\n\n        try:\n            data = await self.http_client.get_json(url, query_params)\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except HttpResponseException as e:\n            logger.info(\n                \"%s returned %i for threepid validation for: %s\",\n                id_server,\n                e.code,\n                creds,\n            )\n            return None\n\n        # Old versions of Sydent return a 200 http code even on a failed validation\n        # check. Thus, in addition to the HttpResponseException check above (which\n        # checks for non-200 errors), we need to make sure validation_session isn't\n        # actually an error, identified by the absence of a \"medium\" key\n        # See https://github.com/matrix-org/sydent/issues/215 for details\n        if \"medium\" in data:\n            return data\n\n        logger.info(\"%s reported non-validated threepid: %s\", id_server, creds)\n        return None\n\n    async def bind_threepid(\n        self,\n        client_secret: str,\n        sid: str,\n        mxid: str,\n        id_server: str,\n        id_access_token: Optional[str] = None,\n        use_v2: bool = True,\n    ) -> JsonDict:\n        \"\"\"Bind a 3PID to an identity server\n\n        Args:\n            client_secret: A unique secret provided by the client\n            sid: The ID of the validation session\n            mxid: The MXID to bind the 3PID to\n            id_server: The domain of the identity server to query\n            id_access_token: The access token to authenticate to the identity\n                server with, if necessary. Required if use_v2 is true\n            use_v2: Whether to use v2 Identity Service API endpoints. Defaults to True\n\n        Returns:\n            The response from the identity server\n        \"\"\"\n        logger.debug(\"Proxying threepid bind request for %s to %s\", mxid, id_server)\n\n        # If an id_access_token is not supplied, force usage of v1\n        if id_access_token is None:\n            use_v2 = False\n\n        # Decide which API endpoint URLs to use\n        headers = {}\n        bind_data = {\"sid\": sid, \"client_secret\": client_secret, \"mxid\": mxid}\n        if use_v2:\n            bind_url = \"https://%s/_matrix/identity/v2/3pid/bind\" % (id_server,)\n            headers[\"Authorization\"] = create_id_access_token_header(id_access_token)  # type: ignore\n        else:\n            bind_url = \"https://%s/_matrix/identity/api/v1/3pid/bind\" % (id_server,)\n\n        try:\n            # Use the blacklisting http client as this call is only to identity servers\n            # provided by a client\n            data = await self.blacklisting_http_client.post_json_get_json(\n                bind_url, bind_data, headers=headers\n            )\n\n            # Remember where we bound the threepid\n            await self.store.add_user_bound_threepid(\n                user_id=mxid,\n                medium=data[\"medium\"],\n                address=data[\"address\"],\n                id_server=id_server,\n            )\n\n            return data\n        except HttpResponseException as e:\n            if e.code != 404 or not use_v2:\n                logger.error(\"3PID bind failed with Matrix error: %r\", e)\n                raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except CodeMessageException as e:\n            data = json_decoder.decode(e.msg)  # XXX WAT?\n            return data\n\n        logger.info(\"Got 404 when POSTing JSON %s, falling back to v1 URL\", bind_url)\n        res = await self.bind_threepid(\n            client_secret, sid, mxid, id_server, id_access_token, use_v2=False\n        )\n        return res\n\n    async def try_unbind_threepid(self, mxid: str, threepid: dict) -> bool:\n        \"\"\"Attempt to remove a 3PID from an identity server, or if one is not provided, all\n        identity servers we're aware the binding is present on\n\n        Args:\n            mxid: Matrix user ID of binding to be removed\n            threepid: Dict with medium & address of binding to be\n                removed, and an optional id_server.\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            True on success, otherwise False if the identity\n            server doesn't support unbinding (or no identity server found to\n            contact).\n        \"\"\"\n        if threepid.get(\"id_server\"):\n            id_servers = [threepid[\"id_server\"]]\n        else:\n            id_servers = await self.store.get_id_servers_user_bound(\n                user_id=mxid, medium=threepid[\"medium\"], address=threepid[\"address\"]\n            )\n\n        # We don't know where to unbind, so we don't have a choice but to return\n        if not id_servers:\n            return False\n\n        changed = True\n        for id_server in id_servers:\n            changed &= await self.try_unbind_threepid_with_id_server(\n                mxid, threepid, id_server\n            )\n\n        return changed\n\n    async def try_unbind_threepid_with_id_server(\n        self, mxid: str, threepid: dict, id_server: str\n    ) -> bool:\n        \"\"\"Removes a binding from an identity server\n\n        Args:\n            mxid: Matrix user ID of binding to be removed\n            threepid: Dict with medium & address of binding to be removed\n            id_server: Identity server to unbind from\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            True on success, otherwise False if the identity\n            server doesn't support unbinding\n        \"\"\"\n        url = \"https://%s/_matrix/identity/api/v1/3pid/unbind\" % (id_server,)\n        url_bytes = \"/_matrix/identity/api/v1/3pid/unbind\".encode(\"ascii\")\n\n        content = {\n            \"mxid\": mxid,\n            \"threepid\": {\"medium\": threepid[\"medium\"], \"address\": threepid[\"address\"]},\n        }\n\n        # we abuse the federation http client to sign the request, but we have to send it\n        # using the normal http client since we don't want the SRV lookup and want normal\n        # 'browser-like' HTTPS.\n        auth_headers = self.federation_http_client.build_auth_headers(\n            destination=None,\n            method=b\"POST\",\n            url_bytes=url_bytes,\n            content=content,\n            destination_is=id_server.encode(\"ascii\"),\n        )\n        headers = {b\"Authorization\": auth_headers}\n\n        try:\n            # Use the blacklisting http client as this call is only to identity servers\n            # provided by a client\n            await self.blacklisting_http_client.post_json_get_json(\n                url, content, headers\n            )\n            changed = True\n        except HttpResponseException as e:\n            changed = False\n            if e.code in (400, 404, 501):\n                # The remote server probably doesn't support unbinding (yet)\n                logger.warning(\"Received %d response while unbinding threepid\", e.code)\n            else:\n                logger.error(\"Failed to unbind threepid on identity server: %s\", e)\n                raise SynapseError(500, \"Failed to contact identity server\")\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        await self.store.remove_user_bound_threepid(\n            user_id=mxid,\n            medium=threepid[\"medium\"],\n            address=threepid[\"address\"],\n            id_server=id_server,\n        )\n\n        return changed\n\n    async def send_threepid_validation(\n        self,\n        email_address: str,\n        client_secret: str,\n        send_attempt: int,\n        send_email_func: Callable[[str, str, str, str], Awaitable],\n        next_link: Optional[str] = None,\n    ) -> str:\n        \"\"\"Send a threepid validation email for password reset or\n        registration purposes\n\n        Args:\n            email_address: The user's email address\n            client_secret: The provided client secret\n            send_attempt: Which send attempt this is\n            send_email_func: A function that takes an email address, token,\n                             client_secret and session_id, sends an email\n                             and returns an Awaitable.\n            next_link: The URL to redirect the user to after validation\n\n        Returns:\n            The new session_id upon success\n\n        Raises:\n            SynapseError is an error occurred when sending the email\n        \"\"\"\n        # Check that this email/client_secret/send_attempt combo is new or\n        # greater than what we've seen previously\n        session = await self.store.get_threepid_validation_session(\n            \"email\", client_secret, address=email_address, validated=False\n        )\n\n        # Check to see if a session already exists and that it is not yet\n        # marked as validated\n        if session and session.get(\"validated_at\") is None:\n            session_id = session[\"session_id\"]\n            last_send_attempt = session[\"last_send_attempt\"]\n\n            # Check that the send_attempt is higher than previous attempts\n            if send_attempt <= last_send_attempt:\n                # If not, just return a success without sending an email\n                return session_id\n        else:\n            # An non-validated session does not exist yet.\n            # Generate a session id\n            session_id = random_string(16)\n\n        if next_link:\n            # Manipulate the next_link to add the sid, because the caller won't get\n            # it until we send a response, by which time we've sent the mail.\n            if \"?\" in next_link:\n                next_link += \"&\"\n            else:\n                next_link += \"?\"\n            next_link += \"sid=\" + urllib.parse.quote(session_id)\n\n        # Generate a new validation token\n        token = random_string(32)\n\n        # Send the mail with the link containing the token, client_secret\n        # and session_id\n        try:\n            await send_email_func(email_address, token, client_secret, session_id)\n        except Exception:\n            logger.exception(\n                \"Error sending threepid validation email to %s\", email_address\n            )\n            raise SynapseError(500, \"An error was encountered when sending the email\")\n\n        token_expires = (\n            self.hs.get_clock().time_msec()\n            + self.hs.config.email_validation_token_lifetime\n        )\n\n        await self.store.start_or_continue_validation_session(\n            \"email\",\n            email_address,\n            session_id,\n            client_secret,\n            send_attempt,\n            next_link,\n            token,\n            token_expires,\n        )\n\n        return session_id\n\n    async def requestEmailToken(\n        self,\n        id_server: str,\n        email: str,\n        client_secret: str,\n        send_attempt: int,\n        next_link: Optional[str] = None,\n    ) -> JsonDict:\n        \"\"\"\n        Request an external server send an email on our behalf for the purposes of threepid\n        validation.\n\n        Args:\n            id_server: The identity server to proxy to\n            email: The email to send the message to\n            client_secret: The unique client_secret sends by the user\n            send_attempt: Which attempt this is\n            next_link: A link to redirect the user to once they submit the token\n\n        Returns:\n            The json response body from the server\n        \"\"\"\n        params = {\n            \"email\": email,\n            \"client_secret\": client_secret,\n            \"send_attempt\": send_attempt,\n        }\n        if next_link:\n            params[\"next_link\"] = next_link\n\n        if self.hs.config.using_identity_server_from_trusted_list:\n            # Warn that a deprecated config option is in use\n            logger.warning(\n                'The config option \"trust_identity_server_for_password_resets\" '\n                'has been replaced by \"account_threepid_delegate\". '\n                \"Please consult the sample config at docs/sample_config.yaml for \"\n                \"details and update your config file.\"\n            )\n\n        try:\n            data = await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/email/requestToken\",\n                params,\n            )\n            return data\n        except HttpResponseException as e:\n            logger.info(\"Proxied requestToken failed: %r\", e)\n            raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n    async def requestMsisdnToken(\n        self,\n        id_server: str,\n        country: str,\n        phone_number: str,\n        client_secret: str,\n        send_attempt: int,\n        next_link: Optional[str] = None,\n    ) -> JsonDict:\n        \"\"\"\n        Request an external server send an SMS message on our behalf for the purposes of\n        threepid validation.\n        Args:\n            id_server: The identity server to proxy to\n            country: The country code of the phone number\n            phone_number: The number to send the message to\n            client_secret: The unique client_secret sends by the user\n            send_attempt: Which attempt this is\n            next_link: A link to redirect the user to once they submit the token\n\n        Returns:\n            The json response body from the server\n        \"\"\"\n        params = {\n            \"country\": country,\n            \"phone_number\": phone_number,\n            \"client_secret\": client_secret,\n            \"send_attempt\": send_attempt,\n        }\n        if next_link:\n            params[\"next_link\"] = next_link\n\n        if self.hs.config.using_identity_server_from_trusted_list:\n            # Warn that a deprecated config option is in use\n            logger.warning(\n                'The config option \"trust_identity_server_for_password_resets\" '\n                'has been replaced by \"account_threepid_delegate\". '\n                \"Please consult the sample config at docs/sample_config.yaml for \"\n                \"details and update your config file.\"\n            )\n\n        try:\n            data = await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/msisdn/requestToken\",\n                params,\n            )\n        except HttpResponseException as e:\n            logger.info(\"Proxied requestToken failed: %r\", e)\n            raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        assert self.hs.config.public_baseurl\n\n        # we need to tell the client to send the token back to us, since it doesn't\n        # otherwise know where to send it, so add submit_url response parameter\n        # (see also MSC2078)\n        data[\"submit_url\"] = (\n            self.hs.config.public_baseurl\n            + \"_matrix/client/unstable/add_threepid/msisdn/submit_token\"\n        )\n        return data\n\n    async def validate_threepid_session(\n        self, client_secret: str, sid: str\n    ) -> Optional[JsonDict]:\n        \"\"\"Validates a threepid session with only the client secret and session ID\n        Tries validating against any configured account_threepid_delegates as well as locally.\n\n        Args:\n            client_secret: A secret provided by the client\n            sid: The ID of the session\n\n        Returns:\n            The json response if validation was successful, otherwise None\n        \"\"\"\n        # XXX: We shouldn't need to keep wrapping and unwrapping this value\n        threepid_creds = {\"client_secret\": client_secret, \"sid\": sid}\n\n        # We don't actually know which medium this 3PID is. Thus we first assume it's email,\n        # and if validation fails we try msisdn\n        validation_session = None\n\n        # Try to validate as email\n        if self.hs.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:\n            # Ask our delegated email identity server\n            validation_session = await self.threepid_from_creds(\n                self.hs.config.account_threepid_delegate_email, threepid_creds\n            )\n        elif self.hs.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:\n            # Get a validated session matching these details\n            validation_session = await self.store.get_threepid_validation_session(\n                \"email\", client_secret, sid=sid, validated=True\n            )\n\n        if validation_session:\n            return validation_session\n\n        # Try to validate as msisdn\n        if self.hs.config.account_threepid_delegate_msisdn:\n            # Ask our delegated msisdn identity server\n            validation_session = await self.threepid_from_creds(\n                self.hs.config.account_threepid_delegate_msisdn, threepid_creds\n            )\n\n        return validation_session\n\n    async def proxy_msisdn_submit_token(\n        self, id_server: str, client_secret: str, sid: str, token: str\n    ) -> JsonDict:\n        \"\"\"Proxy a POST submitToken request to an identity server for verification purposes\n\n        Args:\n            id_server: The identity server URL to contact\n            client_secret: Secret provided by the client\n            sid: The ID of the session\n            token: The verification token\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            The response dict from the identity server\n        \"\"\"\n        body = {\"client_secret\": client_secret, \"sid\": sid, \"token\": token}\n\n        try:\n            return await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/msisdn/submitToken\",\n                body,\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except HttpResponseException as e:\n            logger.warning(\"Error contacting msisdn account_threepid_delegate: %s\", e)\n            raise SynapseError(400, \"Error contacting the identity server\")\n\n    async def lookup_3pid(\n        self,\n        id_server: str,\n        medium: str,\n        address: str,\n        id_access_token: Optional[str] = None,\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n            id_access_token: The access token to authenticate to the identity\n                server with\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognized.\n        \"\"\"\n        if id_access_token is not None:\n            try:\n                results = await self._lookup_3pid_v2(\n                    id_server, id_access_token, medium, address\n                )\n                return results\n\n            except Exception as e:\n                # Catch HttpResponseExcept for a non-200 response code\n                # Check if this identity server does not know about v2 lookups\n                if isinstance(e, HttpResponseException) and e.code == 404:\n                    # This is an old identity server that does not yet support v2 lookups\n                    logger.warning(\n                        \"Attempted v2 lookup on v1 identity server %s. Falling \"\n                        \"back to v1\",\n                        id_server,\n                    )\n                else:\n                    logger.warning(\"Error when looking up hashing details: %s\", e)\n                    return None\n\n        return await self._lookup_3pid_v1(id_server, medium, address)\n\n    async def _lookup_3pid_v1(\n        self, id_server: str, medium: str, address: str\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server using v1 lookup.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognized.\n        \"\"\"\n        try:\n            data = await self.blacklisting_http_client.get_json(\n                \"%s%s/_matrix/identity/api/v1/lookup\" % (id_server_scheme, id_server),\n                {\"medium\": medium, \"address\": address},\n            )\n\n            if \"mxid\" in data:\n                # note: we used to verify the identity server's signature here, but no longer\n                # require or validate it. See the following for context:\n                # https://github.com/matrix-org/synapse/issues/5253#issuecomment-666246950\n                return data[\"mxid\"]\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except IOError as e:\n            logger.warning(\"Error from v1 identity server lookup: %s\" % (e,))\n\n        return None\n\n    async def _lookup_3pid_v2(\n        self, id_server: str, id_access_token: str, medium: str, address: str\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server using v2 lookup.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            id_access_token: The access token to authenticate to the identity server with\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognised.\n        \"\"\"\n        # Check what hashing details are supported by this identity server\n        try:\n            hash_details = await self.blacklisting_http_client.get_json(\n                \"%s%s/_matrix/identity/v2/hash_details\" % (id_server_scheme, id_server),\n                {\"access_token\": id_access_token},\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        if not isinstance(hash_details, dict):\n            logger.warning(\n                \"Got non-dict object when checking hash details of %s%s: %s\",\n                id_server_scheme,\n                id_server,\n                hash_details,\n            )\n            raise SynapseError(\n                400,\n                \"Non-dict object from %s%s during v2 hash_details request: %s\"\n                % (id_server_scheme, id_server, hash_details),\n            )\n\n        # Extract information from hash_details\n        supported_lookup_algorithms = hash_details.get(\"algorithms\")\n        lookup_pepper = hash_details.get(\"lookup_pepper\")\n        if (\n            not supported_lookup_algorithms\n            or not isinstance(supported_lookup_algorithms, list)\n            or not lookup_pepper\n            or not isinstance(lookup_pepper, str)\n        ):\n            raise SynapseError(\n                400,\n                \"Invalid hash details received from identity server %s%s: %s\"\n                % (id_server_scheme, id_server, hash_details),\n            )\n\n        # Check if any of the supported lookup algorithms are present\n        if LookupAlgorithm.SHA256 in supported_lookup_algorithms:\n            # Perform a hashed lookup\n            lookup_algorithm = LookupAlgorithm.SHA256\n\n            # Hash address, medium and the pepper with sha256\n            to_hash = \"%s %s %s\" % (address, medium, lookup_pepper)\n            lookup_value = sha256_and_url_safe_base64(to_hash)\n\n        elif LookupAlgorithm.NONE in supported_lookup_algorithms:\n            # Perform a non-hashed lookup\n            lookup_algorithm = LookupAlgorithm.NONE\n\n            # Combine together plaintext address and medium\n            lookup_value = \"%s %s\" % (address, medium)\n\n        else:\n            logger.warning(\n                \"None of the provided lookup algorithms of %s are supported: %s\",\n                id_server,\n                supported_lookup_algorithms,\n            )\n            raise SynapseError(\n                400,\n                \"Provided identity server does not support any v2 lookup \"\n                \"algorithms that this homeserver supports.\",\n            )\n\n        # Authenticate with identity server given the access token from the client\n        headers = {\"Authorization\": create_id_access_token_header(id_access_token)}\n\n        try:\n            lookup_results = await self.blacklisting_http_client.post_json_get_json(\n                \"%s%s/_matrix/identity/v2/lookup\" % (id_server_scheme, id_server),\n                {\n                    \"addresses\": [lookup_value],\n                    \"algorithm\": lookup_algorithm,\n                    \"pepper\": lookup_pepper,\n                },\n                headers=headers,\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except Exception as e:\n            logger.warning(\"Error when performing a v2 3pid lookup: %s\", e)\n            raise SynapseError(\n                500, \"Unknown error occurred during identity server lookup\"\n            )\n\n        # Check for a mapping from what we looked up to an MXID\n        if \"mappings\" not in lookup_results or not isinstance(\n            lookup_results[\"mappings\"], dict\n        ):\n            logger.warning(\"No results from 3pid lookup\")\n            return None\n\n        # Return the MXID if it's available, or None otherwise\n        mxid = lookup_results[\"mappings\"].get(lookup_value)\n        return mxid\n\n    async def ask_id_server_for_third_party_invite(\n        self,\n        requester: Requester,\n        id_server: str,\n        medium: str,\n        address: str,\n        room_id: str,\n        inviter_user_id: str,\n        room_alias: str,\n        room_avatar_url: str,\n        room_join_rules: str,\n        room_name: str,\n        inviter_display_name: str,\n        inviter_avatar_url: str,\n        id_access_token: Optional[str] = None,\n    ) -> Tuple[str, List[Dict[str, str]], Dict[str, str], str]:\n        \"\"\"\n        Asks an identity server for a third party invite.\n\n        Args:\n            requester\n            id_server: hostname + optional port for the identity server.\n            medium: The literal string \"email\".\n            address: The third party address being invited.\n            room_id: The ID of the room to which the user is invited.\n            inviter_user_id: The user ID of the inviter.\n            room_alias: An alias for the room, for cosmetic notifications.\n            room_avatar_url: The URL of the room's avatar, for cosmetic\n                notifications.\n            room_join_rules: The join rules of the email (e.g. \"public\").\n            room_name: The m.room.name of the room.\n            inviter_display_name: The current display name of the\n                inviter.\n            inviter_avatar_url: The URL of the inviter's avatar.\n            id_access_token (str|None): The access token to authenticate to the identity\n                server with\n\n        Returns:\n            A tuple containing:\n                token: The token which must be signed to prove authenticity.\n                public_keys ([{\"public_key\": str, \"key_validity_url\": str}]):\n                    public_key is a base64-encoded ed25519 public key.\n                fallback_public_key: One element from public_keys.\n                display_name: A user-friendly name to represent the invited user.\n        \"\"\"\n        invite_config = {\n            \"medium\": medium,\n            \"address\": address,\n            \"room_id\": room_id,\n            \"room_alias\": room_alias,\n            \"room_avatar_url\": room_avatar_url,\n            \"room_join_rules\": room_join_rules,\n            \"room_name\": room_name,\n            \"sender\": inviter_user_id,\n            \"sender_display_name\": inviter_display_name,\n            \"sender_avatar_url\": inviter_avatar_url,\n        }\n\n        # Add the identity service access token to the JSON body and use the v2\n        # Identity Service endpoints if id_access_token is present\n        data = None\n        base_url = \"%s%s/_matrix/identity\" % (id_server_scheme, id_server)\n\n        if id_access_token:\n            key_validity_url = \"%s%s/_matrix/identity/v2/pubkey/isvalid\" % (\n                id_server_scheme,\n                id_server,\n            )\n\n            # Attempt a v2 lookup\n            url = base_url + \"/v2/store-invite\"\n            try:\n                data = await self.blacklisting_http_client.post_json_get_json(\n                    url,\n                    invite_config,\n                    {\"Authorization\": create_id_access_token_header(id_access_token)},\n                )\n            except RequestTimedOutError:\n                raise SynapseError(500, \"Timed out contacting identity server\")\n            except HttpResponseException as e:\n                if e.code != 404:\n                    logger.info(\"Failed to POST %s with JSON: %s\", url, e)\n                    raise e\n\n        if data is None:\n            key_validity_url = \"%s%s/_matrix/identity/api/v1/pubkey/isvalid\" % (\n                id_server_scheme,\n                id_server,\n            )\n            url = base_url + \"/api/v1/store-invite\"\n\n            try:\n                data = await self.blacklisting_http_client.post_json_get_json(\n                    url, invite_config\n                )\n            except RequestTimedOutError:\n                raise SynapseError(500, \"Timed out contacting identity server\")\n            except HttpResponseException as e:\n                logger.warning(\n                    \"Error trying to call /store-invite on %s%s: %s\",\n                    id_server_scheme,\n                    id_server,\n                    e,\n                )\n\n            if data is None:\n                # Some identity servers may only support application/x-www-form-urlencoded\n                # types. This is especially true with old instances of Sydent, see\n                # https://github.com/matrix-org/sydent/pull/170\n                try:\n                    data = await self.blacklisting_http_client.post_urlencoded_get_json(\n                        url, invite_config\n                    )\n                except HttpResponseException as e:\n                    logger.warning(\n                        \"Error calling /store-invite on %s%s with fallback \"\n                        \"encoding: %s\",\n                        id_server_scheme,\n                        id_server,\n                        e,\n                    )\n                    raise e\n\n        # TODO: Check for success\n        token = data[\"token\"]\n        public_keys = data.get(\"public_keys\", [])\n        if \"public_key\" in data:\n            fallback_public_key = {\n                \"public_key\": data[\"public_key\"],\n                \"key_validity_url\": key_validity_url,\n            }\n        else:\n            fallback_public_key = public_keys[0]\n\n        if not public_keys:\n            public_keys.append(fallback_public_key)\n        display_name = data[\"display_name\"]\n        return token, public_keys, fallback_public_key, display_name\n\n\ndef create_id_access_token_header(id_access_token: str) -> List[str]:\n    \"\"\"Create an Authorization header for passing to SimpleHttpClient as the header value\n    of an HTTP request.\n\n    Args:\n        id_access_token: An identity server access token.\n\n    Returns:\n        The ascii-encoded bearer token encased in a list.\n    \"\"\"\n    # Prefix with Bearer\n    bearer_token = \"Bearer %s\" % id_access_token\n\n    # Encode headers to standard ascii\n    bearer_token.encode(\"ascii\")\n\n    # Return as a list as that's how SimpleHttpClient takes header values\n    return [bearer_token]\n\n\nclass LookupAlgorithm:\n    \"\"\"\n    Supported hashing algorithms when performing a 3PID lookup.\n\n    SHA256 - Hashing an (address, medium, pepper) combo with sha256, then url-safe base64\n        encoding\n    NONE - Not performing any hashing. Simply sending an (address, medium) combo in plaintext\n    \"\"\"\n\n    SHA256 = \"sha256\"\n    NONE = \"none\"\n", "patch": "@@ -46,13 +46,13 @@ class IdentityHandler(BaseHandler):\n     def __init__(self, hs):\n         super().__init__(hs)\n \n+        # An HTTP client for contacting trusted URLs.\n         self.http_client = SimpleHttpClient(hs)\n-        # We create a blacklisting instance of SimpleHttpClient for contacting identity\n-        # servers specified by clients\n+        # An HTTP client for contacting identity servers specified by clients.\n         self.blacklisting_http_client = SimpleHttpClient(\n             hs, ip_blacklist=hs.config.federation_ip_range_blacklist\n         )\n-        self.federation_http_client = hs.get_http_client()\n+        self.federation_http_client = hs.get_federation_http_client()\n         self.hs = hs\n \n     async def threepid_from_creds(", "file_path": "files/2021_2/23", "file_language": "py", "file_name": "synapse/handlers/identity.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class IdentityHandler(BaseHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        self.http_client = SimpleHttpClient(hs)\n        # We create a blacklisting instance of SimpleHttpClient for contacting identity\n        # servers specified by clients\n        self.blacklisting_http_client = SimpleHttpClient(\n            hs, ip_blacklist=hs.config.federation_ip_range_blacklist\n        )\n        self.federation_http_client = hs.get_http_client()\n        self.hs = hs\n\n    async def threepid_from_creds(\n        self, id_server: str, creds: Dict[str, str]\n    ) -> Optional[JsonDict]:\n        \"\"\"\n        Retrieve and validate a threepid identifier from a \"credentials\" dictionary against a\n        given identity server\n\n        Args:\n            id_server: The identity server to validate 3PIDs against. Must be a\n                complete URL including the protocol (http(s)://)\n            creds: Dictionary containing the following keys:\n                * client_secret|clientSecret: A unique secret str provided by the client\n                * sid: The ID of the validation session\n\n        Returns:\n            A dictionary consisting of response params to the /getValidated3pid\n            endpoint of the Identity Service API, or None if the threepid was not found\n        \"\"\"\n        client_secret = creds.get(\"client_secret\") or creds.get(\"clientSecret\")\n        if not client_secret:\n            raise SynapseError(\n                400, \"Missing param client_secret in creds\", errcode=Codes.MISSING_PARAM\n            )\n        assert_valid_client_secret(client_secret)\n\n        session_id = creds.get(\"sid\")\n        if not session_id:\n            raise SynapseError(\n                400, \"Missing param session_id in creds\", errcode=Codes.MISSING_PARAM\n            )\n\n        query_params = {\"sid\": session_id, \"client_secret\": client_secret}\n\n        url = id_server + \"/_matrix/identity/api/v1/3pid/getValidated3pid\"\n\n        try:\n            data = await self.http_client.get_json(url, query_params)\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except HttpResponseException as e:\n            logger.info(\n                \"%s returned %i for threepid validation for: %s\",\n                id_server,\n                e.code,\n                creds,\n            )\n            return None\n\n        # Old versions of Sydent return a 200 http code even on a failed validation\n        # check. Thus, in addition to the HttpResponseException check above (which\n        # checks for non-200 errors), we need to make sure validation_session isn't\n        # actually an error, identified by the absence of a \"medium\" key\n        # See https://github.com/matrix-org/sydent/issues/215 for details\n        if \"medium\" in data:\n            return data\n\n        logger.info(\"%s reported non-validated threepid: %s\", id_server, creds)\n        return None\n\n    async def bind_threepid(\n        self,\n        client_secret: str,\n        sid: str,\n        mxid: str,\n        id_server: str,\n        id_access_token: Optional[str] = None,\n        use_v2: bool = True,\n    ) -> JsonDict:\n        \"\"\"Bind a 3PID to an identity server\n\n        Args:\n            client_secret: A unique secret provided by the client\n            sid: The ID of the validation session\n            mxid: The MXID to bind the 3PID to\n            id_server: The domain of the identity server to query\n            id_access_token: The access token to authenticate to the identity\n                server with, if necessary. Required if use_v2 is true\n            use_v2: Whether to use v2 Identity Service API endpoints. Defaults to True\n\n        Returns:\n            The response from the identity server\n        \"\"\"\n        logger.debug(\"Proxying threepid bind request for %s to %s\", mxid, id_server)\n\n        # If an id_access_token is not supplied, force usage of v1\n        if id_access_token is None:\n            use_v2 = False\n\n        # Decide which API endpoint URLs to use\n        headers = {}\n        bind_data = {\"sid\": sid, \"client_secret\": client_secret, \"mxid\": mxid}\n        if use_v2:\n            bind_url = \"https://%s/_matrix/identity/v2/3pid/bind\" % (id_server,)\n            headers[\"Authorization\"] = create_id_access_token_header(id_access_token)  # type: ignore\n        else:\n            bind_url = \"https://%s/_matrix/identity/api/v1/3pid/bind\" % (id_server,)\n\n        try:\n            # Use the blacklisting http client as this call is only to identity servers\n            # provided by a client\n            data = await self.blacklisting_http_client.post_json_get_json(\n                bind_url, bind_data, headers=headers\n            )\n\n            # Remember where we bound the threepid\n            await self.store.add_user_bound_threepid(\n                user_id=mxid,\n                medium=data[\"medium\"],\n                address=data[\"address\"],\n                id_server=id_server,\n            )\n\n            return data\n        except HttpResponseException as e:\n            if e.code != 404 or not use_v2:\n                logger.error(\"3PID bind failed with Matrix error: %r\", e)\n                raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except CodeMessageException as e:\n            data = json_decoder.decode(e.msg)  # XXX WAT?\n            return data\n\n        logger.info(\"Got 404 when POSTing JSON %s, falling back to v1 URL\", bind_url)\n        res = await self.bind_threepid(\n            client_secret, sid, mxid, id_server, id_access_token, use_v2=False\n        )\n        return res\n\n    async def try_unbind_threepid(self, mxid: str, threepid: dict) -> bool:\n        \"\"\"Attempt to remove a 3PID from an identity server, or if one is not provided, all\n        identity servers we're aware the binding is present on\n\n        Args:\n            mxid: Matrix user ID of binding to be removed\n            threepid: Dict with medium & address of binding to be\n                removed, and an optional id_server.\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            True on success, otherwise False if the identity\n            server doesn't support unbinding (or no identity server found to\n            contact).\n        \"\"\"\n        if threepid.get(\"id_server\"):\n            id_servers = [threepid[\"id_server\"]]\n        else:\n            id_servers = await self.store.get_id_servers_user_bound(\n                user_id=mxid, medium=threepid[\"medium\"], address=threepid[\"address\"]\n            )\n\n        # We don't know where to unbind, so we don't have a choice but to return\n        if not id_servers:\n            return False\n\n        changed = True\n        for id_server in id_servers:\n            changed &= await self.try_unbind_threepid_with_id_server(\n                mxid, threepid, id_server\n            )\n\n        return changed\n\n    async def try_unbind_threepid_with_id_server(\n        self, mxid: str, threepid: dict, id_server: str\n    ) -> bool:\n        \"\"\"Removes a binding from an identity server\n\n        Args:\n            mxid: Matrix user ID of binding to be removed\n            threepid: Dict with medium & address of binding to be removed\n            id_server: Identity server to unbind from\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            True on success, otherwise False if the identity\n            server doesn't support unbinding\n        \"\"\"\n        url = \"https://%s/_matrix/identity/api/v1/3pid/unbind\" % (id_server,)\n        url_bytes = \"/_matrix/identity/api/v1/3pid/unbind\".encode(\"ascii\")\n\n        content = {\n            \"mxid\": mxid,\n            \"threepid\": {\"medium\": threepid[\"medium\"], \"address\": threepid[\"address\"]},\n        }\n\n        # we abuse the federation http client to sign the request, but we have to send it\n        # using the normal http client since we don't want the SRV lookup and want normal\n        # 'browser-like' HTTPS.\n        auth_headers = self.federation_http_client.build_auth_headers(\n            destination=None,\n            method=b\"POST\",\n            url_bytes=url_bytes,\n            content=content,\n            destination_is=id_server.encode(\"ascii\"),\n        )\n        headers = {b\"Authorization\": auth_headers}\n\n        try:\n            # Use the blacklisting http client as this call is only to identity servers\n            # provided by a client\n            await self.blacklisting_http_client.post_json_get_json(\n                url, content, headers\n            )\n            changed = True\n        except HttpResponseException as e:\n            changed = False\n            if e.code in (400, 404, 501):\n                # The remote server probably doesn't support unbinding (yet)\n                logger.warning(\"Received %d response while unbinding threepid\", e.code)\n            else:\n                logger.error(\"Failed to unbind threepid on identity server: %s\", e)\n                raise SynapseError(500, \"Failed to contact identity server\")\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        await self.store.remove_user_bound_threepid(\n            user_id=mxid,\n            medium=threepid[\"medium\"],\n            address=threepid[\"address\"],\n            id_server=id_server,\n        )\n\n        return changed\n\n    async def send_threepid_validation(\n        self,\n        email_address: str,\n        client_secret: str,\n        send_attempt: int,\n        send_email_func: Callable[[str, str, str, str], Awaitable],\n        next_link: Optional[str] = None,\n    ) -> str:\n        \"\"\"Send a threepid validation email for password reset or\n        registration purposes\n\n        Args:\n            email_address: The user's email address\n            client_secret: The provided client secret\n            send_attempt: Which send attempt this is\n            send_email_func: A function that takes an email address, token,\n                             client_secret and session_id, sends an email\n                             and returns an Awaitable.\n            next_link: The URL to redirect the user to after validation\n\n        Returns:\n            The new session_id upon success\n\n        Raises:\n            SynapseError is an error occurred when sending the email\n        \"\"\"\n        # Check that this email/client_secret/send_attempt combo is new or\n        # greater than what we've seen previously\n        session = await self.store.get_threepid_validation_session(\n            \"email\", client_secret, address=email_address, validated=False\n        )\n\n        # Check to see if a session already exists and that it is not yet\n        # marked as validated\n        if session and session.get(\"validated_at\") is None:\n            session_id = session[\"session_id\"]\n            last_send_attempt = session[\"last_send_attempt\"]\n\n            # Check that the send_attempt is higher than previous attempts\n            if send_attempt <= last_send_attempt:\n                # If not, just return a success without sending an email\n                return session_id\n        else:\n            # An non-validated session does not exist yet.\n            # Generate a session id\n            session_id = random_string(16)\n\n        if next_link:\n            # Manipulate the next_link to add the sid, because the caller won't get\n            # it until we send a response, by which time we've sent the mail.\n            if \"?\" in next_link:\n                next_link += \"&\"\n            else:\n                next_link += \"?\"\n            next_link += \"sid=\" + urllib.parse.quote(session_id)\n\n        # Generate a new validation token\n        token = random_string(32)\n\n        # Send the mail with the link containing the token, client_secret\n        # and session_id\n        try:\n            await send_email_func(email_address, token, client_secret, session_id)\n        except Exception:\n            logger.exception(\n                \"Error sending threepid validation email to %s\", email_address\n            )\n            raise SynapseError(500, \"An error was encountered when sending the email\")\n\n        token_expires = (\n            self.hs.get_clock().time_msec()\n            + self.hs.config.email_validation_token_lifetime\n        )\n\n        await self.store.start_or_continue_validation_session(\n            \"email\",\n            email_address,\n            session_id,\n            client_secret,\n            send_attempt,\n            next_link,\n            token,\n            token_expires,\n        )\n\n        return session_id\n\n    async def requestEmailToken(\n        self,\n        id_server: str,\n        email: str,\n        client_secret: str,\n        send_attempt: int,\n        next_link: Optional[str] = None,\n    ) -> JsonDict:\n        \"\"\"\n        Request an external server send an email on our behalf for the purposes of threepid\n        validation.\n\n        Args:\n            id_server: The identity server to proxy to\n            email: The email to send the message to\n            client_secret: The unique client_secret sends by the user\n            send_attempt: Which attempt this is\n            next_link: A link to redirect the user to once they submit the token\n\n        Returns:\n            The json response body from the server\n        \"\"\"\n        params = {\n            \"email\": email,\n            \"client_secret\": client_secret,\n            \"send_attempt\": send_attempt,\n        }\n        if next_link:\n            params[\"next_link\"] = next_link\n\n        if self.hs.config.using_identity_server_from_trusted_list:\n            # Warn that a deprecated config option is in use\n            logger.warning(\n                'The config option \"trust_identity_server_for_password_resets\" '\n                'has been replaced by \"account_threepid_delegate\". '\n                \"Please consult the sample config at docs/sample_config.yaml for \"\n                \"details and update your config file.\"\n            )\n\n        try:\n            data = await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/email/requestToken\",\n                params,\n            )\n            return data\n        except HttpResponseException as e:\n            logger.info(\"Proxied requestToken failed: %r\", e)\n            raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n    async def requestMsisdnToken(\n        self,\n        id_server: str,\n        country: str,\n        phone_number: str,\n        client_secret: str,\n        send_attempt: int,\n        next_link: Optional[str] = None,\n    ) -> JsonDict:\n        \"\"\"\n        Request an external server send an SMS message on our behalf for the purposes of\n        threepid validation.\n        Args:\n            id_server: The identity server to proxy to\n            country: The country code of the phone number\n            phone_number: The number to send the message to\n            client_secret: The unique client_secret sends by the user\n            send_attempt: Which attempt this is\n            next_link: A link to redirect the user to once they submit the token\n\n        Returns:\n            The json response body from the server\n        \"\"\"\n        params = {\n            \"country\": country,\n            \"phone_number\": phone_number,\n            \"client_secret\": client_secret,\n            \"send_attempt\": send_attempt,\n        }\n        if next_link:\n            params[\"next_link\"] = next_link\n\n        if self.hs.config.using_identity_server_from_trusted_list:\n            # Warn that a deprecated config option is in use\n            logger.warning(\n                'The config option \"trust_identity_server_for_password_resets\" '\n                'has been replaced by \"account_threepid_delegate\". '\n                \"Please consult the sample config at docs/sample_config.yaml for \"\n                \"details and update your config file.\"\n            )\n\n        try:\n            data = await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/msisdn/requestToken\",\n                params,\n            )\n        except HttpResponseException as e:\n            logger.info(\"Proxied requestToken failed: %r\", e)\n            raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        assert self.hs.config.public_baseurl\n\n        # we need to tell the client to send the token back to us, since it doesn't\n        # otherwise know where to send it, so add submit_url response parameter\n        # (see also MSC2078)\n        data[\"submit_url\"] = (\n            self.hs.config.public_baseurl\n            + \"_matrix/client/unstable/add_threepid/msisdn/submit_token\"\n        )\n        return data\n\n    async def validate_threepid_session(\n        self, client_secret: str, sid: str\n    ) -> Optional[JsonDict]:\n        \"\"\"Validates a threepid session with only the client secret and session ID\n        Tries validating against any configured account_threepid_delegates as well as locally.\n\n        Args:\n            client_secret: A secret provided by the client\n            sid: The ID of the session\n\n        Returns:\n            The json response if validation was successful, otherwise None\n        \"\"\"\n        # XXX: We shouldn't need to keep wrapping and unwrapping this value\n        threepid_creds = {\"client_secret\": client_secret, \"sid\": sid}\n\n        # We don't actually know which medium this 3PID is. Thus we first assume it's email,\n        # and if validation fails we try msisdn\n        validation_session = None\n\n        # Try to validate as email\n        if self.hs.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:\n            # Ask our delegated email identity server\n            validation_session = await self.threepid_from_creds(\n                self.hs.config.account_threepid_delegate_email, threepid_creds\n            )\n        elif self.hs.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:\n            # Get a validated session matching these details\n            validation_session = await self.store.get_threepid_validation_session(\n                \"email\", client_secret, sid=sid, validated=True\n            )\n\n        if validation_session:\n            return validation_session\n\n        # Try to validate as msisdn\n        if self.hs.config.account_threepid_delegate_msisdn:\n            # Ask our delegated msisdn identity server\n            validation_session = await self.threepid_from_creds(\n                self.hs.config.account_threepid_delegate_msisdn, threepid_creds\n            )\n\n        return validation_session\n\n    async def proxy_msisdn_submit_token(\n        self, id_server: str, client_secret: str, sid: str, token: str\n    ) -> JsonDict:\n        \"\"\"Proxy a POST submitToken request to an identity server for verification purposes\n\n        Args:\n            id_server: The identity server URL to contact\n            client_secret: Secret provided by the client\n            sid: The ID of the session\n            token: The verification token\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            The response dict from the identity server\n        \"\"\"\n        body = {\"client_secret\": client_secret, \"sid\": sid, \"token\": token}\n\n        try:\n            return await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/msisdn/submitToken\",\n                body,\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except HttpResponseException as e:\n            logger.warning(\"Error contacting msisdn account_threepid_delegate: %s\", e)\n            raise SynapseError(400, \"Error contacting the identity server\")\n\n    async def lookup_3pid(\n        self,\n        id_server: str,\n        medium: str,\n        address: str,\n        id_access_token: Optional[str] = None,\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n            id_access_token: The access token to authenticate to the identity\n                server with\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognized.\n        \"\"\"\n        if id_access_token is not None:\n            try:\n                results = await self._lookup_3pid_v2(\n                    id_server, id_access_token, medium, address\n                )\n                return results\n\n            except Exception as e:\n                # Catch HttpResponseExcept for a non-200 response code\n                # Check if this identity server does not know about v2 lookups\n                if isinstance(e, HttpResponseException) and e.code == 404:\n                    # This is an old identity server that does not yet support v2 lookups\n                    logger.warning(\n                        \"Attempted v2 lookup on v1 identity server %s. Falling \"\n                        \"back to v1\",\n                        id_server,\n                    )\n                else:\n                    logger.warning(\"Error when looking up hashing details: %s\", e)\n                    return None\n\n        return await self._lookup_3pid_v1(id_server, medium, address)\n\n    async def _lookup_3pid_v1(\n        self, id_server: str, medium: str, address: str\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server using v1 lookup.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognized.\n        \"\"\"\n        try:\n            data = await self.blacklisting_http_client.get_json(\n                \"%s%s/_matrix/identity/api/v1/lookup\" % (id_server_scheme, id_server),\n                {\"medium\": medium, \"address\": address},\n            )\n\n            if \"mxid\" in data:\n                # note: we used to verify the identity server's signature here, but no longer\n                # require or validate it. See the following for context:\n                # https://github.com/matrix-org/synapse/issues/5253#issuecomment-666246950\n                return data[\"mxid\"]\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except IOError as e:\n            logger.warning(\"Error from v1 identity server lookup: %s\" % (e,))\n\n        return None\n\n    async def _lookup_3pid_v2(\n        self, id_server: str, id_access_token: str, medium: str, address: str\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server using v2 lookup.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            id_access_token: The access token to authenticate to the identity server with\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognised.\n        \"\"\"\n        # Check what hashing details are supported by this identity server\n        try:\n            hash_details = await self.blacklisting_http_client.get_json(\n                \"%s%s/_matrix/identity/v2/hash_details\" % (id_server_scheme, id_server),\n                {\"access_token\": id_access_token},\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        if not isinstance(hash_details, dict):\n            logger.warning(\n                \"Got non-dict object when checking hash details of %s%s: %s\",\n                id_server_scheme,\n                id_server,\n                hash_details,\n            )\n            raise SynapseError(\n                400,\n                \"Non-dict object from %s%s during v2 hash_details request: %s\"\n                % (id_server_scheme, id_server, hash_details),\n            )\n\n        # Extract information from hash_details\n        supported_lookup_algorithms = hash_details.get(\"algorithms\")\n        lookup_pepper = hash_details.get(\"lookup_pepper\")\n        if (\n            not supported_lookup_algorithms\n            or not isinstance(supported_lookup_algorithms, list)\n            or not lookup_pepper\n            or not isinstance(lookup_pepper, str)\n        ):\n            raise SynapseError(\n                400,\n                \"Invalid hash details received from identity server %s%s: %s\"\n                % (id_server_scheme, id_server, hash_details),\n            )\n\n        # Check if any of the supported lookup algorithms are present\n        if LookupAlgorithm.SHA256 in supported_lookup_algorithms:\n            # Perform a hashed lookup\n            lookup_algorithm = LookupAlgorithm.SHA256\n\n            # Hash address, medium and the pepper with sha256\n            to_hash = \"%s %s %s\" % (address, medium, lookup_pepper)\n            lookup_value = sha256_and_url_safe_base64(to_hash)\n\n        elif LookupAlgorithm.NONE in supported_lookup_algorithms:\n            # Perform a non-hashed lookup\n            lookup_algorithm = LookupAlgorithm.NONE\n\n            # Combine together plaintext address and medium\n            lookup_value = \"%s %s\" % (address, medium)\n\n        else:\n            logger.warning(\n                \"None of the provided lookup algorithms of %s are supported: %s\",\n                id_server,\n                supported_lookup_algorithms,\n            )\n            raise SynapseError(\n                400,\n                \"Provided identity server does not support any v2 lookup \"\n                \"algorithms that this homeserver supports.\",\n            )\n\n        # Authenticate with identity server given the access token from the client\n        headers = {\"Authorization\": create_id_access_token_header(id_access_token)}\n\n        try:\n            lookup_results = await self.blacklisting_http_client.post_json_get_json(\n                \"%s%s/_matrix/identity/v2/lookup\" % (id_server_scheme, id_server),\n                {\n                    \"addresses\": [lookup_value],\n                    \"algorithm\": lookup_algorithm,\n                    \"pepper\": lookup_pepper,\n                },\n                headers=headers,\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except Exception as e:\n            logger.warning(\"Error when performing a v2 3pid lookup: %s\", e)\n            raise SynapseError(\n                500, \"Unknown error occurred during identity server lookup\"\n            )\n\n        # Check for a mapping from what we looked up to an MXID\n        if \"mappings\" not in lookup_results or not isinstance(\n            lookup_results[\"mappings\"], dict\n        ):\n            logger.warning(\"No results from 3pid lookup\")\n            return None\n\n        # Return the MXID if it's available, or None otherwise\n        mxid = lookup_results[\"mappings\"].get(lookup_value)\n        return mxid\n\n    async def ask_id_server_for_third_party_invite(\n        self,\n        requester: Requester,\n        id_server: str,\n        medium: str,\n        address: str,\n        room_id: str,\n        inviter_user_id: str,\n        room_alias: str,\n        room_avatar_url: str,\n        room_join_rules: str,\n        room_name: str,\n        inviter_display_name: str,\n        inviter_avatar_url: str,\n        id_access_token: Optional[str] = None,\n    ) -> Tuple[str, List[Dict[str, str]], Dict[str, str], str]:\n        \"\"\"\n        Asks an identity server for a third party invite.\n\n        Args:\n            requester\n            id_server: hostname + optional port for the identity server.\n            medium: The literal string \"email\".\n            address: The third party address being invited.\n            room_id: The ID of the room to which the user is invited.\n            inviter_user_id: The user ID of the inviter.\n            room_alias: An alias for the room, for cosmetic notifications.\n            room_avatar_url: The URL of the room's avatar, for cosmetic\n                notifications.\n            room_join_rules: The join rules of the email (e.g. \"public\").\n            room_name: The m.room.name of the room.\n            inviter_display_name: The current display name of the\n                inviter.\n            inviter_avatar_url: The URL of the inviter's avatar.\n            id_access_token (str|None): The access token to authenticate to the identity\n                server with\n\n        Returns:\n            A tuple containing:\n                token: The token which must be signed to prove authenticity.\n                public_keys ([{\"public_key\": str, \"key_validity_url\": str}]):\n                    public_key is a base64-encoded ed25519 public key.\n                fallback_public_key: One element from public_keys.\n                display_name: A user-friendly name to represent the invited user.\n        \"\"\"\n        invite_config = {\n            \"medium\": medium,\n            \"address\": address,\n            \"room_id\": room_id,\n            \"room_alias\": room_alias,\n            \"room_avatar_url\": room_avatar_url,\n            \"room_join_rules\": room_join_rules,\n            \"room_name\": room_name,\n            \"sender\": inviter_user_id,\n            \"sender_display_name\": inviter_display_name,\n            \"sender_avatar_url\": inviter_avatar_url,\n        }\n\n        # Add the identity service access token to the JSON body and use the v2\n        # Identity Service endpoints if id_access_token is present\n        data = None\n        base_url = \"%s%s/_matrix/identity\" % (id_server_scheme, id_server)\n\n        if id_access_token:\n            key_validity_url = \"%s%s/_matrix/identity/v2/pubkey/isvalid\" % (\n                id_server_scheme,\n                id_server,\n            )\n\n            # Attempt a v2 lookup\n            url = base_url + \"/v2/store-invite\"\n            try:\n                data = await self.blacklisting_http_client.post_json_get_json(\n                    url,\n                    invite_config,\n                    {\"Authorization\": create_id_access_token_header(id_access_token)},\n                )\n            except RequestTimedOutError:\n                raise SynapseError(500, \"Timed out contacting identity server\")\n            except HttpResponseException as e:\n                if e.code != 404:\n                    logger.info(\"Failed to POST %s with JSON: %s\", url, e)\n                    raise e\n\n        if data is None:\n            key_validity_url = \"%s%s/_matrix/identity/api/v1/pubkey/isvalid\" % (\n                id_server_scheme,\n                id_server,\n            )\n            url = base_url + \"/api/v1/store-invite\"\n\n            try:\n                data = await self.blacklisting_http_client.post_json_get_json(\n                    url, invite_config\n                )\n            except RequestTimedOutError:\n                raise SynapseError(500, \"Timed out contacting identity server\")\n            except HttpResponseException as e:\n                logger.warning(\n                    \"Error trying to call /store-invite on %s%s: %s\",\n                    id_server_scheme,\n                    id_server,\n                    e,\n                )\n\n            if data is None:\n                # Some identity servers may only support application/x-www-form-urlencoded\n                # types. This is especially true with old instances of Sydent, see\n                # https://github.com/matrix-org/sydent/pull/170\n                try:\n                    data = await self.blacklisting_http_client.post_urlencoded_get_json(\n                        url, invite_config\n                    )\n                except HttpResponseException as e:\n                    logger.warning(\n                        \"Error calling /store-invite on %s%s with fallback \"\n                        \"encoding: %s\",\n                        id_server_scheme,\n                        id_server,\n                        e,\n                    )\n                    raise e\n\n        # TODO: Check for success\n        token = data[\"token\"]\n        public_keys = data.get(\"public_keys\", [])\n        if \"public_key\" in data:\n            fallback_public_key = {\n                \"public_key\": data[\"public_key\"],\n                \"key_validity_url\": key_validity_url,\n            }\n        else:\n            fallback_public_key = public_keys[0]\n\n        if not public_keys:\n            public_keys.append(fallback_public_key)\n        display_name = data[\"display_name\"]\n        return token, public_keys, fallback_public_key, display_name", "target": 0}, {"function": "def create_id_access_token_header(id_access_token: str) -> List[str]:\n    \"\"\"Create an Authorization header for passing to SimpleHttpClient as the header value\n    of an HTTP request.\n\n    Args:\n        id_access_token: An identity server access token.\n\n    Returns:\n        The ascii-encoded bearer token encased in a list.\n    \"\"\"\n    # Prefix with Bearer\n    bearer_token = \"Bearer %s\" % id_access_token\n\n    # Encode headers to standard ascii\n    bearer_token.encode(\"ascii\")\n\n    # Return as a list as that's how SimpleHttpClient takes header values\n    return [bearer_token]", "target": 0}, {"function": "class LookupAlgorithm:\n    \"\"\"\n    Supported hashing algorithms when performing a 3PID lookup.\n\n    SHA256 - Hashing an (address, medium, pepper) combo with sha256, then url-safe base64\n        encoding\n    NONE - Not performing any hashing. Simply sending an (address, medium) combo in plaintext\n    \"\"\"\n\n    SHA256 = \"sha256\"\n    NONE = \"none\"", "target": 0}], "function_after": [{"function": "class IdentityHandler(BaseHandler):\n    def __init__(self, hs):\n        super().__init__(hs)\n\n        # An HTTP client for contacting trusted URLs.\n        self.http_client = SimpleHttpClient(hs)\n        # An HTTP client for contacting identity servers specified by clients.\n        self.blacklisting_http_client = SimpleHttpClient(\n            hs, ip_blacklist=hs.config.federation_ip_range_blacklist\n        )\n        self.federation_http_client = hs.get_federation_http_client()\n        self.hs = hs\n\n    async def threepid_from_creds(\n        self, id_server: str, creds: Dict[str, str]\n    ) -> Optional[JsonDict]:\n        \"\"\"\n        Retrieve and validate a threepid identifier from a \"credentials\" dictionary against a\n        given identity server\n\n        Args:\n            id_server: The identity server to validate 3PIDs against. Must be a\n                complete URL including the protocol (http(s)://)\n            creds: Dictionary containing the following keys:\n                * client_secret|clientSecret: A unique secret str provided by the client\n                * sid: The ID of the validation session\n\n        Returns:\n            A dictionary consisting of response params to the /getValidated3pid\n            endpoint of the Identity Service API, or None if the threepid was not found\n        \"\"\"\n        client_secret = creds.get(\"client_secret\") or creds.get(\"clientSecret\")\n        if not client_secret:\n            raise SynapseError(\n                400, \"Missing param client_secret in creds\", errcode=Codes.MISSING_PARAM\n            )\n        assert_valid_client_secret(client_secret)\n\n        session_id = creds.get(\"sid\")\n        if not session_id:\n            raise SynapseError(\n                400, \"Missing param session_id in creds\", errcode=Codes.MISSING_PARAM\n            )\n\n        query_params = {\"sid\": session_id, \"client_secret\": client_secret}\n\n        url = id_server + \"/_matrix/identity/api/v1/3pid/getValidated3pid\"\n\n        try:\n            data = await self.http_client.get_json(url, query_params)\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except HttpResponseException as e:\n            logger.info(\n                \"%s returned %i for threepid validation for: %s\",\n                id_server,\n                e.code,\n                creds,\n            )\n            return None\n\n        # Old versions of Sydent return a 200 http code even on a failed validation\n        # check. Thus, in addition to the HttpResponseException check above (which\n        # checks for non-200 errors), we need to make sure validation_session isn't\n        # actually an error, identified by the absence of a \"medium\" key\n        # See https://github.com/matrix-org/sydent/issues/215 for details\n        if \"medium\" in data:\n            return data\n\n        logger.info(\"%s reported non-validated threepid: %s\", id_server, creds)\n        return None\n\n    async def bind_threepid(\n        self,\n        client_secret: str,\n        sid: str,\n        mxid: str,\n        id_server: str,\n        id_access_token: Optional[str] = None,\n        use_v2: bool = True,\n    ) -> JsonDict:\n        \"\"\"Bind a 3PID to an identity server\n\n        Args:\n            client_secret: A unique secret provided by the client\n            sid: The ID of the validation session\n            mxid: The MXID to bind the 3PID to\n            id_server: The domain of the identity server to query\n            id_access_token: The access token to authenticate to the identity\n                server with, if necessary. Required if use_v2 is true\n            use_v2: Whether to use v2 Identity Service API endpoints. Defaults to True\n\n        Returns:\n            The response from the identity server\n        \"\"\"\n        logger.debug(\"Proxying threepid bind request for %s to %s\", mxid, id_server)\n\n        # If an id_access_token is not supplied, force usage of v1\n        if id_access_token is None:\n            use_v2 = False\n\n        # Decide which API endpoint URLs to use\n        headers = {}\n        bind_data = {\"sid\": sid, \"client_secret\": client_secret, \"mxid\": mxid}\n        if use_v2:\n            bind_url = \"https://%s/_matrix/identity/v2/3pid/bind\" % (id_server,)\n            headers[\"Authorization\"] = create_id_access_token_header(id_access_token)  # type: ignore\n        else:\n            bind_url = \"https://%s/_matrix/identity/api/v1/3pid/bind\" % (id_server,)\n\n        try:\n            # Use the blacklisting http client as this call is only to identity servers\n            # provided by a client\n            data = await self.blacklisting_http_client.post_json_get_json(\n                bind_url, bind_data, headers=headers\n            )\n\n            # Remember where we bound the threepid\n            await self.store.add_user_bound_threepid(\n                user_id=mxid,\n                medium=data[\"medium\"],\n                address=data[\"address\"],\n                id_server=id_server,\n            )\n\n            return data\n        except HttpResponseException as e:\n            if e.code != 404 or not use_v2:\n                logger.error(\"3PID bind failed with Matrix error: %r\", e)\n                raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except CodeMessageException as e:\n            data = json_decoder.decode(e.msg)  # XXX WAT?\n            return data\n\n        logger.info(\"Got 404 when POSTing JSON %s, falling back to v1 URL\", bind_url)\n        res = await self.bind_threepid(\n            client_secret, sid, mxid, id_server, id_access_token, use_v2=False\n        )\n        return res\n\n    async def try_unbind_threepid(self, mxid: str, threepid: dict) -> bool:\n        \"\"\"Attempt to remove a 3PID from an identity server, or if one is not provided, all\n        identity servers we're aware the binding is present on\n\n        Args:\n            mxid: Matrix user ID of binding to be removed\n            threepid: Dict with medium & address of binding to be\n                removed, and an optional id_server.\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            True on success, otherwise False if the identity\n            server doesn't support unbinding (or no identity server found to\n            contact).\n        \"\"\"\n        if threepid.get(\"id_server\"):\n            id_servers = [threepid[\"id_server\"]]\n        else:\n            id_servers = await self.store.get_id_servers_user_bound(\n                user_id=mxid, medium=threepid[\"medium\"], address=threepid[\"address\"]\n            )\n\n        # We don't know where to unbind, so we don't have a choice but to return\n        if not id_servers:\n            return False\n\n        changed = True\n        for id_server in id_servers:\n            changed &= await self.try_unbind_threepid_with_id_server(\n                mxid, threepid, id_server\n            )\n\n        return changed\n\n    async def try_unbind_threepid_with_id_server(\n        self, mxid: str, threepid: dict, id_server: str\n    ) -> bool:\n        \"\"\"Removes a binding from an identity server\n\n        Args:\n            mxid: Matrix user ID of binding to be removed\n            threepid: Dict with medium & address of binding to be removed\n            id_server: Identity server to unbind from\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            True on success, otherwise False if the identity\n            server doesn't support unbinding\n        \"\"\"\n        url = \"https://%s/_matrix/identity/api/v1/3pid/unbind\" % (id_server,)\n        url_bytes = \"/_matrix/identity/api/v1/3pid/unbind\".encode(\"ascii\")\n\n        content = {\n            \"mxid\": mxid,\n            \"threepid\": {\"medium\": threepid[\"medium\"], \"address\": threepid[\"address\"]},\n        }\n\n        # we abuse the federation http client to sign the request, but we have to send it\n        # using the normal http client since we don't want the SRV lookup and want normal\n        # 'browser-like' HTTPS.\n        auth_headers = self.federation_http_client.build_auth_headers(\n            destination=None,\n            method=b\"POST\",\n            url_bytes=url_bytes,\n            content=content,\n            destination_is=id_server.encode(\"ascii\"),\n        )\n        headers = {b\"Authorization\": auth_headers}\n\n        try:\n            # Use the blacklisting http client as this call is only to identity servers\n            # provided by a client\n            await self.blacklisting_http_client.post_json_get_json(\n                url, content, headers\n            )\n            changed = True\n        except HttpResponseException as e:\n            changed = False\n            if e.code in (400, 404, 501):\n                # The remote server probably doesn't support unbinding (yet)\n                logger.warning(\"Received %d response while unbinding threepid\", e.code)\n            else:\n                logger.error(\"Failed to unbind threepid on identity server: %s\", e)\n                raise SynapseError(500, \"Failed to contact identity server\")\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        await self.store.remove_user_bound_threepid(\n            user_id=mxid,\n            medium=threepid[\"medium\"],\n            address=threepid[\"address\"],\n            id_server=id_server,\n        )\n\n        return changed\n\n    async def send_threepid_validation(\n        self,\n        email_address: str,\n        client_secret: str,\n        send_attempt: int,\n        send_email_func: Callable[[str, str, str, str], Awaitable],\n        next_link: Optional[str] = None,\n    ) -> str:\n        \"\"\"Send a threepid validation email for password reset or\n        registration purposes\n\n        Args:\n            email_address: The user's email address\n            client_secret: The provided client secret\n            send_attempt: Which send attempt this is\n            send_email_func: A function that takes an email address, token,\n                             client_secret and session_id, sends an email\n                             and returns an Awaitable.\n            next_link: The URL to redirect the user to after validation\n\n        Returns:\n            The new session_id upon success\n\n        Raises:\n            SynapseError is an error occurred when sending the email\n        \"\"\"\n        # Check that this email/client_secret/send_attempt combo is new or\n        # greater than what we've seen previously\n        session = await self.store.get_threepid_validation_session(\n            \"email\", client_secret, address=email_address, validated=False\n        )\n\n        # Check to see if a session already exists and that it is not yet\n        # marked as validated\n        if session and session.get(\"validated_at\") is None:\n            session_id = session[\"session_id\"]\n            last_send_attempt = session[\"last_send_attempt\"]\n\n            # Check that the send_attempt is higher than previous attempts\n            if send_attempt <= last_send_attempt:\n                # If not, just return a success without sending an email\n                return session_id\n        else:\n            # An non-validated session does not exist yet.\n            # Generate a session id\n            session_id = random_string(16)\n\n        if next_link:\n            # Manipulate the next_link to add the sid, because the caller won't get\n            # it until we send a response, by which time we've sent the mail.\n            if \"?\" in next_link:\n                next_link += \"&\"\n            else:\n                next_link += \"?\"\n            next_link += \"sid=\" + urllib.parse.quote(session_id)\n\n        # Generate a new validation token\n        token = random_string(32)\n\n        # Send the mail with the link containing the token, client_secret\n        # and session_id\n        try:\n            await send_email_func(email_address, token, client_secret, session_id)\n        except Exception:\n            logger.exception(\n                \"Error sending threepid validation email to %s\", email_address\n            )\n            raise SynapseError(500, \"An error was encountered when sending the email\")\n\n        token_expires = (\n            self.hs.get_clock().time_msec()\n            + self.hs.config.email_validation_token_lifetime\n        )\n\n        await self.store.start_or_continue_validation_session(\n            \"email\",\n            email_address,\n            session_id,\n            client_secret,\n            send_attempt,\n            next_link,\n            token,\n            token_expires,\n        )\n\n        return session_id\n\n    async def requestEmailToken(\n        self,\n        id_server: str,\n        email: str,\n        client_secret: str,\n        send_attempt: int,\n        next_link: Optional[str] = None,\n    ) -> JsonDict:\n        \"\"\"\n        Request an external server send an email on our behalf for the purposes of threepid\n        validation.\n\n        Args:\n            id_server: The identity server to proxy to\n            email: The email to send the message to\n            client_secret: The unique client_secret sends by the user\n            send_attempt: Which attempt this is\n            next_link: A link to redirect the user to once they submit the token\n\n        Returns:\n            The json response body from the server\n        \"\"\"\n        params = {\n            \"email\": email,\n            \"client_secret\": client_secret,\n            \"send_attempt\": send_attempt,\n        }\n        if next_link:\n            params[\"next_link\"] = next_link\n\n        if self.hs.config.using_identity_server_from_trusted_list:\n            # Warn that a deprecated config option is in use\n            logger.warning(\n                'The config option \"trust_identity_server_for_password_resets\" '\n                'has been replaced by \"account_threepid_delegate\". '\n                \"Please consult the sample config at docs/sample_config.yaml for \"\n                \"details and update your config file.\"\n            )\n\n        try:\n            data = await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/email/requestToken\",\n                params,\n            )\n            return data\n        except HttpResponseException as e:\n            logger.info(\"Proxied requestToken failed: %r\", e)\n            raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n    async def requestMsisdnToken(\n        self,\n        id_server: str,\n        country: str,\n        phone_number: str,\n        client_secret: str,\n        send_attempt: int,\n        next_link: Optional[str] = None,\n    ) -> JsonDict:\n        \"\"\"\n        Request an external server send an SMS message on our behalf for the purposes of\n        threepid validation.\n        Args:\n            id_server: The identity server to proxy to\n            country: The country code of the phone number\n            phone_number: The number to send the message to\n            client_secret: The unique client_secret sends by the user\n            send_attempt: Which attempt this is\n            next_link: A link to redirect the user to once they submit the token\n\n        Returns:\n            The json response body from the server\n        \"\"\"\n        params = {\n            \"country\": country,\n            \"phone_number\": phone_number,\n            \"client_secret\": client_secret,\n            \"send_attempt\": send_attempt,\n        }\n        if next_link:\n            params[\"next_link\"] = next_link\n\n        if self.hs.config.using_identity_server_from_trusted_list:\n            # Warn that a deprecated config option is in use\n            logger.warning(\n                'The config option \"trust_identity_server_for_password_resets\" '\n                'has been replaced by \"account_threepid_delegate\". '\n                \"Please consult the sample config at docs/sample_config.yaml for \"\n                \"details and update your config file.\"\n            )\n\n        try:\n            data = await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/msisdn/requestToken\",\n                params,\n            )\n        except HttpResponseException as e:\n            logger.info(\"Proxied requestToken failed: %r\", e)\n            raise e.to_synapse_error()\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        assert self.hs.config.public_baseurl\n\n        # we need to tell the client to send the token back to us, since it doesn't\n        # otherwise know where to send it, so add submit_url response parameter\n        # (see also MSC2078)\n        data[\"submit_url\"] = (\n            self.hs.config.public_baseurl\n            + \"_matrix/client/unstable/add_threepid/msisdn/submit_token\"\n        )\n        return data\n\n    async def validate_threepid_session(\n        self, client_secret: str, sid: str\n    ) -> Optional[JsonDict]:\n        \"\"\"Validates a threepid session with only the client secret and session ID\n        Tries validating against any configured account_threepid_delegates as well as locally.\n\n        Args:\n            client_secret: A secret provided by the client\n            sid: The ID of the session\n\n        Returns:\n            The json response if validation was successful, otherwise None\n        \"\"\"\n        # XXX: We shouldn't need to keep wrapping and unwrapping this value\n        threepid_creds = {\"client_secret\": client_secret, \"sid\": sid}\n\n        # We don't actually know which medium this 3PID is. Thus we first assume it's email,\n        # and if validation fails we try msisdn\n        validation_session = None\n\n        # Try to validate as email\n        if self.hs.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:\n            # Ask our delegated email identity server\n            validation_session = await self.threepid_from_creds(\n                self.hs.config.account_threepid_delegate_email, threepid_creds\n            )\n        elif self.hs.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:\n            # Get a validated session matching these details\n            validation_session = await self.store.get_threepid_validation_session(\n                \"email\", client_secret, sid=sid, validated=True\n            )\n\n        if validation_session:\n            return validation_session\n\n        # Try to validate as msisdn\n        if self.hs.config.account_threepid_delegate_msisdn:\n            # Ask our delegated msisdn identity server\n            validation_session = await self.threepid_from_creds(\n                self.hs.config.account_threepid_delegate_msisdn, threepid_creds\n            )\n\n        return validation_session\n\n    async def proxy_msisdn_submit_token(\n        self, id_server: str, client_secret: str, sid: str, token: str\n    ) -> JsonDict:\n        \"\"\"Proxy a POST submitToken request to an identity server for verification purposes\n\n        Args:\n            id_server: The identity server URL to contact\n            client_secret: Secret provided by the client\n            sid: The ID of the session\n            token: The verification token\n\n        Raises:\n            SynapseError: If we failed to contact the identity server\n\n        Returns:\n            The response dict from the identity server\n        \"\"\"\n        body = {\"client_secret\": client_secret, \"sid\": sid, \"token\": token}\n\n        try:\n            return await self.http_client.post_json_get_json(\n                id_server + \"/_matrix/identity/api/v1/validate/msisdn/submitToken\",\n                body,\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except HttpResponseException as e:\n            logger.warning(\"Error contacting msisdn account_threepid_delegate: %s\", e)\n            raise SynapseError(400, \"Error contacting the identity server\")\n\n    async def lookup_3pid(\n        self,\n        id_server: str,\n        medium: str,\n        address: str,\n        id_access_token: Optional[str] = None,\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n            id_access_token: The access token to authenticate to the identity\n                server with\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognized.\n        \"\"\"\n        if id_access_token is not None:\n            try:\n                results = await self._lookup_3pid_v2(\n                    id_server, id_access_token, medium, address\n                )\n                return results\n\n            except Exception as e:\n                # Catch HttpResponseExcept for a non-200 response code\n                # Check if this identity server does not know about v2 lookups\n                if isinstance(e, HttpResponseException) and e.code == 404:\n                    # This is an old identity server that does not yet support v2 lookups\n                    logger.warning(\n                        \"Attempted v2 lookup on v1 identity server %s. Falling \"\n                        \"back to v1\",\n                        id_server,\n                    )\n                else:\n                    logger.warning(\"Error when looking up hashing details: %s\", e)\n                    return None\n\n        return await self._lookup_3pid_v1(id_server, medium, address)\n\n    async def _lookup_3pid_v1(\n        self, id_server: str, medium: str, address: str\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server using v1 lookup.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognized.\n        \"\"\"\n        try:\n            data = await self.blacklisting_http_client.get_json(\n                \"%s%s/_matrix/identity/api/v1/lookup\" % (id_server_scheme, id_server),\n                {\"medium\": medium, \"address\": address},\n            )\n\n            if \"mxid\" in data:\n                # note: we used to verify the identity server's signature here, but no longer\n                # require or validate it. See the following for context:\n                # https://github.com/matrix-org/synapse/issues/5253#issuecomment-666246950\n                return data[\"mxid\"]\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except IOError as e:\n            logger.warning(\"Error from v1 identity server lookup: %s\" % (e,))\n\n        return None\n\n    async def _lookup_3pid_v2(\n        self, id_server: str, id_access_token: str, medium: str, address: str\n    ) -> Optional[str]:\n        \"\"\"Looks up a 3pid in the passed identity server using v2 lookup.\n\n        Args:\n            id_server: The server name (including port, if required)\n                of the identity server to use.\n            id_access_token: The access token to authenticate to the identity server with\n            medium: The type of the third party identifier (e.g. \"email\").\n            address: The third party identifier (e.g. \"foo@example.com\").\n\n        Returns:\n            the matrix ID of the 3pid, or None if it is not recognised.\n        \"\"\"\n        # Check what hashing details are supported by this identity server\n        try:\n            hash_details = await self.blacklisting_http_client.get_json(\n                \"%s%s/_matrix/identity/v2/hash_details\" % (id_server_scheme, id_server),\n                {\"access_token\": id_access_token},\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n\n        if not isinstance(hash_details, dict):\n            logger.warning(\n                \"Got non-dict object when checking hash details of %s%s: %s\",\n                id_server_scheme,\n                id_server,\n                hash_details,\n            )\n            raise SynapseError(\n                400,\n                \"Non-dict object from %s%s during v2 hash_details request: %s\"\n                % (id_server_scheme, id_server, hash_details),\n            )\n\n        # Extract information from hash_details\n        supported_lookup_algorithms = hash_details.get(\"algorithms\")\n        lookup_pepper = hash_details.get(\"lookup_pepper\")\n        if (\n            not supported_lookup_algorithms\n            or not isinstance(supported_lookup_algorithms, list)\n            or not lookup_pepper\n            or not isinstance(lookup_pepper, str)\n        ):\n            raise SynapseError(\n                400,\n                \"Invalid hash details received from identity server %s%s: %s\"\n                % (id_server_scheme, id_server, hash_details),\n            )\n\n        # Check if any of the supported lookup algorithms are present\n        if LookupAlgorithm.SHA256 in supported_lookup_algorithms:\n            # Perform a hashed lookup\n            lookup_algorithm = LookupAlgorithm.SHA256\n\n            # Hash address, medium and the pepper with sha256\n            to_hash = \"%s %s %s\" % (address, medium, lookup_pepper)\n            lookup_value = sha256_and_url_safe_base64(to_hash)\n\n        elif LookupAlgorithm.NONE in supported_lookup_algorithms:\n            # Perform a non-hashed lookup\n            lookup_algorithm = LookupAlgorithm.NONE\n\n            # Combine together plaintext address and medium\n            lookup_value = \"%s %s\" % (address, medium)\n\n        else:\n            logger.warning(\n                \"None of the provided lookup algorithms of %s are supported: %s\",\n                id_server,\n                supported_lookup_algorithms,\n            )\n            raise SynapseError(\n                400,\n                \"Provided identity server does not support any v2 lookup \"\n                \"algorithms that this homeserver supports.\",\n            )\n\n        # Authenticate with identity server given the access token from the client\n        headers = {\"Authorization\": create_id_access_token_header(id_access_token)}\n\n        try:\n            lookup_results = await self.blacklisting_http_client.post_json_get_json(\n                \"%s%s/_matrix/identity/v2/lookup\" % (id_server_scheme, id_server),\n                {\n                    \"addresses\": [lookup_value],\n                    \"algorithm\": lookup_algorithm,\n                    \"pepper\": lookup_pepper,\n                },\n                headers=headers,\n            )\n        except RequestTimedOutError:\n            raise SynapseError(500, \"Timed out contacting identity server\")\n        except Exception as e:\n            logger.warning(\"Error when performing a v2 3pid lookup: %s\", e)\n            raise SynapseError(\n                500, \"Unknown error occurred during identity server lookup\"\n            )\n\n        # Check for a mapping from what we looked up to an MXID\n        if \"mappings\" not in lookup_results or not isinstance(\n            lookup_results[\"mappings\"], dict\n        ):\n            logger.warning(\"No results from 3pid lookup\")\n            return None\n\n        # Return the MXID if it's available, or None otherwise\n        mxid = lookup_results[\"mappings\"].get(lookup_value)\n        return mxid\n\n    async def ask_id_server_for_third_party_invite(\n        self,\n        requester: Requester,\n        id_server: str,\n        medium: str,\n        address: str,\n        room_id: str,\n        inviter_user_id: str,\n        room_alias: str,\n        room_avatar_url: str,\n        room_join_rules: str,\n        room_name: str,\n        inviter_display_name: str,\n        inviter_avatar_url: str,\n        id_access_token: Optional[str] = None,\n    ) -> Tuple[str, List[Dict[str, str]], Dict[str, str], str]:\n        \"\"\"\n        Asks an identity server for a third party invite.\n\n        Args:\n            requester\n            id_server: hostname + optional port for the identity server.\n            medium: The literal string \"email\".\n            address: The third party address being invited.\n            room_id: The ID of the room to which the user is invited.\n            inviter_user_id: The user ID of the inviter.\n            room_alias: An alias for the room, for cosmetic notifications.\n            room_avatar_url: The URL of the room's avatar, for cosmetic\n                notifications.\n            room_join_rules: The join rules of the email (e.g. \"public\").\n            room_name: The m.room.name of the room.\n            inviter_display_name: The current display name of the\n                inviter.\n            inviter_avatar_url: The URL of the inviter's avatar.\n            id_access_token (str|None): The access token to authenticate to the identity\n                server with\n\n        Returns:\n            A tuple containing:\n                token: The token which must be signed to prove authenticity.\n                public_keys ([{\"public_key\": str, \"key_validity_url\": str}]):\n                    public_key is a base64-encoded ed25519 public key.\n                fallback_public_key: One element from public_keys.\n                display_name: A user-friendly name to represent the invited user.\n        \"\"\"\n        invite_config = {\n            \"medium\": medium,\n            \"address\": address,\n            \"room_id\": room_id,\n            \"room_alias\": room_alias,\n            \"room_avatar_url\": room_avatar_url,\n            \"room_join_rules\": room_join_rules,\n            \"room_name\": room_name,\n            \"sender\": inviter_user_id,\n            \"sender_display_name\": inviter_display_name,\n            \"sender_avatar_url\": inviter_avatar_url,\n        }\n\n        # Add the identity service access token to the JSON body and use the v2\n        # Identity Service endpoints if id_access_token is present\n        data = None\n        base_url = \"%s%s/_matrix/identity\" % (id_server_scheme, id_server)\n\n        if id_access_token:\n            key_validity_url = \"%s%s/_matrix/identity/v2/pubkey/isvalid\" % (\n                id_server_scheme,\n                id_server,\n            )\n\n            # Attempt a v2 lookup\n            url = base_url + \"/v2/store-invite\"\n            try:\n                data = await self.blacklisting_http_client.post_json_get_json(\n                    url,\n                    invite_config,\n                    {\"Authorization\": create_id_access_token_header(id_access_token)},\n                )\n            except RequestTimedOutError:\n                raise SynapseError(500, \"Timed out contacting identity server\")\n            except HttpResponseException as e:\n                if e.code != 404:\n                    logger.info(\"Failed to POST %s with JSON: %s\", url, e)\n                    raise e\n\n        if data is None:\n            key_validity_url = \"%s%s/_matrix/identity/api/v1/pubkey/isvalid\" % (\n                id_server_scheme,\n                id_server,\n            )\n            url = base_url + \"/api/v1/store-invite\"\n\n            try:\n                data = await self.blacklisting_http_client.post_json_get_json(\n                    url, invite_config\n                )\n            except RequestTimedOutError:\n                raise SynapseError(500, \"Timed out contacting identity server\")\n            except HttpResponseException as e:\n                logger.warning(\n                    \"Error trying to call /store-invite on %s%s: %s\",\n                    id_server_scheme,\n                    id_server,\n                    e,\n                )\n\n            if data is None:\n                # Some identity servers may only support application/x-www-form-urlencoded\n                # types. This is especially true with old instances of Sydent, see\n                # https://github.com/matrix-org/sydent/pull/170\n                try:\n                    data = await self.blacklisting_http_client.post_urlencoded_get_json(\n                        url, invite_config\n                    )\n                except HttpResponseException as e:\n                    logger.warning(\n                        \"Error calling /store-invite on %s%s with fallback \"\n                        \"encoding: %s\",\n                        id_server_scheme,\n                        id_server,\n                        e,\n                    )\n                    raise e\n\n        # TODO: Check for success\n        token = data[\"token\"]\n        public_keys = data.get(\"public_keys\", [])\n        if \"public_key\" in data:\n            fallback_public_key = {\n                \"public_key\": data[\"public_key\"],\n                \"key_validity_url\": key_validity_url,\n            }\n        else:\n            fallback_public_key = public_keys[0]\n\n        if not public_keys:\n            public_keys.append(fallback_public_key)\n        display_name = data[\"display_name\"]\n        return token, public_keys, fallback_public_key, display_name", "target": 0}, {"function": "def create_id_access_token_header(id_access_token: str) -> List[str]:\n    \"\"\"Create an Authorization header for passing to SimpleHttpClient as the header value\n    of an HTTP request.\n\n    Args:\n        id_access_token: An identity server access token.\n\n    Returns:\n        The ascii-encoded bearer token encased in a list.\n    \"\"\"\n    # Prefix with Bearer\n    bearer_token = \"Bearer %s\" % id_access_token\n\n    # Encode headers to standard ascii\n    bearer_token.encode(\"ascii\")\n\n    # Return as a list as that's how SimpleHttpClient takes header values\n    return [bearer_token]", "target": 0}, {"function": "class LookupAlgorithm:\n    \"\"\"\n    Supported hashing algorithms when performing a 3PID lookup.\n\n    SHA256 - Hashing an (address, medium, pepper) combo with sha256, then url-safe base64\n        encoding\n    NONE - Not performing any hashing. Simply sending an (address, medium) combo in plaintext\n    \"\"\"\n\n    SHA256 = \"sha256\"\n    NONE = \"none\"", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Fhttp%2Fclient.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport urllib.parse\nfrom io import BytesIO\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    BinaryIO,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\n\nimport treq\nfrom canonicaljson import encode_canonical_json\nfrom netaddr import IPAddress, IPSet\nfrom prometheus_client import Counter\nfrom zope.interface import implementer, provider\n\nfrom OpenSSL import SSL\nfrom OpenSSL.SSL import VERIFY_NONE\nfrom twisted.internet import defer, error as twisted_error, protocol, ssl\nfrom twisted.internet.interfaces import (\n    IAddress,\n    IHostResolution,\n    IReactorPluggableNameResolver,\n    IResolutionReceiver,\n)\nfrom twisted.internet.task import Cooperator\nfrom twisted.python.failure import Failure\nfrom twisted.web._newclient import ResponseDone\nfrom twisted.web.client import (\n    Agent,\n    HTTPConnectionPool,\n    ResponseNeverReceived,\n    readBody,\n)\nfrom twisted.web.http import PotentialDataLoss\nfrom twisted.web.http_headers import Headers\nfrom twisted.web.iweb import IAgent, IBodyProducer, IResponse\n\nfrom synapse.api.errors import Codes, HttpResponseException, SynapseError\nfrom synapse.http import QuieterFileBodyProducer, RequestTimedOutError, redact_uri\nfrom synapse.http.proxyagent import ProxyAgent\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.logging.opentracing import set_tag, start_active_span, tags\nfrom synapse.util import json_decoder\nfrom synapse.util.async_helpers import timeout_deferred\n\nif TYPE_CHECKING:\n    from synapse.app.homeserver import HomeServer\n\nlogger = logging.getLogger(__name__)\n\noutgoing_requests_counter = Counter(\"synapse_http_client_requests\", \"\", [\"method\"])\nincoming_responses_counter = Counter(\n    \"synapse_http_client_responses\", \"\", [\"method\", \"code\"]\n)\n\n# the type of the headers list, to be passed to the t.w.h.Headers.\n# Actually we can mix str and bytes keys, but Mapping treats 'key' as invariant so\n# we simplify.\nRawHeaders = Union[Mapping[str, \"RawHeaderValue\"], Mapping[bytes, \"RawHeaderValue\"]]\n\n# the value actually has to be a List, but List is invariant so we can't specify that\n# the entries can either be Lists or bytes.\nRawHeaderValue = Sequence[Union[str, bytes]]\n\n# the type of the query params, to be passed into `urlencode`\nQueryParamValue = Union[str, bytes, Iterable[Union[str, bytes]]]\nQueryParams = Union[Mapping[str, QueryParamValue], Mapping[bytes, QueryParamValue]]\n\n\ndef check_against_blacklist(\n    ip_address: IPAddress, ip_whitelist: Optional[IPSet], ip_blacklist: IPSet\n) -> bool:\n    \"\"\"\n    Compares an IP address to allowed and disallowed IP sets.\n\n    Args:\n        ip_address: The IP address to check\n        ip_whitelist: Allowed IP addresses.\n        ip_blacklist: Disallowed IP addresses.\n\n    Returns:\n        True if the IP address is in the blacklist and not in the whitelist.\n    \"\"\"\n    if ip_address in ip_blacklist:\n        if ip_whitelist is None or ip_address not in ip_whitelist:\n            return True\n    return False\n\n\n_EPSILON = 0.00000001\n\n\ndef _make_scheduler(reactor):\n    \"\"\"Makes a schedular suitable for a Cooperator using the given reactor.\n\n    (This is effectively just a copy from `twisted.internet.task`)\n    \"\"\"\n\n    def _scheduler(x):\n        return reactor.callLater(_EPSILON, x)\n\n    return _scheduler\n\n\nclass _IPBlacklistingResolver:\n    \"\"\"\n    A proxy for reactor.nameResolver which only produces non-blacklisted IP\n    addresses, preventing DNS rebinding attacks on URL preview.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorPluggableNameResolver,\n        ip_whitelist: Optional[IPSet],\n        ip_blacklist: IPSet,\n    ):\n        \"\"\"\n        Args:\n            reactor: The twisted reactor.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._reactor = reactor\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def resolveHostName(\n        self, recv: IResolutionReceiver, hostname: str, portNumber: int = 0\n    ) -> IResolutionReceiver:\n\n        r = recv()\n        addresses = []  # type: List[IAddress]\n\n        def _callback() -> None:\n            r.resolutionBegan(None)\n\n            has_bad_ip = False\n            for i in addresses:\n                ip_address = IPAddress(i.host)\n\n                if check_against_blacklist(\n                    ip_address, self._ip_whitelist, self._ip_blacklist\n                ):\n                    logger.info(\n                        \"Dropped %s from DNS resolution to %s due to blacklist\"\n                        % (ip_address, hostname)\n                    )\n                    has_bad_ip = True\n\n            # if we have a blacklisted IP, we'd like to raise an error to block the\n            # request, but all we can really do from here is claim that there were no\n            # valid results.\n            if not has_bad_ip:\n                for i in addresses:\n                    r.addressResolved(i)\n            r.resolutionComplete()\n\n        @provider(IResolutionReceiver)\n        class EndpointReceiver:\n            @staticmethod\n            def resolutionBegan(resolutionInProgress: IHostResolution) -> None:\n                pass\n\n            @staticmethod\n            def addressResolved(address: IAddress) -> None:\n                addresses.append(address)\n\n            @staticmethod\n            def resolutionComplete() -> None:\n                _callback()\n\n        self._reactor.nameResolver.resolveHostName(\n            EndpointReceiver, hostname, portNumber=portNumber\n        )\n\n        return r\n\n\n@implementer(IReactorPluggableNameResolver)\nclass BlacklistingReactorWrapper:\n    \"\"\"\n    A Reactor wrapper which will prevent DNS resolution to blacklisted IP\n    addresses, to prevent DNS rebinding.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorPluggableNameResolver,\n        ip_whitelist: Optional[IPSet],\n        ip_blacklist: IPSet,\n    ):\n        self._reactor = reactor\n\n        # We need to use a DNS resolver which filters out blacklisted IP\n        # addresses, to prevent DNS rebinding.\n        self._nameResolver = _IPBlacklistingResolver(\n            self._reactor, ip_whitelist, ip_blacklist\n        )\n\n    def __getattr__(self, attr: str) -> Any:\n        # Passthrough to the real reactor except for the DNS resolver.\n        if attr == \"nameResolver\":\n            return self._nameResolver\n        else:\n            return getattr(self._reactor, attr)\n\n\nclass BlacklistingAgentWrapper(Agent):\n    \"\"\"\n    An Agent wrapper which will prevent access to IP addresses being accessed\n    directly (without an IP address lookup).\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: IAgent,\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n    ):\n        \"\"\"\n        Args:\n            agent: The Agent to wrap.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._agent = agent\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        h = urllib.parse.urlparse(uri.decode(\"ascii\"))\n\n        try:\n            ip_address = IPAddress(h.hostname)\n\n            if check_against_blacklist(\n                ip_address, self._ip_whitelist, self._ip_blacklist\n            ):\n                logger.info(\"Blocking access to %s due to blacklist\" % (ip_address,))\n                e = SynapseError(403, \"IP address blocked by IP blacklist entry\")\n                return defer.fail(Failure(e))\n        except Exception:\n            # Not an IP\n            pass\n\n        return self._agent.request(\n            method, uri, headers=headers, bodyProducer=bodyProducer\n        )\n\n\nclass SimpleHttpClient:\n    \"\"\"\n    A simple, no-frills HTTP client with methods that wrap up common ways of\n    using HTTP in Matrix\n    \"\"\"\n\n    def __init__(\n        self,\n        hs: \"HomeServer\",\n        treq_args: Dict[str, Any] = {},\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n        http_proxy: Optional[bytes] = None,\n        https_proxy: Optional[bytes] = None,\n    ):\n        \"\"\"\n        Args:\n            hs\n            treq_args: Extra keyword arguments to be given to treq.request.\n            ip_blacklist: The IP addresses that are blacklisted that\n                we may not request.\n            ip_whitelist: The whitelisted IP addresses, that we can\n               request if it were otherwise caught in a blacklist.\n            http_proxy: proxy server to use for http connections. host[:port]\n            https_proxy: proxy server to use for https connections. host[:port]\n        \"\"\"\n        self.hs = hs\n\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n        self._extra_treq_args = treq_args\n\n        self.user_agent = hs.version_string\n        self.clock = hs.get_clock()\n        if hs.config.user_agent_suffix:\n            self.user_agent = \"%s %s\" % (self.user_agent, hs.config.user_agent_suffix)\n\n        # We use this for our body producers to ensure that they use the correct\n        # reactor.\n        self._cooperator = Cooperator(scheduler=_make_scheduler(hs.get_reactor()))\n\n        self.user_agent = self.user_agent.encode(\"ascii\")\n\n        if self._ip_blacklist:\n            # If we have an IP blacklist, we need to use a DNS resolver which\n            # filters out blacklisted IP addresses, to prevent DNS rebinding.\n            self.reactor = BlacklistingReactorWrapper(\n                hs.get_reactor(), self._ip_whitelist, self._ip_blacklist\n            )\n        else:\n            self.reactor = hs.get_reactor()\n\n        # the pusher makes lots of concurrent SSL connections to sygnal, and\n        # tends to do so in batches, so we need to allow the pool to keep\n        # lots of idle connections around.\n        pool = HTTPConnectionPool(self.reactor)\n        # XXX: The justification for using the cache factor here is that larger instances\n        # will need both more cache and more connections.\n        # Still, this should probably be a separate dial\n        pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))\n        pool.cachedConnectionTimeout = 2 * 60\n\n        self.agent = ProxyAgent(\n            self.reactor,\n            connectTimeout=15,\n            contextFactory=self.hs.get_http_client_context_factory(),\n            pool=pool,\n            http_proxy=http_proxy,\n            https_proxy=https_proxy,\n        )\n\n        if self._ip_blacklist:\n            # If we have an IP blacklist, we then install the blacklisting Agent\n            # which prevents direct access to IP addresses, that are not caught\n            # by the DNS resolution.\n            self.agent = BlacklistingAgentWrapper(\n                self.agent,\n                ip_whitelist=self._ip_whitelist,\n                ip_blacklist=self._ip_blacklist,\n            )\n\n    async def request(\n        self,\n        method: str,\n        uri: str,\n        data: Optional[bytes] = None,\n        headers: Optional[Headers] = None,\n    ) -> IResponse:\n        \"\"\"\n        Args:\n            method: HTTP method to use.\n            uri: URI to query.\n            data: Data to send in the request body, if applicable.\n            headers: Request headers.\n\n        Returns:\n            Response object, once the headers have been read.\n\n        Raises:\n            RequestTimedOutError if the request times out before the headers are read\n\n        \"\"\"\n        outgoing_requests_counter.labels(method).inc()\n\n        # log request but strip `access_token` (AS requests for example include this)\n        logger.debug(\"Sending request %s %s\", method, redact_uri(uri))\n\n        with start_active_span(\n            \"outgoing-client-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.HTTP_METHOD: method,\n                tags.HTTP_URL: uri,\n            },\n            finish_on_close=True,\n        ):\n            try:\n                body_producer = None\n                if data is not None:\n                    body_producer = QuieterFileBodyProducer(\n                        BytesIO(data), cooperator=self._cooperator,\n                    )\n\n                request_deferred = treq.request(\n                    method,\n                    uri,\n                    agent=self.agent,\n                    data=body_producer,\n                    headers=headers,\n                    **self._extra_treq_args,\n                )  # type: defer.Deferred\n\n                # we use our own timeout mechanism rather than treq's as a workaround\n                # for https://twistedmatrix.com/trac/ticket/9534.\n                request_deferred = timeout_deferred(\n                    request_deferred, 60, self.hs.get_reactor(),\n                )\n\n                # turn timeouts into RequestTimedOutErrors\n                request_deferred.addErrback(_timeout_to_request_timed_out_error)\n\n                response = await make_deferred_yieldable(request_deferred)\n\n                incoming_responses_counter.labels(method, response.code).inc()\n                logger.info(\n                    \"Received response to %s %s: %s\",\n                    method,\n                    redact_uri(uri),\n                    response.code,\n                )\n                return response\n            except Exception as e:\n                incoming_responses_counter.labels(method, \"ERR\").inc()\n                logger.info(\n                    \"Error sending request to  %s %s: %s %s\",\n                    method,\n                    redact_uri(uri),\n                    type(e).__name__,\n                    e.args[0],\n                )\n                set_tag(tags.ERROR, True)\n                set_tag(\"error_reason\", e.args[0])\n                raise\n\n    async def post_urlencoded_get_json(\n        self,\n        uri: str,\n        args: Optional[Mapping[str, Union[str, List[str]]]] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"\n        Args:\n            uri: uri to query\n            args: parameters to be url-encoded in the body\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n\n        # TODO: Do we ever want to log message contents?\n        logger.debug(\"post_urlencoded_get_json args: %s\", args)\n\n        query_bytes = encode_query_args(args)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/x-www-form-urlencoded\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=query_bytes\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def post_json_get_json(\n        self, uri: str, post_json: Any, headers: Optional[RawHeaders] = None\n    ) -> Any:\n        \"\"\"\n\n        Args:\n            uri: URI to query.\n            post_json: request body, to be encoded as json\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        json_str = encode_canonical_json(post_json)\n\n        logger.debug(\"HTTP POST %s -> %s\", json_str, uri)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_json(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"Gets some json from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query string\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        actual_headers = {b\"Accept\": [b\"application/json\"]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        body = await self.get_raw(uri, args, headers=headers)\n        return json_decoder.decode(body.decode(\"utf-8\"))\n\n    async def put_json(\n        self,\n        uri: str,\n        json_body: Any,\n        args: Optional[QueryParams] = None,\n        headers: RawHeaders = None,\n    ) -> Any:\n        \"\"\"Puts some json to the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            json_body: The JSON to put in the HTTP body,\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n             RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        json_str = encode_canonical_json(json_body)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"PUT\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_raw(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> bytes:\n        \"\"\"Gets raw text from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the\n            HTTP body as bytes.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException on a non-2xx HTTP response.\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", uri, headers=Headers(actual_headers))\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return body\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    # XXX: FIXME: This is horribly copy-pasted from matrixfederationclient.\n    # The two should be factored out.\n\n    async def get_file(\n        self,\n        url: str,\n        output_stream: BinaryIO,\n        max_size: Optional[int] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:\n        \"\"\"GETs a file from a given URL\n        Args:\n            url: The URL to GET\n            output_stream: File to write the response body to.\n            headers: A map from header name to a list of values for that header\n        Returns:\n            A tuple of the file length, dict of the response\n            headers, absolute URI of the response and HTTP response code.\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            SynapseError: if the response is not a 2xx, the remote file is too large, or\n               another exception happens during the download.\n        \"\"\"\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", url, headers=Headers(actual_headers))\n\n        resp_headers = dict(response.headers.getAllRawHeaders())\n\n        if (\n            b\"Content-Length\" in resp_headers\n            and max_size\n            and int(resp_headers[b\"Content-Length\"][0]) > max_size\n        ):\n            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))\n            raise SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n\n        if response.code > 299:\n            logger.warning(\"Got %d when downloading %s\" % (response.code, url))\n            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)\n\n        # TODO: if our Content-Type is HTML or something, just read the first\n        # N bytes into RAM rather than saving it all to disk only to read it\n        # straight back in again\n\n        try:\n            length = await make_deferred_yieldable(\n                readBodyToFile(response, output_stream, max_size)\n            )\n        except SynapseError:\n            # This can happen e.g. because the body is too large.\n            raise\n        except Exception as e:\n            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e\n\n        return (\n            length,\n            resp_headers,\n            response.request.absoluteURI.decode(\"ascii\"),\n            response.code,\n        )\n\n\ndef _timeout_to_request_timed_out_error(f: Failure):\n    if f.check(twisted_error.TimeoutError, twisted_error.ConnectingCancelledError):\n        # The TCP connection has its own timeout (set by the 'connectTimeout' param\n        # on the Agent), which raises twisted_error.TimeoutError exception.\n        raise RequestTimedOutError(\"Timeout connecting to remote server\")\n    elif f.check(defer.TimeoutError, ResponseNeverReceived):\n        # this one means that we hit our overall timeout on the request\n        raise RequestTimedOutError(\"Timeout waiting for response from remote server\")\n\n    return f\n\n\nclass _ReadBodyToFileProtocol(protocol.Protocol):\n    def __init__(\n        self, stream: BinaryIO, deferred: defer.Deferred, max_size: Optional[int]\n    ):\n        self.stream = stream\n        self.deferred = deferred\n        self.length = 0\n        self.max_size = max_size\n\n    def dataReceived(self, data: bytes) -> None:\n        self.stream.write(data)\n        self.length += len(data)\n        if self.max_size is not None and self.length >= self.max_size:\n            self.deferred.errback(\n                SynapseError(\n                    502,\n                    \"Requested file is too large > %r bytes\" % (self.max_size,),\n                    Codes.TOO_LARGE,\n                )\n            )\n            self.deferred = defer.Deferred()\n            self.transport.loseConnection()\n\n    def connectionLost(self, reason: Failure) -> None:\n        if reason.check(ResponseDone):\n            self.deferred.callback(self.length)\n        elif reason.check(PotentialDataLoss):\n            # stolen from https://github.com/twisted/treq/pull/49/files\n            # http://twistedmatrix.com/trac/ticket/4840\n            self.deferred.callback(self.length)\n        else:\n            self.deferred.errback(reason)\n\n\ndef readBodyToFile(\n    response: IResponse, stream: BinaryIO, max_size: Optional[int]\n) -> defer.Deferred:\n    \"\"\"\n    Read a HTTP response body to a file-object. Optionally enforcing a maximum file size.\n\n    Args:\n        response: The HTTP response to read from.\n        stream: The file-object to write to.\n        max_size: The maximum file size to allow.\n\n    Returns:\n        A Deferred which resolves to the length of the read body.\n    \"\"\"\n\n    d = defer.Deferred()\n    response.deliverBody(_ReadBodyToFileProtocol(stream, d, max_size))\n    return d\n\n\ndef encode_query_args(args: Optional[Mapping[str, Union[str, List[str]]]]) -> bytes:\n    \"\"\"\n    Encodes a map of query arguments to bytes which can be appended to a URL.\n\n    Args:\n        args: The query arguments, a mapping of string to string or list of strings.\n\n    Returns:\n        The query arguments encoded as bytes.\n    \"\"\"\n    if args is None:\n        return b\"\"\n\n    encoded_args = {}\n    for k, vs in args.items():\n        if isinstance(vs, str):\n            vs = [vs]\n        encoded_args[k] = [v.encode(\"utf8\") for v in vs]\n\n    query_str = urllib.parse.urlencode(encoded_args, True)\n\n    return query_str.encode(\"utf8\")\n\n\nclass InsecureInterceptableContextFactory(ssl.ContextFactory):\n    \"\"\"\n    Factory for PyOpenSSL SSL contexts which accepts any certificate for any domain.\n\n    Do not use this since it allows an attacker to intercept your communications.\n    \"\"\"\n\n    def __init__(self):\n        self._context = SSL.Context(SSL.SSLv23_METHOD)\n        self._context.set_verify(VERIFY_NONE, lambda *_: None)\n\n    def getContext(self, hostname=None, port=None):\n        return self._context\n\n    def creatorForNetloc(self, hostname, port):\n        return self\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport urllib.parse\nfrom io import BytesIO\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    BinaryIO,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\n\nimport treq\nfrom canonicaljson import encode_canonical_json\nfrom netaddr import IPAddress, IPSet\nfrom prometheus_client import Counter\nfrom zope.interface import implementer, provider\n\nfrom OpenSSL import SSL\nfrom OpenSSL.SSL import VERIFY_NONE\nfrom twisted.internet import defer, error as twisted_error, protocol, ssl\nfrom twisted.internet.interfaces import (\n    IAddress,\n    IHostResolution,\n    IReactorPluggableNameResolver,\n    IResolutionReceiver,\n)\nfrom twisted.internet.task import Cooperator\nfrom twisted.python.failure import Failure\nfrom twisted.web._newclient import ResponseDone\nfrom twisted.web.client import (\n    Agent,\n    HTTPConnectionPool,\n    ResponseNeverReceived,\n    readBody,\n)\nfrom twisted.web.http import PotentialDataLoss\nfrom twisted.web.http_headers import Headers\nfrom twisted.web.iweb import IAgent, IBodyProducer, IResponse\n\nfrom synapse.api.errors import Codes, HttpResponseException, SynapseError\nfrom synapse.http import QuieterFileBodyProducer, RequestTimedOutError, redact_uri\nfrom synapse.http.proxyagent import ProxyAgent\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.logging.opentracing import set_tag, start_active_span, tags\nfrom synapse.util import json_decoder\nfrom synapse.util.async_helpers import timeout_deferred\n\nif TYPE_CHECKING:\n    from synapse.app.homeserver import HomeServer\n\nlogger = logging.getLogger(__name__)\n\noutgoing_requests_counter = Counter(\"synapse_http_client_requests\", \"\", [\"method\"])\nincoming_responses_counter = Counter(\n    \"synapse_http_client_responses\", \"\", [\"method\", \"code\"]\n)\n\n# the type of the headers list, to be passed to the t.w.h.Headers.\n# Actually we can mix str and bytes keys, but Mapping treats 'key' as invariant so\n# we simplify.\nRawHeaders = Union[Mapping[str, \"RawHeaderValue\"], Mapping[bytes, \"RawHeaderValue\"]]\n\n# the value actually has to be a List, but List is invariant so we can't specify that\n# the entries can either be Lists or bytes.\nRawHeaderValue = Sequence[Union[str, bytes]]\n\n# the type of the query params, to be passed into `urlencode`\nQueryParamValue = Union[str, bytes, Iterable[Union[str, bytes]]]\nQueryParams = Union[Mapping[str, QueryParamValue], Mapping[bytes, QueryParamValue]]\n\n\ndef check_against_blacklist(\n    ip_address: IPAddress, ip_whitelist: Optional[IPSet], ip_blacklist: IPSet\n) -> bool:\n    \"\"\"\n    Compares an IP address to allowed and disallowed IP sets.\n\n    Args:\n        ip_address: The IP address to check\n        ip_whitelist: Allowed IP addresses.\n        ip_blacklist: Disallowed IP addresses.\n\n    Returns:\n        True if the IP address is in the blacklist and not in the whitelist.\n    \"\"\"\n    if ip_address in ip_blacklist:\n        if ip_whitelist is None or ip_address not in ip_whitelist:\n            return True\n    return False\n\n\n_EPSILON = 0.00000001\n\n\ndef _make_scheduler(reactor):\n    \"\"\"Makes a schedular suitable for a Cooperator using the given reactor.\n\n    (This is effectively just a copy from `twisted.internet.task`)\n    \"\"\"\n\n    def _scheduler(x):\n        return reactor.callLater(_EPSILON, x)\n\n    return _scheduler\n\n\nclass IPBlacklistingResolver:\n    \"\"\"\n    A proxy for reactor.nameResolver which only produces non-blacklisted IP\n    addresses, preventing DNS rebinding attacks on URL preview.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorPluggableNameResolver,\n        ip_whitelist: Optional[IPSet],\n        ip_blacklist: IPSet,\n    ):\n        \"\"\"\n        Args:\n            reactor: The twisted reactor.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._reactor = reactor\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def resolveHostName(\n        self, recv: IResolutionReceiver, hostname: str, portNumber: int = 0\n    ) -> IResolutionReceiver:\n\n        r = recv()\n        addresses = []  # type: List[IAddress]\n\n        def _callback() -> None:\n            r.resolutionBegan(None)\n\n            has_bad_ip = False\n            for i in addresses:\n                ip_address = IPAddress(i.host)\n\n                if check_against_blacklist(\n                    ip_address, self._ip_whitelist, self._ip_blacklist\n                ):\n                    logger.info(\n                        \"Dropped %s from DNS resolution to %s due to blacklist\"\n                        % (ip_address, hostname)\n                    )\n                    has_bad_ip = True\n\n            # if we have a blacklisted IP, we'd like to raise an error to block the\n            # request, but all we can really do from here is claim that there were no\n            # valid results.\n            if not has_bad_ip:\n                for i in addresses:\n                    r.addressResolved(i)\n            r.resolutionComplete()\n\n        @provider(IResolutionReceiver)\n        class EndpointReceiver:\n            @staticmethod\n            def resolutionBegan(resolutionInProgress: IHostResolution) -> None:\n                pass\n\n            @staticmethod\n            def addressResolved(address: IAddress) -> None:\n                addresses.append(address)\n\n            @staticmethod\n            def resolutionComplete() -> None:\n                _callback()\n\n        self._reactor.nameResolver.resolveHostName(\n            EndpointReceiver, hostname, portNumber=portNumber\n        )\n\n        return r\n\n\nclass BlacklistingAgentWrapper(Agent):\n    \"\"\"\n    An Agent wrapper which will prevent access to IP addresses being accessed\n    directly (without an IP address lookup).\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: IAgent,\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n    ):\n        \"\"\"\n        Args:\n            agent: The Agent to wrap.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._agent = agent\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        h = urllib.parse.urlparse(uri.decode(\"ascii\"))\n\n        try:\n            ip_address = IPAddress(h.hostname)\n\n            if check_against_blacklist(\n                ip_address, self._ip_whitelist, self._ip_blacklist\n            ):\n                logger.info(\"Blocking access to %s due to blacklist\" % (ip_address,))\n                e = SynapseError(403, \"IP address blocked by IP blacklist entry\")\n                return defer.fail(Failure(e))\n        except Exception:\n            # Not an IP\n            pass\n\n        return self._agent.request(\n            method, uri, headers=headers, bodyProducer=bodyProducer\n        )\n\n\nclass SimpleHttpClient:\n    \"\"\"\n    A simple, no-frills HTTP client with methods that wrap up common ways of\n    using HTTP in Matrix\n    \"\"\"\n\n    def __init__(\n        self,\n        hs: \"HomeServer\",\n        treq_args: Dict[str, Any] = {},\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n        http_proxy: Optional[bytes] = None,\n        https_proxy: Optional[bytes] = None,\n    ):\n        \"\"\"\n        Args:\n            hs\n            treq_args: Extra keyword arguments to be given to treq.request.\n            ip_blacklist: The IP addresses that are blacklisted that\n                we may not request.\n            ip_whitelist: The whitelisted IP addresses, that we can\n               request if it were otherwise caught in a blacklist.\n            http_proxy: proxy server to use for http connections. host[:port]\n            https_proxy: proxy server to use for https connections. host[:port]\n        \"\"\"\n        self.hs = hs\n\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n        self._extra_treq_args = treq_args\n\n        self.user_agent = hs.version_string\n        self.clock = hs.get_clock()\n        if hs.config.user_agent_suffix:\n            self.user_agent = \"%s %s\" % (self.user_agent, hs.config.user_agent_suffix)\n\n        # We use this for our body producers to ensure that they use the correct\n        # reactor.\n        self._cooperator = Cooperator(scheduler=_make_scheduler(hs.get_reactor()))\n\n        self.user_agent = self.user_agent.encode(\"ascii\")\n\n        if self._ip_blacklist:\n            real_reactor = hs.get_reactor()\n            # If we have an IP blacklist, we need to use a DNS resolver which\n            # filters out blacklisted IP addresses, to prevent DNS rebinding.\n            nameResolver = IPBlacklistingResolver(\n                real_reactor, self._ip_whitelist, self._ip_blacklist\n            )\n\n            @implementer(IReactorPluggableNameResolver)\n            class Reactor:\n                def __getattr__(_self, attr):\n                    if attr == \"nameResolver\":\n                        return nameResolver\n                    else:\n                        return getattr(real_reactor, attr)\n\n            self.reactor = Reactor()\n        else:\n            self.reactor = hs.get_reactor()\n\n        # the pusher makes lots of concurrent SSL connections to sygnal, and\n        # tends to do so in batches, so we need to allow the pool to keep\n        # lots of idle connections around.\n        pool = HTTPConnectionPool(self.reactor)\n        # XXX: The justification for using the cache factor here is that larger instances\n        # will need both more cache and more connections.\n        # Still, this should probably be a separate dial\n        pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))\n        pool.cachedConnectionTimeout = 2 * 60\n\n        self.agent = ProxyAgent(\n            self.reactor,\n            connectTimeout=15,\n            contextFactory=self.hs.get_http_client_context_factory(),\n            pool=pool,\n            http_proxy=http_proxy,\n            https_proxy=https_proxy,\n        )\n\n        if self._ip_blacklist:\n            # If we have an IP blacklist, we then install the blacklisting Agent\n            # which prevents direct access to IP addresses, that are not caught\n            # by the DNS resolution.\n            self.agent = BlacklistingAgentWrapper(\n                self.agent,\n                ip_whitelist=self._ip_whitelist,\n                ip_blacklist=self._ip_blacklist,\n            )\n\n    async def request(\n        self,\n        method: str,\n        uri: str,\n        data: Optional[bytes] = None,\n        headers: Optional[Headers] = None,\n    ) -> IResponse:\n        \"\"\"\n        Args:\n            method: HTTP method to use.\n            uri: URI to query.\n            data: Data to send in the request body, if applicable.\n            headers: Request headers.\n\n        Returns:\n            Response object, once the headers have been read.\n\n        Raises:\n            RequestTimedOutError if the request times out before the headers are read\n\n        \"\"\"\n        outgoing_requests_counter.labels(method).inc()\n\n        # log request but strip `access_token` (AS requests for example include this)\n        logger.debug(\"Sending request %s %s\", method, redact_uri(uri))\n\n        with start_active_span(\n            \"outgoing-client-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.HTTP_METHOD: method,\n                tags.HTTP_URL: uri,\n            },\n            finish_on_close=True,\n        ):\n            try:\n                body_producer = None\n                if data is not None:\n                    body_producer = QuieterFileBodyProducer(\n                        BytesIO(data), cooperator=self._cooperator,\n                    )\n\n                request_deferred = treq.request(\n                    method,\n                    uri,\n                    agent=self.agent,\n                    data=body_producer,\n                    headers=headers,\n                    **self._extra_treq_args,\n                )  # type: defer.Deferred\n\n                # we use our own timeout mechanism rather than treq's as a workaround\n                # for https://twistedmatrix.com/trac/ticket/9534.\n                request_deferred = timeout_deferred(\n                    request_deferred, 60, self.hs.get_reactor(),\n                )\n\n                # turn timeouts into RequestTimedOutErrors\n                request_deferred.addErrback(_timeout_to_request_timed_out_error)\n\n                response = await make_deferred_yieldable(request_deferred)\n\n                incoming_responses_counter.labels(method, response.code).inc()\n                logger.info(\n                    \"Received response to %s %s: %s\",\n                    method,\n                    redact_uri(uri),\n                    response.code,\n                )\n                return response\n            except Exception as e:\n                incoming_responses_counter.labels(method, \"ERR\").inc()\n                logger.info(\n                    \"Error sending request to  %s %s: %s %s\",\n                    method,\n                    redact_uri(uri),\n                    type(e).__name__,\n                    e.args[0],\n                )\n                set_tag(tags.ERROR, True)\n                set_tag(\"error_reason\", e.args[0])\n                raise\n\n    async def post_urlencoded_get_json(\n        self,\n        uri: str,\n        args: Optional[Mapping[str, Union[str, List[str]]]] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"\n        Args:\n            uri: uri to query\n            args: parameters to be url-encoded in the body\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n\n        # TODO: Do we ever want to log message contents?\n        logger.debug(\"post_urlencoded_get_json args: %s\", args)\n\n        query_bytes = encode_query_args(args)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/x-www-form-urlencoded\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=query_bytes\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def post_json_get_json(\n        self, uri: str, post_json: Any, headers: Optional[RawHeaders] = None\n    ) -> Any:\n        \"\"\"\n\n        Args:\n            uri: URI to query.\n            post_json: request body, to be encoded as json\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        json_str = encode_canonical_json(post_json)\n\n        logger.debug(\"HTTP POST %s -> %s\", json_str, uri)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_json(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"Gets some json from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query string\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        actual_headers = {b\"Accept\": [b\"application/json\"]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        body = await self.get_raw(uri, args, headers=headers)\n        return json_decoder.decode(body.decode(\"utf-8\"))\n\n    async def put_json(\n        self,\n        uri: str,\n        json_body: Any,\n        args: Optional[QueryParams] = None,\n        headers: RawHeaders = None,\n    ) -> Any:\n        \"\"\"Puts some json to the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            json_body: The JSON to put in the HTTP body,\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n             RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        json_str = encode_canonical_json(json_body)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"PUT\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_raw(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> bytes:\n        \"\"\"Gets raw text from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the\n            HTTP body as bytes.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException on a non-2xx HTTP response.\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", uri, headers=Headers(actual_headers))\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return body\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    # XXX: FIXME: This is horribly copy-pasted from matrixfederationclient.\n    # The two should be factored out.\n\n    async def get_file(\n        self,\n        url: str,\n        output_stream: BinaryIO,\n        max_size: Optional[int] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:\n        \"\"\"GETs a file from a given URL\n        Args:\n            url: The URL to GET\n            output_stream: File to write the response body to.\n            headers: A map from header name to a list of values for that header\n        Returns:\n            A tuple of the file length, dict of the response\n            headers, absolute URI of the response and HTTP response code.\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            SynapseError: if the response is not a 2xx, the remote file is too large, or\n               another exception happens during the download.\n        \"\"\"\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", url, headers=Headers(actual_headers))\n\n        resp_headers = dict(response.headers.getAllRawHeaders())\n\n        if (\n            b\"Content-Length\" in resp_headers\n            and max_size\n            and int(resp_headers[b\"Content-Length\"][0]) > max_size\n        ):\n            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))\n            raise SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n\n        if response.code > 299:\n            logger.warning(\"Got %d when downloading %s\" % (response.code, url))\n            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)\n\n        # TODO: if our Content-Type is HTML or something, just read the first\n        # N bytes into RAM rather than saving it all to disk only to read it\n        # straight back in again\n\n        try:\n            length = await make_deferred_yieldable(\n                readBodyToFile(response, output_stream, max_size)\n            )\n        except SynapseError:\n            # This can happen e.g. because the body is too large.\n            raise\n        except Exception as e:\n            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e\n\n        return (\n            length,\n            resp_headers,\n            response.request.absoluteURI.decode(\"ascii\"),\n            response.code,\n        )\n\n\ndef _timeout_to_request_timed_out_error(f: Failure):\n    if f.check(twisted_error.TimeoutError, twisted_error.ConnectingCancelledError):\n        # The TCP connection has its own timeout (set by the 'connectTimeout' param\n        # on the Agent), which raises twisted_error.TimeoutError exception.\n        raise RequestTimedOutError(\"Timeout connecting to remote server\")\n    elif f.check(defer.TimeoutError, ResponseNeverReceived):\n        # this one means that we hit our overall timeout on the request\n        raise RequestTimedOutError(\"Timeout waiting for response from remote server\")\n\n    return f\n\n\nclass _ReadBodyToFileProtocol(protocol.Protocol):\n    def __init__(\n        self, stream: BinaryIO, deferred: defer.Deferred, max_size: Optional[int]\n    ):\n        self.stream = stream\n        self.deferred = deferred\n        self.length = 0\n        self.max_size = max_size\n\n    def dataReceived(self, data: bytes) -> None:\n        self.stream.write(data)\n        self.length += len(data)\n        if self.max_size is not None and self.length >= self.max_size:\n            self.deferred.errback(\n                SynapseError(\n                    502,\n                    \"Requested file is too large > %r bytes\" % (self.max_size,),\n                    Codes.TOO_LARGE,\n                )\n            )\n            self.deferred = defer.Deferred()\n            self.transport.loseConnection()\n\n    def connectionLost(self, reason: Failure) -> None:\n        if reason.check(ResponseDone):\n            self.deferred.callback(self.length)\n        elif reason.check(PotentialDataLoss):\n            # stolen from https://github.com/twisted/treq/pull/49/files\n            # http://twistedmatrix.com/trac/ticket/4840\n            self.deferred.callback(self.length)\n        else:\n            self.deferred.errback(reason)\n\n\ndef readBodyToFile(\n    response: IResponse, stream: BinaryIO, max_size: Optional[int]\n) -> defer.Deferred:\n    \"\"\"\n    Read a HTTP response body to a file-object. Optionally enforcing a maximum file size.\n\n    Args:\n        response: The HTTP response to read from.\n        stream: The file-object to write to.\n        max_size: The maximum file size to allow.\n\n    Returns:\n        A Deferred which resolves to the length of the read body.\n    \"\"\"\n\n    d = defer.Deferred()\n    response.deliverBody(_ReadBodyToFileProtocol(stream, d, max_size))\n    return d\n\n\ndef encode_query_args(args: Optional[Mapping[str, Union[str, List[str]]]]) -> bytes:\n    \"\"\"\n    Encodes a map of query arguments to bytes which can be appended to a URL.\n\n    Args:\n        args: The query arguments, a mapping of string to string or list of strings.\n\n    Returns:\n        The query arguments encoded as bytes.\n    \"\"\"\n    if args is None:\n        return b\"\"\n\n    encoded_args = {}\n    for k, vs in args.items():\n        if isinstance(vs, str):\n            vs = [vs]\n        encoded_args[k] = [v.encode(\"utf8\") for v in vs]\n\n    query_str = urllib.parse.urlencode(encoded_args, True)\n\n    return query_str.encode(\"utf8\")\n\n\nclass InsecureInterceptableContextFactory(ssl.ContextFactory):\n    \"\"\"\n    Factory for PyOpenSSL SSL contexts which accepts any certificate for any domain.\n\n    Do not use this since it allows an attacker to intercept your communications.\n    \"\"\"\n\n    def __init__(self):\n        self._context = SSL.Context(SSL.SSLv23_METHOD)\n        self._context.set_verify(VERIFY_NONE, lambda *_: None)\n\n    def getContext(self, hostname=None, port=None):\n        return self._context\n\n    def creatorForNetloc(self, hostname, port):\n        return self\n", "patch": "@@ -125,7 +125,7 @@ def _scheduler(x):\n     return _scheduler\n \n \n-class IPBlacklistingResolver:\n+class _IPBlacklistingResolver:\n     \"\"\"\n     A proxy for reactor.nameResolver which only produces non-blacklisted IP\n     addresses, preventing DNS rebinding attacks on URL preview.\n@@ -199,6 +199,35 @@ def resolutionComplete() -> None:\n         return r\n \n \n+@implementer(IReactorPluggableNameResolver)\n+class BlacklistingReactorWrapper:\n+    \"\"\"\n+    A Reactor wrapper which will prevent DNS resolution to blacklisted IP\n+    addresses, to prevent DNS rebinding.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        reactor: IReactorPluggableNameResolver,\n+        ip_whitelist: Optional[IPSet],\n+        ip_blacklist: IPSet,\n+    ):\n+        self._reactor = reactor\n+\n+        # We need to use a DNS resolver which filters out blacklisted IP\n+        # addresses, to prevent DNS rebinding.\n+        self._nameResolver = _IPBlacklistingResolver(\n+            self._reactor, ip_whitelist, ip_blacklist\n+        )\n+\n+    def __getattr__(self, attr: str) -> Any:\n+        # Passthrough to the real reactor except for the DNS resolver.\n+        if attr == \"nameResolver\":\n+            return self._nameResolver\n+        else:\n+            return getattr(self._reactor, attr)\n+\n+\n class BlacklistingAgentWrapper(Agent):\n     \"\"\"\n     An Agent wrapper which will prevent access to IP addresses being accessed\n@@ -292,22 +321,11 @@ def __init__(\n         self.user_agent = self.user_agent.encode(\"ascii\")\n \n         if self._ip_blacklist:\n-            real_reactor = hs.get_reactor()\n             # If we have an IP blacklist, we need to use a DNS resolver which\n             # filters out blacklisted IP addresses, to prevent DNS rebinding.\n-            nameResolver = IPBlacklistingResolver(\n-                real_reactor, self._ip_whitelist, self._ip_blacklist\n+            self.reactor = BlacklistingReactorWrapper(\n+                hs.get_reactor(), self._ip_whitelist, self._ip_blacklist\n             )\n-\n-            @implementer(IReactorPluggableNameResolver)\n-            class Reactor:\n-                def __getattr__(_self, attr):\n-                    if attr == \"nameResolver\":\n-                        return nameResolver\n-                    else:\n-                        return getattr(real_reactor, attr)\n-\n-            self.reactor = Reactor()\n         else:\n             self.reactor = hs.get_reactor()\n ", "file_path": "files/2021_2/24", "file_language": "py", "file_name": "synapse/http/client.py", "outdated_file_modify": 1, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "def check_against_blacklist(\n    ip_address: IPAddress, ip_whitelist: Optional[IPSet], ip_blacklist: IPSet\n) -> bool:\n    \"\"\"\n    Compares an IP address to allowed and disallowed IP sets.\n\n    Args:\n        ip_address: The IP address to check\n        ip_whitelist: Allowed IP addresses.\n        ip_blacklist: Disallowed IP addresses.\n\n    Returns:\n        True if the IP address is in the blacklist and not in the whitelist.\n    \"\"\"\n    if ip_address in ip_blacklist:\n        if ip_whitelist is None or ip_address not in ip_whitelist:\n            return True\n    return False", "target": 0}, {"function": "def _make_scheduler(reactor):\n    \"\"\"Makes a schedular suitable for a Cooperator using the given reactor.\n\n    (This is effectively just a copy from `twisted.internet.task`)\n    \"\"\"\n\n    def _scheduler(x):\n        return reactor.callLater(_EPSILON, x)\n\n    return _scheduler", "target": 0}, {"function": "class IPBlacklistingResolver:\n    \"\"\"\n    A proxy for reactor.nameResolver which only produces non-blacklisted IP\n    addresses, preventing DNS rebinding attacks on URL preview.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorPluggableNameResolver,\n        ip_whitelist: Optional[IPSet],\n        ip_blacklist: IPSet,\n    ):\n        \"\"\"\n        Args:\n            reactor: The twisted reactor.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._reactor = reactor\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def resolveHostName(\n        self, recv: IResolutionReceiver, hostname: str, portNumber: int = 0\n    ) -> IResolutionReceiver:\n\n        r = recv()\n        addresses = []  # type: List[IAddress]\n\n        def _callback() -> None:\n            r.resolutionBegan(None)\n\n            has_bad_ip = False\n            for i in addresses:\n                ip_address = IPAddress(i.host)\n\n                if check_against_blacklist(\n                    ip_address, self._ip_whitelist, self._ip_blacklist\n                ):\n                    logger.info(\n                        \"Dropped %s from DNS resolution to %s due to blacklist\"\n                        % (ip_address, hostname)\n                    )\n                    has_bad_ip = True\n\n            # if we have a blacklisted IP, we'd like to raise an error to block the\n            # request, but all we can really do from here is claim that there were no\n            # valid results.\n            if not has_bad_ip:\n                for i in addresses:\n                    r.addressResolved(i)\n            r.resolutionComplete()\n\n        @provider(IResolutionReceiver)\n        class EndpointReceiver:\n            @staticmethod\n            def resolutionBegan(resolutionInProgress: IHostResolution) -> None:\n                pass\n\n            @staticmethod\n            def addressResolved(address: IAddress) -> None:\n                addresses.append(address)\n\n            @staticmethod\n            def resolutionComplete() -> None:\n                _callback()\n\n        self._reactor.nameResolver.resolveHostName(\n            EndpointReceiver, hostname, portNumber=portNumber\n        )\n\n        return r", "target": 0}, {"function": "class BlacklistingAgentWrapper(Agent):\n    \"\"\"\n    An Agent wrapper which will prevent access to IP addresses being accessed\n    directly (without an IP address lookup).\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: IAgent,\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n    ):\n        \"\"\"\n        Args:\n            agent: The Agent to wrap.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._agent = agent\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        h = urllib.parse.urlparse(uri.decode(\"ascii\"))\n\n        try:\n            ip_address = IPAddress(h.hostname)\n\n            if check_against_blacklist(\n                ip_address, self._ip_whitelist, self._ip_blacklist\n            ):\n                logger.info(\"Blocking access to %s due to blacklist\" % (ip_address,))\n                e = SynapseError(403, \"IP address blocked by IP blacklist entry\")\n                return defer.fail(Failure(e))\n        except Exception:\n            # Not an IP\n            pass\n\n        return self._agent.request(\n            method, uri, headers=headers, bodyProducer=bodyProducer\n        )", "target": 0}, {"function": "class SimpleHttpClient:\n    \"\"\"\n    A simple, no-frills HTTP client with methods that wrap up common ways of\n    using HTTP in Matrix\n    \"\"\"\n\n    def __init__(\n        self,\n        hs: \"HomeServer\",\n        treq_args: Dict[str, Any] = {},\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n        http_proxy: Optional[bytes] = None,\n        https_proxy: Optional[bytes] = None,\n    ):\n        \"\"\"\n        Args:\n            hs\n            treq_args: Extra keyword arguments to be given to treq.request.\n            ip_blacklist: The IP addresses that are blacklisted that\n                we may not request.\n            ip_whitelist: The whitelisted IP addresses, that we can\n               request if it were otherwise caught in a blacklist.\n            http_proxy: proxy server to use for http connections. host[:port]\n            https_proxy: proxy server to use for https connections. host[:port]\n        \"\"\"\n        self.hs = hs\n\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n        self._extra_treq_args = treq_args\n\n        self.user_agent = hs.version_string\n        self.clock = hs.get_clock()\n        if hs.config.user_agent_suffix:\n            self.user_agent = \"%s %s\" % (self.user_agent, hs.config.user_agent_suffix)\n\n        # We use this for our body producers to ensure that they use the correct\n        # reactor.\n        self._cooperator = Cooperator(scheduler=_make_scheduler(hs.get_reactor()))\n\n        self.user_agent = self.user_agent.encode(\"ascii\")\n\n        if self._ip_blacklist:\n            real_reactor = hs.get_reactor()\n            # If we have an IP blacklist, we need to use a DNS resolver which\n            # filters out blacklisted IP addresses, to prevent DNS rebinding.\n            nameResolver = IPBlacklistingResolver(\n                real_reactor, self._ip_whitelist, self._ip_blacklist\n            )\n\n            @implementer(IReactorPluggableNameResolver)\n            class Reactor:\n                def __getattr__(_self, attr):\n                    if attr == \"nameResolver\":\n                        return nameResolver\n                    else:\n                        return getattr(real_reactor, attr)\n\n            self.reactor = Reactor()\n        else:\n            self.reactor = hs.get_reactor()\n\n        # the pusher makes lots of concurrent SSL connections to sygnal, and\n        # tends to do so in batches, so we need to allow the pool to keep\n        # lots of idle connections around.\n        pool = HTTPConnectionPool(self.reactor)\n        # XXX: The justification for using the cache factor here is that larger instances\n        # will need both more cache and more connections.\n        # Still, this should probably be a separate dial\n        pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))\n        pool.cachedConnectionTimeout = 2 * 60\n\n        self.agent = ProxyAgent(\n            self.reactor,\n            connectTimeout=15,\n            contextFactory=self.hs.get_http_client_context_factory(),\n            pool=pool,\n            http_proxy=http_proxy,\n            https_proxy=https_proxy,\n        )\n\n        if self._ip_blacklist:\n            # If we have an IP blacklist, we then install the blacklisting Agent\n            # which prevents direct access to IP addresses, that are not caught\n            # by the DNS resolution.\n            self.agent = BlacklistingAgentWrapper(\n                self.agent,\n                ip_whitelist=self._ip_whitelist,\n                ip_blacklist=self._ip_blacklist,\n            )\n\n    async def request(\n        self,\n        method: str,\n        uri: str,\n        data: Optional[bytes] = None,\n        headers: Optional[Headers] = None,\n    ) -> IResponse:\n        \"\"\"\n        Args:\n            method: HTTP method to use.\n            uri: URI to query.\n            data: Data to send in the request body, if applicable.\n            headers: Request headers.\n\n        Returns:\n            Response object, once the headers have been read.\n\n        Raises:\n            RequestTimedOutError if the request times out before the headers are read\n\n        \"\"\"\n        outgoing_requests_counter.labels(method).inc()\n\n        # log request but strip `access_token` (AS requests for example include this)\n        logger.debug(\"Sending request %s %s\", method, redact_uri(uri))\n\n        with start_active_span(\n            \"outgoing-client-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.HTTP_METHOD: method,\n                tags.HTTP_URL: uri,\n            },\n            finish_on_close=True,\n        ):\n            try:\n                body_producer = None\n                if data is not None:\n                    body_producer = QuieterFileBodyProducer(\n                        BytesIO(data), cooperator=self._cooperator,\n                    )\n\n                request_deferred = treq.request(\n                    method,\n                    uri,\n                    agent=self.agent,\n                    data=body_producer,\n                    headers=headers,\n                    **self._extra_treq_args,\n                )  # type: defer.Deferred\n\n                # we use our own timeout mechanism rather than treq's as a workaround\n                # for https://twistedmatrix.com/trac/ticket/9534.\n                request_deferred = timeout_deferred(\n                    request_deferred, 60, self.hs.get_reactor(),\n                )\n\n                # turn timeouts into RequestTimedOutErrors\n                request_deferred.addErrback(_timeout_to_request_timed_out_error)\n\n                response = await make_deferred_yieldable(request_deferred)\n\n                incoming_responses_counter.labels(method, response.code).inc()\n                logger.info(\n                    \"Received response to %s %s: %s\",\n                    method,\n                    redact_uri(uri),\n                    response.code,\n                )\n                return response\n            except Exception as e:\n                incoming_responses_counter.labels(method, \"ERR\").inc()\n                logger.info(\n                    \"Error sending request to  %s %s: %s %s\",\n                    method,\n                    redact_uri(uri),\n                    type(e).__name__,\n                    e.args[0],\n                )\n                set_tag(tags.ERROR, True)\n                set_tag(\"error_reason\", e.args[0])\n                raise\n\n    async def post_urlencoded_get_json(\n        self,\n        uri: str,\n        args: Optional[Mapping[str, Union[str, List[str]]]] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"\n        Args:\n            uri: uri to query\n            args: parameters to be url-encoded in the body\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n\n        # TODO: Do we ever want to log message contents?\n        logger.debug(\"post_urlencoded_get_json args: %s\", args)\n\n        query_bytes = encode_query_args(args)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/x-www-form-urlencoded\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=query_bytes\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def post_json_get_json(\n        self, uri: str, post_json: Any, headers: Optional[RawHeaders] = None\n    ) -> Any:\n        \"\"\"\n\n        Args:\n            uri: URI to query.\n            post_json: request body, to be encoded as json\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        json_str = encode_canonical_json(post_json)\n\n        logger.debug(\"HTTP POST %s -> %s\", json_str, uri)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_json(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"Gets some json from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query string\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        actual_headers = {b\"Accept\": [b\"application/json\"]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        body = await self.get_raw(uri, args, headers=headers)\n        return json_decoder.decode(body.decode(\"utf-8\"))\n\n    async def put_json(\n        self,\n        uri: str,\n        json_body: Any,\n        args: Optional[QueryParams] = None,\n        headers: RawHeaders = None,\n    ) -> Any:\n        \"\"\"Puts some json to the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            json_body: The JSON to put in the HTTP body,\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n             RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        json_str = encode_canonical_json(json_body)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"PUT\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_raw(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> bytes:\n        \"\"\"Gets raw text from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the\n            HTTP body as bytes.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException on a non-2xx HTTP response.\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", uri, headers=Headers(actual_headers))\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return body\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    # XXX: FIXME: This is horribly copy-pasted from matrixfederationclient.\n    # The two should be factored out.\n\n    async def get_file(\n        self,\n        url: str,\n        output_stream: BinaryIO,\n        max_size: Optional[int] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:\n        \"\"\"GETs a file from a given URL\n        Args:\n            url: The URL to GET\n            output_stream: File to write the response body to.\n            headers: A map from header name to a list of values for that header\n        Returns:\n            A tuple of the file length, dict of the response\n            headers, absolute URI of the response and HTTP response code.\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            SynapseError: if the response is not a 2xx, the remote file is too large, or\n               another exception happens during the download.\n        \"\"\"\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", url, headers=Headers(actual_headers))\n\n        resp_headers = dict(response.headers.getAllRawHeaders())\n\n        if (\n            b\"Content-Length\" in resp_headers\n            and max_size\n            and int(resp_headers[b\"Content-Length\"][0]) > max_size\n        ):\n            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))\n            raise SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n\n        if response.code > 299:\n            logger.warning(\"Got %d when downloading %s\" % (response.code, url))\n            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)\n\n        # TODO: if our Content-Type is HTML or something, just read the first\n        # N bytes into RAM rather than saving it all to disk only to read it\n        # straight back in again\n\n        try:\n            length = await make_deferred_yieldable(\n                readBodyToFile(response, output_stream, max_size)\n            )\n        except SynapseError:\n            # This can happen e.g. because the body is too large.\n            raise\n        except Exception as e:\n            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e\n\n        return (\n            length,\n            resp_headers,\n            response.request.absoluteURI.decode(\"ascii\"),\n            response.code,\n        )", "target": 0}, {"function": "def _timeout_to_request_timed_out_error(f: Failure):\n    if f.check(twisted_error.TimeoutError, twisted_error.ConnectingCancelledError):\n        # The TCP connection has its own timeout (set by the 'connectTimeout' param\n        # on the Agent), which raises twisted_error.TimeoutError exception.\n        raise RequestTimedOutError(\"Timeout connecting to remote server\")\n    elif f.check(defer.TimeoutError, ResponseNeverReceived):\n        # this one means that we hit our overall timeout on the request\n        raise RequestTimedOutError(\"Timeout waiting for response from remote server\")\n\n    return f", "target": 0}, {"function": "class _ReadBodyToFileProtocol(protocol.Protocol):\n    def __init__(\n        self, stream: BinaryIO, deferred: defer.Deferred, max_size: Optional[int]\n    ):\n        self.stream = stream\n        self.deferred = deferred\n        self.length = 0\n        self.max_size = max_size\n\n    def dataReceived(self, data: bytes) -> None:\n        self.stream.write(data)\n        self.length += len(data)\n        if self.max_size is not None and self.length >= self.max_size:\n            self.deferred.errback(\n                SynapseError(\n                    502,\n                    \"Requested file is too large > %r bytes\" % (self.max_size,),\n                    Codes.TOO_LARGE,\n                )\n            )\n            self.deferred = defer.Deferred()\n            self.transport.loseConnection()\n\n    def connectionLost(self, reason: Failure) -> None:\n        if reason.check(ResponseDone):\n            self.deferred.callback(self.length)\n        elif reason.check(PotentialDataLoss):\n            # stolen from https://github.com/twisted/treq/pull/49/files\n            # http://twistedmatrix.com/trac/ticket/4840\n            self.deferred.callback(self.length)\n        else:\n            self.deferred.errback(reason)", "target": 0}, {"function": "def readBodyToFile(\n    response: IResponse, stream: BinaryIO, max_size: Optional[int]\n) -> defer.Deferred:\n    \"\"\"\n    Read a HTTP response body to a file-object. Optionally enforcing a maximum file size.\n\n    Args:\n        response: The HTTP response to read from.\n        stream: The file-object to write to.\n        max_size: The maximum file size to allow.\n\n    Returns:\n        A Deferred which resolves to the length of the read body.\n    \"\"\"\n\n    d = defer.Deferred()\n    response.deliverBody(_ReadBodyToFileProtocol(stream, d, max_size))\n    return d", "target": 0}, {"function": "def encode_query_args(args: Optional[Mapping[str, Union[str, List[str]]]]) -> bytes:\n    \"\"\"\n    Encodes a map of query arguments to bytes which can be appended to a URL.\n\n    Args:\n        args: The query arguments, a mapping of string to string or list of strings.\n\n    Returns:\n        The query arguments encoded as bytes.\n    \"\"\"\n    if args is None:\n        return b\"\"\n\n    encoded_args = {}\n    for k, vs in args.items():\n        if isinstance(vs, str):\n            vs = [vs]\n        encoded_args[k] = [v.encode(\"utf8\") for v in vs]\n\n    query_str = urllib.parse.urlencode(encoded_args, True)\n\n    return query_str.encode(\"utf8\")", "target": 0}, {"function": "class InsecureInterceptableContextFactory(ssl.ContextFactory):\n    \"\"\"\n    Factory for PyOpenSSL SSL contexts which accepts any certificate for any domain.\n\n    Do not use this since it allows an attacker to intercept your communications.\n    \"\"\"\n\n    def __init__(self):\n        self._context = SSL.Context(SSL.SSLv23_METHOD)\n        self._context.set_verify(VERIFY_NONE, lambda *_: None)\n\n    def getContext(self, hostname=None, port=None):\n        return self._context\n\n    def creatorForNetloc(self, hostname, port):\n        return self", "target": 0}], "function_after": [{"function": "def check_against_blacklist(\n    ip_address: IPAddress, ip_whitelist: Optional[IPSet], ip_blacklist: IPSet\n) -> bool:\n    \"\"\"\n    Compares an IP address to allowed and disallowed IP sets.\n\n    Args:\n        ip_address: The IP address to check\n        ip_whitelist: Allowed IP addresses.\n        ip_blacklist: Disallowed IP addresses.\n\n    Returns:\n        True if the IP address is in the blacklist and not in the whitelist.\n    \"\"\"\n    if ip_address in ip_blacklist:\n        if ip_whitelist is None or ip_address not in ip_whitelist:\n            return True\n    return False", "target": 0}, {"function": "def _make_scheduler(reactor):\n    \"\"\"Makes a schedular suitable for a Cooperator using the given reactor.\n\n    (This is effectively just a copy from `twisted.internet.task`)\n    \"\"\"\n\n    def _scheduler(x):\n        return reactor.callLater(_EPSILON, x)\n\n    return _scheduler", "target": 0}, {"function": "class _IPBlacklistingResolver:\n    \"\"\"\n    A proxy for reactor.nameResolver which only produces non-blacklisted IP\n    addresses, preventing DNS rebinding attacks on URL preview.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorPluggableNameResolver,\n        ip_whitelist: Optional[IPSet],\n        ip_blacklist: IPSet,\n    ):\n        \"\"\"\n        Args:\n            reactor: The twisted reactor.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._reactor = reactor\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def resolveHostName(\n        self, recv: IResolutionReceiver, hostname: str, portNumber: int = 0\n    ) -> IResolutionReceiver:\n\n        r = recv()\n        addresses = []  # type: List[IAddress]\n\n        def _callback() -> None:\n            r.resolutionBegan(None)\n\n            has_bad_ip = False\n            for i in addresses:\n                ip_address = IPAddress(i.host)\n\n                if check_against_blacklist(\n                    ip_address, self._ip_whitelist, self._ip_blacklist\n                ):\n                    logger.info(\n                        \"Dropped %s from DNS resolution to %s due to blacklist\"\n                        % (ip_address, hostname)\n                    )\n                    has_bad_ip = True\n\n            # if we have a blacklisted IP, we'd like to raise an error to block the\n            # request, but all we can really do from here is claim that there were no\n            # valid results.\n            if not has_bad_ip:\n                for i in addresses:\n                    r.addressResolved(i)\n            r.resolutionComplete()\n\n        @provider(IResolutionReceiver)\n        class EndpointReceiver:\n            @staticmethod\n            def resolutionBegan(resolutionInProgress: IHostResolution) -> None:\n                pass\n\n            @staticmethod\n            def addressResolved(address: IAddress) -> None:\n                addresses.append(address)\n\n            @staticmethod\n            def resolutionComplete() -> None:\n                _callback()\n\n        self._reactor.nameResolver.resolveHostName(\n            EndpointReceiver, hostname, portNumber=portNumber\n        )\n\n        return r", "target": 0}, {"function": "class BlacklistingAgentWrapper(Agent):\n    \"\"\"\n    An Agent wrapper which will prevent access to IP addresses being accessed\n    directly (without an IP address lookup).\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: IAgent,\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n    ):\n        \"\"\"\n        Args:\n            agent: The Agent to wrap.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._agent = agent\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        h = urllib.parse.urlparse(uri.decode(\"ascii\"))\n\n        try:\n            ip_address = IPAddress(h.hostname)\n\n            if check_against_blacklist(\n                ip_address, self._ip_whitelist, self._ip_blacklist\n            ):\n                logger.info(\"Blocking access to %s due to blacklist\" % (ip_address,))\n                e = SynapseError(403, \"IP address blocked by IP blacklist entry\")\n                return defer.fail(Failure(e))\n        except Exception:\n            # Not an IP\n            pass\n\n        return self._agent.request(\n            method, uri, headers=headers, bodyProducer=bodyProducer\n        )", "target": 0}, {"function": "class SimpleHttpClient:\n    \"\"\"\n    A simple, no-frills HTTP client with methods that wrap up common ways of\n    using HTTP in Matrix\n    \"\"\"\n\n    def __init__(\n        self,\n        hs: \"HomeServer\",\n        treq_args: Dict[str, Any] = {},\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n        http_proxy: Optional[bytes] = None,\n        https_proxy: Optional[bytes] = None,\n    ):\n        \"\"\"\n        Args:\n            hs\n            treq_args: Extra keyword arguments to be given to treq.request.\n            ip_blacklist: The IP addresses that are blacklisted that\n                we may not request.\n            ip_whitelist: The whitelisted IP addresses, that we can\n               request if it were otherwise caught in a blacklist.\n            http_proxy: proxy server to use for http connections. host[:port]\n            https_proxy: proxy server to use for https connections. host[:port]\n        \"\"\"\n        self.hs = hs\n\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n        self._extra_treq_args = treq_args\n\n        self.user_agent = hs.version_string\n        self.clock = hs.get_clock()\n        if hs.config.user_agent_suffix:\n            self.user_agent = \"%s %s\" % (self.user_agent, hs.config.user_agent_suffix)\n\n        # We use this for our body producers to ensure that they use the correct\n        # reactor.\n        self._cooperator = Cooperator(scheduler=_make_scheduler(hs.get_reactor()))\n\n        self.user_agent = self.user_agent.encode(\"ascii\")\n\n        if self._ip_blacklist:\n            # If we have an IP blacklist, we need to use a DNS resolver which\n            # filters out blacklisted IP addresses, to prevent DNS rebinding.\n            self.reactor = BlacklistingReactorWrapper(\n                hs.get_reactor(), self._ip_whitelist, self._ip_blacklist\n            )\n        else:\n            self.reactor = hs.get_reactor()\n\n        # the pusher makes lots of concurrent SSL connections to sygnal, and\n        # tends to do so in batches, so we need to allow the pool to keep\n        # lots of idle connections around.\n        pool = HTTPConnectionPool(self.reactor)\n        # XXX: The justification for using the cache factor here is that larger instances\n        # will need both more cache and more connections.\n        # Still, this should probably be a separate dial\n        pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))\n        pool.cachedConnectionTimeout = 2 * 60\n\n        self.agent = ProxyAgent(\n            self.reactor,\n            connectTimeout=15,\n            contextFactory=self.hs.get_http_client_context_factory(),\n            pool=pool,\n            http_proxy=http_proxy,\n            https_proxy=https_proxy,\n        )\n\n        if self._ip_blacklist:\n            # If we have an IP blacklist, we then install the blacklisting Agent\n            # which prevents direct access to IP addresses, that are not caught\n            # by the DNS resolution.\n            self.agent = BlacklistingAgentWrapper(\n                self.agent,\n                ip_whitelist=self._ip_whitelist,\n                ip_blacklist=self._ip_blacklist,\n            )\n\n    async def request(\n        self,\n        method: str,\n        uri: str,\n        data: Optional[bytes] = None,\n        headers: Optional[Headers] = None,\n    ) -> IResponse:\n        \"\"\"\n        Args:\n            method: HTTP method to use.\n            uri: URI to query.\n            data: Data to send in the request body, if applicable.\n            headers: Request headers.\n\n        Returns:\n            Response object, once the headers have been read.\n\n        Raises:\n            RequestTimedOutError if the request times out before the headers are read\n\n        \"\"\"\n        outgoing_requests_counter.labels(method).inc()\n\n        # log request but strip `access_token` (AS requests for example include this)\n        logger.debug(\"Sending request %s %s\", method, redact_uri(uri))\n\n        with start_active_span(\n            \"outgoing-client-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.HTTP_METHOD: method,\n                tags.HTTP_URL: uri,\n            },\n            finish_on_close=True,\n        ):\n            try:\n                body_producer = None\n                if data is not None:\n                    body_producer = QuieterFileBodyProducer(\n                        BytesIO(data), cooperator=self._cooperator,\n                    )\n\n                request_deferred = treq.request(\n                    method,\n                    uri,\n                    agent=self.agent,\n                    data=body_producer,\n                    headers=headers,\n                    **self._extra_treq_args,\n                )  # type: defer.Deferred\n\n                # we use our own timeout mechanism rather than treq's as a workaround\n                # for https://twistedmatrix.com/trac/ticket/9534.\n                request_deferred = timeout_deferred(\n                    request_deferred, 60, self.hs.get_reactor(),\n                )\n\n                # turn timeouts into RequestTimedOutErrors\n                request_deferred.addErrback(_timeout_to_request_timed_out_error)\n\n                response = await make_deferred_yieldable(request_deferred)\n\n                incoming_responses_counter.labels(method, response.code).inc()\n                logger.info(\n                    \"Received response to %s %s: %s\",\n                    method,\n                    redact_uri(uri),\n                    response.code,\n                )\n                return response\n            except Exception as e:\n                incoming_responses_counter.labels(method, \"ERR\").inc()\n                logger.info(\n                    \"Error sending request to  %s %s: %s %s\",\n                    method,\n                    redact_uri(uri),\n                    type(e).__name__,\n                    e.args[0],\n                )\n                set_tag(tags.ERROR, True)\n                set_tag(\"error_reason\", e.args[0])\n                raise\n\n    async def post_urlencoded_get_json(\n        self,\n        uri: str,\n        args: Optional[Mapping[str, Union[str, List[str]]]] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"\n        Args:\n            uri: uri to query\n            args: parameters to be url-encoded in the body\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n\n        # TODO: Do we ever want to log message contents?\n        logger.debug(\"post_urlencoded_get_json args: %s\", args)\n\n        query_bytes = encode_query_args(args)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/x-www-form-urlencoded\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=query_bytes\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def post_json_get_json(\n        self, uri: str, post_json: Any, headers: Optional[RawHeaders] = None\n    ) -> Any:\n        \"\"\"\n\n        Args:\n            uri: URI to query.\n            post_json: request body, to be encoded as json\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        json_str = encode_canonical_json(post_json)\n\n        logger.debug(\"HTTP POST %s -> %s\", json_str, uri)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_json(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"Gets some json from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query string\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        actual_headers = {b\"Accept\": [b\"application/json\"]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        body = await self.get_raw(uri, args, headers=headers)\n        return json_decoder.decode(body.decode(\"utf-8\"))\n\n    async def put_json(\n        self,\n        uri: str,\n        json_body: Any,\n        args: Optional[QueryParams] = None,\n        headers: RawHeaders = None,\n    ) -> Any:\n        \"\"\"Puts some json to the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            json_body: The JSON to put in the HTTP body,\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n             RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        json_str = encode_canonical_json(json_body)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"PUT\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_raw(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> bytes:\n        \"\"\"Gets raw text from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the\n            HTTP body as bytes.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException on a non-2xx HTTP response.\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", uri, headers=Headers(actual_headers))\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return body\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    # XXX: FIXME: This is horribly copy-pasted from matrixfederationclient.\n    # The two should be factored out.\n\n    async def get_file(\n        self,\n        url: str,\n        output_stream: BinaryIO,\n        max_size: Optional[int] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:\n        \"\"\"GETs a file from a given URL\n        Args:\n            url: The URL to GET\n            output_stream: File to write the response body to.\n            headers: A map from header name to a list of values for that header\n        Returns:\n            A tuple of the file length, dict of the response\n            headers, absolute URI of the response and HTTP response code.\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            SynapseError: if the response is not a 2xx, the remote file is too large, or\n               another exception happens during the download.\n        \"\"\"\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", url, headers=Headers(actual_headers))\n\n        resp_headers = dict(response.headers.getAllRawHeaders())\n\n        if (\n            b\"Content-Length\" in resp_headers\n            and max_size\n            and int(resp_headers[b\"Content-Length\"][0]) > max_size\n        ):\n            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))\n            raise SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n\n        if response.code > 299:\n            logger.warning(\"Got %d when downloading %s\" % (response.code, url))\n            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)\n\n        # TODO: if our Content-Type is HTML or something, just read the first\n        # N bytes into RAM rather than saving it all to disk only to read it\n        # straight back in again\n\n        try:\n            length = await make_deferred_yieldable(\n                readBodyToFile(response, output_stream, max_size)\n            )\n        except SynapseError:\n            # This can happen e.g. because the body is too large.\n            raise\n        except Exception as e:\n            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e\n\n        return (\n            length,\n            resp_headers,\n            response.request.absoluteURI.decode(\"ascii\"),\n            response.code,\n        )", "target": 0}, {"function": "def _timeout_to_request_timed_out_error(f: Failure):\n    if f.check(twisted_error.TimeoutError, twisted_error.ConnectingCancelledError):\n        # The TCP connection has its own timeout (set by the 'connectTimeout' param\n        # on the Agent), which raises twisted_error.TimeoutError exception.\n        raise RequestTimedOutError(\"Timeout connecting to remote server\")\n    elif f.check(defer.TimeoutError, ResponseNeverReceived):\n        # this one means that we hit our overall timeout on the request\n        raise RequestTimedOutError(\"Timeout waiting for response from remote server\")\n\n    return f", "target": 0}, {"function": "class _ReadBodyToFileProtocol(protocol.Protocol):\n    def __init__(\n        self, stream: BinaryIO, deferred: defer.Deferred, max_size: Optional[int]\n    ):\n        self.stream = stream\n        self.deferred = deferred\n        self.length = 0\n        self.max_size = max_size\n\n    def dataReceived(self, data: bytes) -> None:\n        self.stream.write(data)\n        self.length += len(data)\n        if self.max_size is not None and self.length >= self.max_size:\n            self.deferred.errback(\n                SynapseError(\n                    502,\n                    \"Requested file is too large > %r bytes\" % (self.max_size,),\n                    Codes.TOO_LARGE,\n                )\n            )\n            self.deferred = defer.Deferred()\n            self.transport.loseConnection()\n\n    def connectionLost(self, reason: Failure) -> None:\n        if reason.check(ResponseDone):\n            self.deferred.callback(self.length)\n        elif reason.check(PotentialDataLoss):\n            # stolen from https://github.com/twisted/treq/pull/49/files\n            # http://twistedmatrix.com/trac/ticket/4840\n            self.deferred.callback(self.length)\n        else:\n            self.deferred.errback(reason)", "target": 0}, {"function": "def readBodyToFile(\n    response: IResponse, stream: BinaryIO, max_size: Optional[int]\n) -> defer.Deferred:\n    \"\"\"\n    Read a HTTP response body to a file-object. Optionally enforcing a maximum file size.\n\n    Args:\n        response: The HTTP response to read from.\n        stream: The file-object to write to.\n        max_size: The maximum file size to allow.\n\n    Returns:\n        A Deferred which resolves to the length of the read body.\n    \"\"\"\n\n    d = defer.Deferred()\n    response.deliverBody(_ReadBodyToFileProtocol(stream, d, max_size))\n    return d", "target": 0}, {"function": "def encode_query_args(args: Optional[Mapping[str, Union[str, List[str]]]]) -> bytes:\n    \"\"\"\n    Encodes a map of query arguments to bytes which can be appended to a URL.\n\n    Args:\n        args: The query arguments, a mapping of string to string or list of strings.\n\n    Returns:\n        The query arguments encoded as bytes.\n    \"\"\"\n    if args is None:\n        return b\"\"\n\n    encoded_args = {}\n    for k, vs in args.items():\n        if isinstance(vs, str):\n            vs = [vs]\n        encoded_args[k] = [v.encode(\"utf8\") for v in vs]\n\n    query_str = urllib.parse.urlencode(encoded_args, True)\n\n    return query_str.encode(\"utf8\")", "target": 0}, {"function": "class InsecureInterceptableContextFactory(ssl.ContextFactory):\n    \"\"\"\n    Factory for PyOpenSSL SSL contexts which accepts any certificate for any domain.\n\n    Do not use this since it allows an attacker to intercept your communications.\n    \"\"\"\n\n    def __init__(self):\n        self._context = SSL.Context(SSL.SSLv23_METHOD)\n        self._context.set_verify(VERIFY_NONE, lambda *_: None)\n\n    def getContext(self, hostname=None, port=None):\n        return self._context\n\n    def creatorForNetloc(self, hostname, port):\n        return self", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Fhttp%2Ffederation%2Fmatrix_federation_agent.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2019 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport urllib.parse\nfrom typing import List, Optional\n\nfrom netaddr import AddrFormatError, IPAddress, IPSet\nfrom zope.interface import implementer\n\nfrom twisted.internet import defer\nfrom twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS\nfrom twisted.internet.interfaces import (\n    IProtocolFactory,\n    IReactorCore,\n    IStreamClientEndpoint,\n)\nfrom twisted.web.client import URI, Agent, HTTPConnectionPool\nfrom twisted.web.http_headers import Headers\nfrom twisted.web.iweb import IAgent, IAgentEndpointFactory, IBodyProducer\n\nfrom synapse.crypto.context_factory import FederationPolicyForHTTPS\nfrom synapse.http.client import BlacklistingAgentWrapper\nfrom synapse.http.federation.srv_resolver import Server, SrvResolver\nfrom synapse.http.federation.well_known_resolver import WellKnownResolver\nfrom synapse.logging.context import make_deferred_yieldable, run_in_background\nfrom synapse.util import Clock\n\nlogger = logging.getLogger(__name__)\n\n\n@implementer(IAgent)\nclass MatrixFederationAgent:\n    \"\"\"An Agent-like thing which provides a `request` method which correctly\n    handles resolving matrix server names when using matrix://. Handles standard\n    https URIs as normal.\n\n    Doesn't implement any retries. (Those are done in MatrixFederationHttpClient.)\n\n    Args:\n        reactor: twisted reactor to use for underlying requests\n\n        tls_client_options_factory:\n            factory to use for fetching client tls options, or none to disable TLS.\n\n        user_agent:\n            The user agent header to use for federation requests.\n\n        _srv_resolver:\n            SrvResolver implementation to use for looking up SRV records. None\n            to use a default implementation.\n\n        _well_known_resolver:\n            WellKnownResolver to use to perform well-known lookups. None to use a\n            default implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorCore,\n        tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n        user_agent: bytes,\n        ip_blacklist: IPSet,\n        _srv_resolver: Optional[SrvResolver] = None,\n        _well_known_resolver: Optional[WellKnownResolver] = None,\n    ):\n        self._reactor = reactor\n        self._clock = Clock(reactor)\n        self._pool = HTTPConnectionPool(reactor)\n        self._pool.retryAutomatically = False\n        self._pool.maxPersistentPerHost = 5\n        self._pool.cachedConnectionTimeout = 2 * 60\n\n        self._agent = Agent.usingEndpointFactory(\n            self._reactor,\n            MatrixHostnameEndpointFactory(\n                reactor, tls_client_options_factory, _srv_resolver\n            ),\n            pool=self._pool,\n        )\n        self.user_agent = user_agent\n\n        if _well_known_resolver is None:\n            # Note that the name resolver has already been wrapped in a\n            # IPBlacklistingResolver by MatrixFederationHttpClient.\n            _well_known_resolver = WellKnownResolver(\n                self._reactor,\n                agent=BlacklistingAgentWrapper(\n                    Agent(\n                        self._reactor,\n                        pool=self._pool,\n                        contextFactory=tls_client_options_factory,\n                    ),\n                    self._reactor,\n                    ip_blacklist=ip_blacklist,\n                ),\n                user_agent=self.user_agent,\n            )\n\n        self._well_known_resolver = _well_known_resolver\n\n    @defer.inlineCallbacks\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        \"\"\"\n        Args:\n            method: HTTP method: GET/POST/etc\n            uri: Absolute URI to be retrieved\n            headers:\n                HTTP headers to send with the request, or None to send no extra headers.\n            bodyProducer:\n                An object which can generate bytes to make up the\n                body of this request (for example, the properly encoded contents of\n                a file for a file upload).  Or None if the request is to have\n                no body.\n        Returns:\n            Deferred[twisted.web.iweb.IResponse]:\n                fires when the header of the response has been received (regardless of the\n                response status code). Fails if there is any problem which prevents that\n                response from being received (including problems that prevent the request\n                from being sent).\n        \"\"\"\n        # We use urlparse as that will set `port` to None if there is no\n        # explicit port.\n        parsed_uri = urllib.parse.urlparse(uri)\n\n        # There must be a valid hostname.\n        assert parsed_uri.hostname\n\n        # If this is a matrix:// URI check if the server has delegated matrix\n        # traffic using well-known delegation.\n        #\n        # We have to do this here and not in the endpoint as we need to rewrite\n        # the host header with the delegated server name.\n        delegated_server = None\n        if (\n            parsed_uri.scheme == b\"matrix\"\n            and not _is_ip_literal(parsed_uri.hostname)\n            and not parsed_uri.port\n        ):\n            well_known_result = yield defer.ensureDeferred(\n                self._well_known_resolver.get_well_known(parsed_uri.hostname)\n            )\n            delegated_server = well_known_result.delegated_server\n\n        if delegated_server:\n            # Ok, the server has delegated matrix traffic to somewhere else, so\n            # lets rewrite the URL to replace the server with the delegated\n            # server name.\n            uri = urllib.parse.urlunparse(\n                (\n                    parsed_uri.scheme,\n                    delegated_server,\n                    parsed_uri.path,\n                    parsed_uri.params,\n                    parsed_uri.query,\n                    parsed_uri.fragment,\n                )\n            )\n            parsed_uri = urllib.parse.urlparse(uri)\n\n        # We need to make sure the host header is set to the netloc of the\n        # server and that a user-agent is provided.\n        if headers is None:\n            headers = Headers()\n        else:\n            headers = headers.copy()\n\n        if not headers.hasHeader(b\"host\"):\n            headers.addRawHeader(b\"host\", parsed_uri.netloc)\n        if not headers.hasHeader(b\"user-agent\"):\n            headers.addRawHeader(b\"user-agent\", self.user_agent)\n\n        res = yield make_deferred_yieldable(\n            self._agent.request(method, uri, headers, bodyProducer)\n        )\n\n        return res\n\n\n@implementer(IAgentEndpointFactory)\nclass MatrixHostnameEndpointFactory:\n    \"\"\"Factory for MatrixHostnameEndpoint for parsing to an Agent.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorCore,\n        tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n        srv_resolver: Optional[SrvResolver],\n    ):\n        self._reactor = reactor\n        self._tls_client_options_factory = tls_client_options_factory\n\n        if srv_resolver is None:\n            srv_resolver = SrvResolver()\n\n        self._srv_resolver = srv_resolver\n\n    def endpointForURI(self, parsed_uri):\n        return MatrixHostnameEndpoint(\n            self._reactor,\n            self._tls_client_options_factory,\n            self._srv_resolver,\n            parsed_uri,\n        )\n\n\n@implementer(IStreamClientEndpoint)\nclass MatrixHostnameEndpoint:\n    \"\"\"An endpoint that resolves matrix:// URLs using Matrix server name\n    resolution (i.e. via SRV). Does not check for well-known delegation.\n\n    Args:\n        reactor: twisted reactor to use for underlying requests\n        tls_client_options_factory:\n            factory to use for fetching client tls options, or none to disable TLS.\n        srv_resolver: The SRV resolver to use\n        parsed_uri: The parsed URI that we're wanting to connect to.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorCore,\n        tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n        srv_resolver: SrvResolver,\n        parsed_uri: URI,\n    ):\n        self._reactor = reactor\n\n        self._parsed_uri = parsed_uri\n\n        # set up the TLS connection params\n        #\n        # XXX disabling TLS is really only supported here for the benefit of the\n        # unit tests. We should make the UTs cope with TLS rather than having to make\n        # the code support the unit tests.\n\n        if tls_client_options_factory is None:\n            self._tls_options = None\n        else:\n            self._tls_options = tls_client_options_factory.get_options(\n                self._parsed_uri.host\n            )\n\n        self._srv_resolver = srv_resolver\n\n    def connect(self, protocol_factory: IProtocolFactory) -> defer.Deferred:\n        \"\"\"Implements IStreamClientEndpoint interface\n        \"\"\"\n\n        return run_in_background(self._do_connect, protocol_factory)\n\n    async def _do_connect(self, protocol_factory: IProtocolFactory) -> None:\n        first_exception = None\n\n        server_list = await self._resolve_server()\n\n        for server in server_list:\n            host = server.host\n            port = server.port\n\n            try:\n                logger.debug(\"Connecting to %s:%i\", host.decode(\"ascii\"), port)\n                endpoint = HostnameEndpoint(self._reactor, host, port)\n                if self._tls_options:\n                    endpoint = wrapClientTLS(self._tls_options, endpoint)\n                result = await make_deferred_yieldable(\n                    endpoint.connect(protocol_factory)\n                )\n\n                return result\n            except Exception as e:\n                logger.info(\n                    \"Failed to connect to %s:%i: %s\", host.decode(\"ascii\"), port, e\n                )\n                if not first_exception:\n                    first_exception = e\n\n        # We return the first failure because that's probably the most interesting.\n        if first_exception:\n            raise first_exception\n\n        # This shouldn't happen as we should always have at least one host/port\n        # to try and if that doesn't work then we'll have an exception.\n        raise Exception(\"Failed to resolve server %r\" % (self._parsed_uri.netloc,))\n\n    async def _resolve_server(self) -> List[Server]:\n        \"\"\"Resolves the server name to a list of hosts and ports to attempt to\n        connect to.\n        \"\"\"\n\n        if self._parsed_uri.scheme != b\"matrix\":\n            return [Server(host=self._parsed_uri.host, port=self._parsed_uri.port)]\n\n        # Note: We don't do well-known lookup as that needs to have happened\n        # before now, due to needing to rewrite the Host header of the HTTP\n        # request.\n\n        # We reparse the URI so that defaultPort is -1 rather than 80\n        parsed_uri = urllib.parse.urlparse(self._parsed_uri.toBytes())\n\n        host = parsed_uri.hostname\n        port = parsed_uri.port\n\n        # If there is an explicit port or the host is an IP address we bypass\n        # SRV lookups and just use the given host/port.\n        if port or _is_ip_literal(host):\n            return [Server(host, port or 8448)]\n\n        server_list = await self._srv_resolver.resolve_service(b\"_matrix._tcp.\" + host)\n\n        if server_list:\n            return server_list\n\n        # No SRV records, so we fallback to host and 8448\n        return [Server(host, 8448)]\n\n\ndef _is_ip_literal(host: bytes) -> bool:\n    \"\"\"Test if the given host name is either an IPv4 or IPv6 literal.\n\n    Args:\n        host: The host name to check\n\n    Returns:\n        True if the hostname is an IP address literal.\n    \"\"\"\n\n    host_str = host.decode(\"ascii\")\n\n    try:\n        IPAddress(host_str)\n        return True\n    except AddrFormatError:\n        return False\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2019 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport urllib.parse\nfrom typing import List, Optional\n\nfrom netaddr import AddrFormatError, IPAddress\nfrom zope.interface import implementer\n\nfrom twisted.internet import defer\nfrom twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS\nfrom twisted.internet.interfaces import (\n    IProtocolFactory,\n    IReactorCore,\n    IStreamClientEndpoint,\n)\nfrom twisted.web.client import URI, Agent, HTTPConnectionPool\nfrom twisted.web.http_headers import Headers\nfrom twisted.web.iweb import IAgent, IAgentEndpointFactory, IBodyProducer\n\nfrom synapse.crypto.context_factory import FederationPolicyForHTTPS\nfrom synapse.http.federation.srv_resolver import Server, SrvResolver\nfrom synapse.http.federation.well_known_resolver import WellKnownResolver\nfrom synapse.logging.context import make_deferred_yieldable, run_in_background\nfrom synapse.util import Clock\n\nlogger = logging.getLogger(__name__)\n\n\n@implementer(IAgent)\nclass MatrixFederationAgent:\n    \"\"\"An Agent-like thing which provides a `request` method which correctly\n    handles resolving matrix server names when using matrix://. Handles standard\n    https URIs as normal.\n\n    Doesn't implement any retries. (Those are done in MatrixFederationHttpClient.)\n\n    Args:\n        reactor: twisted reactor to use for underlying requests\n\n        tls_client_options_factory:\n            factory to use for fetching client tls options, or none to disable TLS.\n\n        user_agent:\n            The user agent header to use for federation requests.\n\n        _srv_resolver:\n            SrvResolver implementation to use for looking up SRV records. None\n            to use a default implementation.\n\n        _well_known_resolver:\n            WellKnownResolver to use to perform well-known lookups. None to use a\n            default implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorCore,\n        tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n        user_agent: bytes,\n        _srv_resolver: Optional[SrvResolver] = None,\n        _well_known_resolver: Optional[WellKnownResolver] = None,\n    ):\n        self._reactor = reactor\n        self._clock = Clock(reactor)\n        self._pool = HTTPConnectionPool(reactor)\n        self._pool.retryAutomatically = False\n        self._pool.maxPersistentPerHost = 5\n        self._pool.cachedConnectionTimeout = 2 * 60\n\n        self._agent = Agent.usingEndpointFactory(\n            self._reactor,\n            MatrixHostnameEndpointFactory(\n                reactor, tls_client_options_factory, _srv_resolver\n            ),\n            pool=self._pool,\n        )\n        self.user_agent = user_agent\n\n        if _well_known_resolver is None:\n            _well_known_resolver = WellKnownResolver(\n                self._reactor,\n                agent=Agent(\n                    self._reactor,\n                    pool=self._pool,\n                    contextFactory=tls_client_options_factory,\n                ),\n                user_agent=self.user_agent,\n            )\n\n        self._well_known_resolver = _well_known_resolver\n\n    @defer.inlineCallbacks\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        \"\"\"\n        Args:\n            method: HTTP method: GET/POST/etc\n            uri: Absolute URI to be retrieved\n            headers:\n                HTTP headers to send with the request, or None to send no extra headers.\n            bodyProducer:\n                An object which can generate bytes to make up the\n                body of this request (for example, the properly encoded contents of\n                a file for a file upload).  Or None if the request is to have\n                no body.\n        Returns:\n            Deferred[twisted.web.iweb.IResponse]:\n                fires when the header of the response has been received (regardless of the\n                response status code). Fails if there is any problem which prevents that\n                response from being received (including problems that prevent the request\n                from being sent).\n        \"\"\"\n        # We use urlparse as that will set `port` to None if there is no\n        # explicit port.\n        parsed_uri = urllib.parse.urlparse(uri)\n\n        # There must be a valid hostname.\n        assert parsed_uri.hostname\n\n        # If this is a matrix:// URI check if the server has delegated matrix\n        # traffic using well-known delegation.\n        #\n        # We have to do this here and not in the endpoint as we need to rewrite\n        # the host header with the delegated server name.\n        delegated_server = None\n        if (\n            parsed_uri.scheme == b\"matrix\"\n            and not _is_ip_literal(parsed_uri.hostname)\n            and not parsed_uri.port\n        ):\n            well_known_result = yield defer.ensureDeferred(\n                self._well_known_resolver.get_well_known(parsed_uri.hostname)\n            )\n            delegated_server = well_known_result.delegated_server\n\n        if delegated_server:\n            # Ok, the server has delegated matrix traffic to somewhere else, so\n            # lets rewrite the URL to replace the server with the delegated\n            # server name.\n            uri = urllib.parse.urlunparse(\n                (\n                    parsed_uri.scheme,\n                    delegated_server,\n                    parsed_uri.path,\n                    parsed_uri.params,\n                    parsed_uri.query,\n                    parsed_uri.fragment,\n                )\n            )\n            parsed_uri = urllib.parse.urlparse(uri)\n\n        # We need to make sure the host header is set to the netloc of the\n        # server and that a user-agent is provided.\n        if headers is None:\n            headers = Headers()\n        else:\n            headers = headers.copy()\n\n        if not headers.hasHeader(b\"host\"):\n            headers.addRawHeader(b\"host\", parsed_uri.netloc)\n        if not headers.hasHeader(b\"user-agent\"):\n            headers.addRawHeader(b\"user-agent\", self.user_agent)\n\n        res = yield make_deferred_yieldable(\n            self._agent.request(method, uri, headers, bodyProducer)\n        )\n\n        return res\n\n\n@implementer(IAgentEndpointFactory)\nclass MatrixHostnameEndpointFactory:\n    \"\"\"Factory for MatrixHostnameEndpoint for parsing to an Agent.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorCore,\n        tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n        srv_resolver: Optional[SrvResolver],\n    ):\n        self._reactor = reactor\n        self._tls_client_options_factory = tls_client_options_factory\n\n        if srv_resolver is None:\n            srv_resolver = SrvResolver()\n\n        self._srv_resolver = srv_resolver\n\n    def endpointForURI(self, parsed_uri):\n        return MatrixHostnameEndpoint(\n            self._reactor,\n            self._tls_client_options_factory,\n            self._srv_resolver,\n            parsed_uri,\n        )\n\n\n@implementer(IStreamClientEndpoint)\nclass MatrixHostnameEndpoint:\n    \"\"\"An endpoint that resolves matrix:// URLs using Matrix server name\n    resolution (i.e. via SRV). Does not check for well-known delegation.\n\n    Args:\n        reactor: twisted reactor to use for underlying requests\n        tls_client_options_factory:\n            factory to use for fetching client tls options, or none to disable TLS.\n        srv_resolver: The SRV resolver to use\n        parsed_uri: The parsed URI that we're wanting to connect to.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorCore,\n        tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n        srv_resolver: SrvResolver,\n        parsed_uri: URI,\n    ):\n        self._reactor = reactor\n\n        self._parsed_uri = parsed_uri\n\n        # set up the TLS connection params\n        #\n        # XXX disabling TLS is really only supported here for the benefit of the\n        # unit tests. We should make the UTs cope with TLS rather than having to make\n        # the code support the unit tests.\n\n        if tls_client_options_factory is None:\n            self._tls_options = None\n        else:\n            self._tls_options = tls_client_options_factory.get_options(\n                self._parsed_uri.host\n            )\n\n        self._srv_resolver = srv_resolver\n\n    def connect(self, protocol_factory: IProtocolFactory) -> defer.Deferred:\n        \"\"\"Implements IStreamClientEndpoint interface\n        \"\"\"\n\n        return run_in_background(self._do_connect, protocol_factory)\n\n    async def _do_connect(self, protocol_factory: IProtocolFactory) -> None:\n        first_exception = None\n\n        server_list = await self._resolve_server()\n\n        for server in server_list:\n            host = server.host\n            port = server.port\n\n            try:\n                logger.debug(\"Connecting to %s:%i\", host.decode(\"ascii\"), port)\n                endpoint = HostnameEndpoint(self._reactor, host, port)\n                if self._tls_options:\n                    endpoint = wrapClientTLS(self._tls_options, endpoint)\n                result = await make_deferred_yieldable(\n                    endpoint.connect(protocol_factory)\n                )\n\n                return result\n            except Exception as e:\n                logger.info(\n                    \"Failed to connect to %s:%i: %s\", host.decode(\"ascii\"), port, e\n                )\n                if not first_exception:\n                    first_exception = e\n\n        # We return the first failure because that's probably the most interesting.\n        if first_exception:\n            raise first_exception\n\n        # This shouldn't happen as we should always have at least one host/port\n        # to try and if that doesn't work then we'll have an exception.\n        raise Exception(\"Failed to resolve server %r\" % (self._parsed_uri.netloc,))\n\n    async def _resolve_server(self) -> List[Server]:\n        \"\"\"Resolves the server name to a list of hosts and ports to attempt to\n        connect to.\n        \"\"\"\n\n        if self._parsed_uri.scheme != b\"matrix\":\n            return [Server(host=self._parsed_uri.host, port=self._parsed_uri.port)]\n\n        # Note: We don't do well-known lookup as that needs to have happened\n        # before now, due to needing to rewrite the Host header of the HTTP\n        # request.\n\n        # We reparse the URI so that defaultPort is -1 rather than 80\n        parsed_uri = urllib.parse.urlparse(self._parsed_uri.toBytes())\n\n        host = parsed_uri.hostname\n        port = parsed_uri.port\n\n        # If there is an explicit port or the host is an IP address we bypass\n        # SRV lookups and just use the given host/port.\n        if port or _is_ip_literal(host):\n            return [Server(host, port or 8448)]\n\n        server_list = await self._srv_resolver.resolve_service(b\"_matrix._tcp.\" + host)\n\n        if server_list:\n            return server_list\n\n        # No SRV records, so we fallback to host and 8448\n        return [Server(host, 8448)]\n\n\ndef _is_ip_literal(host: bytes) -> bool:\n    \"\"\"Test if the given host name is either an IPv4 or IPv6 literal.\n\n    Args:\n        host: The host name to check\n\n    Returns:\n        True if the hostname is an IP address literal.\n    \"\"\"\n\n    host_str = host.decode(\"ascii\")\n\n    try:\n        IPAddress(host_str)\n        return True\n    except AddrFormatError:\n        return False\n", "patch": "@@ -16,7 +16,7 @@\n import urllib.parse\n from typing import List, Optional\n \n-from netaddr import AddrFormatError, IPAddress\n+from netaddr import AddrFormatError, IPAddress, IPSet\n from zope.interface import implementer\n \n from twisted.internet import defer\n@@ -31,6 +31,7 @@\n from twisted.web.iweb import IAgent, IAgentEndpointFactory, IBodyProducer\n \n from synapse.crypto.context_factory import FederationPolicyForHTTPS\n+from synapse.http.client import BlacklistingAgentWrapper\n from synapse.http.federation.srv_resolver import Server, SrvResolver\n from synapse.http.federation.well_known_resolver import WellKnownResolver\n from synapse.logging.context import make_deferred_yieldable, run_in_background\n@@ -70,6 +71,7 @@ def __init__(\n         reactor: IReactorCore,\n         tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n         user_agent: bytes,\n+        ip_blacklist: IPSet,\n         _srv_resolver: Optional[SrvResolver] = None,\n         _well_known_resolver: Optional[WellKnownResolver] = None,\n     ):\n@@ -90,12 +92,18 @@ def __init__(\n         self.user_agent = user_agent\n \n         if _well_known_resolver is None:\n+            # Note that the name resolver has already been wrapped in a\n+            # IPBlacklistingResolver by MatrixFederationHttpClient.\n             _well_known_resolver = WellKnownResolver(\n                 self._reactor,\n-                agent=Agent(\n+                agent=BlacklistingAgentWrapper(\n+                    Agent(\n+                        self._reactor,\n+                        pool=self._pool,\n+                        contextFactory=tls_client_options_factory,\n+                    ),\n                     self._reactor,\n-                    pool=self._pool,\n-                    contextFactory=tls_client_options_factory,\n+                    ip_blacklist=ip_blacklist,\n                 ),\n                 user_agent=self.user_agent,\n             )", "file_path": "files/2021_2/25", "file_language": "py", "file_name": "synapse/http/federation/matrix_federation_agent.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Fhttp%2Fmatrixfederationclient.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport cgi\nimport logging\nimport random\nimport sys\nimport urllib.parse\nfrom io import BytesIO\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\n\nimport attr\nimport treq\nfrom canonicaljson import encode_canonical_json\nfrom prometheus_client import Counter\nfrom signedjson.sign import sign_json\n\nfrom twisted.internet import defer\nfrom twisted.internet.error import DNSLookupError\nfrom twisted.internet.interfaces import IReactorTime\nfrom twisted.internet.task import _EPSILON, Cooperator\nfrom twisted.web.http_headers import Headers\nfrom twisted.web.iweb import IBodyProducer, IResponse\n\nimport synapse.metrics\nimport synapse.util.retryutils\nfrom synapse.api.errors import (\n    FederationDeniedError,\n    HttpResponseException,\n    RequestSendFailed,\n)\nfrom synapse.http import QuieterFileBodyProducer\nfrom synapse.http.client import (\n    BlacklistingAgentWrapper,\n    BlacklistingReactorWrapper,\n    encode_query_args,\n    readBodyToFile,\n)\nfrom synapse.http.federation.matrix_federation_agent import MatrixFederationAgent\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.logging.opentracing import (\n    inject_active_span_byte_dict,\n    set_tag,\n    start_active_span,\n    tags,\n)\nfrom synapse.types import JsonDict\nfrom synapse.util import json_decoder\nfrom synapse.util.async_helpers import timeout_deferred\nfrom synapse.util.metrics import Measure\n\nlogger = logging.getLogger(__name__)\n\noutgoing_requests_counter = Counter(\n    \"synapse_http_matrixfederationclient_requests\", \"\", [\"method\"]\n)\nincoming_responses_counter = Counter(\n    \"synapse_http_matrixfederationclient_responses\", \"\", [\"method\", \"code\"]\n)\n\n\nMAX_LONG_RETRIES = 10\nMAX_SHORT_RETRIES = 3\nMAXINT = sys.maxsize\n\n\n_next_id = 1\n\n\nQueryArgs = Dict[str, Union[str, List[str]]]\n\n\n@attr.s(slots=True, frozen=True)\nclass MatrixFederationRequest:\n    method = attr.ib(type=str)\n    \"\"\"HTTP method\n    \"\"\"\n\n    path = attr.ib(type=str)\n    \"\"\"HTTP path\n    \"\"\"\n\n    destination = attr.ib(type=str)\n    \"\"\"The remote server to send the HTTP request to.\n    \"\"\"\n\n    json = attr.ib(default=None, type=Optional[JsonDict])\n    \"\"\"JSON to send in the body.\n    \"\"\"\n\n    json_callback = attr.ib(default=None, type=Optional[Callable[[], JsonDict]])\n    \"\"\"A callback to generate the JSON.\n    \"\"\"\n\n    query = attr.ib(default=None, type=Optional[dict])\n    \"\"\"Query arguments.\n    \"\"\"\n\n    txn_id = attr.ib(default=None, type=Optional[str])\n    \"\"\"Unique ID for this request (for logging)\n    \"\"\"\n\n    uri = attr.ib(init=False, type=bytes)\n    \"\"\"The URI of this request\n    \"\"\"\n\n    def __attrs_post_init__(self) -> None:\n        global _next_id\n        txn_id = \"%s-O-%s\" % (self.method, _next_id)\n        _next_id = (_next_id + 1) % (MAXINT - 1)\n\n        object.__setattr__(self, \"txn_id\", txn_id)\n\n        destination_bytes = self.destination.encode(\"ascii\")\n        path_bytes = self.path.encode(\"ascii\")\n        if self.query:\n            query_bytes = encode_query_args(self.query)\n        else:\n            query_bytes = b\"\"\n\n        # The object is frozen so we can pre-compute this.\n        uri = urllib.parse.urlunparse(\n            (b\"matrix\", destination_bytes, path_bytes, None, query_bytes, b\"\")\n        )\n        object.__setattr__(self, \"uri\", uri)\n\n    def get_json(self) -> Optional[JsonDict]:\n        if self.json_callback:\n            return self.json_callback()\n        return self.json\n\n\nasync def _handle_json_response(\n    reactor: IReactorTime,\n    timeout_sec: float,\n    request: MatrixFederationRequest,\n    response: IResponse,\n    start_ms: int,\n) -> JsonDict:\n    \"\"\"\n    Reads the JSON body of a response, with a timeout\n\n    Args:\n        reactor: twisted reactor, for the timeout\n        timeout_sec: number of seconds to wait for response to complete\n        request: the request that triggered the response\n        response: response to the request\n        start_ms: Timestamp when request was made\n\n    Returns:\n        The parsed JSON response\n    \"\"\"\n    try:\n        check_content_type_is_json(response.headers)\n\n        # Use the custom JSON decoder (partially re-implements treq.json_content).\n        d = treq.text_content(response, encoding=\"utf-8\")\n        d.addCallback(json_decoder.decode)\n        d = timeout_deferred(d, timeout=timeout_sec, reactor=reactor)\n\n        body = await make_deferred_yieldable(d)\n    except defer.TimeoutError as e:\n        logger.warning(\n            \"{%s} [%s] Timed out reading response - %s %s\",\n            request.txn_id,\n            request.destination,\n            request.method,\n            request.uri.decode(\"ascii\"),\n        )\n        raise RequestSendFailed(e, can_retry=True) from e\n    except Exception as e:\n        logger.warning(\n            \"{%s} [%s] Error reading response %s %s: %s\",\n            request.txn_id,\n            request.destination,\n            request.method,\n            request.uri.decode(\"ascii\"),\n            e,\n        )\n        raise\n\n    time_taken_secs = reactor.seconds() - start_ms / 1000\n\n    logger.info(\n        \"{%s} [%s] Completed request: %d %s in %.2f secs - %s %s\",\n        request.txn_id,\n        request.destination,\n        response.code,\n        response.phrase.decode(\"ascii\", errors=\"replace\"),\n        time_taken_secs,\n        request.method,\n        request.uri.decode(\"ascii\"),\n    )\n    return body\n\n\nclass MatrixFederationHttpClient:\n    \"\"\"HTTP client used to talk to other homeservers over the federation\n    protocol. Send client certificates and signs requests.\n\n    Attributes:\n        agent (twisted.web.client.Agent): The twisted Agent used to send the\n            requests.\n    \"\"\"\n\n    def __init__(self, hs, tls_client_options_factory):\n        self.hs = hs\n        self.signing_key = hs.signing_key\n        self.server_name = hs.hostname\n\n        # We need to use a DNS resolver which filters out blacklisted IP\n        # addresses, to prevent DNS rebinding.\n        self.reactor = BlacklistingReactorWrapper(\n            hs.get_reactor(), None, hs.config.federation_ip_range_blacklist\n        )\n\n        user_agent = hs.version_string\n        if hs.config.user_agent_suffix:\n            user_agent = \"%s %s\" % (user_agent, hs.config.user_agent_suffix)\n        user_agent = user_agent.encode(\"ascii\")\n\n        self.agent = MatrixFederationAgent(\n            self.reactor,\n            tls_client_options_factory,\n            user_agent,\n            hs.config.federation_ip_range_blacklist,\n        )\n\n        # Use a BlacklistingAgentWrapper to prevent circumventing the IP\n        # blacklist via IP literals in server names\n        self.agent = BlacklistingAgentWrapper(\n            self.agent, ip_blacklist=hs.config.federation_ip_range_blacklist,\n        )\n\n        self.clock = hs.get_clock()\n        self._store = hs.get_datastore()\n        self.version_string_bytes = hs.version_string.encode(\"ascii\")\n        self.default_timeout = 60\n\n        def schedule(x):\n            self.reactor.callLater(_EPSILON, x)\n\n        self._cooperator = Cooperator(scheduler=schedule)\n\n    async def _send_request_with_optional_trailing_slash(\n        self,\n        request: MatrixFederationRequest,\n        try_trailing_slash_on_400: bool = False,\n        **send_request_args\n    ) -> IResponse:\n        \"\"\"Wrapper for _send_request which can optionally retry the request\n        upon receiving a combination of a 400 HTTP response code and a\n        'M_UNRECOGNIZED' errcode. This is a workaround for Synapse <= v0.99.3\n        due to #3622.\n\n        Args:\n            request: details of request to be sent\n            try_trailing_slash_on_400: Whether on receiving a 400\n                'M_UNRECOGNIZED' from the server to retry the request with a\n                trailing slash appended to the request path.\n            send_request_args: A dictionary of arguments to pass to `_send_request()`.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n\n        Returns:\n            Parsed JSON response body.\n        \"\"\"\n        try:\n            response = await self._send_request(request, **send_request_args)\n        except HttpResponseException as e:\n            # Received an HTTP error > 300. Check if it meets the requirements\n            # to retry with a trailing slash\n            if not try_trailing_slash_on_400:\n                raise\n\n            if e.code != 400 or e.to_synapse_error().errcode != \"M_UNRECOGNIZED\":\n                raise\n\n            # Retry with a trailing slash if we received a 400 with\n            # 'M_UNRECOGNIZED' which some endpoints can return when omitting a\n            # trailing slash on Synapse <= v0.99.3.\n            logger.info(\"Retrying request with trailing slash\")\n\n            # Request is frozen so we create a new instance\n            request = attr.evolve(request, path=request.path + \"/\")\n\n            response = await self._send_request(request, **send_request_args)\n\n        return response\n\n    async def _send_request(\n        self,\n        request: MatrixFederationRequest,\n        retry_on_dns_fail: bool = True,\n        timeout: Optional[int] = None,\n        long_retries: bool = False,\n        ignore_backoff: bool = False,\n        backoff_on_404: bool = False,\n    ) -> IResponse:\n        \"\"\"\n        Sends a request to the given server.\n\n        Args:\n            request: details of request to be sent\n\n            retry_on_dns_fail: true if the request should be retied on DNS failures\n\n            timeout: number of milliseconds to wait for the response headers\n                (including connecting to the server), *for each attempt*.\n                60s by default.\n\n            long_retries: whether to use the long retry algorithm.\n\n                The regular retry algorithm makes 4 attempts, with intervals\n                [0.5s, 1s, 2s].\n\n                The long retry algorithm makes 11 attempts, with intervals\n                [4s, 16s, 60s, 60s, ...]\n\n                Both algorithms add -20%/+40% jitter to the retry intervals.\n\n                Note that the above intervals are *in addition* to the time spent\n                waiting for the request to complete (up to `timeout` ms).\n\n                NB: the long retry algorithm takes over 20 minutes to complete, with\n                a default timeout of 60s!\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n            backoff_on_404: Back off if we get a 404\n\n        Returns:\n            Resolves with the HTTP response object on success.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        if timeout:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        if (\n            self.hs.config.federation_domain_whitelist is not None\n            and request.destination not in self.hs.config.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(request.destination)\n\n        limiter = await synapse.util.retryutils.get_retry_limiter(\n            request.destination,\n            self.clock,\n            self._store,\n            backoff_on_404=backoff_on_404,\n            ignore_backoff=ignore_backoff,\n        )\n\n        method_bytes = request.method.encode(\"ascii\")\n        destination_bytes = request.destination.encode(\"ascii\")\n        path_bytes = request.path.encode(\"ascii\")\n        if request.query:\n            query_bytes = encode_query_args(request.query)\n        else:\n            query_bytes = b\"\"\n\n        scope = start_active_span(\n            \"outgoing-federation-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.PEER_ADDRESS: request.destination,\n                tags.HTTP_METHOD: request.method,\n                tags.HTTP_URL: request.path,\n            },\n            finish_on_close=True,\n        )\n\n        # Inject the span into the headers\n        headers_dict = {}  # type: Dict[bytes, List[bytes]]\n        inject_active_span_byte_dict(headers_dict, request.destination)\n\n        headers_dict[b\"User-Agent\"] = [self.version_string_bytes]\n\n        with limiter, scope:\n            # XXX: Would be much nicer to retry only at the transaction-layer\n            # (once we have reliable transactions in place)\n            if long_retries:\n                retries_left = MAX_LONG_RETRIES\n            else:\n                retries_left = MAX_SHORT_RETRIES\n\n            url_bytes = request.uri\n            url_str = url_bytes.decode(\"ascii\")\n\n            url_to_sign_bytes = urllib.parse.urlunparse(\n                (b\"\", b\"\", path_bytes, None, query_bytes, b\"\")\n            )\n\n            while True:\n                try:\n                    json = request.get_json()\n                    if json:\n                        headers_dict[b\"Content-Type\"] = [b\"application/json\"]\n                        auth_headers = self.build_auth_headers(\n                            destination_bytes, method_bytes, url_to_sign_bytes, json\n                        )\n                        data = encode_canonical_json(json)\n                        producer = QuieterFileBodyProducer(\n                            BytesIO(data), cooperator=self._cooperator\n                        )  # type: Optional[IBodyProducer]\n                    else:\n                        producer = None\n                        auth_headers = self.build_auth_headers(\n                            destination_bytes, method_bytes, url_to_sign_bytes\n                        )\n\n                    headers_dict[b\"Authorization\"] = auth_headers\n\n                    logger.debug(\n                        \"{%s} [%s] Sending request: %s %s; timeout %fs\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _sec_timeout,\n                    )\n\n                    outgoing_requests_counter.labels(request.method).inc()\n\n                    try:\n                        with Measure(self.clock, \"outbound_request\"):\n                            # we don't want all the fancy cookie and redirect handling\n                            # that treq.request gives: just use the raw Agent.\n                            request_deferred = self.agent.request(\n                                method_bytes,\n                                url_bytes,\n                                headers=Headers(headers_dict),\n                                bodyProducer=producer,\n                            )\n\n                            request_deferred = timeout_deferred(\n                                request_deferred,\n                                timeout=_sec_timeout,\n                                reactor=self.reactor,\n                            )\n\n                            response = await request_deferred\n                    except DNSLookupError as e:\n                        raise RequestSendFailed(e, can_retry=retry_on_dns_fail) from e\n                    except Exception as e:\n                        raise RequestSendFailed(e, can_retry=True) from e\n\n                    incoming_responses_counter.labels(\n                        request.method, response.code\n                    ).inc()\n\n                    set_tag(tags.HTTP_STATUS_CODE, response.code)\n                    response_phrase = response.phrase.decode(\"ascii\", errors=\"replace\")\n\n                    if 200 <= response.code < 300:\n                        logger.debug(\n                            \"{%s} [%s] Got response headers: %d %s\",\n                            request.txn_id,\n                            request.destination,\n                            response.code,\n                            response_phrase,\n                        )\n                        pass\n                    else:\n                        logger.info(\n                            \"{%s} [%s] Got response headers: %d %s\",\n                            request.txn_id,\n                            request.destination,\n                            response.code,\n                            response_phrase,\n                        )\n                        # :'(\n                        # Update transactions table?\n                        d = treq.content(response)\n                        d = timeout_deferred(\n                            d, timeout=_sec_timeout, reactor=self.reactor\n                        )\n\n                        try:\n                            body = await make_deferred_yieldable(d)\n                        except Exception as e:\n                            # Eh, we're already going to raise an exception so lets\n                            # ignore if this fails.\n                            logger.warning(\n                                \"{%s} [%s] Failed to get error response: %s %s: %s\",\n                                request.txn_id,\n                                request.destination,\n                                request.method,\n                                url_str,\n                                _flatten_response_never_received(e),\n                            )\n                            body = None\n\n                        exc = HttpResponseException(\n                            response.code, response_phrase, body\n                        )\n\n                        # Retry if the error is a 429 (Too Many Requests),\n                        # otherwise just raise a standard HttpResponseException\n                        if response.code == 429:\n                            raise RequestSendFailed(exc, can_retry=True) from exc\n                        else:\n                            raise exc\n\n                    break\n                except RequestSendFailed as e:\n                    logger.info(\n                        \"{%s} [%s] Request failed: %s %s: %s\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _flatten_response_never_received(e.inner_exception),\n                    )\n\n                    if not e.can_retry:\n                        raise\n\n                    if retries_left and not timeout:\n                        if long_retries:\n                            delay = 4 ** (MAX_LONG_RETRIES + 1 - retries_left)\n                            delay = min(delay, 60)\n                            delay *= random.uniform(0.8, 1.4)\n                        else:\n                            delay = 0.5 * 2 ** (MAX_SHORT_RETRIES - retries_left)\n                            delay = min(delay, 2)\n                            delay *= random.uniform(0.8, 1.4)\n\n                        logger.debug(\n                            \"{%s} [%s] Waiting %ss before re-sending...\",\n                            request.txn_id,\n                            request.destination,\n                            delay,\n                        )\n\n                        await self.clock.sleep(delay)\n                        retries_left -= 1\n                    else:\n                        raise\n\n                except Exception as e:\n                    logger.warning(\n                        \"{%s} [%s] Request failed: %s %s: %s\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _flatten_response_never_received(e),\n                    )\n                    raise\n        return response\n\n    def build_auth_headers(\n        self,\n        destination: Optional[bytes],\n        method: bytes,\n        url_bytes: bytes,\n        content: Optional[JsonDict] = None,\n        destination_is: Optional[bytes] = None,\n    ) -> List[bytes]:\n        \"\"\"\n        Builds the Authorization headers for a federation request\n        Args:\n            destination: The destination homeserver of the request.\n                May be None if the destination is an identity server, in which case\n                destination_is must be non-None.\n            method: The HTTP method of the request\n            url_bytes: The URI path of the request\n            content: The body of the request\n            destination_is: As 'destination', but if the destination is an\n                identity server\n\n        Returns:\n            A list of headers to be added as \"Authorization:\" headers\n        \"\"\"\n        request = {\n            \"method\": method.decode(\"ascii\"),\n            \"uri\": url_bytes.decode(\"ascii\"),\n            \"origin\": self.server_name,\n        }\n\n        if destination is not None:\n            request[\"destination\"] = destination.decode(\"ascii\")\n\n        if destination_is is not None:\n            request[\"destination_is\"] = destination_is.decode(\"ascii\")\n\n        if content is not None:\n            request[\"content\"] = content\n\n        request = sign_json(request, self.server_name, self.signing_key)\n\n        auth_headers = []\n\n        for key, sig in request[\"signatures\"][self.server_name].items():\n            auth_headers.append(\n                (\n                    'X-Matrix origin=%s,key=\"%s\",sig=\"%s\"'\n                    % (self.server_name, key, sig)\n                ).encode(\"ascii\")\n            )\n        return auth_headers\n\n    async def put_json(\n        self,\n        destination: str,\n        path: str,\n        args: Optional[QueryArgs] = None,\n        data: Optional[JsonDict] = None,\n        json_data_callback: Optional[Callable[[], JsonDict]] = None,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        backoff_on_404: bool = False,\n        try_trailing_slash_on_400: bool = False,\n    ) -> Union[JsonDict, list]:\n        \"\"\" Sends the specified json data using PUT\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path.\n            args: query params\n            data: A dict containing the data that will be used as\n                the request body. This will be encoded as JSON.\n            json_data_callback: A callable returning the dict to\n                use as the request body.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n            backoff_on_404: True if we should count a 404 response as\n                a failure of the server (and should therefore back off future\n                requests).\n            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED\n                response we should try appending a trailing slash to the end\n                of the request. Workaround for #3622 in Synapse <= v0.99.3. This\n                will be attempted before backing off if backing off has been\n                enabled.\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"PUT\",\n            destination=destination,\n            path=path,\n            query=args,\n            json_callback=json_data_callback,\n            json=data,\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request_with_optional_trailing_slash(\n            request,\n            try_trailing_slash_on_400,\n            backoff_on_404=backoff_on_404,\n            ignore_backoff=ignore_backoff,\n            long_retries=long_retries,\n            timeout=timeout,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n\n        return body\n\n    async def post_json(\n        self,\n        destination: str,\n        path: str,\n        data: Optional[JsonDict] = None,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        args: Optional[QueryArgs] = None,\n    ) -> Union[JsonDict, list]:\n        \"\"\" Sends the specified json data using POST\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n\n            path: The HTTP path.\n\n            data: A dict containing the data that will be used as\n                the request body. This will be encoded as JSON.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data and\n                try the request anyway.\n\n            args: query params\n        Returns:\n            dict|list: Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n\n        request = MatrixFederationRequest(\n            method=\"POST\", destination=destination, path=path, query=args, json=data\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request(\n            request,\n            long_retries=long_retries,\n            timeout=timeout,\n            ignore_backoff=ignore_backoff,\n        )\n\n        if timeout:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms,\n        )\n        return body\n\n    async def get_json(\n        self,\n        destination: str,\n        path: str,\n        args: Optional[QueryArgs] = None,\n        retry_on_dns_fail: bool = True,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        try_trailing_slash_on_400: bool = False,\n    ) -> Union[JsonDict, list]:\n        \"\"\" GETs some json from the given host homeserver and path\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n\n            path: The HTTP path.\n\n            args: A dictionary used to create query strings, defaults to\n                None.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED\n                response we should try appending a trailing slash to the end of\n                the request. Workaround for #3622 in Synapse <= v0.99.3.\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"GET\", destination=destination, path=path, query=args\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request_with_optional_trailing_slash(\n            request,\n            try_trailing_slash_on_400,\n            backoff_on_404=False,\n            ignore_backoff=ignore_backoff,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=timeout,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n\n        return body\n\n    async def delete_json(\n        self,\n        destination: str,\n        path: str,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        args: Optional[QueryArgs] = None,\n    ) -> Union[JsonDict, list]:\n        \"\"\"Send a DELETE request to the remote expecting some json response\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data and\n                try the request anyway.\n\n            args: query params\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"DELETE\", destination=destination, path=path, query=args\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request(\n            request,\n            long_retries=long_retries,\n            timeout=timeout,\n            ignore_backoff=ignore_backoff,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n        return body\n\n    async def get_file(\n        self,\n        destination: str,\n        path: str,\n        output_stream,\n        args: Optional[QueryArgs] = None,\n        retry_on_dns_fail: bool = True,\n        max_size: Optional[int] = None,\n        ignore_backoff: bool = False,\n    ) -> Tuple[int, Dict[bytes, List[bytes]]]:\n        \"\"\"GETs a file from a given homeserver\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path to GET.\n            output_stream: File to write the response body to.\n            args: Optional dictionary used to create the query string.\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n        Returns:\n            Resolves with an (int,dict) tuple of\n            the file length and a dict of the response headers.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"GET\", destination=destination, path=path, query=args\n        )\n\n        response = await self._send_request(\n            request, retry_on_dns_fail=retry_on_dns_fail, ignore_backoff=ignore_backoff\n        )\n\n        headers = dict(response.headers.getAllRawHeaders())\n\n        try:\n            d = readBodyToFile(response, output_stream, max_size)\n            d.addTimeout(self.default_timeout, self.reactor)\n            length = await make_deferred_yieldable(d)\n        except Exception as e:\n            logger.warning(\n                \"{%s} [%s] Error reading response: %s\",\n                request.txn_id,\n                request.destination,\n                e,\n            )\n            raise\n        logger.info(\n            \"{%s} [%s] Completed: %d %s [%d bytes] %s %s\",\n            request.txn_id,\n            request.destination,\n            response.code,\n            response.phrase.decode(\"ascii\", errors=\"replace\"),\n            length,\n            request.method,\n            request.uri.decode(\"ascii\"),\n        )\n        return (length, headers)\n\n\ndef _flatten_response_never_received(e):\n    if hasattr(e, \"reasons\"):\n        reasons = \", \".join(\n            _flatten_response_never_received(f.value) for f in e.reasons\n        )\n\n        return \"%s:[%s]\" % (type(e).__name__, reasons)\n    else:\n        return repr(e)\n\n\ndef check_content_type_is_json(headers: Headers) -> None:\n    \"\"\"\n    Check that a set of HTTP headers have a Content-Type header, and that it\n    is application/json.\n\n    Args:\n        headers: headers to check\n\n    Raises:\n        RequestSendFailed: if the Content-Type header is missing or isn't JSON\n\n    \"\"\"\n    c_type = headers.getRawHeaders(b\"Content-Type\")\n    if c_type is None:\n        raise RequestSendFailed(\n            RuntimeError(\"No Content-Type header received from remote server\"),\n            can_retry=False,\n        )\n\n    c_type = c_type[0].decode(\"ascii\")  # only the first header\n    val, options = cgi.parse_header(c_type)\n    if val != \"application/json\":\n        raise RequestSendFailed(\n            RuntimeError(\n                \"Remote server sent Content-Type header of '%s', not 'application/json'\"\n                % c_type,\n            ),\n            can_retry=False,\n        )\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport cgi\nimport logging\nimport random\nimport sys\nimport urllib.parse\nfrom io import BytesIO\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\n\nimport attr\nimport treq\nfrom canonicaljson import encode_canonical_json\nfrom prometheus_client import Counter\nfrom signedjson.sign import sign_json\nfrom zope.interface import implementer\n\nfrom twisted.internet import defer\nfrom twisted.internet.error import DNSLookupError\nfrom twisted.internet.interfaces import IReactorPluggableNameResolver, IReactorTime\nfrom twisted.internet.task import _EPSILON, Cooperator\nfrom twisted.web.http_headers import Headers\nfrom twisted.web.iweb import IBodyProducer, IResponse\n\nimport synapse.metrics\nimport synapse.util.retryutils\nfrom synapse.api.errors import (\n    FederationDeniedError,\n    HttpResponseException,\n    RequestSendFailed,\n)\nfrom synapse.http import QuieterFileBodyProducer\nfrom synapse.http.client import (\n    BlacklistingAgentWrapper,\n    IPBlacklistingResolver,\n    encode_query_args,\n    readBodyToFile,\n)\nfrom synapse.http.federation.matrix_federation_agent import MatrixFederationAgent\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.logging.opentracing import (\n    inject_active_span_byte_dict,\n    set_tag,\n    start_active_span,\n    tags,\n)\nfrom synapse.types import JsonDict\nfrom synapse.util import json_decoder\nfrom synapse.util.async_helpers import timeout_deferred\nfrom synapse.util.metrics import Measure\n\nlogger = logging.getLogger(__name__)\n\noutgoing_requests_counter = Counter(\n    \"synapse_http_matrixfederationclient_requests\", \"\", [\"method\"]\n)\nincoming_responses_counter = Counter(\n    \"synapse_http_matrixfederationclient_responses\", \"\", [\"method\", \"code\"]\n)\n\n\nMAX_LONG_RETRIES = 10\nMAX_SHORT_RETRIES = 3\nMAXINT = sys.maxsize\n\n\n_next_id = 1\n\n\nQueryArgs = Dict[str, Union[str, List[str]]]\n\n\n@attr.s(slots=True, frozen=True)\nclass MatrixFederationRequest:\n    method = attr.ib(type=str)\n    \"\"\"HTTP method\n    \"\"\"\n\n    path = attr.ib(type=str)\n    \"\"\"HTTP path\n    \"\"\"\n\n    destination = attr.ib(type=str)\n    \"\"\"The remote server to send the HTTP request to.\n    \"\"\"\n\n    json = attr.ib(default=None, type=Optional[JsonDict])\n    \"\"\"JSON to send in the body.\n    \"\"\"\n\n    json_callback = attr.ib(default=None, type=Optional[Callable[[], JsonDict]])\n    \"\"\"A callback to generate the JSON.\n    \"\"\"\n\n    query = attr.ib(default=None, type=Optional[dict])\n    \"\"\"Query arguments.\n    \"\"\"\n\n    txn_id = attr.ib(default=None, type=Optional[str])\n    \"\"\"Unique ID for this request (for logging)\n    \"\"\"\n\n    uri = attr.ib(init=False, type=bytes)\n    \"\"\"The URI of this request\n    \"\"\"\n\n    def __attrs_post_init__(self) -> None:\n        global _next_id\n        txn_id = \"%s-O-%s\" % (self.method, _next_id)\n        _next_id = (_next_id + 1) % (MAXINT - 1)\n\n        object.__setattr__(self, \"txn_id\", txn_id)\n\n        destination_bytes = self.destination.encode(\"ascii\")\n        path_bytes = self.path.encode(\"ascii\")\n        if self.query:\n            query_bytes = encode_query_args(self.query)\n        else:\n            query_bytes = b\"\"\n\n        # The object is frozen so we can pre-compute this.\n        uri = urllib.parse.urlunparse(\n            (b\"matrix\", destination_bytes, path_bytes, None, query_bytes, b\"\")\n        )\n        object.__setattr__(self, \"uri\", uri)\n\n    def get_json(self) -> Optional[JsonDict]:\n        if self.json_callback:\n            return self.json_callback()\n        return self.json\n\n\nasync def _handle_json_response(\n    reactor: IReactorTime,\n    timeout_sec: float,\n    request: MatrixFederationRequest,\n    response: IResponse,\n    start_ms: int,\n) -> JsonDict:\n    \"\"\"\n    Reads the JSON body of a response, with a timeout\n\n    Args:\n        reactor: twisted reactor, for the timeout\n        timeout_sec: number of seconds to wait for response to complete\n        request: the request that triggered the response\n        response: response to the request\n        start_ms: Timestamp when request was made\n\n    Returns:\n        The parsed JSON response\n    \"\"\"\n    try:\n        check_content_type_is_json(response.headers)\n\n        # Use the custom JSON decoder (partially re-implements treq.json_content).\n        d = treq.text_content(response, encoding=\"utf-8\")\n        d.addCallback(json_decoder.decode)\n        d = timeout_deferred(d, timeout=timeout_sec, reactor=reactor)\n\n        body = await make_deferred_yieldable(d)\n    except defer.TimeoutError as e:\n        logger.warning(\n            \"{%s} [%s] Timed out reading response - %s %s\",\n            request.txn_id,\n            request.destination,\n            request.method,\n            request.uri.decode(\"ascii\"),\n        )\n        raise RequestSendFailed(e, can_retry=True) from e\n    except Exception as e:\n        logger.warning(\n            \"{%s} [%s] Error reading response %s %s: %s\",\n            request.txn_id,\n            request.destination,\n            request.method,\n            request.uri.decode(\"ascii\"),\n            e,\n        )\n        raise\n\n    time_taken_secs = reactor.seconds() - start_ms / 1000\n\n    logger.info(\n        \"{%s} [%s] Completed request: %d %s in %.2f secs - %s %s\",\n        request.txn_id,\n        request.destination,\n        response.code,\n        response.phrase.decode(\"ascii\", errors=\"replace\"),\n        time_taken_secs,\n        request.method,\n        request.uri.decode(\"ascii\"),\n    )\n    return body\n\n\nclass MatrixFederationHttpClient:\n    \"\"\"HTTP client used to talk to other homeservers over the federation\n    protocol. Send client certificates and signs requests.\n\n    Attributes:\n        agent (twisted.web.client.Agent): The twisted Agent used to send the\n            requests.\n    \"\"\"\n\n    def __init__(self, hs, tls_client_options_factory):\n        self.hs = hs\n        self.signing_key = hs.signing_key\n        self.server_name = hs.hostname\n\n        real_reactor = hs.get_reactor()\n\n        # We need to use a DNS resolver which filters out blacklisted IP\n        # addresses, to prevent DNS rebinding.\n        nameResolver = IPBlacklistingResolver(\n            real_reactor, None, hs.config.federation_ip_range_blacklist\n        )\n\n        @implementer(IReactorPluggableNameResolver)\n        class Reactor:\n            def __getattr__(_self, attr):\n                if attr == \"nameResolver\":\n                    return nameResolver\n                else:\n                    return getattr(real_reactor, attr)\n\n        self.reactor = Reactor()\n\n        user_agent = hs.version_string\n        if hs.config.user_agent_suffix:\n            user_agent = \"%s %s\" % (user_agent, hs.config.user_agent_suffix)\n        user_agent = user_agent.encode(\"ascii\")\n\n        self.agent = MatrixFederationAgent(\n            self.reactor, tls_client_options_factory, user_agent\n        )\n\n        # Use a BlacklistingAgentWrapper to prevent circumventing the IP\n        # blacklist via IP literals in server names\n        self.agent = BlacklistingAgentWrapper(\n            self.agent, ip_blacklist=hs.config.federation_ip_range_blacklist,\n        )\n\n        self.clock = hs.get_clock()\n        self._store = hs.get_datastore()\n        self.version_string_bytes = hs.version_string.encode(\"ascii\")\n        self.default_timeout = 60\n\n        def schedule(x):\n            self.reactor.callLater(_EPSILON, x)\n\n        self._cooperator = Cooperator(scheduler=schedule)\n\n    async def _send_request_with_optional_trailing_slash(\n        self,\n        request: MatrixFederationRequest,\n        try_trailing_slash_on_400: bool = False,\n        **send_request_args\n    ) -> IResponse:\n        \"\"\"Wrapper for _send_request which can optionally retry the request\n        upon receiving a combination of a 400 HTTP response code and a\n        'M_UNRECOGNIZED' errcode. This is a workaround for Synapse <= v0.99.3\n        due to #3622.\n\n        Args:\n            request: details of request to be sent\n            try_trailing_slash_on_400: Whether on receiving a 400\n                'M_UNRECOGNIZED' from the server to retry the request with a\n                trailing slash appended to the request path.\n            send_request_args: A dictionary of arguments to pass to `_send_request()`.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n\n        Returns:\n            Parsed JSON response body.\n        \"\"\"\n        try:\n            response = await self._send_request(request, **send_request_args)\n        except HttpResponseException as e:\n            # Received an HTTP error > 300. Check if it meets the requirements\n            # to retry with a trailing slash\n            if not try_trailing_slash_on_400:\n                raise\n\n            if e.code != 400 or e.to_synapse_error().errcode != \"M_UNRECOGNIZED\":\n                raise\n\n            # Retry with a trailing slash if we received a 400 with\n            # 'M_UNRECOGNIZED' which some endpoints can return when omitting a\n            # trailing slash on Synapse <= v0.99.3.\n            logger.info(\"Retrying request with trailing slash\")\n\n            # Request is frozen so we create a new instance\n            request = attr.evolve(request, path=request.path + \"/\")\n\n            response = await self._send_request(request, **send_request_args)\n\n        return response\n\n    async def _send_request(\n        self,\n        request: MatrixFederationRequest,\n        retry_on_dns_fail: bool = True,\n        timeout: Optional[int] = None,\n        long_retries: bool = False,\n        ignore_backoff: bool = False,\n        backoff_on_404: bool = False,\n    ) -> IResponse:\n        \"\"\"\n        Sends a request to the given server.\n\n        Args:\n            request: details of request to be sent\n\n            retry_on_dns_fail: true if the request should be retied on DNS failures\n\n            timeout: number of milliseconds to wait for the response headers\n                (including connecting to the server), *for each attempt*.\n                60s by default.\n\n            long_retries: whether to use the long retry algorithm.\n\n                The regular retry algorithm makes 4 attempts, with intervals\n                [0.5s, 1s, 2s].\n\n                The long retry algorithm makes 11 attempts, with intervals\n                [4s, 16s, 60s, 60s, ...]\n\n                Both algorithms add -20%/+40% jitter to the retry intervals.\n\n                Note that the above intervals are *in addition* to the time spent\n                waiting for the request to complete (up to `timeout` ms).\n\n                NB: the long retry algorithm takes over 20 minutes to complete, with\n                a default timeout of 60s!\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n            backoff_on_404: Back off if we get a 404\n\n        Returns:\n            Resolves with the HTTP response object on success.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        if timeout:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        if (\n            self.hs.config.federation_domain_whitelist is not None\n            and request.destination not in self.hs.config.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(request.destination)\n\n        limiter = await synapse.util.retryutils.get_retry_limiter(\n            request.destination,\n            self.clock,\n            self._store,\n            backoff_on_404=backoff_on_404,\n            ignore_backoff=ignore_backoff,\n        )\n\n        method_bytes = request.method.encode(\"ascii\")\n        destination_bytes = request.destination.encode(\"ascii\")\n        path_bytes = request.path.encode(\"ascii\")\n        if request.query:\n            query_bytes = encode_query_args(request.query)\n        else:\n            query_bytes = b\"\"\n\n        scope = start_active_span(\n            \"outgoing-federation-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.PEER_ADDRESS: request.destination,\n                tags.HTTP_METHOD: request.method,\n                tags.HTTP_URL: request.path,\n            },\n            finish_on_close=True,\n        )\n\n        # Inject the span into the headers\n        headers_dict = {}  # type: Dict[bytes, List[bytes]]\n        inject_active_span_byte_dict(headers_dict, request.destination)\n\n        headers_dict[b\"User-Agent\"] = [self.version_string_bytes]\n\n        with limiter, scope:\n            # XXX: Would be much nicer to retry only at the transaction-layer\n            # (once we have reliable transactions in place)\n            if long_retries:\n                retries_left = MAX_LONG_RETRIES\n            else:\n                retries_left = MAX_SHORT_RETRIES\n\n            url_bytes = request.uri\n            url_str = url_bytes.decode(\"ascii\")\n\n            url_to_sign_bytes = urllib.parse.urlunparse(\n                (b\"\", b\"\", path_bytes, None, query_bytes, b\"\")\n            )\n\n            while True:\n                try:\n                    json = request.get_json()\n                    if json:\n                        headers_dict[b\"Content-Type\"] = [b\"application/json\"]\n                        auth_headers = self.build_auth_headers(\n                            destination_bytes, method_bytes, url_to_sign_bytes, json\n                        )\n                        data = encode_canonical_json(json)\n                        producer = QuieterFileBodyProducer(\n                            BytesIO(data), cooperator=self._cooperator\n                        )  # type: Optional[IBodyProducer]\n                    else:\n                        producer = None\n                        auth_headers = self.build_auth_headers(\n                            destination_bytes, method_bytes, url_to_sign_bytes\n                        )\n\n                    headers_dict[b\"Authorization\"] = auth_headers\n\n                    logger.debug(\n                        \"{%s} [%s] Sending request: %s %s; timeout %fs\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _sec_timeout,\n                    )\n\n                    outgoing_requests_counter.labels(request.method).inc()\n\n                    try:\n                        with Measure(self.clock, \"outbound_request\"):\n                            # we don't want all the fancy cookie and redirect handling\n                            # that treq.request gives: just use the raw Agent.\n                            request_deferred = self.agent.request(\n                                method_bytes,\n                                url_bytes,\n                                headers=Headers(headers_dict),\n                                bodyProducer=producer,\n                            )\n\n                            request_deferred = timeout_deferred(\n                                request_deferred,\n                                timeout=_sec_timeout,\n                                reactor=self.reactor,\n                            )\n\n                            response = await request_deferred\n                    except DNSLookupError as e:\n                        raise RequestSendFailed(e, can_retry=retry_on_dns_fail) from e\n                    except Exception as e:\n                        raise RequestSendFailed(e, can_retry=True) from e\n\n                    incoming_responses_counter.labels(\n                        request.method, response.code\n                    ).inc()\n\n                    set_tag(tags.HTTP_STATUS_CODE, response.code)\n                    response_phrase = response.phrase.decode(\"ascii\", errors=\"replace\")\n\n                    if 200 <= response.code < 300:\n                        logger.debug(\n                            \"{%s} [%s] Got response headers: %d %s\",\n                            request.txn_id,\n                            request.destination,\n                            response.code,\n                            response_phrase,\n                        )\n                        pass\n                    else:\n                        logger.info(\n                            \"{%s} [%s] Got response headers: %d %s\",\n                            request.txn_id,\n                            request.destination,\n                            response.code,\n                            response_phrase,\n                        )\n                        # :'(\n                        # Update transactions table?\n                        d = treq.content(response)\n                        d = timeout_deferred(\n                            d, timeout=_sec_timeout, reactor=self.reactor\n                        )\n\n                        try:\n                            body = await make_deferred_yieldable(d)\n                        except Exception as e:\n                            # Eh, we're already going to raise an exception so lets\n                            # ignore if this fails.\n                            logger.warning(\n                                \"{%s} [%s] Failed to get error response: %s %s: %s\",\n                                request.txn_id,\n                                request.destination,\n                                request.method,\n                                url_str,\n                                _flatten_response_never_received(e),\n                            )\n                            body = None\n\n                        exc = HttpResponseException(\n                            response.code, response_phrase, body\n                        )\n\n                        # Retry if the error is a 429 (Too Many Requests),\n                        # otherwise just raise a standard HttpResponseException\n                        if response.code == 429:\n                            raise RequestSendFailed(exc, can_retry=True) from exc\n                        else:\n                            raise exc\n\n                    break\n                except RequestSendFailed as e:\n                    logger.info(\n                        \"{%s} [%s] Request failed: %s %s: %s\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _flatten_response_never_received(e.inner_exception),\n                    )\n\n                    if not e.can_retry:\n                        raise\n\n                    if retries_left and not timeout:\n                        if long_retries:\n                            delay = 4 ** (MAX_LONG_RETRIES + 1 - retries_left)\n                            delay = min(delay, 60)\n                            delay *= random.uniform(0.8, 1.4)\n                        else:\n                            delay = 0.5 * 2 ** (MAX_SHORT_RETRIES - retries_left)\n                            delay = min(delay, 2)\n                            delay *= random.uniform(0.8, 1.4)\n\n                        logger.debug(\n                            \"{%s} [%s] Waiting %ss before re-sending...\",\n                            request.txn_id,\n                            request.destination,\n                            delay,\n                        )\n\n                        await self.clock.sleep(delay)\n                        retries_left -= 1\n                    else:\n                        raise\n\n                except Exception as e:\n                    logger.warning(\n                        \"{%s} [%s] Request failed: %s %s: %s\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _flatten_response_never_received(e),\n                    )\n                    raise\n        return response\n\n    def build_auth_headers(\n        self,\n        destination: Optional[bytes],\n        method: bytes,\n        url_bytes: bytes,\n        content: Optional[JsonDict] = None,\n        destination_is: Optional[bytes] = None,\n    ) -> List[bytes]:\n        \"\"\"\n        Builds the Authorization headers for a federation request\n        Args:\n            destination: The destination homeserver of the request.\n                May be None if the destination is an identity server, in which case\n                destination_is must be non-None.\n            method: The HTTP method of the request\n            url_bytes: The URI path of the request\n            content: The body of the request\n            destination_is: As 'destination', but if the destination is an\n                identity server\n\n        Returns:\n            A list of headers to be added as \"Authorization:\" headers\n        \"\"\"\n        request = {\n            \"method\": method.decode(\"ascii\"),\n            \"uri\": url_bytes.decode(\"ascii\"),\n            \"origin\": self.server_name,\n        }\n\n        if destination is not None:\n            request[\"destination\"] = destination.decode(\"ascii\")\n\n        if destination_is is not None:\n            request[\"destination_is\"] = destination_is.decode(\"ascii\")\n\n        if content is not None:\n            request[\"content\"] = content\n\n        request = sign_json(request, self.server_name, self.signing_key)\n\n        auth_headers = []\n\n        for key, sig in request[\"signatures\"][self.server_name].items():\n            auth_headers.append(\n                (\n                    'X-Matrix origin=%s,key=\"%s\",sig=\"%s\"'\n                    % (self.server_name, key, sig)\n                ).encode(\"ascii\")\n            )\n        return auth_headers\n\n    async def put_json(\n        self,\n        destination: str,\n        path: str,\n        args: Optional[QueryArgs] = None,\n        data: Optional[JsonDict] = None,\n        json_data_callback: Optional[Callable[[], JsonDict]] = None,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        backoff_on_404: bool = False,\n        try_trailing_slash_on_400: bool = False,\n    ) -> Union[JsonDict, list]:\n        \"\"\" Sends the specified json data using PUT\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path.\n            args: query params\n            data: A dict containing the data that will be used as\n                the request body. This will be encoded as JSON.\n            json_data_callback: A callable returning the dict to\n                use as the request body.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n            backoff_on_404: True if we should count a 404 response as\n                a failure of the server (and should therefore back off future\n                requests).\n            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED\n                response we should try appending a trailing slash to the end\n                of the request. Workaround for #3622 in Synapse <= v0.99.3. This\n                will be attempted before backing off if backing off has been\n                enabled.\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"PUT\",\n            destination=destination,\n            path=path,\n            query=args,\n            json_callback=json_data_callback,\n            json=data,\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request_with_optional_trailing_slash(\n            request,\n            try_trailing_slash_on_400,\n            backoff_on_404=backoff_on_404,\n            ignore_backoff=ignore_backoff,\n            long_retries=long_retries,\n            timeout=timeout,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n\n        return body\n\n    async def post_json(\n        self,\n        destination: str,\n        path: str,\n        data: Optional[JsonDict] = None,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        args: Optional[QueryArgs] = None,\n    ) -> Union[JsonDict, list]:\n        \"\"\" Sends the specified json data using POST\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n\n            path: The HTTP path.\n\n            data: A dict containing the data that will be used as\n                the request body. This will be encoded as JSON.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data and\n                try the request anyway.\n\n            args: query params\n        Returns:\n            dict|list: Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n\n        request = MatrixFederationRequest(\n            method=\"POST\", destination=destination, path=path, query=args, json=data\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request(\n            request,\n            long_retries=long_retries,\n            timeout=timeout,\n            ignore_backoff=ignore_backoff,\n        )\n\n        if timeout:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms,\n        )\n        return body\n\n    async def get_json(\n        self,\n        destination: str,\n        path: str,\n        args: Optional[QueryArgs] = None,\n        retry_on_dns_fail: bool = True,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        try_trailing_slash_on_400: bool = False,\n    ) -> Union[JsonDict, list]:\n        \"\"\" GETs some json from the given host homeserver and path\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n\n            path: The HTTP path.\n\n            args: A dictionary used to create query strings, defaults to\n                None.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED\n                response we should try appending a trailing slash to the end of\n                the request. Workaround for #3622 in Synapse <= v0.99.3.\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"GET\", destination=destination, path=path, query=args\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request_with_optional_trailing_slash(\n            request,\n            try_trailing_slash_on_400,\n            backoff_on_404=False,\n            ignore_backoff=ignore_backoff,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=timeout,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n\n        return body\n\n    async def delete_json(\n        self,\n        destination: str,\n        path: str,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        args: Optional[QueryArgs] = None,\n    ) -> Union[JsonDict, list]:\n        \"\"\"Send a DELETE request to the remote expecting some json response\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data and\n                try the request anyway.\n\n            args: query params\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"DELETE\", destination=destination, path=path, query=args\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request(\n            request,\n            long_retries=long_retries,\n            timeout=timeout,\n            ignore_backoff=ignore_backoff,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n        return body\n\n    async def get_file(\n        self,\n        destination: str,\n        path: str,\n        output_stream,\n        args: Optional[QueryArgs] = None,\n        retry_on_dns_fail: bool = True,\n        max_size: Optional[int] = None,\n        ignore_backoff: bool = False,\n    ) -> Tuple[int, Dict[bytes, List[bytes]]]:\n        \"\"\"GETs a file from a given homeserver\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path to GET.\n            output_stream: File to write the response body to.\n            args: Optional dictionary used to create the query string.\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n        Returns:\n            Resolves with an (int,dict) tuple of\n            the file length and a dict of the response headers.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"GET\", destination=destination, path=path, query=args\n        )\n\n        response = await self._send_request(\n            request, retry_on_dns_fail=retry_on_dns_fail, ignore_backoff=ignore_backoff\n        )\n\n        headers = dict(response.headers.getAllRawHeaders())\n\n        try:\n            d = readBodyToFile(response, output_stream, max_size)\n            d.addTimeout(self.default_timeout, self.reactor)\n            length = await make_deferred_yieldable(d)\n        except Exception as e:\n            logger.warning(\n                \"{%s} [%s] Error reading response: %s\",\n                request.txn_id,\n                request.destination,\n                e,\n            )\n            raise\n        logger.info(\n            \"{%s} [%s] Completed: %d %s [%d bytes] %s %s\",\n            request.txn_id,\n            request.destination,\n            response.code,\n            response.phrase.decode(\"ascii\", errors=\"replace\"),\n            length,\n            request.method,\n            request.uri.decode(\"ascii\"),\n        )\n        return (length, headers)\n\n\ndef _flatten_response_never_received(e):\n    if hasattr(e, \"reasons\"):\n        reasons = \", \".join(\n            _flatten_response_never_received(f.value) for f in e.reasons\n        )\n\n        return \"%s:[%s]\" % (type(e).__name__, reasons)\n    else:\n        return repr(e)\n\n\ndef check_content_type_is_json(headers: Headers) -> None:\n    \"\"\"\n    Check that a set of HTTP headers have a Content-Type header, and that it\n    is application/json.\n\n    Args:\n        headers: headers to check\n\n    Raises:\n        RequestSendFailed: if the Content-Type header is missing or isn't JSON\n\n    \"\"\"\n    c_type = headers.getRawHeaders(b\"Content-Type\")\n    if c_type is None:\n        raise RequestSendFailed(\n            RuntimeError(\"No Content-Type header received from remote server\"),\n            can_retry=False,\n        )\n\n    c_type = c_type[0].decode(\"ascii\")  # only the first header\n    val, options = cgi.parse_header(c_type)\n    if val != \"application/json\":\n        raise RequestSendFailed(\n            RuntimeError(\n                \"Remote server sent Content-Type header of '%s', not 'application/json'\"\n                % c_type,\n            ),\n            can_retry=False,\n        )\n", "patch": "@@ -26,11 +26,10 @@\n from canonicaljson import encode_canonical_json\n from prometheus_client import Counter\n from signedjson.sign import sign_json\n-from zope.interface import implementer\n \n from twisted.internet import defer\n from twisted.internet.error import DNSLookupError\n-from twisted.internet.interfaces import IReactorPluggableNameResolver, IReactorTime\n+from twisted.internet.interfaces import IReactorTime\n from twisted.internet.task import _EPSILON, Cooperator\n from twisted.web.http_headers import Headers\n from twisted.web.iweb import IBodyProducer, IResponse\n@@ -45,7 +44,7 @@\n from synapse.http import QuieterFileBodyProducer\n from synapse.http.client import (\n     BlacklistingAgentWrapper,\n-    IPBlacklistingResolver,\n+    BlacklistingReactorWrapper,\n     encode_query_args,\n     readBodyToFile,\n )\n@@ -221,31 +220,22 @@ def __init__(self, hs, tls_client_options_factory):\n         self.signing_key = hs.signing_key\n         self.server_name = hs.hostname\n \n-        real_reactor = hs.get_reactor()\n-\n         # We need to use a DNS resolver which filters out blacklisted IP\n         # addresses, to prevent DNS rebinding.\n-        nameResolver = IPBlacklistingResolver(\n-            real_reactor, None, hs.config.federation_ip_range_blacklist\n+        self.reactor = BlacklistingReactorWrapper(\n+            hs.get_reactor(), None, hs.config.federation_ip_range_blacklist\n         )\n \n-        @implementer(IReactorPluggableNameResolver)\n-        class Reactor:\n-            def __getattr__(_self, attr):\n-                if attr == \"nameResolver\":\n-                    return nameResolver\n-                else:\n-                    return getattr(real_reactor, attr)\n-\n-        self.reactor = Reactor()\n-\n         user_agent = hs.version_string\n         if hs.config.user_agent_suffix:\n             user_agent = \"%s %s\" % (user_agent, hs.config.user_agent_suffix)\n         user_agent = user_agent.encode(\"ascii\")\n \n         self.agent = MatrixFederationAgent(\n-            self.reactor, tls_client_options_factory, user_agent\n+            self.reactor,\n+            tls_client_options_factory,\n+            user_agent,\n+            hs.config.federation_ip_range_blacklist,\n         )\n \n         # Use a BlacklistingAgentWrapper to prevent circumventing the IP", "file_path": "files/2021_2/26", "file_language": "py", "file_name": "synapse/http/matrixfederationclient.py", "outdated_file_modify": 1, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "async def _handle_json_response(\n    reactor: IReactorTime,\n    timeout_sec: float,\n    request: MatrixFederationRequest,\n    response: IResponse,\n    start_ms: int,\n) -> JsonDict:\n    \"\"\"\n    Reads the JSON body of a response, with a timeout\n\n    Args:\n        reactor: twisted reactor, for the timeout\n        timeout_sec: number of seconds to wait for response to complete\n        request: the request that triggered the response\n        response: response to the request\n        start_ms: Timestamp when request was made\n\n    Returns:\n        The parsed JSON response\n    \"\"\"\n    try:\n        check_content_type_is_json(response.headers)\n\n        # Use the custom JSON decoder (partially re-implements treq.json_content).\n        d = treq.text_content(response, encoding=\"utf-8\")\n        d.addCallback(json_decoder.decode)\n        d = timeout_deferred(d, timeout=timeout_sec, reactor=reactor)\n\n        body = await make_deferred_yieldable(d)\n    except defer.TimeoutError as e:\n        logger.warning(\n            \"{%s} [%s] Timed out reading response - %s %s\",\n            request.txn_id,\n            request.destination,\n            request.method,\n            request.uri.decode(\"ascii\"),\n        )\n        raise RequestSendFailed(e, can_retry=True) from e\n    except Exception as e:\n        logger.warning(\n            \"{%s} [%s] Error reading response %s %s: %s\",\n            request.txn_id,\n            request.destination,\n            request.method,\n            request.uri.decode(\"ascii\"),\n            e,\n        )\n        raise\n\n    time_taken_secs = reactor.seconds() - start_ms / 1000\n\n    logger.info(\n        \"{%s} [%s] Completed request: %d %s in %.2f secs - %s %s\",\n        request.txn_id,\n        request.destination,\n        response.code,\n        response.phrase.decode(\"ascii\", errors=\"replace\"),\n        time_taken_secs,\n        request.method,\n        request.uri.decode(\"ascii\"),\n    )\n    return body", "target": 0}, {"function": "class MatrixFederationHttpClient:\n    \"\"\"HTTP client used to talk to other homeservers over the federation\n    protocol. Send client certificates and signs requests.\n\n    Attributes:\n        agent (twisted.web.client.Agent): The twisted Agent used to send the\n            requests.\n    \"\"\"\n\n    def __init__(self, hs, tls_client_options_factory):\n        self.hs = hs\n        self.signing_key = hs.signing_key\n        self.server_name = hs.hostname\n\n        real_reactor = hs.get_reactor()\n\n        # We need to use a DNS resolver which filters out blacklisted IP\n        # addresses, to prevent DNS rebinding.\n        nameResolver = IPBlacklistingResolver(\n            real_reactor, None, hs.config.federation_ip_range_blacklist\n        )\n\n        @implementer(IReactorPluggableNameResolver)\n        class Reactor:\n            def __getattr__(_self, attr):\n                if attr == \"nameResolver\":\n                    return nameResolver\n                else:\n                    return getattr(real_reactor, attr)\n\n        self.reactor = Reactor()\n\n        user_agent = hs.version_string\n        if hs.config.user_agent_suffix:\n            user_agent = \"%s %s\" % (user_agent, hs.config.user_agent_suffix)\n        user_agent = user_agent.encode(\"ascii\")\n\n        self.agent = MatrixFederationAgent(\n            self.reactor, tls_client_options_factory, user_agent\n        )\n\n        # Use a BlacklistingAgentWrapper to prevent circumventing the IP\n        # blacklist via IP literals in server names\n        self.agent = BlacklistingAgentWrapper(\n            self.agent, ip_blacklist=hs.config.federation_ip_range_blacklist,\n        )\n\n        self.clock = hs.get_clock()\n        self._store = hs.get_datastore()\n        self.version_string_bytes = hs.version_string.encode(\"ascii\")\n        self.default_timeout = 60\n\n        def schedule(x):\n            self.reactor.callLater(_EPSILON, x)\n\n        self._cooperator = Cooperator(scheduler=schedule)\n\n    async def _send_request_with_optional_trailing_slash(\n        self,\n        request: MatrixFederationRequest,\n        try_trailing_slash_on_400: bool = False,\n        **send_request_args\n    ) -> IResponse:\n        \"\"\"Wrapper for _send_request which can optionally retry the request\n        upon receiving a combination of a 400 HTTP response code and a\n        'M_UNRECOGNIZED' errcode. This is a workaround for Synapse <= v0.99.3\n        due to #3622.\n\n        Args:\n            request: details of request to be sent\n            try_trailing_slash_on_400: Whether on receiving a 400\n                'M_UNRECOGNIZED' from the server to retry the request with a\n                trailing slash appended to the request path.\n            send_request_args: A dictionary of arguments to pass to `_send_request()`.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n\n        Returns:\n            Parsed JSON response body.\n        \"\"\"\n        try:\n            response = await self._send_request(request, **send_request_args)\n        except HttpResponseException as e:\n            # Received an HTTP error > 300. Check if it meets the requirements\n            # to retry with a trailing slash\n            if not try_trailing_slash_on_400:\n                raise\n\n            if e.code != 400 or e.to_synapse_error().errcode != \"M_UNRECOGNIZED\":\n                raise\n\n            # Retry with a trailing slash if we received a 400 with\n            # 'M_UNRECOGNIZED' which some endpoints can return when omitting a\n            # trailing slash on Synapse <= v0.99.3.\n            logger.info(\"Retrying request with trailing slash\")\n\n            # Request is frozen so we create a new instance\n            request = attr.evolve(request, path=request.path + \"/\")\n\n            response = await self._send_request(request, **send_request_args)\n\n        return response\n\n    async def _send_request(\n        self,\n        request: MatrixFederationRequest,\n        retry_on_dns_fail: bool = True,\n        timeout: Optional[int] = None,\n        long_retries: bool = False,\n        ignore_backoff: bool = False,\n        backoff_on_404: bool = False,\n    ) -> IResponse:\n        \"\"\"\n        Sends a request to the given server.\n\n        Args:\n            request: details of request to be sent\n\n            retry_on_dns_fail: true if the request should be retied on DNS failures\n\n            timeout: number of milliseconds to wait for the response headers\n                (including connecting to the server), *for each attempt*.\n                60s by default.\n\n            long_retries: whether to use the long retry algorithm.\n\n                The regular retry algorithm makes 4 attempts, with intervals\n                [0.5s, 1s, 2s].\n\n                The long retry algorithm makes 11 attempts, with intervals\n                [4s, 16s, 60s, 60s, ...]\n\n                Both algorithms add -20%/+40% jitter to the retry intervals.\n\n                Note that the above intervals are *in addition* to the time spent\n                waiting for the request to complete (up to `timeout` ms).\n\n                NB: the long retry algorithm takes over 20 minutes to complete, with\n                a default timeout of 60s!\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n            backoff_on_404: Back off if we get a 404\n\n        Returns:\n            Resolves with the HTTP response object on success.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        if timeout:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        if (\n            self.hs.config.federation_domain_whitelist is not None\n            and request.destination not in self.hs.config.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(request.destination)\n\n        limiter = await synapse.util.retryutils.get_retry_limiter(\n            request.destination,\n            self.clock,\n            self._store,\n            backoff_on_404=backoff_on_404,\n            ignore_backoff=ignore_backoff,\n        )\n\n        method_bytes = request.method.encode(\"ascii\")\n        destination_bytes = request.destination.encode(\"ascii\")\n        path_bytes = request.path.encode(\"ascii\")\n        if request.query:\n            query_bytes = encode_query_args(request.query)\n        else:\n            query_bytes = b\"\"\n\n        scope = start_active_span(\n            \"outgoing-federation-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.PEER_ADDRESS: request.destination,\n                tags.HTTP_METHOD: request.method,\n                tags.HTTP_URL: request.path,\n            },\n            finish_on_close=True,\n        )\n\n        # Inject the span into the headers\n        headers_dict = {}  # type: Dict[bytes, List[bytes]]\n        inject_active_span_byte_dict(headers_dict, request.destination)\n\n        headers_dict[b\"User-Agent\"] = [self.version_string_bytes]\n\n        with limiter, scope:\n            # XXX: Would be much nicer to retry only at the transaction-layer\n            # (once we have reliable transactions in place)\n            if long_retries:\n                retries_left = MAX_LONG_RETRIES\n            else:\n                retries_left = MAX_SHORT_RETRIES\n\n            url_bytes = request.uri\n            url_str = url_bytes.decode(\"ascii\")\n\n            url_to_sign_bytes = urllib.parse.urlunparse(\n                (b\"\", b\"\", path_bytes, None, query_bytes, b\"\")\n            )\n\n            while True:\n                try:\n                    json = request.get_json()\n                    if json:\n                        headers_dict[b\"Content-Type\"] = [b\"application/json\"]\n                        auth_headers = self.build_auth_headers(\n                            destination_bytes, method_bytes, url_to_sign_bytes, json\n                        )\n                        data = encode_canonical_json(json)\n                        producer = QuieterFileBodyProducer(\n                            BytesIO(data), cooperator=self._cooperator\n                        )  # type: Optional[IBodyProducer]\n                    else:\n                        producer = None\n                        auth_headers = self.build_auth_headers(\n                            destination_bytes, method_bytes, url_to_sign_bytes\n                        )\n\n                    headers_dict[b\"Authorization\"] = auth_headers\n\n                    logger.debug(\n                        \"{%s} [%s] Sending request: %s %s; timeout %fs\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _sec_timeout,\n                    )\n\n                    outgoing_requests_counter.labels(request.method).inc()\n\n                    try:\n                        with Measure(self.clock, \"outbound_request\"):\n                            # we don't want all the fancy cookie and redirect handling\n                            # that treq.request gives: just use the raw Agent.\n                            request_deferred = self.agent.request(\n                                method_bytes,\n                                url_bytes,\n                                headers=Headers(headers_dict),\n                                bodyProducer=producer,\n                            )\n\n                            request_deferred = timeout_deferred(\n                                request_deferred,\n                                timeout=_sec_timeout,\n                                reactor=self.reactor,\n                            )\n\n                            response = await request_deferred\n                    except DNSLookupError as e:\n                        raise RequestSendFailed(e, can_retry=retry_on_dns_fail) from e\n                    except Exception as e:\n                        raise RequestSendFailed(e, can_retry=True) from e\n\n                    incoming_responses_counter.labels(\n                        request.method, response.code\n                    ).inc()\n\n                    set_tag(tags.HTTP_STATUS_CODE, response.code)\n                    response_phrase = response.phrase.decode(\"ascii\", errors=\"replace\")\n\n                    if 200 <= response.code < 300:\n                        logger.debug(\n                            \"{%s} [%s] Got response headers: %d %s\",\n                            request.txn_id,\n                            request.destination,\n                            response.code,\n                            response_phrase,\n                        )\n                        pass\n                    else:\n                        logger.info(\n                            \"{%s} [%s] Got response headers: %d %s\",\n                            request.txn_id,\n                            request.destination,\n                            response.code,\n                            response_phrase,\n                        )\n                        # :'(\n                        # Update transactions table?\n                        d = treq.content(response)\n                        d = timeout_deferred(\n                            d, timeout=_sec_timeout, reactor=self.reactor\n                        )\n\n                        try:\n                            body = await make_deferred_yieldable(d)\n                        except Exception as e:\n                            # Eh, we're already going to raise an exception so lets\n                            # ignore if this fails.\n                            logger.warning(\n                                \"{%s} [%s] Failed to get error response: %s %s: %s\",\n                                request.txn_id,\n                                request.destination,\n                                request.method,\n                                url_str,\n                                _flatten_response_never_received(e),\n                            )\n                            body = None\n\n                        exc = HttpResponseException(\n                            response.code, response_phrase, body\n                        )\n\n                        # Retry if the error is a 429 (Too Many Requests),\n                        # otherwise just raise a standard HttpResponseException\n                        if response.code == 429:\n                            raise RequestSendFailed(exc, can_retry=True) from exc\n                        else:\n                            raise exc\n\n                    break\n                except RequestSendFailed as e:\n                    logger.info(\n                        \"{%s} [%s] Request failed: %s %s: %s\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _flatten_response_never_received(e.inner_exception),\n                    )\n\n                    if not e.can_retry:\n                        raise\n\n                    if retries_left and not timeout:\n                        if long_retries:\n                            delay = 4 ** (MAX_LONG_RETRIES + 1 - retries_left)\n                            delay = min(delay, 60)\n                            delay *= random.uniform(0.8, 1.4)\n                        else:\n                            delay = 0.5 * 2 ** (MAX_SHORT_RETRIES - retries_left)\n                            delay = min(delay, 2)\n                            delay *= random.uniform(0.8, 1.4)\n\n                        logger.debug(\n                            \"{%s} [%s] Waiting %ss before re-sending...\",\n                            request.txn_id,\n                            request.destination,\n                            delay,\n                        )\n\n                        await self.clock.sleep(delay)\n                        retries_left -= 1\n                    else:\n                        raise\n\n                except Exception as e:\n                    logger.warning(\n                        \"{%s} [%s] Request failed: %s %s: %s\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _flatten_response_never_received(e),\n                    )\n                    raise\n        return response\n\n    def build_auth_headers(\n        self,\n        destination: Optional[bytes],\n        method: bytes,\n        url_bytes: bytes,\n        content: Optional[JsonDict] = None,\n        destination_is: Optional[bytes] = None,\n    ) -> List[bytes]:\n        \"\"\"\n        Builds the Authorization headers for a federation request\n        Args:\n            destination: The destination homeserver of the request.\n                May be None if the destination is an identity server, in which case\n                destination_is must be non-None.\n            method: The HTTP method of the request\n            url_bytes: The URI path of the request\n            content: The body of the request\n            destination_is: As 'destination', but if the destination is an\n                identity server\n\n        Returns:\n            A list of headers to be added as \"Authorization:\" headers\n        \"\"\"\n        request = {\n            \"method\": method.decode(\"ascii\"),\n            \"uri\": url_bytes.decode(\"ascii\"),\n            \"origin\": self.server_name,\n        }\n\n        if destination is not None:\n            request[\"destination\"] = destination.decode(\"ascii\")\n\n        if destination_is is not None:\n            request[\"destination_is\"] = destination_is.decode(\"ascii\")\n\n        if content is not None:\n            request[\"content\"] = content\n\n        request = sign_json(request, self.server_name, self.signing_key)\n\n        auth_headers = []\n\n        for key, sig in request[\"signatures\"][self.server_name].items():\n            auth_headers.append(\n                (\n                    'X-Matrix origin=%s,key=\"%s\",sig=\"%s\"'\n                    % (self.server_name, key, sig)\n                ).encode(\"ascii\")\n            )\n        return auth_headers\n\n    async def put_json(\n        self,\n        destination: str,\n        path: str,\n        args: Optional[QueryArgs] = None,\n        data: Optional[JsonDict] = None,\n        json_data_callback: Optional[Callable[[], JsonDict]] = None,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        backoff_on_404: bool = False,\n        try_trailing_slash_on_400: bool = False,\n    ) -> Union[JsonDict, list]:\n        \"\"\" Sends the specified json data using PUT\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path.\n            args: query params\n            data: A dict containing the data that will be used as\n                the request body. This will be encoded as JSON.\n            json_data_callback: A callable returning the dict to\n                use as the request body.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n            backoff_on_404: True if we should count a 404 response as\n                a failure of the server (and should therefore back off future\n                requests).\n            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED\n                response we should try appending a trailing slash to the end\n                of the request. Workaround for #3622 in Synapse <= v0.99.3. This\n                will be attempted before backing off if backing off has been\n                enabled.\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"PUT\",\n            destination=destination,\n            path=path,\n            query=args,\n            json_callback=json_data_callback,\n            json=data,\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request_with_optional_trailing_slash(\n            request,\n            try_trailing_slash_on_400,\n            backoff_on_404=backoff_on_404,\n            ignore_backoff=ignore_backoff,\n            long_retries=long_retries,\n            timeout=timeout,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n\n        return body\n\n    async def post_json(\n        self,\n        destination: str,\n        path: str,\n        data: Optional[JsonDict] = None,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        args: Optional[QueryArgs] = None,\n    ) -> Union[JsonDict, list]:\n        \"\"\" Sends the specified json data using POST\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n\n            path: The HTTP path.\n\n            data: A dict containing the data that will be used as\n                the request body. This will be encoded as JSON.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data and\n                try the request anyway.\n\n            args: query params\n        Returns:\n            dict|list: Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n\n        request = MatrixFederationRequest(\n            method=\"POST\", destination=destination, path=path, query=args, json=data\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request(\n            request,\n            long_retries=long_retries,\n            timeout=timeout,\n            ignore_backoff=ignore_backoff,\n        )\n\n        if timeout:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms,\n        )\n        return body\n\n    async def get_json(\n        self,\n        destination: str,\n        path: str,\n        args: Optional[QueryArgs] = None,\n        retry_on_dns_fail: bool = True,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        try_trailing_slash_on_400: bool = False,\n    ) -> Union[JsonDict, list]:\n        \"\"\" GETs some json from the given host homeserver and path\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n\n            path: The HTTP path.\n\n            args: A dictionary used to create query strings, defaults to\n                None.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED\n                response we should try appending a trailing slash to the end of\n                the request. Workaround for #3622 in Synapse <= v0.99.3.\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"GET\", destination=destination, path=path, query=args\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request_with_optional_trailing_slash(\n            request,\n            try_trailing_slash_on_400,\n            backoff_on_404=False,\n            ignore_backoff=ignore_backoff,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=timeout,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n\n        return body\n\n    async def delete_json(\n        self,\n        destination: str,\n        path: str,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        args: Optional[QueryArgs] = None,\n    ) -> Union[JsonDict, list]:\n        \"\"\"Send a DELETE request to the remote expecting some json response\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data and\n                try the request anyway.\n\n            args: query params\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"DELETE\", destination=destination, path=path, query=args\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request(\n            request,\n            long_retries=long_retries,\n            timeout=timeout,\n            ignore_backoff=ignore_backoff,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n        return body\n\n    async def get_file(\n        self,\n        destination: str,\n        path: str,\n        output_stream,\n        args: Optional[QueryArgs] = None,\n        retry_on_dns_fail: bool = True,\n        max_size: Optional[int] = None,\n        ignore_backoff: bool = False,\n    ) -> Tuple[int, Dict[bytes, List[bytes]]]:\n        \"\"\"GETs a file from a given homeserver\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path to GET.\n            output_stream: File to write the response body to.\n            args: Optional dictionary used to create the query string.\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n        Returns:\n            Resolves with an (int,dict) tuple of\n            the file length and a dict of the response headers.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"GET\", destination=destination, path=path, query=args\n        )\n\n        response = await self._send_request(\n            request, retry_on_dns_fail=retry_on_dns_fail, ignore_backoff=ignore_backoff\n        )\n\n        headers = dict(response.headers.getAllRawHeaders())\n\n        try:\n            d = readBodyToFile(response, output_stream, max_size)\n            d.addTimeout(self.default_timeout, self.reactor)\n            length = await make_deferred_yieldable(d)\n        except Exception as e:\n            logger.warning(\n                \"{%s} [%s] Error reading response: %s\",\n                request.txn_id,\n                request.destination,\n                e,\n            )\n            raise\n        logger.info(\n            \"{%s} [%s] Completed: %d %s [%d bytes] %s %s\",\n            request.txn_id,\n            request.destination,\n            response.code,\n            response.phrase.decode(\"ascii\", errors=\"replace\"),\n            length,\n            request.method,\n            request.uri.decode(\"ascii\"),\n        )\n        return (length, headers)", "target": 0}, {"function": "def _flatten_response_never_received(e):\n    if hasattr(e, \"reasons\"):\n        reasons = \", \".join(\n            _flatten_response_never_received(f.value) for f in e.reasons\n        )\n\n        return \"%s:[%s]\" % (type(e).__name__, reasons)\n    else:\n        return repr(e)", "target": 0}, {"function": "def check_content_type_is_json(headers: Headers) -> None:\n    \"\"\"\n    Check that a set of HTTP headers have a Content-Type header, and that it\n    is application/json.\n\n    Args:\n        headers: headers to check\n\n    Raises:\n        RequestSendFailed: if the Content-Type header is missing or isn't JSON\n\n    \"\"\"\n    c_type = headers.getRawHeaders(b\"Content-Type\")\n    if c_type is None:\n        raise RequestSendFailed(\n            RuntimeError(\"No Content-Type header received from remote server\"),\n            can_retry=False,\n        )\n\n    c_type = c_type[0].decode(\"ascii\")  # only the first header\n    val, options = cgi.parse_header(c_type)\n    if val != \"application/json\":\n        raise RequestSendFailed(\n            RuntimeError(\n                \"Remote server sent Content-Type header of '%s', not 'application/json'\"\n                % c_type,\n            ),\n            can_retry=False,\n        )", "target": 0}], "function_after": [{"function": "async def _handle_json_response(\n    reactor: IReactorTime,\n    timeout_sec: float,\n    request: MatrixFederationRequest,\n    response: IResponse,\n    start_ms: int,\n) -> JsonDict:\n    \"\"\"\n    Reads the JSON body of a response, with a timeout\n\n    Args:\n        reactor: twisted reactor, for the timeout\n        timeout_sec: number of seconds to wait for response to complete\n        request: the request that triggered the response\n        response: response to the request\n        start_ms: Timestamp when request was made\n\n    Returns:\n        The parsed JSON response\n    \"\"\"\n    try:\n        check_content_type_is_json(response.headers)\n\n        # Use the custom JSON decoder (partially re-implements treq.json_content).\n        d = treq.text_content(response, encoding=\"utf-8\")\n        d.addCallback(json_decoder.decode)\n        d = timeout_deferred(d, timeout=timeout_sec, reactor=reactor)\n\n        body = await make_deferred_yieldable(d)\n    except defer.TimeoutError as e:\n        logger.warning(\n            \"{%s} [%s] Timed out reading response - %s %s\",\n            request.txn_id,\n            request.destination,\n            request.method,\n            request.uri.decode(\"ascii\"),\n        )\n        raise RequestSendFailed(e, can_retry=True) from e\n    except Exception as e:\n        logger.warning(\n            \"{%s} [%s] Error reading response %s %s: %s\",\n            request.txn_id,\n            request.destination,\n            request.method,\n            request.uri.decode(\"ascii\"),\n            e,\n        )\n        raise\n\n    time_taken_secs = reactor.seconds() - start_ms / 1000\n\n    logger.info(\n        \"{%s} [%s] Completed request: %d %s in %.2f secs - %s %s\",\n        request.txn_id,\n        request.destination,\n        response.code,\n        response.phrase.decode(\"ascii\", errors=\"replace\"),\n        time_taken_secs,\n        request.method,\n        request.uri.decode(\"ascii\"),\n    )\n    return body", "target": 0}, {"function": "class MatrixFederationHttpClient:\n    \"\"\"HTTP client used to talk to other homeservers over the federation\n    protocol. Send client certificates and signs requests.\n\n    Attributes:\n        agent (twisted.web.client.Agent): The twisted Agent used to send the\n            requests.\n    \"\"\"\n\n    def __init__(self, hs, tls_client_options_factory):\n        self.hs = hs\n        self.signing_key = hs.signing_key\n        self.server_name = hs.hostname\n\n        # We need to use a DNS resolver which filters out blacklisted IP\n        # addresses, to prevent DNS rebinding.\n        self.reactor = BlacklistingReactorWrapper(\n            hs.get_reactor(), None, hs.config.federation_ip_range_blacklist\n        )\n\n        user_agent = hs.version_string\n        if hs.config.user_agent_suffix:\n            user_agent = \"%s %s\" % (user_agent, hs.config.user_agent_suffix)\n        user_agent = user_agent.encode(\"ascii\")\n\n        self.agent = MatrixFederationAgent(\n            self.reactor,\n            tls_client_options_factory,\n            user_agent,\n            hs.config.federation_ip_range_blacklist,\n        )\n\n        # Use a BlacklistingAgentWrapper to prevent circumventing the IP\n        # blacklist via IP literals in server names\n        self.agent = BlacklistingAgentWrapper(\n            self.agent, ip_blacklist=hs.config.federation_ip_range_blacklist,\n        )\n\n        self.clock = hs.get_clock()\n        self._store = hs.get_datastore()\n        self.version_string_bytes = hs.version_string.encode(\"ascii\")\n        self.default_timeout = 60\n\n        def schedule(x):\n            self.reactor.callLater(_EPSILON, x)\n\n        self._cooperator = Cooperator(scheduler=schedule)\n\n    async def _send_request_with_optional_trailing_slash(\n        self,\n        request: MatrixFederationRequest,\n        try_trailing_slash_on_400: bool = False,\n        **send_request_args\n    ) -> IResponse:\n        \"\"\"Wrapper for _send_request which can optionally retry the request\n        upon receiving a combination of a 400 HTTP response code and a\n        'M_UNRECOGNIZED' errcode. This is a workaround for Synapse <= v0.99.3\n        due to #3622.\n\n        Args:\n            request: details of request to be sent\n            try_trailing_slash_on_400: Whether on receiving a 400\n                'M_UNRECOGNIZED' from the server to retry the request with a\n                trailing slash appended to the request path.\n            send_request_args: A dictionary of arguments to pass to `_send_request()`.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n\n        Returns:\n            Parsed JSON response body.\n        \"\"\"\n        try:\n            response = await self._send_request(request, **send_request_args)\n        except HttpResponseException as e:\n            # Received an HTTP error > 300. Check if it meets the requirements\n            # to retry with a trailing slash\n            if not try_trailing_slash_on_400:\n                raise\n\n            if e.code != 400 or e.to_synapse_error().errcode != \"M_UNRECOGNIZED\":\n                raise\n\n            # Retry with a trailing slash if we received a 400 with\n            # 'M_UNRECOGNIZED' which some endpoints can return when omitting a\n            # trailing slash on Synapse <= v0.99.3.\n            logger.info(\"Retrying request with trailing slash\")\n\n            # Request is frozen so we create a new instance\n            request = attr.evolve(request, path=request.path + \"/\")\n\n            response = await self._send_request(request, **send_request_args)\n\n        return response\n\n    async def _send_request(\n        self,\n        request: MatrixFederationRequest,\n        retry_on_dns_fail: bool = True,\n        timeout: Optional[int] = None,\n        long_retries: bool = False,\n        ignore_backoff: bool = False,\n        backoff_on_404: bool = False,\n    ) -> IResponse:\n        \"\"\"\n        Sends a request to the given server.\n\n        Args:\n            request: details of request to be sent\n\n            retry_on_dns_fail: true if the request should be retied on DNS failures\n\n            timeout: number of milliseconds to wait for the response headers\n                (including connecting to the server), *for each attempt*.\n                60s by default.\n\n            long_retries: whether to use the long retry algorithm.\n\n                The regular retry algorithm makes 4 attempts, with intervals\n                [0.5s, 1s, 2s].\n\n                The long retry algorithm makes 11 attempts, with intervals\n                [4s, 16s, 60s, 60s, ...]\n\n                Both algorithms add -20%/+40% jitter to the retry intervals.\n\n                Note that the above intervals are *in addition* to the time spent\n                waiting for the request to complete (up to `timeout` ms).\n\n                NB: the long retry algorithm takes over 20 minutes to complete, with\n                a default timeout of 60s!\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n            backoff_on_404: Back off if we get a 404\n\n        Returns:\n            Resolves with the HTTP response object on success.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        if timeout:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        if (\n            self.hs.config.federation_domain_whitelist is not None\n            and request.destination not in self.hs.config.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(request.destination)\n\n        limiter = await synapse.util.retryutils.get_retry_limiter(\n            request.destination,\n            self.clock,\n            self._store,\n            backoff_on_404=backoff_on_404,\n            ignore_backoff=ignore_backoff,\n        )\n\n        method_bytes = request.method.encode(\"ascii\")\n        destination_bytes = request.destination.encode(\"ascii\")\n        path_bytes = request.path.encode(\"ascii\")\n        if request.query:\n            query_bytes = encode_query_args(request.query)\n        else:\n            query_bytes = b\"\"\n\n        scope = start_active_span(\n            \"outgoing-federation-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.PEER_ADDRESS: request.destination,\n                tags.HTTP_METHOD: request.method,\n                tags.HTTP_URL: request.path,\n            },\n            finish_on_close=True,\n        )\n\n        # Inject the span into the headers\n        headers_dict = {}  # type: Dict[bytes, List[bytes]]\n        inject_active_span_byte_dict(headers_dict, request.destination)\n\n        headers_dict[b\"User-Agent\"] = [self.version_string_bytes]\n\n        with limiter, scope:\n            # XXX: Would be much nicer to retry only at the transaction-layer\n            # (once we have reliable transactions in place)\n            if long_retries:\n                retries_left = MAX_LONG_RETRIES\n            else:\n                retries_left = MAX_SHORT_RETRIES\n\n            url_bytes = request.uri\n            url_str = url_bytes.decode(\"ascii\")\n\n            url_to_sign_bytes = urllib.parse.urlunparse(\n                (b\"\", b\"\", path_bytes, None, query_bytes, b\"\")\n            )\n\n            while True:\n                try:\n                    json = request.get_json()\n                    if json:\n                        headers_dict[b\"Content-Type\"] = [b\"application/json\"]\n                        auth_headers = self.build_auth_headers(\n                            destination_bytes, method_bytes, url_to_sign_bytes, json\n                        )\n                        data = encode_canonical_json(json)\n                        producer = QuieterFileBodyProducer(\n                            BytesIO(data), cooperator=self._cooperator\n                        )  # type: Optional[IBodyProducer]\n                    else:\n                        producer = None\n                        auth_headers = self.build_auth_headers(\n                            destination_bytes, method_bytes, url_to_sign_bytes\n                        )\n\n                    headers_dict[b\"Authorization\"] = auth_headers\n\n                    logger.debug(\n                        \"{%s} [%s] Sending request: %s %s; timeout %fs\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _sec_timeout,\n                    )\n\n                    outgoing_requests_counter.labels(request.method).inc()\n\n                    try:\n                        with Measure(self.clock, \"outbound_request\"):\n                            # we don't want all the fancy cookie and redirect handling\n                            # that treq.request gives: just use the raw Agent.\n                            request_deferred = self.agent.request(\n                                method_bytes,\n                                url_bytes,\n                                headers=Headers(headers_dict),\n                                bodyProducer=producer,\n                            )\n\n                            request_deferred = timeout_deferred(\n                                request_deferred,\n                                timeout=_sec_timeout,\n                                reactor=self.reactor,\n                            )\n\n                            response = await request_deferred\n                    except DNSLookupError as e:\n                        raise RequestSendFailed(e, can_retry=retry_on_dns_fail) from e\n                    except Exception as e:\n                        raise RequestSendFailed(e, can_retry=True) from e\n\n                    incoming_responses_counter.labels(\n                        request.method, response.code\n                    ).inc()\n\n                    set_tag(tags.HTTP_STATUS_CODE, response.code)\n                    response_phrase = response.phrase.decode(\"ascii\", errors=\"replace\")\n\n                    if 200 <= response.code < 300:\n                        logger.debug(\n                            \"{%s} [%s] Got response headers: %d %s\",\n                            request.txn_id,\n                            request.destination,\n                            response.code,\n                            response_phrase,\n                        )\n                        pass\n                    else:\n                        logger.info(\n                            \"{%s} [%s] Got response headers: %d %s\",\n                            request.txn_id,\n                            request.destination,\n                            response.code,\n                            response_phrase,\n                        )\n                        # :'(\n                        # Update transactions table?\n                        d = treq.content(response)\n                        d = timeout_deferred(\n                            d, timeout=_sec_timeout, reactor=self.reactor\n                        )\n\n                        try:\n                            body = await make_deferred_yieldable(d)\n                        except Exception as e:\n                            # Eh, we're already going to raise an exception so lets\n                            # ignore if this fails.\n                            logger.warning(\n                                \"{%s} [%s] Failed to get error response: %s %s: %s\",\n                                request.txn_id,\n                                request.destination,\n                                request.method,\n                                url_str,\n                                _flatten_response_never_received(e),\n                            )\n                            body = None\n\n                        exc = HttpResponseException(\n                            response.code, response_phrase, body\n                        )\n\n                        # Retry if the error is a 429 (Too Many Requests),\n                        # otherwise just raise a standard HttpResponseException\n                        if response.code == 429:\n                            raise RequestSendFailed(exc, can_retry=True) from exc\n                        else:\n                            raise exc\n\n                    break\n                except RequestSendFailed as e:\n                    logger.info(\n                        \"{%s} [%s] Request failed: %s %s: %s\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _flatten_response_never_received(e.inner_exception),\n                    )\n\n                    if not e.can_retry:\n                        raise\n\n                    if retries_left and not timeout:\n                        if long_retries:\n                            delay = 4 ** (MAX_LONG_RETRIES + 1 - retries_left)\n                            delay = min(delay, 60)\n                            delay *= random.uniform(0.8, 1.4)\n                        else:\n                            delay = 0.5 * 2 ** (MAX_SHORT_RETRIES - retries_left)\n                            delay = min(delay, 2)\n                            delay *= random.uniform(0.8, 1.4)\n\n                        logger.debug(\n                            \"{%s} [%s] Waiting %ss before re-sending...\",\n                            request.txn_id,\n                            request.destination,\n                            delay,\n                        )\n\n                        await self.clock.sleep(delay)\n                        retries_left -= 1\n                    else:\n                        raise\n\n                except Exception as e:\n                    logger.warning(\n                        \"{%s} [%s] Request failed: %s %s: %s\",\n                        request.txn_id,\n                        request.destination,\n                        request.method,\n                        url_str,\n                        _flatten_response_never_received(e),\n                    )\n                    raise\n        return response\n\n    def build_auth_headers(\n        self,\n        destination: Optional[bytes],\n        method: bytes,\n        url_bytes: bytes,\n        content: Optional[JsonDict] = None,\n        destination_is: Optional[bytes] = None,\n    ) -> List[bytes]:\n        \"\"\"\n        Builds the Authorization headers for a federation request\n        Args:\n            destination: The destination homeserver of the request.\n                May be None if the destination is an identity server, in which case\n                destination_is must be non-None.\n            method: The HTTP method of the request\n            url_bytes: The URI path of the request\n            content: The body of the request\n            destination_is: As 'destination', but if the destination is an\n                identity server\n\n        Returns:\n            A list of headers to be added as \"Authorization:\" headers\n        \"\"\"\n        request = {\n            \"method\": method.decode(\"ascii\"),\n            \"uri\": url_bytes.decode(\"ascii\"),\n            \"origin\": self.server_name,\n        }\n\n        if destination is not None:\n            request[\"destination\"] = destination.decode(\"ascii\")\n\n        if destination_is is not None:\n            request[\"destination_is\"] = destination_is.decode(\"ascii\")\n\n        if content is not None:\n            request[\"content\"] = content\n\n        request = sign_json(request, self.server_name, self.signing_key)\n\n        auth_headers = []\n\n        for key, sig in request[\"signatures\"][self.server_name].items():\n            auth_headers.append(\n                (\n                    'X-Matrix origin=%s,key=\"%s\",sig=\"%s\"'\n                    % (self.server_name, key, sig)\n                ).encode(\"ascii\")\n            )\n        return auth_headers\n\n    async def put_json(\n        self,\n        destination: str,\n        path: str,\n        args: Optional[QueryArgs] = None,\n        data: Optional[JsonDict] = None,\n        json_data_callback: Optional[Callable[[], JsonDict]] = None,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        backoff_on_404: bool = False,\n        try_trailing_slash_on_400: bool = False,\n    ) -> Union[JsonDict, list]:\n        \"\"\" Sends the specified json data using PUT\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path.\n            args: query params\n            data: A dict containing the data that will be used as\n                the request body. This will be encoded as JSON.\n            json_data_callback: A callable returning the dict to\n                use as the request body.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n            backoff_on_404: True if we should count a 404 response as\n                a failure of the server (and should therefore back off future\n                requests).\n            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED\n                response we should try appending a trailing slash to the end\n                of the request. Workaround for #3622 in Synapse <= v0.99.3. This\n                will be attempted before backing off if backing off has been\n                enabled.\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"PUT\",\n            destination=destination,\n            path=path,\n            query=args,\n            json_callback=json_data_callback,\n            json=data,\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request_with_optional_trailing_slash(\n            request,\n            try_trailing_slash_on_400,\n            backoff_on_404=backoff_on_404,\n            ignore_backoff=ignore_backoff,\n            long_retries=long_retries,\n            timeout=timeout,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n\n        return body\n\n    async def post_json(\n        self,\n        destination: str,\n        path: str,\n        data: Optional[JsonDict] = None,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        args: Optional[QueryArgs] = None,\n    ) -> Union[JsonDict, list]:\n        \"\"\" Sends the specified json data using POST\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n\n            path: The HTTP path.\n\n            data: A dict containing the data that will be used as\n                the request body. This will be encoded as JSON.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data and\n                try the request anyway.\n\n            args: query params\n        Returns:\n            dict|list: Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n\n        request = MatrixFederationRequest(\n            method=\"POST\", destination=destination, path=path, query=args, json=data\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request(\n            request,\n            long_retries=long_retries,\n            timeout=timeout,\n            ignore_backoff=ignore_backoff,\n        )\n\n        if timeout:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms,\n        )\n        return body\n\n    async def get_json(\n        self,\n        destination: str,\n        path: str,\n        args: Optional[QueryArgs] = None,\n        retry_on_dns_fail: bool = True,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        try_trailing_slash_on_400: bool = False,\n    ) -> Union[JsonDict, list]:\n        \"\"\" GETs some json from the given host homeserver and path\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n\n            path: The HTTP path.\n\n            args: A dictionary used to create query strings, defaults to\n                None.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED\n                response we should try appending a trailing slash to the end of\n                the request. Workaround for #3622 in Synapse <= v0.99.3.\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"GET\", destination=destination, path=path, query=args\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request_with_optional_trailing_slash(\n            request,\n            try_trailing_slash_on_400,\n            backoff_on_404=False,\n            ignore_backoff=ignore_backoff,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=timeout,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n\n        return body\n\n    async def delete_json(\n        self,\n        destination: str,\n        path: str,\n        long_retries: bool = False,\n        timeout: Optional[int] = None,\n        ignore_backoff: bool = False,\n        args: Optional[QueryArgs] = None,\n    ) -> Union[JsonDict, list]:\n        \"\"\"Send a DELETE request to the remote expecting some json response\n\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path.\n\n            long_retries: whether to use the long retry algorithm. See\n                docs on _send_request for details.\n\n            timeout: number of milliseconds to wait for the response.\n                self._default_timeout (60s) by default.\n\n                Note that we may make several attempts to send the request; this\n                timeout applies to the time spent waiting for response headers for\n                *each* attempt (including connection time) as well as the time spent\n                reading the response body after a 200 response.\n\n            ignore_backoff: true to ignore the historical backoff data and\n                try the request anyway.\n\n            args: query params\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The\n            result will be the decoded JSON body.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"DELETE\", destination=destination, path=path, query=args\n        )\n\n        start_ms = self.clock.time_msec()\n\n        response = await self._send_request(\n            request,\n            long_retries=long_retries,\n            timeout=timeout,\n            ignore_backoff=ignore_backoff,\n        )\n\n        if timeout is not None:\n            _sec_timeout = timeout / 1000\n        else:\n            _sec_timeout = self.default_timeout\n\n        body = await _handle_json_response(\n            self.reactor, _sec_timeout, request, response, start_ms\n        )\n        return body\n\n    async def get_file(\n        self,\n        destination: str,\n        path: str,\n        output_stream,\n        args: Optional[QueryArgs] = None,\n        retry_on_dns_fail: bool = True,\n        max_size: Optional[int] = None,\n        ignore_backoff: bool = False,\n    ) -> Tuple[int, Dict[bytes, List[bytes]]]:\n        \"\"\"GETs a file from a given homeserver\n        Args:\n            destination: The remote server to send the HTTP request to.\n            path: The HTTP path to GET.\n            output_stream: File to write the response body to.\n            args: Optional dictionary used to create the query string.\n            ignore_backoff: true to ignore the historical backoff data\n                and try the request anyway.\n\n        Returns:\n            Resolves with an (int,dict) tuple of\n            the file length and a dict of the response headers.\n\n        Raises:\n            HttpResponseException: If we get an HTTP response code >= 300\n                (except 429).\n            NotRetryingDestination: If we are not yet ready to retry this\n                server.\n            FederationDeniedError: If this destination  is not on our\n                federation whitelist\n            RequestSendFailed: If there were problems connecting to the\n                remote, due to e.g. DNS failures, connection timeouts etc.\n        \"\"\"\n        request = MatrixFederationRequest(\n            method=\"GET\", destination=destination, path=path, query=args\n        )\n\n        response = await self._send_request(\n            request, retry_on_dns_fail=retry_on_dns_fail, ignore_backoff=ignore_backoff\n        )\n\n        headers = dict(response.headers.getAllRawHeaders())\n\n        try:\n            d = readBodyToFile(response, output_stream, max_size)\n            d.addTimeout(self.default_timeout, self.reactor)\n            length = await make_deferred_yieldable(d)\n        except Exception as e:\n            logger.warning(\n                \"{%s} [%s] Error reading response: %s\",\n                request.txn_id,\n                request.destination,\n                e,\n            )\n            raise\n        logger.info(\n            \"{%s} [%s] Completed: %d %s [%d bytes] %s %s\",\n            request.txn_id,\n            request.destination,\n            response.code,\n            response.phrase.decode(\"ascii\", errors=\"replace\"),\n            length,\n            request.method,\n            request.uri.decode(\"ascii\"),\n        )\n        return (length, headers)", "target": 0}, {"function": "def _flatten_response_never_received(e):\n    if hasattr(e, \"reasons\"):\n        reasons = \", \".join(\n            _flatten_response_never_received(f.value) for f in e.reasons\n        )\n\n        return \"%s:[%s]\" % (type(e).__name__, reasons)\n    else:\n        return repr(e)", "target": 0}, {"function": "def check_content_type_is_json(headers: Headers) -> None:\n    \"\"\"\n    Check that a set of HTTP headers have a Content-Type header, and that it\n    is application/json.\n\n    Args:\n        headers: headers to check\n\n    Raises:\n        RequestSendFailed: if the Content-Type header is missing or isn't JSON\n\n    \"\"\"\n    c_type = headers.getRawHeaders(b\"Content-Type\")\n    if c_type is None:\n        raise RequestSendFailed(\n            RuntimeError(\"No Content-Type header received from remote server\"),\n            can_retry=False,\n        )\n\n    c_type = c_type[0].decode(\"ascii\")  # only the first header\n    val, options = cgi.parse_header(c_type)\n    if val != \"application/json\":\n        raise RequestSendFailed(\n            RuntimeError(\n                \"Remote server sent Content-Type header of '%s', not 'application/json'\"\n                % c_type,\n            ),\n            can_retry=False,\n        )", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Fpush%2Fhttppusher.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2015, 2016 OpenMarket Ltd\n# Copyright 2017 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\n\nfrom prometheus_client import Counter\n\nfrom twisted.internet.error import AlreadyCalled, AlreadyCancelled\n\nfrom synapse.api.constants import EventTypes\nfrom synapse.logging import opentracing\nfrom synapse.metrics.background_process_metrics import run_as_background_process\nfrom synapse.push import PusherConfigException\nfrom synapse.types import RoomStreamToken\n\nfrom . import push_rule_evaluator, push_tools\n\nlogger = logging.getLogger(__name__)\n\nhttp_push_processed_counter = Counter(\n    \"synapse_http_httppusher_http_pushes_processed\",\n    \"Number of push notifications successfully sent\",\n)\n\nhttp_push_failed_counter = Counter(\n    \"synapse_http_httppusher_http_pushes_failed\",\n    \"Number of push notifications which failed\",\n)\n\nhttp_badges_processed_counter = Counter(\n    \"synapse_http_httppusher_badge_updates_processed\",\n    \"Number of badge updates successfully sent\",\n)\n\nhttp_badges_failed_counter = Counter(\n    \"synapse_http_httppusher_badge_updates_failed\",\n    \"Number of badge updates which failed\",\n)\n\n\nclass HttpPusher:\n    INITIAL_BACKOFF_SEC = 1  # in seconds because that's what Twisted takes\n    MAX_BACKOFF_SEC = 60 * 60\n\n    # This one's in ms because we compare it against the clock\n    GIVE_UP_AFTER_MS = 24 * 60 * 60 * 1000\n\n    def __init__(self, hs, pusherdict):\n        self.hs = hs\n        self.store = self.hs.get_datastore()\n        self.storage = self.hs.get_storage()\n        self.clock = self.hs.get_clock()\n        self.state_handler = self.hs.get_state_handler()\n        self.user_id = pusherdict[\"user_name\"]\n        self.app_id = pusherdict[\"app_id\"]\n        self.app_display_name = pusherdict[\"app_display_name\"]\n        self.device_display_name = pusherdict[\"device_display_name\"]\n        self.pushkey = pusherdict[\"pushkey\"]\n        self.pushkey_ts = pusherdict[\"ts\"]\n        self.data = pusherdict[\"data\"]\n        self.last_stream_ordering = pusherdict[\"last_stream_ordering\"]\n        self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n        self.failing_since = pusherdict[\"failing_since\"]\n        self.timed_call = None\n        self._is_processing = False\n        self._group_unread_count_by_room = hs.config.push_group_unread_count_by_room\n\n        # This is the highest stream ordering we know it's safe to process.\n        # When new events arrive, we'll be given a window of new events: we\n        # should honour this rather than just looking for anything higher\n        # because of potential out-of-order event serialisation. This starts\n        # off as None though as we don't know any better.\n        self.max_stream_ordering = None\n\n        if \"data\" not in pusherdict:\n            raise PusherConfigException(\"No 'data' key for HTTP pusher\")\n        self.data = pusherdict[\"data\"]\n\n        self.name = \"%s/%s/%s\" % (\n            pusherdict[\"user_name\"],\n            pusherdict[\"app_id\"],\n            pusherdict[\"pushkey\"],\n        )\n\n        if self.data is None:\n            raise PusherConfigException(\"data can not be null for HTTP pusher\")\n\n        if \"url\" not in self.data:\n            raise PusherConfigException(\"'url' required in data for HTTP pusher\")\n        self.url = self.data[\"url\"]\n        self.http_client = hs.get_proxied_blacklisted_http_client()\n        self.data_minus_url = {}\n        self.data_minus_url.update(self.data)\n        del self.data_minus_url[\"url\"]\n\n    def on_started(self, should_check_for_notifs):\n        \"\"\"Called when this pusher has been started.\n\n        Args:\n            should_check_for_notifs (bool): Whether we should immediately\n                check for push to send. Set to False only if it's known there\n                is nothing to send\n        \"\"\"\n        if should_check_for_notifs:\n            self._start_processing()\n\n    def on_new_notifications(self, max_token: RoomStreamToken):\n        # We just use the minimum stream ordering and ignore the vector clock\n        # component. This is safe to do as long as we *always* ignore the vector\n        # clock components.\n        max_stream_ordering = max_token.stream\n\n        self.max_stream_ordering = max(\n            max_stream_ordering, self.max_stream_ordering or 0\n        )\n        self._start_processing()\n\n    def on_new_receipts(self, min_stream_id, max_stream_id):\n        # Note that the min here shouldn't be relied upon to be accurate.\n\n        # We could check the receipts are actually m.read receipts here,\n        # but currently that's the only type of receipt anyway...\n        run_as_background_process(\"http_pusher.on_new_receipts\", self._update_badge)\n\n    async def _update_badge(self):\n        # XXX as per https://github.com/matrix-org/matrix-doc/issues/2627, this seems\n        # to be largely redundant. perhaps we can remove it.\n        badge = await push_tools.get_badge_count(\n            self.hs.get_datastore(),\n            self.user_id,\n            group_by_room=self._group_unread_count_by_room,\n        )\n        await self._send_badge(badge)\n\n    def on_timer(self):\n        self._start_processing()\n\n    def on_stop(self):\n        if self.timed_call:\n            try:\n                self.timed_call.cancel()\n            except (AlreadyCalled, AlreadyCancelled):\n                pass\n            self.timed_call = None\n\n    def _start_processing(self):\n        if self._is_processing:\n            return\n\n        run_as_background_process(\"httppush.process\", self._process)\n\n    async def _process(self):\n        # we should never get here if we are already processing\n        assert not self._is_processing\n\n        try:\n            self._is_processing = True\n            # if the max ordering changes while we're running _unsafe_process,\n            # call it again, and so on until we've caught up.\n            while True:\n                starting_max_ordering = self.max_stream_ordering\n                try:\n                    await self._unsafe_process()\n                except Exception:\n                    logger.exception(\"Exception processing notifs\")\n                if self.max_stream_ordering == starting_max_ordering:\n                    break\n        finally:\n            self._is_processing = False\n\n    async def _unsafe_process(self):\n        \"\"\"\n        Looks for unset notifications and dispatch them, in order\n        Never call this directly: use _process which will only allow this to\n        run once per pusher.\n        \"\"\"\n\n        fn = self.store.get_unread_push_actions_for_user_in_range_for_http\n        unprocessed = await fn(\n            self.user_id, self.last_stream_ordering, self.max_stream_ordering\n        )\n\n        logger.info(\n            \"Processing %i unprocessed push actions for %s starting at \"\n            \"stream_ordering %s\",\n            len(unprocessed),\n            self.name,\n            self.last_stream_ordering,\n        )\n\n        for push_action in unprocessed:\n            with opentracing.start_active_span(\n                \"http-push\",\n                tags={\n                    \"authenticated_entity\": self.user_id,\n                    \"event_id\": push_action[\"event_id\"],\n                    \"app_id\": self.app_id,\n                    \"app_display_name\": self.app_display_name,\n                },\n            ):\n                processed = await self._process_one(push_action)\n\n            if processed:\n                http_push_processed_counter.inc()\n                self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n                self.last_stream_ordering = push_action[\"stream_ordering\"]\n                pusher_still_exists = await self.store.update_pusher_last_stream_ordering_and_success(\n                    self.app_id,\n                    self.pushkey,\n                    self.user_id,\n                    self.last_stream_ordering,\n                    self.clock.time_msec(),\n                )\n                if not pusher_still_exists:\n                    # The pusher has been deleted while we were processing, so\n                    # lets just stop and return.\n                    self.on_stop()\n                    return\n\n                if self.failing_since:\n                    self.failing_since = None\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n            else:\n                http_push_failed_counter.inc()\n                if not self.failing_since:\n                    self.failing_since = self.clock.time_msec()\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n\n                if (\n                    self.failing_since\n                    and self.failing_since\n                    < self.clock.time_msec() - HttpPusher.GIVE_UP_AFTER_MS\n                ):\n                    # we really only give up so that if the URL gets\n                    # fixed, we don't suddenly deliver a load\n                    # of old notifications.\n                    logger.warning(\n                        \"Giving up on a notification to user %s, pushkey %s\",\n                        self.user_id,\n                        self.pushkey,\n                    )\n                    self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n                    self.last_stream_ordering = push_action[\"stream_ordering\"]\n                    pusher_still_exists = await self.store.update_pusher_last_stream_ordering(\n                        self.app_id,\n                        self.pushkey,\n                        self.user_id,\n                        self.last_stream_ordering,\n                    )\n                    if not pusher_still_exists:\n                        # The pusher has been deleted while we were processing, so\n                        # lets just stop and return.\n                        self.on_stop()\n                        return\n\n                    self.failing_since = None\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n                else:\n                    logger.info(\"Push failed: delaying for %ds\", self.backoff_delay)\n                    self.timed_call = self.hs.get_reactor().callLater(\n                        self.backoff_delay, self.on_timer\n                    )\n                    self.backoff_delay = min(\n                        self.backoff_delay * 2, self.MAX_BACKOFF_SEC\n                    )\n                    break\n\n    async def _process_one(self, push_action):\n        if \"notify\" not in push_action[\"actions\"]:\n            return True\n\n        tweaks = push_rule_evaluator.tweaks_for_actions(push_action[\"actions\"])\n        badge = await push_tools.get_badge_count(\n            self.hs.get_datastore(),\n            self.user_id,\n            group_by_room=self._group_unread_count_by_room,\n        )\n\n        event = await self.store.get_event(push_action[\"event_id\"], allow_none=True)\n        if event is None:\n            return True  # It's been redacted\n        rejected = await self.dispatch_push(event, tweaks, badge)\n        if rejected is False:\n            return False\n\n        if isinstance(rejected, list) or isinstance(rejected, tuple):\n            for pk in rejected:\n                if pk != self.pushkey:\n                    # for sanity, we only remove the pushkey if it\n                    # was the one we actually sent...\n                    logger.warning(\n                        (\"Ignoring rejected pushkey %s because we didn't send it\"), pk,\n                    )\n                else:\n                    logger.info(\"Pushkey %s was rejected: removing\", pk)\n                    await self.hs.remove_pusher(self.app_id, pk, self.user_id)\n        return True\n\n    async def _build_notification_dict(self, event, tweaks, badge):\n        priority = \"low\"\n        if (\n            event.type == EventTypes.Encrypted\n            or tweaks.get(\"highlight\")\n            or tweaks.get(\"sound\")\n        ):\n            # HACK send our push as high priority only if it generates a sound, highlight\n            #  or may do so (i.e. is encrypted so has unknown effects).\n            priority = \"high\"\n\n        if self.data.get(\"format\") == \"event_id_only\":\n            d = {\n                \"notification\": {\n                    \"event_id\": event.event_id,\n                    \"room_id\": event.room_id,\n                    \"counts\": {\"unread\": badge},\n                    \"prio\": priority,\n                    \"devices\": [\n                        {\n                            \"app_id\": self.app_id,\n                            \"pushkey\": self.pushkey,\n                            \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                            \"data\": self.data_minus_url,\n                        }\n                    ],\n                }\n            }\n            return d\n\n        ctx = await push_tools.get_context_for_event(\n            self.storage, self.state_handler, event, self.user_id\n        )\n\n        d = {\n            \"notification\": {\n                \"id\": event.event_id,  # deprecated: remove soon\n                \"event_id\": event.event_id,\n                \"room_id\": event.room_id,\n                \"type\": event.type,\n                \"sender\": event.user_id,\n                \"prio\": priority,\n                \"counts\": {\n                    \"unread\": badge,\n                    # 'missed_calls': 2\n                },\n                \"devices\": [\n                    {\n                        \"app_id\": self.app_id,\n                        \"pushkey\": self.pushkey,\n                        \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                        \"data\": self.data_minus_url,\n                        \"tweaks\": tweaks,\n                    }\n                ],\n            }\n        }\n        if event.type == \"m.room.member\" and event.is_state():\n            d[\"notification\"][\"membership\"] = event.content[\"membership\"]\n            d[\"notification\"][\"user_is_target\"] = event.state_key == self.user_id\n        if self.hs.config.push_include_content and event.content:\n            d[\"notification\"][\"content\"] = event.content\n\n        # We no longer send aliases separately, instead, we send the human\n        # readable name of the room, which may be an alias.\n        if \"sender_display_name\" in ctx and len(ctx[\"sender_display_name\"]) > 0:\n            d[\"notification\"][\"sender_display_name\"] = ctx[\"sender_display_name\"]\n        if \"name\" in ctx and len(ctx[\"name\"]) > 0:\n            d[\"notification\"][\"room_name\"] = ctx[\"name\"]\n\n        return d\n\n    async def dispatch_push(self, event, tweaks, badge):\n        notification_dict = await self._build_notification_dict(event, tweaks, badge)\n        if not notification_dict:\n            return []\n        try:\n            resp = await self.http_client.post_json_get_json(\n                self.url, notification_dict\n            )\n        except Exception as e:\n            logger.warning(\n                \"Failed to push event %s to %s: %s %s\",\n                event.event_id,\n                self.name,\n                type(e),\n                e,\n            )\n            return False\n        rejected = []\n        if \"rejected\" in resp:\n            rejected = resp[\"rejected\"]\n        return rejected\n\n    async def _send_badge(self, badge):\n        \"\"\"\n        Args:\n            badge (int): number of unread messages\n        \"\"\"\n        logger.debug(\"Sending updated badge count %d to %s\", badge, self.name)\n        d = {\n            \"notification\": {\n                \"id\": \"\",\n                \"type\": None,\n                \"sender\": \"\",\n                \"counts\": {\"unread\": badge},\n                \"devices\": [\n                    {\n                        \"app_id\": self.app_id,\n                        \"pushkey\": self.pushkey,\n                        \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                        \"data\": self.data_minus_url,\n                    }\n                ],\n            }\n        }\n        try:\n            await self.http_client.post_json_get_json(self.url, d)\n            http_badges_processed_counter.inc()\n        except Exception as e:\n            logger.warning(\n                \"Failed to send badge count to %s: %s %s\", self.name, type(e), e\n            )\n            http_badges_failed_counter.inc()\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2015, 2016 OpenMarket Ltd\n# Copyright 2017 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\n\nfrom prometheus_client import Counter\n\nfrom twisted.internet.error import AlreadyCalled, AlreadyCancelled\n\nfrom synapse.api.constants import EventTypes\nfrom synapse.logging import opentracing\nfrom synapse.metrics.background_process_metrics import run_as_background_process\nfrom synapse.push import PusherConfigException\nfrom synapse.types import RoomStreamToken\n\nfrom . import push_rule_evaluator, push_tools\n\nlogger = logging.getLogger(__name__)\n\nhttp_push_processed_counter = Counter(\n    \"synapse_http_httppusher_http_pushes_processed\",\n    \"Number of push notifications successfully sent\",\n)\n\nhttp_push_failed_counter = Counter(\n    \"synapse_http_httppusher_http_pushes_failed\",\n    \"Number of push notifications which failed\",\n)\n\nhttp_badges_processed_counter = Counter(\n    \"synapse_http_httppusher_badge_updates_processed\",\n    \"Number of badge updates successfully sent\",\n)\n\nhttp_badges_failed_counter = Counter(\n    \"synapse_http_httppusher_badge_updates_failed\",\n    \"Number of badge updates which failed\",\n)\n\n\nclass HttpPusher:\n    INITIAL_BACKOFF_SEC = 1  # in seconds because that's what Twisted takes\n    MAX_BACKOFF_SEC = 60 * 60\n\n    # This one's in ms because we compare it against the clock\n    GIVE_UP_AFTER_MS = 24 * 60 * 60 * 1000\n\n    def __init__(self, hs, pusherdict):\n        self.hs = hs\n        self.store = self.hs.get_datastore()\n        self.storage = self.hs.get_storage()\n        self.clock = self.hs.get_clock()\n        self.state_handler = self.hs.get_state_handler()\n        self.user_id = pusherdict[\"user_name\"]\n        self.app_id = pusherdict[\"app_id\"]\n        self.app_display_name = pusherdict[\"app_display_name\"]\n        self.device_display_name = pusherdict[\"device_display_name\"]\n        self.pushkey = pusherdict[\"pushkey\"]\n        self.pushkey_ts = pusherdict[\"ts\"]\n        self.data = pusherdict[\"data\"]\n        self.last_stream_ordering = pusherdict[\"last_stream_ordering\"]\n        self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n        self.failing_since = pusherdict[\"failing_since\"]\n        self.timed_call = None\n        self._is_processing = False\n        self._group_unread_count_by_room = hs.config.push_group_unread_count_by_room\n\n        # This is the highest stream ordering we know it's safe to process.\n        # When new events arrive, we'll be given a window of new events: we\n        # should honour this rather than just looking for anything higher\n        # because of potential out-of-order event serialisation. This starts\n        # off as None though as we don't know any better.\n        self.max_stream_ordering = None\n\n        if \"data\" not in pusherdict:\n            raise PusherConfigException(\"No 'data' key for HTTP pusher\")\n        self.data = pusherdict[\"data\"]\n\n        self.name = \"%s/%s/%s\" % (\n            pusherdict[\"user_name\"],\n            pusherdict[\"app_id\"],\n            pusherdict[\"pushkey\"],\n        )\n\n        if self.data is None:\n            raise PusherConfigException(\"data can not be null for HTTP pusher\")\n\n        if \"url\" not in self.data:\n            raise PusherConfigException(\"'url' required in data for HTTP pusher\")\n        self.url = self.data[\"url\"]\n        self.http_client = hs.get_proxied_http_client()\n        self.data_minus_url = {}\n        self.data_minus_url.update(self.data)\n        del self.data_minus_url[\"url\"]\n\n    def on_started(self, should_check_for_notifs):\n        \"\"\"Called when this pusher has been started.\n\n        Args:\n            should_check_for_notifs (bool): Whether we should immediately\n                check for push to send. Set to False only if it's known there\n                is nothing to send\n        \"\"\"\n        if should_check_for_notifs:\n            self._start_processing()\n\n    def on_new_notifications(self, max_token: RoomStreamToken):\n        # We just use the minimum stream ordering and ignore the vector clock\n        # component. This is safe to do as long as we *always* ignore the vector\n        # clock components.\n        max_stream_ordering = max_token.stream\n\n        self.max_stream_ordering = max(\n            max_stream_ordering, self.max_stream_ordering or 0\n        )\n        self._start_processing()\n\n    def on_new_receipts(self, min_stream_id, max_stream_id):\n        # Note that the min here shouldn't be relied upon to be accurate.\n\n        # We could check the receipts are actually m.read receipts here,\n        # but currently that's the only type of receipt anyway...\n        run_as_background_process(\"http_pusher.on_new_receipts\", self._update_badge)\n\n    async def _update_badge(self):\n        # XXX as per https://github.com/matrix-org/matrix-doc/issues/2627, this seems\n        # to be largely redundant. perhaps we can remove it.\n        badge = await push_tools.get_badge_count(\n            self.hs.get_datastore(),\n            self.user_id,\n            group_by_room=self._group_unread_count_by_room,\n        )\n        await self._send_badge(badge)\n\n    def on_timer(self):\n        self._start_processing()\n\n    def on_stop(self):\n        if self.timed_call:\n            try:\n                self.timed_call.cancel()\n            except (AlreadyCalled, AlreadyCancelled):\n                pass\n            self.timed_call = None\n\n    def _start_processing(self):\n        if self._is_processing:\n            return\n\n        run_as_background_process(\"httppush.process\", self._process)\n\n    async def _process(self):\n        # we should never get here if we are already processing\n        assert not self._is_processing\n\n        try:\n            self._is_processing = True\n            # if the max ordering changes while we're running _unsafe_process,\n            # call it again, and so on until we've caught up.\n            while True:\n                starting_max_ordering = self.max_stream_ordering\n                try:\n                    await self._unsafe_process()\n                except Exception:\n                    logger.exception(\"Exception processing notifs\")\n                if self.max_stream_ordering == starting_max_ordering:\n                    break\n        finally:\n            self._is_processing = False\n\n    async def _unsafe_process(self):\n        \"\"\"\n        Looks for unset notifications and dispatch them, in order\n        Never call this directly: use _process which will only allow this to\n        run once per pusher.\n        \"\"\"\n\n        fn = self.store.get_unread_push_actions_for_user_in_range_for_http\n        unprocessed = await fn(\n            self.user_id, self.last_stream_ordering, self.max_stream_ordering\n        )\n\n        logger.info(\n            \"Processing %i unprocessed push actions for %s starting at \"\n            \"stream_ordering %s\",\n            len(unprocessed),\n            self.name,\n            self.last_stream_ordering,\n        )\n\n        for push_action in unprocessed:\n            with opentracing.start_active_span(\n                \"http-push\",\n                tags={\n                    \"authenticated_entity\": self.user_id,\n                    \"event_id\": push_action[\"event_id\"],\n                    \"app_id\": self.app_id,\n                    \"app_display_name\": self.app_display_name,\n                },\n            ):\n                processed = await self._process_one(push_action)\n\n            if processed:\n                http_push_processed_counter.inc()\n                self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n                self.last_stream_ordering = push_action[\"stream_ordering\"]\n                pusher_still_exists = await self.store.update_pusher_last_stream_ordering_and_success(\n                    self.app_id,\n                    self.pushkey,\n                    self.user_id,\n                    self.last_stream_ordering,\n                    self.clock.time_msec(),\n                )\n                if not pusher_still_exists:\n                    # The pusher has been deleted while we were processing, so\n                    # lets just stop and return.\n                    self.on_stop()\n                    return\n\n                if self.failing_since:\n                    self.failing_since = None\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n            else:\n                http_push_failed_counter.inc()\n                if not self.failing_since:\n                    self.failing_since = self.clock.time_msec()\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n\n                if (\n                    self.failing_since\n                    and self.failing_since\n                    < self.clock.time_msec() - HttpPusher.GIVE_UP_AFTER_MS\n                ):\n                    # we really only give up so that if the URL gets\n                    # fixed, we don't suddenly deliver a load\n                    # of old notifications.\n                    logger.warning(\n                        \"Giving up on a notification to user %s, pushkey %s\",\n                        self.user_id,\n                        self.pushkey,\n                    )\n                    self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n                    self.last_stream_ordering = push_action[\"stream_ordering\"]\n                    pusher_still_exists = await self.store.update_pusher_last_stream_ordering(\n                        self.app_id,\n                        self.pushkey,\n                        self.user_id,\n                        self.last_stream_ordering,\n                    )\n                    if not pusher_still_exists:\n                        # The pusher has been deleted while we were processing, so\n                        # lets just stop and return.\n                        self.on_stop()\n                        return\n\n                    self.failing_since = None\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n                else:\n                    logger.info(\"Push failed: delaying for %ds\", self.backoff_delay)\n                    self.timed_call = self.hs.get_reactor().callLater(\n                        self.backoff_delay, self.on_timer\n                    )\n                    self.backoff_delay = min(\n                        self.backoff_delay * 2, self.MAX_BACKOFF_SEC\n                    )\n                    break\n\n    async def _process_one(self, push_action):\n        if \"notify\" not in push_action[\"actions\"]:\n            return True\n\n        tweaks = push_rule_evaluator.tweaks_for_actions(push_action[\"actions\"])\n        badge = await push_tools.get_badge_count(\n            self.hs.get_datastore(),\n            self.user_id,\n            group_by_room=self._group_unread_count_by_room,\n        )\n\n        event = await self.store.get_event(push_action[\"event_id\"], allow_none=True)\n        if event is None:\n            return True  # It's been redacted\n        rejected = await self.dispatch_push(event, tweaks, badge)\n        if rejected is False:\n            return False\n\n        if isinstance(rejected, list) or isinstance(rejected, tuple):\n            for pk in rejected:\n                if pk != self.pushkey:\n                    # for sanity, we only remove the pushkey if it\n                    # was the one we actually sent...\n                    logger.warning(\n                        (\"Ignoring rejected pushkey %s because we didn't send it\"), pk,\n                    )\n                else:\n                    logger.info(\"Pushkey %s was rejected: removing\", pk)\n                    await self.hs.remove_pusher(self.app_id, pk, self.user_id)\n        return True\n\n    async def _build_notification_dict(self, event, tweaks, badge):\n        priority = \"low\"\n        if (\n            event.type == EventTypes.Encrypted\n            or tweaks.get(\"highlight\")\n            or tweaks.get(\"sound\")\n        ):\n            # HACK send our push as high priority only if it generates a sound, highlight\n            #  or may do so (i.e. is encrypted so has unknown effects).\n            priority = \"high\"\n\n        if self.data.get(\"format\") == \"event_id_only\":\n            d = {\n                \"notification\": {\n                    \"event_id\": event.event_id,\n                    \"room_id\": event.room_id,\n                    \"counts\": {\"unread\": badge},\n                    \"prio\": priority,\n                    \"devices\": [\n                        {\n                            \"app_id\": self.app_id,\n                            \"pushkey\": self.pushkey,\n                            \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                            \"data\": self.data_minus_url,\n                        }\n                    ],\n                }\n            }\n            return d\n\n        ctx = await push_tools.get_context_for_event(\n            self.storage, self.state_handler, event, self.user_id\n        )\n\n        d = {\n            \"notification\": {\n                \"id\": event.event_id,  # deprecated: remove soon\n                \"event_id\": event.event_id,\n                \"room_id\": event.room_id,\n                \"type\": event.type,\n                \"sender\": event.user_id,\n                \"prio\": priority,\n                \"counts\": {\n                    \"unread\": badge,\n                    # 'missed_calls': 2\n                },\n                \"devices\": [\n                    {\n                        \"app_id\": self.app_id,\n                        \"pushkey\": self.pushkey,\n                        \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                        \"data\": self.data_minus_url,\n                        \"tweaks\": tweaks,\n                    }\n                ],\n            }\n        }\n        if event.type == \"m.room.member\" and event.is_state():\n            d[\"notification\"][\"membership\"] = event.content[\"membership\"]\n            d[\"notification\"][\"user_is_target\"] = event.state_key == self.user_id\n        if self.hs.config.push_include_content and event.content:\n            d[\"notification\"][\"content\"] = event.content\n\n        # We no longer send aliases separately, instead, we send the human\n        # readable name of the room, which may be an alias.\n        if \"sender_display_name\" in ctx and len(ctx[\"sender_display_name\"]) > 0:\n            d[\"notification\"][\"sender_display_name\"] = ctx[\"sender_display_name\"]\n        if \"name\" in ctx and len(ctx[\"name\"]) > 0:\n            d[\"notification\"][\"room_name\"] = ctx[\"name\"]\n\n        return d\n\n    async def dispatch_push(self, event, tweaks, badge):\n        notification_dict = await self._build_notification_dict(event, tweaks, badge)\n        if not notification_dict:\n            return []\n        try:\n            resp = await self.http_client.post_json_get_json(\n                self.url, notification_dict\n            )\n        except Exception as e:\n            logger.warning(\n                \"Failed to push event %s to %s: %s %s\",\n                event.event_id,\n                self.name,\n                type(e),\n                e,\n            )\n            return False\n        rejected = []\n        if \"rejected\" in resp:\n            rejected = resp[\"rejected\"]\n        return rejected\n\n    async def _send_badge(self, badge):\n        \"\"\"\n        Args:\n            badge (int): number of unread messages\n        \"\"\"\n        logger.debug(\"Sending updated badge count %d to %s\", badge, self.name)\n        d = {\n            \"notification\": {\n                \"id\": \"\",\n                \"type\": None,\n                \"sender\": \"\",\n                \"counts\": {\"unread\": badge},\n                \"devices\": [\n                    {\n                        \"app_id\": self.app_id,\n                        \"pushkey\": self.pushkey,\n                        \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                        \"data\": self.data_minus_url,\n                    }\n                ],\n            }\n        }\n        try:\n            await self.http_client.post_json_get_json(self.url, d)\n            http_badges_processed_counter.inc()\n        except Exception as e:\n            logger.warning(\n                \"Failed to send badge count to %s: %s %s\", self.name, type(e), e\n            )\n            http_badges_failed_counter.inc()\n", "patch": "@@ -100,7 +100,7 @@ def __init__(self, hs, pusherdict):\n         if \"url\" not in self.data:\n             raise PusherConfigException(\"'url' required in data for HTTP pusher\")\n         self.url = self.data[\"url\"]\n-        self.http_client = hs.get_proxied_http_client()\n+        self.http_client = hs.get_proxied_blacklisted_http_client()\n         self.data_minus_url = {}\n         self.data_minus_url.update(self.data)\n         del self.data_minus_url[\"url\"]", "file_path": "files/2021_2/27", "file_language": "py", "file_name": "synapse/push/httppusher.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class HttpPusher:\n    INITIAL_BACKOFF_SEC = 1  # in seconds because that's what Twisted takes\n    MAX_BACKOFF_SEC = 60 * 60\n\n    # This one's in ms because we compare it against the clock\n    GIVE_UP_AFTER_MS = 24 * 60 * 60 * 1000\n\n    def __init__(self, hs, pusherdict):\n        self.hs = hs\n        self.store = self.hs.get_datastore()\n        self.storage = self.hs.get_storage()\n        self.clock = self.hs.get_clock()\n        self.state_handler = self.hs.get_state_handler()\n        self.user_id = pusherdict[\"user_name\"]\n        self.app_id = pusherdict[\"app_id\"]\n        self.app_display_name = pusherdict[\"app_display_name\"]\n        self.device_display_name = pusherdict[\"device_display_name\"]\n        self.pushkey = pusherdict[\"pushkey\"]\n        self.pushkey_ts = pusherdict[\"ts\"]\n        self.data = pusherdict[\"data\"]\n        self.last_stream_ordering = pusherdict[\"last_stream_ordering\"]\n        self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n        self.failing_since = pusherdict[\"failing_since\"]\n        self.timed_call = None\n        self._is_processing = False\n        self._group_unread_count_by_room = hs.config.push_group_unread_count_by_room\n\n        # This is the highest stream ordering we know it's safe to process.\n        # When new events arrive, we'll be given a window of new events: we\n        # should honour this rather than just looking for anything higher\n        # because of potential out-of-order event serialisation. This starts\n        # off as None though as we don't know any better.\n        self.max_stream_ordering = None\n\n        if \"data\" not in pusherdict:\n            raise PusherConfigException(\"No 'data' key for HTTP pusher\")\n        self.data = pusherdict[\"data\"]\n\n        self.name = \"%s/%s/%s\" % (\n            pusherdict[\"user_name\"],\n            pusherdict[\"app_id\"],\n            pusherdict[\"pushkey\"],\n        )\n\n        if self.data is None:\n            raise PusherConfigException(\"data can not be null for HTTP pusher\")\n\n        if \"url\" not in self.data:\n            raise PusherConfigException(\"'url' required in data for HTTP pusher\")\n        self.url = self.data[\"url\"]\n        self.http_client = hs.get_proxied_http_client()\n        self.data_minus_url = {}\n        self.data_minus_url.update(self.data)\n        del self.data_minus_url[\"url\"]\n\n    def on_started(self, should_check_for_notifs):\n        \"\"\"Called when this pusher has been started.\n\n        Args:\n            should_check_for_notifs (bool): Whether we should immediately\n                check for push to send. Set to False only if it's known there\n                is nothing to send\n        \"\"\"\n        if should_check_for_notifs:\n            self._start_processing()\n\n    def on_new_notifications(self, max_token: RoomStreamToken):\n        # We just use the minimum stream ordering and ignore the vector clock\n        # component. This is safe to do as long as we *always* ignore the vector\n        # clock components.\n        max_stream_ordering = max_token.stream\n\n        self.max_stream_ordering = max(\n            max_stream_ordering, self.max_stream_ordering or 0\n        )\n        self._start_processing()\n\n    def on_new_receipts(self, min_stream_id, max_stream_id):\n        # Note that the min here shouldn't be relied upon to be accurate.\n\n        # We could check the receipts are actually m.read receipts here,\n        # but currently that's the only type of receipt anyway...\n        run_as_background_process(\"http_pusher.on_new_receipts\", self._update_badge)\n\n    async def _update_badge(self):\n        # XXX as per https://github.com/matrix-org/matrix-doc/issues/2627, this seems\n        # to be largely redundant. perhaps we can remove it.\n        badge = await push_tools.get_badge_count(\n            self.hs.get_datastore(),\n            self.user_id,\n            group_by_room=self._group_unread_count_by_room,\n        )\n        await self._send_badge(badge)\n\n    def on_timer(self):\n        self._start_processing()\n\n    def on_stop(self):\n        if self.timed_call:\n            try:\n                self.timed_call.cancel()\n            except (AlreadyCalled, AlreadyCancelled):\n                pass\n            self.timed_call = None\n\n    def _start_processing(self):\n        if self._is_processing:\n            return\n\n        run_as_background_process(\"httppush.process\", self._process)\n\n    async def _process(self):\n        # we should never get here if we are already processing\n        assert not self._is_processing\n\n        try:\n            self._is_processing = True\n            # if the max ordering changes while we're running _unsafe_process,\n            # call it again, and so on until we've caught up.\n            while True:\n                starting_max_ordering = self.max_stream_ordering\n                try:\n                    await self._unsafe_process()\n                except Exception:\n                    logger.exception(\"Exception processing notifs\")\n                if self.max_stream_ordering == starting_max_ordering:\n                    break\n        finally:\n            self._is_processing = False\n\n    async def _unsafe_process(self):\n        \"\"\"\n        Looks for unset notifications and dispatch them, in order\n        Never call this directly: use _process which will only allow this to\n        run once per pusher.\n        \"\"\"\n\n        fn = self.store.get_unread_push_actions_for_user_in_range_for_http\n        unprocessed = await fn(\n            self.user_id, self.last_stream_ordering, self.max_stream_ordering\n        )\n\n        logger.info(\n            \"Processing %i unprocessed push actions for %s starting at \"\n            \"stream_ordering %s\",\n            len(unprocessed),\n            self.name,\n            self.last_stream_ordering,\n        )\n\n        for push_action in unprocessed:\n            with opentracing.start_active_span(\n                \"http-push\",\n                tags={\n                    \"authenticated_entity\": self.user_id,\n                    \"event_id\": push_action[\"event_id\"],\n                    \"app_id\": self.app_id,\n                    \"app_display_name\": self.app_display_name,\n                },\n            ):\n                processed = await self._process_one(push_action)\n\n            if processed:\n                http_push_processed_counter.inc()\n                self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n                self.last_stream_ordering = push_action[\"stream_ordering\"]\n                pusher_still_exists = await self.store.update_pusher_last_stream_ordering_and_success(\n                    self.app_id,\n                    self.pushkey,\n                    self.user_id,\n                    self.last_stream_ordering,\n                    self.clock.time_msec(),\n                )\n                if not pusher_still_exists:\n                    # The pusher has been deleted while we were processing, so\n                    # lets just stop and return.\n                    self.on_stop()\n                    return\n\n                if self.failing_since:\n                    self.failing_since = None\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n            else:\n                http_push_failed_counter.inc()\n                if not self.failing_since:\n                    self.failing_since = self.clock.time_msec()\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n\n                if (\n                    self.failing_since\n                    and self.failing_since\n                    < self.clock.time_msec() - HttpPusher.GIVE_UP_AFTER_MS\n                ):\n                    # we really only give up so that if the URL gets\n                    # fixed, we don't suddenly deliver a load\n                    # of old notifications.\n                    logger.warning(\n                        \"Giving up on a notification to user %s, pushkey %s\",\n                        self.user_id,\n                        self.pushkey,\n                    )\n                    self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n                    self.last_stream_ordering = push_action[\"stream_ordering\"]\n                    pusher_still_exists = await self.store.update_pusher_last_stream_ordering(\n                        self.app_id,\n                        self.pushkey,\n                        self.user_id,\n                        self.last_stream_ordering,\n                    )\n                    if not pusher_still_exists:\n                        # The pusher has been deleted while we were processing, so\n                        # lets just stop and return.\n                        self.on_stop()\n                        return\n\n                    self.failing_since = None\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n                else:\n                    logger.info(\"Push failed: delaying for %ds\", self.backoff_delay)\n                    self.timed_call = self.hs.get_reactor().callLater(\n                        self.backoff_delay, self.on_timer\n                    )\n                    self.backoff_delay = min(\n                        self.backoff_delay * 2, self.MAX_BACKOFF_SEC\n                    )\n                    break\n\n    async def _process_one(self, push_action):\n        if \"notify\" not in push_action[\"actions\"]:\n            return True\n\n        tweaks = push_rule_evaluator.tweaks_for_actions(push_action[\"actions\"])\n        badge = await push_tools.get_badge_count(\n            self.hs.get_datastore(),\n            self.user_id,\n            group_by_room=self._group_unread_count_by_room,\n        )\n\n        event = await self.store.get_event(push_action[\"event_id\"], allow_none=True)\n        if event is None:\n            return True  # It's been redacted\n        rejected = await self.dispatch_push(event, tweaks, badge)\n        if rejected is False:\n            return False\n\n        if isinstance(rejected, list) or isinstance(rejected, tuple):\n            for pk in rejected:\n                if pk != self.pushkey:\n                    # for sanity, we only remove the pushkey if it\n                    # was the one we actually sent...\n                    logger.warning(\n                        (\"Ignoring rejected pushkey %s because we didn't send it\"), pk,\n                    )\n                else:\n                    logger.info(\"Pushkey %s was rejected: removing\", pk)\n                    await self.hs.remove_pusher(self.app_id, pk, self.user_id)\n        return True\n\n    async def _build_notification_dict(self, event, tweaks, badge):\n        priority = \"low\"\n        if (\n            event.type == EventTypes.Encrypted\n            or tweaks.get(\"highlight\")\n            or tweaks.get(\"sound\")\n        ):\n            # HACK send our push as high priority only if it generates a sound, highlight\n            #  or may do so (i.e. is encrypted so has unknown effects).\n            priority = \"high\"\n\n        if self.data.get(\"format\") == \"event_id_only\":\n            d = {\n                \"notification\": {\n                    \"event_id\": event.event_id,\n                    \"room_id\": event.room_id,\n                    \"counts\": {\"unread\": badge},\n                    \"prio\": priority,\n                    \"devices\": [\n                        {\n                            \"app_id\": self.app_id,\n                            \"pushkey\": self.pushkey,\n                            \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                            \"data\": self.data_minus_url,\n                        }\n                    ],\n                }\n            }\n            return d\n\n        ctx = await push_tools.get_context_for_event(\n            self.storage, self.state_handler, event, self.user_id\n        )\n\n        d = {\n            \"notification\": {\n                \"id\": event.event_id,  # deprecated: remove soon\n                \"event_id\": event.event_id,\n                \"room_id\": event.room_id,\n                \"type\": event.type,\n                \"sender\": event.user_id,\n                \"prio\": priority,\n                \"counts\": {\n                    \"unread\": badge,\n                    # 'missed_calls': 2\n                },\n                \"devices\": [\n                    {\n                        \"app_id\": self.app_id,\n                        \"pushkey\": self.pushkey,\n                        \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                        \"data\": self.data_minus_url,\n                        \"tweaks\": tweaks,\n                    }\n                ],\n            }\n        }\n        if event.type == \"m.room.member\" and event.is_state():\n            d[\"notification\"][\"membership\"] = event.content[\"membership\"]\n            d[\"notification\"][\"user_is_target\"] = event.state_key == self.user_id\n        if self.hs.config.push_include_content and event.content:\n            d[\"notification\"][\"content\"] = event.content\n\n        # We no longer send aliases separately, instead, we send the human\n        # readable name of the room, which may be an alias.\n        if \"sender_display_name\" in ctx and len(ctx[\"sender_display_name\"]) > 0:\n            d[\"notification\"][\"sender_display_name\"] = ctx[\"sender_display_name\"]\n        if \"name\" in ctx and len(ctx[\"name\"]) > 0:\n            d[\"notification\"][\"room_name\"] = ctx[\"name\"]\n\n        return d\n\n    async def dispatch_push(self, event, tweaks, badge):\n        notification_dict = await self._build_notification_dict(event, tweaks, badge)\n        if not notification_dict:\n            return []\n        try:\n            resp = await self.http_client.post_json_get_json(\n                self.url, notification_dict\n            )\n        except Exception as e:\n            logger.warning(\n                \"Failed to push event %s to %s: %s %s\",\n                event.event_id,\n                self.name,\n                type(e),\n                e,\n            )\n            return False\n        rejected = []\n        if \"rejected\" in resp:\n            rejected = resp[\"rejected\"]\n        return rejected\n\n    async def _send_badge(self, badge):\n        \"\"\"\n        Args:\n            badge (int): number of unread messages\n        \"\"\"\n        logger.debug(\"Sending updated badge count %d to %s\", badge, self.name)\n        d = {\n            \"notification\": {\n                \"id\": \"\",\n                \"type\": None,\n                \"sender\": \"\",\n                \"counts\": {\"unread\": badge},\n                \"devices\": [\n                    {\n                        \"app_id\": self.app_id,\n                        \"pushkey\": self.pushkey,\n                        \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                        \"data\": self.data_minus_url,\n                    }\n                ],\n            }\n        }\n        try:\n            await self.http_client.post_json_get_json(self.url, d)\n            http_badges_processed_counter.inc()\n        except Exception as e:\n            logger.warning(\n                \"Failed to send badge count to %s: %s %s\", self.name, type(e), e\n            )\n            http_badges_failed_counter.inc()", "target": 0}], "function_after": [{"function": "class HttpPusher:\n    INITIAL_BACKOFF_SEC = 1  # in seconds because that's what Twisted takes\n    MAX_BACKOFF_SEC = 60 * 60\n\n    # This one's in ms because we compare it against the clock\n    GIVE_UP_AFTER_MS = 24 * 60 * 60 * 1000\n\n    def __init__(self, hs, pusherdict):\n        self.hs = hs\n        self.store = self.hs.get_datastore()\n        self.storage = self.hs.get_storage()\n        self.clock = self.hs.get_clock()\n        self.state_handler = self.hs.get_state_handler()\n        self.user_id = pusherdict[\"user_name\"]\n        self.app_id = pusherdict[\"app_id\"]\n        self.app_display_name = pusherdict[\"app_display_name\"]\n        self.device_display_name = pusherdict[\"device_display_name\"]\n        self.pushkey = pusherdict[\"pushkey\"]\n        self.pushkey_ts = pusherdict[\"ts\"]\n        self.data = pusherdict[\"data\"]\n        self.last_stream_ordering = pusherdict[\"last_stream_ordering\"]\n        self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n        self.failing_since = pusherdict[\"failing_since\"]\n        self.timed_call = None\n        self._is_processing = False\n        self._group_unread_count_by_room = hs.config.push_group_unread_count_by_room\n\n        # This is the highest stream ordering we know it's safe to process.\n        # When new events arrive, we'll be given a window of new events: we\n        # should honour this rather than just looking for anything higher\n        # because of potential out-of-order event serialisation. This starts\n        # off as None though as we don't know any better.\n        self.max_stream_ordering = None\n\n        if \"data\" not in pusherdict:\n            raise PusherConfigException(\"No 'data' key for HTTP pusher\")\n        self.data = pusherdict[\"data\"]\n\n        self.name = \"%s/%s/%s\" % (\n            pusherdict[\"user_name\"],\n            pusherdict[\"app_id\"],\n            pusherdict[\"pushkey\"],\n        )\n\n        if self.data is None:\n            raise PusherConfigException(\"data can not be null for HTTP pusher\")\n\n        if \"url\" not in self.data:\n            raise PusherConfigException(\"'url' required in data for HTTP pusher\")\n        self.url = self.data[\"url\"]\n        self.http_client = hs.get_proxied_blacklisted_http_client()\n        self.data_minus_url = {}\n        self.data_minus_url.update(self.data)\n        del self.data_minus_url[\"url\"]\n\n    def on_started(self, should_check_for_notifs):\n        \"\"\"Called when this pusher has been started.\n\n        Args:\n            should_check_for_notifs (bool): Whether we should immediately\n                check for push to send. Set to False only if it's known there\n                is nothing to send\n        \"\"\"\n        if should_check_for_notifs:\n            self._start_processing()\n\n    def on_new_notifications(self, max_token: RoomStreamToken):\n        # We just use the minimum stream ordering and ignore the vector clock\n        # component. This is safe to do as long as we *always* ignore the vector\n        # clock components.\n        max_stream_ordering = max_token.stream\n\n        self.max_stream_ordering = max(\n            max_stream_ordering, self.max_stream_ordering or 0\n        )\n        self._start_processing()\n\n    def on_new_receipts(self, min_stream_id, max_stream_id):\n        # Note that the min here shouldn't be relied upon to be accurate.\n\n        # We could check the receipts are actually m.read receipts here,\n        # but currently that's the only type of receipt anyway...\n        run_as_background_process(\"http_pusher.on_new_receipts\", self._update_badge)\n\n    async def _update_badge(self):\n        # XXX as per https://github.com/matrix-org/matrix-doc/issues/2627, this seems\n        # to be largely redundant. perhaps we can remove it.\n        badge = await push_tools.get_badge_count(\n            self.hs.get_datastore(),\n            self.user_id,\n            group_by_room=self._group_unread_count_by_room,\n        )\n        await self._send_badge(badge)\n\n    def on_timer(self):\n        self._start_processing()\n\n    def on_stop(self):\n        if self.timed_call:\n            try:\n                self.timed_call.cancel()\n            except (AlreadyCalled, AlreadyCancelled):\n                pass\n            self.timed_call = None\n\n    def _start_processing(self):\n        if self._is_processing:\n            return\n\n        run_as_background_process(\"httppush.process\", self._process)\n\n    async def _process(self):\n        # we should never get here if we are already processing\n        assert not self._is_processing\n\n        try:\n            self._is_processing = True\n            # if the max ordering changes while we're running _unsafe_process,\n            # call it again, and so on until we've caught up.\n            while True:\n                starting_max_ordering = self.max_stream_ordering\n                try:\n                    await self._unsafe_process()\n                except Exception:\n                    logger.exception(\"Exception processing notifs\")\n                if self.max_stream_ordering == starting_max_ordering:\n                    break\n        finally:\n            self._is_processing = False\n\n    async def _unsafe_process(self):\n        \"\"\"\n        Looks for unset notifications and dispatch them, in order\n        Never call this directly: use _process which will only allow this to\n        run once per pusher.\n        \"\"\"\n\n        fn = self.store.get_unread_push_actions_for_user_in_range_for_http\n        unprocessed = await fn(\n            self.user_id, self.last_stream_ordering, self.max_stream_ordering\n        )\n\n        logger.info(\n            \"Processing %i unprocessed push actions for %s starting at \"\n            \"stream_ordering %s\",\n            len(unprocessed),\n            self.name,\n            self.last_stream_ordering,\n        )\n\n        for push_action in unprocessed:\n            with opentracing.start_active_span(\n                \"http-push\",\n                tags={\n                    \"authenticated_entity\": self.user_id,\n                    \"event_id\": push_action[\"event_id\"],\n                    \"app_id\": self.app_id,\n                    \"app_display_name\": self.app_display_name,\n                },\n            ):\n                processed = await self._process_one(push_action)\n\n            if processed:\n                http_push_processed_counter.inc()\n                self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n                self.last_stream_ordering = push_action[\"stream_ordering\"]\n                pusher_still_exists = await self.store.update_pusher_last_stream_ordering_and_success(\n                    self.app_id,\n                    self.pushkey,\n                    self.user_id,\n                    self.last_stream_ordering,\n                    self.clock.time_msec(),\n                )\n                if not pusher_still_exists:\n                    # The pusher has been deleted while we were processing, so\n                    # lets just stop and return.\n                    self.on_stop()\n                    return\n\n                if self.failing_since:\n                    self.failing_since = None\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n            else:\n                http_push_failed_counter.inc()\n                if not self.failing_since:\n                    self.failing_since = self.clock.time_msec()\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n\n                if (\n                    self.failing_since\n                    and self.failing_since\n                    < self.clock.time_msec() - HttpPusher.GIVE_UP_AFTER_MS\n                ):\n                    # we really only give up so that if the URL gets\n                    # fixed, we don't suddenly deliver a load\n                    # of old notifications.\n                    logger.warning(\n                        \"Giving up on a notification to user %s, pushkey %s\",\n                        self.user_id,\n                        self.pushkey,\n                    )\n                    self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n                    self.last_stream_ordering = push_action[\"stream_ordering\"]\n                    pusher_still_exists = await self.store.update_pusher_last_stream_ordering(\n                        self.app_id,\n                        self.pushkey,\n                        self.user_id,\n                        self.last_stream_ordering,\n                    )\n                    if not pusher_still_exists:\n                        # The pusher has been deleted while we were processing, so\n                        # lets just stop and return.\n                        self.on_stop()\n                        return\n\n                    self.failing_since = None\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n                else:\n                    logger.info(\"Push failed: delaying for %ds\", self.backoff_delay)\n                    self.timed_call = self.hs.get_reactor().callLater(\n                        self.backoff_delay, self.on_timer\n                    )\n                    self.backoff_delay = min(\n                        self.backoff_delay * 2, self.MAX_BACKOFF_SEC\n                    )\n                    break\n\n    async def _process_one(self, push_action):\n        if \"notify\" not in push_action[\"actions\"]:\n            return True\n\n        tweaks = push_rule_evaluator.tweaks_for_actions(push_action[\"actions\"])\n        badge = await push_tools.get_badge_count(\n            self.hs.get_datastore(),\n            self.user_id,\n            group_by_room=self._group_unread_count_by_room,\n        )\n\n        event = await self.store.get_event(push_action[\"event_id\"], allow_none=True)\n        if event is None:\n            return True  # It's been redacted\n        rejected = await self.dispatch_push(event, tweaks, badge)\n        if rejected is False:\n            return False\n\n        if isinstance(rejected, list) or isinstance(rejected, tuple):\n            for pk in rejected:\n                if pk != self.pushkey:\n                    # for sanity, we only remove the pushkey if it\n                    # was the one we actually sent...\n                    logger.warning(\n                        (\"Ignoring rejected pushkey %s because we didn't send it\"), pk,\n                    )\n                else:\n                    logger.info(\"Pushkey %s was rejected: removing\", pk)\n                    await self.hs.remove_pusher(self.app_id, pk, self.user_id)\n        return True\n\n    async def _build_notification_dict(self, event, tweaks, badge):\n        priority = \"low\"\n        if (\n            event.type == EventTypes.Encrypted\n            or tweaks.get(\"highlight\")\n            or tweaks.get(\"sound\")\n        ):\n            # HACK send our push as high priority only if it generates a sound, highlight\n            #  or may do so (i.e. is encrypted so has unknown effects).\n            priority = \"high\"\n\n        if self.data.get(\"format\") == \"event_id_only\":\n            d = {\n                \"notification\": {\n                    \"event_id\": event.event_id,\n                    \"room_id\": event.room_id,\n                    \"counts\": {\"unread\": badge},\n                    \"prio\": priority,\n                    \"devices\": [\n                        {\n                            \"app_id\": self.app_id,\n                            \"pushkey\": self.pushkey,\n                            \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                            \"data\": self.data_minus_url,\n                        }\n                    ],\n                }\n            }\n            return d\n\n        ctx = await push_tools.get_context_for_event(\n            self.storage, self.state_handler, event, self.user_id\n        )\n\n        d = {\n            \"notification\": {\n                \"id\": event.event_id,  # deprecated: remove soon\n                \"event_id\": event.event_id,\n                \"room_id\": event.room_id,\n                \"type\": event.type,\n                \"sender\": event.user_id,\n                \"prio\": priority,\n                \"counts\": {\n                    \"unread\": badge,\n                    # 'missed_calls': 2\n                },\n                \"devices\": [\n                    {\n                        \"app_id\": self.app_id,\n                        \"pushkey\": self.pushkey,\n                        \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                        \"data\": self.data_minus_url,\n                        \"tweaks\": tweaks,\n                    }\n                ],\n            }\n        }\n        if event.type == \"m.room.member\" and event.is_state():\n            d[\"notification\"][\"membership\"] = event.content[\"membership\"]\n            d[\"notification\"][\"user_is_target\"] = event.state_key == self.user_id\n        if self.hs.config.push_include_content and event.content:\n            d[\"notification\"][\"content\"] = event.content\n\n        # We no longer send aliases separately, instead, we send the human\n        # readable name of the room, which may be an alias.\n        if \"sender_display_name\" in ctx and len(ctx[\"sender_display_name\"]) > 0:\n            d[\"notification\"][\"sender_display_name\"] = ctx[\"sender_display_name\"]\n        if \"name\" in ctx and len(ctx[\"name\"]) > 0:\n            d[\"notification\"][\"room_name\"] = ctx[\"name\"]\n\n        return d\n\n    async def dispatch_push(self, event, tweaks, badge):\n        notification_dict = await self._build_notification_dict(event, tweaks, badge)\n        if not notification_dict:\n            return []\n        try:\n            resp = await self.http_client.post_json_get_json(\n                self.url, notification_dict\n            )\n        except Exception as e:\n            logger.warning(\n                \"Failed to push event %s to %s: %s %s\",\n                event.event_id,\n                self.name,\n                type(e),\n                e,\n            )\n            return False\n        rejected = []\n        if \"rejected\" in resp:\n            rejected = resp[\"rejected\"]\n        return rejected\n\n    async def _send_badge(self, badge):\n        \"\"\"\n        Args:\n            badge (int): number of unread messages\n        \"\"\"\n        logger.debug(\"Sending updated badge count %d to %s\", badge, self.name)\n        d = {\n            \"notification\": {\n                \"id\": \"\",\n                \"type\": None,\n                \"sender\": \"\",\n                \"counts\": {\"unread\": badge},\n                \"devices\": [\n                    {\n                        \"app_id\": self.app_id,\n                        \"pushkey\": self.pushkey,\n                        \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                        \"data\": self.data_minus_url,\n                    }\n                ],\n            }\n        }\n        try:\n            await self.http_client.post_json_get_json(self.url, d)\n            http_badges_processed_counter.inc()\n        except Exception as e:\n            logger.warning(\n                \"Failed to send badge count to %s: %s %s\", self.name, type(e), e\n            )\n            http_badges_failed_counter.inc()", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Frest%2Fmedia%2Fv1%2Fmedia_repository.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport errno\nimport logging\nimport os\nimport shutil\nfrom typing import IO, Dict, List, Optional, Tuple\n\nimport twisted.internet.error\nimport twisted.web.http\nfrom twisted.web.http import Request\nfrom twisted.web.resource import Resource\n\nfrom synapse.api.errors import (\n    FederationDeniedError,\n    HttpResponseException,\n    NotFoundError,\n    RequestSendFailed,\n    SynapseError,\n)\nfrom synapse.config._base import ConfigError\nfrom synapse.logging.context import defer_to_thread\nfrom synapse.metrics.background_process_metrics import run_as_background_process\nfrom synapse.util.async_helpers import Linearizer\nfrom synapse.util.retryutils import NotRetryingDestination\nfrom synapse.util.stringutils import random_string\n\nfrom ._base import (\n    FileInfo,\n    Responder,\n    get_filename_from_headers,\n    respond_404,\n    respond_with_responder,\n)\nfrom .config_resource import MediaConfigResource\nfrom .download_resource import DownloadResource\nfrom .filepath import MediaFilePaths\nfrom .media_storage import MediaStorage\nfrom .preview_url_resource import PreviewUrlResource\nfrom .storage_provider import StorageProviderWrapper\nfrom .thumbnail_resource import ThumbnailResource\nfrom .thumbnailer import Thumbnailer, ThumbnailError\nfrom .upload_resource import UploadResource\n\nlogger = logging.getLogger(__name__)\n\n\nUPDATE_RECENTLY_ACCESSED_TS = 60 * 1000\n\n\nclass MediaRepository:\n    def __init__(self, hs):\n        self.hs = hs\n        self.auth = hs.get_auth()\n        self.client = hs.get_federation_http_client()\n        self.clock = hs.get_clock()\n        self.server_name = hs.hostname\n        self.store = hs.get_datastore()\n        self.max_upload_size = hs.config.max_upload_size\n        self.max_image_pixels = hs.config.max_image_pixels\n\n        self.primary_base_path = hs.config.media_store_path\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n\n        self.dynamic_thumbnails = hs.config.dynamic_thumbnails\n        self.thumbnail_requirements = hs.config.thumbnail_requirements\n\n        self.remote_media_linearizer = Linearizer(name=\"media_remote\")\n\n        self.recently_accessed_remotes = set()\n        self.recently_accessed_locals = set()\n\n        self.federation_domain_whitelist = hs.config.federation_domain_whitelist\n\n        # List of StorageProviders where we should search for media and\n        # potentially upload to.\n        storage_providers = []\n\n        for clz, provider_config, wrapper_config in hs.config.media_storage_providers:\n            backend = clz(hs, provider_config)\n            provider = StorageProviderWrapper(\n                backend,\n                store_local=wrapper_config.store_local,\n                store_remote=wrapper_config.store_remote,\n                store_synchronous=wrapper_config.store_synchronous,\n            )\n            storage_providers.append(provider)\n\n        self.media_storage = MediaStorage(\n            self.hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n\n        self.clock.looping_call(\n            self._start_update_recently_accessed, UPDATE_RECENTLY_ACCESSED_TS\n        )\n\n    def _start_update_recently_accessed(self):\n        return run_as_background_process(\n            \"update_recently_accessed_media\", self._update_recently_accessed\n        )\n\n    async def _update_recently_accessed(self):\n        remote_media = self.recently_accessed_remotes\n        self.recently_accessed_remotes = set()\n\n        local_media = self.recently_accessed_locals\n        self.recently_accessed_locals = set()\n\n        await self.store.update_cached_last_access_time(\n            local_media, remote_media, self.clock.time_msec()\n        )\n\n    def mark_recently_accessed(self, server_name, media_id):\n        \"\"\"Mark the given media as recently accessed.\n\n        Args:\n            server_name (str|None): Origin server of media, or None if local\n            media_id (str): The media ID of the content\n        \"\"\"\n        if server_name:\n            self.recently_accessed_remotes.add((server_name, media_id))\n        else:\n            self.recently_accessed_locals.add(media_id)\n\n    async def create_content(\n        self,\n        media_type: str,\n        upload_name: Optional[str],\n        content: IO,\n        content_length: int,\n        auth_user: str,\n    ) -> str:\n        \"\"\"Store uploaded content for a local user and return the mxc URL\n\n        Args:\n            media_type: The content type of the file.\n            upload_name: The name of the file, if provided.\n            content: A file like object that is the content to store\n            content_length: The length of the content\n            auth_user: The user_id of the uploader\n\n        Returns:\n            The mxc url of the stored content\n        \"\"\"\n\n        media_id = random_string(24)\n\n        file_info = FileInfo(server_name=None, file_id=media_id)\n\n        fname = await self.media_storage.store_file(content, file_info)\n\n        logger.info(\"Stored local media in file %r\", fname)\n\n        await self.store.store_local_media(\n            media_id=media_id,\n            media_type=media_type,\n            time_now_ms=self.clock.time_msec(),\n            upload_name=upload_name,\n            media_length=content_length,\n            user_id=auth_user,\n        )\n\n        await self._generate_thumbnails(None, media_id, media_id, media_type)\n\n        return \"mxc://%s/%s\" % (self.server_name, media_id)\n\n    async def get_local_media(\n        self, request: Request, media_id: str, name: Optional[str]\n    ) -> None:\n        \"\"\"Responds to reqests for local media, if exists, or returns 404.\n\n        Args:\n            request: The incoming request.\n            media_id: The media ID of the content. (This is the same as\n                the file_id for local content.)\n            name: Optional name that, if specified, will be used as\n                the filename in the Content-Disposition header of the response.\n\n        Returns:\n            Resolves once a response has successfully been written to request\n        \"\"\"\n        media_info = await self.store.get_local_media(media_id)\n        if not media_info or media_info[\"quarantined_by\"]:\n            respond_404(request)\n            return\n\n        self.mark_recently_accessed(None, media_id)\n\n        media_type = media_info[\"media_type\"]\n        media_length = media_info[\"media_length\"]\n        upload_name = name if name else media_info[\"upload_name\"]\n        url_cache = media_info[\"url_cache\"]\n\n        file_info = FileInfo(None, media_id, url_cache=url_cache)\n\n        responder = await self.media_storage.fetch_media(file_info)\n        await respond_with_responder(\n            request, responder, media_type, media_length, upload_name\n        )\n\n    async def get_remote_media(\n        self, request: Request, server_name: str, media_id: str, name: Optional[str]\n    ) -> None:\n        \"\"\"Respond to requests for remote media.\n\n        Args:\n            request: The incoming request.\n            server_name: Remote server_name where the media originated.\n            media_id: The media ID of the content (as defined by the remote server).\n            name: Optional name that, if specified, will be used as\n                the filename in the Content-Disposition header of the response.\n\n        Returns:\n            Resolves once a response has successfully been written to request\n        \"\"\"\n        if (\n            self.federation_domain_whitelist is not None\n            and server_name not in self.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(server_name)\n\n        self.mark_recently_accessed(server_name, media_id)\n\n        # We linearize here to ensure that we don't try and download remote\n        # media multiple times concurrently\n        key = (server_name, media_id)\n        with (await self.remote_media_linearizer.queue(key)):\n            responder, media_info = await self._get_remote_media_impl(\n                server_name, media_id\n            )\n\n        # We deliberately stream the file outside the lock\n        if responder:\n            media_type = media_info[\"media_type\"]\n            media_length = media_info[\"media_length\"]\n            upload_name = name if name else media_info[\"upload_name\"]\n            await respond_with_responder(\n                request, responder, media_type, media_length, upload_name\n            )\n        else:\n            respond_404(request)\n\n    async def get_remote_media_info(self, server_name: str, media_id: str) -> dict:\n        \"\"\"Gets the media info associated with the remote file, downloading\n        if necessary.\n\n        Args:\n            server_name: Remote server_name where the media originated.\n            media_id: The media ID of the content (as defined by the remote server).\n\n        Returns:\n            The media info of the file\n        \"\"\"\n        if (\n            self.federation_domain_whitelist is not None\n            and server_name not in self.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(server_name)\n\n        # We linearize here to ensure that we don't try and download remote\n        # media multiple times concurrently\n        key = (server_name, media_id)\n        with (await self.remote_media_linearizer.queue(key)):\n            responder, media_info = await self._get_remote_media_impl(\n                server_name, media_id\n            )\n\n        # Ensure we actually use the responder so that it releases resources\n        if responder:\n            with responder:\n                pass\n\n        return media_info\n\n    async def _get_remote_media_impl(\n        self, server_name: str, media_id: str\n    ) -> Tuple[Optional[Responder], dict]:\n        \"\"\"Looks for media in local cache, if not there then attempt to\n        download from remote server.\n\n        Args:\n            server_name (str): Remote server_name where the media originated.\n            media_id (str): The media ID of the content (as defined by the\n                remote server).\n\n        Returns:\n            A tuple of responder and the media info of the file.\n        \"\"\"\n        media_info = await self.store.get_cached_remote_media(server_name, media_id)\n\n        # file_id is the ID we use to track the file locally. If we've already\n        # seen the file then reuse the existing ID, otherwise genereate a new\n        # one.\n\n        # If we have an entry in the DB, try and look for it\n        if media_info:\n            file_id = media_info[\"filesystem_id\"]\n            file_info = FileInfo(server_name, file_id)\n\n            if media_info[\"quarantined_by\"]:\n                logger.info(\"Media is quarantined\")\n                raise NotFoundError()\n\n            responder = await self.media_storage.fetch_media(file_info)\n            if responder:\n                return responder, media_info\n\n        # Failed to find the file anywhere, lets download it.\n\n        try:\n            media_info = await self._download_remote_file(server_name, media_id,)\n        except SynapseError:\n            raise\n        except Exception as e:\n            # An exception may be because we downloaded media in another\n            # process, so let's check if we magically have the media.\n            media_info = await self.store.get_cached_remote_media(server_name, media_id)\n            if not media_info:\n                raise e\n\n        file_id = media_info[\"filesystem_id\"]\n        file_info = FileInfo(server_name, file_id)\n\n        # We generate thumbnails even if another process downloaded the media\n        # as a) it's conceivable that the other download request dies before it\n        # generates thumbnails, but mainly b) we want to be sure the thumbnails\n        # have finished being generated before responding to the client,\n        # otherwise they'll request thumbnails and get a 404 if they're not\n        # ready yet.\n        await self._generate_thumbnails(\n            server_name, media_id, file_id, media_info[\"media_type\"]\n        )\n\n        responder = await self.media_storage.fetch_media(file_info)\n        return responder, media_info\n\n    async def _download_remote_file(self, server_name: str, media_id: str,) -> dict:\n        \"\"\"Attempt to download the remote file from the given server name,\n        using the given file_id as the local id.\n\n        Args:\n            server_name: Originating server\n            media_id: The media ID of the content (as defined by the\n                remote server). This is different than the file_id, which is\n                locally generated.\n            file_id: Local file ID\n\n        Returns:\n            The media info of the file.\n        \"\"\"\n\n        file_id = random_string(24)\n\n        file_info = FileInfo(server_name=server_name, file_id=file_id)\n\n        with self.media_storage.store_into_file(file_info) as (f, fname, finish):\n            request_path = \"/\".join(\n                (\"/_matrix/media/r0/download\", server_name, media_id)\n            )\n            try:\n                length, headers = await self.client.get_file(\n                    server_name,\n                    request_path,\n                    output_stream=f,\n                    max_size=self.max_upload_size,\n                    args={\n                        # tell the remote server to 404 if it doesn't\n                        # recognise the server_name, to make sure we don't\n                        # end up with a routing loop.\n                        \"allow_remote\": \"false\"\n                    },\n                )\n            except RequestSendFailed as e:\n                logger.warning(\n                    \"Request failed fetching remote media %s/%s: %r\",\n                    server_name,\n                    media_id,\n                    e,\n                )\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            except HttpResponseException as e:\n                logger.warning(\n                    \"HTTP error fetching remote media %s/%s: %s\",\n                    server_name,\n                    media_id,\n                    e.response,\n                )\n                if e.code == twisted.web.http.NOT_FOUND:\n                    raise e.to_synapse_error()\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            except SynapseError:\n                logger.warning(\n                    \"Failed to fetch remote media %s/%s\", server_name, media_id\n                )\n                raise\n            except NotRetryingDestination:\n                logger.warning(\"Not retrying destination %r\", server_name)\n                raise SynapseError(502, \"Failed to fetch remote media\")\n            except Exception:\n                logger.exception(\n                    \"Failed to fetch remote media %s/%s\", server_name, media_id\n                )\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            await finish()\n\n            media_type = headers[b\"Content-Type\"][0].decode(\"ascii\")\n            upload_name = get_filename_from_headers(headers)\n            time_now_ms = self.clock.time_msec()\n\n            # Multiple remote media download requests can race (when using\n            # multiple media repos), so this may throw a violation constraint\n            # exception. If it does we'll delete the newly downloaded file from\n            # disk (as we're in the ctx manager).\n            #\n            # However: we've already called `finish()` so we may have also\n            # written to the storage providers. This is preferable to the\n            # alternative where we call `finish()` *after* this, where we could\n            # end up having an entry in the DB but fail to write the files to\n            # the storage providers.\n            await self.store.store_cached_remote_media(\n                origin=server_name,\n                media_id=media_id,\n                media_type=media_type,\n                time_now_ms=self.clock.time_msec(),\n                upload_name=upload_name,\n                media_length=length,\n                filesystem_id=file_id,\n            )\n\n        logger.info(\"Stored remote media in file %r\", fname)\n\n        media_info = {\n            \"media_type\": media_type,\n            \"media_length\": length,\n            \"upload_name\": upload_name,\n            \"created_ts\": time_now_ms,\n            \"filesystem_id\": file_id,\n        }\n\n        return media_info\n\n    def _get_thumbnail_requirements(self, media_type):\n        return self.thumbnail_requirements.get(media_type, ())\n\n    def _generate_thumbnail(self, thumbnailer, t_width, t_height, t_method, t_type):\n        m_width = thumbnailer.width\n        m_height = thumbnailer.height\n\n        if m_width * m_height >= self.max_image_pixels:\n            logger.info(\n                \"Image too large to thumbnail %r x %r > %r\",\n                m_width,\n                m_height,\n                self.max_image_pixels,\n            )\n            return\n\n        if thumbnailer.transpose_method is not None:\n            m_width, m_height = thumbnailer.transpose()\n\n        if t_method == \"crop\":\n            t_byte_source = thumbnailer.crop(t_width, t_height, t_type)\n        elif t_method == \"scale\":\n            t_width, t_height = thumbnailer.aspect(t_width, t_height)\n            t_width = min(m_width, t_width)\n            t_height = min(m_height, t_height)\n            t_byte_source = thumbnailer.scale(t_width, t_height, t_type)\n        else:\n            t_byte_source = None\n\n        return t_byte_source\n\n    async def generate_local_exact_thumbnail(\n        self,\n        media_id: str,\n        t_width: int,\n        t_height: int,\n        t_method: str,\n        t_type: str,\n        url_cache: str,\n    ) -> Optional[str]:\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(None, media_id, url_cache=url_cache)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate a thumbnail for local media %s using a method of %s and type of %s: %s\",\n                media_id,\n                t_method,\n                t_type,\n                e,\n            )\n            return None\n\n        t_byte_source = await defer_to_thread(\n            self.hs.get_reactor(),\n            self._generate_thumbnail,\n            thumbnailer,\n            t_width,\n            t_height,\n            t_method,\n            t_type,\n        )\n\n        if t_byte_source:\n            try:\n                file_info = FileInfo(\n                    server_name=None,\n                    file_id=media_id,\n                    url_cache=url_cache,\n                    thumbnail=True,\n                    thumbnail_width=t_width,\n                    thumbnail_height=t_height,\n                    thumbnail_method=t_method,\n                    thumbnail_type=t_type,\n                )\n\n                output_path = await self.media_storage.store_file(\n                    t_byte_source, file_info\n                )\n            finally:\n                t_byte_source.close()\n\n            logger.info(\"Stored thumbnail in file %r\", output_path)\n\n            t_len = os.path.getsize(output_path)\n\n            await self.store.store_local_thumbnail(\n                media_id, t_width, t_height, t_type, t_method, t_len\n            )\n\n            return output_path\n\n        # Could not generate thumbnail.\n        return None\n\n    async def generate_remote_exact_thumbnail(\n        self,\n        server_name: str,\n        file_id: str,\n        media_id: str,\n        t_width: int,\n        t_height: int,\n        t_method: str,\n        t_type: str,\n    ) -> Optional[str]:\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(server_name, file_id, url_cache=False)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate a thumbnail for remote media %s from %s using a method of %s and type of %s: %s\",\n                media_id,\n                server_name,\n                t_method,\n                t_type,\n                e,\n            )\n            return None\n\n        t_byte_source = await defer_to_thread(\n            self.hs.get_reactor(),\n            self._generate_thumbnail,\n            thumbnailer,\n            t_width,\n            t_height,\n            t_method,\n            t_type,\n        )\n\n        if t_byte_source:\n            try:\n                file_info = FileInfo(\n                    server_name=server_name,\n                    file_id=file_id,\n                    thumbnail=True,\n                    thumbnail_width=t_width,\n                    thumbnail_height=t_height,\n                    thumbnail_method=t_method,\n                    thumbnail_type=t_type,\n                )\n\n                output_path = await self.media_storage.store_file(\n                    t_byte_source, file_info\n                )\n            finally:\n                t_byte_source.close()\n\n            logger.info(\"Stored thumbnail in file %r\", output_path)\n\n            t_len = os.path.getsize(output_path)\n\n            await self.store.store_remote_media_thumbnail(\n                server_name,\n                media_id,\n                file_id,\n                t_width,\n                t_height,\n                t_type,\n                t_method,\n                t_len,\n            )\n\n            return output_path\n\n        # Could not generate thumbnail.\n        return None\n\n    async def _generate_thumbnails(\n        self,\n        server_name: Optional[str],\n        media_id: str,\n        file_id: str,\n        media_type: str,\n        url_cache: bool = False,\n    ) -> Optional[dict]:\n        \"\"\"Generate and store thumbnails for an image.\n\n        Args:\n            server_name: The server name if remote media, else None if local\n            media_id: The media ID of the content. (This is the same as\n                the file_id for local content)\n            file_id: Local file ID\n            media_type: The content type of the file\n            url_cache: If we are thumbnailing images downloaded for the URL cache,\n                used exclusively by the url previewer\n\n        Returns:\n            Dict with \"width\" and \"height\" keys of original image or None if the\n            media cannot be thumbnailed.\n        \"\"\"\n        requirements = self._get_thumbnail_requirements(media_type)\n        if not requirements:\n            return None\n\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(server_name, file_id, url_cache=url_cache)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate thumbnails for remote media %s from %s of type %s: %s\",\n                media_id,\n                server_name,\n                media_type,\n                e,\n            )\n            return None\n\n        m_width = thumbnailer.width\n        m_height = thumbnailer.height\n\n        if m_width * m_height >= self.max_image_pixels:\n            logger.info(\n                \"Image too large to thumbnail %r x %r > %r\",\n                m_width,\n                m_height,\n                self.max_image_pixels,\n            )\n            return None\n\n        if thumbnailer.transpose_method is not None:\n            m_width, m_height = await defer_to_thread(\n                self.hs.get_reactor(), thumbnailer.transpose\n            )\n\n        # We deduplicate the thumbnail sizes by ignoring the cropped versions if\n        # they have the same dimensions of a scaled one.\n        thumbnails = {}  # type: Dict[Tuple[int, int, str], str]\n        for r_width, r_height, r_method, r_type in requirements:\n            if r_method == \"crop\":\n                thumbnails.setdefault((r_width, r_height, r_type), r_method)\n            elif r_method == \"scale\":\n                t_width, t_height = thumbnailer.aspect(r_width, r_height)\n                t_width = min(m_width, t_width)\n                t_height = min(m_height, t_height)\n                thumbnails[(t_width, t_height, r_type)] = r_method\n\n        # Now we generate the thumbnails for each dimension, store it\n        for (t_width, t_height, t_type), t_method in thumbnails.items():\n            # Generate the thumbnail\n            if t_method == \"crop\":\n                t_byte_source = await defer_to_thread(\n                    self.hs.get_reactor(), thumbnailer.crop, t_width, t_height, t_type\n                )\n            elif t_method == \"scale\":\n                t_byte_source = await defer_to_thread(\n                    self.hs.get_reactor(), thumbnailer.scale, t_width, t_height, t_type\n                )\n            else:\n                logger.error(\"Unrecognized method: %r\", t_method)\n                continue\n\n            if not t_byte_source:\n                continue\n\n            file_info = FileInfo(\n                server_name=server_name,\n                file_id=file_id,\n                thumbnail=True,\n                thumbnail_width=t_width,\n                thumbnail_height=t_height,\n                thumbnail_method=t_method,\n                thumbnail_type=t_type,\n                url_cache=url_cache,\n            )\n\n            with self.media_storage.store_into_file(file_info) as (f, fname, finish):\n                try:\n                    await self.media_storage.write_to_file(t_byte_source, f)\n                    await finish()\n                finally:\n                    t_byte_source.close()\n\n                t_len = os.path.getsize(fname)\n\n                # Write to database\n                if server_name:\n                    # Multiple remote media download requests can race (when\n                    # using multiple media repos), so this may throw a violation\n                    # constraint exception. If it does we'll delete the newly\n                    # generated thumbnail from disk (as we're in the ctx\n                    # manager).\n                    #\n                    # However: we've already called `finish()` so we may have\n                    # also written to the storage providers. This is preferable\n                    # to the alternative where we call `finish()` *after* this,\n                    # where we could end up having an entry in the DB but fail\n                    # to write the files to the storage providers.\n                    try:\n                        await self.store.store_remote_media_thumbnail(\n                            server_name,\n                            media_id,\n                            file_id,\n                            t_width,\n                            t_height,\n                            t_type,\n                            t_method,\n                            t_len,\n                        )\n                    except Exception as e:\n                        thumbnail_exists = await self.store.get_remote_media_thumbnail(\n                            server_name, media_id, t_width, t_height, t_type,\n                        )\n                        if not thumbnail_exists:\n                            raise e\n                else:\n                    await self.store.store_local_thumbnail(\n                        media_id, t_width, t_height, t_type, t_method, t_len\n                    )\n\n        return {\"width\": m_width, \"height\": m_height}\n\n    async def delete_old_remote_media(self, before_ts):\n        old_media = await self.store.get_remote_media_before(before_ts)\n\n        deleted = 0\n\n        for media in old_media:\n            origin = media[\"media_origin\"]\n            media_id = media[\"media_id\"]\n            file_id = media[\"filesystem_id\"]\n            key = (origin, media_id)\n\n            logger.info(\"Deleting: %r\", key)\n\n            # TODO: Should we delete from the backup store\n\n            with (await self.remote_media_linearizer.queue(key)):\n                full_path = self.filepaths.remote_media_filepath(origin, file_id)\n                try:\n                    os.remove(full_path)\n                except OSError as e:\n                    logger.warning(\"Failed to remove file: %r\", full_path)\n                    if e.errno == errno.ENOENT:\n                        pass\n                    else:\n                        continue\n\n                thumbnail_dir = self.filepaths.remote_media_thumbnail_dir(\n                    origin, file_id\n                )\n                shutil.rmtree(thumbnail_dir, ignore_errors=True)\n\n                await self.store.delete_remote_media(origin, media_id)\n                deleted += 1\n\n        return {\"deleted\": deleted}\n\n    async def delete_local_media(self, media_id: str) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete the given local or remote media ID from this server\n\n        Args:\n            media_id: The media ID to delete.\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        return await self._remove_local_media_from_disk([media_id])\n\n    async def delete_old_local_media(\n        self, before_ts: int, size_gt: int = 0, keep_profiles: bool = True,\n    ) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete local or remote media from this server by size and timestamp. Removes\n        media files, any thumbnails and cached URLs.\n\n        Args:\n            before_ts: Unix timestamp in ms.\n                       Files that were last used before this timestamp will be deleted\n            size_gt: Size of the media in bytes. Files that are larger will be deleted\n            keep_profiles: Switch to delete also files that are still used in image data\n                           (e.g user profile, room avatar)\n                           If false these files will be deleted\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        old_media = await self.store.get_local_media_before(\n            before_ts, size_gt, keep_profiles,\n        )\n        return await self._remove_local_media_from_disk(old_media)\n\n    async def _remove_local_media_from_disk(\n        self, media_ids: List[str]\n    ) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete local or remote media from this server. Removes media files,\n        any thumbnails and cached URLs.\n\n        Args:\n            media_ids: List of media_id to delete\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        removed_media = []\n        for media_id in media_ids:\n            logger.info(\"Deleting media with ID '%s'\", media_id)\n            full_path = self.filepaths.local_media_filepath(media_id)\n            try:\n                os.remove(full_path)\n            except OSError as e:\n                logger.warning(\"Failed to remove file: %r: %s\", full_path, e)\n                if e.errno == errno.ENOENT:\n                    pass\n                else:\n                    continue\n\n            thumbnail_dir = self.filepaths.local_media_thumbnail_dir(media_id)\n            shutil.rmtree(thumbnail_dir, ignore_errors=True)\n\n            await self.store.delete_remote_media(self.server_name, media_id)\n\n            await self.store.delete_url_cache((media_id,))\n            await self.store.delete_url_cache_media((media_id,))\n\n            removed_media.append(media_id)\n\n        return removed_media, len(removed_media)\n\n\nclass MediaRepositoryResource(Resource):\n    \"\"\"File uploading and downloading.\n\n    Uploads are POSTed to a resource which returns a token which is used to GET\n    the download::\n\n        => POST /_matrix/media/r0/upload HTTP/1.1\n           Content-Type: <media-type>\n           Content-Length: <content-length>\n\n           <media>\n\n        <= HTTP/1.1 200 OK\n           Content-Type: application/json\n\n           { \"content_uri\": \"mxc://<server-name>/<media-id>\" }\n\n        => GET /_matrix/media/r0/download/<server-name>/<media-id> HTTP/1.1\n\n        <= HTTP/1.1 200 OK\n           Content-Type: <media-type>\n           Content-Disposition: attachment;filename=<upload-filename>\n\n           <media>\n\n    Clients can get thumbnails by supplying a desired width and height and\n    thumbnailing method::\n\n        => GET /_matrix/media/r0/thumbnail/<server_name>\n                /<media-id>?width=<w>&height=<h>&method=<m> HTTP/1.1\n\n        <= HTTP/1.1 200 OK\n           Content-Type: image/jpeg or image/png\n\n           <thumbnail>\n\n    The thumbnail methods are \"crop\" and \"scale\". \"scale\" trys to return an\n    image where either the width or the height is smaller than the requested\n    size. The client should then scale and letterbox the image if it needs to\n    fit within a given rectangle. \"crop\" trys to return an image where the\n    width and height are close to the requested size and the aspect matches\n    the requested size. The client should scale the image if it needs to fit\n    within a given rectangle.\n    \"\"\"\n\n    def __init__(self, hs):\n        # If we're not configured to use it, raise if we somehow got here.\n        if not hs.config.can_load_media_repo:\n            raise ConfigError(\"Synapse is not configured to use a media repo.\")\n\n        super().__init__()\n        media_repo = hs.get_media_repository()\n\n        self.putChild(b\"upload\", UploadResource(hs, media_repo))\n        self.putChild(b\"download\", DownloadResource(hs, media_repo))\n        self.putChild(\n            b\"thumbnail\", ThumbnailResource(hs, media_repo, media_repo.media_storage)\n        )\n        if hs.config.url_preview_enabled:\n            self.putChild(\n                b\"preview_url\",\n                PreviewUrlResource(hs, media_repo, media_repo.media_storage),\n            )\n        self.putChild(b\"config\", MediaConfigResource(hs))\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport errno\nimport logging\nimport os\nimport shutil\nfrom typing import IO, Dict, List, Optional, Tuple\n\nimport twisted.internet.error\nimport twisted.web.http\nfrom twisted.web.http import Request\nfrom twisted.web.resource import Resource\n\nfrom synapse.api.errors import (\n    FederationDeniedError,\n    HttpResponseException,\n    NotFoundError,\n    RequestSendFailed,\n    SynapseError,\n)\nfrom synapse.config._base import ConfigError\nfrom synapse.logging.context import defer_to_thread\nfrom synapse.metrics.background_process_metrics import run_as_background_process\nfrom synapse.util.async_helpers import Linearizer\nfrom synapse.util.retryutils import NotRetryingDestination\nfrom synapse.util.stringutils import random_string\n\nfrom ._base import (\n    FileInfo,\n    Responder,\n    get_filename_from_headers,\n    respond_404,\n    respond_with_responder,\n)\nfrom .config_resource import MediaConfigResource\nfrom .download_resource import DownloadResource\nfrom .filepath import MediaFilePaths\nfrom .media_storage import MediaStorage\nfrom .preview_url_resource import PreviewUrlResource\nfrom .storage_provider import StorageProviderWrapper\nfrom .thumbnail_resource import ThumbnailResource\nfrom .thumbnailer import Thumbnailer, ThumbnailError\nfrom .upload_resource import UploadResource\n\nlogger = logging.getLogger(__name__)\n\n\nUPDATE_RECENTLY_ACCESSED_TS = 60 * 1000\n\n\nclass MediaRepository:\n    def __init__(self, hs):\n        self.hs = hs\n        self.auth = hs.get_auth()\n        self.client = hs.get_http_client()\n        self.clock = hs.get_clock()\n        self.server_name = hs.hostname\n        self.store = hs.get_datastore()\n        self.max_upload_size = hs.config.max_upload_size\n        self.max_image_pixels = hs.config.max_image_pixels\n\n        self.primary_base_path = hs.config.media_store_path\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n\n        self.dynamic_thumbnails = hs.config.dynamic_thumbnails\n        self.thumbnail_requirements = hs.config.thumbnail_requirements\n\n        self.remote_media_linearizer = Linearizer(name=\"media_remote\")\n\n        self.recently_accessed_remotes = set()\n        self.recently_accessed_locals = set()\n\n        self.federation_domain_whitelist = hs.config.federation_domain_whitelist\n\n        # List of StorageProviders where we should search for media and\n        # potentially upload to.\n        storage_providers = []\n\n        for clz, provider_config, wrapper_config in hs.config.media_storage_providers:\n            backend = clz(hs, provider_config)\n            provider = StorageProviderWrapper(\n                backend,\n                store_local=wrapper_config.store_local,\n                store_remote=wrapper_config.store_remote,\n                store_synchronous=wrapper_config.store_synchronous,\n            )\n            storage_providers.append(provider)\n\n        self.media_storage = MediaStorage(\n            self.hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n\n        self.clock.looping_call(\n            self._start_update_recently_accessed, UPDATE_RECENTLY_ACCESSED_TS\n        )\n\n    def _start_update_recently_accessed(self):\n        return run_as_background_process(\n            \"update_recently_accessed_media\", self._update_recently_accessed\n        )\n\n    async def _update_recently_accessed(self):\n        remote_media = self.recently_accessed_remotes\n        self.recently_accessed_remotes = set()\n\n        local_media = self.recently_accessed_locals\n        self.recently_accessed_locals = set()\n\n        await self.store.update_cached_last_access_time(\n            local_media, remote_media, self.clock.time_msec()\n        )\n\n    def mark_recently_accessed(self, server_name, media_id):\n        \"\"\"Mark the given media as recently accessed.\n\n        Args:\n            server_name (str|None): Origin server of media, or None if local\n            media_id (str): The media ID of the content\n        \"\"\"\n        if server_name:\n            self.recently_accessed_remotes.add((server_name, media_id))\n        else:\n            self.recently_accessed_locals.add(media_id)\n\n    async def create_content(\n        self,\n        media_type: str,\n        upload_name: Optional[str],\n        content: IO,\n        content_length: int,\n        auth_user: str,\n    ) -> str:\n        \"\"\"Store uploaded content for a local user and return the mxc URL\n\n        Args:\n            media_type: The content type of the file.\n            upload_name: The name of the file, if provided.\n            content: A file like object that is the content to store\n            content_length: The length of the content\n            auth_user: The user_id of the uploader\n\n        Returns:\n            The mxc url of the stored content\n        \"\"\"\n\n        media_id = random_string(24)\n\n        file_info = FileInfo(server_name=None, file_id=media_id)\n\n        fname = await self.media_storage.store_file(content, file_info)\n\n        logger.info(\"Stored local media in file %r\", fname)\n\n        await self.store.store_local_media(\n            media_id=media_id,\n            media_type=media_type,\n            time_now_ms=self.clock.time_msec(),\n            upload_name=upload_name,\n            media_length=content_length,\n            user_id=auth_user,\n        )\n\n        await self._generate_thumbnails(None, media_id, media_id, media_type)\n\n        return \"mxc://%s/%s\" % (self.server_name, media_id)\n\n    async def get_local_media(\n        self, request: Request, media_id: str, name: Optional[str]\n    ) -> None:\n        \"\"\"Responds to reqests for local media, if exists, or returns 404.\n\n        Args:\n            request: The incoming request.\n            media_id: The media ID of the content. (This is the same as\n                the file_id for local content.)\n            name: Optional name that, if specified, will be used as\n                the filename in the Content-Disposition header of the response.\n\n        Returns:\n            Resolves once a response has successfully been written to request\n        \"\"\"\n        media_info = await self.store.get_local_media(media_id)\n        if not media_info or media_info[\"quarantined_by\"]:\n            respond_404(request)\n            return\n\n        self.mark_recently_accessed(None, media_id)\n\n        media_type = media_info[\"media_type\"]\n        media_length = media_info[\"media_length\"]\n        upload_name = name if name else media_info[\"upload_name\"]\n        url_cache = media_info[\"url_cache\"]\n\n        file_info = FileInfo(None, media_id, url_cache=url_cache)\n\n        responder = await self.media_storage.fetch_media(file_info)\n        await respond_with_responder(\n            request, responder, media_type, media_length, upload_name\n        )\n\n    async def get_remote_media(\n        self, request: Request, server_name: str, media_id: str, name: Optional[str]\n    ) -> None:\n        \"\"\"Respond to requests for remote media.\n\n        Args:\n            request: The incoming request.\n            server_name: Remote server_name where the media originated.\n            media_id: The media ID of the content (as defined by the remote server).\n            name: Optional name that, if specified, will be used as\n                the filename in the Content-Disposition header of the response.\n\n        Returns:\n            Resolves once a response has successfully been written to request\n        \"\"\"\n        if (\n            self.federation_domain_whitelist is not None\n            and server_name not in self.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(server_name)\n\n        self.mark_recently_accessed(server_name, media_id)\n\n        # We linearize here to ensure that we don't try and download remote\n        # media multiple times concurrently\n        key = (server_name, media_id)\n        with (await self.remote_media_linearizer.queue(key)):\n            responder, media_info = await self._get_remote_media_impl(\n                server_name, media_id\n            )\n\n        # We deliberately stream the file outside the lock\n        if responder:\n            media_type = media_info[\"media_type\"]\n            media_length = media_info[\"media_length\"]\n            upload_name = name if name else media_info[\"upload_name\"]\n            await respond_with_responder(\n                request, responder, media_type, media_length, upload_name\n            )\n        else:\n            respond_404(request)\n\n    async def get_remote_media_info(self, server_name: str, media_id: str) -> dict:\n        \"\"\"Gets the media info associated with the remote file, downloading\n        if necessary.\n\n        Args:\n            server_name: Remote server_name where the media originated.\n            media_id: The media ID of the content (as defined by the remote server).\n\n        Returns:\n            The media info of the file\n        \"\"\"\n        if (\n            self.federation_domain_whitelist is not None\n            and server_name not in self.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(server_name)\n\n        # We linearize here to ensure that we don't try and download remote\n        # media multiple times concurrently\n        key = (server_name, media_id)\n        with (await self.remote_media_linearizer.queue(key)):\n            responder, media_info = await self._get_remote_media_impl(\n                server_name, media_id\n            )\n\n        # Ensure we actually use the responder so that it releases resources\n        if responder:\n            with responder:\n                pass\n\n        return media_info\n\n    async def _get_remote_media_impl(\n        self, server_name: str, media_id: str\n    ) -> Tuple[Optional[Responder], dict]:\n        \"\"\"Looks for media in local cache, if not there then attempt to\n        download from remote server.\n\n        Args:\n            server_name (str): Remote server_name where the media originated.\n            media_id (str): The media ID of the content (as defined by the\n                remote server).\n\n        Returns:\n            A tuple of responder and the media info of the file.\n        \"\"\"\n        media_info = await self.store.get_cached_remote_media(server_name, media_id)\n\n        # file_id is the ID we use to track the file locally. If we've already\n        # seen the file then reuse the existing ID, otherwise genereate a new\n        # one.\n\n        # If we have an entry in the DB, try and look for it\n        if media_info:\n            file_id = media_info[\"filesystem_id\"]\n            file_info = FileInfo(server_name, file_id)\n\n            if media_info[\"quarantined_by\"]:\n                logger.info(\"Media is quarantined\")\n                raise NotFoundError()\n\n            responder = await self.media_storage.fetch_media(file_info)\n            if responder:\n                return responder, media_info\n\n        # Failed to find the file anywhere, lets download it.\n\n        try:\n            media_info = await self._download_remote_file(server_name, media_id,)\n        except SynapseError:\n            raise\n        except Exception as e:\n            # An exception may be because we downloaded media in another\n            # process, so let's check if we magically have the media.\n            media_info = await self.store.get_cached_remote_media(server_name, media_id)\n            if not media_info:\n                raise e\n\n        file_id = media_info[\"filesystem_id\"]\n        file_info = FileInfo(server_name, file_id)\n\n        # We generate thumbnails even if another process downloaded the media\n        # as a) it's conceivable that the other download request dies before it\n        # generates thumbnails, but mainly b) we want to be sure the thumbnails\n        # have finished being generated before responding to the client,\n        # otherwise they'll request thumbnails and get a 404 if they're not\n        # ready yet.\n        await self._generate_thumbnails(\n            server_name, media_id, file_id, media_info[\"media_type\"]\n        )\n\n        responder = await self.media_storage.fetch_media(file_info)\n        return responder, media_info\n\n    async def _download_remote_file(self, server_name: str, media_id: str,) -> dict:\n        \"\"\"Attempt to download the remote file from the given server name,\n        using the given file_id as the local id.\n\n        Args:\n            server_name: Originating server\n            media_id: The media ID of the content (as defined by the\n                remote server). This is different than the file_id, which is\n                locally generated.\n            file_id: Local file ID\n\n        Returns:\n            The media info of the file.\n        \"\"\"\n\n        file_id = random_string(24)\n\n        file_info = FileInfo(server_name=server_name, file_id=file_id)\n\n        with self.media_storage.store_into_file(file_info) as (f, fname, finish):\n            request_path = \"/\".join(\n                (\"/_matrix/media/r0/download\", server_name, media_id)\n            )\n            try:\n                length, headers = await self.client.get_file(\n                    server_name,\n                    request_path,\n                    output_stream=f,\n                    max_size=self.max_upload_size,\n                    args={\n                        # tell the remote server to 404 if it doesn't\n                        # recognise the server_name, to make sure we don't\n                        # end up with a routing loop.\n                        \"allow_remote\": \"false\"\n                    },\n                )\n            except RequestSendFailed as e:\n                logger.warning(\n                    \"Request failed fetching remote media %s/%s: %r\",\n                    server_name,\n                    media_id,\n                    e,\n                )\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            except HttpResponseException as e:\n                logger.warning(\n                    \"HTTP error fetching remote media %s/%s: %s\",\n                    server_name,\n                    media_id,\n                    e.response,\n                )\n                if e.code == twisted.web.http.NOT_FOUND:\n                    raise e.to_synapse_error()\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            except SynapseError:\n                logger.warning(\n                    \"Failed to fetch remote media %s/%s\", server_name, media_id\n                )\n                raise\n            except NotRetryingDestination:\n                logger.warning(\"Not retrying destination %r\", server_name)\n                raise SynapseError(502, \"Failed to fetch remote media\")\n            except Exception:\n                logger.exception(\n                    \"Failed to fetch remote media %s/%s\", server_name, media_id\n                )\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            await finish()\n\n            media_type = headers[b\"Content-Type\"][0].decode(\"ascii\")\n            upload_name = get_filename_from_headers(headers)\n            time_now_ms = self.clock.time_msec()\n\n            # Multiple remote media download requests can race (when using\n            # multiple media repos), so this may throw a violation constraint\n            # exception. If it does we'll delete the newly downloaded file from\n            # disk (as we're in the ctx manager).\n            #\n            # However: we've already called `finish()` so we may have also\n            # written to the storage providers. This is preferable to the\n            # alternative where we call `finish()` *after* this, where we could\n            # end up having an entry in the DB but fail to write the files to\n            # the storage providers.\n            await self.store.store_cached_remote_media(\n                origin=server_name,\n                media_id=media_id,\n                media_type=media_type,\n                time_now_ms=self.clock.time_msec(),\n                upload_name=upload_name,\n                media_length=length,\n                filesystem_id=file_id,\n            )\n\n        logger.info(\"Stored remote media in file %r\", fname)\n\n        media_info = {\n            \"media_type\": media_type,\n            \"media_length\": length,\n            \"upload_name\": upload_name,\n            \"created_ts\": time_now_ms,\n            \"filesystem_id\": file_id,\n        }\n\n        return media_info\n\n    def _get_thumbnail_requirements(self, media_type):\n        return self.thumbnail_requirements.get(media_type, ())\n\n    def _generate_thumbnail(self, thumbnailer, t_width, t_height, t_method, t_type):\n        m_width = thumbnailer.width\n        m_height = thumbnailer.height\n\n        if m_width * m_height >= self.max_image_pixels:\n            logger.info(\n                \"Image too large to thumbnail %r x %r > %r\",\n                m_width,\n                m_height,\n                self.max_image_pixels,\n            )\n            return\n\n        if thumbnailer.transpose_method is not None:\n            m_width, m_height = thumbnailer.transpose()\n\n        if t_method == \"crop\":\n            t_byte_source = thumbnailer.crop(t_width, t_height, t_type)\n        elif t_method == \"scale\":\n            t_width, t_height = thumbnailer.aspect(t_width, t_height)\n            t_width = min(m_width, t_width)\n            t_height = min(m_height, t_height)\n            t_byte_source = thumbnailer.scale(t_width, t_height, t_type)\n        else:\n            t_byte_source = None\n\n        return t_byte_source\n\n    async def generate_local_exact_thumbnail(\n        self,\n        media_id: str,\n        t_width: int,\n        t_height: int,\n        t_method: str,\n        t_type: str,\n        url_cache: str,\n    ) -> Optional[str]:\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(None, media_id, url_cache=url_cache)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate a thumbnail for local media %s using a method of %s and type of %s: %s\",\n                media_id,\n                t_method,\n                t_type,\n                e,\n            )\n            return None\n\n        t_byte_source = await defer_to_thread(\n            self.hs.get_reactor(),\n            self._generate_thumbnail,\n            thumbnailer,\n            t_width,\n            t_height,\n            t_method,\n            t_type,\n        )\n\n        if t_byte_source:\n            try:\n                file_info = FileInfo(\n                    server_name=None,\n                    file_id=media_id,\n                    url_cache=url_cache,\n                    thumbnail=True,\n                    thumbnail_width=t_width,\n                    thumbnail_height=t_height,\n                    thumbnail_method=t_method,\n                    thumbnail_type=t_type,\n                )\n\n                output_path = await self.media_storage.store_file(\n                    t_byte_source, file_info\n                )\n            finally:\n                t_byte_source.close()\n\n            logger.info(\"Stored thumbnail in file %r\", output_path)\n\n            t_len = os.path.getsize(output_path)\n\n            await self.store.store_local_thumbnail(\n                media_id, t_width, t_height, t_type, t_method, t_len\n            )\n\n            return output_path\n\n        # Could not generate thumbnail.\n        return None\n\n    async def generate_remote_exact_thumbnail(\n        self,\n        server_name: str,\n        file_id: str,\n        media_id: str,\n        t_width: int,\n        t_height: int,\n        t_method: str,\n        t_type: str,\n    ) -> Optional[str]:\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(server_name, file_id, url_cache=False)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate a thumbnail for remote media %s from %s using a method of %s and type of %s: %s\",\n                media_id,\n                server_name,\n                t_method,\n                t_type,\n                e,\n            )\n            return None\n\n        t_byte_source = await defer_to_thread(\n            self.hs.get_reactor(),\n            self._generate_thumbnail,\n            thumbnailer,\n            t_width,\n            t_height,\n            t_method,\n            t_type,\n        )\n\n        if t_byte_source:\n            try:\n                file_info = FileInfo(\n                    server_name=server_name,\n                    file_id=file_id,\n                    thumbnail=True,\n                    thumbnail_width=t_width,\n                    thumbnail_height=t_height,\n                    thumbnail_method=t_method,\n                    thumbnail_type=t_type,\n                )\n\n                output_path = await self.media_storage.store_file(\n                    t_byte_source, file_info\n                )\n            finally:\n                t_byte_source.close()\n\n            logger.info(\"Stored thumbnail in file %r\", output_path)\n\n            t_len = os.path.getsize(output_path)\n\n            await self.store.store_remote_media_thumbnail(\n                server_name,\n                media_id,\n                file_id,\n                t_width,\n                t_height,\n                t_type,\n                t_method,\n                t_len,\n            )\n\n            return output_path\n\n        # Could not generate thumbnail.\n        return None\n\n    async def _generate_thumbnails(\n        self,\n        server_name: Optional[str],\n        media_id: str,\n        file_id: str,\n        media_type: str,\n        url_cache: bool = False,\n    ) -> Optional[dict]:\n        \"\"\"Generate and store thumbnails for an image.\n\n        Args:\n            server_name: The server name if remote media, else None if local\n            media_id: The media ID of the content. (This is the same as\n                the file_id for local content)\n            file_id: Local file ID\n            media_type: The content type of the file\n            url_cache: If we are thumbnailing images downloaded for the URL cache,\n                used exclusively by the url previewer\n\n        Returns:\n            Dict with \"width\" and \"height\" keys of original image or None if the\n            media cannot be thumbnailed.\n        \"\"\"\n        requirements = self._get_thumbnail_requirements(media_type)\n        if not requirements:\n            return None\n\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(server_name, file_id, url_cache=url_cache)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate thumbnails for remote media %s from %s of type %s: %s\",\n                media_id,\n                server_name,\n                media_type,\n                e,\n            )\n            return None\n\n        m_width = thumbnailer.width\n        m_height = thumbnailer.height\n\n        if m_width * m_height >= self.max_image_pixels:\n            logger.info(\n                \"Image too large to thumbnail %r x %r > %r\",\n                m_width,\n                m_height,\n                self.max_image_pixels,\n            )\n            return None\n\n        if thumbnailer.transpose_method is not None:\n            m_width, m_height = await defer_to_thread(\n                self.hs.get_reactor(), thumbnailer.transpose\n            )\n\n        # We deduplicate the thumbnail sizes by ignoring the cropped versions if\n        # they have the same dimensions of a scaled one.\n        thumbnails = {}  # type: Dict[Tuple[int, int, str], str]\n        for r_width, r_height, r_method, r_type in requirements:\n            if r_method == \"crop\":\n                thumbnails.setdefault((r_width, r_height, r_type), r_method)\n            elif r_method == \"scale\":\n                t_width, t_height = thumbnailer.aspect(r_width, r_height)\n                t_width = min(m_width, t_width)\n                t_height = min(m_height, t_height)\n                thumbnails[(t_width, t_height, r_type)] = r_method\n\n        # Now we generate the thumbnails for each dimension, store it\n        for (t_width, t_height, t_type), t_method in thumbnails.items():\n            # Generate the thumbnail\n            if t_method == \"crop\":\n                t_byte_source = await defer_to_thread(\n                    self.hs.get_reactor(), thumbnailer.crop, t_width, t_height, t_type\n                )\n            elif t_method == \"scale\":\n                t_byte_source = await defer_to_thread(\n                    self.hs.get_reactor(), thumbnailer.scale, t_width, t_height, t_type\n                )\n            else:\n                logger.error(\"Unrecognized method: %r\", t_method)\n                continue\n\n            if not t_byte_source:\n                continue\n\n            file_info = FileInfo(\n                server_name=server_name,\n                file_id=file_id,\n                thumbnail=True,\n                thumbnail_width=t_width,\n                thumbnail_height=t_height,\n                thumbnail_method=t_method,\n                thumbnail_type=t_type,\n                url_cache=url_cache,\n            )\n\n            with self.media_storage.store_into_file(file_info) as (f, fname, finish):\n                try:\n                    await self.media_storage.write_to_file(t_byte_source, f)\n                    await finish()\n                finally:\n                    t_byte_source.close()\n\n                t_len = os.path.getsize(fname)\n\n                # Write to database\n                if server_name:\n                    # Multiple remote media download requests can race (when\n                    # using multiple media repos), so this may throw a violation\n                    # constraint exception. If it does we'll delete the newly\n                    # generated thumbnail from disk (as we're in the ctx\n                    # manager).\n                    #\n                    # However: we've already called `finish()` so we may have\n                    # also written to the storage providers. This is preferable\n                    # to the alternative where we call `finish()` *after* this,\n                    # where we could end up having an entry in the DB but fail\n                    # to write the files to the storage providers.\n                    try:\n                        await self.store.store_remote_media_thumbnail(\n                            server_name,\n                            media_id,\n                            file_id,\n                            t_width,\n                            t_height,\n                            t_type,\n                            t_method,\n                            t_len,\n                        )\n                    except Exception as e:\n                        thumbnail_exists = await self.store.get_remote_media_thumbnail(\n                            server_name, media_id, t_width, t_height, t_type,\n                        )\n                        if not thumbnail_exists:\n                            raise e\n                else:\n                    await self.store.store_local_thumbnail(\n                        media_id, t_width, t_height, t_type, t_method, t_len\n                    )\n\n        return {\"width\": m_width, \"height\": m_height}\n\n    async def delete_old_remote_media(self, before_ts):\n        old_media = await self.store.get_remote_media_before(before_ts)\n\n        deleted = 0\n\n        for media in old_media:\n            origin = media[\"media_origin\"]\n            media_id = media[\"media_id\"]\n            file_id = media[\"filesystem_id\"]\n            key = (origin, media_id)\n\n            logger.info(\"Deleting: %r\", key)\n\n            # TODO: Should we delete from the backup store\n\n            with (await self.remote_media_linearizer.queue(key)):\n                full_path = self.filepaths.remote_media_filepath(origin, file_id)\n                try:\n                    os.remove(full_path)\n                except OSError as e:\n                    logger.warning(\"Failed to remove file: %r\", full_path)\n                    if e.errno == errno.ENOENT:\n                        pass\n                    else:\n                        continue\n\n                thumbnail_dir = self.filepaths.remote_media_thumbnail_dir(\n                    origin, file_id\n                )\n                shutil.rmtree(thumbnail_dir, ignore_errors=True)\n\n                await self.store.delete_remote_media(origin, media_id)\n                deleted += 1\n\n        return {\"deleted\": deleted}\n\n    async def delete_local_media(self, media_id: str) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete the given local or remote media ID from this server\n\n        Args:\n            media_id: The media ID to delete.\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        return await self._remove_local_media_from_disk([media_id])\n\n    async def delete_old_local_media(\n        self, before_ts: int, size_gt: int = 0, keep_profiles: bool = True,\n    ) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete local or remote media from this server by size and timestamp. Removes\n        media files, any thumbnails and cached URLs.\n\n        Args:\n            before_ts: Unix timestamp in ms.\n                       Files that were last used before this timestamp will be deleted\n            size_gt: Size of the media in bytes. Files that are larger will be deleted\n            keep_profiles: Switch to delete also files that are still used in image data\n                           (e.g user profile, room avatar)\n                           If false these files will be deleted\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        old_media = await self.store.get_local_media_before(\n            before_ts, size_gt, keep_profiles,\n        )\n        return await self._remove_local_media_from_disk(old_media)\n\n    async def _remove_local_media_from_disk(\n        self, media_ids: List[str]\n    ) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete local or remote media from this server. Removes media files,\n        any thumbnails and cached URLs.\n\n        Args:\n            media_ids: List of media_id to delete\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        removed_media = []\n        for media_id in media_ids:\n            logger.info(\"Deleting media with ID '%s'\", media_id)\n            full_path = self.filepaths.local_media_filepath(media_id)\n            try:\n                os.remove(full_path)\n            except OSError as e:\n                logger.warning(\"Failed to remove file: %r: %s\", full_path, e)\n                if e.errno == errno.ENOENT:\n                    pass\n                else:\n                    continue\n\n            thumbnail_dir = self.filepaths.local_media_thumbnail_dir(media_id)\n            shutil.rmtree(thumbnail_dir, ignore_errors=True)\n\n            await self.store.delete_remote_media(self.server_name, media_id)\n\n            await self.store.delete_url_cache((media_id,))\n            await self.store.delete_url_cache_media((media_id,))\n\n            removed_media.append(media_id)\n\n        return removed_media, len(removed_media)\n\n\nclass MediaRepositoryResource(Resource):\n    \"\"\"File uploading and downloading.\n\n    Uploads are POSTed to a resource which returns a token which is used to GET\n    the download::\n\n        => POST /_matrix/media/r0/upload HTTP/1.1\n           Content-Type: <media-type>\n           Content-Length: <content-length>\n\n           <media>\n\n        <= HTTP/1.1 200 OK\n           Content-Type: application/json\n\n           { \"content_uri\": \"mxc://<server-name>/<media-id>\" }\n\n        => GET /_matrix/media/r0/download/<server-name>/<media-id> HTTP/1.1\n\n        <= HTTP/1.1 200 OK\n           Content-Type: <media-type>\n           Content-Disposition: attachment;filename=<upload-filename>\n\n           <media>\n\n    Clients can get thumbnails by supplying a desired width and height and\n    thumbnailing method::\n\n        => GET /_matrix/media/r0/thumbnail/<server_name>\n                /<media-id>?width=<w>&height=<h>&method=<m> HTTP/1.1\n\n        <= HTTP/1.1 200 OK\n           Content-Type: image/jpeg or image/png\n\n           <thumbnail>\n\n    The thumbnail methods are \"crop\" and \"scale\". \"scale\" trys to return an\n    image where either the width or the height is smaller than the requested\n    size. The client should then scale and letterbox the image if it needs to\n    fit within a given rectangle. \"crop\" trys to return an image where the\n    width and height are close to the requested size and the aspect matches\n    the requested size. The client should scale the image if it needs to fit\n    within a given rectangle.\n    \"\"\"\n\n    def __init__(self, hs):\n        # If we're not configured to use it, raise if we somehow got here.\n        if not hs.config.can_load_media_repo:\n            raise ConfigError(\"Synapse is not configured to use a media repo.\")\n\n        super().__init__()\n        media_repo = hs.get_media_repository()\n\n        self.putChild(b\"upload\", UploadResource(hs, media_repo))\n        self.putChild(b\"download\", DownloadResource(hs, media_repo))\n        self.putChild(\n            b\"thumbnail\", ThumbnailResource(hs, media_repo, media_repo.media_storage)\n        )\n        if hs.config.url_preview_enabled:\n            self.putChild(\n                b\"preview_url\",\n                PreviewUrlResource(hs, media_repo, media_repo.media_storage),\n            )\n        self.putChild(b\"config\", MediaConfigResource(hs))\n", "patch": "@@ -66,7 +66,7 @@ class MediaRepository:\n     def __init__(self, hs):\n         self.hs = hs\n         self.auth = hs.get_auth()\n-        self.client = hs.get_http_client()\n+        self.client = hs.get_federation_http_client()\n         self.clock = hs.get_clock()\n         self.server_name = hs.hostname\n         self.store = hs.get_datastore()", "file_path": "files/2021_2/28", "file_language": "py", "file_name": "synapse/rest/media/v1/media_repository.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class MediaRepository:\n    def __init__(self, hs):\n        self.hs = hs\n        self.auth = hs.get_auth()\n        self.client = hs.get_http_client()\n        self.clock = hs.get_clock()\n        self.server_name = hs.hostname\n        self.store = hs.get_datastore()\n        self.max_upload_size = hs.config.max_upload_size\n        self.max_image_pixels = hs.config.max_image_pixels\n\n        self.primary_base_path = hs.config.media_store_path\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n\n        self.dynamic_thumbnails = hs.config.dynamic_thumbnails\n        self.thumbnail_requirements = hs.config.thumbnail_requirements\n\n        self.remote_media_linearizer = Linearizer(name=\"media_remote\")\n\n        self.recently_accessed_remotes = set()\n        self.recently_accessed_locals = set()\n\n        self.federation_domain_whitelist = hs.config.federation_domain_whitelist\n\n        # List of StorageProviders where we should search for media and\n        # potentially upload to.\n        storage_providers = []\n\n        for clz, provider_config, wrapper_config in hs.config.media_storage_providers:\n            backend = clz(hs, provider_config)\n            provider = StorageProviderWrapper(\n                backend,\n                store_local=wrapper_config.store_local,\n                store_remote=wrapper_config.store_remote,\n                store_synchronous=wrapper_config.store_synchronous,\n            )\n            storage_providers.append(provider)\n\n        self.media_storage = MediaStorage(\n            self.hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n\n        self.clock.looping_call(\n            self._start_update_recently_accessed, UPDATE_RECENTLY_ACCESSED_TS\n        )\n\n    def _start_update_recently_accessed(self):\n        return run_as_background_process(\n            \"update_recently_accessed_media\", self._update_recently_accessed\n        )\n\n    async def _update_recently_accessed(self):\n        remote_media = self.recently_accessed_remotes\n        self.recently_accessed_remotes = set()\n\n        local_media = self.recently_accessed_locals\n        self.recently_accessed_locals = set()\n\n        await self.store.update_cached_last_access_time(\n            local_media, remote_media, self.clock.time_msec()\n        )\n\n    def mark_recently_accessed(self, server_name, media_id):\n        \"\"\"Mark the given media as recently accessed.\n\n        Args:\n            server_name (str|None): Origin server of media, or None if local\n            media_id (str): The media ID of the content\n        \"\"\"\n        if server_name:\n            self.recently_accessed_remotes.add((server_name, media_id))\n        else:\n            self.recently_accessed_locals.add(media_id)\n\n    async def create_content(\n        self,\n        media_type: str,\n        upload_name: Optional[str],\n        content: IO,\n        content_length: int,\n        auth_user: str,\n    ) -> str:\n        \"\"\"Store uploaded content for a local user and return the mxc URL\n\n        Args:\n            media_type: The content type of the file.\n            upload_name: The name of the file, if provided.\n            content: A file like object that is the content to store\n            content_length: The length of the content\n            auth_user: The user_id of the uploader\n\n        Returns:\n            The mxc url of the stored content\n        \"\"\"\n\n        media_id = random_string(24)\n\n        file_info = FileInfo(server_name=None, file_id=media_id)\n\n        fname = await self.media_storage.store_file(content, file_info)\n\n        logger.info(\"Stored local media in file %r\", fname)\n\n        await self.store.store_local_media(\n            media_id=media_id,\n            media_type=media_type,\n            time_now_ms=self.clock.time_msec(),\n            upload_name=upload_name,\n            media_length=content_length,\n            user_id=auth_user,\n        )\n\n        await self._generate_thumbnails(None, media_id, media_id, media_type)\n\n        return \"mxc://%s/%s\" % (self.server_name, media_id)\n\n    async def get_local_media(\n        self, request: Request, media_id: str, name: Optional[str]\n    ) -> None:\n        \"\"\"Responds to reqests for local media, if exists, or returns 404.\n\n        Args:\n            request: The incoming request.\n            media_id: The media ID of the content. (This is the same as\n                the file_id for local content.)\n            name: Optional name that, if specified, will be used as\n                the filename in the Content-Disposition header of the response.\n\n        Returns:\n            Resolves once a response has successfully been written to request\n        \"\"\"\n        media_info = await self.store.get_local_media(media_id)\n        if not media_info or media_info[\"quarantined_by\"]:\n            respond_404(request)\n            return\n\n        self.mark_recently_accessed(None, media_id)\n\n        media_type = media_info[\"media_type\"]\n        media_length = media_info[\"media_length\"]\n        upload_name = name if name else media_info[\"upload_name\"]\n        url_cache = media_info[\"url_cache\"]\n\n        file_info = FileInfo(None, media_id, url_cache=url_cache)\n\n        responder = await self.media_storage.fetch_media(file_info)\n        await respond_with_responder(\n            request, responder, media_type, media_length, upload_name\n        )\n\n    async def get_remote_media(\n        self, request: Request, server_name: str, media_id: str, name: Optional[str]\n    ) -> None:\n        \"\"\"Respond to requests for remote media.\n\n        Args:\n            request: The incoming request.\n            server_name: Remote server_name where the media originated.\n            media_id: The media ID of the content (as defined by the remote server).\n            name: Optional name that, if specified, will be used as\n                the filename in the Content-Disposition header of the response.\n\n        Returns:\n            Resolves once a response has successfully been written to request\n        \"\"\"\n        if (\n            self.federation_domain_whitelist is not None\n            and server_name not in self.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(server_name)\n\n        self.mark_recently_accessed(server_name, media_id)\n\n        # We linearize here to ensure that we don't try and download remote\n        # media multiple times concurrently\n        key = (server_name, media_id)\n        with (await self.remote_media_linearizer.queue(key)):\n            responder, media_info = await self._get_remote_media_impl(\n                server_name, media_id\n            )\n\n        # We deliberately stream the file outside the lock\n        if responder:\n            media_type = media_info[\"media_type\"]\n            media_length = media_info[\"media_length\"]\n            upload_name = name if name else media_info[\"upload_name\"]\n            await respond_with_responder(\n                request, responder, media_type, media_length, upload_name\n            )\n        else:\n            respond_404(request)\n\n    async def get_remote_media_info(self, server_name: str, media_id: str) -> dict:\n        \"\"\"Gets the media info associated with the remote file, downloading\n        if necessary.\n\n        Args:\n            server_name: Remote server_name where the media originated.\n            media_id: The media ID of the content (as defined by the remote server).\n\n        Returns:\n            The media info of the file\n        \"\"\"\n        if (\n            self.federation_domain_whitelist is not None\n            and server_name not in self.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(server_name)\n\n        # We linearize here to ensure that we don't try and download remote\n        # media multiple times concurrently\n        key = (server_name, media_id)\n        with (await self.remote_media_linearizer.queue(key)):\n            responder, media_info = await self._get_remote_media_impl(\n                server_name, media_id\n            )\n\n        # Ensure we actually use the responder so that it releases resources\n        if responder:\n            with responder:\n                pass\n\n        return media_info\n\n    async def _get_remote_media_impl(\n        self, server_name: str, media_id: str\n    ) -> Tuple[Optional[Responder], dict]:\n        \"\"\"Looks for media in local cache, if not there then attempt to\n        download from remote server.\n\n        Args:\n            server_name (str): Remote server_name where the media originated.\n            media_id (str): The media ID of the content (as defined by the\n                remote server).\n\n        Returns:\n            A tuple of responder and the media info of the file.\n        \"\"\"\n        media_info = await self.store.get_cached_remote_media(server_name, media_id)\n\n        # file_id is the ID we use to track the file locally. If we've already\n        # seen the file then reuse the existing ID, otherwise genereate a new\n        # one.\n\n        # If we have an entry in the DB, try and look for it\n        if media_info:\n            file_id = media_info[\"filesystem_id\"]\n            file_info = FileInfo(server_name, file_id)\n\n            if media_info[\"quarantined_by\"]:\n                logger.info(\"Media is quarantined\")\n                raise NotFoundError()\n\n            responder = await self.media_storage.fetch_media(file_info)\n            if responder:\n                return responder, media_info\n\n        # Failed to find the file anywhere, lets download it.\n\n        try:\n            media_info = await self._download_remote_file(server_name, media_id,)\n        except SynapseError:\n            raise\n        except Exception as e:\n            # An exception may be because we downloaded media in another\n            # process, so let's check if we magically have the media.\n            media_info = await self.store.get_cached_remote_media(server_name, media_id)\n            if not media_info:\n                raise e\n\n        file_id = media_info[\"filesystem_id\"]\n        file_info = FileInfo(server_name, file_id)\n\n        # We generate thumbnails even if another process downloaded the media\n        # as a) it's conceivable that the other download request dies before it\n        # generates thumbnails, but mainly b) we want to be sure the thumbnails\n        # have finished being generated before responding to the client,\n        # otherwise they'll request thumbnails and get a 404 if they're not\n        # ready yet.\n        await self._generate_thumbnails(\n            server_name, media_id, file_id, media_info[\"media_type\"]\n        )\n\n        responder = await self.media_storage.fetch_media(file_info)\n        return responder, media_info\n\n    async def _download_remote_file(self, server_name: str, media_id: str,) -> dict:\n        \"\"\"Attempt to download the remote file from the given server name,\n        using the given file_id as the local id.\n\n        Args:\n            server_name: Originating server\n            media_id: The media ID of the content (as defined by the\n                remote server). This is different than the file_id, which is\n                locally generated.\n            file_id: Local file ID\n\n        Returns:\n            The media info of the file.\n        \"\"\"\n\n        file_id = random_string(24)\n\n        file_info = FileInfo(server_name=server_name, file_id=file_id)\n\n        with self.media_storage.store_into_file(file_info) as (f, fname, finish):\n            request_path = \"/\".join(\n                (\"/_matrix/media/r0/download\", server_name, media_id)\n            )\n            try:\n                length, headers = await self.client.get_file(\n                    server_name,\n                    request_path,\n                    output_stream=f,\n                    max_size=self.max_upload_size,\n                    args={\n                        # tell the remote server to 404 if it doesn't\n                        # recognise the server_name, to make sure we don't\n                        # end up with a routing loop.\n                        \"allow_remote\": \"false\"\n                    },\n                )\n            except RequestSendFailed as e:\n                logger.warning(\n                    \"Request failed fetching remote media %s/%s: %r\",\n                    server_name,\n                    media_id,\n                    e,\n                )\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            except HttpResponseException as e:\n                logger.warning(\n                    \"HTTP error fetching remote media %s/%s: %s\",\n                    server_name,\n                    media_id,\n                    e.response,\n                )\n                if e.code == twisted.web.http.NOT_FOUND:\n                    raise e.to_synapse_error()\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            except SynapseError:\n                logger.warning(\n                    \"Failed to fetch remote media %s/%s\", server_name, media_id\n                )\n                raise\n            except NotRetryingDestination:\n                logger.warning(\"Not retrying destination %r\", server_name)\n                raise SynapseError(502, \"Failed to fetch remote media\")\n            except Exception:\n                logger.exception(\n                    \"Failed to fetch remote media %s/%s\", server_name, media_id\n                )\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            await finish()\n\n            media_type = headers[b\"Content-Type\"][0].decode(\"ascii\")\n            upload_name = get_filename_from_headers(headers)\n            time_now_ms = self.clock.time_msec()\n\n            # Multiple remote media download requests can race (when using\n            # multiple media repos), so this may throw a violation constraint\n            # exception. If it does we'll delete the newly downloaded file from\n            # disk (as we're in the ctx manager).\n            #\n            # However: we've already called `finish()` so we may have also\n            # written to the storage providers. This is preferable to the\n            # alternative where we call `finish()` *after* this, where we could\n            # end up having an entry in the DB but fail to write the files to\n            # the storage providers.\n            await self.store.store_cached_remote_media(\n                origin=server_name,\n                media_id=media_id,\n                media_type=media_type,\n                time_now_ms=self.clock.time_msec(),\n                upload_name=upload_name,\n                media_length=length,\n                filesystem_id=file_id,\n            )\n\n        logger.info(\"Stored remote media in file %r\", fname)\n\n        media_info = {\n            \"media_type\": media_type,\n            \"media_length\": length,\n            \"upload_name\": upload_name,\n            \"created_ts\": time_now_ms,\n            \"filesystem_id\": file_id,\n        }\n\n        return media_info\n\n    def _get_thumbnail_requirements(self, media_type):\n        return self.thumbnail_requirements.get(media_type, ())\n\n    def _generate_thumbnail(self, thumbnailer, t_width, t_height, t_method, t_type):\n        m_width = thumbnailer.width\n        m_height = thumbnailer.height\n\n        if m_width * m_height >= self.max_image_pixels:\n            logger.info(\n                \"Image too large to thumbnail %r x %r > %r\",\n                m_width,\n                m_height,\n                self.max_image_pixels,\n            )\n            return\n\n        if thumbnailer.transpose_method is not None:\n            m_width, m_height = thumbnailer.transpose()\n\n        if t_method == \"crop\":\n            t_byte_source = thumbnailer.crop(t_width, t_height, t_type)\n        elif t_method == \"scale\":\n            t_width, t_height = thumbnailer.aspect(t_width, t_height)\n            t_width = min(m_width, t_width)\n            t_height = min(m_height, t_height)\n            t_byte_source = thumbnailer.scale(t_width, t_height, t_type)\n        else:\n            t_byte_source = None\n\n        return t_byte_source\n\n    async def generate_local_exact_thumbnail(\n        self,\n        media_id: str,\n        t_width: int,\n        t_height: int,\n        t_method: str,\n        t_type: str,\n        url_cache: str,\n    ) -> Optional[str]:\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(None, media_id, url_cache=url_cache)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate a thumbnail for local media %s using a method of %s and type of %s: %s\",\n                media_id,\n                t_method,\n                t_type,\n                e,\n            )\n            return None\n\n        t_byte_source = await defer_to_thread(\n            self.hs.get_reactor(),\n            self._generate_thumbnail,\n            thumbnailer,\n            t_width,\n            t_height,\n            t_method,\n            t_type,\n        )\n\n        if t_byte_source:\n            try:\n                file_info = FileInfo(\n                    server_name=None,\n                    file_id=media_id,\n                    url_cache=url_cache,\n                    thumbnail=True,\n                    thumbnail_width=t_width,\n                    thumbnail_height=t_height,\n                    thumbnail_method=t_method,\n                    thumbnail_type=t_type,\n                )\n\n                output_path = await self.media_storage.store_file(\n                    t_byte_source, file_info\n                )\n            finally:\n                t_byte_source.close()\n\n            logger.info(\"Stored thumbnail in file %r\", output_path)\n\n            t_len = os.path.getsize(output_path)\n\n            await self.store.store_local_thumbnail(\n                media_id, t_width, t_height, t_type, t_method, t_len\n            )\n\n            return output_path\n\n        # Could not generate thumbnail.\n        return None\n\n    async def generate_remote_exact_thumbnail(\n        self,\n        server_name: str,\n        file_id: str,\n        media_id: str,\n        t_width: int,\n        t_height: int,\n        t_method: str,\n        t_type: str,\n    ) -> Optional[str]:\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(server_name, file_id, url_cache=False)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate a thumbnail for remote media %s from %s using a method of %s and type of %s: %s\",\n                media_id,\n                server_name,\n                t_method,\n                t_type,\n                e,\n            )\n            return None\n\n        t_byte_source = await defer_to_thread(\n            self.hs.get_reactor(),\n            self._generate_thumbnail,\n            thumbnailer,\n            t_width,\n            t_height,\n            t_method,\n            t_type,\n        )\n\n        if t_byte_source:\n            try:\n                file_info = FileInfo(\n                    server_name=server_name,\n                    file_id=file_id,\n                    thumbnail=True,\n                    thumbnail_width=t_width,\n                    thumbnail_height=t_height,\n                    thumbnail_method=t_method,\n                    thumbnail_type=t_type,\n                )\n\n                output_path = await self.media_storage.store_file(\n                    t_byte_source, file_info\n                )\n            finally:\n                t_byte_source.close()\n\n            logger.info(\"Stored thumbnail in file %r\", output_path)\n\n            t_len = os.path.getsize(output_path)\n\n            await self.store.store_remote_media_thumbnail(\n                server_name,\n                media_id,\n                file_id,\n                t_width,\n                t_height,\n                t_type,\n                t_method,\n                t_len,\n            )\n\n            return output_path\n\n        # Could not generate thumbnail.\n        return None\n\n    async def _generate_thumbnails(\n        self,\n        server_name: Optional[str],\n        media_id: str,\n        file_id: str,\n        media_type: str,\n        url_cache: bool = False,\n    ) -> Optional[dict]:\n        \"\"\"Generate and store thumbnails for an image.\n\n        Args:\n            server_name: The server name if remote media, else None if local\n            media_id: The media ID of the content. (This is the same as\n                the file_id for local content)\n            file_id: Local file ID\n            media_type: The content type of the file\n            url_cache: If we are thumbnailing images downloaded for the URL cache,\n                used exclusively by the url previewer\n\n        Returns:\n            Dict with \"width\" and \"height\" keys of original image or None if the\n            media cannot be thumbnailed.\n        \"\"\"\n        requirements = self._get_thumbnail_requirements(media_type)\n        if not requirements:\n            return None\n\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(server_name, file_id, url_cache=url_cache)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate thumbnails for remote media %s from %s of type %s: %s\",\n                media_id,\n                server_name,\n                media_type,\n                e,\n            )\n            return None\n\n        m_width = thumbnailer.width\n        m_height = thumbnailer.height\n\n        if m_width * m_height >= self.max_image_pixels:\n            logger.info(\n                \"Image too large to thumbnail %r x %r > %r\",\n                m_width,\n                m_height,\n                self.max_image_pixels,\n            )\n            return None\n\n        if thumbnailer.transpose_method is not None:\n            m_width, m_height = await defer_to_thread(\n                self.hs.get_reactor(), thumbnailer.transpose\n            )\n\n        # We deduplicate the thumbnail sizes by ignoring the cropped versions if\n        # they have the same dimensions of a scaled one.\n        thumbnails = {}  # type: Dict[Tuple[int, int, str], str]\n        for r_width, r_height, r_method, r_type in requirements:\n            if r_method == \"crop\":\n                thumbnails.setdefault((r_width, r_height, r_type), r_method)\n            elif r_method == \"scale\":\n                t_width, t_height = thumbnailer.aspect(r_width, r_height)\n                t_width = min(m_width, t_width)\n                t_height = min(m_height, t_height)\n                thumbnails[(t_width, t_height, r_type)] = r_method\n\n        # Now we generate the thumbnails for each dimension, store it\n        for (t_width, t_height, t_type), t_method in thumbnails.items():\n            # Generate the thumbnail\n            if t_method == \"crop\":\n                t_byte_source = await defer_to_thread(\n                    self.hs.get_reactor(), thumbnailer.crop, t_width, t_height, t_type\n                )\n            elif t_method == \"scale\":\n                t_byte_source = await defer_to_thread(\n                    self.hs.get_reactor(), thumbnailer.scale, t_width, t_height, t_type\n                )\n            else:\n                logger.error(\"Unrecognized method: %r\", t_method)\n                continue\n\n            if not t_byte_source:\n                continue\n\n            file_info = FileInfo(\n                server_name=server_name,\n                file_id=file_id,\n                thumbnail=True,\n                thumbnail_width=t_width,\n                thumbnail_height=t_height,\n                thumbnail_method=t_method,\n                thumbnail_type=t_type,\n                url_cache=url_cache,\n            )\n\n            with self.media_storage.store_into_file(file_info) as (f, fname, finish):\n                try:\n                    await self.media_storage.write_to_file(t_byte_source, f)\n                    await finish()\n                finally:\n                    t_byte_source.close()\n\n                t_len = os.path.getsize(fname)\n\n                # Write to database\n                if server_name:\n                    # Multiple remote media download requests can race (when\n                    # using multiple media repos), so this may throw a violation\n                    # constraint exception. If it does we'll delete the newly\n                    # generated thumbnail from disk (as we're in the ctx\n                    # manager).\n                    #\n                    # However: we've already called `finish()` so we may have\n                    # also written to the storage providers. This is preferable\n                    # to the alternative where we call `finish()` *after* this,\n                    # where we could end up having an entry in the DB but fail\n                    # to write the files to the storage providers.\n                    try:\n                        await self.store.store_remote_media_thumbnail(\n                            server_name,\n                            media_id,\n                            file_id,\n                            t_width,\n                            t_height,\n                            t_type,\n                            t_method,\n                            t_len,\n                        )\n                    except Exception as e:\n                        thumbnail_exists = await self.store.get_remote_media_thumbnail(\n                            server_name, media_id, t_width, t_height, t_type,\n                        )\n                        if not thumbnail_exists:\n                            raise e\n                else:\n                    await self.store.store_local_thumbnail(\n                        media_id, t_width, t_height, t_type, t_method, t_len\n                    )\n\n        return {\"width\": m_width, \"height\": m_height}\n\n    async def delete_old_remote_media(self, before_ts):\n        old_media = await self.store.get_remote_media_before(before_ts)\n\n        deleted = 0\n\n        for media in old_media:\n            origin = media[\"media_origin\"]\n            media_id = media[\"media_id\"]\n            file_id = media[\"filesystem_id\"]\n            key = (origin, media_id)\n\n            logger.info(\"Deleting: %r\", key)\n\n            # TODO: Should we delete from the backup store\n\n            with (await self.remote_media_linearizer.queue(key)):\n                full_path = self.filepaths.remote_media_filepath(origin, file_id)\n                try:\n                    os.remove(full_path)\n                except OSError as e:\n                    logger.warning(\"Failed to remove file: %r\", full_path)\n                    if e.errno == errno.ENOENT:\n                        pass\n                    else:\n                        continue\n\n                thumbnail_dir = self.filepaths.remote_media_thumbnail_dir(\n                    origin, file_id\n                )\n                shutil.rmtree(thumbnail_dir, ignore_errors=True)\n\n                await self.store.delete_remote_media(origin, media_id)\n                deleted += 1\n\n        return {\"deleted\": deleted}\n\n    async def delete_local_media(self, media_id: str) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete the given local or remote media ID from this server\n\n        Args:\n            media_id: The media ID to delete.\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        return await self._remove_local_media_from_disk([media_id])\n\n    async def delete_old_local_media(\n        self, before_ts: int, size_gt: int = 0, keep_profiles: bool = True,\n    ) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete local or remote media from this server by size and timestamp. Removes\n        media files, any thumbnails and cached URLs.\n\n        Args:\n            before_ts: Unix timestamp in ms.\n                       Files that were last used before this timestamp will be deleted\n            size_gt: Size of the media in bytes. Files that are larger will be deleted\n            keep_profiles: Switch to delete also files that are still used in image data\n                           (e.g user profile, room avatar)\n                           If false these files will be deleted\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        old_media = await self.store.get_local_media_before(\n            before_ts, size_gt, keep_profiles,\n        )\n        return await self._remove_local_media_from_disk(old_media)\n\n    async def _remove_local_media_from_disk(\n        self, media_ids: List[str]\n    ) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete local or remote media from this server. Removes media files,\n        any thumbnails and cached URLs.\n\n        Args:\n            media_ids: List of media_id to delete\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        removed_media = []\n        for media_id in media_ids:\n            logger.info(\"Deleting media with ID '%s'\", media_id)\n            full_path = self.filepaths.local_media_filepath(media_id)\n            try:\n                os.remove(full_path)\n            except OSError as e:\n                logger.warning(\"Failed to remove file: %r: %s\", full_path, e)\n                if e.errno == errno.ENOENT:\n                    pass\n                else:\n                    continue\n\n            thumbnail_dir = self.filepaths.local_media_thumbnail_dir(media_id)\n            shutil.rmtree(thumbnail_dir, ignore_errors=True)\n\n            await self.store.delete_remote_media(self.server_name, media_id)\n\n            await self.store.delete_url_cache((media_id,))\n            await self.store.delete_url_cache_media((media_id,))\n\n            removed_media.append(media_id)\n\n        return removed_media, len(removed_media)", "target": 0}, {"function": "class MediaRepositoryResource(Resource):\n    \"\"\"File uploading and downloading.\n\n    Uploads are POSTed to a resource which returns a token which is used to GET\n    the download::\n\n        => POST /_matrix/media/r0/upload HTTP/1.1\n           Content-Type: <media-type>\n           Content-Length: <content-length>\n\n           <media>\n\n        <= HTTP/1.1 200 OK\n           Content-Type: application/json\n\n           { \"content_uri\": \"mxc://<server-name>/<media-id>\" }\n\n        => GET /_matrix/media/r0/download/<server-name>/<media-id> HTTP/1.1\n\n        <= HTTP/1.1 200 OK\n           Content-Type: <media-type>\n           Content-Disposition: attachment;filename=<upload-filename>\n\n           <media>\n\n    Clients can get thumbnails by supplying a desired width and height and\n    thumbnailing method::\n\n        => GET /_matrix/media/r0/thumbnail/<server_name>\n                /<media-id>?width=<w>&height=<h>&method=<m> HTTP/1.1\n\n        <= HTTP/1.1 200 OK\n           Content-Type: image/jpeg or image/png\n\n           <thumbnail>\n\n    The thumbnail methods are \"crop\" and \"scale\". \"scale\" trys to return an\n    image where either the width or the height is smaller than the requested\n    size. The client should then scale and letterbox the image if it needs to\n    fit within a given rectangle. \"crop\" trys to return an image where the\n    width and height are close to the requested size and the aspect matches\n    the requested size. The client should scale the image if it needs to fit\n    within a given rectangle.\n    \"\"\"\n\n    def __init__(self, hs):\n        # If we're not configured to use it, raise if we somehow got here.\n        if not hs.config.can_load_media_repo:\n            raise ConfigError(\"Synapse is not configured to use a media repo.\")\n\n        super().__init__()\n        media_repo = hs.get_media_repository()\n\n        self.putChild(b\"upload\", UploadResource(hs, media_repo))\n        self.putChild(b\"download\", DownloadResource(hs, media_repo))\n        self.putChild(\n            b\"thumbnail\", ThumbnailResource(hs, media_repo, media_repo.media_storage)\n        )\n        if hs.config.url_preview_enabled:\n            self.putChild(\n                b\"preview_url\",\n                PreviewUrlResource(hs, media_repo, media_repo.media_storage),\n            )\n        self.putChild(b\"config\", MediaConfigResource(hs))", "target": 0}], "function_after": [{"function": "class MediaRepository:\n    def __init__(self, hs):\n        self.hs = hs\n        self.auth = hs.get_auth()\n        self.client = hs.get_federation_http_client()\n        self.clock = hs.get_clock()\n        self.server_name = hs.hostname\n        self.store = hs.get_datastore()\n        self.max_upload_size = hs.config.max_upload_size\n        self.max_image_pixels = hs.config.max_image_pixels\n\n        self.primary_base_path = hs.config.media_store_path\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n\n        self.dynamic_thumbnails = hs.config.dynamic_thumbnails\n        self.thumbnail_requirements = hs.config.thumbnail_requirements\n\n        self.remote_media_linearizer = Linearizer(name=\"media_remote\")\n\n        self.recently_accessed_remotes = set()\n        self.recently_accessed_locals = set()\n\n        self.federation_domain_whitelist = hs.config.federation_domain_whitelist\n\n        # List of StorageProviders where we should search for media and\n        # potentially upload to.\n        storage_providers = []\n\n        for clz, provider_config, wrapper_config in hs.config.media_storage_providers:\n            backend = clz(hs, provider_config)\n            provider = StorageProviderWrapper(\n                backend,\n                store_local=wrapper_config.store_local,\n                store_remote=wrapper_config.store_remote,\n                store_synchronous=wrapper_config.store_synchronous,\n            )\n            storage_providers.append(provider)\n\n        self.media_storage = MediaStorage(\n            self.hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n\n        self.clock.looping_call(\n            self._start_update_recently_accessed, UPDATE_RECENTLY_ACCESSED_TS\n        )\n\n    def _start_update_recently_accessed(self):\n        return run_as_background_process(\n            \"update_recently_accessed_media\", self._update_recently_accessed\n        )\n\n    async def _update_recently_accessed(self):\n        remote_media = self.recently_accessed_remotes\n        self.recently_accessed_remotes = set()\n\n        local_media = self.recently_accessed_locals\n        self.recently_accessed_locals = set()\n\n        await self.store.update_cached_last_access_time(\n            local_media, remote_media, self.clock.time_msec()\n        )\n\n    def mark_recently_accessed(self, server_name, media_id):\n        \"\"\"Mark the given media as recently accessed.\n\n        Args:\n            server_name (str|None): Origin server of media, or None if local\n            media_id (str): The media ID of the content\n        \"\"\"\n        if server_name:\n            self.recently_accessed_remotes.add((server_name, media_id))\n        else:\n            self.recently_accessed_locals.add(media_id)\n\n    async def create_content(\n        self,\n        media_type: str,\n        upload_name: Optional[str],\n        content: IO,\n        content_length: int,\n        auth_user: str,\n    ) -> str:\n        \"\"\"Store uploaded content for a local user and return the mxc URL\n\n        Args:\n            media_type: The content type of the file.\n            upload_name: The name of the file, if provided.\n            content: A file like object that is the content to store\n            content_length: The length of the content\n            auth_user: The user_id of the uploader\n\n        Returns:\n            The mxc url of the stored content\n        \"\"\"\n\n        media_id = random_string(24)\n\n        file_info = FileInfo(server_name=None, file_id=media_id)\n\n        fname = await self.media_storage.store_file(content, file_info)\n\n        logger.info(\"Stored local media in file %r\", fname)\n\n        await self.store.store_local_media(\n            media_id=media_id,\n            media_type=media_type,\n            time_now_ms=self.clock.time_msec(),\n            upload_name=upload_name,\n            media_length=content_length,\n            user_id=auth_user,\n        )\n\n        await self._generate_thumbnails(None, media_id, media_id, media_type)\n\n        return \"mxc://%s/%s\" % (self.server_name, media_id)\n\n    async def get_local_media(\n        self, request: Request, media_id: str, name: Optional[str]\n    ) -> None:\n        \"\"\"Responds to reqests for local media, if exists, or returns 404.\n\n        Args:\n            request: The incoming request.\n            media_id: The media ID of the content. (This is the same as\n                the file_id for local content.)\n            name: Optional name that, if specified, will be used as\n                the filename in the Content-Disposition header of the response.\n\n        Returns:\n            Resolves once a response has successfully been written to request\n        \"\"\"\n        media_info = await self.store.get_local_media(media_id)\n        if not media_info or media_info[\"quarantined_by\"]:\n            respond_404(request)\n            return\n\n        self.mark_recently_accessed(None, media_id)\n\n        media_type = media_info[\"media_type\"]\n        media_length = media_info[\"media_length\"]\n        upload_name = name if name else media_info[\"upload_name\"]\n        url_cache = media_info[\"url_cache\"]\n\n        file_info = FileInfo(None, media_id, url_cache=url_cache)\n\n        responder = await self.media_storage.fetch_media(file_info)\n        await respond_with_responder(\n            request, responder, media_type, media_length, upload_name\n        )\n\n    async def get_remote_media(\n        self, request: Request, server_name: str, media_id: str, name: Optional[str]\n    ) -> None:\n        \"\"\"Respond to requests for remote media.\n\n        Args:\n            request: The incoming request.\n            server_name: Remote server_name where the media originated.\n            media_id: The media ID of the content (as defined by the remote server).\n            name: Optional name that, if specified, will be used as\n                the filename in the Content-Disposition header of the response.\n\n        Returns:\n            Resolves once a response has successfully been written to request\n        \"\"\"\n        if (\n            self.federation_domain_whitelist is not None\n            and server_name not in self.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(server_name)\n\n        self.mark_recently_accessed(server_name, media_id)\n\n        # We linearize here to ensure that we don't try and download remote\n        # media multiple times concurrently\n        key = (server_name, media_id)\n        with (await self.remote_media_linearizer.queue(key)):\n            responder, media_info = await self._get_remote_media_impl(\n                server_name, media_id\n            )\n\n        # We deliberately stream the file outside the lock\n        if responder:\n            media_type = media_info[\"media_type\"]\n            media_length = media_info[\"media_length\"]\n            upload_name = name if name else media_info[\"upload_name\"]\n            await respond_with_responder(\n                request, responder, media_type, media_length, upload_name\n            )\n        else:\n            respond_404(request)\n\n    async def get_remote_media_info(self, server_name: str, media_id: str) -> dict:\n        \"\"\"Gets the media info associated with the remote file, downloading\n        if necessary.\n\n        Args:\n            server_name: Remote server_name where the media originated.\n            media_id: The media ID of the content (as defined by the remote server).\n\n        Returns:\n            The media info of the file\n        \"\"\"\n        if (\n            self.federation_domain_whitelist is not None\n            and server_name not in self.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(server_name)\n\n        # We linearize here to ensure that we don't try and download remote\n        # media multiple times concurrently\n        key = (server_name, media_id)\n        with (await self.remote_media_linearizer.queue(key)):\n            responder, media_info = await self._get_remote_media_impl(\n                server_name, media_id\n            )\n\n        # Ensure we actually use the responder so that it releases resources\n        if responder:\n            with responder:\n                pass\n\n        return media_info\n\n    async def _get_remote_media_impl(\n        self, server_name: str, media_id: str\n    ) -> Tuple[Optional[Responder], dict]:\n        \"\"\"Looks for media in local cache, if not there then attempt to\n        download from remote server.\n\n        Args:\n            server_name (str): Remote server_name where the media originated.\n            media_id (str): The media ID of the content (as defined by the\n                remote server).\n\n        Returns:\n            A tuple of responder and the media info of the file.\n        \"\"\"\n        media_info = await self.store.get_cached_remote_media(server_name, media_id)\n\n        # file_id is the ID we use to track the file locally. If we've already\n        # seen the file then reuse the existing ID, otherwise genereate a new\n        # one.\n\n        # If we have an entry in the DB, try and look for it\n        if media_info:\n            file_id = media_info[\"filesystem_id\"]\n            file_info = FileInfo(server_name, file_id)\n\n            if media_info[\"quarantined_by\"]:\n                logger.info(\"Media is quarantined\")\n                raise NotFoundError()\n\n            responder = await self.media_storage.fetch_media(file_info)\n            if responder:\n                return responder, media_info\n\n        # Failed to find the file anywhere, lets download it.\n\n        try:\n            media_info = await self._download_remote_file(server_name, media_id,)\n        except SynapseError:\n            raise\n        except Exception as e:\n            # An exception may be because we downloaded media in another\n            # process, so let's check if we magically have the media.\n            media_info = await self.store.get_cached_remote_media(server_name, media_id)\n            if not media_info:\n                raise e\n\n        file_id = media_info[\"filesystem_id\"]\n        file_info = FileInfo(server_name, file_id)\n\n        # We generate thumbnails even if another process downloaded the media\n        # as a) it's conceivable that the other download request dies before it\n        # generates thumbnails, but mainly b) we want to be sure the thumbnails\n        # have finished being generated before responding to the client,\n        # otherwise they'll request thumbnails and get a 404 if they're not\n        # ready yet.\n        await self._generate_thumbnails(\n            server_name, media_id, file_id, media_info[\"media_type\"]\n        )\n\n        responder = await self.media_storage.fetch_media(file_info)\n        return responder, media_info\n\n    async def _download_remote_file(self, server_name: str, media_id: str,) -> dict:\n        \"\"\"Attempt to download the remote file from the given server name,\n        using the given file_id as the local id.\n\n        Args:\n            server_name: Originating server\n            media_id: The media ID of the content (as defined by the\n                remote server). This is different than the file_id, which is\n                locally generated.\n            file_id: Local file ID\n\n        Returns:\n            The media info of the file.\n        \"\"\"\n\n        file_id = random_string(24)\n\n        file_info = FileInfo(server_name=server_name, file_id=file_id)\n\n        with self.media_storage.store_into_file(file_info) as (f, fname, finish):\n            request_path = \"/\".join(\n                (\"/_matrix/media/r0/download\", server_name, media_id)\n            )\n            try:\n                length, headers = await self.client.get_file(\n                    server_name,\n                    request_path,\n                    output_stream=f,\n                    max_size=self.max_upload_size,\n                    args={\n                        # tell the remote server to 404 if it doesn't\n                        # recognise the server_name, to make sure we don't\n                        # end up with a routing loop.\n                        \"allow_remote\": \"false\"\n                    },\n                )\n            except RequestSendFailed as e:\n                logger.warning(\n                    \"Request failed fetching remote media %s/%s: %r\",\n                    server_name,\n                    media_id,\n                    e,\n                )\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            except HttpResponseException as e:\n                logger.warning(\n                    \"HTTP error fetching remote media %s/%s: %s\",\n                    server_name,\n                    media_id,\n                    e.response,\n                )\n                if e.code == twisted.web.http.NOT_FOUND:\n                    raise e.to_synapse_error()\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            except SynapseError:\n                logger.warning(\n                    \"Failed to fetch remote media %s/%s\", server_name, media_id\n                )\n                raise\n            except NotRetryingDestination:\n                logger.warning(\"Not retrying destination %r\", server_name)\n                raise SynapseError(502, \"Failed to fetch remote media\")\n            except Exception:\n                logger.exception(\n                    \"Failed to fetch remote media %s/%s\", server_name, media_id\n                )\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            await finish()\n\n            media_type = headers[b\"Content-Type\"][0].decode(\"ascii\")\n            upload_name = get_filename_from_headers(headers)\n            time_now_ms = self.clock.time_msec()\n\n            # Multiple remote media download requests can race (when using\n            # multiple media repos), so this may throw a violation constraint\n            # exception. If it does we'll delete the newly downloaded file from\n            # disk (as we're in the ctx manager).\n            #\n            # However: we've already called `finish()` so we may have also\n            # written to the storage providers. This is preferable to the\n            # alternative where we call `finish()` *after* this, where we could\n            # end up having an entry in the DB but fail to write the files to\n            # the storage providers.\n            await self.store.store_cached_remote_media(\n                origin=server_name,\n                media_id=media_id,\n                media_type=media_type,\n                time_now_ms=self.clock.time_msec(),\n                upload_name=upload_name,\n                media_length=length,\n                filesystem_id=file_id,\n            )\n\n        logger.info(\"Stored remote media in file %r\", fname)\n\n        media_info = {\n            \"media_type\": media_type,\n            \"media_length\": length,\n            \"upload_name\": upload_name,\n            \"created_ts\": time_now_ms,\n            \"filesystem_id\": file_id,\n        }\n\n        return media_info\n\n    def _get_thumbnail_requirements(self, media_type):\n        return self.thumbnail_requirements.get(media_type, ())\n\n    def _generate_thumbnail(self, thumbnailer, t_width, t_height, t_method, t_type):\n        m_width = thumbnailer.width\n        m_height = thumbnailer.height\n\n        if m_width * m_height >= self.max_image_pixels:\n            logger.info(\n                \"Image too large to thumbnail %r x %r > %r\",\n                m_width,\n                m_height,\n                self.max_image_pixels,\n            )\n            return\n\n        if thumbnailer.transpose_method is not None:\n            m_width, m_height = thumbnailer.transpose()\n\n        if t_method == \"crop\":\n            t_byte_source = thumbnailer.crop(t_width, t_height, t_type)\n        elif t_method == \"scale\":\n            t_width, t_height = thumbnailer.aspect(t_width, t_height)\n            t_width = min(m_width, t_width)\n            t_height = min(m_height, t_height)\n            t_byte_source = thumbnailer.scale(t_width, t_height, t_type)\n        else:\n            t_byte_source = None\n\n        return t_byte_source\n\n    async def generate_local_exact_thumbnail(\n        self,\n        media_id: str,\n        t_width: int,\n        t_height: int,\n        t_method: str,\n        t_type: str,\n        url_cache: str,\n    ) -> Optional[str]:\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(None, media_id, url_cache=url_cache)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate a thumbnail for local media %s using a method of %s and type of %s: %s\",\n                media_id,\n                t_method,\n                t_type,\n                e,\n            )\n            return None\n\n        t_byte_source = await defer_to_thread(\n            self.hs.get_reactor(),\n            self._generate_thumbnail,\n            thumbnailer,\n            t_width,\n            t_height,\n            t_method,\n            t_type,\n        )\n\n        if t_byte_source:\n            try:\n                file_info = FileInfo(\n                    server_name=None,\n                    file_id=media_id,\n                    url_cache=url_cache,\n                    thumbnail=True,\n                    thumbnail_width=t_width,\n                    thumbnail_height=t_height,\n                    thumbnail_method=t_method,\n                    thumbnail_type=t_type,\n                )\n\n                output_path = await self.media_storage.store_file(\n                    t_byte_source, file_info\n                )\n            finally:\n                t_byte_source.close()\n\n            logger.info(\"Stored thumbnail in file %r\", output_path)\n\n            t_len = os.path.getsize(output_path)\n\n            await self.store.store_local_thumbnail(\n                media_id, t_width, t_height, t_type, t_method, t_len\n            )\n\n            return output_path\n\n        # Could not generate thumbnail.\n        return None\n\n    async def generate_remote_exact_thumbnail(\n        self,\n        server_name: str,\n        file_id: str,\n        media_id: str,\n        t_width: int,\n        t_height: int,\n        t_method: str,\n        t_type: str,\n    ) -> Optional[str]:\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(server_name, file_id, url_cache=False)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate a thumbnail for remote media %s from %s using a method of %s and type of %s: %s\",\n                media_id,\n                server_name,\n                t_method,\n                t_type,\n                e,\n            )\n            return None\n\n        t_byte_source = await defer_to_thread(\n            self.hs.get_reactor(),\n            self._generate_thumbnail,\n            thumbnailer,\n            t_width,\n            t_height,\n            t_method,\n            t_type,\n        )\n\n        if t_byte_source:\n            try:\n                file_info = FileInfo(\n                    server_name=server_name,\n                    file_id=file_id,\n                    thumbnail=True,\n                    thumbnail_width=t_width,\n                    thumbnail_height=t_height,\n                    thumbnail_method=t_method,\n                    thumbnail_type=t_type,\n                )\n\n                output_path = await self.media_storage.store_file(\n                    t_byte_source, file_info\n                )\n            finally:\n                t_byte_source.close()\n\n            logger.info(\"Stored thumbnail in file %r\", output_path)\n\n            t_len = os.path.getsize(output_path)\n\n            await self.store.store_remote_media_thumbnail(\n                server_name,\n                media_id,\n                file_id,\n                t_width,\n                t_height,\n                t_type,\n                t_method,\n                t_len,\n            )\n\n            return output_path\n\n        # Could not generate thumbnail.\n        return None\n\n    async def _generate_thumbnails(\n        self,\n        server_name: Optional[str],\n        media_id: str,\n        file_id: str,\n        media_type: str,\n        url_cache: bool = False,\n    ) -> Optional[dict]:\n        \"\"\"Generate and store thumbnails for an image.\n\n        Args:\n            server_name: The server name if remote media, else None if local\n            media_id: The media ID of the content. (This is the same as\n                the file_id for local content)\n            file_id: Local file ID\n            media_type: The content type of the file\n            url_cache: If we are thumbnailing images downloaded for the URL cache,\n                used exclusively by the url previewer\n\n        Returns:\n            Dict with \"width\" and \"height\" keys of original image or None if the\n            media cannot be thumbnailed.\n        \"\"\"\n        requirements = self._get_thumbnail_requirements(media_type)\n        if not requirements:\n            return None\n\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(server_name, file_id, url_cache=url_cache)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate thumbnails for remote media %s from %s of type %s: %s\",\n                media_id,\n                server_name,\n                media_type,\n                e,\n            )\n            return None\n\n        m_width = thumbnailer.width\n        m_height = thumbnailer.height\n\n        if m_width * m_height >= self.max_image_pixels:\n            logger.info(\n                \"Image too large to thumbnail %r x %r > %r\",\n                m_width,\n                m_height,\n                self.max_image_pixels,\n            )\n            return None\n\n        if thumbnailer.transpose_method is not None:\n            m_width, m_height = await defer_to_thread(\n                self.hs.get_reactor(), thumbnailer.transpose\n            )\n\n        # We deduplicate the thumbnail sizes by ignoring the cropped versions if\n        # they have the same dimensions of a scaled one.\n        thumbnails = {}  # type: Dict[Tuple[int, int, str], str]\n        for r_width, r_height, r_method, r_type in requirements:\n            if r_method == \"crop\":\n                thumbnails.setdefault((r_width, r_height, r_type), r_method)\n            elif r_method == \"scale\":\n                t_width, t_height = thumbnailer.aspect(r_width, r_height)\n                t_width = min(m_width, t_width)\n                t_height = min(m_height, t_height)\n                thumbnails[(t_width, t_height, r_type)] = r_method\n\n        # Now we generate the thumbnails for each dimension, store it\n        for (t_width, t_height, t_type), t_method in thumbnails.items():\n            # Generate the thumbnail\n            if t_method == \"crop\":\n                t_byte_source = await defer_to_thread(\n                    self.hs.get_reactor(), thumbnailer.crop, t_width, t_height, t_type\n                )\n            elif t_method == \"scale\":\n                t_byte_source = await defer_to_thread(\n                    self.hs.get_reactor(), thumbnailer.scale, t_width, t_height, t_type\n                )\n            else:\n                logger.error(\"Unrecognized method: %r\", t_method)\n                continue\n\n            if not t_byte_source:\n                continue\n\n            file_info = FileInfo(\n                server_name=server_name,\n                file_id=file_id,\n                thumbnail=True,\n                thumbnail_width=t_width,\n                thumbnail_height=t_height,\n                thumbnail_method=t_method,\n                thumbnail_type=t_type,\n                url_cache=url_cache,\n            )\n\n            with self.media_storage.store_into_file(file_info) as (f, fname, finish):\n                try:\n                    await self.media_storage.write_to_file(t_byte_source, f)\n                    await finish()\n                finally:\n                    t_byte_source.close()\n\n                t_len = os.path.getsize(fname)\n\n                # Write to database\n                if server_name:\n                    # Multiple remote media download requests can race (when\n                    # using multiple media repos), so this may throw a violation\n                    # constraint exception. If it does we'll delete the newly\n                    # generated thumbnail from disk (as we're in the ctx\n                    # manager).\n                    #\n                    # However: we've already called `finish()` so we may have\n                    # also written to the storage providers. This is preferable\n                    # to the alternative where we call `finish()` *after* this,\n                    # where we could end up having an entry in the DB but fail\n                    # to write the files to the storage providers.\n                    try:\n                        await self.store.store_remote_media_thumbnail(\n                            server_name,\n                            media_id,\n                            file_id,\n                            t_width,\n                            t_height,\n                            t_type,\n                            t_method,\n                            t_len,\n                        )\n                    except Exception as e:\n                        thumbnail_exists = await self.store.get_remote_media_thumbnail(\n                            server_name, media_id, t_width, t_height, t_type,\n                        )\n                        if not thumbnail_exists:\n                            raise e\n                else:\n                    await self.store.store_local_thumbnail(\n                        media_id, t_width, t_height, t_type, t_method, t_len\n                    )\n\n        return {\"width\": m_width, \"height\": m_height}\n\n    async def delete_old_remote_media(self, before_ts):\n        old_media = await self.store.get_remote_media_before(before_ts)\n\n        deleted = 0\n\n        for media in old_media:\n            origin = media[\"media_origin\"]\n            media_id = media[\"media_id\"]\n            file_id = media[\"filesystem_id\"]\n            key = (origin, media_id)\n\n            logger.info(\"Deleting: %r\", key)\n\n            # TODO: Should we delete from the backup store\n\n            with (await self.remote_media_linearizer.queue(key)):\n                full_path = self.filepaths.remote_media_filepath(origin, file_id)\n                try:\n                    os.remove(full_path)\n                except OSError as e:\n                    logger.warning(\"Failed to remove file: %r\", full_path)\n                    if e.errno == errno.ENOENT:\n                        pass\n                    else:\n                        continue\n\n                thumbnail_dir = self.filepaths.remote_media_thumbnail_dir(\n                    origin, file_id\n                )\n                shutil.rmtree(thumbnail_dir, ignore_errors=True)\n\n                await self.store.delete_remote_media(origin, media_id)\n                deleted += 1\n\n        return {\"deleted\": deleted}\n\n    async def delete_local_media(self, media_id: str) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete the given local or remote media ID from this server\n\n        Args:\n            media_id: The media ID to delete.\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        return await self._remove_local_media_from_disk([media_id])\n\n    async def delete_old_local_media(\n        self, before_ts: int, size_gt: int = 0, keep_profiles: bool = True,\n    ) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete local or remote media from this server by size and timestamp. Removes\n        media files, any thumbnails and cached URLs.\n\n        Args:\n            before_ts: Unix timestamp in ms.\n                       Files that were last used before this timestamp will be deleted\n            size_gt: Size of the media in bytes. Files that are larger will be deleted\n            keep_profiles: Switch to delete also files that are still used in image data\n                           (e.g user profile, room avatar)\n                           If false these files will be deleted\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        old_media = await self.store.get_local_media_before(\n            before_ts, size_gt, keep_profiles,\n        )\n        return await self._remove_local_media_from_disk(old_media)\n\n    async def _remove_local_media_from_disk(\n        self, media_ids: List[str]\n    ) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete local or remote media from this server. Removes media files,\n        any thumbnails and cached URLs.\n\n        Args:\n            media_ids: List of media_id to delete\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        removed_media = []\n        for media_id in media_ids:\n            logger.info(\"Deleting media with ID '%s'\", media_id)\n            full_path = self.filepaths.local_media_filepath(media_id)\n            try:\n                os.remove(full_path)\n            except OSError as e:\n                logger.warning(\"Failed to remove file: %r: %s\", full_path, e)\n                if e.errno == errno.ENOENT:\n                    pass\n                else:\n                    continue\n\n            thumbnail_dir = self.filepaths.local_media_thumbnail_dir(media_id)\n            shutil.rmtree(thumbnail_dir, ignore_errors=True)\n\n            await self.store.delete_remote_media(self.server_name, media_id)\n\n            await self.store.delete_url_cache((media_id,))\n            await self.store.delete_url_cache_media((media_id,))\n\n            removed_media.append(media_id)\n\n        return removed_media, len(removed_media)", "target": 0}, {"function": "class MediaRepositoryResource(Resource):\n    \"\"\"File uploading and downloading.\n\n    Uploads are POSTed to a resource which returns a token which is used to GET\n    the download::\n\n        => POST /_matrix/media/r0/upload HTTP/1.1\n           Content-Type: <media-type>\n           Content-Length: <content-length>\n\n           <media>\n\n        <= HTTP/1.1 200 OK\n           Content-Type: application/json\n\n           { \"content_uri\": \"mxc://<server-name>/<media-id>\" }\n\n        => GET /_matrix/media/r0/download/<server-name>/<media-id> HTTP/1.1\n\n        <= HTTP/1.1 200 OK\n           Content-Type: <media-type>\n           Content-Disposition: attachment;filename=<upload-filename>\n\n           <media>\n\n    Clients can get thumbnails by supplying a desired width and height and\n    thumbnailing method::\n\n        => GET /_matrix/media/r0/thumbnail/<server_name>\n                /<media-id>?width=<w>&height=<h>&method=<m> HTTP/1.1\n\n        <= HTTP/1.1 200 OK\n           Content-Type: image/jpeg or image/png\n\n           <thumbnail>\n\n    The thumbnail methods are \"crop\" and \"scale\". \"scale\" trys to return an\n    image where either the width or the height is smaller than the requested\n    size. The client should then scale and letterbox the image if it needs to\n    fit within a given rectangle. \"crop\" trys to return an image where the\n    width and height are close to the requested size and the aspect matches\n    the requested size. The client should scale the image if it needs to fit\n    within a given rectangle.\n    \"\"\"\n\n    def __init__(self, hs):\n        # If we're not configured to use it, raise if we somehow got here.\n        if not hs.config.can_load_media_repo:\n            raise ConfigError(\"Synapse is not configured to use a media repo.\")\n\n        super().__init__()\n        media_repo = hs.get_media_repository()\n\n        self.putChild(b\"upload\", UploadResource(hs, media_repo))\n        self.putChild(b\"download\", DownloadResource(hs, media_repo))\n        self.putChild(\n            b\"thumbnail\", ThumbnailResource(hs, media_repo, media_repo.media_storage)\n        )\n        if hs.config.url_preview_enabled:\n            self.putChild(\n                b\"preview_url\",\n                PreviewUrlResource(hs, media_repo, media_repo.media_storage),\n            )\n        self.putChild(b\"config\", MediaConfigResource(hs))", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/synapse%2Fserver.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2017-2018 New Vector Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# This file provides some classes for setting up (partially-populated)\n# homeservers; either as a full homeserver as a real application, or a small\n# partial one for unit test mocking.\n\n# Imports required for the default HomeServer() implementation\nimport abc\nimport functools\nimport logging\nimport os\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, TypeVar, cast\n\nimport twisted.internet.base\nimport twisted.internet.tcp\nfrom twisted.mail.smtp import sendmail\nfrom twisted.web.iweb import IPolicyForHTTPS\n\nfrom synapse.api.auth import Auth\nfrom synapse.api.filtering import Filtering\nfrom synapse.api.ratelimiting import Ratelimiter\nfrom synapse.appservice.api import ApplicationServiceApi\nfrom synapse.appservice.scheduler import ApplicationServiceScheduler\nfrom synapse.config.homeserver import HomeServerConfig\nfrom synapse.crypto import context_factory\nfrom synapse.crypto.context_factory import RegularPolicyForHTTPS\nfrom synapse.crypto.keyring import Keyring\nfrom synapse.events.builder import EventBuilderFactory\nfrom synapse.events.spamcheck import SpamChecker\nfrom synapse.events.third_party_rules import ThirdPartyEventRules\nfrom synapse.events.utils import EventClientSerializer\nfrom synapse.federation.federation_client import FederationClient\nfrom synapse.federation.federation_server import (\n    FederationHandlerRegistry,\n    FederationServer,\n)\nfrom synapse.federation.send_queue import FederationRemoteSendQueue\nfrom synapse.federation.sender import FederationSender\nfrom synapse.federation.transport.client import TransportLayerClient\nfrom synapse.groups.attestations import GroupAttestationSigning, GroupAttestionRenewer\nfrom synapse.groups.groups_server import GroupsServerHandler, GroupsServerWorkerHandler\nfrom synapse.handlers.account_validity import AccountValidityHandler\nfrom synapse.handlers.acme import AcmeHandler\nfrom synapse.handlers.admin import AdminHandler\nfrom synapse.handlers.appservice import ApplicationServicesHandler\nfrom synapse.handlers.auth import AuthHandler, MacaroonGenerator\nfrom synapse.handlers.cas_handler import CasHandler\nfrom synapse.handlers.deactivate_account import DeactivateAccountHandler\nfrom synapse.handlers.device import DeviceHandler, DeviceWorkerHandler\nfrom synapse.handlers.devicemessage import DeviceMessageHandler\nfrom synapse.handlers.directory import DirectoryHandler\nfrom synapse.handlers.e2e_keys import E2eKeysHandler\nfrom synapse.handlers.e2e_room_keys import E2eRoomKeysHandler\nfrom synapse.handlers.events import EventHandler, EventStreamHandler\nfrom synapse.handlers.federation import FederationHandler\nfrom synapse.handlers.groups_local import GroupsLocalHandler, GroupsLocalWorkerHandler\nfrom synapse.handlers.identity import IdentityHandler\nfrom synapse.handlers.initial_sync import InitialSyncHandler\nfrom synapse.handlers.message import EventCreationHandler, MessageHandler\nfrom synapse.handlers.pagination import PaginationHandler\nfrom synapse.handlers.password_policy import PasswordPolicyHandler\nfrom synapse.handlers.presence import PresenceHandler\nfrom synapse.handlers.profile import ProfileHandler\nfrom synapse.handlers.read_marker import ReadMarkerHandler\nfrom synapse.handlers.receipts import ReceiptsHandler\nfrom synapse.handlers.register import RegistrationHandler\nfrom synapse.handlers.room import (\n    RoomContextHandler,\n    RoomCreationHandler,\n    RoomShutdownHandler,\n)\nfrom synapse.handlers.room_list import RoomListHandler\nfrom synapse.handlers.room_member import RoomMemberMasterHandler\nfrom synapse.handlers.room_member_worker import RoomMemberWorkerHandler\nfrom synapse.handlers.search import SearchHandler\nfrom synapse.handlers.set_password import SetPasswordHandler\nfrom synapse.handlers.sso import SsoHandler\nfrom synapse.handlers.stats import StatsHandler\nfrom synapse.handlers.sync import SyncHandler\nfrom synapse.handlers.typing import FollowerTypingHandler, TypingWriterHandler\nfrom synapse.handlers.user_directory import UserDirectoryHandler\nfrom synapse.http.client import InsecureInterceptableContextFactory, SimpleHttpClient\nfrom synapse.http.matrixfederationclient import MatrixFederationHttpClient\nfrom synapse.module_api import ModuleApi\nfrom synapse.notifier import Notifier\nfrom synapse.push.action_generator import ActionGenerator\nfrom synapse.push.pusherpool import PusherPool\nfrom synapse.replication.tcp.client import ReplicationDataHandler\nfrom synapse.replication.tcp.handler import ReplicationCommandHandler\nfrom synapse.replication.tcp.resource import ReplicationStreamer\nfrom synapse.replication.tcp.streams import STREAMS_MAP, Stream\nfrom synapse.rest.media.v1.media_repository import (\n    MediaRepository,\n    MediaRepositoryResource,\n)\nfrom synapse.secrets import Secrets\nfrom synapse.server_notices.server_notices_manager import ServerNoticesManager\nfrom synapse.server_notices.server_notices_sender import ServerNoticesSender\nfrom synapse.server_notices.worker_server_notices_sender import (\n    WorkerServerNoticesSender,\n)\nfrom synapse.state import StateHandler, StateResolutionHandler\nfrom synapse.storage import Databases, DataStore, Storage\nfrom synapse.streams.events import EventSources\nfrom synapse.types import DomainSpecificString\nfrom synapse.util import Clock\nfrom synapse.util.distributor import Distributor\nfrom synapse.util.ratelimitutils import FederationRateLimiter\nfrom synapse.util.stringutils import random_string\n\nlogger = logging.getLogger(__name__)\n\nif TYPE_CHECKING:\n    from synapse.handlers.oidc_handler import OidcHandler\n    from synapse.handlers.saml_handler import SamlHandler\n\n\nT = TypeVar(\"T\", bound=Callable[..., Any])\n\n\ndef cache_in_self(builder: T) -> T:\n    \"\"\"Wraps a function called e.g. `get_foo`, checking if `self.foo` exists and\n    returning if so. If not, calls the given function and sets `self.foo` to it.\n\n    Also ensures that dependency cycles throw an exception correctly, rather\n    than overflowing the stack.\n    \"\"\"\n\n    if not builder.__name__.startswith(\"get_\"):\n        raise Exception(\n            \"@cache_in_self can only be used on functions starting with `get_`\"\n        )\n\n    # get_attr -> _attr\n    depname = builder.__name__[len(\"get\") :]\n\n    building = [False]\n\n    @functools.wraps(builder)\n    def _get(self):\n        try:\n            return getattr(self, depname)\n        except AttributeError:\n            pass\n\n        # Prevent cyclic dependencies from deadlocking\n        if building[0]:\n            raise ValueError(\"Cyclic dependency while building %s\" % (depname,))\n\n        building[0] = True\n        try:\n            dep = builder(self)\n            setattr(self, depname, dep)\n        finally:\n            building[0] = False\n\n        return dep\n\n    # We cast here as we need to tell mypy that `_get` has the same signature as\n    # `builder`.\n    return cast(T, _get)\n\n\nclass HomeServer(metaclass=abc.ABCMeta):\n    \"\"\"A basic homeserver object without lazy component builders.\n\n    This will need all of the components it requires to either be passed as\n    constructor arguments, or the relevant methods overriding to create them.\n    Typically this would only be used for unit tests.\n\n    Dependencies should be added by creating a `def get_<depname>(self)`\n    function, wrapping it in `@cache_in_self`.\n\n    Attributes:\n        config (synapse.config.homeserver.HomeserverConfig):\n        _listening_services (list[twisted.internet.tcp.Port]): TCP ports that\n            we are listening on to provide HTTP services.\n    \"\"\"\n\n    REQUIRED_ON_BACKGROUND_TASK_STARTUP = [\n        \"account_validity\",\n        \"auth\",\n        \"deactivate_account\",\n        \"message\",\n        \"pagination\",\n        \"profile\",\n        \"stats\",\n    ]\n\n    # This is overridden in derived application classes\n    # (such as synapse.app.homeserver.SynapseHomeServer) and gives the class to be\n    # instantiated during setup() for future return by get_datastore()\n    DATASTORE_CLASS = abc.abstractproperty()\n\n    def __init__(\n        self,\n        hostname: str,\n        config: HomeServerConfig,\n        reactor=None,\n        version_string=\"Synapse\",\n    ):\n        \"\"\"\n        Args:\n            hostname : The hostname for the server.\n            config: The full config for the homeserver.\n        \"\"\"\n        if not reactor:\n            from twisted.internet import reactor as _reactor\n\n            reactor = _reactor\n\n        self._reactor = reactor\n        self.hostname = hostname\n        # the key we use to sign events and requests\n        self.signing_key = config.key.signing_key[0]\n        self.config = config\n        self._listening_services = []  # type: List[twisted.internet.tcp.Port]\n        self.start_time = None  # type: Optional[int]\n\n        self._instance_id = random_string(5)\n        self._instance_name = config.worker_name or \"master\"\n\n        self.version_string = version_string\n\n        self.datastores = None  # type: Optional[Databases]\n\n    def get_instance_id(self) -> str:\n        \"\"\"A unique ID for this synapse process instance.\n\n        This is used to distinguish running instances in worker-based\n        deployments.\n        \"\"\"\n        return self._instance_id\n\n    def get_instance_name(self) -> str:\n        \"\"\"A unique name for this synapse process.\n\n        Used to identify the process over replication and in config. Does not\n        change over restarts.\n        \"\"\"\n        return self._instance_name\n\n    def setup(self) -> None:\n        logger.info(\"Setting up.\")\n        self.start_time = int(self.get_clock().time())\n        self.datastores = Databases(self.DATASTORE_CLASS, self)\n        logger.info(\"Finished setting up.\")\n\n        # Register background tasks required by this server. This must be done\n        # somewhat manually due to the background tasks not being registered\n        # unless handlers are instantiated.\n        if self.config.run_background_tasks:\n            self.setup_background_tasks()\n\n    def setup_background_tasks(self) -> None:\n        \"\"\"\n        Some handlers have side effects on instantiation (like registering\n        background updates). This function causes them to be fetched, and\n        therefore instantiated, to run those side effects.\n        \"\"\"\n        for i in self.REQUIRED_ON_BACKGROUND_TASK_STARTUP:\n            getattr(self, \"get_\" + i + \"_handler\")()\n\n    def get_reactor(self) -> twisted.internet.base.ReactorBase:\n        \"\"\"\n        Fetch the Twisted reactor in use by this HomeServer.\n        \"\"\"\n        return self._reactor\n\n    def get_ip_from_request(self, request) -> str:\n        # X-Forwarded-For is handled by our custom request type.\n        return request.getClientIP()\n\n    def is_mine(self, domain_specific_string: DomainSpecificString) -> bool:\n        return domain_specific_string.domain == self.hostname\n\n    def is_mine_id(self, string: str) -> bool:\n        return string.split(\":\", 1)[1] == self.hostname\n\n    @cache_in_self\n    def get_clock(self) -> Clock:\n        return Clock(self._reactor)\n\n    def get_datastore(self) -> DataStore:\n        if not self.datastores:\n            raise Exception(\"HomeServer.setup must be called before getting datastores\")\n\n        return self.datastores.main\n\n    def get_datastores(self) -> Databases:\n        if not self.datastores:\n            raise Exception(\"HomeServer.setup must be called before getting datastores\")\n\n        return self.datastores\n\n    def get_config(self) -> HomeServerConfig:\n        return self.config\n\n    @cache_in_self\n    def get_distributor(self) -> Distributor:\n        return Distributor()\n\n    @cache_in_self\n    def get_registration_ratelimiter(self) -> Ratelimiter:\n        return Ratelimiter(\n            clock=self.get_clock(),\n            rate_hz=self.config.rc_registration.per_second,\n            burst_count=self.config.rc_registration.burst_count,\n        )\n\n    @cache_in_self\n    def get_federation_client(self) -> FederationClient:\n        return FederationClient(self)\n\n    @cache_in_self\n    def get_federation_server(self) -> FederationServer:\n        return FederationServer(self)\n\n    @cache_in_self\n    def get_notifier(self) -> Notifier:\n        return Notifier(self)\n\n    @cache_in_self\n    def get_auth(self) -> Auth:\n        return Auth(self)\n\n    @cache_in_self\n    def get_http_client_context_factory(self) -> IPolicyForHTTPS:\n        return (\n            InsecureInterceptableContextFactory()\n            if self.config.use_insecure_ssl_client_just_for_testing_do_not_use\n            else RegularPolicyForHTTPS()\n        )\n\n    @cache_in_self\n    def get_simple_http_client(self) -> SimpleHttpClient:\n        \"\"\"\n        An HTTP client with no special configuration.\n        \"\"\"\n        return SimpleHttpClient(self)\n\n    @cache_in_self\n    def get_proxied_http_client(self) -> SimpleHttpClient:\n        \"\"\"\n        An HTTP client that uses configured HTTP(S) proxies.\n        \"\"\"\n        return SimpleHttpClient(\n            self,\n            http_proxy=os.getenvb(b\"http_proxy\"),\n            https_proxy=os.getenvb(b\"HTTPS_PROXY\"),\n        )\n\n    @cache_in_self\n    def get_proxied_blacklisted_http_client(self) -> SimpleHttpClient:\n        \"\"\"\n        An HTTP client that uses configured HTTP(S) proxies and blacklists IPs\n        based on the IP range blacklist.\n        \"\"\"\n        return SimpleHttpClient(\n            self,\n            ip_blacklist=self.config.ip_range_blacklist,\n            http_proxy=os.getenvb(b\"http_proxy\"),\n            https_proxy=os.getenvb(b\"HTTPS_PROXY\"),\n        )\n\n    @cache_in_self\n    def get_federation_http_client(self) -> MatrixFederationHttpClient:\n        \"\"\"\n        An HTTP client for federation.\n        \"\"\"\n        tls_client_options_factory = context_factory.FederationPolicyForHTTPS(\n            self.config\n        )\n        return MatrixFederationHttpClient(self, tls_client_options_factory)\n\n    @cache_in_self\n    def get_room_creation_handler(self) -> RoomCreationHandler:\n        return RoomCreationHandler(self)\n\n    @cache_in_self\n    def get_room_shutdown_handler(self) -> RoomShutdownHandler:\n        return RoomShutdownHandler(self)\n\n    @cache_in_self\n    def get_sendmail(self) -> sendmail:\n        return sendmail\n\n    @cache_in_self\n    def get_state_handler(self) -> StateHandler:\n        return StateHandler(self)\n\n    @cache_in_self\n    def get_state_resolution_handler(self) -> StateResolutionHandler:\n        return StateResolutionHandler(self)\n\n    @cache_in_self\n    def get_presence_handler(self) -> PresenceHandler:\n        return PresenceHandler(self)\n\n    @cache_in_self\n    def get_typing_handler(self):\n        if self.config.worker.writers.typing == self.get_instance_name():\n            return TypingWriterHandler(self)\n        else:\n            return FollowerTypingHandler(self)\n\n    @cache_in_self\n    def get_sso_handler(self) -> SsoHandler:\n        return SsoHandler(self)\n\n    @cache_in_self\n    def get_sync_handler(self) -> SyncHandler:\n        return SyncHandler(self)\n\n    @cache_in_self\n    def get_room_list_handler(self) -> RoomListHandler:\n        return RoomListHandler(self)\n\n    @cache_in_self\n    def get_auth_handler(self) -> AuthHandler:\n        return AuthHandler(self)\n\n    @cache_in_self\n    def get_macaroon_generator(self) -> MacaroonGenerator:\n        return MacaroonGenerator(self)\n\n    @cache_in_self\n    def get_device_handler(self):\n        if self.config.worker_app:\n            return DeviceWorkerHandler(self)\n        else:\n            return DeviceHandler(self)\n\n    @cache_in_self\n    def get_device_message_handler(self) -> DeviceMessageHandler:\n        return DeviceMessageHandler(self)\n\n    @cache_in_self\n    def get_directory_handler(self) -> DirectoryHandler:\n        return DirectoryHandler(self)\n\n    @cache_in_self\n    def get_e2e_keys_handler(self) -> E2eKeysHandler:\n        return E2eKeysHandler(self)\n\n    @cache_in_self\n    def get_e2e_room_keys_handler(self) -> E2eRoomKeysHandler:\n        return E2eRoomKeysHandler(self)\n\n    @cache_in_self\n    def get_acme_handler(self) -> AcmeHandler:\n        return AcmeHandler(self)\n\n    @cache_in_self\n    def get_admin_handler(self) -> AdminHandler:\n        return AdminHandler(self)\n\n    @cache_in_self\n    def get_application_service_api(self) -> ApplicationServiceApi:\n        return ApplicationServiceApi(self)\n\n    @cache_in_self\n    def get_application_service_scheduler(self) -> ApplicationServiceScheduler:\n        return ApplicationServiceScheduler(self)\n\n    @cache_in_self\n    def get_application_service_handler(self) -> ApplicationServicesHandler:\n        return ApplicationServicesHandler(self)\n\n    @cache_in_self\n    def get_event_handler(self) -> EventHandler:\n        return EventHandler(self)\n\n    @cache_in_self\n    def get_event_stream_handler(self) -> EventStreamHandler:\n        return EventStreamHandler(self)\n\n    @cache_in_self\n    def get_federation_handler(self) -> FederationHandler:\n        return FederationHandler(self)\n\n    @cache_in_self\n    def get_identity_handler(self) -> IdentityHandler:\n        return IdentityHandler(self)\n\n    @cache_in_self\n    def get_initial_sync_handler(self) -> InitialSyncHandler:\n        return InitialSyncHandler(self)\n\n    @cache_in_self\n    def get_profile_handler(self):\n        return ProfileHandler(self)\n\n    @cache_in_self\n    def get_event_creation_handler(self) -> EventCreationHandler:\n        return EventCreationHandler(self)\n\n    @cache_in_self\n    def get_deactivate_account_handler(self) -> DeactivateAccountHandler:\n        return DeactivateAccountHandler(self)\n\n    @cache_in_self\n    def get_search_handler(self) -> SearchHandler:\n        return SearchHandler(self)\n\n    @cache_in_self\n    def get_set_password_handler(self) -> SetPasswordHandler:\n        return SetPasswordHandler(self)\n\n    @cache_in_self\n    def get_event_sources(self) -> EventSources:\n        return EventSources(self)\n\n    @cache_in_self\n    def get_keyring(self) -> Keyring:\n        return Keyring(self)\n\n    @cache_in_self\n    def get_event_builder_factory(self) -> EventBuilderFactory:\n        return EventBuilderFactory(self)\n\n    @cache_in_self\n    def get_filtering(self) -> Filtering:\n        return Filtering(self)\n\n    @cache_in_self\n    def get_pusherpool(self) -> PusherPool:\n        return PusherPool(self)\n\n    @cache_in_self\n    def get_media_repository_resource(self) -> MediaRepositoryResource:\n        # build the media repo resource. This indirects through the HomeServer\n        # to ensure that we only have a single instance of\n        return MediaRepositoryResource(self)\n\n    @cache_in_self\n    def get_media_repository(self) -> MediaRepository:\n        return MediaRepository(self)\n\n    @cache_in_self\n    def get_federation_transport_client(self) -> TransportLayerClient:\n        return TransportLayerClient(self)\n\n    @cache_in_self\n    def get_federation_sender(self):\n        if self.should_send_federation():\n            return FederationSender(self)\n        elif not self.config.worker_app:\n            return FederationRemoteSendQueue(self)\n        else:\n            raise Exception(\"Workers cannot send federation traffic\")\n\n    @cache_in_self\n    def get_receipts_handler(self) -> ReceiptsHandler:\n        return ReceiptsHandler(self)\n\n    @cache_in_self\n    def get_read_marker_handler(self) -> ReadMarkerHandler:\n        return ReadMarkerHandler(self)\n\n    @cache_in_self\n    def get_tcp_replication(self) -> ReplicationCommandHandler:\n        return ReplicationCommandHandler(self)\n\n    @cache_in_self\n    def get_action_generator(self) -> ActionGenerator:\n        return ActionGenerator(self)\n\n    @cache_in_self\n    def get_user_directory_handler(self) -> UserDirectoryHandler:\n        return UserDirectoryHandler(self)\n\n    @cache_in_self\n    def get_groups_local_handler(self):\n        if self.config.worker_app:\n            return GroupsLocalWorkerHandler(self)\n        else:\n            return GroupsLocalHandler(self)\n\n    @cache_in_self\n    def get_groups_server_handler(self):\n        if self.config.worker_app:\n            return GroupsServerWorkerHandler(self)\n        else:\n            return GroupsServerHandler(self)\n\n    @cache_in_self\n    def get_groups_attestation_signing(self) -> GroupAttestationSigning:\n        return GroupAttestationSigning(self)\n\n    @cache_in_self\n    def get_groups_attestation_renewer(self) -> GroupAttestionRenewer:\n        return GroupAttestionRenewer(self)\n\n    @cache_in_self\n    def get_secrets(self) -> Secrets:\n        return Secrets()\n\n    @cache_in_self\n    def get_stats_handler(self) -> StatsHandler:\n        return StatsHandler(self)\n\n    @cache_in_self\n    def get_spam_checker(self):\n        return SpamChecker(self)\n\n    @cache_in_self\n    def get_third_party_event_rules(self) -> ThirdPartyEventRules:\n        return ThirdPartyEventRules(self)\n\n    @cache_in_self\n    def get_room_member_handler(self):\n        if self.config.worker_app:\n            return RoomMemberWorkerHandler(self)\n        return RoomMemberMasterHandler(self)\n\n    @cache_in_self\n    def get_federation_registry(self) -> FederationHandlerRegistry:\n        return FederationHandlerRegistry(self)\n\n    @cache_in_self\n    def get_server_notices_manager(self):\n        if self.config.worker_app:\n            raise Exception(\"Workers cannot send server notices\")\n        return ServerNoticesManager(self)\n\n    @cache_in_self\n    def get_server_notices_sender(self):\n        if self.config.worker_app:\n            return WorkerServerNoticesSender(self)\n        return ServerNoticesSender(self)\n\n    @cache_in_self\n    def get_message_handler(self) -> MessageHandler:\n        return MessageHandler(self)\n\n    @cache_in_self\n    def get_pagination_handler(self) -> PaginationHandler:\n        return PaginationHandler(self)\n\n    @cache_in_self\n    def get_room_context_handler(self) -> RoomContextHandler:\n        return RoomContextHandler(self)\n\n    @cache_in_self\n    def get_registration_handler(self) -> RegistrationHandler:\n        return RegistrationHandler(self)\n\n    @cache_in_self\n    def get_account_validity_handler(self) -> AccountValidityHandler:\n        return AccountValidityHandler(self)\n\n    @cache_in_self\n    def get_cas_handler(self) -> CasHandler:\n        return CasHandler(self)\n\n    @cache_in_self\n    def get_saml_handler(self) -> \"SamlHandler\":\n        from synapse.handlers.saml_handler import SamlHandler\n\n        return SamlHandler(self)\n\n    @cache_in_self\n    def get_oidc_handler(self) -> \"OidcHandler\":\n        from synapse.handlers.oidc_handler import OidcHandler\n\n        return OidcHandler(self)\n\n    @cache_in_self\n    def get_event_client_serializer(self) -> EventClientSerializer:\n        return EventClientSerializer(self)\n\n    @cache_in_self\n    def get_password_policy_handler(self) -> PasswordPolicyHandler:\n        return PasswordPolicyHandler(self)\n\n    @cache_in_self\n    def get_storage(self) -> Storage:\n        return Storage(self, self.get_datastores())\n\n    @cache_in_self\n    def get_replication_streamer(self) -> ReplicationStreamer:\n        return ReplicationStreamer(self)\n\n    @cache_in_self\n    def get_replication_data_handler(self) -> ReplicationDataHandler:\n        return ReplicationDataHandler(self)\n\n    @cache_in_self\n    def get_replication_streams(self) -> Dict[str, Stream]:\n        return {stream.NAME: stream(self) for stream in STREAMS_MAP.values()}\n\n    @cache_in_self\n    def get_federation_ratelimiter(self) -> FederationRateLimiter:\n        return FederationRateLimiter(self.get_clock(), config=self.config.rc_federation)\n\n    @cache_in_self\n    def get_module_api(self) -> ModuleApi:\n        return ModuleApi(self, self.get_auth_handler())\n\n    async def remove_pusher(self, app_id: str, push_key: str, user_id: str):\n        return await self.get_pusherpool().remove_pusher(app_id, push_key, user_id)\n\n    def should_send_federation(self) -> bool:\n        \"Should this server be sending federation traffic directly?\"\n        return self.config.send_federation and (\n            not self.config.worker_app\n            or self.config.worker_app == \"synapse.app.federation_sender\"\n        )\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2017-2018 New Vector Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# This file provides some classes for setting up (partially-populated)\n# homeservers; either as a full homeserver as a real application, or a small\n# partial one for unit test mocking.\n\n# Imports required for the default HomeServer() implementation\nimport abc\nimport functools\nimport logging\nimport os\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, TypeVar, cast\n\nimport twisted.internet.base\nimport twisted.internet.tcp\nfrom twisted.mail.smtp import sendmail\nfrom twisted.web.iweb import IPolicyForHTTPS\n\nfrom synapse.api.auth import Auth\nfrom synapse.api.filtering import Filtering\nfrom synapse.api.ratelimiting import Ratelimiter\nfrom synapse.appservice.api import ApplicationServiceApi\nfrom synapse.appservice.scheduler import ApplicationServiceScheduler\nfrom synapse.config.homeserver import HomeServerConfig\nfrom synapse.crypto import context_factory\nfrom synapse.crypto.context_factory import RegularPolicyForHTTPS\nfrom synapse.crypto.keyring import Keyring\nfrom synapse.events.builder import EventBuilderFactory\nfrom synapse.events.spamcheck import SpamChecker\nfrom synapse.events.third_party_rules import ThirdPartyEventRules\nfrom synapse.events.utils import EventClientSerializer\nfrom synapse.federation.federation_client import FederationClient\nfrom synapse.federation.federation_server import (\n    FederationHandlerRegistry,\n    FederationServer,\n)\nfrom synapse.federation.send_queue import FederationRemoteSendQueue\nfrom synapse.federation.sender import FederationSender\nfrom synapse.federation.transport.client import TransportLayerClient\nfrom synapse.groups.attestations import GroupAttestationSigning, GroupAttestionRenewer\nfrom synapse.groups.groups_server import GroupsServerHandler, GroupsServerWorkerHandler\nfrom synapse.handlers.account_validity import AccountValidityHandler\nfrom synapse.handlers.acme import AcmeHandler\nfrom synapse.handlers.admin import AdminHandler\nfrom synapse.handlers.appservice import ApplicationServicesHandler\nfrom synapse.handlers.auth import AuthHandler, MacaroonGenerator\nfrom synapse.handlers.cas_handler import CasHandler\nfrom synapse.handlers.deactivate_account import DeactivateAccountHandler\nfrom synapse.handlers.device import DeviceHandler, DeviceWorkerHandler\nfrom synapse.handlers.devicemessage import DeviceMessageHandler\nfrom synapse.handlers.directory import DirectoryHandler\nfrom synapse.handlers.e2e_keys import E2eKeysHandler\nfrom synapse.handlers.e2e_room_keys import E2eRoomKeysHandler\nfrom synapse.handlers.events import EventHandler, EventStreamHandler\nfrom synapse.handlers.federation import FederationHandler\nfrom synapse.handlers.groups_local import GroupsLocalHandler, GroupsLocalWorkerHandler\nfrom synapse.handlers.identity import IdentityHandler\nfrom synapse.handlers.initial_sync import InitialSyncHandler\nfrom synapse.handlers.message import EventCreationHandler, MessageHandler\nfrom synapse.handlers.pagination import PaginationHandler\nfrom synapse.handlers.password_policy import PasswordPolicyHandler\nfrom synapse.handlers.presence import PresenceHandler\nfrom synapse.handlers.profile import ProfileHandler\nfrom synapse.handlers.read_marker import ReadMarkerHandler\nfrom synapse.handlers.receipts import ReceiptsHandler\nfrom synapse.handlers.register import RegistrationHandler\nfrom synapse.handlers.room import (\n    RoomContextHandler,\n    RoomCreationHandler,\n    RoomShutdownHandler,\n)\nfrom synapse.handlers.room_list import RoomListHandler\nfrom synapse.handlers.room_member import RoomMemberMasterHandler\nfrom synapse.handlers.room_member_worker import RoomMemberWorkerHandler\nfrom synapse.handlers.search import SearchHandler\nfrom synapse.handlers.set_password import SetPasswordHandler\nfrom synapse.handlers.sso import SsoHandler\nfrom synapse.handlers.stats import StatsHandler\nfrom synapse.handlers.sync import SyncHandler\nfrom synapse.handlers.typing import FollowerTypingHandler, TypingWriterHandler\nfrom synapse.handlers.user_directory import UserDirectoryHandler\nfrom synapse.http.client import InsecureInterceptableContextFactory, SimpleHttpClient\nfrom synapse.http.matrixfederationclient import MatrixFederationHttpClient\nfrom synapse.module_api import ModuleApi\nfrom synapse.notifier import Notifier\nfrom synapse.push.action_generator import ActionGenerator\nfrom synapse.push.pusherpool import PusherPool\nfrom synapse.replication.tcp.client import ReplicationDataHandler\nfrom synapse.replication.tcp.handler import ReplicationCommandHandler\nfrom synapse.replication.tcp.resource import ReplicationStreamer\nfrom synapse.replication.tcp.streams import STREAMS_MAP, Stream\nfrom synapse.rest.media.v1.media_repository import (\n    MediaRepository,\n    MediaRepositoryResource,\n)\nfrom synapse.secrets import Secrets\nfrom synapse.server_notices.server_notices_manager import ServerNoticesManager\nfrom synapse.server_notices.server_notices_sender import ServerNoticesSender\nfrom synapse.server_notices.worker_server_notices_sender import (\n    WorkerServerNoticesSender,\n)\nfrom synapse.state import StateHandler, StateResolutionHandler\nfrom synapse.storage import Databases, DataStore, Storage\nfrom synapse.streams.events import EventSources\nfrom synapse.types import DomainSpecificString\nfrom synapse.util import Clock\nfrom synapse.util.distributor import Distributor\nfrom synapse.util.ratelimitutils import FederationRateLimiter\nfrom synapse.util.stringutils import random_string\n\nlogger = logging.getLogger(__name__)\n\nif TYPE_CHECKING:\n    from synapse.handlers.oidc_handler import OidcHandler\n    from synapse.handlers.saml_handler import SamlHandler\n\n\nT = TypeVar(\"T\", bound=Callable[..., Any])\n\n\ndef cache_in_self(builder: T) -> T:\n    \"\"\"Wraps a function called e.g. `get_foo`, checking if `self.foo` exists and\n    returning if so. If not, calls the given function and sets `self.foo` to it.\n\n    Also ensures that dependency cycles throw an exception correctly, rather\n    than overflowing the stack.\n    \"\"\"\n\n    if not builder.__name__.startswith(\"get_\"):\n        raise Exception(\n            \"@cache_in_self can only be used on functions starting with `get_`\"\n        )\n\n    # get_attr -> _attr\n    depname = builder.__name__[len(\"get\") :]\n\n    building = [False]\n\n    @functools.wraps(builder)\n    def _get(self):\n        try:\n            return getattr(self, depname)\n        except AttributeError:\n            pass\n\n        # Prevent cyclic dependencies from deadlocking\n        if building[0]:\n            raise ValueError(\"Cyclic dependency while building %s\" % (depname,))\n\n        building[0] = True\n        try:\n            dep = builder(self)\n            setattr(self, depname, dep)\n        finally:\n            building[0] = False\n\n        return dep\n\n    # We cast here as we need to tell mypy that `_get` has the same signature as\n    # `builder`.\n    return cast(T, _get)\n\n\nclass HomeServer(metaclass=abc.ABCMeta):\n    \"\"\"A basic homeserver object without lazy component builders.\n\n    This will need all of the components it requires to either be passed as\n    constructor arguments, or the relevant methods overriding to create them.\n    Typically this would only be used for unit tests.\n\n    Dependencies should be added by creating a `def get_<depname>(self)`\n    function, wrapping it in `@cache_in_self`.\n\n    Attributes:\n        config (synapse.config.homeserver.HomeserverConfig):\n        _listening_services (list[twisted.internet.tcp.Port]): TCP ports that\n            we are listening on to provide HTTP services.\n    \"\"\"\n\n    REQUIRED_ON_BACKGROUND_TASK_STARTUP = [\n        \"account_validity\",\n        \"auth\",\n        \"deactivate_account\",\n        \"message\",\n        \"pagination\",\n        \"profile\",\n        \"stats\",\n    ]\n\n    # This is overridden in derived application classes\n    # (such as synapse.app.homeserver.SynapseHomeServer) and gives the class to be\n    # instantiated during setup() for future return by get_datastore()\n    DATASTORE_CLASS = abc.abstractproperty()\n\n    def __init__(\n        self,\n        hostname: str,\n        config: HomeServerConfig,\n        reactor=None,\n        version_string=\"Synapse\",\n    ):\n        \"\"\"\n        Args:\n            hostname : The hostname for the server.\n            config: The full config for the homeserver.\n        \"\"\"\n        if not reactor:\n            from twisted.internet import reactor as _reactor\n\n            reactor = _reactor\n\n        self._reactor = reactor\n        self.hostname = hostname\n        # the key we use to sign events and requests\n        self.signing_key = config.key.signing_key[0]\n        self.config = config\n        self._listening_services = []  # type: List[twisted.internet.tcp.Port]\n        self.start_time = None  # type: Optional[int]\n\n        self._instance_id = random_string(5)\n        self._instance_name = config.worker_name or \"master\"\n\n        self.version_string = version_string\n\n        self.datastores = None  # type: Optional[Databases]\n\n    def get_instance_id(self) -> str:\n        \"\"\"A unique ID for this synapse process instance.\n\n        This is used to distinguish running instances in worker-based\n        deployments.\n        \"\"\"\n        return self._instance_id\n\n    def get_instance_name(self) -> str:\n        \"\"\"A unique name for this synapse process.\n\n        Used to identify the process over replication and in config. Does not\n        change over restarts.\n        \"\"\"\n        return self._instance_name\n\n    def setup(self) -> None:\n        logger.info(\"Setting up.\")\n        self.start_time = int(self.get_clock().time())\n        self.datastores = Databases(self.DATASTORE_CLASS, self)\n        logger.info(\"Finished setting up.\")\n\n        # Register background tasks required by this server. This must be done\n        # somewhat manually due to the background tasks not being registered\n        # unless handlers are instantiated.\n        if self.config.run_background_tasks:\n            self.setup_background_tasks()\n\n    def setup_background_tasks(self) -> None:\n        \"\"\"\n        Some handlers have side effects on instantiation (like registering\n        background updates). This function causes them to be fetched, and\n        therefore instantiated, to run those side effects.\n        \"\"\"\n        for i in self.REQUIRED_ON_BACKGROUND_TASK_STARTUP:\n            getattr(self, \"get_\" + i + \"_handler\")()\n\n    def get_reactor(self) -> twisted.internet.base.ReactorBase:\n        \"\"\"\n        Fetch the Twisted reactor in use by this HomeServer.\n        \"\"\"\n        return self._reactor\n\n    def get_ip_from_request(self, request) -> str:\n        # X-Forwarded-For is handled by our custom request type.\n        return request.getClientIP()\n\n    def is_mine(self, domain_specific_string: DomainSpecificString) -> bool:\n        return domain_specific_string.domain == self.hostname\n\n    def is_mine_id(self, string: str) -> bool:\n        return string.split(\":\", 1)[1] == self.hostname\n\n    @cache_in_self\n    def get_clock(self) -> Clock:\n        return Clock(self._reactor)\n\n    def get_datastore(self) -> DataStore:\n        if not self.datastores:\n            raise Exception(\"HomeServer.setup must be called before getting datastores\")\n\n        return self.datastores.main\n\n    def get_datastores(self) -> Databases:\n        if not self.datastores:\n            raise Exception(\"HomeServer.setup must be called before getting datastores\")\n\n        return self.datastores\n\n    def get_config(self) -> HomeServerConfig:\n        return self.config\n\n    @cache_in_self\n    def get_distributor(self) -> Distributor:\n        return Distributor()\n\n    @cache_in_self\n    def get_registration_ratelimiter(self) -> Ratelimiter:\n        return Ratelimiter(\n            clock=self.get_clock(),\n            rate_hz=self.config.rc_registration.per_second,\n            burst_count=self.config.rc_registration.burst_count,\n        )\n\n    @cache_in_self\n    def get_federation_client(self) -> FederationClient:\n        return FederationClient(self)\n\n    @cache_in_self\n    def get_federation_server(self) -> FederationServer:\n        return FederationServer(self)\n\n    @cache_in_self\n    def get_notifier(self) -> Notifier:\n        return Notifier(self)\n\n    @cache_in_self\n    def get_auth(self) -> Auth:\n        return Auth(self)\n\n    @cache_in_self\n    def get_http_client_context_factory(self) -> IPolicyForHTTPS:\n        return (\n            InsecureInterceptableContextFactory()\n            if self.config.use_insecure_ssl_client_just_for_testing_do_not_use\n            else RegularPolicyForHTTPS()\n        )\n\n    @cache_in_self\n    def get_simple_http_client(self) -> SimpleHttpClient:\n        return SimpleHttpClient(self)\n\n    @cache_in_self\n    def get_proxied_http_client(self) -> SimpleHttpClient:\n        return SimpleHttpClient(\n            self,\n            http_proxy=os.getenvb(b\"http_proxy\"),\n            https_proxy=os.getenvb(b\"HTTPS_PROXY\"),\n        )\n\n    @cache_in_self\n    def get_room_creation_handler(self) -> RoomCreationHandler:\n        return RoomCreationHandler(self)\n\n    @cache_in_self\n    def get_room_shutdown_handler(self) -> RoomShutdownHandler:\n        return RoomShutdownHandler(self)\n\n    @cache_in_self\n    def get_sendmail(self) -> sendmail:\n        return sendmail\n\n    @cache_in_self\n    def get_state_handler(self) -> StateHandler:\n        return StateHandler(self)\n\n    @cache_in_self\n    def get_state_resolution_handler(self) -> StateResolutionHandler:\n        return StateResolutionHandler(self)\n\n    @cache_in_self\n    def get_presence_handler(self) -> PresenceHandler:\n        return PresenceHandler(self)\n\n    @cache_in_self\n    def get_typing_handler(self):\n        if self.config.worker.writers.typing == self.get_instance_name():\n            return TypingWriterHandler(self)\n        else:\n            return FollowerTypingHandler(self)\n\n    @cache_in_self\n    def get_sso_handler(self) -> SsoHandler:\n        return SsoHandler(self)\n\n    @cache_in_self\n    def get_sync_handler(self) -> SyncHandler:\n        return SyncHandler(self)\n\n    @cache_in_self\n    def get_room_list_handler(self) -> RoomListHandler:\n        return RoomListHandler(self)\n\n    @cache_in_self\n    def get_auth_handler(self) -> AuthHandler:\n        return AuthHandler(self)\n\n    @cache_in_self\n    def get_macaroon_generator(self) -> MacaroonGenerator:\n        return MacaroonGenerator(self)\n\n    @cache_in_self\n    def get_device_handler(self):\n        if self.config.worker_app:\n            return DeviceWorkerHandler(self)\n        else:\n            return DeviceHandler(self)\n\n    @cache_in_self\n    def get_device_message_handler(self) -> DeviceMessageHandler:\n        return DeviceMessageHandler(self)\n\n    @cache_in_self\n    def get_directory_handler(self) -> DirectoryHandler:\n        return DirectoryHandler(self)\n\n    @cache_in_self\n    def get_e2e_keys_handler(self) -> E2eKeysHandler:\n        return E2eKeysHandler(self)\n\n    @cache_in_self\n    def get_e2e_room_keys_handler(self) -> E2eRoomKeysHandler:\n        return E2eRoomKeysHandler(self)\n\n    @cache_in_self\n    def get_acme_handler(self) -> AcmeHandler:\n        return AcmeHandler(self)\n\n    @cache_in_self\n    def get_admin_handler(self) -> AdminHandler:\n        return AdminHandler(self)\n\n    @cache_in_self\n    def get_application_service_api(self) -> ApplicationServiceApi:\n        return ApplicationServiceApi(self)\n\n    @cache_in_self\n    def get_application_service_scheduler(self) -> ApplicationServiceScheduler:\n        return ApplicationServiceScheduler(self)\n\n    @cache_in_self\n    def get_application_service_handler(self) -> ApplicationServicesHandler:\n        return ApplicationServicesHandler(self)\n\n    @cache_in_self\n    def get_event_handler(self) -> EventHandler:\n        return EventHandler(self)\n\n    @cache_in_self\n    def get_event_stream_handler(self) -> EventStreamHandler:\n        return EventStreamHandler(self)\n\n    @cache_in_self\n    def get_federation_handler(self) -> FederationHandler:\n        return FederationHandler(self)\n\n    @cache_in_self\n    def get_identity_handler(self) -> IdentityHandler:\n        return IdentityHandler(self)\n\n    @cache_in_self\n    def get_initial_sync_handler(self) -> InitialSyncHandler:\n        return InitialSyncHandler(self)\n\n    @cache_in_self\n    def get_profile_handler(self):\n        return ProfileHandler(self)\n\n    @cache_in_self\n    def get_event_creation_handler(self) -> EventCreationHandler:\n        return EventCreationHandler(self)\n\n    @cache_in_self\n    def get_deactivate_account_handler(self) -> DeactivateAccountHandler:\n        return DeactivateAccountHandler(self)\n\n    @cache_in_self\n    def get_search_handler(self) -> SearchHandler:\n        return SearchHandler(self)\n\n    @cache_in_self\n    def get_set_password_handler(self) -> SetPasswordHandler:\n        return SetPasswordHandler(self)\n\n    @cache_in_self\n    def get_event_sources(self) -> EventSources:\n        return EventSources(self)\n\n    @cache_in_self\n    def get_keyring(self) -> Keyring:\n        return Keyring(self)\n\n    @cache_in_self\n    def get_event_builder_factory(self) -> EventBuilderFactory:\n        return EventBuilderFactory(self)\n\n    @cache_in_self\n    def get_filtering(self) -> Filtering:\n        return Filtering(self)\n\n    @cache_in_self\n    def get_pusherpool(self) -> PusherPool:\n        return PusherPool(self)\n\n    @cache_in_self\n    def get_http_client(self) -> MatrixFederationHttpClient:\n        tls_client_options_factory = context_factory.FederationPolicyForHTTPS(\n            self.config\n        )\n        return MatrixFederationHttpClient(self, tls_client_options_factory)\n\n    @cache_in_self\n    def get_media_repository_resource(self) -> MediaRepositoryResource:\n        # build the media repo resource. This indirects through the HomeServer\n        # to ensure that we only have a single instance of\n        return MediaRepositoryResource(self)\n\n    @cache_in_self\n    def get_media_repository(self) -> MediaRepository:\n        return MediaRepository(self)\n\n    @cache_in_self\n    def get_federation_transport_client(self) -> TransportLayerClient:\n        return TransportLayerClient(self)\n\n    @cache_in_self\n    def get_federation_sender(self):\n        if self.should_send_federation():\n            return FederationSender(self)\n        elif not self.config.worker_app:\n            return FederationRemoteSendQueue(self)\n        else:\n            raise Exception(\"Workers cannot send federation traffic\")\n\n    @cache_in_self\n    def get_receipts_handler(self) -> ReceiptsHandler:\n        return ReceiptsHandler(self)\n\n    @cache_in_self\n    def get_read_marker_handler(self) -> ReadMarkerHandler:\n        return ReadMarkerHandler(self)\n\n    @cache_in_self\n    def get_tcp_replication(self) -> ReplicationCommandHandler:\n        return ReplicationCommandHandler(self)\n\n    @cache_in_self\n    def get_action_generator(self) -> ActionGenerator:\n        return ActionGenerator(self)\n\n    @cache_in_self\n    def get_user_directory_handler(self) -> UserDirectoryHandler:\n        return UserDirectoryHandler(self)\n\n    @cache_in_self\n    def get_groups_local_handler(self):\n        if self.config.worker_app:\n            return GroupsLocalWorkerHandler(self)\n        else:\n            return GroupsLocalHandler(self)\n\n    @cache_in_self\n    def get_groups_server_handler(self):\n        if self.config.worker_app:\n            return GroupsServerWorkerHandler(self)\n        else:\n            return GroupsServerHandler(self)\n\n    @cache_in_self\n    def get_groups_attestation_signing(self) -> GroupAttestationSigning:\n        return GroupAttestationSigning(self)\n\n    @cache_in_self\n    def get_groups_attestation_renewer(self) -> GroupAttestionRenewer:\n        return GroupAttestionRenewer(self)\n\n    @cache_in_self\n    def get_secrets(self) -> Secrets:\n        return Secrets()\n\n    @cache_in_self\n    def get_stats_handler(self) -> StatsHandler:\n        return StatsHandler(self)\n\n    @cache_in_self\n    def get_spam_checker(self):\n        return SpamChecker(self)\n\n    @cache_in_self\n    def get_third_party_event_rules(self) -> ThirdPartyEventRules:\n        return ThirdPartyEventRules(self)\n\n    @cache_in_self\n    def get_room_member_handler(self):\n        if self.config.worker_app:\n            return RoomMemberWorkerHandler(self)\n        return RoomMemberMasterHandler(self)\n\n    @cache_in_self\n    def get_federation_registry(self) -> FederationHandlerRegistry:\n        return FederationHandlerRegistry(self)\n\n    @cache_in_self\n    def get_server_notices_manager(self):\n        if self.config.worker_app:\n            raise Exception(\"Workers cannot send server notices\")\n        return ServerNoticesManager(self)\n\n    @cache_in_self\n    def get_server_notices_sender(self):\n        if self.config.worker_app:\n            return WorkerServerNoticesSender(self)\n        return ServerNoticesSender(self)\n\n    @cache_in_self\n    def get_message_handler(self) -> MessageHandler:\n        return MessageHandler(self)\n\n    @cache_in_self\n    def get_pagination_handler(self) -> PaginationHandler:\n        return PaginationHandler(self)\n\n    @cache_in_self\n    def get_room_context_handler(self) -> RoomContextHandler:\n        return RoomContextHandler(self)\n\n    @cache_in_self\n    def get_registration_handler(self) -> RegistrationHandler:\n        return RegistrationHandler(self)\n\n    @cache_in_self\n    def get_account_validity_handler(self) -> AccountValidityHandler:\n        return AccountValidityHandler(self)\n\n    @cache_in_self\n    def get_cas_handler(self) -> CasHandler:\n        return CasHandler(self)\n\n    @cache_in_self\n    def get_saml_handler(self) -> \"SamlHandler\":\n        from synapse.handlers.saml_handler import SamlHandler\n\n        return SamlHandler(self)\n\n    @cache_in_self\n    def get_oidc_handler(self) -> \"OidcHandler\":\n        from synapse.handlers.oidc_handler import OidcHandler\n\n        return OidcHandler(self)\n\n    @cache_in_self\n    def get_event_client_serializer(self) -> EventClientSerializer:\n        return EventClientSerializer(self)\n\n    @cache_in_self\n    def get_password_policy_handler(self) -> PasswordPolicyHandler:\n        return PasswordPolicyHandler(self)\n\n    @cache_in_self\n    def get_storage(self) -> Storage:\n        return Storage(self, self.get_datastores())\n\n    @cache_in_self\n    def get_replication_streamer(self) -> ReplicationStreamer:\n        return ReplicationStreamer(self)\n\n    @cache_in_self\n    def get_replication_data_handler(self) -> ReplicationDataHandler:\n        return ReplicationDataHandler(self)\n\n    @cache_in_self\n    def get_replication_streams(self) -> Dict[str, Stream]:\n        return {stream.NAME: stream(self) for stream in STREAMS_MAP.values()}\n\n    @cache_in_self\n    def get_federation_ratelimiter(self) -> FederationRateLimiter:\n        return FederationRateLimiter(self.get_clock(), config=self.config.rc_federation)\n\n    @cache_in_self\n    def get_module_api(self) -> ModuleApi:\n        return ModuleApi(self, self.get_auth_handler())\n\n    async def remove_pusher(self, app_id: str, push_key: str, user_id: str):\n        return await self.get_pusherpool().remove_pusher(app_id, push_key, user_id)\n\n    def should_send_federation(self) -> bool:\n        \"Should this server be sending federation traffic directly?\"\n        return self.config.send_federation and (\n            not self.config.worker_app\n            or self.config.worker_app == \"synapse.app.federation_sender\"\n        )\n", "patch": "@@ -350,16 +350,45 @@ def get_http_client_context_factory(self) -> IPolicyForHTTPS:\n \n     @cache_in_self\n     def get_simple_http_client(self) -> SimpleHttpClient:\n+        \"\"\"\n+        An HTTP client with no special configuration.\n+        \"\"\"\n         return SimpleHttpClient(self)\n \n     @cache_in_self\n     def get_proxied_http_client(self) -> SimpleHttpClient:\n+        \"\"\"\n+        An HTTP client that uses configured HTTP(S) proxies.\n+        \"\"\"\n+        return SimpleHttpClient(\n+            self,\n+            http_proxy=os.getenvb(b\"http_proxy\"),\n+            https_proxy=os.getenvb(b\"HTTPS_PROXY\"),\n+        )\n+\n+    @cache_in_self\n+    def get_proxied_blacklisted_http_client(self) -> SimpleHttpClient:\n+        \"\"\"\n+        An HTTP client that uses configured HTTP(S) proxies and blacklists IPs\n+        based on the IP range blacklist.\n+        \"\"\"\n         return SimpleHttpClient(\n             self,\n+            ip_blacklist=self.config.ip_range_blacklist,\n             http_proxy=os.getenvb(b\"http_proxy\"),\n             https_proxy=os.getenvb(b\"HTTPS_PROXY\"),\n         )\n \n+    @cache_in_self\n+    def get_federation_http_client(self) -> MatrixFederationHttpClient:\n+        \"\"\"\n+        An HTTP client for federation.\n+        \"\"\"\n+        tls_client_options_factory = context_factory.FederationPolicyForHTTPS(\n+            self.config\n+        )\n+        return MatrixFederationHttpClient(self, tls_client_options_factory)\n+\n     @cache_in_self\n     def get_room_creation_handler(self) -> RoomCreationHandler:\n         return RoomCreationHandler(self)\n@@ -514,13 +543,6 @@ def get_filtering(self) -> Filtering:\n     def get_pusherpool(self) -> PusherPool:\n         return PusherPool(self)\n \n-    @cache_in_self\n-    def get_http_client(self) -> MatrixFederationHttpClient:\n-        tls_client_options_factory = context_factory.FederationPolicyForHTTPS(\n-            self.config\n-        )\n-        return MatrixFederationHttpClient(self, tls_client_options_factory)\n-\n     @cache_in_self\n     def get_media_repository_resource(self) -> MediaRepositoryResource:\n         # build the media repo resource. This indirects through the HomeServer", "file_path": "files/2021_2/29", "file_language": "py", "file_name": "synapse/server.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fapi%2Ftest_filtering.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2015, 2016 OpenMarket Ltd\n# Copyright 2017 Vector Creations Ltd\n# Copyright 2018-2019 New Vector Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom mock import Mock\n\nimport jsonschema\n\nfrom twisted.internet import defer\n\nfrom synapse.api.constants import EventContentFields\nfrom synapse.api.errors import SynapseError\nfrom synapse.api.filtering import Filter\nfrom synapse.events import make_event_from_dict\n\nfrom tests import unittest\nfrom tests.utils import DeferredMockCallable, MockHttpResource, setup_test_homeserver\n\nuser_localpart = \"test_user\"\n\n\ndef MockEvent(**kwargs):\n    if \"event_id\" not in kwargs:\n        kwargs[\"event_id\"] = \"fake_event_id\"\n    if \"type\" not in kwargs:\n        kwargs[\"type\"] = \"fake_type\"\n    return make_event_from_dict(kwargs)\n\n\nclass FilteringTestCase(unittest.TestCase):\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_federation_resource = MockHttpResource()\n\n        self.mock_http_client = Mock(spec=[])\n        self.mock_http_client.put_json = DeferredMockCallable()\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup,\n            federation_http_client=self.mock_http_client,\n            keyring=Mock(),\n        )\n\n        self.filtering = hs.get_filtering()\n\n        self.datastore = hs.get_datastore()\n\n    def test_errors_on_invalid_filters(self):\n        invalid_filters = [\n            {\"boom\": {}},\n            {\"account_data\": \"Hello World\"},\n            {\"event_fields\": [r\"\\\\foo\"]},\n            {\"room\": {\"timeline\": {\"limit\": 0}, \"state\": {\"not_bars\": [\"*\"]}}},\n            {\"event_format\": \"other\"},\n            {\"room\": {\"not_rooms\": [\"#foo:pik-test\"]}},\n            {\"presence\": {\"senders\": [\"@bar;pik.test.com\"]}},\n        ]\n        for filter in invalid_filters:\n            with self.assertRaises(SynapseError) as check_filter_error:\n                self.filtering.check_valid_filter(filter)\n                self.assertIsInstance(check_filter_error.exception, SynapseError)\n\n    def test_valid_filters(self):\n        valid_filters = [\n            {\n                \"room\": {\n                    \"timeline\": {\"limit\": 20},\n                    \"state\": {\"not_types\": [\"m.room.member\"]},\n                    \"ephemeral\": {\"limit\": 0, \"not_types\": [\"*\"]},\n                    \"include_leave\": False,\n                    \"rooms\": [\"!dee:pik-test\"],\n                    \"not_rooms\": [\"!gee:pik-test\"],\n                    \"account_data\": {\"limit\": 0, \"types\": [\"*\"]},\n                }\n            },\n            {\n                \"room\": {\n                    \"state\": {\n                        \"types\": [\"m.room.*\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                    },\n                    \"timeline\": {\n                        \"limit\": 10,\n                        \"types\": [\"m.room.message\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                        \"not_senders\": [\"@spam:example.com\"],\n                        \"org.matrix.labels\": [\"#fun\"],\n                        \"org.matrix.not_labels\": [\"#work\"],\n                    },\n                    \"ephemeral\": {\n                        \"types\": [\"m.receipt\", \"m.typing\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                        \"not_senders\": [\"@spam:example.com\"],\n                    },\n                },\n                \"presence\": {\n                    \"types\": [\"m.presence\"],\n                    \"not_senders\": [\"@alice:example.com\"],\n                },\n                \"event_format\": \"client\",\n                \"event_fields\": [\"type\", \"content\", \"sender\"],\n            },\n            # a single backslash should be permitted (though it is debatable whether\n            # it should be permitted before anything other than `.`, and what that\n            # actually means)\n            #\n            # (note that event_fields is implemented in\n            # synapse.events.utils.serialize_event, and so whether this actually works\n            # is tested elsewhere. We just want to check that it is allowed through the\n            # filter validation)\n            {\"event_fields\": [r\"foo\\.bar\"]},\n        ]\n        for filter in valid_filters:\n            try:\n                self.filtering.check_valid_filter(filter)\n            except jsonschema.ValidationError as e:\n                self.fail(e)\n\n    def test_limits_are_applied(self):\n        # TODO\n        pass\n\n    def test_definition_types_works_with_literals(self):\n        definition = {\"types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_types_works_with_wildcards(self):\n        definition = {\"types\": [\"m.*\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_types_works_with_unknowns(self):\n        definition = {\"types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"now.for.something.completely.different\",\n            room_id=\"!foo:bar\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_literals(self):\n        definition = {\"not_types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_wildcards(self):\n        definition = {\"not_types\": [\"m.room.message\", \"org.matrix.*\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"org.matrix.custom.event\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_unknowns(self):\n        definition = {\"not_types\": [\"m.*\", \"org.*\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\")\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_types_takes_priority_over_types(self):\n        definition = {\n            \"not_types\": [\"m.*\", \"org.*\"],\n            \"types\": [\"m.room.message\", \"m.room.topic\"],\n        }\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.topic\", room_id=\"!foo:bar\")\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_senders_works_with_literals(self):\n        definition = {\"senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@flibble:wibble\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_senders_works_with_unknowns(self):\n        definition = {\"senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@challenger:appears\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_senders_works_with_literals(self):\n        definition = {\"not_senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@flibble:wibble\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_senders_works_with_unknowns(self):\n        definition = {\"not_senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@challenger:appears\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_senders_takes_priority_over_senders(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\", \"@misspiggy:muppets\"],\n        }\n        event = MockEvent(\n            sender=\"@misspiggy:muppets\", type=\"m.room.topic\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_rooms_works_with_literals(self):\n        definition = {\"rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!secretbase:unknown\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_rooms_works_with_unknowns(self):\n        definition = {\"rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_rooms_works_with_literals(self):\n        definition = {\"not_rooms\": [\"!anothersecretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_rooms_works_with_unknowns(self):\n        definition = {\"not_rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_rooms_takes_priority_over_rooms(self):\n        definition = {\n            \"not_rooms\": [\"!secretbase:unknown\"],\n            \"rooms\": [\"!secretbase:unknown\"],\n        }\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!secretbase:unknown\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"m.room.message\",  # yup\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_sender(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@misspiggy:muppets\",  # nope\n            type=\"m.room.message\",  # yup\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_room(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"m.room.message\",  # yup\n            room_id=\"!piggyshouse:muppets\",  # nope\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_type(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"muppets.misspiggy.kisses\",  # nope\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_filter_labels(self):\n        definition = {\"org.matrix.labels\": [\"#fun\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#fun\"]},\n        )\n\n        self.assertTrue(Filter(definition).check(event))\n\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#notfun\"]},\n        )\n\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_filter_not_labels(self):\n        definition = {\"org.matrix.not_labels\": [\"#fun\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#fun\"]},\n        )\n\n        self.assertFalse(Filter(definition).check(event))\n\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#notfun\"]},\n        )\n\n        self.assertTrue(Filter(definition).check(event))\n\n    @defer.inlineCallbacks\n    def test_filter_presence_match(self):\n        user_filter_json = {\"presence\": {\"types\": [\"m.*\"]}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.profile\")\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_presence(events=events)\n        self.assertEquals(events, results)\n\n    @defer.inlineCallbacks\n    def test_filter_presence_no_match(self):\n        user_filter_json = {\"presence\": {\"types\": [\"m.*\"]}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart + \"2\", user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(\n            event_id=\"$asdasd:localhost\",\n            sender=\"@foo:bar\",\n            type=\"custom.avatar.3d.crazy\",\n        )\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart + \"2\", filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_presence(events=events)\n        self.assertEquals([], results)\n\n    @defer.inlineCallbacks\n    def test_filter_room_state_match(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.topic\", room_id=\"!foo:bar\")\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_room_state(events=events)\n        self.assertEquals(events, results)\n\n    @defer.inlineCallbacks\n    def test_filter_room_state_no_match(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"org.matrix.custom.event\", room_id=\"!foo:bar\"\n        )\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_room_state(events)\n        self.assertEquals([], results)\n\n    def test_filter_rooms(self):\n        definition = {\n            \"rooms\": [\"!allowed:example.com\", \"!excluded:example.com\"],\n            \"not_rooms\": [\"!excluded:example.com\"],\n        }\n\n        room_ids = [\n            \"!allowed:example.com\",  # Allowed because in rooms and not in not_rooms.\n            \"!excluded:example.com\",  # Disallowed because in not_rooms.\n            \"!not_included:example.com\",  # Disallowed because not in rooms.\n        ]\n\n        filtered_room_ids = list(Filter(definition).filter_rooms(room_ids))\n\n        self.assertEquals(filtered_room_ids, [\"!allowed:example.com\"])\n\n    @defer.inlineCallbacks\n    def test_add_filter(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.filtering.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n\n        self.assertEquals(filter_id, 0)\n        self.assertEquals(\n            user_filter_json,\n            (\n                yield defer.ensureDeferred(\n                    self.datastore.get_user_filter(\n                        user_localpart=user_localpart, filter_id=0\n                    )\n                )\n            ),\n        )\n\n    @defer.inlineCallbacks\n    def test_get_filter(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n\n        filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        self.assertEquals(filter.get_filter_json(), user_filter_json)\n\n        self.assertRegexpMatches(repr(filter), r\"<FilterCollection \\{.*\\}>\")\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2015, 2016 OpenMarket Ltd\n# Copyright 2017 Vector Creations Ltd\n# Copyright 2018-2019 New Vector Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom mock import Mock\n\nimport jsonschema\n\nfrom twisted.internet import defer\n\nfrom synapse.api.constants import EventContentFields\nfrom synapse.api.errors import SynapseError\nfrom synapse.api.filtering import Filter\nfrom synapse.events import make_event_from_dict\n\nfrom tests import unittest\nfrom tests.utils import DeferredMockCallable, MockHttpResource, setup_test_homeserver\n\nuser_localpart = \"test_user\"\n\n\ndef MockEvent(**kwargs):\n    if \"event_id\" not in kwargs:\n        kwargs[\"event_id\"] = \"fake_event_id\"\n    if \"type\" not in kwargs:\n        kwargs[\"type\"] = \"fake_type\"\n    return make_event_from_dict(kwargs)\n\n\nclass FilteringTestCase(unittest.TestCase):\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_federation_resource = MockHttpResource()\n\n        self.mock_http_client = Mock(spec=[])\n        self.mock_http_client.put_json = DeferredMockCallable()\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup, http_client=self.mock_http_client, keyring=Mock(),\n        )\n\n        self.filtering = hs.get_filtering()\n\n        self.datastore = hs.get_datastore()\n\n    def test_errors_on_invalid_filters(self):\n        invalid_filters = [\n            {\"boom\": {}},\n            {\"account_data\": \"Hello World\"},\n            {\"event_fields\": [r\"\\\\foo\"]},\n            {\"room\": {\"timeline\": {\"limit\": 0}, \"state\": {\"not_bars\": [\"*\"]}}},\n            {\"event_format\": \"other\"},\n            {\"room\": {\"not_rooms\": [\"#foo:pik-test\"]}},\n            {\"presence\": {\"senders\": [\"@bar;pik.test.com\"]}},\n        ]\n        for filter in invalid_filters:\n            with self.assertRaises(SynapseError) as check_filter_error:\n                self.filtering.check_valid_filter(filter)\n                self.assertIsInstance(check_filter_error.exception, SynapseError)\n\n    def test_valid_filters(self):\n        valid_filters = [\n            {\n                \"room\": {\n                    \"timeline\": {\"limit\": 20},\n                    \"state\": {\"not_types\": [\"m.room.member\"]},\n                    \"ephemeral\": {\"limit\": 0, \"not_types\": [\"*\"]},\n                    \"include_leave\": False,\n                    \"rooms\": [\"!dee:pik-test\"],\n                    \"not_rooms\": [\"!gee:pik-test\"],\n                    \"account_data\": {\"limit\": 0, \"types\": [\"*\"]},\n                }\n            },\n            {\n                \"room\": {\n                    \"state\": {\n                        \"types\": [\"m.room.*\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                    },\n                    \"timeline\": {\n                        \"limit\": 10,\n                        \"types\": [\"m.room.message\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                        \"not_senders\": [\"@spam:example.com\"],\n                        \"org.matrix.labels\": [\"#fun\"],\n                        \"org.matrix.not_labels\": [\"#work\"],\n                    },\n                    \"ephemeral\": {\n                        \"types\": [\"m.receipt\", \"m.typing\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                        \"not_senders\": [\"@spam:example.com\"],\n                    },\n                },\n                \"presence\": {\n                    \"types\": [\"m.presence\"],\n                    \"not_senders\": [\"@alice:example.com\"],\n                },\n                \"event_format\": \"client\",\n                \"event_fields\": [\"type\", \"content\", \"sender\"],\n            },\n            # a single backslash should be permitted (though it is debatable whether\n            # it should be permitted before anything other than `.`, and what that\n            # actually means)\n            #\n            # (note that event_fields is implemented in\n            # synapse.events.utils.serialize_event, and so whether this actually works\n            # is tested elsewhere. We just want to check that it is allowed through the\n            # filter validation)\n            {\"event_fields\": [r\"foo\\.bar\"]},\n        ]\n        for filter in valid_filters:\n            try:\n                self.filtering.check_valid_filter(filter)\n            except jsonschema.ValidationError as e:\n                self.fail(e)\n\n    def test_limits_are_applied(self):\n        # TODO\n        pass\n\n    def test_definition_types_works_with_literals(self):\n        definition = {\"types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_types_works_with_wildcards(self):\n        definition = {\"types\": [\"m.*\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_types_works_with_unknowns(self):\n        definition = {\"types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"now.for.something.completely.different\",\n            room_id=\"!foo:bar\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_literals(self):\n        definition = {\"not_types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_wildcards(self):\n        definition = {\"not_types\": [\"m.room.message\", \"org.matrix.*\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"org.matrix.custom.event\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_unknowns(self):\n        definition = {\"not_types\": [\"m.*\", \"org.*\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\")\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_types_takes_priority_over_types(self):\n        definition = {\n            \"not_types\": [\"m.*\", \"org.*\"],\n            \"types\": [\"m.room.message\", \"m.room.topic\"],\n        }\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.topic\", room_id=\"!foo:bar\")\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_senders_works_with_literals(self):\n        definition = {\"senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@flibble:wibble\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_senders_works_with_unknowns(self):\n        definition = {\"senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@challenger:appears\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_senders_works_with_literals(self):\n        definition = {\"not_senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@flibble:wibble\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_senders_works_with_unknowns(self):\n        definition = {\"not_senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@challenger:appears\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_senders_takes_priority_over_senders(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\", \"@misspiggy:muppets\"],\n        }\n        event = MockEvent(\n            sender=\"@misspiggy:muppets\", type=\"m.room.topic\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_rooms_works_with_literals(self):\n        definition = {\"rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!secretbase:unknown\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_rooms_works_with_unknowns(self):\n        definition = {\"rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_rooms_works_with_literals(self):\n        definition = {\"not_rooms\": [\"!anothersecretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_rooms_works_with_unknowns(self):\n        definition = {\"not_rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_rooms_takes_priority_over_rooms(self):\n        definition = {\n            \"not_rooms\": [\"!secretbase:unknown\"],\n            \"rooms\": [\"!secretbase:unknown\"],\n        }\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!secretbase:unknown\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"m.room.message\",  # yup\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_sender(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@misspiggy:muppets\",  # nope\n            type=\"m.room.message\",  # yup\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_room(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"m.room.message\",  # yup\n            room_id=\"!piggyshouse:muppets\",  # nope\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_type(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"muppets.misspiggy.kisses\",  # nope\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_filter_labels(self):\n        definition = {\"org.matrix.labels\": [\"#fun\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#fun\"]},\n        )\n\n        self.assertTrue(Filter(definition).check(event))\n\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#notfun\"]},\n        )\n\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_filter_not_labels(self):\n        definition = {\"org.matrix.not_labels\": [\"#fun\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#fun\"]},\n        )\n\n        self.assertFalse(Filter(definition).check(event))\n\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#notfun\"]},\n        )\n\n        self.assertTrue(Filter(definition).check(event))\n\n    @defer.inlineCallbacks\n    def test_filter_presence_match(self):\n        user_filter_json = {\"presence\": {\"types\": [\"m.*\"]}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.profile\")\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_presence(events=events)\n        self.assertEquals(events, results)\n\n    @defer.inlineCallbacks\n    def test_filter_presence_no_match(self):\n        user_filter_json = {\"presence\": {\"types\": [\"m.*\"]}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart + \"2\", user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(\n            event_id=\"$asdasd:localhost\",\n            sender=\"@foo:bar\",\n            type=\"custom.avatar.3d.crazy\",\n        )\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart + \"2\", filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_presence(events=events)\n        self.assertEquals([], results)\n\n    @defer.inlineCallbacks\n    def test_filter_room_state_match(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.topic\", room_id=\"!foo:bar\")\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_room_state(events=events)\n        self.assertEquals(events, results)\n\n    @defer.inlineCallbacks\n    def test_filter_room_state_no_match(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"org.matrix.custom.event\", room_id=\"!foo:bar\"\n        )\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_room_state(events)\n        self.assertEquals([], results)\n\n    def test_filter_rooms(self):\n        definition = {\n            \"rooms\": [\"!allowed:example.com\", \"!excluded:example.com\"],\n            \"not_rooms\": [\"!excluded:example.com\"],\n        }\n\n        room_ids = [\n            \"!allowed:example.com\",  # Allowed because in rooms and not in not_rooms.\n            \"!excluded:example.com\",  # Disallowed because in not_rooms.\n            \"!not_included:example.com\",  # Disallowed because not in rooms.\n        ]\n\n        filtered_room_ids = list(Filter(definition).filter_rooms(room_ids))\n\n        self.assertEquals(filtered_room_ids, [\"!allowed:example.com\"])\n\n    @defer.inlineCallbacks\n    def test_add_filter(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.filtering.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n\n        self.assertEquals(filter_id, 0)\n        self.assertEquals(\n            user_filter_json,\n            (\n                yield defer.ensureDeferred(\n                    self.datastore.get_user_filter(\n                        user_localpart=user_localpart, filter_id=0\n                    )\n                )\n            ),\n        )\n\n    @defer.inlineCallbacks\n    def test_get_filter(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n\n        filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        self.assertEquals(filter.get_filter_json(), user_filter_json)\n\n        self.assertRegexpMatches(repr(filter), r\"<FilterCollection \\{.*\\}>\")\n", "patch": "@@ -50,7 +50,9 @@ def setUp(self):\n         self.mock_http_client.put_json = DeferredMockCallable()\n \n         hs = yield setup_test_homeserver(\n-            self.addCleanup, http_client=self.mock_http_client, keyring=Mock(),\n+            self.addCleanup,\n+            federation_http_client=self.mock_http_client,\n+            keyring=Mock(),\n         )\n \n         self.filtering = hs.get_filtering()", "file_path": "files/2021_2/30", "file_language": "py", "file_name": "tests/api/test_filtering.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "def MockEvent(**kwargs):\n    if \"event_id\" not in kwargs:\n        kwargs[\"event_id\"] = \"fake_event_id\"\n    if \"type\" not in kwargs:\n        kwargs[\"type\"] = \"fake_type\"\n    return make_event_from_dict(kwargs)", "target": 0}, {"function": "class FilteringTestCase(unittest.TestCase):\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_federation_resource = MockHttpResource()\n\n        self.mock_http_client = Mock(spec=[])\n        self.mock_http_client.put_json = DeferredMockCallable()\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup, http_client=self.mock_http_client, keyring=Mock(),\n        )\n\n        self.filtering = hs.get_filtering()\n\n        self.datastore = hs.get_datastore()\n\n    def test_errors_on_invalid_filters(self):\n        invalid_filters = [\n            {\"boom\": {}},\n            {\"account_data\": \"Hello World\"},\n            {\"event_fields\": [r\"\\\\foo\"]},\n            {\"room\": {\"timeline\": {\"limit\": 0}, \"state\": {\"not_bars\": [\"*\"]}}},\n            {\"event_format\": \"other\"},\n            {\"room\": {\"not_rooms\": [\"#foo:pik-test\"]}},\n            {\"presence\": {\"senders\": [\"@bar;pik.test.com\"]}},\n        ]\n        for filter in invalid_filters:\n            with self.assertRaises(SynapseError) as check_filter_error:\n                self.filtering.check_valid_filter(filter)\n                self.assertIsInstance(check_filter_error.exception, SynapseError)\n\n    def test_valid_filters(self):\n        valid_filters = [\n            {\n                \"room\": {\n                    \"timeline\": {\"limit\": 20},\n                    \"state\": {\"not_types\": [\"m.room.member\"]},\n                    \"ephemeral\": {\"limit\": 0, \"not_types\": [\"*\"]},\n                    \"include_leave\": False,\n                    \"rooms\": [\"!dee:pik-test\"],\n                    \"not_rooms\": [\"!gee:pik-test\"],\n                    \"account_data\": {\"limit\": 0, \"types\": [\"*\"]},\n                }\n            },\n            {\n                \"room\": {\n                    \"state\": {\n                        \"types\": [\"m.room.*\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                    },\n                    \"timeline\": {\n                        \"limit\": 10,\n                        \"types\": [\"m.room.message\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                        \"not_senders\": [\"@spam:example.com\"],\n                        \"org.matrix.labels\": [\"#fun\"],\n                        \"org.matrix.not_labels\": [\"#work\"],\n                    },\n                    \"ephemeral\": {\n                        \"types\": [\"m.receipt\", \"m.typing\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                        \"not_senders\": [\"@spam:example.com\"],\n                    },\n                },\n                \"presence\": {\n                    \"types\": [\"m.presence\"],\n                    \"not_senders\": [\"@alice:example.com\"],\n                },\n                \"event_format\": \"client\",\n                \"event_fields\": [\"type\", \"content\", \"sender\"],\n            },\n            # a single backslash should be permitted (though it is debatable whether\n            # it should be permitted before anything other than `.`, and what that\n            # actually means)\n            #\n            # (note that event_fields is implemented in\n            # synapse.events.utils.serialize_event, and so whether this actually works\n            # is tested elsewhere. We just want to check that it is allowed through the\n            # filter validation)\n            {\"event_fields\": [r\"foo\\.bar\"]},\n        ]\n        for filter in valid_filters:\n            try:\n                self.filtering.check_valid_filter(filter)\n            except jsonschema.ValidationError as e:\n                self.fail(e)\n\n    def test_limits_are_applied(self):\n        # TODO\n        pass\n\n    def test_definition_types_works_with_literals(self):\n        definition = {\"types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_types_works_with_wildcards(self):\n        definition = {\"types\": [\"m.*\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_types_works_with_unknowns(self):\n        definition = {\"types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"now.for.something.completely.different\",\n            room_id=\"!foo:bar\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_literals(self):\n        definition = {\"not_types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_wildcards(self):\n        definition = {\"not_types\": [\"m.room.message\", \"org.matrix.*\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"org.matrix.custom.event\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_unknowns(self):\n        definition = {\"not_types\": [\"m.*\", \"org.*\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\")\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_types_takes_priority_over_types(self):\n        definition = {\n            \"not_types\": [\"m.*\", \"org.*\"],\n            \"types\": [\"m.room.message\", \"m.room.topic\"],\n        }\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.topic\", room_id=\"!foo:bar\")\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_senders_works_with_literals(self):\n        definition = {\"senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@flibble:wibble\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_senders_works_with_unknowns(self):\n        definition = {\"senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@challenger:appears\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_senders_works_with_literals(self):\n        definition = {\"not_senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@flibble:wibble\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_senders_works_with_unknowns(self):\n        definition = {\"not_senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@challenger:appears\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_senders_takes_priority_over_senders(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\", \"@misspiggy:muppets\"],\n        }\n        event = MockEvent(\n            sender=\"@misspiggy:muppets\", type=\"m.room.topic\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_rooms_works_with_literals(self):\n        definition = {\"rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!secretbase:unknown\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_rooms_works_with_unknowns(self):\n        definition = {\"rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_rooms_works_with_literals(self):\n        definition = {\"not_rooms\": [\"!anothersecretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_rooms_works_with_unknowns(self):\n        definition = {\"not_rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_rooms_takes_priority_over_rooms(self):\n        definition = {\n            \"not_rooms\": [\"!secretbase:unknown\"],\n            \"rooms\": [\"!secretbase:unknown\"],\n        }\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!secretbase:unknown\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"m.room.message\",  # yup\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_sender(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@misspiggy:muppets\",  # nope\n            type=\"m.room.message\",  # yup\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_room(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"m.room.message\",  # yup\n            room_id=\"!piggyshouse:muppets\",  # nope\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_type(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"muppets.misspiggy.kisses\",  # nope\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_filter_labels(self):\n        definition = {\"org.matrix.labels\": [\"#fun\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#fun\"]},\n        )\n\n        self.assertTrue(Filter(definition).check(event))\n\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#notfun\"]},\n        )\n\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_filter_not_labels(self):\n        definition = {\"org.matrix.not_labels\": [\"#fun\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#fun\"]},\n        )\n\n        self.assertFalse(Filter(definition).check(event))\n\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#notfun\"]},\n        )\n\n        self.assertTrue(Filter(definition).check(event))\n\n    @defer.inlineCallbacks\n    def test_filter_presence_match(self):\n        user_filter_json = {\"presence\": {\"types\": [\"m.*\"]}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.profile\")\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_presence(events=events)\n        self.assertEquals(events, results)\n\n    @defer.inlineCallbacks\n    def test_filter_presence_no_match(self):\n        user_filter_json = {\"presence\": {\"types\": [\"m.*\"]}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart + \"2\", user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(\n            event_id=\"$asdasd:localhost\",\n            sender=\"@foo:bar\",\n            type=\"custom.avatar.3d.crazy\",\n        )\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart + \"2\", filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_presence(events=events)\n        self.assertEquals([], results)\n\n    @defer.inlineCallbacks\n    def test_filter_room_state_match(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.topic\", room_id=\"!foo:bar\")\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_room_state(events=events)\n        self.assertEquals(events, results)\n\n    @defer.inlineCallbacks\n    def test_filter_room_state_no_match(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"org.matrix.custom.event\", room_id=\"!foo:bar\"\n        )\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_room_state(events)\n        self.assertEquals([], results)\n\n    def test_filter_rooms(self):\n        definition = {\n            \"rooms\": [\"!allowed:example.com\", \"!excluded:example.com\"],\n            \"not_rooms\": [\"!excluded:example.com\"],\n        }\n\n        room_ids = [\n            \"!allowed:example.com\",  # Allowed because in rooms and not in not_rooms.\n            \"!excluded:example.com\",  # Disallowed because in not_rooms.\n            \"!not_included:example.com\",  # Disallowed because not in rooms.\n        ]\n\n        filtered_room_ids = list(Filter(definition).filter_rooms(room_ids))\n\n        self.assertEquals(filtered_room_ids, [\"!allowed:example.com\"])\n\n    @defer.inlineCallbacks\n    def test_add_filter(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.filtering.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n\n        self.assertEquals(filter_id, 0)\n        self.assertEquals(\n            user_filter_json,\n            (\n                yield defer.ensureDeferred(\n                    self.datastore.get_user_filter(\n                        user_localpart=user_localpart, filter_id=0\n                    )\n                )\n            ),\n        )\n\n    @defer.inlineCallbacks\n    def test_get_filter(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n\n        filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        self.assertEquals(filter.get_filter_json(), user_filter_json)\n\n        self.assertRegexpMatches(repr(filter), r\"<FilterCollection \\{.*\\}>\")", "target": 0}], "function_after": [{"function": "def MockEvent(**kwargs):\n    if \"event_id\" not in kwargs:\n        kwargs[\"event_id\"] = \"fake_event_id\"\n    if \"type\" not in kwargs:\n        kwargs[\"type\"] = \"fake_type\"\n    return make_event_from_dict(kwargs)", "target": 0}, {"function": "class FilteringTestCase(unittest.TestCase):\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_federation_resource = MockHttpResource()\n\n        self.mock_http_client = Mock(spec=[])\n        self.mock_http_client.put_json = DeferredMockCallable()\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup,\n            federation_http_client=self.mock_http_client,\n            keyring=Mock(),\n        )\n\n        self.filtering = hs.get_filtering()\n\n        self.datastore = hs.get_datastore()\n\n    def test_errors_on_invalid_filters(self):\n        invalid_filters = [\n            {\"boom\": {}},\n            {\"account_data\": \"Hello World\"},\n            {\"event_fields\": [r\"\\\\foo\"]},\n            {\"room\": {\"timeline\": {\"limit\": 0}, \"state\": {\"not_bars\": [\"*\"]}}},\n            {\"event_format\": \"other\"},\n            {\"room\": {\"not_rooms\": [\"#foo:pik-test\"]}},\n            {\"presence\": {\"senders\": [\"@bar;pik.test.com\"]}},\n        ]\n        for filter in invalid_filters:\n            with self.assertRaises(SynapseError) as check_filter_error:\n                self.filtering.check_valid_filter(filter)\n                self.assertIsInstance(check_filter_error.exception, SynapseError)\n\n    def test_valid_filters(self):\n        valid_filters = [\n            {\n                \"room\": {\n                    \"timeline\": {\"limit\": 20},\n                    \"state\": {\"not_types\": [\"m.room.member\"]},\n                    \"ephemeral\": {\"limit\": 0, \"not_types\": [\"*\"]},\n                    \"include_leave\": False,\n                    \"rooms\": [\"!dee:pik-test\"],\n                    \"not_rooms\": [\"!gee:pik-test\"],\n                    \"account_data\": {\"limit\": 0, \"types\": [\"*\"]},\n                }\n            },\n            {\n                \"room\": {\n                    \"state\": {\n                        \"types\": [\"m.room.*\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                    },\n                    \"timeline\": {\n                        \"limit\": 10,\n                        \"types\": [\"m.room.message\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                        \"not_senders\": [\"@spam:example.com\"],\n                        \"org.matrix.labels\": [\"#fun\"],\n                        \"org.matrix.not_labels\": [\"#work\"],\n                    },\n                    \"ephemeral\": {\n                        \"types\": [\"m.receipt\", \"m.typing\"],\n                        \"not_rooms\": [\"!726s6s6q:example.com\"],\n                        \"not_senders\": [\"@spam:example.com\"],\n                    },\n                },\n                \"presence\": {\n                    \"types\": [\"m.presence\"],\n                    \"not_senders\": [\"@alice:example.com\"],\n                },\n                \"event_format\": \"client\",\n                \"event_fields\": [\"type\", \"content\", \"sender\"],\n            },\n            # a single backslash should be permitted (though it is debatable whether\n            # it should be permitted before anything other than `.`, and what that\n            # actually means)\n            #\n            # (note that event_fields is implemented in\n            # synapse.events.utils.serialize_event, and so whether this actually works\n            # is tested elsewhere. We just want to check that it is allowed through the\n            # filter validation)\n            {\"event_fields\": [r\"foo\\.bar\"]},\n        ]\n        for filter in valid_filters:\n            try:\n                self.filtering.check_valid_filter(filter)\n            except jsonschema.ValidationError as e:\n                self.fail(e)\n\n    def test_limits_are_applied(self):\n        # TODO\n        pass\n\n    def test_definition_types_works_with_literals(self):\n        definition = {\"types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_types_works_with_wildcards(self):\n        definition = {\"types\": [\"m.*\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_types_works_with_unknowns(self):\n        definition = {\"types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"now.for.something.completely.different\",\n            room_id=\"!foo:bar\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_literals(self):\n        definition = {\"not_types\": [\"m.room.message\", \"org.matrix.foo.bar\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!foo:bar\")\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_wildcards(self):\n        definition = {\"not_types\": [\"m.room.message\", \"org.matrix.*\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"org.matrix.custom.event\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_types_works_with_unknowns(self):\n        definition = {\"not_types\": [\"m.*\", \"org.*\"]}\n        event = MockEvent(sender=\"@foo:bar\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\")\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_types_takes_priority_over_types(self):\n        definition = {\n            \"not_types\": [\"m.*\", \"org.*\"],\n            \"types\": [\"m.room.message\", \"m.room.topic\"],\n        }\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.topic\", room_id=\"!foo:bar\")\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_senders_works_with_literals(self):\n        definition = {\"senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@flibble:wibble\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_senders_works_with_unknowns(self):\n        definition = {\"senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@challenger:appears\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_senders_works_with_literals(self):\n        definition = {\"not_senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@flibble:wibble\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_senders_works_with_unknowns(self):\n        definition = {\"not_senders\": [\"@flibble:wibble\"]}\n        event = MockEvent(\n            sender=\"@challenger:appears\", type=\"com.nom.nom.nom\", room_id=\"!foo:bar\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_senders_takes_priority_over_senders(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\", \"@misspiggy:muppets\"],\n        }\n        event = MockEvent(\n            sender=\"@misspiggy:muppets\", type=\"m.room.topic\", room_id=\"!foo:bar\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_rooms_works_with_literals(self):\n        definition = {\"rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!secretbase:unknown\"\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_rooms_works_with_unknowns(self):\n        definition = {\"rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_rooms_works_with_literals(self):\n        definition = {\"not_rooms\": [\"!anothersecretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_not_rooms_works_with_unknowns(self):\n        definition = {\"not_rooms\": [\"!secretbase:unknown\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!anothersecretbase:unknown\",\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_not_rooms_takes_priority_over_rooms(self):\n        definition = {\n            \"not_rooms\": [\"!secretbase:unknown\"],\n            \"rooms\": [\"!secretbase:unknown\"],\n        }\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"m.room.message\", room_id=\"!secretbase:unknown\"\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"m.room.message\",  # yup\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertTrue(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_sender(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@misspiggy:muppets\",  # nope\n            type=\"m.room.message\",  # yup\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_room(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"m.room.message\",  # yup\n            room_id=\"!piggyshouse:muppets\",  # nope\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_definition_combined_event_bad_type(self):\n        definition = {\n            \"not_senders\": [\"@misspiggy:muppets\"],\n            \"senders\": [\"@kermit:muppets\"],\n            \"rooms\": [\"!stage:unknown\"],\n            \"not_rooms\": [\"!piggyshouse:muppets\"],\n            \"types\": [\"m.room.message\", \"muppets.kermit.*\"],\n            \"not_types\": [\"muppets.misspiggy.*\"],\n        }\n        event = MockEvent(\n            sender=\"@kermit:muppets\",  # yup\n            type=\"muppets.misspiggy.kisses\",  # nope\n            room_id=\"!stage:unknown\",  # yup\n        )\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_filter_labels(self):\n        definition = {\"org.matrix.labels\": [\"#fun\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#fun\"]},\n        )\n\n        self.assertTrue(Filter(definition).check(event))\n\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#notfun\"]},\n        )\n\n        self.assertFalse(Filter(definition).check(event))\n\n    def test_filter_not_labels(self):\n        definition = {\"org.matrix.not_labels\": [\"#fun\"]}\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#fun\"]},\n        )\n\n        self.assertFalse(Filter(definition).check(event))\n\n        event = MockEvent(\n            sender=\"@foo:bar\",\n            type=\"m.room.message\",\n            room_id=\"!secretbase:unknown\",\n            content={EventContentFields.LABELS: [\"#notfun\"]},\n        )\n\n        self.assertTrue(Filter(definition).check(event))\n\n    @defer.inlineCallbacks\n    def test_filter_presence_match(self):\n        user_filter_json = {\"presence\": {\"types\": [\"m.*\"]}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.profile\")\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_presence(events=events)\n        self.assertEquals(events, results)\n\n    @defer.inlineCallbacks\n    def test_filter_presence_no_match(self):\n        user_filter_json = {\"presence\": {\"types\": [\"m.*\"]}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart + \"2\", user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(\n            event_id=\"$asdasd:localhost\",\n            sender=\"@foo:bar\",\n            type=\"custom.avatar.3d.crazy\",\n        )\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart + \"2\", filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_presence(events=events)\n        self.assertEquals([], results)\n\n    @defer.inlineCallbacks\n    def test_filter_room_state_match(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(sender=\"@foo:bar\", type=\"m.room.topic\", room_id=\"!foo:bar\")\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_room_state(events=events)\n        self.assertEquals(events, results)\n\n    @defer.inlineCallbacks\n    def test_filter_room_state_no_match(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n        event = MockEvent(\n            sender=\"@foo:bar\", type=\"org.matrix.custom.event\", room_id=\"!foo:bar\"\n        )\n        events = [event]\n\n        user_filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        results = user_filter.filter_room_state(events)\n        self.assertEquals([], results)\n\n    def test_filter_rooms(self):\n        definition = {\n            \"rooms\": [\"!allowed:example.com\", \"!excluded:example.com\"],\n            \"not_rooms\": [\"!excluded:example.com\"],\n        }\n\n        room_ids = [\n            \"!allowed:example.com\",  # Allowed because in rooms and not in not_rooms.\n            \"!excluded:example.com\",  # Disallowed because in not_rooms.\n            \"!not_included:example.com\",  # Disallowed because not in rooms.\n        ]\n\n        filtered_room_ids = list(Filter(definition).filter_rooms(room_ids))\n\n        self.assertEquals(filtered_room_ids, [\"!allowed:example.com\"])\n\n    @defer.inlineCallbacks\n    def test_add_filter(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.filtering.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n\n        self.assertEquals(filter_id, 0)\n        self.assertEquals(\n            user_filter_json,\n            (\n                yield defer.ensureDeferred(\n                    self.datastore.get_user_filter(\n                        user_localpart=user_localpart, filter_id=0\n                    )\n                )\n            ),\n        )\n\n    @defer.inlineCallbacks\n    def test_get_filter(self):\n        user_filter_json = {\"room\": {\"state\": {\"types\": [\"m.*\"]}}}\n\n        filter_id = yield defer.ensureDeferred(\n            self.datastore.add_user_filter(\n                user_localpart=user_localpart, user_filter=user_filter_json\n            )\n        )\n\n        filter = yield defer.ensureDeferred(\n            self.filtering.get_user_filter(\n                user_localpart=user_localpart, filter_id=filter_id\n            )\n        )\n\n        self.assertEquals(filter.get_filter_json(), user_filter_json)\n\n        self.assertRegexpMatches(repr(filter), r\"<FilterCollection \\{.*\\}>\")", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fapp%2Ftest_frontend_proxy.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom synapse.app.generic_worker import GenericWorkerServer\n\nfrom tests.server import make_request\nfrom tests.unittest import HomeserverTestCase\n\n\nclass FrontendProxyTests(HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n\n        hs = self.setup_test_homeserver(\n            federation_http_client=None, homeserver_to_use=GenericWorkerServer\n        )\n\n        return hs\n\n    def default_config(self):\n        c = super().default_config()\n        c[\"worker_app\"] = \"synapse.app.frontend_proxy\"\n\n        c[\"worker_listeners\"] = [\n            {\n                \"type\": \"http\",\n                \"port\": 8080,\n                \"bind_addresses\": [\"0.0.0.0\"],\n                \"resources\": [{\"names\": [\"client\"]}],\n            }\n        ]\n\n        return c\n\n    def test_listen_http_with_presence_enabled(self):\n        \"\"\"\n        When presence is on, the stub servlet will not register.\n        \"\"\"\n        # Presence is on\n        self.hs.config.use_presence = True\n\n        # Listen with the config\n        self.hs._listen_http(self.hs.config.worker.worker_listeners[0])\n\n        # Grab the resource from the site that was told to listen\n        self.assertEqual(len(self.reactor.tcpServers), 1)\n        site = self.reactor.tcpServers[0][1]\n\n        _, channel = make_request(self.reactor, site, \"PUT\", \"presence/a/status\")\n\n        # 400 + unrecognised, because nothing is registered\n        self.assertEqual(channel.code, 400)\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_UNRECOGNIZED\")\n\n    def test_listen_http_with_presence_disabled(self):\n        \"\"\"\n        When presence is off, the stub servlet will register.\n        \"\"\"\n        # Presence is off\n        self.hs.config.use_presence = False\n\n        # Listen with the config\n        self.hs._listen_http(self.hs.config.worker.worker_listeners[0])\n\n        # Grab the resource from the site that was told to listen\n        self.assertEqual(len(self.reactor.tcpServers), 1)\n        site = self.reactor.tcpServers[0][1]\n\n        _, channel = make_request(self.reactor, site, \"PUT\", \"presence/a/status\")\n\n        # 401, because the stub servlet still checks authentication\n        self.assertEqual(channel.code, 401)\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_MISSING_TOKEN\")\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom synapse.app.generic_worker import GenericWorkerServer\n\nfrom tests.server import make_request\nfrom tests.unittest import HomeserverTestCase\n\n\nclass FrontendProxyTests(HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n\n        hs = self.setup_test_homeserver(\n            http_client=None, homeserver_to_use=GenericWorkerServer\n        )\n\n        return hs\n\n    def default_config(self):\n        c = super().default_config()\n        c[\"worker_app\"] = \"synapse.app.frontend_proxy\"\n\n        c[\"worker_listeners\"] = [\n            {\n                \"type\": \"http\",\n                \"port\": 8080,\n                \"bind_addresses\": [\"0.0.0.0\"],\n                \"resources\": [{\"names\": [\"client\"]}],\n            }\n        ]\n\n        return c\n\n    def test_listen_http_with_presence_enabled(self):\n        \"\"\"\n        When presence is on, the stub servlet will not register.\n        \"\"\"\n        # Presence is on\n        self.hs.config.use_presence = True\n\n        # Listen with the config\n        self.hs._listen_http(self.hs.config.worker.worker_listeners[0])\n\n        # Grab the resource from the site that was told to listen\n        self.assertEqual(len(self.reactor.tcpServers), 1)\n        site = self.reactor.tcpServers[0][1]\n\n        _, channel = make_request(self.reactor, site, \"PUT\", \"presence/a/status\")\n\n        # 400 + unrecognised, because nothing is registered\n        self.assertEqual(channel.code, 400)\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_UNRECOGNIZED\")\n\n    def test_listen_http_with_presence_disabled(self):\n        \"\"\"\n        When presence is off, the stub servlet will register.\n        \"\"\"\n        # Presence is off\n        self.hs.config.use_presence = False\n\n        # Listen with the config\n        self.hs._listen_http(self.hs.config.worker.worker_listeners[0])\n\n        # Grab the resource from the site that was told to listen\n        self.assertEqual(len(self.reactor.tcpServers), 1)\n        site = self.reactor.tcpServers[0][1]\n\n        _, channel = make_request(self.reactor, site, \"PUT\", \"presence/a/status\")\n\n        # 401, because the stub servlet still checks authentication\n        self.assertEqual(channel.code, 401)\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_MISSING_TOKEN\")\n", "patch": "@@ -23,7 +23,7 @@ class FrontendProxyTests(HomeserverTestCase):\n     def make_homeserver(self, reactor, clock):\n \n         hs = self.setup_test_homeserver(\n-            http_client=None, homeserver_to_use=GenericWorkerServer\n+            federation_http_client=None, homeserver_to_use=GenericWorkerServer\n         )\n \n         return hs", "file_path": "files/2021_2/31", "file_language": "py", "file_name": "tests/app/test_frontend_proxy.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class FrontendProxyTests(HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n\n        hs = self.setup_test_homeserver(\n            http_client=None, homeserver_to_use=GenericWorkerServer\n        )\n\n        return hs\n\n    def default_config(self):\n        c = super().default_config()\n        c[\"worker_app\"] = \"synapse.app.frontend_proxy\"\n\n        c[\"worker_listeners\"] = [\n            {\n                \"type\": \"http\",\n                \"port\": 8080,\n                \"bind_addresses\": [\"0.0.0.0\"],\n                \"resources\": [{\"names\": [\"client\"]}],\n            }\n        ]\n\n        return c\n\n    def test_listen_http_with_presence_enabled(self):\n        \"\"\"\n        When presence is on, the stub servlet will not register.\n        \"\"\"\n        # Presence is on\n        self.hs.config.use_presence = True\n\n        # Listen with the config\n        self.hs._listen_http(self.hs.config.worker.worker_listeners[0])\n\n        # Grab the resource from the site that was told to listen\n        self.assertEqual(len(self.reactor.tcpServers), 1)\n        site = self.reactor.tcpServers[0][1]\n\n        _, channel = make_request(self.reactor, site, \"PUT\", \"presence/a/status\")\n\n        # 400 + unrecognised, because nothing is registered\n        self.assertEqual(channel.code, 400)\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_UNRECOGNIZED\")\n\n    def test_listen_http_with_presence_disabled(self):\n        \"\"\"\n        When presence is off, the stub servlet will register.\n        \"\"\"\n        # Presence is off\n        self.hs.config.use_presence = False\n\n        # Listen with the config\n        self.hs._listen_http(self.hs.config.worker.worker_listeners[0])\n\n        # Grab the resource from the site that was told to listen\n        self.assertEqual(len(self.reactor.tcpServers), 1)\n        site = self.reactor.tcpServers[0][1]\n\n        _, channel = make_request(self.reactor, site, \"PUT\", \"presence/a/status\")\n\n        # 401, because the stub servlet still checks authentication\n        self.assertEqual(channel.code, 401)\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_MISSING_TOKEN\")", "target": 0}], "function_after": [{"function": "class FrontendProxyTests(HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n\n        hs = self.setup_test_homeserver(\n            federation_http_client=None, homeserver_to_use=GenericWorkerServer\n        )\n\n        return hs\n\n    def default_config(self):\n        c = super().default_config()\n        c[\"worker_app\"] = \"synapse.app.frontend_proxy\"\n\n        c[\"worker_listeners\"] = [\n            {\n                \"type\": \"http\",\n                \"port\": 8080,\n                \"bind_addresses\": [\"0.0.0.0\"],\n                \"resources\": [{\"names\": [\"client\"]}],\n            }\n        ]\n\n        return c\n\n    def test_listen_http_with_presence_enabled(self):\n        \"\"\"\n        When presence is on, the stub servlet will not register.\n        \"\"\"\n        # Presence is on\n        self.hs.config.use_presence = True\n\n        # Listen with the config\n        self.hs._listen_http(self.hs.config.worker.worker_listeners[0])\n\n        # Grab the resource from the site that was told to listen\n        self.assertEqual(len(self.reactor.tcpServers), 1)\n        site = self.reactor.tcpServers[0][1]\n\n        _, channel = make_request(self.reactor, site, \"PUT\", \"presence/a/status\")\n\n        # 400 + unrecognised, because nothing is registered\n        self.assertEqual(channel.code, 400)\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_UNRECOGNIZED\")\n\n    def test_listen_http_with_presence_disabled(self):\n        \"\"\"\n        When presence is off, the stub servlet will register.\n        \"\"\"\n        # Presence is off\n        self.hs.config.use_presence = False\n\n        # Listen with the config\n        self.hs._listen_http(self.hs.config.worker.worker_listeners[0])\n\n        # Grab the resource from the site that was told to listen\n        self.assertEqual(len(self.reactor.tcpServers), 1)\n        site = self.reactor.tcpServers[0][1]\n\n        _, channel = make_request(self.reactor, site, \"PUT\", \"presence/a/status\")\n\n        # 401, because the stub servlet still checks authentication\n        self.assertEqual(channel.code, 401)\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_MISSING_TOKEN\")", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fapp%2Ftest_openid_listener.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2019 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom mock import Mock, patch\n\nfrom parameterized import parameterized\n\nfrom synapse.app.generic_worker import GenericWorkerServer\nfrom synapse.app.homeserver import SynapseHomeServer\nfrom synapse.config.server import parse_listener_def\n\nfrom tests.server import make_request\nfrom tests.unittest import HomeserverTestCase\n\n\nclass FederationReaderOpenIDListenerTests(HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            federation_http_client=None, homeserver_to_use=GenericWorkerServer\n        )\n        return hs\n\n    def default_config(self):\n        conf = super().default_config()\n        # we're using FederationReaderServer, which uses a SlavedStore, so we\n        # have to tell the FederationHandler not to try to access stuff that is only\n        # in the primary store.\n        conf[\"worker_app\"] = \"yes\"\n\n        return conf\n\n    @parameterized.expand(\n        [\n            ([\"federation\"], \"auth_fail\"),\n            ([], \"no_resource\"),\n            ([\"openid\", \"federation\"], \"auth_fail\"),\n            ([\"openid\"], \"auth_fail\"),\n        ]\n    )\n    def test_openid_listener(self, names, expectation):\n        \"\"\"\n        Test different openid listener configurations.\n\n        401 is success here since it means we hit the handler and auth failed.\n        \"\"\"\n        config = {\n            \"port\": 8080,\n            \"type\": \"http\",\n            \"bind_addresses\": [\"0.0.0.0\"],\n            \"resources\": [{\"names\": names}],\n        }\n\n        # Listen with the config\n        self.hs._listen_http(parse_listener_def(config))\n\n        # Grab the resource from the site that was told to listen\n        site = self.reactor.tcpServers[0][1]\n        try:\n            site.resource.children[b\"_matrix\"].children[b\"federation\"]\n        except KeyError:\n            if expectation == \"no_resource\":\n                return\n            raise\n\n        _, channel = make_request(\n            self.reactor, site, \"GET\", \"/_matrix/federation/v1/openid/userinfo\"\n        )\n\n        self.assertEqual(channel.code, 401)\n\n\n@patch(\"synapse.app.homeserver.KeyApiV2Resource\", new=Mock())\nclass SynapseHomeserverOpenIDListenerTests(HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            federation_http_client=None, homeserver_to_use=SynapseHomeServer\n        )\n        return hs\n\n    @parameterized.expand(\n        [\n            ([\"federation\"], \"auth_fail\"),\n            ([], \"no_resource\"),\n            ([\"openid\", \"federation\"], \"auth_fail\"),\n            ([\"openid\"], \"auth_fail\"),\n        ]\n    )\n    def test_openid_listener(self, names, expectation):\n        \"\"\"\n        Test different openid listener configurations.\n\n        401 is success here since it means we hit the handler and auth failed.\n        \"\"\"\n        config = {\n            \"port\": 8080,\n            \"type\": \"http\",\n            \"bind_addresses\": [\"0.0.0.0\"],\n            \"resources\": [{\"names\": names}],\n        }\n\n        # Listen with the config\n        self.hs._listener_http(self.hs.get_config(), parse_listener_def(config))\n\n        # Grab the resource from the site that was told to listen\n        site = self.reactor.tcpServers[0][1]\n        try:\n            site.resource.children[b\"_matrix\"].children[b\"federation\"]\n        except KeyError:\n            if expectation == \"no_resource\":\n                return\n            raise\n\n        _, channel = make_request(\n            self.reactor, site, \"GET\", \"/_matrix/federation/v1/openid/userinfo\"\n        )\n\n        self.assertEqual(channel.code, 401)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2019 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom mock import Mock, patch\n\nfrom parameterized import parameterized\n\nfrom synapse.app.generic_worker import GenericWorkerServer\nfrom synapse.app.homeserver import SynapseHomeServer\nfrom synapse.config.server import parse_listener_def\n\nfrom tests.server import make_request\nfrom tests.unittest import HomeserverTestCase\n\n\nclass FederationReaderOpenIDListenerTests(HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            http_client=None, homeserver_to_use=GenericWorkerServer\n        )\n        return hs\n\n    def default_config(self):\n        conf = super().default_config()\n        # we're using FederationReaderServer, which uses a SlavedStore, so we\n        # have to tell the FederationHandler not to try to access stuff that is only\n        # in the primary store.\n        conf[\"worker_app\"] = \"yes\"\n\n        return conf\n\n    @parameterized.expand(\n        [\n            ([\"federation\"], \"auth_fail\"),\n            ([], \"no_resource\"),\n            ([\"openid\", \"federation\"], \"auth_fail\"),\n            ([\"openid\"], \"auth_fail\"),\n        ]\n    )\n    def test_openid_listener(self, names, expectation):\n        \"\"\"\n        Test different openid listener configurations.\n\n        401 is success here since it means we hit the handler and auth failed.\n        \"\"\"\n        config = {\n            \"port\": 8080,\n            \"type\": \"http\",\n            \"bind_addresses\": [\"0.0.0.0\"],\n            \"resources\": [{\"names\": names}],\n        }\n\n        # Listen with the config\n        self.hs._listen_http(parse_listener_def(config))\n\n        # Grab the resource from the site that was told to listen\n        site = self.reactor.tcpServers[0][1]\n        try:\n            site.resource.children[b\"_matrix\"].children[b\"federation\"]\n        except KeyError:\n            if expectation == \"no_resource\":\n                return\n            raise\n\n        _, channel = make_request(\n            self.reactor, site, \"GET\", \"/_matrix/federation/v1/openid/userinfo\"\n        )\n\n        self.assertEqual(channel.code, 401)\n\n\n@patch(\"synapse.app.homeserver.KeyApiV2Resource\", new=Mock())\nclass SynapseHomeserverOpenIDListenerTests(HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            http_client=None, homeserver_to_use=SynapseHomeServer\n        )\n        return hs\n\n    @parameterized.expand(\n        [\n            ([\"federation\"], \"auth_fail\"),\n            ([], \"no_resource\"),\n            ([\"openid\", \"federation\"], \"auth_fail\"),\n            ([\"openid\"], \"auth_fail\"),\n        ]\n    )\n    def test_openid_listener(self, names, expectation):\n        \"\"\"\n        Test different openid listener configurations.\n\n        401 is success here since it means we hit the handler and auth failed.\n        \"\"\"\n        config = {\n            \"port\": 8080,\n            \"type\": \"http\",\n            \"bind_addresses\": [\"0.0.0.0\"],\n            \"resources\": [{\"names\": names}],\n        }\n\n        # Listen with the config\n        self.hs._listener_http(self.hs.get_config(), parse_listener_def(config))\n\n        # Grab the resource from the site that was told to listen\n        site = self.reactor.tcpServers[0][1]\n        try:\n            site.resource.children[b\"_matrix\"].children[b\"federation\"]\n        except KeyError:\n            if expectation == \"no_resource\":\n                return\n            raise\n\n        _, channel = make_request(\n            self.reactor, site, \"GET\", \"/_matrix/federation/v1/openid/userinfo\"\n        )\n\n        self.assertEqual(channel.code, 401)\n", "patch": "@@ -27,7 +27,7 @@\n class FederationReaderOpenIDListenerTests(HomeserverTestCase):\n     def make_homeserver(self, reactor, clock):\n         hs = self.setup_test_homeserver(\n-            http_client=None, homeserver_to_use=GenericWorkerServer\n+            federation_http_client=None, homeserver_to_use=GenericWorkerServer\n         )\n         return hs\n \n@@ -84,7 +84,7 @@ def test_openid_listener(self, names, expectation):\n class SynapseHomeserverOpenIDListenerTests(HomeserverTestCase):\n     def make_homeserver(self, reactor, clock):\n         hs = self.setup_test_homeserver(\n-            http_client=None, homeserver_to_use=SynapseHomeServer\n+            federation_http_client=None, homeserver_to_use=SynapseHomeServer\n         )\n         return hs\n ", "file_path": "files/2021_2/32", "file_language": "py", "file_name": "tests/app/test_openid_listener.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class FederationReaderOpenIDListenerTests(HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            http_client=None, homeserver_to_use=GenericWorkerServer\n        )\n        return hs\n\n    def default_config(self):\n        conf = super().default_config()\n        # we're using FederationReaderServer, which uses a SlavedStore, so we\n        # have to tell the FederationHandler not to try to access stuff that is only\n        # in the primary store.\n        conf[\"worker_app\"] = \"yes\"\n\n        return conf\n\n    @parameterized.expand(\n        [\n            ([\"federation\"], \"auth_fail\"),\n            ([], \"no_resource\"),\n            ([\"openid\", \"federation\"], \"auth_fail\"),\n            ([\"openid\"], \"auth_fail\"),\n        ]\n    )\n    def test_openid_listener(self, names, expectation):\n        \"\"\"\n        Test different openid listener configurations.\n\n        401 is success here since it means we hit the handler and auth failed.\n        \"\"\"\n        config = {\n            \"port\": 8080,\n            \"type\": \"http\",\n            \"bind_addresses\": [\"0.0.0.0\"],\n            \"resources\": [{\"names\": names}],\n        }\n\n        # Listen with the config\n        self.hs._listen_http(parse_listener_def(config))\n\n        # Grab the resource from the site that was told to listen\n        site = self.reactor.tcpServers[0][1]\n        try:\n            site.resource.children[b\"_matrix\"].children[b\"federation\"]\n        except KeyError:\n            if expectation == \"no_resource\":\n                return\n            raise\n\n        _, channel = make_request(\n            self.reactor, site, \"GET\", \"/_matrix/federation/v1/openid/userinfo\"\n        )\n\n        self.assertEqual(channel.code, 401)", "target": 0}], "function_after": [{"function": "class FederationReaderOpenIDListenerTests(HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            federation_http_client=None, homeserver_to_use=GenericWorkerServer\n        )\n        return hs\n\n    def default_config(self):\n        conf = super().default_config()\n        # we're using FederationReaderServer, which uses a SlavedStore, so we\n        # have to tell the FederationHandler not to try to access stuff that is only\n        # in the primary store.\n        conf[\"worker_app\"] = \"yes\"\n\n        return conf\n\n    @parameterized.expand(\n        [\n            ([\"federation\"], \"auth_fail\"),\n            ([], \"no_resource\"),\n            ([\"openid\", \"federation\"], \"auth_fail\"),\n            ([\"openid\"], \"auth_fail\"),\n        ]\n    )\n    def test_openid_listener(self, names, expectation):\n        \"\"\"\n        Test different openid listener configurations.\n\n        401 is success here since it means we hit the handler and auth failed.\n        \"\"\"\n        config = {\n            \"port\": 8080,\n            \"type\": \"http\",\n            \"bind_addresses\": [\"0.0.0.0\"],\n            \"resources\": [{\"names\": names}],\n        }\n\n        # Listen with the config\n        self.hs._listen_http(parse_listener_def(config))\n\n        # Grab the resource from the site that was told to listen\n        site = self.reactor.tcpServers[0][1]\n        try:\n            site.resource.children[b\"_matrix\"].children[b\"federation\"]\n        except KeyError:\n            if expectation == \"no_resource\":\n                return\n            raise\n\n        _, channel = make_request(\n            self.reactor, site, \"GET\", \"/_matrix/federation/v1/openid/userinfo\"\n        )\n\n        self.assertEqual(channel.code, 401)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fcrypto%2Ftest_keyring.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2017 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport time\n\nfrom mock import Mock\n\nimport canonicaljson\nimport signedjson.key\nimport signedjson.sign\nfrom nacl.signing import SigningKey\nfrom signedjson.key import encode_verify_key_base64, get_verify_key\n\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred, ensureDeferred\n\nfrom synapse.api.errors import SynapseError\nfrom synapse.crypto import keyring\nfrom synapse.crypto.keyring import (\n    PerspectivesKeyFetcher,\n    ServerKeyFetcher,\n    StoreKeyFetcher,\n)\nfrom synapse.logging.context import (\n    LoggingContext,\n    current_context,\n    make_deferred_yieldable,\n)\nfrom synapse.storage.keys import FetchKeyResult\n\nfrom tests import unittest\nfrom tests.test_utils import make_awaitable\nfrom tests.unittest import logcontext_clean\n\n\nclass MockPerspectiveServer:\n    def __init__(self):\n        self.server_name = \"mock_server\"\n        self.key = signedjson.key.generate_signing_key(0)\n\n    def get_verify_keys(self):\n        vk = signedjson.key.get_verify_key(self.key)\n        return {\"%s:%s\" % (vk.alg, vk.version): encode_verify_key_base64(vk)}\n\n    def get_signed_key(self, server_name, verify_key):\n        key_id = \"%s:%s\" % (verify_key.alg, verify_key.version)\n        res = {\n            \"server_name\": server_name,\n            \"old_verify_keys\": {},\n            \"valid_until_ts\": time.time() * 1000 + 3600,\n            \"verify_keys\": {key_id: {\"key\": encode_verify_key_base64(verify_key)}},\n        }\n        self.sign_response(res)\n        return res\n\n    def sign_response(self, res):\n        signedjson.sign.sign_json(res, self.server_name, self.key)\n\n\n@logcontext_clean\nclass KeyringTestCase(unittest.HomeserverTestCase):\n    def check_context(self, val, expected):\n        self.assertEquals(getattr(current_context(), \"request\", None), expected)\n        return val\n\n    def test_verify_json_objects_for_server_awaits_previous_requests(self):\n        mock_fetcher = keyring.KeyFetcher()\n        mock_fetcher.get_keys = Mock()\n        kr = keyring.Keyring(self.hs, key_fetchers=(mock_fetcher,))\n\n        # a signed object that we are going to try to validate\n        key1 = signedjson.key.generate_signing_key(1)\n        json1 = {}\n        signedjson.sign.sign_json(json1, \"server10\", key1)\n\n        # start off a first set of lookups. We make the mock fetcher block until this\n        # deferred completes.\n        first_lookup_deferred = Deferred()\n\n        async def first_lookup_fetch(keys_to_fetch):\n            self.assertEquals(current_context().request, \"context_11\")\n            self.assertEqual(keys_to_fetch, {\"server10\": {get_key_id(key1): 0}})\n\n            await make_deferred_yieldable(first_lookup_deferred)\n            return {\n                \"server10\": {\n                    get_key_id(key1): FetchKeyResult(get_verify_key(key1), 100)\n                }\n            }\n\n        mock_fetcher.get_keys.side_effect = first_lookup_fetch\n\n        async def first_lookup():\n            with LoggingContext(\"context_11\") as context_11:\n                context_11.request = \"context_11\"\n\n                res_deferreds = kr.verify_json_objects_for_server(\n                    [(\"server10\", json1, 0, \"test10\"), (\"server11\", {}, 0, \"test11\")]\n                )\n\n                # the unsigned json should be rejected pretty quickly\n                self.assertTrue(res_deferreds[1].called)\n                try:\n                    await res_deferreds[1]\n                    self.assertFalse(\"unsigned json didn't cause a failure\")\n                except SynapseError:\n                    pass\n\n                self.assertFalse(res_deferreds[0].called)\n                res_deferreds[0].addBoth(self.check_context, None)\n\n                await make_deferred_yieldable(res_deferreds[0])\n\n        d0 = ensureDeferred(first_lookup())\n\n        mock_fetcher.get_keys.assert_called_once()\n\n        # a second request for a server with outstanding requests\n        # should block rather than start a second call\n\n        async def second_lookup_fetch(keys_to_fetch):\n            self.assertEquals(current_context().request, \"context_12\")\n            return {\n                \"server10\": {\n                    get_key_id(key1): FetchKeyResult(get_verify_key(key1), 100)\n                }\n            }\n\n        mock_fetcher.get_keys.reset_mock()\n        mock_fetcher.get_keys.side_effect = second_lookup_fetch\n        second_lookup_state = [0]\n\n        async def second_lookup():\n            with LoggingContext(\"context_12\") as context_12:\n                context_12.request = \"context_12\"\n\n                res_deferreds_2 = kr.verify_json_objects_for_server(\n                    [(\"server10\", json1, 0, \"test\")]\n                )\n                res_deferreds_2[0].addBoth(self.check_context, None)\n                second_lookup_state[0] = 1\n                await make_deferred_yieldable(res_deferreds_2[0])\n                second_lookup_state[0] = 2\n\n        d2 = ensureDeferred(second_lookup())\n\n        self.pump()\n        # the second request should be pending, but the fetcher should not yet have been\n        # called\n        self.assertEqual(second_lookup_state[0], 1)\n        mock_fetcher.get_keys.assert_not_called()\n\n        # complete the first request\n        first_lookup_deferred.callback(None)\n\n        # and now both verifications should succeed.\n        self.get_success(d0)\n        self.get_success(d2)\n\n    def test_verify_json_for_server(self):\n        kr = keyring.Keyring(self.hs)\n\n        key1 = signedjson.key.generate_signing_key(1)\n        r = self.hs.get_datastore().store_server_verify_keys(\n            \"server9\",\n            time.time() * 1000,\n            [(\"server9\", get_key_id(key1), FetchKeyResult(get_verify_key(key1), 1000))],\n        )\n        self.get_success(r)\n\n        json1 = {}\n        signedjson.sign.sign_json(json1, \"server9\", key1)\n\n        # should fail immediately on an unsigned object\n        d = _verify_json_for_server(kr, \"server9\", {}, 0, \"test unsigned\")\n        self.get_failure(d, SynapseError)\n\n        # should succeed on a signed object\n        d = _verify_json_for_server(kr, \"server9\", json1, 500, \"test signed\")\n        # self.assertFalse(d.called)\n        self.get_success(d)\n\n    def test_verify_json_for_server_with_null_valid_until_ms(self):\n        \"\"\"Tests that we correctly handle key requests for keys we've stored\n        with a null `ts_valid_until_ms`\n        \"\"\"\n        mock_fetcher = keyring.KeyFetcher()\n        mock_fetcher.get_keys = Mock(return_value=make_awaitable({}))\n\n        kr = keyring.Keyring(\n            self.hs, key_fetchers=(StoreKeyFetcher(self.hs), mock_fetcher)\n        )\n\n        key1 = signedjson.key.generate_signing_key(1)\n        r = self.hs.get_datastore().store_server_verify_keys(\n            \"server9\",\n            time.time() * 1000,\n            [(\"server9\", get_key_id(key1), FetchKeyResult(get_verify_key(key1), None))],\n        )\n        self.get_success(r)\n\n        json1 = {}\n        signedjson.sign.sign_json(json1, \"server9\", key1)\n\n        # should fail immediately on an unsigned object\n        d = _verify_json_for_server(kr, \"server9\", {}, 0, \"test unsigned\")\n        self.get_failure(d, SynapseError)\n\n        # should fail on a signed object with a non-zero minimum_valid_until_ms,\n        # as it tries to refetch the keys and fails.\n        d = _verify_json_for_server(\n            kr, \"server9\", json1, 500, \"test signed non-zero min\"\n        )\n        self.get_failure(d, SynapseError)\n\n        # We expect the keyring tried to refetch the key once.\n        mock_fetcher.get_keys.assert_called_once_with(\n            {\"server9\": {get_key_id(key1): 500}}\n        )\n\n        # should succeed on a signed object with a 0 minimum_valid_until_ms\n        d = _verify_json_for_server(\n            kr, \"server9\", json1, 0, \"test signed with zero min\"\n        )\n        self.get_success(d)\n\n    def test_verify_json_dedupes_key_requests(self):\n        \"\"\"Two requests for the same key should be deduped.\"\"\"\n        key1 = signedjson.key.generate_signing_key(1)\n\n        async def get_keys(keys_to_fetch):\n            # there should only be one request object (with the max validity)\n            self.assertEqual(keys_to_fetch, {\"server1\": {get_key_id(key1): 1500}})\n\n            return {\n                \"server1\": {\n                    get_key_id(key1): FetchKeyResult(get_verify_key(key1), 1200)\n                }\n            }\n\n        mock_fetcher = keyring.KeyFetcher()\n        mock_fetcher.get_keys = Mock(side_effect=get_keys)\n        kr = keyring.Keyring(self.hs, key_fetchers=(mock_fetcher,))\n\n        json1 = {}\n        signedjson.sign.sign_json(json1, \"server1\", key1)\n\n        # the first request should succeed; the second should fail because the key\n        # has expired\n        results = kr.verify_json_objects_for_server(\n            [(\"server1\", json1, 500, \"test1\"), (\"server1\", json1, 1500, \"test2\")]\n        )\n        self.assertEqual(len(results), 2)\n        self.get_success(results[0])\n        e = self.get_failure(results[1], SynapseError).value\n        self.assertEqual(e.errcode, \"M_UNAUTHORIZED\")\n        self.assertEqual(e.code, 401)\n\n        # there should have been a single call to the fetcher\n        mock_fetcher.get_keys.assert_called_once()\n\n    def test_verify_json_falls_back_to_other_fetchers(self):\n        \"\"\"If the first fetcher cannot provide a recent enough key, we fall back\"\"\"\n        key1 = signedjson.key.generate_signing_key(1)\n\n        async def get_keys1(keys_to_fetch):\n            self.assertEqual(keys_to_fetch, {\"server1\": {get_key_id(key1): 1500}})\n            return {\n                \"server1\": {get_key_id(key1): FetchKeyResult(get_verify_key(key1), 800)}\n            }\n\n        async def get_keys2(keys_to_fetch):\n            self.assertEqual(keys_to_fetch, {\"server1\": {get_key_id(key1): 1500}})\n            return {\n                \"server1\": {\n                    get_key_id(key1): FetchKeyResult(get_verify_key(key1), 1200)\n                }\n            }\n\n        mock_fetcher1 = keyring.KeyFetcher()\n        mock_fetcher1.get_keys = Mock(side_effect=get_keys1)\n        mock_fetcher2 = keyring.KeyFetcher()\n        mock_fetcher2.get_keys = Mock(side_effect=get_keys2)\n        kr = keyring.Keyring(self.hs, key_fetchers=(mock_fetcher1, mock_fetcher2))\n\n        json1 = {}\n        signedjson.sign.sign_json(json1, \"server1\", key1)\n\n        results = kr.verify_json_objects_for_server(\n            [(\"server1\", json1, 1200, \"test1\"), (\"server1\", json1, 1500, \"test2\")]\n        )\n        self.assertEqual(len(results), 2)\n        self.get_success(results[0])\n        e = self.get_failure(results[1], SynapseError).value\n        self.assertEqual(e.errcode, \"M_UNAUTHORIZED\")\n        self.assertEqual(e.code, 401)\n\n        # there should have been a single call to each fetcher\n        mock_fetcher1.get_keys.assert_called_once()\n        mock_fetcher2.get_keys.assert_called_once()\n\n\n@logcontext_clean\nclass ServerKeyFetcherTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        self.http_client = Mock()\n        hs = self.setup_test_homeserver(federation_http_client=self.http_client)\n        return hs\n\n    def test_get_keys_from_server(self):\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        SERVER_NAME = \"server2\"\n        fetcher = ServerKeyFetcher(self.hs)\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        # valid response\n        response = {\n            \"server_name\": SERVER_NAME,\n            \"old_verify_keys\": {},\n            \"valid_until_ts\": VALID_UNTIL_TS,\n            \"verify_keys\": {\n                testverifykey_id: {\n                    \"key\": signedjson.key.encode_verify_key_base64(testverifykey)\n                }\n            },\n        }\n        signedjson.sign.sign_json(response, SERVER_NAME, testkey)\n\n        async def get_json(destination, path, **kwargs):\n            self.assertEqual(destination, SERVER_NAME)\n            self.assertEqual(path, \"/_matrix/key/v2/server/key1\")\n            return response\n\n        self.http_client.get_json.side_effect = get_json\n\n        keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.valid_until_ts, VALID_UNTIL_TS)\n        self.assertEqual(k.verify_key, testverifykey)\n        self.assertEqual(k.verify_key.alg, \"ed25519\")\n        self.assertEqual(k.verify_key.version, \"ver1\")\n\n        # check that the perspectives store is correctly updated\n        lookup_triplet = (SERVER_NAME, testverifykey_id, None)\n        key_json = self.get_success(\n            self.hs.get_datastore().get_server_keys_json([lookup_triplet])\n        )\n        res = key_json[lookup_triplet]\n        self.assertEqual(len(res), 1)\n        res = res[0]\n        self.assertEqual(res[\"key_id\"], testverifykey_id)\n        self.assertEqual(res[\"from_server\"], SERVER_NAME)\n        self.assertEqual(res[\"ts_added_ms\"], self.reactor.seconds() * 1000)\n        self.assertEqual(res[\"ts_valid_until_ms\"], VALID_UNTIL_TS)\n\n        # we expect it to be encoded as canonical json *before* it hits the db\n        self.assertEqual(\n            bytes(res[\"key_json\"]), canonicaljson.encode_canonical_json(response)\n        )\n\n        # change the server name: the result should be ignored\n        response[\"server_name\"] = \"OTHER_SERVER\"\n\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        self.assertEqual(keys, {})\n\n\nclass PerspectivesKeyFetcherTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        self.mock_perspective_server = MockPerspectiveServer()\n        self.http_client = Mock()\n\n        config = self.default_config()\n        config[\"trusted_key_servers\"] = [\n            {\n                \"server_name\": self.mock_perspective_server.server_name,\n                \"verify_keys\": self.mock_perspective_server.get_verify_keys(),\n            }\n        ]\n\n        return self.setup_test_homeserver(\n            federation_http_client=self.http_client, config=config\n        )\n\n    def build_perspectives_response(\n        self, server_name: str, signing_key: SigningKey, valid_until_ts: int,\n    ) -> dict:\n        \"\"\"\n        Build a valid perspectives server response to a request for the given key\n        \"\"\"\n        verify_key = signedjson.key.get_verify_key(signing_key)\n        verifykey_id = \"%s:%s\" % (verify_key.alg, verify_key.version)\n\n        response = {\n            \"server_name\": server_name,\n            \"old_verify_keys\": {},\n            \"valid_until_ts\": valid_until_ts,\n            \"verify_keys\": {\n                verifykey_id: {\n                    \"key\": signedjson.key.encode_verify_key_base64(verify_key)\n                }\n            },\n        }\n        # the response must be signed by both the origin server and the perspectives\n        # server.\n        signedjson.sign.sign_json(response, server_name, signing_key)\n        self.mock_perspective_server.sign_response(response)\n        return response\n\n    def expect_outgoing_key_query(\n        self, expected_server_name: str, expected_key_id: str, response: dict\n    ) -> None:\n        \"\"\"\n        Tell the mock http client to expect a perspectives-server key query\n        \"\"\"\n\n        async def post_json(destination, path, data, **kwargs):\n            self.assertEqual(destination, self.mock_perspective_server.server_name)\n            self.assertEqual(path, \"/_matrix/key/v2/query\")\n\n            # check that the request is for the expected key\n            q = data[\"server_keys\"]\n            self.assertEqual(list(q[expected_server_name].keys()), [expected_key_id])\n            return {\"server_keys\": [response]}\n\n        self.http_client.post_json.side_effect = post_json\n\n    def test_get_keys_from_perspectives(self):\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        fetcher = PerspectivesKeyFetcher(self.hs)\n\n        SERVER_NAME = \"server2\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        response = self.build_perspectives_response(\n            SERVER_NAME, testkey, VALID_UNTIL_TS,\n        )\n\n        self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n\n        keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        self.assertIn(SERVER_NAME, keys)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.valid_until_ts, VALID_UNTIL_TS)\n        self.assertEqual(k.verify_key, testverifykey)\n        self.assertEqual(k.verify_key.alg, \"ed25519\")\n        self.assertEqual(k.verify_key.version, \"ver1\")\n\n        # check that the perspectives store is correctly updated\n        lookup_triplet = (SERVER_NAME, testverifykey_id, None)\n        key_json = self.get_success(\n            self.hs.get_datastore().get_server_keys_json([lookup_triplet])\n        )\n        res = key_json[lookup_triplet]\n        self.assertEqual(len(res), 1)\n        res = res[0]\n        self.assertEqual(res[\"key_id\"], testverifykey_id)\n        self.assertEqual(res[\"from_server\"], self.mock_perspective_server.server_name)\n        self.assertEqual(res[\"ts_added_ms\"], self.reactor.seconds() * 1000)\n        self.assertEqual(res[\"ts_valid_until_ms\"], VALID_UNTIL_TS)\n\n        self.assertEqual(\n            bytes(res[\"key_json\"]), canonicaljson.encode_canonical_json(response)\n        )\n\n    def test_get_perspectives_own_key(self):\n        \"\"\"Check that we can get the perspectives server's own keys\n\n        This is slightly complicated by the fact that the perspectives server may\n        use different keys for signing notary responses.\n        \"\"\"\n\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        fetcher = PerspectivesKeyFetcher(self.hs)\n\n        SERVER_NAME = self.mock_perspective_server.server_name\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        response = self.build_perspectives_response(\n            SERVER_NAME, testkey, VALID_UNTIL_TS\n        )\n\n        self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n\n        keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        self.assertIn(SERVER_NAME, keys)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.valid_until_ts, VALID_UNTIL_TS)\n        self.assertEqual(k.verify_key, testverifykey)\n        self.assertEqual(k.verify_key.alg, \"ed25519\")\n        self.assertEqual(k.verify_key.version, \"ver1\")\n\n        # check that the perspectives store is correctly updated\n        lookup_triplet = (SERVER_NAME, testverifykey_id, None)\n        key_json = self.get_success(\n            self.hs.get_datastore().get_server_keys_json([lookup_triplet])\n        )\n        res = key_json[lookup_triplet]\n        self.assertEqual(len(res), 1)\n        res = res[0]\n        self.assertEqual(res[\"key_id\"], testverifykey_id)\n        self.assertEqual(res[\"from_server\"], self.mock_perspective_server.server_name)\n        self.assertEqual(res[\"ts_added_ms\"], self.reactor.seconds() * 1000)\n        self.assertEqual(res[\"ts_valid_until_ms\"], VALID_UNTIL_TS)\n\n        self.assertEqual(\n            bytes(res[\"key_json\"]), canonicaljson.encode_canonical_json(response)\n        )\n\n    def test_invalid_perspectives_responses(self):\n        \"\"\"Check that invalid responses from the perspectives server are rejected\"\"\"\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        SERVER_NAME = \"server2\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        def build_response():\n            return self.build_perspectives_response(\n                SERVER_NAME, testkey, VALID_UNTIL_TS\n            )\n\n        def get_key_from_perspectives(response):\n            fetcher = PerspectivesKeyFetcher(self.hs)\n            keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n            self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n            return self.get_success(fetcher.get_keys(keys_to_fetch))\n\n        # start with a valid response so we can check we are testing the right thing\n        response = build_response()\n        keys = get_key_from_perspectives(response)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.verify_key, testverifykey)\n\n        # remove the perspectives server's signature\n        response = build_response()\n        del response[\"signatures\"][self.mock_perspective_server.server_name]\n        keys = get_key_from_perspectives(response)\n        self.assertEqual(keys, {}, \"Expected empty dict with missing persp server sig\")\n\n        # remove the origin server's signature\n        response = build_response()\n        del response[\"signatures\"][SERVER_NAME]\n        keys = get_key_from_perspectives(response)\n        self.assertEqual(keys, {}, \"Expected empty dict with missing origin server sig\")\n\n\ndef get_key_id(key):\n    \"\"\"Get the matrix ID tag for a given SigningKey or VerifyKey\"\"\"\n    return \"%s:%s\" % (key.alg, key.version)\n\n\n@defer.inlineCallbacks\ndef run_in_context(f, *args, **kwargs):\n    with LoggingContext(\"testctx\") as ctx:\n        # we set the \"request\" prop to make it easier to follow what's going on in the\n        # logs.\n        ctx.request = \"testctx\"\n        rv = yield f(*args, **kwargs)\n    return rv\n\n\ndef _verify_json_for_server(kr, *args):\n    \"\"\"thin wrapper around verify_json_for_server which makes sure it is wrapped\n    with the patched defer.inlineCallbacks.\n    \"\"\"\n\n    @defer.inlineCallbacks\n    def v():\n        rv1 = yield kr.verify_json_for_server(*args)\n        return rv1\n\n    return run_in_context(v)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2017 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport time\n\nfrom mock import Mock\n\nimport canonicaljson\nimport signedjson.key\nimport signedjson.sign\nfrom nacl.signing import SigningKey\nfrom signedjson.key import encode_verify_key_base64, get_verify_key\n\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred, ensureDeferred\n\nfrom synapse.api.errors import SynapseError\nfrom synapse.crypto import keyring\nfrom synapse.crypto.keyring import (\n    PerspectivesKeyFetcher,\n    ServerKeyFetcher,\n    StoreKeyFetcher,\n)\nfrom synapse.logging.context import (\n    LoggingContext,\n    current_context,\n    make_deferred_yieldable,\n)\nfrom synapse.storage.keys import FetchKeyResult\n\nfrom tests import unittest\nfrom tests.test_utils import make_awaitable\nfrom tests.unittest import logcontext_clean\n\n\nclass MockPerspectiveServer:\n    def __init__(self):\n        self.server_name = \"mock_server\"\n        self.key = signedjson.key.generate_signing_key(0)\n\n    def get_verify_keys(self):\n        vk = signedjson.key.get_verify_key(self.key)\n        return {\"%s:%s\" % (vk.alg, vk.version): encode_verify_key_base64(vk)}\n\n    def get_signed_key(self, server_name, verify_key):\n        key_id = \"%s:%s\" % (verify_key.alg, verify_key.version)\n        res = {\n            \"server_name\": server_name,\n            \"old_verify_keys\": {},\n            \"valid_until_ts\": time.time() * 1000 + 3600,\n            \"verify_keys\": {key_id: {\"key\": encode_verify_key_base64(verify_key)}},\n        }\n        self.sign_response(res)\n        return res\n\n    def sign_response(self, res):\n        signedjson.sign.sign_json(res, self.server_name, self.key)\n\n\n@logcontext_clean\nclass KeyringTestCase(unittest.HomeserverTestCase):\n    def check_context(self, val, expected):\n        self.assertEquals(getattr(current_context(), \"request\", None), expected)\n        return val\n\n    def test_verify_json_objects_for_server_awaits_previous_requests(self):\n        mock_fetcher = keyring.KeyFetcher()\n        mock_fetcher.get_keys = Mock()\n        kr = keyring.Keyring(self.hs, key_fetchers=(mock_fetcher,))\n\n        # a signed object that we are going to try to validate\n        key1 = signedjson.key.generate_signing_key(1)\n        json1 = {}\n        signedjson.sign.sign_json(json1, \"server10\", key1)\n\n        # start off a first set of lookups. We make the mock fetcher block until this\n        # deferred completes.\n        first_lookup_deferred = Deferred()\n\n        async def first_lookup_fetch(keys_to_fetch):\n            self.assertEquals(current_context().request, \"context_11\")\n            self.assertEqual(keys_to_fetch, {\"server10\": {get_key_id(key1): 0}})\n\n            await make_deferred_yieldable(first_lookup_deferred)\n            return {\n                \"server10\": {\n                    get_key_id(key1): FetchKeyResult(get_verify_key(key1), 100)\n                }\n            }\n\n        mock_fetcher.get_keys.side_effect = first_lookup_fetch\n\n        async def first_lookup():\n            with LoggingContext(\"context_11\") as context_11:\n                context_11.request = \"context_11\"\n\n                res_deferreds = kr.verify_json_objects_for_server(\n                    [(\"server10\", json1, 0, \"test10\"), (\"server11\", {}, 0, \"test11\")]\n                )\n\n                # the unsigned json should be rejected pretty quickly\n                self.assertTrue(res_deferreds[1].called)\n                try:\n                    await res_deferreds[1]\n                    self.assertFalse(\"unsigned json didn't cause a failure\")\n                except SynapseError:\n                    pass\n\n                self.assertFalse(res_deferreds[0].called)\n                res_deferreds[0].addBoth(self.check_context, None)\n\n                await make_deferred_yieldable(res_deferreds[0])\n\n        d0 = ensureDeferred(first_lookup())\n\n        mock_fetcher.get_keys.assert_called_once()\n\n        # a second request for a server with outstanding requests\n        # should block rather than start a second call\n\n        async def second_lookup_fetch(keys_to_fetch):\n            self.assertEquals(current_context().request, \"context_12\")\n            return {\n                \"server10\": {\n                    get_key_id(key1): FetchKeyResult(get_verify_key(key1), 100)\n                }\n            }\n\n        mock_fetcher.get_keys.reset_mock()\n        mock_fetcher.get_keys.side_effect = second_lookup_fetch\n        second_lookup_state = [0]\n\n        async def second_lookup():\n            with LoggingContext(\"context_12\") as context_12:\n                context_12.request = \"context_12\"\n\n                res_deferreds_2 = kr.verify_json_objects_for_server(\n                    [(\"server10\", json1, 0, \"test\")]\n                )\n                res_deferreds_2[0].addBoth(self.check_context, None)\n                second_lookup_state[0] = 1\n                await make_deferred_yieldable(res_deferreds_2[0])\n                second_lookup_state[0] = 2\n\n        d2 = ensureDeferred(second_lookup())\n\n        self.pump()\n        # the second request should be pending, but the fetcher should not yet have been\n        # called\n        self.assertEqual(second_lookup_state[0], 1)\n        mock_fetcher.get_keys.assert_not_called()\n\n        # complete the first request\n        first_lookup_deferred.callback(None)\n\n        # and now both verifications should succeed.\n        self.get_success(d0)\n        self.get_success(d2)\n\n    def test_verify_json_for_server(self):\n        kr = keyring.Keyring(self.hs)\n\n        key1 = signedjson.key.generate_signing_key(1)\n        r = self.hs.get_datastore().store_server_verify_keys(\n            \"server9\",\n            time.time() * 1000,\n            [(\"server9\", get_key_id(key1), FetchKeyResult(get_verify_key(key1), 1000))],\n        )\n        self.get_success(r)\n\n        json1 = {}\n        signedjson.sign.sign_json(json1, \"server9\", key1)\n\n        # should fail immediately on an unsigned object\n        d = _verify_json_for_server(kr, \"server9\", {}, 0, \"test unsigned\")\n        self.get_failure(d, SynapseError)\n\n        # should succeed on a signed object\n        d = _verify_json_for_server(kr, \"server9\", json1, 500, \"test signed\")\n        # self.assertFalse(d.called)\n        self.get_success(d)\n\n    def test_verify_json_for_server_with_null_valid_until_ms(self):\n        \"\"\"Tests that we correctly handle key requests for keys we've stored\n        with a null `ts_valid_until_ms`\n        \"\"\"\n        mock_fetcher = keyring.KeyFetcher()\n        mock_fetcher.get_keys = Mock(return_value=make_awaitable({}))\n\n        kr = keyring.Keyring(\n            self.hs, key_fetchers=(StoreKeyFetcher(self.hs), mock_fetcher)\n        )\n\n        key1 = signedjson.key.generate_signing_key(1)\n        r = self.hs.get_datastore().store_server_verify_keys(\n            \"server9\",\n            time.time() * 1000,\n            [(\"server9\", get_key_id(key1), FetchKeyResult(get_verify_key(key1), None))],\n        )\n        self.get_success(r)\n\n        json1 = {}\n        signedjson.sign.sign_json(json1, \"server9\", key1)\n\n        # should fail immediately on an unsigned object\n        d = _verify_json_for_server(kr, \"server9\", {}, 0, \"test unsigned\")\n        self.get_failure(d, SynapseError)\n\n        # should fail on a signed object with a non-zero minimum_valid_until_ms,\n        # as it tries to refetch the keys and fails.\n        d = _verify_json_for_server(\n            kr, \"server9\", json1, 500, \"test signed non-zero min\"\n        )\n        self.get_failure(d, SynapseError)\n\n        # We expect the keyring tried to refetch the key once.\n        mock_fetcher.get_keys.assert_called_once_with(\n            {\"server9\": {get_key_id(key1): 500}}\n        )\n\n        # should succeed on a signed object with a 0 minimum_valid_until_ms\n        d = _verify_json_for_server(\n            kr, \"server9\", json1, 0, \"test signed with zero min\"\n        )\n        self.get_success(d)\n\n    def test_verify_json_dedupes_key_requests(self):\n        \"\"\"Two requests for the same key should be deduped.\"\"\"\n        key1 = signedjson.key.generate_signing_key(1)\n\n        async def get_keys(keys_to_fetch):\n            # there should only be one request object (with the max validity)\n            self.assertEqual(keys_to_fetch, {\"server1\": {get_key_id(key1): 1500}})\n\n            return {\n                \"server1\": {\n                    get_key_id(key1): FetchKeyResult(get_verify_key(key1), 1200)\n                }\n            }\n\n        mock_fetcher = keyring.KeyFetcher()\n        mock_fetcher.get_keys = Mock(side_effect=get_keys)\n        kr = keyring.Keyring(self.hs, key_fetchers=(mock_fetcher,))\n\n        json1 = {}\n        signedjson.sign.sign_json(json1, \"server1\", key1)\n\n        # the first request should succeed; the second should fail because the key\n        # has expired\n        results = kr.verify_json_objects_for_server(\n            [(\"server1\", json1, 500, \"test1\"), (\"server1\", json1, 1500, \"test2\")]\n        )\n        self.assertEqual(len(results), 2)\n        self.get_success(results[0])\n        e = self.get_failure(results[1], SynapseError).value\n        self.assertEqual(e.errcode, \"M_UNAUTHORIZED\")\n        self.assertEqual(e.code, 401)\n\n        # there should have been a single call to the fetcher\n        mock_fetcher.get_keys.assert_called_once()\n\n    def test_verify_json_falls_back_to_other_fetchers(self):\n        \"\"\"If the first fetcher cannot provide a recent enough key, we fall back\"\"\"\n        key1 = signedjson.key.generate_signing_key(1)\n\n        async def get_keys1(keys_to_fetch):\n            self.assertEqual(keys_to_fetch, {\"server1\": {get_key_id(key1): 1500}})\n            return {\n                \"server1\": {get_key_id(key1): FetchKeyResult(get_verify_key(key1), 800)}\n            }\n\n        async def get_keys2(keys_to_fetch):\n            self.assertEqual(keys_to_fetch, {\"server1\": {get_key_id(key1): 1500}})\n            return {\n                \"server1\": {\n                    get_key_id(key1): FetchKeyResult(get_verify_key(key1), 1200)\n                }\n            }\n\n        mock_fetcher1 = keyring.KeyFetcher()\n        mock_fetcher1.get_keys = Mock(side_effect=get_keys1)\n        mock_fetcher2 = keyring.KeyFetcher()\n        mock_fetcher2.get_keys = Mock(side_effect=get_keys2)\n        kr = keyring.Keyring(self.hs, key_fetchers=(mock_fetcher1, mock_fetcher2))\n\n        json1 = {}\n        signedjson.sign.sign_json(json1, \"server1\", key1)\n\n        results = kr.verify_json_objects_for_server(\n            [(\"server1\", json1, 1200, \"test1\"), (\"server1\", json1, 1500, \"test2\")]\n        )\n        self.assertEqual(len(results), 2)\n        self.get_success(results[0])\n        e = self.get_failure(results[1], SynapseError).value\n        self.assertEqual(e.errcode, \"M_UNAUTHORIZED\")\n        self.assertEqual(e.code, 401)\n\n        # there should have been a single call to each fetcher\n        mock_fetcher1.get_keys.assert_called_once()\n        mock_fetcher2.get_keys.assert_called_once()\n\n\n@logcontext_clean\nclass ServerKeyFetcherTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        self.http_client = Mock()\n        hs = self.setup_test_homeserver(http_client=self.http_client)\n        return hs\n\n    def test_get_keys_from_server(self):\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        SERVER_NAME = \"server2\"\n        fetcher = ServerKeyFetcher(self.hs)\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        # valid response\n        response = {\n            \"server_name\": SERVER_NAME,\n            \"old_verify_keys\": {},\n            \"valid_until_ts\": VALID_UNTIL_TS,\n            \"verify_keys\": {\n                testverifykey_id: {\n                    \"key\": signedjson.key.encode_verify_key_base64(testverifykey)\n                }\n            },\n        }\n        signedjson.sign.sign_json(response, SERVER_NAME, testkey)\n\n        async def get_json(destination, path, **kwargs):\n            self.assertEqual(destination, SERVER_NAME)\n            self.assertEqual(path, \"/_matrix/key/v2/server/key1\")\n            return response\n\n        self.http_client.get_json.side_effect = get_json\n\n        keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.valid_until_ts, VALID_UNTIL_TS)\n        self.assertEqual(k.verify_key, testverifykey)\n        self.assertEqual(k.verify_key.alg, \"ed25519\")\n        self.assertEqual(k.verify_key.version, \"ver1\")\n\n        # check that the perspectives store is correctly updated\n        lookup_triplet = (SERVER_NAME, testverifykey_id, None)\n        key_json = self.get_success(\n            self.hs.get_datastore().get_server_keys_json([lookup_triplet])\n        )\n        res = key_json[lookup_triplet]\n        self.assertEqual(len(res), 1)\n        res = res[0]\n        self.assertEqual(res[\"key_id\"], testverifykey_id)\n        self.assertEqual(res[\"from_server\"], SERVER_NAME)\n        self.assertEqual(res[\"ts_added_ms\"], self.reactor.seconds() * 1000)\n        self.assertEqual(res[\"ts_valid_until_ms\"], VALID_UNTIL_TS)\n\n        # we expect it to be encoded as canonical json *before* it hits the db\n        self.assertEqual(\n            bytes(res[\"key_json\"]), canonicaljson.encode_canonical_json(response)\n        )\n\n        # change the server name: the result should be ignored\n        response[\"server_name\"] = \"OTHER_SERVER\"\n\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        self.assertEqual(keys, {})\n\n\nclass PerspectivesKeyFetcherTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        self.mock_perspective_server = MockPerspectiveServer()\n        self.http_client = Mock()\n\n        config = self.default_config()\n        config[\"trusted_key_servers\"] = [\n            {\n                \"server_name\": self.mock_perspective_server.server_name,\n                \"verify_keys\": self.mock_perspective_server.get_verify_keys(),\n            }\n        ]\n\n        return self.setup_test_homeserver(http_client=self.http_client, config=config)\n\n    def build_perspectives_response(\n        self, server_name: str, signing_key: SigningKey, valid_until_ts: int,\n    ) -> dict:\n        \"\"\"\n        Build a valid perspectives server response to a request for the given key\n        \"\"\"\n        verify_key = signedjson.key.get_verify_key(signing_key)\n        verifykey_id = \"%s:%s\" % (verify_key.alg, verify_key.version)\n\n        response = {\n            \"server_name\": server_name,\n            \"old_verify_keys\": {},\n            \"valid_until_ts\": valid_until_ts,\n            \"verify_keys\": {\n                verifykey_id: {\n                    \"key\": signedjson.key.encode_verify_key_base64(verify_key)\n                }\n            },\n        }\n        # the response must be signed by both the origin server and the perspectives\n        # server.\n        signedjson.sign.sign_json(response, server_name, signing_key)\n        self.mock_perspective_server.sign_response(response)\n        return response\n\n    def expect_outgoing_key_query(\n        self, expected_server_name: str, expected_key_id: str, response: dict\n    ) -> None:\n        \"\"\"\n        Tell the mock http client to expect a perspectives-server key query\n        \"\"\"\n\n        async def post_json(destination, path, data, **kwargs):\n            self.assertEqual(destination, self.mock_perspective_server.server_name)\n            self.assertEqual(path, \"/_matrix/key/v2/query\")\n\n            # check that the request is for the expected key\n            q = data[\"server_keys\"]\n            self.assertEqual(list(q[expected_server_name].keys()), [expected_key_id])\n            return {\"server_keys\": [response]}\n\n        self.http_client.post_json.side_effect = post_json\n\n    def test_get_keys_from_perspectives(self):\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        fetcher = PerspectivesKeyFetcher(self.hs)\n\n        SERVER_NAME = \"server2\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        response = self.build_perspectives_response(\n            SERVER_NAME, testkey, VALID_UNTIL_TS,\n        )\n\n        self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n\n        keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        self.assertIn(SERVER_NAME, keys)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.valid_until_ts, VALID_UNTIL_TS)\n        self.assertEqual(k.verify_key, testverifykey)\n        self.assertEqual(k.verify_key.alg, \"ed25519\")\n        self.assertEqual(k.verify_key.version, \"ver1\")\n\n        # check that the perspectives store is correctly updated\n        lookup_triplet = (SERVER_NAME, testverifykey_id, None)\n        key_json = self.get_success(\n            self.hs.get_datastore().get_server_keys_json([lookup_triplet])\n        )\n        res = key_json[lookup_triplet]\n        self.assertEqual(len(res), 1)\n        res = res[0]\n        self.assertEqual(res[\"key_id\"], testverifykey_id)\n        self.assertEqual(res[\"from_server\"], self.mock_perspective_server.server_name)\n        self.assertEqual(res[\"ts_added_ms\"], self.reactor.seconds() * 1000)\n        self.assertEqual(res[\"ts_valid_until_ms\"], VALID_UNTIL_TS)\n\n        self.assertEqual(\n            bytes(res[\"key_json\"]), canonicaljson.encode_canonical_json(response)\n        )\n\n    def test_get_perspectives_own_key(self):\n        \"\"\"Check that we can get the perspectives server's own keys\n\n        This is slightly complicated by the fact that the perspectives server may\n        use different keys for signing notary responses.\n        \"\"\"\n\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        fetcher = PerspectivesKeyFetcher(self.hs)\n\n        SERVER_NAME = self.mock_perspective_server.server_name\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        response = self.build_perspectives_response(\n            SERVER_NAME, testkey, VALID_UNTIL_TS\n        )\n\n        self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n\n        keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        self.assertIn(SERVER_NAME, keys)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.valid_until_ts, VALID_UNTIL_TS)\n        self.assertEqual(k.verify_key, testverifykey)\n        self.assertEqual(k.verify_key.alg, \"ed25519\")\n        self.assertEqual(k.verify_key.version, \"ver1\")\n\n        # check that the perspectives store is correctly updated\n        lookup_triplet = (SERVER_NAME, testverifykey_id, None)\n        key_json = self.get_success(\n            self.hs.get_datastore().get_server_keys_json([lookup_triplet])\n        )\n        res = key_json[lookup_triplet]\n        self.assertEqual(len(res), 1)\n        res = res[0]\n        self.assertEqual(res[\"key_id\"], testverifykey_id)\n        self.assertEqual(res[\"from_server\"], self.mock_perspective_server.server_name)\n        self.assertEqual(res[\"ts_added_ms\"], self.reactor.seconds() * 1000)\n        self.assertEqual(res[\"ts_valid_until_ms\"], VALID_UNTIL_TS)\n\n        self.assertEqual(\n            bytes(res[\"key_json\"]), canonicaljson.encode_canonical_json(response)\n        )\n\n    def test_invalid_perspectives_responses(self):\n        \"\"\"Check that invalid responses from the perspectives server are rejected\"\"\"\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        SERVER_NAME = \"server2\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        def build_response():\n            return self.build_perspectives_response(\n                SERVER_NAME, testkey, VALID_UNTIL_TS\n            )\n\n        def get_key_from_perspectives(response):\n            fetcher = PerspectivesKeyFetcher(self.hs)\n            keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n            self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n            return self.get_success(fetcher.get_keys(keys_to_fetch))\n\n        # start with a valid response so we can check we are testing the right thing\n        response = build_response()\n        keys = get_key_from_perspectives(response)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.verify_key, testverifykey)\n\n        # remove the perspectives server's signature\n        response = build_response()\n        del response[\"signatures\"][self.mock_perspective_server.server_name]\n        keys = get_key_from_perspectives(response)\n        self.assertEqual(keys, {}, \"Expected empty dict with missing persp server sig\")\n\n        # remove the origin server's signature\n        response = build_response()\n        del response[\"signatures\"][SERVER_NAME]\n        keys = get_key_from_perspectives(response)\n        self.assertEqual(keys, {}, \"Expected empty dict with missing origin server sig\")\n\n\ndef get_key_id(key):\n    \"\"\"Get the matrix ID tag for a given SigningKey or VerifyKey\"\"\"\n    return \"%s:%s\" % (key.alg, key.version)\n\n\n@defer.inlineCallbacks\ndef run_in_context(f, *args, **kwargs):\n    with LoggingContext(\"testctx\") as ctx:\n        # we set the \"request\" prop to make it easier to follow what's going on in the\n        # logs.\n        ctx.request = \"testctx\"\n        rv = yield f(*args, **kwargs)\n    return rv\n\n\ndef _verify_json_for_server(kr, *args):\n    \"\"\"thin wrapper around verify_json_for_server which makes sure it is wrapped\n    with the patched defer.inlineCallbacks.\n    \"\"\"\n\n    @defer.inlineCallbacks\n    def v():\n        rv1 = yield kr.verify_json_for_server(*args)\n        return rv1\n\n    return run_in_context(v)\n", "patch": "@@ -315,7 +315,7 @@ async def get_keys2(keys_to_fetch):\n class ServerKeyFetcherTestCase(unittest.HomeserverTestCase):\n     def make_homeserver(self, reactor, clock):\n         self.http_client = Mock()\n-        hs = self.setup_test_homeserver(http_client=self.http_client)\n+        hs = self.setup_test_homeserver(federation_http_client=self.http_client)\n         return hs\n \n     def test_get_keys_from_server(self):\n@@ -395,7 +395,9 @@ def make_homeserver(self, reactor, clock):\n             }\n         ]\n \n-        return self.setup_test_homeserver(http_client=self.http_client, config=config)\n+        return self.setup_test_homeserver(\n+            federation_http_client=self.http_client, config=config\n+        )\n \n     def build_perspectives_response(\n         self, server_name: str, signing_key: SigningKey, valid_until_ts: int,", "file_path": "files/2021_2/33", "file_language": "py", "file_name": "tests/crypto/test_keyring.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class MockPerspectiveServer:\n    def __init__(self):\n        self.server_name = \"mock_server\"\n        self.key = signedjson.key.generate_signing_key(0)\n\n    def get_verify_keys(self):\n        vk = signedjson.key.get_verify_key(self.key)\n        return {\"%s:%s\" % (vk.alg, vk.version): encode_verify_key_base64(vk)}\n\n    def get_signed_key(self, server_name, verify_key):\n        key_id = \"%s:%s\" % (verify_key.alg, verify_key.version)\n        res = {\n            \"server_name\": server_name,\n            \"old_verify_keys\": {},\n            \"valid_until_ts\": time.time() * 1000 + 3600,\n            \"verify_keys\": {key_id: {\"key\": encode_verify_key_base64(verify_key)}},\n        }\n        self.sign_response(res)\n        return res\n\n    def sign_response(self, res):\n        signedjson.sign.sign_json(res, self.server_name, self.key)", "target": 0}, {"function": "class PerspectivesKeyFetcherTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        self.mock_perspective_server = MockPerspectiveServer()\n        self.http_client = Mock()\n\n        config = self.default_config()\n        config[\"trusted_key_servers\"] = [\n            {\n                \"server_name\": self.mock_perspective_server.server_name,\n                \"verify_keys\": self.mock_perspective_server.get_verify_keys(),\n            }\n        ]\n\n        return self.setup_test_homeserver(http_client=self.http_client, config=config)\n\n    def build_perspectives_response(\n        self, server_name: str, signing_key: SigningKey, valid_until_ts: int,\n    ) -> dict:\n        \"\"\"\n        Build a valid perspectives server response to a request for the given key\n        \"\"\"\n        verify_key = signedjson.key.get_verify_key(signing_key)\n        verifykey_id = \"%s:%s\" % (verify_key.alg, verify_key.version)\n\n        response = {\n            \"server_name\": server_name,\n            \"old_verify_keys\": {},\n            \"valid_until_ts\": valid_until_ts,\n            \"verify_keys\": {\n                verifykey_id: {\n                    \"key\": signedjson.key.encode_verify_key_base64(verify_key)\n                }\n            },\n        }\n        # the response must be signed by both the origin server and the perspectives\n        # server.\n        signedjson.sign.sign_json(response, server_name, signing_key)\n        self.mock_perspective_server.sign_response(response)\n        return response\n\n    def expect_outgoing_key_query(\n        self, expected_server_name: str, expected_key_id: str, response: dict\n    ) -> None:\n        \"\"\"\n        Tell the mock http client to expect a perspectives-server key query\n        \"\"\"\n\n        async def post_json(destination, path, data, **kwargs):\n            self.assertEqual(destination, self.mock_perspective_server.server_name)\n            self.assertEqual(path, \"/_matrix/key/v2/query\")\n\n            # check that the request is for the expected key\n            q = data[\"server_keys\"]\n            self.assertEqual(list(q[expected_server_name].keys()), [expected_key_id])\n            return {\"server_keys\": [response]}\n\n        self.http_client.post_json.side_effect = post_json\n\n    def test_get_keys_from_perspectives(self):\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        fetcher = PerspectivesKeyFetcher(self.hs)\n\n        SERVER_NAME = \"server2\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        response = self.build_perspectives_response(\n            SERVER_NAME, testkey, VALID_UNTIL_TS,\n        )\n\n        self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n\n        keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        self.assertIn(SERVER_NAME, keys)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.valid_until_ts, VALID_UNTIL_TS)\n        self.assertEqual(k.verify_key, testverifykey)\n        self.assertEqual(k.verify_key.alg, \"ed25519\")\n        self.assertEqual(k.verify_key.version, \"ver1\")\n\n        # check that the perspectives store is correctly updated\n        lookup_triplet = (SERVER_NAME, testverifykey_id, None)\n        key_json = self.get_success(\n            self.hs.get_datastore().get_server_keys_json([lookup_triplet])\n        )\n        res = key_json[lookup_triplet]\n        self.assertEqual(len(res), 1)\n        res = res[0]\n        self.assertEqual(res[\"key_id\"], testverifykey_id)\n        self.assertEqual(res[\"from_server\"], self.mock_perspective_server.server_name)\n        self.assertEqual(res[\"ts_added_ms\"], self.reactor.seconds() * 1000)\n        self.assertEqual(res[\"ts_valid_until_ms\"], VALID_UNTIL_TS)\n\n        self.assertEqual(\n            bytes(res[\"key_json\"]), canonicaljson.encode_canonical_json(response)\n        )\n\n    def test_get_perspectives_own_key(self):\n        \"\"\"Check that we can get the perspectives server's own keys\n\n        This is slightly complicated by the fact that the perspectives server may\n        use different keys for signing notary responses.\n        \"\"\"\n\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        fetcher = PerspectivesKeyFetcher(self.hs)\n\n        SERVER_NAME = self.mock_perspective_server.server_name\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        response = self.build_perspectives_response(\n            SERVER_NAME, testkey, VALID_UNTIL_TS\n        )\n\n        self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n\n        keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        self.assertIn(SERVER_NAME, keys)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.valid_until_ts, VALID_UNTIL_TS)\n        self.assertEqual(k.verify_key, testverifykey)\n        self.assertEqual(k.verify_key.alg, \"ed25519\")\n        self.assertEqual(k.verify_key.version, \"ver1\")\n\n        # check that the perspectives store is correctly updated\n        lookup_triplet = (SERVER_NAME, testverifykey_id, None)\n        key_json = self.get_success(\n            self.hs.get_datastore().get_server_keys_json([lookup_triplet])\n        )\n        res = key_json[lookup_triplet]\n        self.assertEqual(len(res), 1)\n        res = res[0]\n        self.assertEqual(res[\"key_id\"], testverifykey_id)\n        self.assertEqual(res[\"from_server\"], self.mock_perspective_server.server_name)\n        self.assertEqual(res[\"ts_added_ms\"], self.reactor.seconds() * 1000)\n        self.assertEqual(res[\"ts_valid_until_ms\"], VALID_UNTIL_TS)\n\n        self.assertEqual(\n            bytes(res[\"key_json\"]), canonicaljson.encode_canonical_json(response)\n        )\n\n    def test_invalid_perspectives_responses(self):\n        \"\"\"Check that invalid responses from the perspectives server are rejected\"\"\"\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        SERVER_NAME = \"server2\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        def build_response():\n            return self.build_perspectives_response(\n                SERVER_NAME, testkey, VALID_UNTIL_TS\n            )\n\n        def get_key_from_perspectives(response):\n            fetcher = PerspectivesKeyFetcher(self.hs)\n            keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n            self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n            return self.get_success(fetcher.get_keys(keys_to_fetch))\n\n        # start with a valid response so we can check we are testing the right thing\n        response = build_response()\n        keys = get_key_from_perspectives(response)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.verify_key, testverifykey)\n\n        # remove the perspectives server's signature\n        response = build_response()\n        del response[\"signatures\"][self.mock_perspective_server.server_name]\n        keys = get_key_from_perspectives(response)\n        self.assertEqual(keys, {}, \"Expected empty dict with missing persp server sig\")\n\n        # remove the origin server's signature\n        response = build_response()\n        del response[\"signatures\"][SERVER_NAME]\n        keys = get_key_from_perspectives(response)\n        self.assertEqual(keys, {}, \"Expected empty dict with missing origin server sig\")", "target": 0}, {"function": "def get_key_id(key):\n    \"\"\"Get the matrix ID tag for a given SigningKey or VerifyKey\"\"\"\n    return \"%s:%s\" % (key.alg, key.version)", "target": 0}, {"function": "def _verify_json_for_server(kr, *args):\n    \"\"\"thin wrapper around verify_json_for_server which makes sure it is wrapped\n    with the patched defer.inlineCallbacks.\n    \"\"\"\n\n    @defer.inlineCallbacks\n    def v():\n        rv1 = yield kr.verify_json_for_server(*args)\n        return rv1\n\n    return run_in_context(v)", "target": 0}], "function_after": [{"function": "class MockPerspectiveServer:\n    def __init__(self):\n        self.server_name = \"mock_server\"\n        self.key = signedjson.key.generate_signing_key(0)\n\n    def get_verify_keys(self):\n        vk = signedjson.key.get_verify_key(self.key)\n        return {\"%s:%s\" % (vk.alg, vk.version): encode_verify_key_base64(vk)}\n\n    def get_signed_key(self, server_name, verify_key):\n        key_id = \"%s:%s\" % (verify_key.alg, verify_key.version)\n        res = {\n            \"server_name\": server_name,\n            \"old_verify_keys\": {},\n            \"valid_until_ts\": time.time() * 1000 + 3600,\n            \"verify_keys\": {key_id: {\"key\": encode_verify_key_base64(verify_key)}},\n        }\n        self.sign_response(res)\n        return res\n\n    def sign_response(self, res):\n        signedjson.sign.sign_json(res, self.server_name, self.key)", "target": 0}, {"function": "class PerspectivesKeyFetcherTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        self.mock_perspective_server = MockPerspectiveServer()\n        self.http_client = Mock()\n\n        config = self.default_config()\n        config[\"trusted_key_servers\"] = [\n            {\n                \"server_name\": self.mock_perspective_server.server_name,\n                \"verify_keys\": self.mock_perspective_server.get_verify_keys(),\n            }\n        ]\n\n        return self.setup_test_homeserver(\n            federation_http_client=self.http_client, config=config\n        )\n\n    def build_perspectives_response(\n        self, server_name: str, signing_key: SigningKey, valid_until_ts: int,\n    ) -> dict:\n        \"\"\"\n        Build a valid perspectives server response to a request for the given key\n        \"\"\"\n        verify_key = signedjson.key.get_verify_key(signing_key)\n        verifykey_id = \"%s:%s\" % (verify_key.alg, verify_key.version)\n\n        response = {\n            \"server_name\": server_name,\n            \"old_verify_keys\": {},\n            \"valid_until_ts\": valid_until_ts,\n            \"verify_keys\": {\n                verifykey_id: {\n                    \"key\": signedjson.key.encode_verify_key_base64(verify_key)\n                }\n            },\n        }\n        # the response must be signed by both the origin server and the perspectives\n        # server.\n        signedjson.sign.sign_json(response, server_name, signing_key)\n        self.mock_perspective_server.sign_response(response)\n        return response\n\n    def expect_outgoing_key_query(\n        self, expected_server_name: str, expected_key_id: str, response: dict\n    ) -> None:\n        \"\"\"\n        Tell the mock http client to expect a perspectives-server key query\n        \"\"\"\n\n        async def post_json(destination, path, data, **kwargs):\n            self.assertEqual(destination, self.mock_perspective_server.server_name)\n            self.assertEqual(path, \"/_matrix/key/v2/query\")\n\n            # check that the request is for the expected key\n            q = data[\"server_keys\"]\n            self.assertEqual(list(q[expected_server_name].keys()), [expected_key_id])\n            return {\"server_keys\": [response]}\n\n        self.http_client.post_json.side_effect = post_json\n\n    def test_get_keys_from_perspectives(self):\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        fetcher = PerspectivesKeyFetcher(self.hs)\n\n        SERVER_NAME = \"server2\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        response = self.build_perspectives_response(\n            SERVER_NAME, testkey, VALID_UNTIL_TS,\n        )\n\n        self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n\n        keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        self.assertIn(SERVER_NAME, keys)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.valid_until_ts, VALID_UNTIL_TS)\n        self.assertEqual(k.verify_key, testverifykey)\n        self.assertEqual(k.verify_key.alg, \"ed25519\")\n        self.assertEqual(k.verify_key.version, \"ver1\")\n\n        # check that the perspectives store is correctly updated\n        lookup_triplet = (SERVER_NAME, testverifykey_id, None)\n        key_json = self.get_success(\n            self.hs.get_datastore().get_server_keys_json([lookup_triplet])\n        )\n        res = key_json[lookup_triplet]\n        self.assertEqual(len(res), 1)\n        res = res[0]\n        self.assertEqual(res[\"key_id\"], testverifykey_id)\n        self.assertEqual(res[\"from_server\"], self.mock_perspective_server.server_name)\n        self.assertEqual(res[\"ts_added_ms\"], self.reactor.seconds() * 1000)\n        self.assertEqual(res[\"ts_valid_until_ms\"], VALID_UNTIL_TS)\n\n        self.assertEqual(\n            bytes(res[\"key_json\"]), canonicaljson.encode_canonical_json(response)\n        )\n\n    def test_get_perspectives_own_key(self):\n        \"\"\"Check that we can get the perspectives server's own keys\n\n        This is slightly complicated by the fact that the perspectives server may\n        use different keys for signing notary responses.\n        \"\"\"\n\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        fetcher = PerspectivesKeyFetcher(self.hs)\n\n        SERVER_NAME = self.mock_perspective_server.server_name\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        response = self.build_perspectives_response(\n            SERVER_NAME, testkey, VALID_UNTIL_TS\n        )\n\n        self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n\n        keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n        keys = self.get_success(fetcher.get_keys(keys_to_fetch))\n        self.assertIn(SERVER_NAME, keys)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.valid_until_ts, VALID_UNTIL_TS)\n        self.assertEqual(k.verify_key, testverifykey)\n        self.assertEqual(k.verify_key.alg, \"ed25519\")\n        self.assertEqual(k.verify_key.version, \"ver1\")\n\n        # check that the perspectives store is correctly updated\n        lookup_triplet = (SERVER_NAME, testverifykey_id, None)\n        key_json = self.get_success(\n            self.hs.get_datastore().get_server_keys_json([lookup_triplet])\n        )\n        res = key_json[lookup_triplet]\n        self.assertEqual(len(res), 1)\n        res = res[0]\n        self.assertEqual(res[\"key_id\"], testverifykey_id)\n        self.assertEqual(res[\"from_server\"], self.mock_perspective_server.server_name)\n        self.assertEqual(res[\"ts_added_ms\"], self.reactor.seconds() * 1000)\n        self.assertEqual(res[\"ts_valid_until_ms\"], VALID_UNTIL_TS)\n\n        self.assertEqual(\n            bytes(res[\"key_json\"]), canonicaljson.encode_canonical_json(response)\n        )\n\n    def test_invalid_perspectives_responses(self):\n        \"\"\"Check that invalid responses from the perspectives server are rejected\"\"\"\n        # arbitrarily advance the clock a bit\n        self.reactor.advance(100)\n\n        SERVER_NAME = \"server2\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        testverifykey = signedjson.key.get_verify_key(testkey)\n        testverifykey_id = \"ed25519:ver1\"\n        VALID_UNTIL_TS = 200 * 1000\n\n        def build_response():\n            return self.build_perspectives_response(\n                SERVER_NAME, testkey, VALID_UNTIL_TS\n            )\n\n        def get_key_from_perspectives(response):\n            fetcher = PerspectivesKeyFetcher(self.hs)\n            keys_to_fetch = {SERVER_NAME: {\"key1\": 0}}\n            self.expect_outgoing_key_query(SERVER_NAME, \"key1\", response)\n            return self.get_success(fetcher.get_keys(keys_to_fetch))\n\n        # start with a valid response so we can check we are testing the right thing\n        response = build_response()\n        keys = get_key_from_perspectives(response)\n        k = keys[SERVER_NAME][testverifykey_id]\n        self.assertEqual(k.verify_key, testverifykey)\n\n        # remove the perspectives server's signature\n        response = build_response()\n        del response[\"signatures\"][self.mock_perspective_server.server_name]\n        keys = get_key_from_perspectives(response)\n        self.assertEqual(keys, {}, \"Expected empty dict with missing persp server sig\")\n\n        # remove the origin server's signature\n        response = build_response()\n        del response[\"signatures\"][SERVER_NAME]\n        keys = get_key_from_perspectives(response)\n        self.assertEqual(keys, {}, \"Expected empty dict with missing origin server sig\")", "target": 0}, {"function": "def get_key_id(key):\n    \"\"\"Get the matrix ID tag for a given SigningKey or VerifyKey\"\"\"\n    return \"%s:%s\" % (key.alg, key.version)", "target": 0}, {"function": "def _verify_json_for_server(kr, *args):\n    \"\"\"thin wrapper around verify_json_for_server which makes sure it is wrapped\n    with the patched defer.inlineCallbacks.\n    \"\"\"\n\n    @defer.inlineCallbacks\n    def v():\n        rv1 = yield kr.verify_json_for_server(*args)\n        return rv1\n\n    return run_in_context(v)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fhandlers%2Ftest_device.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport synapse.api.errors\nimport synapse.handlers.device\nimport synapse.storage\n\nfrom tests import unittest\n\nuser1 = \"@boris:aaa\"\nuser2 = \"@theresa:bbb\"\n\n\nclass DeviceTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n        self.handler = hs.get_device_handler()\n        self.store = hs.get_datastore()\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        # These tests assume that it starts 1000 seconds in.\n        self.reactor.advance(1000)\n\n    def test_device_is_created_with_invalid_name(self):\n        self.get_failure(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"foo\",\n                initial_device_display_name=\"a\"\n                * (synapse.handlers.device.MAX_DEVICE_DISPLAY_NAME_LEN + 1),\n            ),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_device_is_created_if_doesnt_exist(self):\n        res = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"display name\",\n            )\n        )\n        self.assertEqual(res, \"fco\")\n\n        dev = self.get_success(self.handler.store.get_device(\"@boris:foo\", \"fco\"))\n        self.assertEqual(dev[\"display_name\"], \"display name\")\n\n    def test_device_is_preserved_if_exists(self):\n        res1 = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"display name\",\n            )\n        )\n        self.assertEqual(res1, \"fco\")\n\n        res2 = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"new display name\",\n            )\n        )\n        self.assertEqual(res2, \"fco\")\n\n        dev = self.get_success(self.handler.store.get_device(\"@boris:foo\", \"fco\"))\n        self.assertEqual(dev[\"display_name\"], \"display name\")\n\n    def test_device_id_is_made_up_if_unspecified(self):\n        device_id = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@theresa:foo\",\n                device_id=None,\n                initial_device_display_name=\"display\",\n            )\n        )\n\n        dev = self.get_success(self.handler.store.get_device(\"@theresa:foo\", device_id))\n        self.assertEqual(dev[\"display_name\"], \"display\")\n\n    def test_get_devices_by_user(self):\n        self._record_users()\n\n        res = self.get_success(self.handler.get_devices_by_user(user1))\n\n        self.assertEqual(3, len(res))\n        device_map = {d[\"device_id\"]: d for d in res}\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"xyz\",\n                \"display_name\": \"display 0\",\n                \"last_seen_ip\": None,\n                \"last_seen_ts\": None,\n            },\n            device_map[\"xyz\"],\n        )\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"fco\",\n                \"display_name\": \"display 1\",\n                \"last_seen_ip\": \"ip1\",\n                \"last_seen_ts\": 1000000,\n            },\n            device_map[\"fco\"],\n        )\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"abc\",\n                \"display_name\": \"display 2\",\n                \"last_seen_ip\": \"ip3\",\n                \"last_seen_ts\": 3000000,\n            },\n            device_map[\"abc\"],\n        )\n\n    def test_get_device(self):\n        self._record_users()\n\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"abc\",\n                \"display_name\": \"display 2\",\n                \"last_seen_ip\": \"ip3\",\n                \"last_seen_ts\": 3000000,\n            },\n            res,\n        )\n\n    def test_delete_device(self):\n        self._record_users()\n\n        # delete the device\n        self.get_success(self.handler.delete_device(user1, \"abc\"))\n\n        # check the device was deleted\n        self.get_failure(\n            self.handler.get_device(user1, \"abc\"), synapse.api.errors.NotFoundError\n        )\n\n        # we'd like to check the access token was invalidated, but that's a\n        # bit of a PITA.\n\n    def test_update_device(self):\n        self._record_users()\n\n        update = {\"display_name\": \"new display\"}\n        self.get_success(self.handler.update_device(user1, \"abc\", update))\n\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertEqual(res[\"display_name\"], \"new display\")\n\n    def test_update_device_too_long_display_name(self):\n        \"\"\"Update a device with a display name that is invalid (too long).\"\"\"\n        self._record_users()\n\n        # Request to update a device display name with a new value that is longer than allowed.\n        update = {\n            \"display_name\": \"a\"\n            * (synapse.handlers.device.MAX_DEVICE_DISPLAY_NAME_LEN + 1)\n        }\n        self.get_failure(\n            self.handler.update_device(user1, \"abc\", update),\n            synapse.api.errors.SynapseError,\n        )\n\n        # Ensure the display name was not updated.\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertEqual(res[\"display_name\"], \"display 2\")\n\n    def test_update_unknown_device(self):\n        update = {\"display_name\": \"new_display\"}\n        self.get_failure(\n            self.handler.update_device(\"user_id\", \"unknown_device_id\", update),\n            synapse.api.errors.NotFoundError,\n        )\n\n    def _record_users(self):\n        # check this works for both devices which have a recorded client_ip,\n        # and those which don't.\n        self._record_user(user1, \"xyz\", \"display 0\")\n        self._record_user(user1, \"fco\", \"display 1\", \"token1\", \"ip1\")\n        self._record_user(user1, \"abc\", \"display 2\", \"token2\", \"ip2\")\n        self._record_user(user1, \"abc\", \"display 2\", \"token3\", \"ip3\")\n\n        self._record_user(user2, \"def\", \"dispkay\", \"token4\", \"ip4\")\n\n        self.reactor.advance(10000)\n\n    def _record_user(\n        self, user_id, device_id, display_name, access_token=None, ip=None\n    ):\n        device_id = self.get_success(\n            self.handler.check_device_registered(\n                user_id=user_id,\n                device_id=device_id,\n                initial_device_display_name=display_name,\n            )\n        )\n\n        if ip is not None:\n            self.get_success(\n                self.store.insert_client_ip(\n                    user_id, access_token, ip, \"user_agent\", device_id\n                )\n            )\n            self.reactor.advance(1000)\n\n\nclass DehydrationTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n        self.handler = hs.get_device_handler()\n        self.registration = hs.get_registration_handler()\n        self.auth = hs.get_auth()\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_dehydrate_and_rehydrate_device(self):\n        user_id = \"@boris:dehydration\"\n\n        self.get_success(self.store.register_user(user_id, \"foobar\"))\n\n        # First check if we can store and fetch a dehydrated device\n        stored_dehydrated_device_id = self.get_success(\n            self.handler.store_dehydrated_device(\n                user_id=user_id,\n                device_data={\"device_data\": {\"foo\": \"bar\"}},\n                initial_device_display_name=\"dehydrated device\",\n            )\n        )\n\n        retrieved_device_id, device_data = self.get_success(\n            self.handler.get_dehydrated_device(user_id=user_id)\n        )\n\n        self.assertEqual(retrieved_device_id, stored_dehydrated_device_id)\n        self.assertEqual(device_data, {\"device_data\": {\"foo\": \"bar\"}})\n\n        # Create a new login for the user and dehydrated the device\n        device_id, access_token = self.get_success(\n            self.registration.register_device(\n                user_id=user_id, device_id=None, initial_display_name=\"new device\",\n            )\n        )\n\n        # Trying to claim a nonexistent device should throw an error\n        self.get_failure(\n            self.handler.rehydrate_device(\n                user_id=user_id,\n                access_token=access_token,\n                device_id=\"not the right device ID\",\n            ),\n            synapse.api.errors.NotFoundError,\n        )\n\n        # dehydrating the right devices should succeed and change our device ID\n        # to the dehydrated device's ID\n        res = self.get_success(\n            self.handler.rehydrate_device(\n                user_id=user_id,\n                access_token=access_token,\n                device_id=retrieved_device_id,\n            )\n        )\n\n        self.assertEqual(res, {\"success\": True})\n\n        # make sure that our device ID has changed\n        user_info = self.get_success(self.auth.get_user_by_access_token(access_token))\n\n        self.assertEqual(user_info.device_id, retrieved_device_id)\n\n        # make sure the device has the display name that was set from the login\n        res = self.get_success(self.handler.get_device(user_id, retrieved_device_id))\n\n        self.assertEqual(res[\"display_name\"], \"new device\")\n\n        # make sure that the device ID that we were initially assigned no longer exists\n        self.get_failure(\n            self.handler.get_device(user_id, device_id),\n            synapse.api.errors.NotFoundError,\n        )\n\n        # make sure that there's no device available for dehydrating now\n        ret = self.get_success(self.handler.get_dehydrated_device(user_id=user_id))\n\n        self.assertIsNone(ret)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport synapse.api.errors\nimport synapse.handlers.device\nimport synapse.storage\n\nfrom tests import unittest\n\nuser1 = \"@boris:aaa\"\nuser2 = \"@theresa:bbb\"\n\n\nclass DeviceTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", http_client=None)\n        self.handler = hs.get_device_handler()\n        self.store = hs.get_datastore()\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        # These tests assume that it starts 1000 seconds in.\n        self.reactor.advance(1000)\n\n    def test_device_is_created_with_invalid_name(self):\n        self.get_failure(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"foo\",\n                initial_device_display_name=\"a\"\n                * (synapse.handlers.device.MAX_DEVICE_DISPLAY_NAME_LEN + 1),\n            ),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_device_is_created_if_doesnt_exist(self):\n        res = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"display name\",\n            )\n        )\n        self.assertEqual(res, \"fco\")\n\n        dev = self.get_success(self.handler.store.get_device(\"@boris:foo\", \"fco\"))\n        self.assertEqual(dev[\"display_name\"], \"display name\")\n\n    def test_device_is_preserved_if_exists(self):\n        res1 = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"display name\",\n            )\n        )\n        self.assertEqual(res1, \"fco\")\n\n        res2 = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"new display name\",\n            )\n        )\n        self.assertEqual(res2, \"fco\")\n\n        dev = self.get_success(self.handler.store.get_device(\"@boris:foo\", \"fco\"))\n        self.assertEqual(dev[\"display_name\"], \"display name\")\n\n    def test_device_id_is_made_up_if_unspecified(self):\n        device_id = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@theresa:foo\",\n                device_id=None,\n                initial_device_display_name=\"display\",\n            )\n        )\n\n        dev = self.get_success(self.handler.store.get_device(\"@theresa:foo\", device_id))\n        self.assertEqual(dev[\"display_name\"], \"display\")\n\n    def test_get_devices_by_user(self):\n        self._record_users()\n\n        res = self.get_success(self.handler.get_devices_by_user(user1))\n\n        self.assertEqual(3, len(res))\n        device_map = {d[\"device_id\"]: d for d in res}\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"xyz\",\n                \"display_name\": \"display 0\",\n                \"last_seen_ip\": None,\n                \"last_seen_ts\": None,\n            },\n            device_map[\"xyz\"],\n        )\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"fco\",\n                \"display_name\": \"display 1\",\n                \"last_seen_ip\": \"ip1\",\n                \"last_seen_ts\": 1000000,\n            },\n            device_map[\"fco\"],\n        )\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"abc\",\n                \"display_name\": \"display 2\",\n                \"last_seen_ip\": \"ip3\",\n                \"last_seen_ts\": 3000000,\n            },\n            device_map[\"abc\"],\n        )\n\n    def test_get_device(self):\n        self._record_users()\n\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"abc\",\n                \"display_name\": \"display 2\",\n                \"last_seen_ip\": \"ip3\",\n                \"last_seen_ts\": 3000000,\n            },\n            res,\n        )\n\n    def test_delete_device(self):\n        self._record_users()\n\n        # delete the device\n        self.get_success(self.handler.delete_device(user1, \"abc\"))\n\n        # check the device was deleted\n        self.get_failure(\n            self.handler.get_device(user1, \"abc\"), synapse.api.errors.NotFoundError\n        )\n\n        # we'd like to check the access token was invalidated, but that's a\n        # bit of a PITA.\n\n    def test_update_device(self):\n        self._record_users()\n\n        update = {\"display_name\": \"new display\"}\n        self.get_success(self.handler.update_device(user1, \"abc\", update))\n\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertEqual(res[\"display_name\"], \"new display\")\n\n    def test_update_device_too_long_display_name(self):\n        \"\"\"Update a device with a display name that is invalid (too long).\"\"\"\n        self._record_users()\n\n        # Request to update a device display name with a new value that is longer than allowed.\n        update = {\n            \"display_name\": \"a\"\n            * (synapse.handlers.device.MAX_DEVICE_DISPLAY_NAME_LEN + 1)\n        }\n        self.get_failure(\n            self.handler.update_device(user1, \"abc\", update),\n            synapse.api.errors.SynapseError,\n        )\n\n        # Ensure the display name was not updated.\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertEqual(res[\"display_name\"], \"display 2\")\n\n    def test_update_unknown_device(self):\n        update = {\"display_name\": \"new_display\"}\n        self.get_failure(\n            self.handler.update_device(\"user_id\", \"unknown_device_id\", update),\n            synapse.api.errors.NotFoundError,\n        )\n\n    def _record_users(self):\n        # check this works for both devices which have a recorded client_ip,\n        # and those which don't.\n        self._record_user(user1, \"xyz\", \"display 0\")\n        self._record_user(user1, \"fco\", \"display 1\", \"token1\", \"ip1\")\n        self._record_user(user1, \"abc\", \"display 2\", \"token2\", \"ip2\")\n        self._record_user(user1, \"abc\", \"display 2\", \"token3\", \"ip3\")\n\n        self._record_user(user2, \"def\", \"dispkay\", \"token4\", \"ip4\")\n\n        self.reactor.advance(10000)\n\n    def _record_user(\n        self, user_id, device_id, display_name, access_token=None, ip=None\n    ):\n        device_id = self.get_success(\n            self.handler.check_device_registered(\n                user_id=user_id,\n                device_id=device_id,\n                initial_device_display_name=display_name,\n            )\n        )\n\n        if ip is not None:\n            self.get_success(\n                self.store.insert_client_ip(\n                    user_id, access_token, ip, \"user_agent\", device_id\n                )\n            )\n            self.reactor.advance(1000)\n\n\nclass DehydrationTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", http_client=None)\n        self.handler = hs.get_device_handler()\n        self.registration = hs.get_registration_handler()\n        self.auth = hs.get_auth()\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_dehydrate_and_rehydrate_device(self):\n        user_id = \"@boris:dehydration\"\n\n        self.get_success(self.store.register_user(user_id, \"foobar\"))\n\n        # First check if we can store and fetch a dehydrated device\n        stored_dehydrated_device_id = self.get_success(\n            self.handler.store_dehydrated_device(\n                user_id=user_id,\n                device_data={\"device_data\": {\"foo\": \"bar\"}},\n                initial_device_display_name=\"dehydrated device\",\n            )\n        )\n\n        retrieved_device_id, device_data = self.get_success(\n            self.handler.get_dehydrated_device(user_id=user_id)\n        )\n\n        self.assertEqual(retrieved_device_id, stored_dehydrated_device_id)\n        self.assertEqual(device_data, {\"device_data\": {\"foo\": \"bar\"}})\n\n        # Create a new login for the user and dehydrated the device\n        device_id, access_token = self.get_success(\n            self.registration.register_device(\n                user_id=user_id, device_id=None, initial_display_name=\"new device\",\n            )\n        )\n\n        # Trying to claim a nonexistent device should throw an error\n        self.get_failure(\n            self.handler.rehydrate_device(\n                user_id=user_id,\n                access_token=access_token,\n                device_id=\"not the right device ID\",\n            ),\n            synapse.api.errors.NotFoundError,\n        )\n\n        # dehydrating the right devices should succeed and change our device ID\n        # to the dehydrated device's ID\n        res = self.get_success(\n            self.handler.rehydrate_device(\n                user_id=user_id,\n                access_token=access_token,\n                device_id=retrieved_device_id,\n            )\n        )\n\n        self.assertEqual(res, {\"success\": True})\n\n        # make sure that our device ID has changed\n        user_info = self.get_success(self.auth.get_user_by_access_token(access_token))\n\n        self.assertEqual(user_info.device_id, retrieved_device_id)\n\n        # make sure the device has the display name that was set from the login\n        res = self.get_success(self.handler.get_device(user_id, retrieved_device_id))\n\n        self.assertEqual(res[\"display_name\"], \"new device\")\n\n        # make sure that the device ID that we were initially assigned no longer exists\n        self.get_failure(\n            self.handler.get_device(user_id, device_id),\n            synapse.api.errors.NotFoundError,\n        )\n\n        # make sure that there's no device available for dehydrating now\n        ret = self.get_success(self.handler.get_dehydrated_device(user_id=user_id))\n\n        self.assertIsNone(ret)\n", "patch": "@@ -27,7 +27,7 @@\n \n class DeviceTestCase(unittest.HomeserverTestCase):\n     def make_homeserver(self, reactor, clock):\n-        hs = self.setup_test_homeserver(\"server\", http_client=None)\n+        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n         self.handler = hs.get_device_handler()\n         self.store = hs.get_datastore()\n         return hs\n@@ -229,7 +229,7 @@ def _record_user(\n \n class DehydrationTestCase(unittest.HomeserverTestCase):\n     def make_homeserver(self, reactor, clock):\n-        hs = self.setup_test_homeserver(\"server\", http_client=None)\n+        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n         self.handler = hs.get_device_handler()\n         self.registration = hs.get_registration_handler()\n         self.auth = hs.get_auth()", "file_path": "files/2021_2/34", "file_language": "py", "file_name": "tests/handlers/test_device.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class DeviceTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", http_client=None)\n        self.handler = hs.get_device_handler()\n        self.store = hs.get_datastore()\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        # These tests assume that it starts 1000 seconds in.\n        self.reactor.advance(1000)\n\n    def test_device_is_created_with_invalid_name(self):\n        self.get_failure(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"foo\",\n                initial_device_display_name=\"a\"\n                * (synapse.handlers.device.MAX_DEVICE_DISPLAY_NAME_LEN + 1),\n            ),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_device_is_created_if_doesnt_exist(self):\n        res = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"display name\",\n            )\n        )\n        self.assertEqual(res, \"fco\")\n\n        dev = self.get_success(self.handler.store.get_device(\"@boris:foo\", \"fco\"))\n        self.assertEqual(dev[\"display_name\"], \"display name\")\n\n    def test_device_is_preserved_if_exists(self):\n        res1 = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"display name\",\n            )\n        )\n        self.assertEqual(res1, \"fco\")\n\n        res2 = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"new display name\",\n            )\n        )\n        self.assertEqual(res2, \"fco\")\n\n        dev = self.get_success(self.handler.store.get_device(\"@boris:foo\", \"fco\"))\n        self.assertEqual(dev[\"display_name\"], \"display name\")\n\n    def test_device_id_is_made_up_if_unspecified(self):\n        device_id = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@theresa:foo\",\n                device_id=None,\n                initial_device_display_name=\"display\",\n            )\n        )\n\n        dev = self.get_success(self.handler.store.get_device(\"@theresa:foo\", device_id))\n        self.assertEqual(dev[\"display_name\"], \"display\")\n\n    def test_get_devices_by_user(self):\n        self._record_users()\n\n        res = self.get_success(self.handler.get_devices_by_user(user1))\n\n        self.assertEqual(3, len(res))\n        device_map = {d[\"device_id\"]: d for d in res}\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"xyz\",\n                \"display_name\": \"display 0\",\n                \"last_seen_ip\": None,\n                \"last_seen_ts\": None,\n            },\n            device_map[\"xyz\"],\n        )\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"fco\",\n                \"display_name\": \"display 1\",\n                \"last_seen_ip\": \"ip1\",\n                \"last_seen_ts\": 1000000,\n            },\n            device_map[\"fco\"],\n        )\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"abc\",\n                \"display_name\": \"display 2\",\n                \"last_seen_ip\": \"ip3\",\n                \"last_seen_ts\": 3000000,\n            },\n            device_map[\"abc\"],\n        )\n\n    def test_get_device(self):\n        self._record_users()\n\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"abc\",\n                \"display_name\": \"display 2\",\n                \"last_seen_ip\": \"ip3\",\n                \"last_seen_ts\": 3000000,\n            },\n            res,\n        )\n\n    def test_delete_device(self):\n        self._record_users()\n\n        # delete the device\n        self.get_success(self.handler.delete_device(user1, \"abc\"))\n\n        # check the device was deleted\n        self.get_failure(\n            self.handler.get_device(user1, \"abc\"), synapse.api.errors.NotFoundError\n        )\n\n        # we'd like to check the access token was invalidated, but that's a\n        # bit of a PITA.\n\n    def test_update_device(self):\n        self._record_users()\n\n        update = {\"display_name\": \"new display\"}\n        self.get_success(self.handler.update_device(user1, \"abc\", update))\n\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertEqual(res[\"display_name\"], \"new display\")\n\n    def test_update_device_too_long_display_name(self):\n        \"\"\"Update a device with a display name that is invalid (too long).\"\"\"\n        self._record_users()\n\n        # Request to update a device display name with a new value that is longer than allowed.\n        update = {\n            \"display_name\": \"a\"\n            * (synapse.handlers.device.MAX_DEVICE_DISPLAY_NAME_LEN + 1)\n        }\n        self.get_failure(\n            self.handler.update_device(user1, \"abc\", update),\n            synapse.api.errors.SynapseError,\n        )\n\n        # Ensure the display name was not updated.\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertEqual(res[\"display_name\"], \"display 2\")\n\n    def test_update_unknown_device(self):\n        update = {\"display_name\": \"new_display\"}\n        self.get_failure(\n            self.handler.update_device(\"user_id\", \"unknown_device_id\", update),\n            synapse.api.errors.NotFoundError,\n        )\n\n    def _record_users(self):\n        # check this works for both devices which have a recorded client_ip,\n        # and those which don't.\n        self._record_user(user1, \"xyz\", \"display 0\")\n        self._record_user(user1, \"fco\", \"display 1\", \"token1\", \"ip1\")\n        self._record_user(user1, \"abc\", \"display 2\", \"token2\", \"ip2\")\n        self._record_user(user1, \"abc\", \"display 2\", \"token3\", \"ip3\")\n\n        self._record_user(user2, \"def\", \"dispkay\", \"token4\", \"ip4\")\n\n        self.reactor.advance(10000)\n\n    def _record_user(\n        self, user_id, device_id, display_name, access_token=None, ip=None\n    ):\n        device_id = self.get_success(\n            self.handler.check_device_registered(\n                user_id=user_id,\n                device_id=device_id,\n                initial_device_display_name=display_name,\n            )\n        )\n\n        if ip is not None:\n            self.get_success(\n                self.store.insert_client_ip(\n                    user_id, access_token, ip, \"user_agent\", device_id\n                )\n            )\n            self.reactor.advance(1000)", "target": 0}, {"function": "class DehydrationTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", http_client=None)\n        self.handler = hs.get_device_handler()\n        self.registration = hs.get_registration_handler()\n        self.auth = hs.get_auth()\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_dehydrate_and_rehydrate_device(self):\n        user_id = \"@boris:dehydration\"\n\n        self.get_success(self.store.register_user(user_id, \"foobar\"))\n\n        # First check if we can store and fetch a dehydrated device\n        stored_dehydrated_device_id = self.get_success(\n            self.handler.store_dehydrated_device(\n                user_id=user_id,\n                device_data={\"device_data\": {\"foo\": \"bar\"}},\n                initial_device_display_name=\"dehydrated device\",\n            )\n        )\n\n        retrieved_device_id, device_data = self.get_success(\n            self.handler.get_dehydrated_device(user_id=user_id)\n        )\n\n        self.assertEqual(retrieved_device_id, stored_dehydrated_device_id)\n        self.assertEqual(device_data, {\"device_data\": {\"foo\": \"bar\"}})\n\n        # Create a new login for the user and dehydrated the device\n        device_id, access_token = self.get_success(\n            self.registration.register_device(\n                user_id=user_id, device_id=None, initial_display_name=\"new device\",\n            )\n        )\n\n        # Trying to claim a nonexistent device should throw an error\n        self.get_failure(\n            self.handler.rehydrate_device(\n                user_id=user_id,\n                access_token=access_token,\n                device_id=\"not the right device ID\",\n            ),\n            synapse.api.errors.NotFoundError,\n        )\n\n        # dehydrating the right devices should succeed and change our device ID\n        # to the dehydrated device's ID\n        res = self.get_success(\n            self.handler.rehydrate_device(\n                user_id=user_id,\n                access_token=access_token,\n                device_id=retrieved_device_id,\n            )\n        )\n\n        self.assertEqual(res, {\"success\": True})\n\n        # make sure that our device ID has changed\n        user_info = self.get_success(self.auth.get_user_by_access_token(access_token))\n\n        self.assertEqual(user_info.device_id, retrieved_device_id)\n\n        # make sure the device has the display name that was set from the login\n        res = self.get_success(self.handler.get_device(user_id, retrieved_device_id))\n\n        self.assertEqual(res[\"display_name\"], \"new device\")\n\n        # make sure that the device ID that we were initially assigned no longer exists\n        self.get_failure(\n            self.handler.get_device(user_id, device_id),\n            synapse.api.errors.NotFoundError,\n        )\n\n        # make sure that there's no device available for dehydrating now\n        ret = self.get_success(self.handler.get_dehydrated_device(user_id=user_id))\n\n        self.assertIsNone(ret)", "target": 0}], "function_after": [{"function": "class DeviceTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n        self.handler = hs.get_device_handler()\n        self.store = hs.get_datastore()\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        # These tests assume that it starts 1000 seconds in.\n        self.reactor.advance(1000)\n\n    def test_device_is_created_with_invalid_name(self):\n        self.get_failure(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"foo\",\n                initial_device_display_name=\"a\"\n                * (synapse.handlers.device.MAX_DEVICE_DISPLAY_NAME_LEN + 1),\n            ),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_device_is_created_if_doesnt_exist(self):\n        res = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"display name\",\n            )\n        )\n        self.assertEqual(res, \"fco\")\n\n        dev = self.get_success(self.handler.store.get_device(\"@boris:foo\", \"fco\"))\n        self.assertEqual(dev[\"display_name\"], \"display name\")\n\n    def test_device_is_preserved_if_exists(self):\n        res1 = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"display name\",\n            )\n        )\n        self.assertEqual(res1, \"fco\")\n\n        res2 = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@boris:foo\",\n                device_id=\"fco\",\n                initial_device_display_name=\"new display name\",\n            )\n        )\n        self.assertEqual(res2, \"fco\")\n\n        dev = self.get_success(self.handler.store.get_device(\"@boris:foo\", \"fco\"))\n        self.assertEqual(dev[\"display_name\"], \"display name\")\n\n    def test_device_id_is_made_up_if_unspecified(self):\n        device_id = self.get_success(\n            self.handler.check_device_registered(\n                user_id=\"@theresa:foo\",\n                device_id=None,\n                initial_device_display_name=\"display\",\n            )\n        )\n\n        dev = self.get_success(self.handler.store.get_device(\"@theresa:foo\", device_id))\n        self.assertEqual(dev[\"display_name\"], \"display\")\n\n    def test_get_devices_by_user(self):\n        self._record_users()\n\n        res = self.get_success(self.handler.get_devices_by_user(user1))\n\n        self.assertEqual(3, len(res))\n        device_map = {d[\"device_id\"]: d for d in res}\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"xyz\",\n                \"display_name\": \"display 0\",\n                \"last_seen_ip\": None,\n                \"last_seen_ts\": None,\n            },\n            device_map[\"xyz\"],\n        )\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"fco\",\n                \"display_name\": \"display 1\",\n                \"last_seen_ip\": \"ip1\",\n                \"last_seen_ts\": 1000000,\n            },\n            device_map[\"fco\"],\n        )\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"abc\",\n                \"display_name\": \"display 2\",\n                \"last_seen_ip\": \"ip3\",\n                \"last_seen_ts\": 3000000,\n            },\n            device_map[\"abc\"],\n        )\n\n    def test_get_device(self):\n        self._record_users()\n\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertDictContainsSubset(\n            {\n                \"user_id\": user1,\n                \"device_id\": \"abc\",\n                \"display_name\": \"display 2\",\n                \"last_seen_ip\": \"ip3\",\n                \"last_seen_ts\": 3000000,\n            },\n            res,\n        )\n\n    def test_delete_device(self):\n        self._record_users()\n\n        # delete the device\n        self.get_success(self.handler.delete_device(user1, \"abc\"))\n\n        # check the device was deleted\n        self.get_failure(\n            self.handler.get_device(user1, \"abc\"), synapse.api.errors.NotFoundError\n        )\n\n        # we'd like to check the access token was invalidated, but that's a\n        # bit of a PITA.\n\n    def test_update_device(self):\n        self._record_users()\n\n        update = {\"display_name\": \"new display\"}\n        self.get_success(self.handler.update_device(user1, \"abc\", update))\n\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertEqual(res[\"display_name\"], \"new display\")\n\n    def test_update_device_too_long_display_name(self):\n        \"\"\"Update a device with a display name that is invalid (too long).\"\"\"\n        self._record_users()\n\n        # Request to update a device display name with a new value that is longer than allowed.\n        update = {\n            \"display_name\": \"a\"\n            * (synapse.handlers.device.MAX_DEVICE_DISPLAY_NAME_LEN + 1)\n        }\n        self.get_failure(\n            self.handler.update_device(user1, \"abc\", update),\n            synapse.api.errors.SynapseError,\n        )\n\n        # Ensure the display name was not updated.\n        res = self.get_success(self.handler.get_device(user1, \"abc\"))\n        self.assertEqual(res[\"display_name\"], \"display 2\")\n\n    def test_update_unknown_device(self):\n        update = {\"display_name\": \"new_display\"}\n        self.get_failure(\n            self.handler.update_device(\"user_id\", \"unknown_device_id\", update),\n            synapse.api.errors.NotFoundError,\n        )\n\n    def _record_users(self):\n        # check this works for both devices which have a recorded client_ip,\n        # and those which don't.\n        self._record_user(user1, \"xyz\", \"display 0\")\n        self._record_user(user1, \"fco\", \"display 1\", \"token1\", \"ip1\")\n        self._record_user(user1, \"abc\", \"display 2\", \"token2\", \"ip2\")\n        self._record_user(user1, \"abc\", \"display 2\", \"token3\", \"ip3\")\n\n        self._record_user(user2, \"def\", \"dispkay\", \"token4\", \"ip4\")\n\n        self.reactor.advance(10000)\n\n    def _record_user(\n        self, user_id, device_id, display_name, access_token=None, ip=None\n    ):\n        device_id = self.get_success(\n            self.handler.check_device_registered(\n                user_id=user_id,\n                device_id=device_id,\n                initial_device_display_name=display_name,\n            )\n        )\n\n        if ip is not None:\n            self.get_success(\n                self.store.insert_client_ip(\n                    user_id, access_token, ip, \"user_agent\", device_id\n                )\n            )\n            self.reactor.advance(1000)", "target": 0}, {"function": "class DehydrationTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n        self.handler = hs.get_device_handler()\n        self.registration = hs.get_registration_handler()\n        self.auth = hs.get_auth()\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_dehydrate_and_rehydrate_device(self):\n        user_id = \"@boris:dehydration\"\n\n        self.get_success(self.store.register_user(user_id, \"foobar\"))\n\n        # First check if we can store and fetch a dehydrated device\n        stored_dehydrated_device_id = self.get_success(\n            self.handler.store_dehydrated_device(\n                user_id=user_id,\n                device_data={\"device_data\": {\"foo\": \"bar\"}},\n                initial_device_display_name=\"dehydrated device\",\n            )\n        )\n\n        retrieved_device_id, device_data = self.get_success(\n            self.handler.get_dehydrated_device(user_id=user_id)\n        )\n\n        self.assertEqual(retrieved_device_id, stored_dehydrated_device_id)\n        self.assertEqual(device_data, {\"device_data\": {\"foo\": \"bar\"}})\n\n        # Create a new login for the user and dehydrated the device\n        device_id, access_token = self.get_success(\n            self.registration.register_device(\n                user_id=user_id, device_id=None, initial_display_name=\"new device\",\n            )\n        )\n\n        # Trying to claim a nonexistent device should throw an error\n        self.get_failure(\n            self.handler.rehydrate_device(\n                user_id=user_id,\n                access_token=access_token,\n                device_id=\"not the right device ID\",\n            ),\n            synapse.api.errors.NotFoundError,\n        )\n\n        # dehydrating the right devices should succeed and change our device ID\n        # to the dehydrated device's ID\n        res = self.get_success(\n            self.handler.rehydrate_device(\n                user_id=user_id,\n                access_token=access_token,\n                device_id=retrieved_device_id,\n            )\n        )\n\n        self.assertEqual(res, {\"success\": True})\n\n        # make sure that our device ID has changed\n        user_info = self.get_success(self.auth.get_user_by_access_token(access_token))\n\n        self.assertEqual(user_info.device_id, retrieved_device_id)\n\n        # make sure the device has the display name that was set from the login\n        res = self.get_success(self.handler.get_device(user_id, retrieved_device_id))\n\n        self.assertEqual(res[\"display_name\"], \"new device\")\n\n        # make sure that the device ID that we were initially assigned no longer exists\n        self.get_failure(\n            self.handler.get_device(user_id, device_id),\n            synapse.api.errors.NotFoundError,\n        )\n\n        # make sure that there's no device available for dehydrating now\n        ret = self.get_success(self.handler.get_dehydrated_device(user_id=user_id))\n\n        self.assertIsNone(ret)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fhandlers%2Ftest_directory.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom mock import Mock\n\nimport synapse\nimport synapse.api.errors\nfrom synapse.api.constants import EventTypes\nfrom synapse.config.room_directory import RoomDirectoryConfig\nfrom synapse.rest.client.v1 import directory, login, room\nfrom synapse.types import RoomAlias, create_requester\n\nfrom tests import unittest\nfrom tests.test_utils import make_awaitable\n\n\nclass DirectoryTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests the directory service. \"\"\"\n\n    def make_homeserver(self, reactor, clock):\n        self.mock_federation = Mock()\n        self.mock_registry = Mock()\n\n        self.query_handlers = {}\n\n        def register_query_handler(query_type, handler):\n            self.query_handlers[query_type] = handler\n\n        self.mock_registry.register_query_handler = register_query_handler\n\n        hs = self.setup_test_homeserver(\n            federation_http_client=None,\n            resource_for_federation=Mock(),\n            federation_client=self.mock_federation,\n            federation_registry=self.mock_registry,\n        )\n\n        self.handler = hs.get_directory_handler()\n\n        self.store = hs.get_datastore()\n\n        self.my_room = RoomAlias.from_string(\"#my-room:test\")\n        self.your_room = RoomAlias.from_string(\"#your-room:test\")\n        self.remote_room = RoomAlias.from_string(\"#another:remote\")\n\n        return hs\n\n    def test_get_local_association(self):\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.my_room, \"!8765qwer:test\", [\"test\"]\n            )\n        )\n\n        result = self.get_success(self.handler.get_association(self.my_room))\n\n        self.assertEquals({\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\"]}, result)\n\n    def test_get_remote_association(self):\n        self.mock_federation.make_query.return_value = make_awaitable(\n            {\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\", \"remote\"]}\n        )\n\n        result = self.get_success(self.handler.get_association(self.remote_room))\n\n        self.assertEquals(\n            {\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\", \"remote\"]}, result\n        )\n        self.mock_federation.make_query.assert_called_with(\n            destination=\"remote\",\n            query_type=\"directory\",\n            args={\"room_alias\": \"#another:remote\"},\n            retry_on_dns_fail=False,\n            ignore_backoff=True,\n        )\n\n    def test_incoming_fed_query(self):\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.your_room, \"!8765asdf:test\", [\"test\"]\n            )\n        )\n\n        response = self.get_success(\n            self.handler.on_directory_query({\"room_alias\": \"#your-room:test\"})\n        )\n\n        self.assertEquals({\"room_id\": \"!8765asdf:test\", \"servers\": [\"test\"]}, response)\n\n\nclass TestCreateAlias(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.handler = hs.get_directory_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = RoomAlias.from_string(self.test_alias)\n\n        # Create a test user.\n        self.test_user = self.register_user(\"user\", \"pass\", admin=False)\n        self.test_user_tok = self.login(\"user\", \"pass\")\n        self.helper.join(room=self.room_id, user=self.test_user, tok=self.test_user_tok)\n\n    def test_create_alias_joined_room(self):\n        \"\"\"A user can create an alias for a room they're in.\"\"\"\n        self.get_success(\n            self.handler.create_association(\n                create_requester(self.test_user), self.room_alias, self.room_id,\n            )\n        )\n\n    def test_create_alias_other_room(self):\n        \"\"\"A user cannot create an alias for a room they're NOT in.\"\"\"\n        other_room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.get_failure(\n            self.handler.create_association(\n                create_requester(self.test_user), self.room_alias, other_room_id,\n            ),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_create_alias_admin(self):\n        \"\"\"An admin can create an alias for a room they're NOT in.\"\"\"\n        other_room_id = self.helper.create_room_as(\n            self.test_user, tok=self.test_user_tok\n        )\n\n        self.get_success(\n            self.handler.create_association(\n                create_requester(self.admin_user), self.room_alias, other_room_id,\n            )\n        )\n\n\nclass TestDeleteAlias(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.handler = hs.get_directory_handler()\n        self.state_handler = hs.get_state_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = RoomAlias.from_string(self.test_alias)\n\n        # Create a test user.\n        self.test_user = self.register_user(\"user\", \"pass\", admin=False)\n        self.test_user_tok = self.login(\"user\", \"pass\")\n        self.helper.join(room=self.room_id, user=self.test_user, tok=self.test_user_tok)\n\n    def _create_alias(self, user):\n        # Create a new alias to this room.\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.room_alias, self.room_id, [\"test\"], user\n            )\n        )\n\n    def test_delete_alias_not_allowed(self):\n        \"\"\"A user that doesn't meet the expected guidelines cannot delete an alias.\"\"\"\n        self._create_alias(self.admin_user)\n        self.get_failure(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            ),\n            synapse.api.errors.AuthError,\n        )\n\n    def test_delete_alias_creator(self):\n        \"\"\"An alias creator can delete their own alias.\"\"\"\n        # Create an alias from a different user.\n        self._create_alias(self.test_user)\n\n        # Delete the user's alias.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_delete_alias_admin(self):\n        \"\"\"A server admin can delete an alias created by another user.\"\"\"\n        # Create an alias from a different user.\n        self._create_alias(self.test_user)\n\n        # Delete the user's alias as the admin.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_delete_alias_sufficient_power(self):\n        \"\"\"A user with a sufficient power level should be able to delete an alias.\"\"\"\n        self._create_alias(self.admin_user)\n\n        # Increase the user's power level.\n        self.helper.send_state(\n            self.room_id,\n            \"m.room.power_levels\",\n            {\"users\": {self.test_user: 100}},\n            tok=self.admin_user_tok,\n        )\n\n        # They can now delete the alias.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )\n\n\nclass CanonicalAliasTestCase(unittest.HomeserverTestCase):\n    \"\"\"Test modifications of the canonical alias when delete aliases.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.handler = hs.get_directory_handler()\n        self.state_handler = hs.get_state_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = self._add_alias(self.test_alias)\n\n    def _add_alias(self, alias: str) -> RoomAlias:\n        \"\"\"Add an alias to the test room.\"\"\"\n        room_alias = RoomAlias.from_string(alias)\n\n        # Create a new alias to this room.\n        self.get_success(\n            self.store.create_room_alias_association(\n                room_alias, self.room_id, [\"test\"], self.admin_user\n            )\n        )\n        return room_alias\n\n    def _set_canonical_alias(self, content):\n        \"\"\"Configure the canonical alias state on the room.\"\"\"\n        self.helper.send_state(\n            self.room_id, \"m.room.canonical_alias\", content, tok=self.admin_user_tok,\n        )\n\n    def _get_canonical_alias(self):\n        \"\"\"Get the canonical alias state of the room.\"\"\"\n        return self.get_success(\n            self.state_handler.get_current_state(\n                self.room_id, EventTypes.CanonicalAlias, \"\"\n            )\n        )\n\n    def test_remove_alias(self):\n        \"\"\"Removing an alias that is the canonical alias should remove it there too.\"\"\"\n        # Set this new alias as the canonical alias for this room\n        self._set_canonical_alias(\n            {\"alias\": self.test_alias, \"alt_aliases\": [self.test_alias]}\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(data[\"content\"][\"alt_aliases\"], [self.test_alias])\n\n        # Finally, delete the alias.\n        self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), self.room_alias\n            )\n        )\n\n        data = self._get_canonical_alias()\n        self.assertNotIn(\"alias\", data[\"content\"])\n        self.assertNotIn(\"alt_aliases\", data[\"content\"])\n\n    def test_remove_other_alias(self):\n        \"\"\"Removing an alias listed as in alt_aliases should remove it there too.\"\"\"\n        # Create a second alias.\n        other_test_alias = \"#test2:test\"\n        other_room_alias = self._add_alias(other_test_alias)\n\n        # Set the alias as the canonical alias for this room.\n        self._set_canonical_alias(\n            {\n                \"alias\": self.test_alias,\n                \"alt_aliases\": [self.test_alias, other_test_alias],\n            }\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(\n            data[\"content\"][\"alt_aliases\"], [self.test_alias, other_test_alias]\n        )\n\n        # Delete the second alias.\n        self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), other_room_alias\n            )\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(data[\"content\"][\"alt_aliases\"], [self.test_alias])\n\n\nclass TestCreateAliasACL(unittest.HomeserverTestCase):\n    user_id = \"@test:test\"\n\n    servlets = [directory.register_servlets, room.register_servlets]\n\n    def prepare(self, reactor, clock, hs):\n        # We cheekily override the config to add custom alias creation rules\n        config = {}\n        config[\"alias_creation_rules\"] = [\n            {\"user_id\": \"*\", \"alias\": \"#unofficial_*\", \"action\": \"allow\"}\n        ]\n        config[\"room_list_publication_rules\"] = []\n\n        rd_config = RoomDirectoryConfig()\n        rd_config.read_config(config)\n\n        self.hs.config.is_alias_creation_allowed = rd_config.is_alias_creation_allowed\n\n        return hs\n\n    def test_denied(self):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            b\"directory/room/%23test%3Atest\",\n            ('{\"room_id\":\"%s\"}' % (room_id,)).encode(\"ascii\"),\n        )\n        self.assertEquals(403, channel.code, channel.result)\n\n    def test_allowed(self):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            b\"directory/room/%23unofficial_test%3Atest\",\n            ('{\"room_id\":\"%s\"}' % (room_id,)).encode(\"ascii\"),\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n\nclass TestRoomListSearchDisabled(unittest.HomeserverTestCase):\n    user_id = \"@test:test\"\n\n    servlets = [directory.register_servlets, room.register_servlets]\n\n    def prepare(self, reactor, clock, hs):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\", b\"directory/list/room/%s\" % (room_id.encode(\"ascii\"),), b\"{}\"\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        self.room_list_handler = hs.get_room_list_handler()\n        self.directory_handler = hs.get_directory_handler()\n\n        return hs\n\n    def test_disabling_room_list(self):\n        self.room_list_handler.enable_room_list_search = True\n        self.directory_handler.enable_room_list_search = True\n\n        # Room list is enabled so we should get some results\n        request, channel = self.make_request(\"GET\", b\"publicRooms\")\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(len(channel.json_body[\"chunk\"]) > 0)\n\n        self.room_list_handler.enable_room_list_search = False\n        self.directory_handler.enable_room_list_search = False\n\n        # Room list disabled so we should get no results\n        request, channel = self.make_request(\"GET\", b\"publicRooms\")\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(len(channel.json_body[\"chunk\"]) == 0)\n\n        # Room list disabled so we shouldn't be allowed to publish rooms\n        room_id = self.helper.create_room_as(self.user_id)\n        request, channel = self.make_request(\n            \"PUT\", b\"directory/list/room/%s\" % (room_id.encode(\"ascii\"),), b\"{}\"\n        )\n        self.assertEquals(403, channel.code, channel.result)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom mock import Mock\n\nimport synapse\nimport synapse.api.errors\nfrom synapse.api.constants import EventTypes\nfrom synapse.config.room_directory import RoomDirectoryConfig\nfrom synapse.rest.client.v1 import directory, login, room\nfrom synapse.types import RoomAlias, create_requester\n\nfrom tests import unittest\nfrom tests.test_utils import make_awaitable\n\n\nclass DirectoryTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests the directory service. \"\"\"\n\n    def make_homeserver(self, reactor, clock):\n        self.mock_federation = Mock()\n        self.mock_registry = Mock()\n\n        self.query_handlers = {}\n\n        def register_query_handler(query_type, handler):\n            self.query_handlers[query_type] = handler\n\n        self.mock_registry.register_query_handler = register_query_handler\n\n        hs = self.setup_test_homeserver(\n            http_client=None,\n            resource_for_federation=Mock(),\n            federation_client=self.mock_federation,\n            federation_registry=self.mock_registry,\n        )\n\n        self.handler = hs.get_directory_handler()\n\n        self.store = hs.get_datastore()\n\n        self.my_room = RoomAlias.from_string(\"#my-room:test\")\n        self.your_room = RoomAlias.from_string(\"#your-room:test\")\n        self.remote_room = RoomAlias.from_string(\"#another:remote\")\n\n        return hs\n\n    def test_get_local_association(self):\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.my_room, \"!8765qwer:test\", [\"test\"]\n            )\n        )\n\n        result = self.get_success(self.handler.get_association(self.my_room))\n\n        self.assertEquals({\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\"]}, result)\n\n    def test_get_remote_association(self):\n        self.mock_federation.make_query.return_value = make_awaitable(\n            {\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\", \"remote\"]}\n        )\n\n        result = self.get_success(self.handler.get_association(self.remote_room))\n\n        self.assertEquals(\n            {\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\", \"remote\"]}, result\n        )\n        self.mock_federation.make_query.assert_called_with(\n            destination=\"remote\",\n            query_type=\"directory\",\n            args={\"room_alias\": \"#another:remote\"},\n            retry_on_dns_fail=False,\n            ignore_backoff=True,\n        )\n\n    def test_incoming_fed_query(self):\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.your_room, \"!8765asdf:test\", [\"test\"]\n            )\n        )\n\n        response = self.get_success(\n            self.handler.on_directory_query({\"room_alias\": \"#your-room:test\"})\n        )\n\n        self.assertEquals({\"room_id\": \"!8765asdf:test\", \"servers\": [\"test\"]}, response)\n\n\nclass TestCreateAlias(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.handler = hs.get_directory_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = RoomAlias.from_string(self.test_alias)\n\n        # Create a test user.\n        self.test_user = self.register_user(\"user\", \"pass\", admin=False)\n        self.test_user_tok = self.login(\"user\", \"pass\")\n        self.helper.join(room=self.room_id, user=self.test_user, tok=self.test_user_tok)\n\n    def test_create_alias_joined_room(self):\n        \"\"\"A user can create an alias for a room they're in.\"\"\"\n        self.get_success(\n            self.handler.create_association(\n                create_requester(self.test_user), self.room_alias, self.room_id,\n            )\n        )\n\n    def test_create_alias_other_room(self):\n        \"\"\"A user cannot create an alias for a room they're NOT in.\"\"\"\n        other_room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.get_failure(\n            self.handler.create_association(\n                create_requester(self.test_user), self.room_alias, other_room_id,\n            ),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_create_alias_admin(self):\n        \"\"\"An admin can create an alias for a room they're NOT in.\"\"\"\n        other_room_id = self.helper.create_room_as(\n            self.test_user, tok=self.test_user_tok\n        )\n\n        self.get_success(\n            self.handler.create_association(\n                create_requester(self.admin_user), self.room_alias, other_room_id,\n            )\n        )\n\n\nclass TestDeleteAlias(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.handler = hs.get_directory_handler()\n        self.state_handler = hs.get_state_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = RoomAlias.from_string(self.test_alias)\n\n        # Create a test user.\n        self.test_user = self.register_user(\"user\", \"pass\", admin=False)\n        self.test_user_tok = self.login(\"user\", \"pass\")\n        self.helper.join(room=self.room_id, user=self.test_user, tok=self.test_user_tok)\n\n    def _create_alias(self, user):\n        # Create a new alias to this room.\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.room_alias, self.room_id, [\"test\"], user\n            )\n        )\n\n    def test_delete_alias_not_allowed(self):\n        \"\"\"A user that doesn't meet the expected guidelines cannot delete an alias.\"\"\"\n        self._create_alias(self.admin_user)\n        self.get_failure(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            ),\n            synapse.api.errors.AuthError,\n        )\n\n    def test_delete_alias_creator(self):\n        \"\"\"An alias creator can delete their own alias.\"\"\"\n        # Create an alias from a different user.\n        self._create_alias(self.test_user)\n\n        # Delete the user's alias.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_delete_alias_admin(self):\n        \"\"\"A server admin can delete an alias created by another user.\"\"\"\n        # Create an alias from a different user.\n        self._create_alias(self.test_user)\n\n        # Delete the user's alias as the admin.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_delete_alias_sufficient_power(self):\n        \"\"\"A user with a sufficient power level should be able to delete an alias.\"\"\"\n        self._create_alias(self.admin_user)\n\n        # Increase the user's power level.\n        self.helper.send_state(\n            self.room_id,\n            \"m.room.power_levels\",\n            {\"users\": {self.test_user: 100}},\n            tok=self.admin_user_tok,\n        )\n\n        # They can now delete the alias.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )\n\n\nclass CanonicalAliasTestCase(unittest.HomeserverTestCase):\n    \"\"\"Test modifications of the canonical alias when delete aliases.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.handler = hs.get_directory_handler()\n        self.state_handler = hs.get_state_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = self._add_alias(self.test_alias)\n\n    def _add_alias(self, alias: str) -> RoomAlias:\n        \"\"\"Add an alias to the test room.\"\"\"\n        room_alias = RoomAlias.from_string(alias)\n\n        # Create a new alias to this room.\n        self.get_success(\n            self.store.create_room_alias_association(\n                room_alias, self.room_id, [\"test\"], self.admin_user\n            )\n        )\n        return room_alias\n\n    def _set_canonical_alias(self, content):\n        \"\"\"Configure the canonical alias state on the room.\"\"\"\n        self.helper.send_state(\n            self.room_id, \"m.room.canonical_alias\", content, tok=self.admin_user_tok,\n        )\n\n    def _get_canonical_alias(self):\n        \"\"\"Get the canonical alias state of the room.\"\"\"\n        return self.get_success(\n            self.state_handler.get_current_state(\n                self.room_id, EventTypes.CanonicalAlias, \"\"\n            )\n        )\n\n    def test_remove_alias(self):\n        \"\"\"Removing an alias that is the canonical alias should remove it there too.\"\"\"\n        # Set this new alias as the canonical alias for this room\n        self._set_canonical_alias(\n            {\"alias\": self.test_alias, \"alt_aliases\": [self.test_alias]}\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(data[\"content\"][\"alt_aliases\"], [self.test_alias])\n\n        # Finally, delete the alias.\n        self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), self.room_alias\n            )\n        )\n\n        data = self._get_canonical_alias()\n        self.assertNotIn(\"alias\", data[\"content\"])\n        self.assertNotIn(\"alt_aliases\", data[\"content\"])\n\n    def test_remove_other_alias(self):\n        \"\"\"Removing an alias listed as in alt_aliases should remove it there too.\"\"\"\n        # Create a second alias.\n        other_test_alias = \"#test2:test\"\n        other_room_alias = self._add_alias(other_test_alias)\n\n        # Set the alias as the canonical alias for this room.\n        self._set_canonical_alias(\n            {\n                \"alias\": self.test_alias,\n                \"alt_aliases\": [self.test_alias, other_test_alias],\n            }\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(\n            data[\"content\"][\"alt_aliases\"], [self.test_alias, other_test_alias]\n        )\n\n        # Delete the second alias.\n        self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), other_room_alias\n            )\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(data[\"content\"][\"alt_aliases\"], [self.test_alias])\n\n\nclass TestCreateAliasACL(unittest.HomeserverTestCase):\n    user_id = \"@test:test\"\n\n    servlets = [directory.register_servlets, room.register_servlets]\n\n    def prepare(self, reactor, clock, hs):\n        # We cheekily override the config to add custom alias creation rules\n        config = {}\n        config[\"alias_creation_rules\"] = [\n            {\"user_id\": \"*\", \"alias\": \"#unofficial_*\", \"action\": \"allow\"}\n        ]\n        config[\"room_list_publication_rules\"] = []\n\n        rd_config = RoomDirectoryConfig()\n        rd_config.read_config(config)\n\n        self.hs.config.is_alias_creation_allowed = rd_config.is_alias_creation_allowed\n\n        return hs\n\n    def test_denied(self):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            b\"directory/room/%23test%3Atest\",\n            ('{\"room_id\":\"%s\"}' % (room_id,)).encode(\"ascii\"),\n        )\n        self.assertEquals(403, channel.code, channel.result)\n\n    def test_allowed(self):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            b\"directory/room/%23unofficial_test%3Atest\",\n            ('{\"room_id\":\"%s\"}' % (room_id,)).encode(\"ascii\"),\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n\nclass TestRoomListSearchDisabled(unittest.HomeserverTestCase):\n    user_id = \"@test:test\"\n\n    servlets = [directory.register_servlets, room.register_servlets]\n\n    def prepare(self, reactor, clock, hs):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\", b\"directory/list/room/%s\" % (room_id.encode(\"ascii\"),), b\"{}\"\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        self.room_list_handler = hs.get_room_list_handler()\n        self.directory_handler = hs.get_directory_handler()\n\n        return hs\n\n    def test_disabling_room_list(self):\n        self.room_list_handler.enable_room_list_search = True\n        self.directory_handler.enable_room_list_search = True\n\n        # Room list is enabled so we should get some results\n        request, channel = self.make_request(\"GET\", b\"publicRooms\")\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(len(channel.json_body[\"chunk\"]) > 0)\n\n        self.room_list_handler.enable_room_list_search = False\n        self.directory_handler.enable_room_list_search = False\n\n        # Room list disabled so we should get no results\n        request, channel = self.make_request(\"GET\", b\"publicRooms\")\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(len(channel.json_body[\"chunk\"]) == 0)\n\n        # Room list disabled so we shouldn't be allowed to publish rooms\n        room_id = self.helper.create_room_as(self.user_id)\n        request, channel = self.make_request(\n            \"PUT\", b\"directory/list/room/%s\" % (room_id.encode(\"ascii\"),), b\"{}\"\n        )\n        self.assertEquals(403, channel.code, channel.result)\n", "patch": "@@ -42,7 +42,7 @@ def register_query_handler(query_type, handler):\n         self.mock_registry.register_query_handler = register_query_handler\n \n         hs = self.setup_test_homeserver(\n-            http_client=None,\n+            federation_http_client=None,\n             resource_for_federation=Mock(),\n             federation_client=self.mock_federation,\n             federation_registry=self.mock_registry,", "file_path": "files/2021_2/35", "file_language": "py", "file_name": "tests/handlers/test_directory.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class DirectoryTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests the directory service. \"\"\"\n\n    def make_homeserver(self, reactor, clock):\n        self.mock_federation = Mock()\n        self.mock_registry = Mock()\n\n        self.query_handlers = {}\n\n        def register_query_handler(query_type, handler):\n            self.query_handlers[query_type] = handler\n\n        self.mock_registry.register_query_handler = register_query_handler\n\n        hs = self.setup_test_homeserver(\n            http_client=None,\n            resource_for_federation=Mock(),\n            federation_client=self.mock_federation,\n            federation_registry=self.mock_registry,\n        )\n\n        self.handler = hs.get_directory_handler()\n\n        self.store = hs.get_datastore()\n\n        self.my_room = RoomAlias.from_string(\"#my-room:test\")\n        self.your_room = RoomAlias.from_string(\"#your-room:test\")\n        self.remote_room = RoomAlias.from_string(\"#another:remote\")\n\n        return hs\n\n    def test_get_local_association(self):\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.my_room, \"!8765qwer:test\", [\"test\"]\n            )\n        )\n\n        result = self.get_success(self.handler.get_association(self.my_room))\n\n        self.assertEquals({\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\"]}, result)\n\n    def test_get_remote_association(self):\n        self.mock_federation.make_query.return_value = make_awaitable(\n            {\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\", \"remote\"]}\n        )\n\n        result = self.get_success(self.handler.get_association(self.remote_room))\n\n        self.assertEquals(\n            {\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\", \"remote\"]}, result\n        )\n        self.mock_federation.make_query.assert_called_with(\n            destination=\"remote\",\n            query_type=\"directory\",\n            args={\"room_alias\": \"#another:remote\"},\n            retry_on_dns_fail=False,\n            ignore_backoff=True,\n        )\n\n    def test_incoming_fed_query(self):\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.your_room, \"!8765asdf:test\", [\"test\"]\n            )\n        )\n\n        response = self.get_success(\n            self.handler.on_directory_query({\"room_alias\": \"#your-room:test\"})\n        )\n\n        self.assertEquals({\"room_id\": \"!8765asdf:test\", \"servers\": [\"test\"]}, response)", "target": 0}, {"function": "class TestCreateAlias(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.handler = hs.get_directory_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = RoomAlias.from_string(self.test_alias)\n\n        # Create a test user.\n        self.test_user = self.register_user(\"user\", \"pass\", admin=False)\n        self.test_user_tok = self.login(\"user\", \"pass\")\n        self.helper.join(room=self.room_id, user=self.test_user, tok=self.test_user_tok)\n\n    def test_create_alias_joined_room(self):\n        \"\"\"A user can create an alias for a room they're in.\"\"\"\n        self.get_success(\n            self.handler.create_association(\n                create_requester(self.test_user), self.room_alias, self.room_id,\n            )\n        )\n\n    def test_create_alias_other_room(self):\n        \"\"\"A user cannot create an alias for a room they're NOT in.\"\"\"\n        other_room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.get_failure(\n            self.handler.create_association(\n                create_requester(self.test_user), self.room_alias, other_room_id,\n            ),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_create_alias_admin(self):\n        \"\"\"An admin can create an alias for a room they're NOT in.\"\"\"\n        other_room_id = self.helper.create_room_as(\n            self.test_user, tok=self.test_user_tok\n        )\n\n        self.get_success(\n            self.handler.create_association(\n                create_requester(self.admin_user), self.room_alias, other_room_id,\n            )\n        )", "target": 0}, {"function": "class TestDeleteAlias(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.handler = hs.get_directory_handler()\n        self.state_handler = hs.get_state_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = RoomAlias.from_string(self.test_alias)\n\n        # Create a test user.\n        self.test_user = self.register_user(\"user\", \"pass\", admin=False)\n        self.test_user_tok = self.login(\"user\", \"pass\")\n        self.helper.join(room=self.room_id, user=self.test_user, tok=self.test_user_tok)\n\n    def _create_alias(self, user):\n        # Create a new alias to this room.\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.room_alias, self.room_id, [\"test\"], user\n            )\n        )\n\n    def test_delete_alias_not_allowed(self):\n        \"\"\"A user that doesn't meet the expected guidelines cannot delete an alias.\"\"\"\n        self._create_alias(self.admin_user)\n        self.get_failure(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            ),\n            synapse.api.errors.AuthError,\n        )\n\n    def test_delete_alias_creator(self):\n        \"\"\"An alias creator can delete their own alias.\"\"\"\n        # Create an alias from a different user.\n        self._create_alias(self.test_user)\n\n        # Delete the user's alias.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_delete_alias_admin(self):\n        \"\"\"A server admin can delete an alias created by another user.\"\"\"\n        # Create an alias from a different user.\n        self._create_alias(self.test_user)\n\n        # Delete the user's alias as the admin.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_delete_alias_sufficient_power(self):\n        \"\"\"A user with a sufficient power level should be able to delete an alias.\"\"\"\n        self._create_alias(self.admin_user)\n\n        # Increase the user's power level.\n        self.helper.send_state(\n            self.room_id,\n            \"m.room.power_levels\",\n            {\"users\": {self.test_user: 100}},\n            tok=self.admin_user_tok,\n        )\n\n        # They can now delete the alias.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )", "target": 0}, {"function": "class CanonicalAliasTestCase(unittest.HomeserverTestCase):\n    \"\"\"Test modifications of the canonical alias when delete aliases.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.handler = hs.get_directory_handler()\n        self.state_handler = hs.get_state_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = self._add_alias(self.test_alias)\n\n    def _add_alias(self, alias: str) -> RoomAlias:\n        \"\"\"Add an alias to the test room.\"\"\"\n        room_alias = RoomAlias.from_string(alias)\n\n        # Create a new alias to this room.\n        self.get_success(\n            self.store.create_room_alias_association(\n                room_alias, self.room_id, [\"test\"], self.admin_user\n            )\n        )\n        return room_alias\n\n    def _set_canonical_alias(self, content):\n        \"\"\"Configure the canonical alias state on the room.\"\"\"\n        self.helper.send_state(\n            self.room_id, \"m.room.canonical_alias\", content, tok=self.admin_user_tok,\n        )\n\n    def _get_canonical_alias(self):\n        \"\"\"Get the canonical alias state of the room.\"\"\"\n        return self.get_success(\n            self.state_handler.get_current_state(\n                self.room_id, EventTypes.CanonicalAlias, \"\"\n            )\n        )\n\n    def test_remove_alias(self):\n        \"\"\"Removing an alias that is the canonical alias should remove it there too.\"\"\"\n        # Set this new alias as the canonical alias for this room\n        self._set_canonical_alias(\n            {\"alias\": self.test_alias, \"alt_aliases\": [self.test_alias]}\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(data[\"content\"][\"alt_aliases\"], [self.test_alias])\n\n        # Finally, delete the alias.\n        self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), self.room_alias\n            )\n        )\n\n        data = self._get_canonical_alias()\n        self.assertNotIn(\"alias\", data[\"content\"])\n        self.assertNotIn(\"alt_aliases\", data[\"content\"])\n\n    def test_remove_other_alias(self):\n        \"\"\"Removing an alias listed as in alt_aliases should remove it there too.\"\"\"\n        # Create a second alias.\n        other_test_alias = \"#test2:test\"\n        other_room_alias = self._add_alias(other_test_alias)\n\n        # Set the alias as the canonical alias for this room.\n        self._set_canonical_alias(\n            {\n                \"alias\": self.test_alias,\n                \"alt_aliases\": [self.test_alias, other_test_alias],\n            }\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(\n            data[\"content\"][\"alt_aliases\"], [self.test_alias, other_test_alias]\n        )\n\n        # Delete the second alias.\n        self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), other_room_alias\n            )\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(data[\"content\"][\"alt_aliases\"], [self.test_alias])", "target": 0}, {"function": "class TestCreateAliasACL(unittest.HomeserverTestCase):\n    user_id = \"@test:test\"\n\n    servlets = [directory.register_servlets, room.register_servlets]\n\n    def prepare(self, reactor, clock, hs):\n        # We cheekily override the config to add custom alias creation rules\n        config = {}\n        config[\"alias_creation_rules\"] = [\n            {\"user_id\": \"*\", \"alias\": \"#unofficial_*\", \"action\": \"allow\"}\n        ]\n        config[\"room_list_publication_rules\"] = []\n\n        rd_config = RoomDirectoryConfig()\n        rd_config.read_config(config)\n\n        self.hs.config.is_alias_creation_allowed = rd_config.is_alias_creation_allowed\n\n        return hs\n\n    def test_denied(self):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            b\"directory/room/%23test%3Atest\",\n            ('{\"room_id\":\"%s\"}' % (room_id,)).encode(\"ascii\"),\n        )\n        self.assertEquals(403, channel.code, channel.result)\n\n    def test_allowed(self):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            b\"directory/room/%23unofficial_test%3Atest\",\n            ('{\"room_id\":\"%s\"}' % (room_id,)).encode(\"ascii\"),\n        )\n        self.assertEquals(200, channel.code, channel.result)", "target": 0}, {"function": "class TestRoomListSearchDisabled(unittest.HomeserverTestCase):\n    user_id = \"@test:test\"\n\n    servlets = [directory.register_servlets, room.register_servlets]\n\n    def prepare(self, reactor, clock, hs):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\", b\"directory/list/room/%s\" % (room_id.encode(\"ascii\"),), b\"{}\"\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        self.room_list_handler = hs.get_room_list_handler()\n        self.directory_handler = hs.get_directory_handler()\n\n        return hs\n\n    def test_disabling_room_list(self):\n        self.room_list_handler.enable_room_list_search = True\n        self.directory_handler.enable_room_list_search = True\n\n        # Room list is enabled so we should get some results\n        request, channel = self.make_request(\"GET\", b\"publicRooms\")\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(len(channel.json_body[\"chunk\"]) > 0)\n\n        self.room_list_handler.enable_room_list_search = False\n        self.directory_handler.enable_room_list_search = False\n\n        # Room list disabled so we should get no results\n        request, channel = self.make_request(\"GET\", b\"publicRooms\")\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(len(channel.json_body[\"chunk\"]) == 0)\n\n        # Room list disabled so we shouldn't be allowed to publish rooms\n        room_id = self.helper.create_room_as(self.user_id)\n        request, channel = self.make_request(\n            \"PUT\", b\"directory/list/room/%s\" % (room_id.encode(\"ascii\"),), b\"{}\"\n        )\n        self.assertEquals(403, channel.code, channel.result)", "target": 0}], "function_after": [{"function": "class DirectoryTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests the directory service. \"\"\"\n\n    def make_homeserver(self, reactor, clock):\n        self.mock_federation = Mock()\n        self.mock_registry = Mock()\n\n        self.query_handlers = {}\n\n        def register_query_handler(query_type, handler):\n            self.query_handlers[query_type] = handler\n\n        self.mock_registry.register_query_handler = register_query_handler\n\n        hs = self.setup_test_homeserver(\n            federation_http_client=None,\n            resource_for_federation=Mock(),\n            federation_client=self.mock_federation,\n            federation_registry=self.mock_registry,\n        )\n\n        self.handler = hs.get_directory_handler()\n\n        self.store = hs.get_datastore()\n\n        self.my_room = RoomAlias.from_string(\"#my-room:test\")\n        self.your_room = RoomAlias.from_string(\"#your-room:test\")\n        self.remote_room = RoomAlias.from_string(\"#another:remote\")\n\n        return hs\n\n    def test_get_local_association(self):\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.my_room, \"!8765qwer:test\", [\"test\"]\n            )\n        )\n\n        result = self.get_success(self.handler.get_association(self.my_room))\n\n        self.assertEquals({\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\"]}, result)\n\n    def test_get_remote_association(self):\n        self.mock_federation.make_query.return_value = make_awaitable(\n            {\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\", \"remote\"]}\n        )\n\n        result = self.get_success(self.handler.get_association(self.remote_room))\n\n        self.assertEquals(\n            {\"room_id\": \"!8765qwer:test\", \"servers\": [\"test\", \"remote\"]}, result\n        )\n        self.mock_federation.make_query.assert_called_with(\n            destination=\"remote\",\n            query_type=\"directory\",\n            args={\"room_alias\": \"#another:remote\"},\n            retry_on_dns_fail=False,\n            ignore_backoff=True,\n        )\n\n    def test_incoming_fed_query(self):\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.your_room, \"!8765asdf:test\", [\"test\"]\n            )\n        )\n\n        response = self.get_success(\n            self.handler.on_directory_query({\"room_alias\": \"#your-room:test\"})\n        )\n\n        self.assertEquals({\"room_id\": \"!8765asdf:test\", \"servers\": [\"test\"]}, response)", "target": 0}, {"function": "class TestCreateAlias(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.handler = hs.get_directory_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = RoomAlias.from_string(self.test_alias)\n\n        # Create a test user.\n        self.test_user = self.register_user(\"user\", \"pass\", admin=False)\n        self.test_user_tok = self.login(\"user\", \"pass\")\n        self.helper.join(room=self.room_id, user=self.test_user, tok=self.test_user_tok)\n\n    def test_create_alias_joined_room(self):\n        \"\"\"A user can create an alias for a room they're in.\"\"\"\n        self.get_success(\n            self.handler.create_association(\n                create_requester(self.test_user), self.room_alias, self.room_id,\n            )\n        )\n\n    def test_create_alias_other_room(self):\n        \"\"\"A user cannot create an alias for a room they're NOT in.\"\"\"\n        other_room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.get_failure(\n            self.handler.create_association(\n                create_requester(self.test_user), self.room_alias, other_room_id,\n            ),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_create_alias_admin(self):\n        \"\"\"An admin can create an alias for a room they're NOT in.\"\"\"\n        other_room_id = self.helper.create_room_as(\n            self.test_user, tok=self.test_user_tok\n        )\n\n        self.get_success(\n            self.handler.create_association(\n                create_requester(self.admin_user), self.room_alias, other_room_id,\n            )\n        )", "target": 0}, {"function": "class TestDeleteAlias(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.handler = hs.get_directory_handler()\n        self.state_handler = hs.get_state_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = RoomAlias.from_string(self.test_alias)\n\n        # Create a test user.\n        self.test_user = self.register_user(\"user\", \"pass\", admin=False)\n        self.test_user_tok = self.login(\"user\", \"pass\")\n        self.helper.join(room=self.room_id, user=self.test_user, tok=self.test_user_tok)\n\n    def _create_alias(self, user):\n        # Create a new alias to this room.\n        self.get_success(\n            self.store.create_room_alias_association(\n                self.room_alias, self.room_id, [\"test\"], user\n            )\n        )\n\n    def test_delete_alias_not_allowed(self):\n        \"\"\"A user that doesn't meet the expected guidelines cannot delete an alias.\"\"\"\n        self._create_alias(self.admin_user)\n        self.get_failure(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            ),\n            synapse.api.errors.AuthError,\n        )\n\n    def test_delete_alias_creator(self):\n        \"\"\"An alias creator can delete their own alias.\"\"\"\n        # Create an alias from a different user.\n        self._create_alias(self.test_user)\n\n        # Delete the user's alias.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_delete_alias_admin(self):\n        \"\"\"A server admin can delete an alias created by another user.\"\"\"\n        # Create an alias from a different user.\n        self._create_alias(self.test_user)\n\n        # Delete the user's alias as the admin.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )\n\n    def test_delete_alias_sufficient_power(self):\n        \"\"\"A user with a sufficient power level should be able to delete an alias.\"\"\"\n        self._create_alias(self.admin_user)\n\n        # Increase the user's power level.\n        self.helper.send_state(\n            self.room_id,\n            \"m.room.power_levels\",\n            {\"users\": {self.test_user: 100}},\n            tok=self.admin_user_tok,\n        )\n\n        # They can now delete the alias.\n        result = self.get_success(\n            self.handler.delete_association(\n                create_requester(self.test_user), self.room_alias\n            )\n        )\n        self.assertEquals(self.room_id, result)\n\n        # Confirm the alias is gone.\n        self.get_failure(\n            self.handler.get_association(self.room_alias),\n            synapse.api.errors.SynapseError,\n        )", "target": 0}, {"function": "class CanonicalAliasTestCase(unittest.HomeserverTestCase):\n    \"\"\"Test modifications of the canonical alias when delete aliases.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n        directory.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.handler = hs.get_directory_handler()\n        self.state_handler = hs.get_state_handler()\n\n        # Create user\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        # Create a test room\n        self.room_id = self.helper.create_room_as(\n            self.admin_user, tok=self.admin_user_tok\n        )\n\n        self.test_alias = \"#test:test\"\n        self.room_alias = self._add_alias(self.test_alias)\n\n    def _add_alias(self, alias: str) -> RoomAlias:\n        \"\"\"Add an alias to the test room.\"\"\"\n        room_alias = RoomAlias.from_string(alias)\n\n        # Create a new alias to this room.\n        self.get_success(\n            self.store.create_room_alias_association(\n                room_alias, self.room_id, [\"test\"], self.admin_user\n            )\n        )\n        return room_alias\n\n    def _set_canonical_alias(self, content):\n        \"\"\"Configure the canonical alias state on the room.\"\"\"\n        self.helper.send_state(\n            self.room_id, \"m.room.canonical_alias\", content, tok=self.admin_user_tok,\n        )\n\n    def _get_canonical_alias(self):\n        \"\"\"Get the canonical alias state of the room.\"\"\"\n        return self.get_success(\n            self.state_handler.get_current_state(\n                self.room_id, EventTypes.CanonicalAlias, \"\"\n            )\n        )\n\n    def test_remove_alias(self):\n        \"\"\"Removing an alias that is the canonical alias should remove it there too.\"\"\"\n        # Set this new alias as the canonical alias for this room\n        self._set_canonical_alias(\n            {\"alias\": self.test_alias, \"alt_aliases\": [self.test_alias]}\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(data[\"content\"][\"alt_aliases\"], [self.test_alias])\n\n        # Finally, delete the alias.\n        self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), self.room_alias\n            )\n        )\n\n        data = self._get_canonical_alias()\n        self.assertNotIn(\"alias\", data[\"content\"])\n        self.assertNotIn(\"alt_aliases\", data[\"content\"])\n\n    def test_remove_other_alias(self):\n        \"\"\"Removing an alias listed as in alt_aliases should remove it there too.\"\"\"\n        # Create a second alias.\n        other_test_alias = \"#test2:test\"\n        other_room_alias = self._add_alias(other_test_alias)\n\n        # Set the alias as the canonical alias for this room.\n        self._set_canonical_alias(\n            {\n                \"alias\": self.test_alias,\n                \"alt_aliases\": [self.test_alias, other_test_alias],\n            }\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(\n            data[\"content\"][\"alt_aliases\"], [self.test_alias, other_test_alias]\n        )\n\n        # Delete the second alias.\n        self.get_success(\n            self.handler.delete_association(\n                create_requester(self.admin_user), other_room_alias\n            )\n        )\n\n        data = self._get_canonical_alias()\n        self.assertEqual(data[\"content\"][\"alias\"], self.test_alias)\n        self.assertEqual(data[\"content\"][\"alt_aliases\"], [self.test_alias])", "target": 0}, {"function": "class TestCreateAliasACL(unittest.HomeserverTestCase):\n    user_id = \"@test:test\"\n\n    servlets = [directory.register_servlets, room.register_servlets]\n\n    def prepare(self, reactor, clock, hs):\n        # We cheekily override the config to add custom alias creation rules\n        config = {}\n        config[\"alias_creation_rules\"] = [\n            {\"user_id\": \"*\", \"alias\": \"#unofficial_*\", \"action\": \"allow\"}\n        ]\n        config[\"room_list_publication_rules\"] = []\n\n        rd_config = RoomDirectoryConfig()\n        rd_config.read_config(config)\n\n        self.hs.config.is_alias_creation_allowed = rd_config.is_alias_creation_allowed\n\n        return hs\n\n    def test_denied(self):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            b\"directory/room/%23test%3Atest\",\n            ('{\"room_id\":\"%s\"}' % (room_id,)).encode(\"ascii\"),\n        )\n        self.assertEquals(403, channel.code, channel.result)\n\n    def test_allowed(self):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            b\"directory/room/%23unofficial_test%3Atest\",\n            ('{\"room_id\":\"%s\"}' % (room_id,)).encode(\"ascii\"),\n        )\n        self.assertEquals(200, channel.code, channel.result)", "target": 0}, {"function": "class TestRoomListSearchDisabled(unittest.HomeserverTestCase):\n    user_id = \"@test:test\"\n\n    servlets = [directory.register_servlets, room.register_servlets]\n\n    def prepare(self, reactor, clock, hs):\n        room_id = self.helper.create_room_as(self.user_id)\n\n        request, channel = self.make_request(\n            \"PUT\", b\"directory/list/room/%s\" % (room_id.encode(\"ascii\"),), b\"{}\"\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        self.room_list_handler = hs.get_room_list_handler()\n        self.directory_handler = hs.get_directory_handler()\n\n        return hs\n\n    def test_disabling_room_list(self):\n        self.room_list_handler.enable_room_list_search = True\n        self.directory_handler.enable_room_list_search = True\n\n        # Room list is enabled so we should get some results\n        request, channel = self.make_request(\"GET\", b\"publicRooms\")\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(len(channel.json_body[\"chunk\"]) > 0)\n\n        self.room_list_handler.enable_room_list_search = False\n        self.directory_handler.enable_room_list_search = False\n\n        # Room list disabled so we should get no results\n        request, channel = self.make_request(\"GET\", b\"publicRooms\")\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(len(channel.json_body[\"chunk\"]) == 0)\n\n        # Room list disabled so we shouldn't be allowed to publish rooms\n        room_id = self.helper.create_room_as(self.user_id)\n        request, channel = self.make_request(\n            \"PUT\", b\"directory/list/room/%s\" % (room_id.encode(\"ascii\"),), b\"{}\"\n        )\n        self.assertEquals(403, channel.code, channel.result)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fhandlers%2Ftest_federation.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nfrom unittest import TestCase\n\nfrom synapse.api.constants import EventTypes\nfrom synapse.api.errors import AuthError, Codes, SynapseError\nfrom synapse.api.room_versions import RoomVersions\nfrom synapse.events import EventBase\nfrom synapse.federation.federation_base import event_from_pdu_json\nfrom synapse.logging.context import LoggingContext, run_in_background\nfrom synapse.rest import admin\nfrom synapse.rest.client.v1 import login, room\n\nfrom tests import unittest\n\nlogger = logging.getLogger(__name__)\n\n\nclass FederationTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(federation_http_client=None)\n        self.handler = hs.get_federation_handler()\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_exchange_revoked_invite(self):\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n\n        # Send a 3PID invite event with an empty body so it's considered as a revoked one.\n        invite_token = \"sometoken\"\n        self.helper.send_state(\n            room_id=room_id,\n            event_type=EventTypes.ThirdPartyInvite,\n            state_key=invite_token,\n            body={},\n            tok=tok,\n        )\n\n        d = self.handler.on_exchange_third_party_invite_request(\n            event_dict={\n                \"type\": EventTypes.Member,\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": \"@someone:example.org\",\n                \"content\": {\n                    \"membership\": \"invite\",\n                    \"third_party_invite\": {\n                        \"display_name\": \"alice\",\n                        \"signed\": {\n                            \"mxid\": \"@alice:localhost\",\n                            \"token\": invite_token,\n                            \"signatures\": {\n                                \"magic.forest\": {\n                                    \"ed25519:3\": \"fQpGIW1Snz+pwLZu6sTy2aHy/DYWWTspTJRPyNp0PKkymfIsNffysMl6ObMMFdIJhk6g6pwlIqZ54rxo8SLmAg\"\n                                }\n                            },\n                        },\n                    },\n                },\n            },\n        )\n\n        failure = self.get_failure(d, AuthError).value\n\n        self.assertEqual(failure.code, 403, failure)\n        self.assertEqual(failure.errcode, Codes.FORBIDDEN, failure)\n        self.assertEqual(failure.msg, \"You are not invited to this room.\")\n\n    def test_rejected_message_event_state(self):\n        \"\"\"\n        Check that we store the state group correctly for rejected non-state events.\n\n        Regression test for #6289.\n        \"\"\"\n        OTHER_SERVER = \"otherserver\"\n        OTHER_USER = \"@otheruser:\" + OTHER_SERVER\n\n        # create the room\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n        room_version = self.get_success(self.store.get_room_version(room_id))\n\n        # pretend that another server has joined\n        join_event = self._build_and_send_join_event(OTHER_SERVER, OTHER_USER, room_id)\n\n        # check the state group\n        sg = self.successResultOf(\n            self.store._get_state_group_for_event(join_event.event_id)\n        )\n\n        # build and send an event which will be rejected\n        ev = event_from_pdu_json(\n            {\n                \"type\": EventTypes.Message,\n                \"content\": {},\n                \"room_id\": room_id,\n                \"sender\": \"@yetanotheruser:\" + OTHER_SERVER,\n                \"depth\": join_event[\"depth\"] + 1,\n                \"prev_events\": [join_event.event_id],\n                \"auth_events\": [],\n                \"origin_server_ts\": self.clock.time_msec(),\n            },\n            room_version,\n        )\n\n        with LoggingContext(request=\"send_rejected\"):\n            d = run_in_background(self.handler.on_receive_pdu, OTHER_SERVER, ev)\n        self.get_success(d)\n\n        # that should have been rejected\n        e = self.get_success(self.store.get_event(ev.event_id, allow_rejected=True))\n        self.assertIsNotNone(e.rejected_reason)\n\n        # ... and the state group should be the same as before\n        sg2 = self.successResultOf(self.store._get_state_group_for_event(ev.event_id))\n\n        self.assertEqual(sg, sg2)\n\n    def test_rejected_state_event_state(self):\n        \"\"\"\n        Check that we store the state group correctly for rejected state events.\n\n        Regression test for #6289.\n        \"\"\"\n        OTHER_SERVER = \"otherserver\"\n        OTHER_USER = \"@otheruser:\" + OTHER_SERVER\n\n        # create the room\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n        room_version = self.get_success(self.store.get_room_version(room_id))\n\n        # pretend that another server has joined\n        join_event = self._build_and_send_join_event(OTHER_SERVER, OTHER_USER, room_id)\n\n        # check the state group\n        sg = self.successResultOf(\n            self.store._get_state_group_for_event(join_event.event_id)\n        )\n\n        # build and send an event which will be rejected\n        ev = event_from_pdu_json(\n            {\n                \"type\": \"org.matrix.test\",\n                \"state_key\": \"test_key\",\n                \"content\": {},\n                \"room_id\": room_id,\n                \"sender\": \"@yetanotheruser:\" + OTHER_SERVER,\n                \"depth\": join_event[\"depth\"] + 1,\n                \"prev_events\": [join_event.event_id],\n                \"auth_events\": [],\n                \"origin_server_ts\": self.clock.time_msec(),\n            },\n            room_version,\n        )\n\n        with LoggingContext(request=\"send_rejected\"):\n            d = run_in_background(self.handler.on_receive_pdu, OTHER_SERVER, ev)\n        self.get_success(d)\n\n        # that should have been rejected\n        e = self.get_success(self.store.get_event(ev.event_id, allow_rejected=True))\n        self.assertIsNotNone(e.rejected_reason)\n\n        # ... and the state group should be the same as before\n        sg2 = self.successResultOf(self.store._get_state_group_for_event(ev.event_id))\n\n        self.assertEqual(sg, sg2)\n\n    def _build_and_send_join_event(self, other_server, other_user, room_id):\n        join_event = self.get_success(\n            self.handler.on_make_join_request(other_server, room_id, other_user)\n        )\n        # the auth code requires that a signature exists, but doesn't check that\n        # signature... go figure.\n        join_event.signatures[other_server] = {\"x\": \"y\"}\n        with LoggingContext(request=\"send_join\"):\n            d = run_in_background(\n                self.handler.on_send_join_request, other_server, join_event\n            )\n        self.get_success(d)\n\n        # sanity-check: the room should show that the new user is a member\n        r = self.get_success(self.store.get_current_state_ids(room_id))\n        self.assertEqual(r[(EventTypes.Member, other_user)], join_event.event_id)\n\n        return join_event\n\n\nclass EventFromPduTestCase(TestCase):\n    def test_valid_json(self):\n        \"\"\"Valid JSON should be turned into an event.\"\"\"\n        ev = event_from_pdu_json(\n            {\n                \"type\": EventTypes.Message,\n                \"content\": {\"bool\": True, \"null\": None, \"int\": 1, \"str\": \"foobar\"},\n                \"room_id\": \"!room:test\",\n                \"sender\": \"@user:test\",\n                \"depth\": 1,\n                \"prev_events\": [],\n                \"auth_events\": [],\n                \"origin_server_ts\": 1234,\n            },\n            RoomVersions.V6,\n        )\n\n        self.assertIsInstance(ev, EventBase)\n\n    def test_invalid_numbers(self):\n        \"\"\"Invalid values for an integer should be rejected, all floats should be rejected.\"\"\"\n        for value in [\n            -(2 ** 53),\n            2 ** 53,\n            1.0,\n            float(\"inf\"),\n            float(\"-inf\"),\n            float(\"nan\"),\n        ]:\n            with self.assertRaises(SynapseError):\n                event_from_pdu_json(\n                    {\n                        \"type\": EventTypes.Message,\n                        \"content\": {\"foo\": value},\n                        \"room_id\": \"!room:test\",\n                        \"sender\": \"@user:test\",\n                        \"depth\": 1,\n                        \"prev_events\": [],\n                        \"auth_events\": [],\n                        \"origin_server_ts\": 1234,\n                    },\n                    RoomVersions.V6,\n                )\n\n    def test_invalid_nested(self):\n        \"\"\"List and dictionaries are recursively searched.\"\"\"\n        with self.assertRaises(SynapseError):\n            event_from_pdu_json(\n                {\n                    \"type\": EventTypes.Message,\n                    \"content\": {\"foo\": [{\"bar\": 2 ** 56}]},\n                    \"room_id\": \"!room:test\",\n                    \"sender\": \"@user:test\",\n                    \"depth\": 1,\n                    \"prev_events\": [],\n                    \"auth_events\": [],\n                    \"origin_server_ts\": 1234,\n                },\n                RoomVersions.V6,\n            )\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nfrom unittest import TestCase\n\nfrom synapse.api.constants import EventTypes\nfrom synapse.api.errors import AuthError, Codes, SynapseError\nfrom synapse.api.room_versions import RoomVersions\nfrom synapse.events import EventBase\nfrom synapse.federation.federation_base import event_from_pdu_json\nfrom synapse.logging.context import LoggingContext, run_in_background\nfrom synapse.rest import admin\nfrom synapse.rest.client.v1 import login, room\n\nfrom tests import unittest\n\nlogger = logging.getLogger(__name__)\n\n\nclass FederationTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(http_client=None)\n        self.handler = hs.get_federation_handler()\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_exchange_revoked_invite(self):\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n\n        # Send a 3PID invite event with an empty body so it's considered as a revoked one.\n        invite_token = \"sometoken\"\n        self.helper.send_state(\n            room_id=room_id,\n            event_type=EventTypes.ThirdPartyInvite,\n            state_key=invite_token,\n            body={},\n            tok=tok,\n        )\n\n        d = self.handler.on_exchange_third_party_invite_request(\n            event_dict={\n                \"type\": EventTypes.Member,\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": \"@someone:example.org\",\n                \"content\": {\n                    \"membership\": \"invite\",\n                    \"third_party_invite\": {\n                        \"display_name\": \"alice\",\n                        \"signed\": {\n                            \"mxid\": \"@alice:localhost\",\n                            \"token\": invite_token,\n                            \"signatures\": {\n                                \"magic.forest\": {\n                                    \"ed25519:3\": \"fQpGIW1Snz+pwLZu6sTy2aHy/DYWWTspTJRPyNp0PKkymfIsNffysMl6ObMMFdIJhk6g6pwlIqZ54rxo8SLmAg\"\n                                }\n                            },\n                        },\n                    },\n                },\n            },\n        )\n\n        failure = self.get_failure(d, AuthError).value\n\n        self.assertEqual(failure.code, 403, failure)\n        self.assertEqual(failure.errcode, Codes.FORBIDDEN, failure)\n        self.assertEqual(failure.msg, \"You are not invited to this room.\")\n\n    def test_rejected_message_event_state(self):\n        \"\"\"\n        Check that we store the state group correctly for rejected non-state events.\n\n        Regression test for #6289.\n        \"\"\"\n        OTHER_SERVER = \"otherserver\"\n        OTHER_USER = \"@otheruser:\" + OTHER_SERVER\n\n        # create the room\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n        room_version = self.get_success(self.store.get_room_version(room_id))\n\n        # pretend that another server has joined\n        join_event = self._build_and_send_join_event(OTHER_SERVER, OTHER_USER, room_id)\n\n        # check the state group\n        sg = self.successResultOf(\n            self.store._get_state_group_for_event(join_event.event_id)\n        )\n\n        # build and send an event which will be rejected\n        ev = event_from_pdu_json(\n            {\n                \"type\": EventTypes.Message,\n                \"content\": {},\n                \"room_id\": room_id,\n                \"sender\": \"@yetanotheruser:\" + OTHER_SERVER,\n                \"depth\": join_event[\"depth\"] + 1,\n                \"prev_events\": [join_event.event_id],\n                \"auth_events\": [],\n                \"origin_server_ts\": self.clock.time_msec(),\n            },\n            room_version,\n        )\n\n        with LoggingContext(request=\"send_rejected\"):\n            d = run_in_background(self.handler.on_receive_pdu, OTHER_SERVER, ev)\n        self.get_success(d)\n\n        # that should have been rejected\n        e = self.get_success(self.store.get_event(ev.event_id, allow_rejected=True))\n        self.assertIsNotNone(e.rejected_reason)\n\n        # ... and the state group should be the same as before\n        sg2 = self.successResultOf(self.store._get_state_group_for_event(ev.event_id))\n\n        self.assertEqual(sg, sg2)\n\n    def test_rejected_state_event_state(self):\n        \"\"\"\n        Check that we store the state group correctly for rejected state events.\n\n        Regression test for #6289.\n        \"\"\"\n        OTHER_SERVER = \"otherserver\"\n        OTHER_USER = \"@otheruser:\" + OTHER_SERVER\n\n        # create the room\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n        room_version = self.get_success(self.store.get_room_version(room_id))\n\n        # pretend that another server has joined\n        join_event = self._build_and_send_join_event(OTHER_SERVER, OTHER_USER, room_id)\n\n        # check the state group\n        sg = self.successResultOf(\n            self.store._get_state_group_for_event(join_event.event_id)\n        )\n\n        # build and send an event which will be rejected\n        ev = event_from_pdu_json(\n            {\n                \"type\": \"org.matrix.test\",\n                \"state_key\": \"test_key\",\n                \"content\": {},\n                \"room_id\": room_id,\n                \"sender\": \"@yetanotheruser:\" + OTHER_SERVER,\n                \"depth\": join_event[\"depth\"] + 1,\n                \"prev_events\": [join_event.event_id],\n                \"auth_events\": [],\n                \"origin_server_ts\": self.clock.time_msec(),\n            },\n            room_version,\n        )\n\n        with LoggingContext(request=\"send_rejected\"):\n            d = run_in_background(self.handler.on_receive_pdu, OTHER_SERVER, ev)\n        self.get_success(d)\n\n        # that should have been rejected\n        e = self.get_success(self.store.get_event(ev.event_id, allow_rejected=True))\n        self.assertIsNotNone(e.rejected_reason)\n\n        # ... and the state group should be the same as before\n        sg2 = self.successResultOf(self.store._get_state_group_for_event(ev.event_id))\n\n        self.assertEqual(sg, sg2)\n\n    def _build_and_send_join_event(self, other_server, other_user, room_id):\n        join_event = self.get_success(\n            self.handler.on_make_join_request(other_server, room_id, other_user)\n        )\n        # the auth code requires that a signature exists, but doesn't check that\n        # signature... go figure.\n        join_event.signatures[other_server] = {\"x\": \"y\"}\n        with LoggingContext(request=\"send_join\"):\n            d = run_in_background(\n                self.handler.on_send_join_request, other_server, join_event\n            )\n        self.get_success(d)\n\n        # sanity-check: the room should show that the new user is a member\n        r = self.get_success(self.store.get_current_state_ids(room_id))\n        self.assertEqual(r[(EventTypes.Member, other_user)], join_event.event_id)\n\n        return join_event\n\n\nclass EventFromPduTestCase(TestCase):\n    def test_valid_json(self):\n        \"\"\"Valid JSON should be turned into an event.\"\"\"\n        ev = event_from_pdu_json(\n            {\n                \"type\": EventTypes.Message,\n                \"content\": {\"bool\": True, \"null\": None, \"int\": 1, \"str\": \"foobar\"},\n                \"room_id\": \"!room:test\",\n                \"sender\": \"@user:test\",\n                \"depth\": 1,\n                \"prev_events\": [],\n                \"auth_events\": [],\n                \"origin_server_ts\": 1234,\n            },\n            RoomVersions.V6,\n        )\n\n        self.assertIsInstance(ev, EventBase)\n\n    def test_invalid_numbers(self):\n        \"\"\"Invalid values for an integer should be rejected, all floats should be rejected.\"\"\"\n        for value in [\n            -(2 ** 53),\n            2 ** 53,\n            1.0,\n            float(\"inf\"),\n            float(\"-inf\"),\n            float(\"nan\"),\n        ]:\n            with self.assertRaises(SynapseError):\n                event_from_pdu_json(\n                    {\n                        \"type\": EventTypes.Message,\n                        \"content\": {\"foo\": value},\n                        \"room_id\": \"!room:test\",\n                        \"sender\": \"@user:test\",\n                        \"depth\": 1,\n                        \"prev_events\": [],\n                        \"auth_events\": [],\n                        \"origin_server_ts\": 1234,\n                    },\n                    RoomVersions.V6,\n                )\n\n    def test_invalid_nested(self):\n        \"\"\"List and dictionaries are recursively searched.\"\"\"\n        with self.assertRaises(SynapseError):\n            event_from_pdu_json(\n                {\n                    \"type\": EventTypes.Message,\n                    \"content\": {\"foo\": [{\"bar\": 2 ** 56}]},\n                    \"room_id\": \"!room:test\",\n                    \"sender\": \"@user:test\",\n                    \"depth\": 1,\n                    \"prev_events\": [],\n                    \"auth_events\": [],\n                    \"origin_server_ts\": 1234,\n                },\n                RoomVersions.V6,\n            )\n", "patch": "@@ -37,7 +37,7 @@ class FederationTestCase(unittest.HomeserverTestCase):\n     ]\n \n     def make_homeserver(self, reactor, clock):\n-        hs = self.setup_test_homeserver(http_client=None)\n+        hs = self.setup_test_homeserver(federation_http_client=None)\n         self.handler = hs.get_federation_handler()\n         self.store = hs.get_datastore()\n         return hs", "file_path": "files/2021_2/36", "file_language": "py", "file_name": "tests/handlers/test_federation.py", "outdated_file_modify": 1, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class FederationTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(http_client=None)\n        self.handler = hs.get_federation_handler()\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_exchange_revoked_invite(self):\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n\n        # Send a 3PID invite event with an empty body so it's considered as a revoked one.\n        invite_token = \"sometoken\"\n        self.helper.send_state(\n            room_id=room_id,\n            event_type=EventTypes.ThirdPartyInvite,\n            state_key=invite_token,\n            body={},\n            tok=tok,\n        )\n\n        d = self.handler.on_exchange_third_party_invite_request(\n            event_dict={\n                \"type\": EventTypes.Member,\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": \"@someone:example.org\",\n                \"content\": {\n                    \"membership\": \"invite\",\n                    \"third_party_invite\": {\n                        \"display_name\": \"alice\",\n                        \"signed\": {\n                            \"mxid\": \"@alice:localhost\",\n                            \"token\": invite_token,\n                            \"signatures\": {\n                                \"magic.forest\": {\n                                    \"ed25519:3\": \"fQpGIW1Snz+pwLZu6sTy2aHy/DYWWTspTJRPyNp0PKkymfIsNffysMl6ObMMFdIJhk6g6pwlIqZ54rxo8SLmAg\"\n                                }\n                            },\n                        },\n                    },\n                },\n            },\n        )\n\n        failure = self.get_failure(d, AuthError).value\n\n        self.assertEqual(failure.code, 403, failure)\n        self.assertEqual(failure.errcode, Codes.FORBIDDEN, failure)\n        self.assertEqual(failure.msg, \"You are not invited to this room.\")\n\n    def test_rejected_message_event_state(self):\n        \"\"\"\n        Check that we store the state group correctly for rejected non-state events.\n\n        Regression test for #6289.\n        \"\"\"\n        OTHER_SERVER = \"otherserver\"\n        OTHER_USER = \"@otheruser:\" + OTHER_SERVER\n\n        # create the room\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n        room_version = self.get_success(self.store.get_room_version(room_id))\n\n        # pretend that another server has joined\n        join_event = self._build_and_send_join_event(OTHER_SERVER, OTHER_USER, room_id)\n\n        # check the state group\n        sg = self.successResultOf(\n            self.store._get_state_group_for_event(join_event.event_id)\n        )\n\n        # build and send an event which will be rejected\n        ev = event_from_pdu_json(\n            {\n                \"type\": EventTypes.Message,\n                \"content\": {},\n                \"room_id\": room_id,\n                \"sender\": \"@yetanotheruser:\" + OTHER_SERVER,\n                \"depth\": join_event[\"depth\"] + 1,\n                \"prev_events\": [join_event.event_id],\n                \"auth_events\": [],\n                \"origin_server_ts\": self.clock.time_msec(),\n            },\n            room_version,\n        )\n\n        with LoggingContext(request=\"send_rejected\"):\n            d = run_in_background(self.handler.on_receive_pdu, OTHER_SERVER, ev)\n        self.get_success(d)\n\n        # that should have been rejected\n        e = self.get_success(self.store.get_event(ev.event_id, allow_rejected=True))\n        self.assertIsNotNone(e.rejected_reason)\n\n        # ... and the state group should be the same as before\n        sg2 = self.successResultOf(self.store._get_state_group_for_event(ev.event_id))\n\n        self.assertEqual(sg, sg2)\n\n    def test_rejected_state_event_state(self):\n        \"\"\"\n        Check that we store the state group correctly for rejected state events.\n\n        Regression test for #6289.\n        \"\"\"\n        OTHER_SERVER = \"otherserver\"\n        OTHER_USER = \"@otheruser:\" + OTHER_SERVER\n\n        # create the room\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n        room_version = self.get_success(self.store.get_room_version(room_id))\n\n        # pretend that another server has joined\n        join_event = self._build_and_send_join_event(OTHER_SERVER, OTHER_USER, room_id)\n\n        # check the state group\n        sg = self.successResultOf(\n            self.store._get_state_group_for_event(join_event.event_id)\n        )\n\n        # build and send an event which will be rejected\n        ev = event_from_pdu_json(\n            {\n                \"type\": \"org.matrix.test\",\n                \"state_key\": \"test_key\",\n                \"content\": {},\n                \"room_id\": room_id,\n                \"sender\": \"@yetanotheruser:\" + OTHER_SERVER,\n                \"depth\": join_event[\"depth\"] + 1,\n                \"prev_events\": [join_event.event_id],\n                \"auth_events\": [],\n                \"origin_server_ts\": self.clock.time_msec(),\n            },\n            room_version,\n        )\n\n        with LoggingContext(request=\"send_rejected\"):\n            d = run_in_background(self.handler.on_receive_pdu, OTHER_SERVER, ev)\n        self.get_success(d)\n\n        # that should have been rejected\n        e = self.get_success(self.store.get_event(ev.event_id, allow_rejected=True))\n        self.assertIsNotNone(e.rejected_reason)\n\n        # ... and the state group should be the same as before\n        sg2 = self.successResultOf(self.store._get_state_group_for_event(ev.event_id))\n\n        self.assertEqual(sg, sg2)\n\n    def _build_and_send_join_event(self, other_server, other_user, room_id):\n        join_event = self.get_success(\n            self.handler.on_make_join_request(other_server, room_id, other_user)\n        )\n        # the auth code requires that a signature exists, but doesn't check that\n        # signature... go figure.\n        join_event.signatures[other_server] = {\"x\": \"y\"}\n        with LoggingContext(request=\"send_join\"):\n            d = run_in_background(\n                self.handler.on_send_join_request, other_server, join_event\n            )\n        self.get_success(d)\n\n        # sanity-check: the room should show that the new user is a member\n        r = self.get_success(self.store.get_current_state_ids(room_id))\n        self.assertEqual(r[(EventTypes.Member, other_user)], join_event.event_id)\n\n        return join_event", "target": 0}, {"function": "class EventFromPduTestCase(TestCase):\n    def test_valid_json(self):\n        \"\"\"Valid JSON should be turned into an event.\"\"\"\n        ev = event_from_pdu_json(\n            {\n                \"type\": EventTypes.Message,\n                \"content\": {\"bool\": True, \"null\": None, \"int\": 1, \"str\": \"foobar\"},\n                \"room_id\": \"!room:test\",\n                \"sender\": \"@user:test\",\n                \"depth\": 1,\n                \"prev_events\": [],\n                \"auth_events\": [],\n                \"origin_server_ts\": 1234,\n            },\n            RoomVersions.V6,\n        )\n\n        self.assertIsInstance(ev, EventBase)\n\n    def test_invalid_numbers(self):\n        \"\"\"Invalid values for an integer should be rejected, all floats should be rejected.\"\"\"\n        for value in [\n            -(2 ** 53),\n            2 ** 53,\n            1.0,\n            float(\"inf\"),\n            float(\"-inf\"),\n            float(\"nan\"),\n        ]:\n            with self.assertRaises(SynapseError):\n                event_from_pdu_json(\n                    {\n                        \"type\": EventTypes.Message,\n                        \"content\": {\"foo\": value},\n                        \"room_id\": \"!room:test\",\n                        \"sender\": \"@user:test\",\n                        \"depth\": 1,\n                        \"prev_events\": [],\n                        \"auth_events\": [],\n                        \"origin_server_ts\": 1234,\n                    },\n                    RoomVersions.V6,\n                )\n\n    def test_invalid_nested(self):\n        \"\"\"List and dictionaries are recursively searched.\"\"\"\n        with self.assertRaises(SynapseError):\n            event_from_pdu_json(\n                {\n                    \"type\": EventTypes.Message,\n                    \"content\": {\"foo\": [{\"bar\": 2 ** 56}]},\n                    \"room_id\": \"!room:test\",\n                    \"sender\": \"@user:test\",\n                    \"depth\": 1,\n                    \"prev_events\": [],\n                    \"auth_events\": [],\n                    \"origin_server_ts\": 1234,\n                },\n                RoomVersions.V6,\n            )", "target": 0}], "function_after": [{"function": "class FederationTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        admin.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(federation_http_client=None)\n        self.handler = hs.get_federation_handler()\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_exchange_revoked_invite(self):\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n\n        # Send a 3PID invite event with an empty body so it's considered as a revoked one.\n        invite_token = \"sometoken\"\n        self.helper.send_state(\n            room_id=room_id,\n            event_type=EventTypes.ThirdPartyInvite,\n            state_key=invite_token,\n            body={},\n            tok=tok,\n        )\n\n        d = self.handler.on_exchange_third_party_invite_request(\n            event_dict={\n                \"type\": EventTypes.Member,\n                \"room_id\": room_id,\n                \"sender\": user_id,\n                \"state_key\": \"@someone:example.org\",\n                \"content\": {\n                    \"membership\": \"invite\",\n                    \"third_party_invite\": {\n                        \"display_name\": \"alice\",\n                        \"signed\": {\n                            \"mxid\": \"@alice:localhost\",\n                            \"token\": invite_token,\n                            \"signatures\": {\n                                \"magic.forest\": {\n                                    \"ed25519:3\": \"fQpGIW1Snz+pwLZu6sTy2aHy/DYWWTspTJRPyNp0PKkymfIsNffysMl6ObMMFdIJhk6g6pwlIqZ54rxo8SLmAg\"\n                                }\n                            },\n                        },\n                    },\n                },\n            },\n        )\n\n        failure = self.get_failure(d, AuthError).value\n\n        self.assertEqual(failure.code, 403, failure)\n        self.assertEqual(failure.errcode, Codes.FORBIDDEN, failure)\n        self.assertEqual(failure.msg, \"You are not invited to this room.\")\n\n    def test_rejected_message_event_state(self):\n        \"\"\"\n        Check that we store the state group correctly for rejected non-state events.\n\n        Regression test for #6289.\n        \"\"\"\n        OTHER_SERVER = \"otherserver\"\n        OTHER_USER = \"@otheruser:\" + OTHER_SERVER\n\n        # create the room\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n        room_version = self.get_success(self.store.get_room_version(room_id))\n\n        # pretend that another server has joined\n        join_event = self._build_and_send_join_event(OTHER_SERVER, OTHER_USER, room_id)\n\n        # check the state group\n        sg = self.successResultOf(\n            self.store._get_state_group_for_event(join_event.event_id)\n        )\n\n        # build and send an event which will be rejected\n        ev = event_from_pdu_json(\n            {\n                \"type\": EventTypes.Message,\n                \"content\": {},\n                \"room_id\": room_id,\n                \"sender\": \"@yetanotheruser:\" + OTHER_SERVER,\n                \"depth\": join_event[\"depth\"] + 1,\n                \"prev_events\": [join_event.event_id],\n                \"auth_events\": [],\n                \"origin_server_ts\": self.clock.time_msec(),\n            },\n            room_version,\n        )\n\n        with LoggingContext(request=\"send_rejected\"):\n            d = run_in_background(self.handler.on_receive_pdu, OTHER_SERVER, ev)\n        self.get_success(d)\n\n        # that should have been rejected\n        e = self.get_success(self.store.get_event(ev.event_id, allow_rejected=True))\n        self.assertIsNotNone(e.rejected_reason)\n\n        # ... and the state group should be the same as before\n        sg2 = self.successResultOf(self.store._get_state_group_for_event(ev.event_id))\n\n        self.assertEqual(sg, sg2)\n\n    def test_rejected_state_event_state(self):\n        \"\"\"\n        Check that we store the state group correctly for rejected state events.\n\n        Regression test for #6289.\n        \"\"\"\n        OTHER_SERVER = \"otherserver\"\n        OTHER_USER = \"@otheruser:\" + OTHER_SERVER\n\n        # create the room\n        user_id = self.register_user(\"kermit\", \"test\")\n        tok = self.login(\"kermit\", \"test\")\n        room_id = self.helper.create_room_as(room_creator=user_id, tok=tok)\n        room_version = self.get_success(self.store.get_room_version(room_id))\n\n        # pretend that another server has joined\n        join_event = self._build_and_send_join_event(OTHER_SERVER, OTHER_USER, room_id)\n\n        # check the state group\n        sg = self.successResultOf(\n            self.store._get_state_group_for_event(join_event.event_id)\n        )\n\n        # build and send an event which will be rejected\n        ev = event_from_pdu_json(\n            {\n                \"type\": \"org.matrix.test\",\n                \"state_key\": \"test_key\",\n                \"content\": {},\n                \"room_id\": room_id,\n                \"sender\": \"@yetanotheruser:\" + OTHER_SERVER,\n                \"depth\": join_event[\"depth\"] + 1,\n                \"prev_events\": [join_event.event_id],\n                \"auth_events\": [],\n                \"origin_server_ts\": self.clock.time_msec(),\n            },\n            room_version,\n        )\n\n        with LoggingContext(request=\"send_rejected\"):\n            d = run_in_background(self.handler.on_receive_pdu, OTHER_SERVER, ev)\n        self.get_success(d)\n\n        # that should have been rejected\n        e = self.get_success(self.store.get_event(ev.event_id, allow_rejected=True))\n        self.assertIsNotNone(e.rejected_reason)\n\n        # ... and the state group should be the same as before\n        sg2 = self.successResultOf(self.store._get_state_group_for_event(ev.event_id))\n\n        self.assertEqual(sg, sg2)\n\n    def _build_and_send_join_event(self, other_server, other_user, room_id):\n        join_event = self.get_success(\n            self.handler.on_make_join_request(other_server, room_id, other_user)\n        )\n        # the auth code requires that a signature exists, but doesn't check that\n        # signature... go figure.\n        join_event.signatures[other_server] = {\"x\": \"y\"}\n        with LoggingContext(request=\"send_join\"):\n            d = run_in_background(\n                self.handler.on_send_join_request, other_server, join_event\n            )\n        self.get_success(d)\n\n        # sanity-check: the room should show that the new user is a member\n        r = self.get_success(self.store.get_current_state_ids(room_id))\n        self.assertEqual(r[(EventTypes.Member, other_user)], join_event.event_id)\n\n        return join_event", "target": 0}, {"function": "class EventFromPduTestCase(TestCase):\n    def test_valid_json(self):\n        \"\"\"Valid JSON should be turned into an event.\"\"\"\n        ev = event_from_pdu_json(\n            {\n                \"type\": EventTypes.Message,\n                \"content\": {\"bool\": True, \"null\": None, \"int\": 1, \"str\": \"foobar\"},\n                \"room_id\": \"!room:test\",\n                \"sender\": \"@user:test\",\n                \"depth\": 1,\n                \"prev_events\": [],\n                \"auth_events\": [],\n                \"origin_server_ts\": 1234,\n            },\n            RoomVersions.V6,\n        )\n\n        self.assertIsInstance(ev, EventBase)\n\n    def test_invalid_numbers(self):\n        \"\"\"Invalid values for an integer should be rejected, all floats should be rejected.\"\"\"\n        for value in [\n            -(2 ** 53),\n            2 ** 53,\n            1.0,\n            float(\"inf\"),\n            float(\"-inf\"),\n            float(\"nan\"),\n        ]:\n            with self.assertRaises(SynapseError):\n                event_from_pdu_json(\n                    {\n                        \"type\": EventTypes.Message,\n                        \"content\": {\"foo\": value},\n                        \"room_id\": \"!room:test\",\n                        \"sender\": \"@user:test\",\n                        \"depth\": 1,\n                        \"prev_events\": [],\n                        \"auth_events\": [],\n                        \"origin_server_ts\": 1234,\n                    },\n                    RoomVersions.V6,\n                )\n\n    def test_invalid_nested(self):\n        \"\"\"List and dictionaries are recursively searched.\"\"\"\n        with self.assertRaises(SynapseError):\n            event_from_pdu_json(\n                {\n                    \"type\": EventTypes.Message,\n                    \"content\": {\"foo\": [{\"bar\": 2 ** 56}]},\n                    \"room_id\": \"!room:test\",\n                    \"sender\": \"@user:test\",\n                    \"depth\": 1,\n                    \"prev_events\": [],\n                    \"auth_events\": [],\n                    \"origin_server_ts\": 1234,\n                },\n                RoomVersions.V6,\n            )", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fhandlers%2Ftest_presence.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom mock import Mock, call\n\nfrom signedjson.key import generate_signing_key\n\nfrom synapse.api.constants import EventTypes, Membership, PresenceState\nfrom synapse.api.presence import UserPresenceState\nfrom synapse.api.room_versions import KNOWN_ROOM_VERSIONS\nfrom synapse.events.builder import EventBuilder\nfrom synapse.handlers.presence import (\n    EXTERNAL_PROCESS_EXPIRY,\n    FEDERATION_PING_INTERVAL,\n    FEDERATION_TIMEOUT,\n    IDLE_TIMER,\n    LAST_ACTIVE_GRANULARITY,\n    SYNC_ONLINE_TIMEOUT,\n    handle_timeout,\n    handle_update,\n)\nfrom synapse.rest.client.v1 import room\nfrom synapse.types import UserID, get_domain_from_id\n\nfrom tests import unittest\n\n\nclass PresenceUpdateTestCase(unittest.TestCase):\n    def test_offline_to_online(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertTrue(federation_ping)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online_last_active_noop(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 10,\n            currently_active=True,\n        )\n\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertTrue(federation_ping)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online_last_active(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 1,\n            currently_active=True,\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.ONLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertFalse(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 2)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_remote_ping_timer(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.ONLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=False, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertFalse(federation_ping)\n        self.assertFalse(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n\n        self.assertEquals(wheel_timer.insert.call_count, 1)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_federation_update_ts + FEDERATION_TIMEOUT,\n                )\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_offline(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.OFFLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 0)\n\n    def test_online_to_idle(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.UNAVAILABLE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(state.last_federation_update_ts, now)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n\n        self.assertEquals(wheel_timer.insert.call_count, 1)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                )\n            ],\n            any_order=True,\n        )\n\n\nclass PresenceTimeoutTestCase(unittest.TestCase):\n    def test_idle_timer(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - IDLE_TIMER - 1,\n            last_user_sync_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.UNAVAILABLE)\n\n    def test_sync_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=0,\n            last_user_sync_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.OFFLINE)\n\n    def test_sync_online(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n            last_user_sync_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(\n            state, is_mine=True, syncing_user_ids={user_id}, now=now\n        )\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.ONLINE)\n\n    def test_federation_ping(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now - FEDERATION_PING_INTERVAL - 1,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state, new_state)\n\n    def test_no_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNone(new_state)\n\n    def test_federation_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now - FEDERATION_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(\n            state, is_mine=False, syncing_user_ids=set(), now=now\n        )\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.OFFLINE)\n\n    def test_last_active(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 1,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(state, new_state)\n\n\nclass PresenceHandlerTestCase(unittest.HomeserverTestCase):\n    def prepare(self, reactor, clock, hs):\n        self.presence_handler = hs.get_presence_handler()\n        self.clock = hs.get_clock()\n\n    def test_external_process_timeout(self):\n        \"\"\"Test that if an external process doesn't update the records for a while\n        we time out their syncing users presence.\n        \"\"\"\n        process_id = 1\n        user_id = \"@test:server\"\n\n        # Notify handler that a user is now syncing.\n        self.get_success(\n            self.presence_handler.update_external_syncs_row(\n                process_id, user_id, True, self.clock.time_msec()\n            )\n        )\n\n        # Check that if we wait a while without telling the handler the user has\n        # stopped syncing that their presence state doesn't get timed out.\n        self.reactor.advance(EXTERNAL_PROCESS_EXPIRY / 2)\n\n        state = self.get_success(\n            self.presence_handler.get_state(UserID.from_string(user_id))\n        )\n        self.assertEqual(state.state, PresenceState.ONLINE)\n\n        # Check that if the external process timeout fires, then the syncing\n        # user gets timed out\n        self.reactor.advance(EXTERNAL_PROCESS_EXPIRY)\n\n        state = self.get_success(\n            self.presence_handler.get_state(UserID.from_string(user_id))\n        )\n        self.assertEqual(state.state, PresenceState.OFFLINE)\n\n\nclass PresenceJoinTestCase(unittest.HomeserverTestCase):\n    \"\"\"Tests remote servers get told about presence of users in the room when\n    they join and when new local users join.\n    \"\"\"\n\n    user_id = \"@test:server\"\n\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            \"server\", federation_http_client=None, federation_sender=Mock()\n        )\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.federation_sender = hs.get_federation_sender()\n        self.event_builder_factory = hs.get_event_builder_factory()\n        self.federation_handler = hs.get_federation_handler()\n        self.presence_handler = hs.get_presence_handler()\n\n        # self.event_builder_for_2 = EventBuilderFactory(hs)\n        # self.event_builder_for_2.hostname = \"test2\"\n\n        self.store = hs.get_datastore()\n        self.state = hs.get_state_handler()\n        self.auth = hs.get_auth()\n\n        # We don't actually check signatures in tests, so lets just create a\n        # random key to use.\n        self.random_signing_key = generate_signing_key(\"ver\")\n\n    def test_remote_joins(self):\n        # We advance time to something that isn't 0, as we use 0 as a special\n        # value.\n        self.reactor.advance(1000000000000)\n\n        # Create a room with two local users\n        room_id = self.helper.create_room_as(self.user_id)\n        self.helper.join(room_id, \"@test2:server\")\n\n        # Mark test2 as online, test will be offline with a last_active of 0\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test2:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        #\n        # Test that a new server gets told about existing presence\n        #\n\n        self.federation_sender.reset_mock()\n\n        # Add a new remote server to the room\n        self._add_new_user(room_id, \"@alice:server2\")\n\n        # We shouldn't have sent out any local presence *updates*\n        self.federation_sender.send_presence.assert_not_called()\n\n        # When new server is joined we send it the local users presence states.\n        # We expect to only see user @test2:server, as @test:server is offline\n        # and has a zero last_active_ts\n        expected_state = self.get_success(\n            self.presence_handler.current_state_for_user(\"@test2:server\")\n        )\n        self.assertEqual(expected_state.state, PresenceState.ONLINE)\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations=[\"server2\"], states=[expected_state]\n        )\n\n        #\n        # Test that only the new server gets sent presence and not existing servers\n        #\n\n        self.federation_sender.reset_mock()\n        self._add_new_user(room_id, \"@bob:server3\")\n\n        self.federation_sender.send_presence.assert_not_called()\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations=[\"server3\"], states=[expected_state]\n        )\n\n    def test_remote_gets_presence_when_local_user_joins(self):\n        # We advance time to something that isn't 0, as we use 0 as a special\n        # value.\n        self.reactor.advance(1000000000000)\n\n        # Create a room with one local users\n        room_id = self.helper.create_room_as(self.user_id)\n\n        # Mark test as online\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n\n        # Mark test2 as online, test will be offline with a last_active of 0.\n        # Note we don't join them to the room yet\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test2:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n\n        # Add servers to the room\n        self._add_new_user(room_id, \"@alice:server2\")\n        self._add_new_user(room_id, \"@bob:server3\")\n\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        #\n        # Test that when a local join happens remote servers get told about it\n        #\n\n        self.federation_sender.reset_mock()\n\n        # Join local user to room\n        self.helper.join(room_id, \"@test2:server\")\n\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        # We shouldn't have sent out any local presence *updates*\n        self.federation_sender.send_presence.assert_not_called()\n\n        # We expect to only send test2 presence to server2 and server3\n        expected_state = self.get_success(\n            self.presence_handler.current_state_for_user(\"@test2:server\")\n        )\n        self.assertEqual(expected_state.state, PresenceState.ONLINE)\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations={\"server2\", \"server3\"}, states=[expected_state]\n        )\n\n    def _add_new_user(self, room_id, user_id):\n        \"\"\"Add new user to the room by creating an event and poking the federation API.\n        \"\"\"\n\n        hostname = get_domain_from_id(user_id)\n\n        room_version = self.get_success(self.store.get_room_version_id(room_id))\n\n        builder = EventBuilder(\n            state=self.state,\n            auth=self.auth,\n            store=self.store,\n            clock=self.clock,\n            hostname=hostname,\n            signing_key=self.random_signing_key,\n            room_version=KNOWN_ROOM_VERSIONS[room_version],\n            room_id=room_id,\n            type=EventTypes.Member,\n            sender=user_id,\n            state_key=user_id,\n            content={\"membership\": Membership.JOIN},\n        )\n\n        prev_event_ids = self.get_success(\n            self.store.get_latest_event_ids_in_room(room_id)\n        )\n\n        event = self.get_success(builder.build(prev_event_ids, None))\n\n        self.get_success(self.federation_handler.on_receive_pdu(hostname, event))\n\n        # Check that it was successfully persisted.\n        self.get_success(self.store.get_event(event.event_id))\n        self.get_success(self.store.get_event(event.event_id))\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom mock import Mock, call\n\nfrom signedjson.key import generate_signing_key\n\nfrom synapse.api.constants import EventTypes, Membership, PresenceState\nfrom synapse.api.presence import UserPresenceState\nfrom synapse.api.room_versions import KNOWN_ROOM_VERSIONS\nfrom synapse.events.builder import EventBuilder\nfrom synapse.handlers.presence import (\n    EXTERNAL_PROCESS_EXPIRY,\n    FEDERATION_PING_INTERVAL,\n    FEDERATION_TIMEOUT,\n    IDLE_TIMER,\n    LAST_ACTIVE_GRANULARITY,\n    SYNC_ONLINE_TIMEOUT,\n    handle_timeout,\n    handle_update,\n)\nfrom synapse.rest.client.v1 import room\nfrom synapse.types import UserID, get_domain_from_id\n\nfrom tests import unittest\n\n\nclass PresenceUpdateTestCase(unittest.TestCase):\n    def test_offline_to_online(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertTrue(federation_ping)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online_last_active_noop(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 10,\n            currently_active=True,\n        )\n\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertTrue(federation_ping)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online_last_active(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 1,\n            currently_active=True,\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.ONLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertFalse(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 2)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_remote_ping_timer(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.ONLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=False, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertFalse(federation_ping)\n        self.assertFalse(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n\n        self.assertEquals(wheel_timer.insert.call_count, 1)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_federation_update_ts + FEDERATION_TIMEOUT,\n                )\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_offline(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.OFFLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 0)\n\n    def test_online_to_idle(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.UNAVAILABLE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(state.last_federation_update_ts, now)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n\n        self.assertEquals(wheel_timer.insert.call_count, 1)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                )\n            ],\n            any_order=True,\n        )\n\n\nclass PresenceTimeoutTestCase(unittest.TestCase):\n    def test_idle_timer(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - IDLE_TIMER - 1,\n            last_user_sync_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.UNAVAILABLE)\n\n    def test_sync_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=0,\n            last_user_sync_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.OFFLINE)\n\n    def test_sync_online(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n            last_user_sync_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(\n            state, is_mine=True, syncing_user_ids={user_id}, now=now\n        )\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.ONLINE)\n\n    def test_federation_ping(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now - FEDERATION_PING_INTERVAL - 1,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state, new_state)\n\n    def test_no_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNone(new_state)\n\n    def test_federation_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now - FEDERATION_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(\n            state, is_mine=False, syncing_user_ids=set(), now=now\n        )\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.OFFLINE)\n\n    def test_last_active(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 1,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(state, new_state)\n\n\nclass PresenceHandlerTestCase(unittest.HomeserverTestCase):\n    def prepare(self, reactor, clock, hs):\n        self.presence_handler = hs.get_presence_handler()\n        self.clock = hs.get_clock()\n\n    def test_external_process_timeout(self):\n        \"\"\"Test that if an external process doesn't update the records for a while\n        we time out their syncing users presence.\n        \"\"\"\n        process_id = 1\n        user_id = \"@test:server\"\n\n        # Notify handler that a user is now syncing.\n        self.get_success(\n            self.presence_handler.update_external_syncs_row(\n                process_id, user_id, True, self.clock.time_msec()\n            )\n        )\n\n        # Check that if we wait a while without telling the handler the user has\n        # stopped syncing that their presence state doesn't get timed out.\n        self.reactor.advance(EXTERNAL_PROCESS_EXPIRY / 2)\n\n        state = self.get_success(\n            self.presence_handler.get_state(UserID.from_string(user_id))\n        )\n        self.assertEqual(state.state, PresenceState.ONLINE)\n\n        # Check that if the external process timeout fires, then the syncing\n        # user gets timed out\n        self.reactor.advance(EXTERNAL_PROCESS_EXPIRY)\n\n        state = self.get_success(\n            self.presence_handler.get_state(UserID.from_string(user_id))\n        )\n        self.assertEqual(state.state, PresenceState.OFFLINE)\n\n\nclass PresenceJoinTestCase(unittest.HomeserverTestCase):\n    \"\"\"Tests remote servers get told about presence of users in the room when\n    they join and when new local users join.\n    \"\"\"\n\n    user_id = \"@test:server\"\n\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            \"server\", http_client=None, federation_sender=Mock()\n        )\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.federation_sender = hs.get_federation_sender()\n        self.event_builder_factory = hs.get_event_builder_factory()\n        self.federation_handler = hs.get_federation_handler()\n        self.presence_handler = hs.get_presence_handler()\n\n        # self.event_builder_for_2 = EventBuilderFactory(hs)\n        # self.event_builder_for_2.hostname = \"test2\"\n\n        self.store = hs.get_datastore()\n        self.state = hs.get_state_handler()\n        self.auth = hs.get_auth()\n\n        # We don't actually check signatures in tests, so lets just create a\n        # random key to use.\n        self.random_signing_key = generate_signing_key(\"ver\")\n\n    def test_remote_joins(self):\n        # We advance time to something that isn't 0, as we use 0 as a special\n        # value.\n        self.reactor.advance(1000000000000)\n\n        # Create a room with two local users\n        room_id = self.helper.create_room_as(self.user_id)\n        self.helper.join(room_id, \"@test2:server\")\n\n        # Mark test2 as online, test will be offline with a last_active of 0\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test2:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        #\n        # Test that a new server gets told about existing presence\n        #\n\n        self.federation_sender.reset_mock()\n\n        # Add a new remote server to the room\n        self._add_new_user(room_id, \"@alice:server2\")\n\n        # We shouldn't have sent out any local presence *updates*\n        self.federation_sender.send_presence.assert_not_called()\n\n        # When new server is joined we send it the local users presence states.\n        # We expect to only see user @test2:server, as @test:server is offline\n        # and has a zero last_active_ts\n        expected_state = self.get_success(\n            self.presence_handler.current_state_for_user(\"@test2:server\")\n        )\n        self.assertEqual(expected_state.state, PresenceState.ONLINE)\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations=[\"server2\"], states=[expected_state]\n        )\n\n        #\n        # Test that only the new server gets sent presence and not existing servers\n        #\n\n        self.federation_sender.reset_mock()\n        self._add_new_user(room_id, \"@bob:server3\")\n\n        self.federation_sender.send_presence.assert_not_called()\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations=[\"server3\"], states=[expected_state]\n        )\n\n    def test_remote_gets_presence_when_local_user_joins(self):\n        # We advance time to something that isn't 0, as we use 0 as a special\n        # value.\n        self.reactor.advance(1000000000000)\n\n        # Create a room with one local users\n        room_id = self.helper.create_room_as(self.user_id)\n\n        # Mark test as online\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n\n        # Mark test2 as online, test will be offline with a last_active of 0.\n        # Note we don't join them to the room yet\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test2:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n\n        # Add servers to the room\n        self._add_new_user(room_id, \"@alice:server2\")\n        self._add_new_user(room_id, \"@bob:server3\")\n\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        #\n        # Test that when a local join happens remote servers get told about it\n        #\n\n        self.federation_sender.reset_mock()\n\n        # Join local user to room\n        self.helper.join(room_id, \"@test2:server\")\n\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        # We shouldn't have sent out any local presence *updates*\n        self.federation_sender.send_presence.assert_not_called()\n\n        # We expect to only send test2 presence to server2 and server3\n        expected_state = self.get_success(\n            self.presence_handler.current_state_for_user(\"@test2:server\")\n        )\n        self.assertEqual(expected_state.state, PresenceState.ONLINE)\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations={\"server2\", \"server3\"}, states=[expected_state]\n        )\n\n    def _add_new_user(self, room_id, user_id):\n        \"\"\"Add new user to the room by creating an event and poking the federation API.\n        \"\"\"\n\n        hostname = get_domain_from_id(user_id)\n\n        room_version = self.get_success(self.store.get_room_version_id(room_id))\n\n        builder = EventBuilder(\n            state=self.state,\n            auth=self.auth,\n            store=self.store,\n            clock=self.clock,\n            hostname=hostname,\n            signing_key=self.random_signing_key,\n            room_version=KNOWN_ROOM_VERSIONS[room_version],\n            room_id=room_id,\n            type=EventTypes.Member,\n            sender=user_id,\n            state_key=user_id,\n            content={\"membership\": Membership.JOIN},\n        )\n\n        prev_event_ids = self.get_success(\n            self.store.get_latest_event_ids_in_room(room_id)\n        )\n\n        event = self.get_success(builder.build(prev_event_ids, None))\n\n        self.get_success(self.federation_handler.on_receive_pdu(hostname, event))\n\n        # Check that it was successfully persisted.\n        self.get_success(self.store.get_event(event.event_id))\n        self.get_success(self.store.get_event(event.event_id))\n", "patch": "@@ -463,7 +463,7 @@ class PresenceJoinTestCase(unittest.HomeserverTestCase):\n \n     def make_homeserver(self, reactor, clock):\n         hs = self.setup_test_homeserver(\n-            \"server\", http_client=None, federation_sender=Mock()\n+            \"server\", federation_http_client=None, federation_sender=Mock()\n         )\n         return hs\n ", "file_path": "files/2021_2/37", "file_language": "py", "file_name": "tests/handlers/test_presence.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class PresenceUpdateTestCase(unittest.TestCase):\n    def test_offline_to_online(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertTrue(federation_ping)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online_last_active_noop(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 10,\n            currently_active=True,\n        )\n\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertTrue(federation_ping)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online_last_active(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 1,\n            currently_active=True,\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.ONLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertFalse(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 2)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_remote_ping_timer(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.ONLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=False, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertFalse(federation_ping)\n        self.assertFalse(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n\n        self.assertEquals(wheel_timer.insert.call_count, 1)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_federation_update_ts + FEDERATION_TIMEOUT,\n                )\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_offline(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.OFFLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 0)\n\n    def test_online_to_idle(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.UNAVAILABLE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(state.last_federation_update_ts, now)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n\n        self.assertEquals(wheel_timer.insert.call_count, 1)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                )\n            ],\n            any_order=True,\n        )", "target": 0}, {"function": "class PresenceTimeoutTestCase(unittest.TestCase):\n    def test_idle_timer(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - IDLE_TIMER - 1,\n            last_user_sync_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.UNAVAILABLE)\n\n    def test_sync_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=0,\n            last_user_sync_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.OFFLINE)\n\n    def test_sync_online(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n            last_user_sync_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(\n            state, is_mine=True, syncing_user_ids={user_id}, now=now\n        )\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.ONLINE)\n\n    def test_federation_ping(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now - FEDERATION_PING_INTERVAL - 1,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state, new_state)\n\n    def test_no_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNone(new_state)\n\n    def test_federation_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now - FEDERATION_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(\n            state, is_mine=False, syncing_user_ids=set(), now=now\n        )\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.OFFLINE)\n\n    def test_last_active(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 1,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(state, new_state)", "target": 0}, {"function": "class PresenceHandlerTestCase(unittest.HomeserverTestCase):\n    def prepare(self, reactor, clock, hs):\n        self.presence_handler = hs.get_presence_handler()\n        self.clock = hs.get_clock()\n\n    def test_external_process_timeout(self):\n        \"\"\"Test that if an external process doesn't update the records for a while\n        we time out their syncing users presence.\n        \"\"\"\n        process_id = 1\n        user_id = \"@test:server\"\n\n        # Notify handler that a user is now syncing.\n        self.get_success(\n            self.presence_handler.update_external_syncs_row(\n                process_id, user_id, True, self.clock.time_msec()\n            )\n        )\n\n        # Check that if we wait a while without telling the handler the user has\n        # stopped syncing that their presence state doesn't get timed out.\n        self.reactor.advance(EXTERNAL_PROCESS_EXPIRY / 2)\n\n        state = self.get_success(\n            self.presence_handler.get_state(UserID.from_string(user_id))\n        )\n        self.assertEqual(state.state, PresenceState.ONLINE)\n\n        # Check that if the external process timeout fires, then the syncing\n        # user gets timed out\n        self.reactor.advance(EXTERNAL_PROCESS_EXPIRY)\n\n        state = self.get_success(\n            self.presence_handler.get_state(UserID.from_string(user_id))\n        )\n        self.assertEqual(state.state, PresenceState.OFFLINE)", "target": 0}, {"function": "class PresenceJoinTestCase(unittest.HomeserverTestCase):\n    \"\"\"Tests remote servers get told about presence of users in the room when\n    they join and when new local users join.\n    \"\"\"\n\n    user_id = \"@test:server\"\n\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            \"server\", http_client=None, federation_sender=Mock()\n        )\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.federation_sender = hs.get_federation_sender()\n        self.event_builder_factory = hs.get_event_builder_factory()\n        self.federation_handler = hs.get_federation_handler()\n        self.presence_handler = hs.get_presence_handler()\n\n        # self.event_builder_for_2 = EventBuilderFactory(hs)\n        # self.event_builder_for_2.hostname = \"test2\"\n\n        self.store = hs.get_datastore()\n        self.state = hs.get_state_handler()\n        self.auth = hs.get_auth()\n\n        # We don't actually check signatures in tests, so lets just create a\n        # random key to use.\n        self.random_signing_key = generate_signing_key(\"ver\")\n\n    def test_remote_joins(self):\n        # We advance time to something that isn't 0, as we use 0 as a special\n        # value.\n        self.reactor.advance(1000000000000)\n\n        # Create a room with two local users\n        room_id = self.helper.create_room_as(self.user_id)\n        self.helper.join(room_id, \"@test2:server\")\n\n        # Mark test2 as online, test will be offline with a last_active of 0\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test2:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        #\n        # Test that a new server gets told about existing presence\n        #\n\n        self.federation_sender.reset_mock()\n\n        # Add a new remote server to the room\n        self._add_new_user(room_id, \"@alice:server2\")\n\n        # We shouldn't have sent out any local presence *updates*\n        self.federation_sender.send_presence.assert_not_called()\n\n        # When new server is joined we send it the local users presence states.\n        # We expect to only see user @test2:server, as @test:server is offline\n        # and has a zero last_active_ts\n        expected_state = self.get_success(\n            self.presence_handler.current_state_for_user(\"@test2:server\")\n        )\n        self.assertEqual(expected_state.state, PresenceState.ONLINE)\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations=[\"server2\"], states=[expected_state]\n        )\n\n        #\n        # Test that only the new server gets sent presence and not existing servers\n        #\n\n        self.federation_sender.reset_mock()\n        self._add_new_user(room_id, \"@bob:server3\")\n\n        self.federation_sender.send_presence.assert_not_called()\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations=[\"server3\"], states=[expected_state]\n        )\n\n    def test_remote_gets_presence_when_local_user_joins(self):\n        # We advance time to something that isn't 0, as we use 0 as a special\n        # value.\n        self.reactor.advance(1000000000000)\n\n        # Create a room with one local users\n        room_id = self.helper.create_room_as(self.user_id)\n\n        # Mark test as online\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n\n        # Mark test2 as online, test will be offline with a last_active of 0.\n        # Note we don't join them to the room yet\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test2:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n\n        # Add servers to the room\n        self._add_new_user(room_id, \"@alice:server2\")\n        self._add_new_user(room_id, \"@bob:server3\")\n\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        #\n        # Test that when a local join happens remote servers get told about it\n        #\n\n        self.federation_sender.reset_mock()\n\n        # Join local user to room\n        self.helper.join(room_id, \"@test2:server\")\n\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        # We shouldn't have sent out any local presence *updates*\n        self.federation_sender.send_presence.assert_not_called()\n\n        # We expect to only send test2 presence to server2 and server3\n        expected_state = self.get_success(\n            self.presence_handler.current_state_for_user(\"@test2:server\")\n        )\n        self.assertEqual(expected_state.state, PresenceState.ONLINE)\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations={\"server2\", \"server3\"}, states=[expected_state]\n        )\n\n    def _add_new_user(self, room_id, user_id):\n        \"\"\"Add new user to the room by creating an event and poking the federation API.\n        \"\"\"\n\n        hostname = get_domain_from_id(user_id)\n\n        room_version = self.get_success(self.store.get_room_version_id(room_id))\n\n        builder = EventBuilder(\n            state=self.state,\n            auth=self.auth,\n            store=self.store,\n            clock=self.clock,\n            hostname=hostname,\n            signing_key=self.random_signing_key,\n            room_version=KNOWN_ROOM_VERSIONS[room_version],\n            room_id=room_id,\n            type=EventTypes.Member,\n            sender=user_id,\n            state_key=user_id,\n            content={\"membership\": Membership.JOIN},\n        )\n\n        prev_event_ids = self.get_success(\n            self.store.get_latest_event_ids_in_room(room_id)\n        )\n\n        event = self.get_success(builder.build(prev_event_ids, None))\n\n        self.get_success(self.federation_handler.on_receive_pdu(hostname, event))\n\n        # Check that it was successfully persisted.\n        self.get_success(self.store.get_event(event.event_id))\n        self.get_success(self.store.get_event(event.event_id))", "target": 0}], "function_after": [{"function": "class PresenceUpdateTestCase(unittest.TestCase):\n    def test_offline_to_online(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertTrue(federation_ping)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online_last_active_noop(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 10,\n            currently_active=True,\n        )\n\n        new_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertTrue(federation_ping)\n        self.assertTrue(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 3)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_active_ts + LAST_ACTIVE_GRANULARITY,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_online_last_active(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 1,\n            currently_active=True,\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.ONLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertFalse(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 2)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(now=now, obj=user_id, then=new_state.last_active_ts + IDLE_TIMER),\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                ),\n            ],\n            any_order=True,\n        )\n\n    def test_remote_ping_timer(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.ONLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=False, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertFalse(persist_and_notify)\n        self.assertFalse(federation_ping)\n        self.assertFalse(state.currently_active)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n\n        self.assertEquals(wheel_timer.insert.call_count, 1)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_federation_update_ts + FEDERATION_TIMEOUT,\n                )\n            ],\n            any_order=True,\n        )\n\n    def test_online_to_offline(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.OFFLINE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(state.last_federation_update_ts, now)\n\n        self.assertEquals(wheel_timer.insert.call_count, 0)\n\n    def test_online_to_idle(self):\n        wheel_timer = Mock()\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        prev_state = UserPresenceState.default(user_id)\n        prev_state = prev_state.copy_and_replace(\n            state=PresenceState.ONLINE, last_active_ts=now, currently_active=True\n        )\n\n        new_state = prev_state.copy_and_replace(state=PresenceState.UNAVAILABLE)\n\n        state, persist_and_notify, federation_ping = handle_update(\n            prev_state, new_state, is_mine=True, wheel_timer=wheel_timer, now=now\n        )\n\n        self.assertTrue(persist_and_notify)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(state.last_federation_update_ts, now)\n        self.assertEquals(new_state.state, state.state)\n        self.assertEquals(new_state.status_msg, state.status_msg)\n\n        self.assertEquals(wheel_timer.insert.call_count, 1)\n        wheel_timer.insert.assert_has_calls(\n            [\n                call(\n                    now=now,\n                    obj=user_id,\n                    then=new_state.last_user_sync_ts + SYNC_ONLINE_TIMEOUT,\n                )\n            ],\n            any_order=True,\n        )", "target": 0}, {"function": "class PresenceTimeoutTestCase(unittest.TestCase):\n    def test_idle_timer(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - IDLE_TIMER - 1,\n            last_user_sync_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.UNAVAILABLE)\n\n    def test_sync_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=0,\n            last_user_sync_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.OFFLINE)\n\n    def test_sync_online(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n            last_user_sync_ts=now - SYNC_ONLINE_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(\n            state, is_mine=True, syncing_user_ids={user_id}, now=now\n        )\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.ONLINE)\n\n    def test_federation_ping(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now - FEDERATION_PING_INTERVAL - 1,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state, new_state)\n\n    def test_no_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNone(new_state)\n\n    def test_federation_timeout(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now - FEDERATION_TIMEOUT - 1,\n        )\n\n        new_state = handle_timeout(\n            state, is_mine=False, syncing_user_ids=set(), now=now\n        )\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(new_state.state, PresenceState.OFFLINE)\n\n    def test_last_active(self):\n        user_id = \"@foo:bar\"\n        now = 5000000\n\n        state = UserPresenceState.default(user_id)\n        state = state.copy_and_replace(\n            state=PresenceState.ONLINE,\n            last_active_ts=now - LAST_ACTIVE_GRANULARITY - 1,\n            last_user_sync_ts=now,\n            last_federation_update_ts=now,\n        )\n\n        new_state = handle_timeout(state, is_mine=True, syncing_user_ids=set(), now=now)\n\n        self.assertIsNotNone(new_state)\n        self.assertEquals(state, new_state)", "target": 0}, {"function": "class PresenceHandlerTestCase(unittest.HomeserverTestCase):\n    def prepare(self, reactor, clock, hs):\n        self.presence_handler = hs.get_presence_handler()\n        self.clock = hs.get_clock()\n\n    def test_external_process_timeout(self):\n        \"\"\"Test that if an external process doesn't update the records for a while\n        we time out their syncing users presence.\n        \"\"\"\n        process_id = 1\n        user_id = \"@test:server\"\n\n        # Notify handler that a user is now syncing.\n        self.get_success(\n            self.presence_handler.update_external_syncs_row(\n                process_id, user_id, True, self.clock.time_msec()\n            )\n        )\n\n        # Check that if we wait a while without telling the handler the user has\n        # stopped syncing that their presence state doesn't get timed out.\n        self.reactor.advance(EXTERNAL_PROCESS_EXPIRY / 2)\n\n        state = self.get_success(\n            self.presence_handler.get_state(UserID.from_string(user_id))\n        )\n        self.assertEqual(state.state, PresenceState.ONLINE)\n\n        # Check that if the external process timeout fires, then the syncing\n        # user gets timed out\n        self.reactor.advance(EXTERNAL_PROCESS_EXPIRY)\n\n        state = self.get_success(\n            self.presence_handler.get_state(UserID.from_string(user_id))\n        )\n        self.assertEqual(state.state, PresenceState.OFFLINE)", "target": 0}, {"function": "class PresenceJoinTestCase(unittest.HomeserverTestCase):\n    \"\"\"Tests remote servers get told about presence of users in the room when\n    they join and when new local users join.\n    \"\"\"\n\n    user_id = \"@test:server\"\n\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            \"server\", federation_http_client=None, federation_sender=Mock()\n        )\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.federation_sender = hs.get_federation_sender()\n        self.event_builder_factory = hs.get_event_builder_factory()\n        self.federation_handler = hs.get_federation_handler()\n        self.presence_handler = hs.get_presence_handler()\n\n        # self.event_builder_for_2 = EventBuilderFactory(hs)\n        # self.event_builder_for_2.hostname = \"test2\"\n\n        self.store = hs.get_datastore()\n        self.state = hs.get_state_handler()\n        self.auth = hs.get_auth()\n\n        # We don't actually check signatures in tests, so lets just create a\n        # random key to use.\n        self.random_signing_key = generate_signing_key(\"ver\")\n\n    def test_remote_joins(self):\n        # We advance time to something that isn't 0, as we use 0 as a special\n        # value.\n        self.reactor.advance(1000000000000)\n\n        # Create a room with two local users\n        room_id = self.helper.create_room_as(self.user_id)\n        self.helper.join(room_id, \"@test2:server\")\n\n        # Mark test2 as online, test will be offline with a last_active of 0\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test2:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        #\n        # Test that a new server gets told about existing presence\n        #\n\n        self.federation_sender.reset_mock()\n\n        # Add a new remote server to the room\n        self._add_new_user(room_id, \"@alice:server2\")\n\n        # We shouldn't have sent out any local presence *updates*\n        self.federation_sender.send_presence.assert_not_called()\n\n        # When new server is joined we send it the local users presence states.\n        # We expect to only see user @test2:server, as @test:server is offline\n        # and has a zero last_active_ts\n        expected_state = self.get_success(\n            self.presence_handler.current_state_for_user(\"@test2:server\")\n        )\n        self.assertEqual(expected_state.state, PresenceState.ONLINE)\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations=[\"server2\"], states=[expected_state]\n        )\n\n        #\n        # Test that only the new server gets sent presence and not existing servers\n        #\n\n        self.federation_sender.reset_mock()\n        self._add_new_user(room_id, \"@bob:server3\")\n\n        self.federation_sender.send_presence.assert_not_called()\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations=[\"server3\"], states=[expected_state]\n        )\n\n    def test_remote_gets_presence_when_local_user_joins(self):\n        # We advance time to something that isn't 0, as we use 0 as a special\n        # value.\n        self.reactor.advance(1000000000000)\n\n        # Create a room with one local users\n        room_id = self.helper.create_room_as(self.user_id)\n\n        # Mark test as online\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n\n        # Mark test2 as online, test will be offline with a last_active of 0.\n        # Note we don't join them to the room yet\n        self.get_success(\n            self.presence_handler.set_state(\n                UserID.from_string(\"@test2:server\"), {\"presence\": PresenceState.ONLINE}\n            )\n        )\n\n        # Add servers to the room\n        self._add_new_user(room_id, \"@alice:server2\")\n        self._add_new_user(room_id, \"@bob:server3\")\n\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        #\n        # Test that when a local join happens remote servers get told about it\n        #\n\n        self.federation_sender.reset_mock()\n\n        # Join local user to room\n        self.helper.join(room_id, \"@test2:server\")\n\n        self.reactor.pump([0])  # Wait for presence updates to be handled\n\n        # We shouldn't have sent out any local presence *updates*\n        self.federation_sender.send_presence.assert_not_called()\n\n        # We expect to only send test2 presence to server2 and server3\n        expected_state = self.get_success(\n            self.presence_handler.current_state_for_user(\"@test2:server\")\n        )\n        self.assertEqual(expected_state.state, PresenceState.ONLINE)\n        self.federation_sender.send_presence_to_destinations.assert_called_once_with(\n            destinations={\"server2\", \"server3\"}, states=[expected_state]\n        )\n\n    def _add_new_user(self, room_id, user_id):\n        \"\"\"Add new user to the room by creating an event and poking the federation API.\n        \"\"\"\n\n        hostname = get_domain_from_id(user_id)\n\n        room_version = self.get_success(self.store.get_room_version_id(room_id))\n\n        builder = EventBuilder(\n            state=self.state,\n            auth=self.auth,\n            store=self.store,\n            clock=self.clock,\n            hostname=hostname,\n            signing_key=self.random_signing_key,\n            room_version=KNOWN_ROOM_VERSIONS[room_version],\n            room_id=room_id,\n            type=EventTypes.Member,\n            sender=user_id,\n            state_key=user_id,\n            content={\"membership\": Membership.JOIN},\n        )\n\n        prev_event_ids = self.get_success(\n            self.store.get_latest_event_ids_in_room(room_id)\n        )\n\n        event = self.get_success(builder.build(prev_event_ids, None))\n\n        self.get_success(self.federation_handler.on_receive_pdu(hostname, event))\n\n        # Check that it was successfully persisted.\n        self.get_success(self.store.get_event(event.event_id))\n        self.get_success(self.store.get_event(event.event_id))", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fhandlers%2Ftest_profile.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom mock import Mock\n\nfrom twisted.internet import defer\n\nimport synapse.types\nfrom synapse.api.errors import AuthError, SynapseError\nfrom synapse.types import UserID\n\nfrom tests import unittest\nfrom tests.test_utils import make_awaitable\nfrom tests.utils import setup_test_homeserver\n\n\nclass ProfileTestCase(unittest.TestCase):\n    \"\"\" Tests profile management. \"\"\"\n\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_federation = Mock()\n        self.mock_registry = Mock()\n\n        self.query_handlers = {}\n\n        def register_query_handler(query_type, handler):\n            self.query_handlers[query_type] = handler\n\n        self.mock_registry.register_query_handler = register_query_handler\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup,\n            federation_http_client=None,\n            resource_for_federation=Mock(),\n            federation_client=self.mock_federation,\n            federation_server=Mock(),\n            federation_registry=self.mock_registry,\n        )\n\n        self.store = hs.get_datastore()\n\n        self.frank = UserID.from_string(\"@1234ABCD:test\")\n        self.bob = UserID.from_string(\"@4567:test\")\n        self.alice = UserID.from_string(\"@alice:remote\")\n\n        yield defer.ensureDeferred(self.store.create_profile(self.frank.localpart))\n\n        self.handler = hs.get_profile_handler()\n        self.hs = hs\n\n    @defer.inlineCallbacks\n    def test_get_my_name(self):\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(self.frank.localpart, \"Frank\")\n        )\n\n        displayname = yield defer.ensureDeferred(\n            self.handler.get_displayname(self.frank)\n        )\n\n        self.assertEquals(\"Frank\", displayname)\n\n    @defer.inlineCallbacks\n    def test_set_my_name(self):\n        yield defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank Jr.\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank Jr.\",\n        )\n\n        # Set displayname again\n        yield defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank\",\n        )\n\n    @defer.inlineCallbacks\n    def test_set_my_name_if_disabled(self):\n        self.hs.config.enable_set_displayname = False\n\n        # Setting displayname for the first time is allowed\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(self.frank.localpart, \"Frank\")\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank\",\n        )\n\n        # Setting displayname a second time is forbidden\n        d = defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank Jr.\"\n            )\n        )\n\n        yield self.assertFailure(d, SynapseError)\n\n    @defer.inlineCallbacks\n    def test_set_my_name_noauth(self):\n        d = defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.bob), \"Frank Jr.\"\n            )\n        )\n\n        yield self.assertFailure(d, AuthError)\n\n    @defer.inlineCallbacks\n    def test_get_other_name(self):\n        self.mock_federation.make_query.return_value = make_awaitable(\n            {\"displayname\": \"Alice\"}\n        )\n\n        displayname = yield defer.ensureDeferred(\n            self.handler.get_displayname(self.alice)\n        )\n\n        self.assertEquals(displayname, \"Alice\")\n        self.mock_federation.make_query.assert_called_with(\n            destination=\"remote\",\n            query_type=\"profile\",\n            args={\"user_id\": \"@alice:remote\", \"field\": \"displayname\"},\n            ignore_backoff=True,\n        )\n\n    @defer.inlineCallbacks\n    def test_incoming_fed_query(self):\n        yield defer.ensureDeferred(self.store.create_profile(\"caroline\"))\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(\"caroline\", \"Caroline\")\n        )\n\n        response = yield defer.ensureDeferred(\n            self.query_handlers[\"profile\"](\n                {\"user_id\": \"@caroline:test\", \"field\": \"displayname\"}\n            )\n        )\n\n        self.assertEquals({\"displayname\": \"Caroline\"}, response)\n\n    @defer.inlineCallbacks\n    def test_get_my_avatar(self):\n        yield defer.ensureDeferred(\n            self.store.set_profile_avatar_url(\n                self.frank.localpart, \"http://my.server/me.png\"\n            )\n        )\n        avatar_url = yield defer.ensureDeferred(self.handler.get_avatar_url(self.frank))\n\n        self.assertEquals(\"http://my.server/me.png\", avatar_url)\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar(self):\n        yield defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/pic.gif\",\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/pic.gif\",\n        )\n\n        # Set avatar again\n        yield defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/me.png\",\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/me.png\",\n        )\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar_if_disabled(self):\n        self.hs.config.enable_set_avatar_url = False\n\n        # Setting displayname for the first time is allowed\n        yield defer.ensureDeferred(\n            self.store.set_profile_avatar_url(\n                self.frank.localpart, \"http://my.server/me.png\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/me.png\",\n        )\n\n        # Set avatar a second time is forbidden\n        d = defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/pic.gif\",\n            )\n        )\n\n        yield self.assertFailure(d, SynapseError)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom mock import Mock\n\nfrom twisted.internet import defer\n\nimport synapse.types\nfrom synapse.api.errors import AuthError, SynapseError\nfrom synapse.types import UserID\n\nfrom tests import unittest\nfrom tests.test_utils import make_awaitable\nfrom tests.utils import setup_test_homeserver\n\n\nclass ProfileTestCase(unittest.TestCase):\n    \"\"\" Tests profile management. \"\"\"\n\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_federation = Mock()\n        self.mock_registry = Mock()\n\n        self.query_handlers = {}\n\n        def register_query_handler(query_type, handler):\n            self.query_handlers[query_type] = handler\n\n        self.mock_registry.register_query_handler = register_query_handler\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup,\n            http_client=None,\n            resource_for_federation=Mock(),\n            federation_client=self.mock_federation,\n            federation_server=Mock(),\n            federation_registry=self.mock_registry,\n        )\n\n        self.store = hs.get_datastore()\n\n        self.frank = UserID.from_string(\"@1234ABCD:test\")\n        self.bob = UserID.from_string(\"@4567:test\")\n        self.alice = UserID.from_string(\"@alice:remote\")\n\n        yield defer.ensureDeferred(self.store.create_profile(self.frank.localpart))\n\n        self.handler = hs.get_profile_handler()\n        self.hs = hs\n\n    @defer.inlineCallbacks\n    def test_get_my_name(self):\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(self.frank.localpart, \"Frank\")\n        )\n\n        displayname = yield defer.ensureDeferred(\n            self.handler.get_displayname(self.frank)\n        )\n\n        self.assertEquals(\"Frank\", displayname)\n\n    @defer.inlineCallbacks\n    def test_set_my_name(self):\n        yield defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank Jr.\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank Jr.\",\n        )\n\n        # Set displayname again\n        yield defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank\",\n        )\n\n    @defer.inlineCallbacks\n    def test_set_my_name_if_disabled(self):\n        self.hs.config.enable_set_displayname = False\n\n        # Setting displayname for the first time is allowed\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(self.frank.localpart, \"Frank\")\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank\",\n        )\n\n        # Setting displayname a second time is forbidden\n        d = defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank Jr.\"\n            )\n        )\n\n        yield self.assertFailure(d, SynapseError)\n\n    @defer.inlineCallbacks\n    def test_set_my_name_noauth(self):\n        d = defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.bob), \"Frank Jr.\"\n            )\n        )\n\n        yield self.assertFailure(d, AuthError)\n\n    @defer.inlineCallbacks\n    def test_get_other_name(self):\n        self.mock_federation.make_query.return_value = make_awaitable(\n            {\"displayname\": \"Alice\"}\n        )\n\n        displayname = yield defer.ensureDeferred(\n            self.handler.get_displayname(self.alice)\n        )\n\n        self.assertEquals(displayname, \"Alice\")\n        self.mock_federation.make_query.assert_called_with(\n            destination=\"remote\",\n            query_type=\"profile\",\n            args={\"user_id\": \"@alice:remote\", \"field\": \"displayname\"},\n            ignore_backoff=True,\n        )\n\n    @defer.inlineCallbacks\n    def test_incoming_fed_query(self):\n        yield defer.ensureDeferred(self.store.create_profile(\"caroline\"))\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(\"caroline\", \"Caroline\")\n        )\n\n        response = yield defer.ensureDeferred(\n            self.query_handlers[\"profile\"](\n                {\"user_id\": \"@caroline:test\", \"field\": \"displayname\"}\n            )\n        )\n\n        self.assertEquals({\"displayname\": \"Caroline\"}, response)\n\n    @defer.inlineCallbacks\n    def test_get_my_avatar(self):\n        yield defer.ensureDeferred(\n            self.store.set_profile_avatar_url(\n                self.frank.localpart, \"http://my.server/me.png\"\n            )\n        )\n        avatar_url = yield defer.ensureDeferred(self.handler.get_avatar_url(self.frank))\n\n        self.assertEquals(\"http://my.server/me.png\", avatar_url)\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar(self):\n        yield defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/pic.gif\",\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/pic.gif\",\n        )\n\n        # Set avatar again\n        yield defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/me.png\",\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/me.png\",\n        )\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar_if_disabled(self):\n        self.hs.config.enable_set_avatar_url = False\n\n        # Setting displayname for the first time is allowed\n        yield defer.ensureDeferred(\n            self.store.set_profile_avatar_url(\n                self.frank.localpart, \"http://my.server/me.png\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/me.png\",\n        )\n\n        # Set avatar a second time is forbidden\n        d = defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/pic.gif\",\n            )\n        )\n\n        yield self.assertFailure(d, SynapseError)\n", "patch": "@@ -44,7 +44,7 @@ def register_query_handler(query_type, handler):\n \n         hs = yield setup_test_homeserver(\n             self.addCleanup,\n-            http_client=None,\n+            federation_http_client=None,\n             resource_for_federation=Mock(),\n             federation_client=self.mock_federation,\n             federation_server=Mock(),", "file_path": "files/2021_2/38", "file_language": "py", "file_name": "tests/handlers/test_profile.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class ProfileTestCase(unittest.TestCase):\n    \"\"\" Tests profile management. \"\"\"\n\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_federation = Mock()\n        self.mock_registry = Mock()\n\n        self.query_handlers = {}\n\n        def register_query_handler(query_type, handler):\n            self.query_handlers[query_type] = handler\n\n        self.mock_registry.register_query_handler = register_query_handler\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup,\n            http_client=None,\n            resource_for_federation=Mock(),\n            federation_client=self.mock_federation,\n            federation_server=Mock(),\n            federation_registry=self.mock_registry,\n        )\n\n        self.store = hs.get_datastore()\n\n        self.frank = UserID.from_string(\"@1234ABCD:test\")\n        self.bob = UserID.from_string(\"@4567:test\")\n        self.alice = UserID.from_string(\"@alice:remote\")\n\n        yield defer.ensureDeferred(self.store.create_profile(self.frank.localpart))\n\n        self.handler = hs.get_profile_handler()\n        self.hs = hs\n\n    @defer.inlineCallbacks\n    def test_get_my_name(self):\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(self.frank.localpart, \"Frank\")\n        )\n\n        displayname = yield defer.ensureDeferred(\n            self.handler.get_displayname(self.frank)\n        )\n\n        self.assertEquals(\"Frank\", displayname)\n\n    @defer.inlineCallbacks\n    def test_set_my_name(self):\n        yield defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank Jr.\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank Jr.\",\n        )\n\n        # Set displayname again\n        yield defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank\",\n        )\n\n    @defer.inlineCallbacks\n    def test_set_my_name_if_disabled(self):\n        self.hs.config.enable_set_displayname = False\n\n        # Setting displayname for the first time is allowed\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(self.frank.localpart, \"Frank\")\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank\",\n        )\n\n        # Setting displayname a second time is forbidden\n        d = defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank Jr.\"\n            )\n        )\n\n        yield self.assertFailure(d, SynapseError)\n\n    @defer.inlineCallbacks\n    def test_set_my_name_noauth(self):\n        d = defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.bob), \"Frank Jr.\"\n            )\n        )\n\n        yield self.assertFailure(d, AuthError)\n\n    @defer.inlineCallbacks\n    def test_get_other_name(self):\n        self.mock_federation.make_query.return_value = make_awaitable(\n            {\"displayname\": \"Alice\"}\n        )\n\n        displayname = yield defer.ensureDeferred(\n            self.handler.get_displayname(self.alice)\n        )\n\n        self.assertEquals(displayname, \"Alice\")\n        self.mock_federation.make_query.assert_called_with(\n            destination=\"remote\",\n            query_type=\"profile\",\n            args={\"user_id\": \"@alice:remote\", \"field\": \"displayname\"},\n            ignore_backoff=True,\n        )\n\n    @defer.inlineCallbacks\n    def test_incoming_fed_query(self):\n        yield defer.ensureDeferred(self.store.create_profile(\"caroline\"))\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(\"caroline\", \"Caroline\")\n        )\n\n        response = yield defer.ensureDeferred(\n            self.query_handlers[\"profile\"](\n                {\"user_id\": \"@caroline:test\", \"field\": \"displayname\"}\n            )\n        )\n\n        self.assertEquals({\"displayname\": \"Caroline\"}, response)\n\n    @defer.inlineCallbacks\n    def test_get_my_avatar(self):\n        yield defer.ensureDeferred(\n            self.store.set_profile_avatar_url(\n                self.frank.localpart, \"http://my.server/me.png\"\n            )\n        )\n        avatar_url = yield defer.ensureDeferred(self.handler.get_avatar_url(self.frank))\n\n        self.assertEquals(\"http://my.server/me.png\", avatar_url)\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar(self):\n        yield defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/pic.gif\",\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/pic.gif\",\n        )\n\n        # Set avatar again\n        yield defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/me.png\",\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/me.png\",\n        )\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar_if_disabled(self):\n        self.hs.config.enable_set_avatar_url = False\n\n        # Setting displayname for the first time is allowed\n        yield defer.ensureDeferred(\n            self.store.set_profile_avatar_url(\n                self.frank.localpart, \"http://my.server/me.png\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/me.png\",\n        )\n\n        # Set avatar a second time is forbidden\n        d = defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/pic.gif\",\n            )\n        )\n\n        yield self.assertFailure(d, SynapseError)", "target": 0}], "function_after": [{"function": "class ProfileTestCase(unittest.TestCase):\n    \"\"\" Tests profile management. \"\"\"\n\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_federation = Mock()\n        self.mock_registry = Mock()\n\n        self.query_handlers = {}\n\n        def register_query_handler(query_type, handler):\n            self.query_handlers[query_type] = handler\n\n        self.mock_registry.register_query_handler = register_query_handler\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup,\n            federation_http_client=None,\n            resource_for_federation=Mock(),\n            federation_client=self.mock_federation,\n            federation_server=Mock(),\n            federation_registry=self.mock_registry,\n        )\n\n        self.store = hs.get_datastore()\n\n        self.frank = UserID.from_string(\"@1234ABCD:test\")\n        self.bob = UserID.from_string(\"@4567:test\")\n        self.alice = UserID.from_string(\"@alice:remote\")\n\n        yield defer.ensureDeferred(self.store.create_profile(self.frank.localpart))\n\n        self.handler = hs.get_profile_handler()\n        self.hs = hs\n\n    @defer.inlineCallbacks\n    def test_get_my_name(self):\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(self.frank.localpart, \"Frank\")\n        )\n\n        displayname = yield defer.ensureDeferred(\n            self.handler.get_displayname(self.frank)\n        )\n\n        self.assertEquals(\"Frank\", displayname)\n\n    @defer.inlineCallbacks\n    def test_set_my_name(self):\n        yield defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank Jr.\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank Jr.\",\n        )\n\n        # Set displayname again\n        yield defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank\",\n        )\n\n    @defer.inlineCallbacks\n    def test_set_my_name_if_disabled(self):\n        self.hs.config.enable_set_displayname = False\n\n        # Setting displayname for the first time is allowed\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(self.frank.localpart, \"Frank\")\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_displayname(self.frank.localpart)\n                )\n            ),\n            \"Frank\",\n        )\n\n        # Setting displayname a second time is forbidden\n        d = defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.frank), \"Frank Jr.\"\n            )\n        )\n\n        yield self.assertFailure(d, SynapseError)\n\n    @defer.inlineCallbacks\n    def test_set_my_name_noauth(self):\n        d = defer.ensureDeferred(\n            self.handler.set_displayname(\n                self.frank, synapse.types.create_requester(self.bob), \"Frank Jr.\"\n            )\n        )\n\n        yield self.assertFailure(d, AuthError)\n\n    @defer.inlineCallbacks\n    def test_get_other_name(self):\n        self.mock_federation.make_query.return_value = make_awaitable(\n            {\"displayname\": \"Alice\"}\n        )\n\n        displayname = yield defer.ensureDeferred(\n            self.handler.get_displayname(self.alice)\n        )\n\n        self.assertEquals(displayname, \"Alice\")\n        self.mock_federation.make_query.assert_called_with(\n            destination=\"remote\",\n            query_type=\"profile\",\n            args={\"user_id\": \"@alice:remote\", \"field\": \"displayname\"},\n            ignore_backoff=True,\n        )\n\n    @defer.inlineCallbacks\n    def test_incoming_fed_query(self):\n        yield defer.ensureDeferred(self.store.create_profile(\"caroline\"))\n        yield defer.ensureDeferred(\n            self.store.set_profile_displayname(\"caroline\", \"Caroline\")\n        )\n\n        response = yield defer.ensureDeferred(\n            self.query_handlers[\"profile\"](\n                {\"user_id\": \"@caroline:test\", \"field\": \"displayname\"}\n            )\n        )\n\n        self.assertEquals({\"displayname\": \"Caroline\"}, response)\n\n    @defer.inlineCallbacks\n    def test_get_my_avatar(self):\n        yield defer.ensureDeferred(\n            self.store.set_profile_avatar_url(\n                self.frank.localpart, \"http://my.server/me.png\"\n            )\n        )\n        avatar_url = yield defer.ensureDeferred(self.handler.get_avatar_url(self.frank))\n\n        self.assertEquals(\"http://my.server/me.png\", avatar_url)\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar(self):\n        yield defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/pic.gif\",\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/pic.gif\",\n        )\n\n        # Set avatar again\n        yield defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/me.png\",\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/me.png\",\n        )\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar_if_disabled(self):\n        self.hs.config.enable_set_avatar_url = False\n\n        # Setting displayname for the first time is allowed\n        yield defer.ensureDeferred(\n            self.store.set_profile_avatar_url(\n                self.frank.localpart, \"http://my.server/me.png\"\n            )\n        )\n\n        self.assertEquals(\n            (\n                yield defer.ensureDeferred(\n                    self.store.get_profile_avatar_url(self.frank.localpart)\n                )\n            ),\n            \"http://my.server/me.png\",\n        )\n\n        # Set avatar a second time is forbidden\n        d = defer.ensureDeferred(\n            self.handler.set_avatar_url(\n                self.frank,\n                synapse.types.create_requester(self.frank),\n                \"http://my.server/pic.gif\",\n            )\n        )\n\n        yield self.assertFailure(d, SynapseError)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fhandlers%2Ftest_typing.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport json\n\nfrom mock import ANY, Mock, call\n\nfrom twisted.internet import defer\n\nfrom synapse.api.errors import AuthError\nfrom synapse.types import UserID, create_requester\n\nfrom tests import unittest\nfrom tests.test_utils import make_awaitable\nfrom tests.unittest import override_config\nfrom tests.utils import register_federation_servlets\n\n# Some local users to test with\nU_APPLE = UserID.from_string(\"@apple:test\")\nU_BANANA = UserID.from_string(\"@banana:test\")\n\n# Remote user\nU_ONION = UserID.from_string(\"@onion:farm\")\n\n# Test room id\nROOM_ID = \"a-room\"\n\n\ndef _expect_edu_transaction(edu_type, content, origin=\"test\"):\n    return {\n        \"origin\": origin,\n        \"origin_server_ts\": 1000000,\n        \"pdus\": [],\n        \"edus\": [{\"edu_type\": edu_type, \"content\": content}],\n    }\n\n\ndef _make_edu_transaction_json(edu_type, content):\n    return json.dumps(_expect_edu_transaction(edu_type, content)).encode(\"utf8\")\n\n\nclass TypingNotificationsTestCase(unittest.HomeserverTestCase):\n    servlets = [register_federation_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        # we mock out the keyring so as to skip the authentication check on the\n        # federation API call.\n        mock_keyring = Mock(spec=[\"verify_json_for_server\"])\n        mock_keyring.verify_json_for_server.return_value = defer.succeed(True)\n\n        # we mock out the federation client too\n        mock_federation_client = Mock(spec=[\"put_json\"])\n        mock_federation_client.put_json.return_value = defer.succeed((200, \"OK\"))\n\n        # the tests assume that we are starting at unix time 1000\n        reactor.pump((1000,))\n\n        hs = self.setup_test_homeserver(\n            notifier=Mock(),\n            federation_http_client=mock_federation_client,\n            keyring=mock_keyring,\n            replication_streams={},\n        )\n\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        mock_notifier = hs.get_notifier()\n        self.on_new_event = mock_notifier.on_new_event\n\n        self.handler = hs.get_typing_handler()\n\n        self.event_source = hs.get_event_sources().sources[\"typing\"]\n\n        self.datastore = hs.get_datastore()\n        retry_timings_res = {\n            \"destination\": \"\",\n            \"retry_last_ts\": 0,\n            \"retry_interval\": 0,\n            \"failure_ts\": None,\n        }\n        self.datastore.get_destination_retry_timings = Mock(\n            return_value=defer.succeed(retry_timings_res)\n        )\n\n        self.datastore.get_device_updates_by_remote = Mock(\n            return_value=make_awaitable((0, []))\n        )\n\n        self.datastore.get_destination_last_successful_stream_ordering = Mock(\n            return_value=make_awaitable(None)\n        )\n\n        def get_received_txn_response(*args):\n            return defer.succeed(None)\n\n        self.datastore.get_received_txn_response = get_received_txn_response\n\n        self.room_members = []\n\n        async def check_user_in_room(room_id, user_id):\n            if user_id not in [u.to_string() for u in self.room_members]:\n                raise AuthError(401, \"User is not in the room\")\n            return None\n\n        hs.get_auth().check_user_in_room = check_user_in_room\n\n        def get_joined_hosts_for_room(room_id):\n            return {member.domain for member in self.room_members}\n\n        self.datastore.get_joined_hosts_for_room = get_joined_hosts_for_room\n\n        async def get_users_in_room(room_id):\n            return {str(u) for u in self.room_members}\n\n        self.datastore.get_users_in_room = get_users_in_room\n\n        self.datastore.get_user_directory_stream_pos = Mock(\n            side_effect=(\n                # we deliberately return a non-None stream pos to avoid doing an initial_spam\n                lambda: make_awaitable(1)\n            )\n        )\n\n        self.datastore.get_current_state_deltas = Mock(return_value=(0, None))\n\n        self.datastore.get_to_device_stream_token = lambda: 0\n        self.datastore.get_new_device_msgs_for_remote = lambda *args, **kargs: make_awaitable(\n            ([], 0)\n        )\n        self.datastore.delete_device_msgs_for_remote = lambda *args, **kargs: make_awaitable(\n            None\n        )\n        self.datastore.set_received_txn_response = lambda *args, **kwargs: make_awaitable(\n            None\n        )\n\n    def test_started_typing_local(self):\n        self.room_members = [U_APPLE, U_BANANA]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=20000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )\n\n    @override_config({\"send_federation\": True})\n    def test_started_typing_remote_send(self):\n        self.room_members = [U_APPLE, U_ONION]\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=20000,\n            )\n        )\n\n        put_json = self.hs.get_federation_http_client().put_json\n        put_json.assert_called_once_with(\n            \"farm\",\n            path=\"/_matrix/federation/v1/send/1000000\",\n            data=_expect_edu_transaction(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_APPLE.to_string(),\n                    \"typing\": True,\n                },\n            ),\n            json_data_callback=ANY,\n            long_retries=True,\n            backoff_on_404=True,\n            try_trailing_slash_on_400=True,\n        )\n\n    def test_started_typing_remote_recv(self):\n        self.room_members = [U_APPLE, U_ONION]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        (request, channel) = self.make_request(\n            \"PUT\",\n            \"/_matrix/federation/v1/send/1000000\",\n            _make_edu_transaction_json(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_ONION.to_string(),\n                    \"typing\": True,\n                },\n            ),\n            federation_auth_origin=b\"farm\",\n        )\n        self.assertEqual(channel.code, 200)\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_ONION.to_string()]},\n                }\n            ],\n        )\n\n    @override_config({\"send_federation\": True})\n    def test_stopped_typing(self):\n        self.room_members = [U_APPLE, U_BANANA, U_ONION]\n\n        # Gut-wrenching\n        from synapse.handlers.typing import RoomMember\n\n        member = RoomMember(ROOM_ID, U_APPLE.to_string())\n        self.handler._member_typing_until[member] = 1002000\n        self.handler._room_typing[ROOM_ID] = {U_APPLE.to_string()}\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.stopped_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        put_json = self.hs.get_federation_http_client().put_json\n        put_json.assert_called_once_with(\n            \"farm\",\n            path=\"/_matrix/federation/v1/send/1000000\",\n            data=_expect_edu_transaction(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_APPLE.to_string(),\n                    \"typing\": False,\n                },\n            ),\n            json_data_callback=ANY,\n            long_retries=True,\n            backoff_on_404=True,\n            try_trailing_slash_on_400=True,\n        )\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [{\"type\": \"m.typing\", \"room_id\": ROOM_ID, \"content\": {\"user_ids\": []}}],\n        )\n\n    def test_typing_timeout(self):\n        self.room_members = [U_APPLE, U_BANANA]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=10000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n        self.on_new_event.reset_mock()\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )\n\n        self.reactor.pump([16])\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 2, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 2)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=1)\n        )\n        self.assertEquals(\n            events[0],\n            [{\"type\": \"m.typing\", \"room_id\": ROOM_ID, \"content\": {\"user_ids\": []}}],\n        )\n\n        # SYN-230 - see if we can still set after timeout\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=10000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 3, rooms=[ROOM_ID])])\n        self.on_new_event.reset_mock()\n\n        self.assertEquals(self.event_source.get_current_key(), 3)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport json\n\nfrom mock import ANY, Mock, call\n\nfrom twisted.internet import defer\n\nfrom synapse.api.errors import AuthError\nfrom synapse.types import UserID, create_requester\n\nfrom tests import unittest\nfrom tests.test_utils import make_awaitable\nfrom tests.unittest import override_config\nfrom tests.utils import register_federation_servlets\n\n# Some local users to test with\nU_APPLE = UserID.from_string(\"@apple:test\")\nU_BANANA = UserID.from_string(\"@banana:test\")\n\n# Remote user\nU_ONION = UserID.from_string(\"@onion:farm\")\n\n# Test room id\nROOM_ID = \"a-room\"\n\n\ndef _expect_edu_transaction(edu_type, content, origin=\"test\"):\n    return {\n        \"origin\": origin,\n        \"origin_server_ts\": 1000000,\n        \"pdus\": [],\n        \"edus\": [{\"edu_type\": edu_type, \"content\": content}],\n    }\n\n\ndef _make_edu_transaction_json(edu_type, content):\n    return json.dumps(_expect_edu_transaction(edu_type, content)).encode(\"utf8\")\n\n\nclass TypingNotificationsTestCase(unittest.HomeserverTestCase):\n    servlets = [register_federation_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        # we mock out the keyring so as to skip the authentication check on the\n        # federation API call.\n        mock_keyring = Mock(spec=[\"verify_json_for_server\"])\n        mock_keyring.verify_json_for_server.return_value = defer.succeed(True)\n\n        # we mock out the federation client too\n        mock_federation_client = Mock(spec=[\"put_json\"])\n        mock_federation_client.put_json.return_value = defer.succeed((200, \"OK\"))\n\n        # the tests assume that we are starting at unix time 1000\n        reactor.pump((1000,))\n\n        hs = self.setup_test_homeserver(\n            notifier=Mock(),\n            http_client=mock_federation_client,\n            keyring=mock_keyring,\n            replication_streams={},\n        )\n\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        mock_notifier = hs.get_notifier()\n        self.on_new_event = mock_notifier.on_new_event\n\n        self.handler = hs.get_typing_handler()\n\n        self.event_source = hs.get_event_sources().sources[\"typing\"]\n\n        self.datastore = hs.get_datastore()\n        retry_timings_res = {\n            \"destination\": \"\",\n            \"retry_last_ts\": 0,\n            \"retry_interval\": 0,\n            \"failure_ts\": None,\n        }\n        self.datastore.get_destination_retry_timings = Mock(\n            return_value=defer.succeed(retry_timings_res)\n        )\n\n        self.datastore.get_device_updates_by_remote = Mock(\n            return_value=make_awaitable((0, []))\n        )\n\n        self.datastore.get_destination_last_successful_stream_ordering = Mock(\n            return_value=make_awaitable(None)\n        )\n\n        def get_received_txn_response(*args):\n            return defer.succeed(None)\n\n        self.datastore.get_received_txn_response = get_received_txn_response\n\n        self.room_members = []\n\n        async def check_user_in_room(room_id, user_id):\n            if user_id not in [u.to_string() for u in self.room_members]:\n                raise AuthError(401, \"User is not in the room\")\n            return None\n\n        hs.get_auth().check_user_in_room = check_user_in_room\n\n        def get_joined_hosts_for_room(room_id):\n            return {member.domain for member in self.room_members}\n\n        self.datastore.get_joined_hosts_for_room = get_joined_hosts_for_room\n\n        async def get_users_in_room(room_id):\n            return {str(u) for u in self.room_members}\n\n        self.datastore.get_users_in_room = get_users_in_room\n\n        self.datastore.get_user_directory_stream_pos = Mock(\n            side_effect=(\n                # we deliberately return a non-None stream pos to avoid doing an initial_spam\n                lambda: make_awaitable(1)\n            )\n        )\n\n        self.datastore.get_current_state_deltas = Mock(return_value=(0, None))\n\n        self.datastore.get_to_device_stream_token = lambda: 0\n        self.datastore.get_new_device_msgs_for_remote = lambda *args, **kargs: make_awaitable(\n            ([], 0)\n        )\n        self.datastore.delete_device_msgs_for_remote = lambda *args, **kargs: make_awaitable(\n            None\n        )\n        self.datastore.set_received_txn_response = lambda *args, **kwargs: make_awaitable(\n            None\n        )\n\n    def test_started_typing_local(self):\n        self.room_members = [U_APPLE, U_BANANA]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=20000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )\n\n    @override_config({\"send_federation\": True})\n    def test_started_typing_remote_send(self):\n        self.room_members = [U_APPLE, U_ONION]\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=20000,\n            )\n        )\n\n        put_json = self.hs.get_http_client().put_json\n        put_json.assert_called_once_with(\n            \"farm\",\n            path=\"/_matrix/federation/v1/send/1000000\",\n            data=_expect_edu_transaction(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_APPLE.to_string(),\n                    \"typing\": True,\n                },\n            ),\n            json_data_callback=ANY,\n            long_retries=True,\n            backoff_on_404=True,\n            try_trailing_slash_on_400=True,\n        )\n\n    def test_started_typing_remote_recv(self):\n        self.room_members = [U_APPLE, U_ONION]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        (request, channel) = self.make_request(\n            \"PUT\",\n            \"/_matrix/federation/v1/send/1000000\",\n            _make_edu_transaction_json(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_ONION.to_string(),\n                    \"typing\": True,\n                },\n            ),\n            federation_auth_origin=b\"farm\",\n        )\n        self.assertEqual(channel.code, 200)\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_ONION.to_string()]},\n                }\n            ],\n        )\n\n    @override_config({\"send_federation\": True})\n    def test_stopped_typing(self):\n        self.room_members = [U_APPLE, U_BANANA, U_ONION]\n\n        # Gut-wrenching\n        from synapse.handlers.typing import RoomMember\n\n        member = RoomMember(ROOM_ID, U_APPLE.to_string())\n        self.handler._member_typing_until[member] = 1002000\n        self.handler._room_typing[ROOM_ID] = {U_APPLE.to_string()}\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.stopped_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        put_json = self.hs.get_http_client().put_json\n        put_json.assert_called_once_with(\n            \"farm\",\n            path=\"/_matrix/federation/v1/send/1000000\",\n            data=_expect_edu_transaction(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_APPLE.to_string(),\n                    \"typing\": False,\n                },\n            ),\n            json_data_callback=ANY,\n            long_retries=True,\n            backoff_on_404=True,\n            try_trailing_slash_on_400=True,\n        )\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [{\"type\": \"m.typing\", \"room_id\": ROOM_ID, \"content\": {\"user_ids\": []}}],\n        )\n\n    def test_typing_timeout(self):\n        self.room_members = [U_APPLE, U_BANANA]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=10000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n        self.on_new_event.reset_mock()\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )\n\n        self.reactor.pump([16])\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 2, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 2)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=1)\n        )\n        self.assertEquals(\n            events[0],\n            [{\"type\": \"m.typing\", \"room_id\": ROOM_ID, \"content\": {\"user_ids\": []}}],\n        )\n\n        # SYN-230 - see if we can still set after timeout\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=10000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 3, rooms=[ROOM_ID])])\n        self.on_new_event.reset_mock()\n\n        self.assertEquals(self.event_source.get_current_key(), 3)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )\n", "patch": "@@ -70,7 +70,7 @@ def make_homeserver(self, reactor, clock):\n \n         hs = self.setup_test_homeserver(\n             notifier=Mock(),\n-            http_client=mock_federation_client,\n+            federation_http_client=mock_federation_client,\n             keyring=mock_keyring,\n             replication_streams={},\n         )\n@@ -192,7 +192,7 @@ def test_started_typing_remote_send(self):\n             )\n         )\n \n-        put_json = self.hs.get_http_client().put_json\n+        put_json = self.hs.get_federation_http_client().put_json\n         put_json.assert_called_once_with(\n             \"farm\",\n             path=\"/_matrix/federation/v1/send/1000000\",\n@@ -270,7 +270,7 @@ def test_stopped_typing(self):\n \n         self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n \n-        put_json = self.hs.get_http_client().put_json\n+        put_json = self.hs.get_federation_http_client().put_json\n         put_json.assert_called_once_with(\n             \"farm\",\n             path=\"/_matrix/federation/v1/send/1000000\",", "file_path": "files/2021_2/39", "file_language": "py", "file_name": "tests/handlers/test_typing.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 1, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "def _expect_edu_transaction(edu_type, content, origin=\"test\"):\n    return {\n        \"origin\": origin,\n        \"origin_server_ts\": 1000000,\n        \"pdus\": [],\n        \"edus\": [{\"edu_type\": edu_type, \"content\": content}],\n    }", "target": 0}, {"function": "def _make_edu_transaction_json(edu_type, content):\n    return json.dumps(_expect_edu_transaction(edu_type, content)).encode(\"utf8\")", "target": 0}, {"function": "class TypingNotificationsTestCase(unittest.HomeserverTestCase):\n    servlets = [register_federation_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        # we mock out the keyring so as to skip the authentication check on the\n        # federation API call.\n        mock_keyring = Mock(spec=[\"verify_json_for_server\"])\n        mock_keyring.verify_json_for_server.return_value = defer.succeed(True)\n\n        # we mock out the federation client too\n        mock_federation_client = Mock(spec=[\"put_json\"])\n        mock_federation_client.put_json.return_value = defer.succeed((200, \"OK\"))\n\n        # the tests assume that we are starting at unix time 1000\n        reactor.pump((1000,))\n\n        hs = self.setup_test_homeserver(\n            notifier=Mock(),\n            http_client=mock_federation_client,\n            keyring=mock_keyring,\n            replication_streams={},\n        )\n\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        mock_notifier = hs.get_notifier()\n        self.on_new_event = mock_notifier.on_new_event\n\n        self.handler = hs.get_typing_handler()\n\n        self.event_source = hs.get_event_sources().sources[\"typing\"]\n\n        self.datastore = hs.get_datastore()\n        retry_timings_res = {\n            \"destination\": \"\",\n            \"retry_last_ts\": 0,\n            \"retry_interval\": 0,\n            \"failure_ts\": None,\n        }\n        self.datastore.get_destination_retry_timings = Mock(\n            return_value=defer.succeed(retry_timings_res)\n        )\n\n        self.datastore.get_device_updates_by_remote = Mock(\n            return_value=make_awaitable((0, []))\n        )\n\n        self.datastore.get_destination_last_successful_stream_ordering = Mock(\n            return_value=make_awaitable(None)\n        )\n\n        def get_received_txn_response(*args):\n            return defer.succeed(None)\n\n        self.datastore.get_received_txn_response = get_received_txn_response\n\n        self.room_members = []\n\n        async def check_user_in_room(room_id, user_id):\n            if user_id not in [u.to_string() for u in self.room_members]:\n                raise AuthError(401, \"User is not in the room\")\n            return None\n\n        hs.get_auth().check_user_in_room = check_user_in_room\n\n        def get_joined_hosts_for_room(room_id):\n            return {member.domain for member in self.room_members}\n\n        self.datastore.get_joined_hosts_for_room = get_joined_hosts_for_room\n\n        async def get_users_in_room(room_id):\n            return {str(u) for u in self.room_members}\n\n        self.datastore.get_users_in_room = get_users_in_room\n\n        self.datastore.get_user_directory_stream_pos = Mock(\n            side_effect=(\n                # we deliberately return a non-None stream pos to avoid doing an initial_spam\n                lambda: make_awaitable(1)\n            )\n        )\n\n        self.datastore.get_current_state_deltas = Mock(return_value=(0, None))\n\n        self.datastore.get_to_device_stream_token = lambda: 0\n        self.datastore.get_new_device_msgs_for_remote = lambda *args, **kargs: make_awaitable(\n            ([], 0)\n        )\n        self.datastore.delete_device_msgs_for_remote = lambda *args, **kargs: make_awaitable(\n            None\n        )\n        self.datastore.set_received_txn_response = lambda *args, **kwargs: make_awaitable(\n            None\n        )\n\n    def test_started_typing_local(self):\n        self.room_members = [U_APPLE, U_BANANA]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=20000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )\n\n    @override_config({\"send_federation\": True})\n    def test_started_typing_remote_send(self):\n        self.room_members = [U_APPLE, U_ONION]\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=20000,\n            )\n        )\n\n        put_json = self.hs.get_http_client().put_json\n        put_json.assert_called_once_with(\n            \"farm\",\n            path=\"/_matrix/federation/v1/send/1000000\",\n            data=_expect_edu_transaction(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_APPLE.to_string(),\n                    \"typing\": True,\n                },\n            ),\n            json_data_callback=ANY,\n            long_retries=True,\n            backoff_on_404=True,\n            try_trailing_slash_on_400=True,\n        )\n\n    def test_started_typing_remote_recv(self):\n        self.room_members = [U_APPLE, U_ONION]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        (request, channel) = self.make_request(\n            \"PUT\",\n            \"/_matrix/federation/v1/send/1000000\",\n            _make_edu_transaction_json(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_ONION.to_string(),\n                    \"typing\": True,\n                },\n            ),\n            federation_auth_origin=b\"farm\",\n        )\n        self.assertEqual(channel.code, 200)\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_ONION.to_string()]},\n                }\n            ],\n        )\n\n    @override_config({\"send_federation\": True})\n    def test_stopped_typing(self):\n        self.room_members = [U_APPLE, U_BANANA, U_ONION]\n\n        # Gut-wrenching\n        from synapse.handlers.typing import RoomMember\n\n        member = RoomMember(ROOM_ID, U_APPLE.to_string())\n        self.handler._member_typing_until[member] = 1002000\n        self.handler._room_typing[ROOM_ID] = {U_APPLE.to_string()}\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.stopped_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        put_json = self.hs.get_http_client().put_json\n        put_json.assert_called_once_with(\n            \"farm\",\n            path=\"/_matrix/federation/v1/send/1000000\",\n            data=_expect_edu_transaction(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_APPLE.to_string(),\n                    \"typing\": False,\n                },\n            ),\n            json_data_callback=ANY,\n            long_retries=True,\n            backoff_on_404=True,\n            try_trailing_slash_on_400=True,\n        )\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [{\"type\": \"m.typing\", \"room_id\": ROOM_ID, \"content\": {\"user_ids\": []}}],\n        )\n\n    def test_typing_timeout(self):\n        self.room_members = [U_APPLE, U_BANANA]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=10000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n        self.on_new_event.reset_mock()\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )\n\n        self.reactor.pump([16])\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 2, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 2)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=1)\n        )\n        self.assertEquals(\n            events[0],\n            [{\"type\": \"m.typing\", \"room_id\": ROOM_ID, \"content\": {\"user_ids\": []}}],\n        )\n\n        # SYN-230 - see if we can still set after timeout\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=10000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 3, rooms=[ROOM_ID])])\n        self.on_new_event.reset_mock()\n\n        self.assertEquals(self.event_source.get_current_key(), 3)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )", "target": 0}], "function_after": [{"function": "def _expect_edu_transaction(edu_type, content, origin=\"test\"):\n    return {\n        \"origin\": origin,\n        \"origin_server_ts\": 1000000,\n        \"pdus\": [],\n        \"edus\": [{\"edu_type\": edu_type, \"content\": content}],\n    }", "target": 0}, {"function": "def _make_edu_transaction_json(edu_type, content):\n    return json.dumps(_expect_edu_transaction(edu_type, content)).encode(\"utf8\")", "target": 0}, {"function": "class TypingNotificationsTestCase(unittest.HomeserverTestCase):\n    servlets = [register_federation_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        # we mock out the keyring so as to skip the authentication check on the\n        # federation API call.\n        mock_keyring = Mock(spec=[\"verify_json_for_server\"])\n        mock_keyring.verify_json_for_server.return_value = defer.succeed(True)\n\n        # we mock out the federation client too\n        mock_federation_client = Mock(spec=[\"put_json\"])\n        mock_federation_client.put_json.return_value = defer.succeed((200, \"OK\"))\n\n        # the tests assume that we are starting at unix time 1000\n        reactor.pump((1000,))\n\n        hs = self.setup_test_homeserver(\n            notifier=Mock(),\n            federation_http_client=mock_federation_client,\n            keyring=mock_keyring,\n            replication_streams={},\n        )\n\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        mock_notifier = hs.get_notifier()\n        self.on_new_event = mock_notifier.on_new_event\n\n        self.handler = hs.get_typing_handler()\n\n        self.event_source = hs.get_event_sources().sources[\"typing\"]\n\n        self.datastore = hs.get_datastore()\n        retry_timings_res = {\n            \"destination\": \"\",\n            \"retry_last_ts\": 0,\n            \"retry_interval\": 0,\n            \"failure_ts\": None,\n        }\n        self.datastore.get_destination_retry_timings = Mock(\n            return_value=defer.succeed(retry_timings_res)\n        )\n\n        self.datastore.get_device_updates_by_remote = Mock(\n            return_value=make_awaitable((0, []))\n        )\n\n        self.datastore.get_destination_last_successful_stream_ordering = Mock(\n            return_value=make_awaitable(None)\n        )\n\n        def get_received_txn_response(*args):\n            return defer.succeed(None)\n\n        self.datastore.get_received_txn_response = get_received_txn_response\n\n        self.room_members = []\n\n        async def check_user_in_room(room_id, user_id):\n            if user_id not in [u.to_string() for u in self.room_members]:\n                raise AuthError(401, \"User is not in the room\")\n            return None\n\n        hs.get_auth().check_user_in_room = check_user_in_room\n\n        def get_joined_hosts_for_room(room_id):\n            return {member.domain for member in self.room_members}\n\n        self.datastore.get_joined_hosts_for_room = get_joined_hosts_for_room\n\n        async def get_users_in_room(room_id):\n            return {str(u) for u in self.room_members}\n\n        self.datastore.get_users_in_room = get_users_in_room\n\n        self.datastore.get_user_directory_stream_pos = Mock(\n            side_effect=(\n                # we deliberately return a non-None stream pos to avoid doing an initial_spam\n                lambda: make_awaitable(1)\n            )\n        )\n\n        self.datastore.get_current_state_deltas = Mock(return_value=(0, None))\n\n        self.datastore.get_to_device_stream_token = lambda: 0\n        self.datastore.get_new_device_msgs_for_remote = lambda *args, **kargs: make_awaitable(\n            ([], 0)\n        )\n        self.datastore.delete_device_msgs_for_remote = lambda *args, **kargs: make_awaitable(\n            None\n        )\n        self.datastore.set_received_txn_response = lambda *args, **kwargs: make_awaitable(\n            None\n        )\n\n    def test_started_typing_local(self):\n        self.room_members = [U_APPLE, U_BANANA]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=20000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )\n\n    @override_config({\"send_federation\": True})\n    def test_started_typing_remote_send(self):\n        self.room_members = [U_APPLE, U_ONION]\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=20000,\n            )\n        )\n\n        put_json = self.hs.get_federation_http_client().put_json\n        put_json.assert_called_once_with(\n            \"farm\",\n            path=\"/_matrix/federation/v1/send/1000000\",\n            data=_expect_edu_transaction(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_APPLE.to_string(),\n                    \"typing\": True,\n                },\n            ),\n            json_data_callback=ANY,\n            long_retries=True,\n            backoff_on_404=True,\n            try_trailing_slash_on_400=True,\n        )\n\n    def test_started_typing_remote_recv(self):\n        self.room_members = [U_APPLE, U_ONION]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        (request, channel) = self.make_request(\n            \"PUT\",\n            \"/_matrix/federation/v1/send/1000000\",\n            _make_edu_transaction_json(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_ONION.to_string(),\n                    \"typing\": True,\n                },\n            ),\n            federation_auth_origin=b\"farm\",\n        )\n        self.assertEqual(channel.code, 200)\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_ONION.to_string()]},\n                }\n            ],\n        )\n\n    @override_config({\"send_federation\": True})\n    def test_stopped_typing(self):\n        self.room_members = [U_APPLE, U_BANANA, U_ONION]\n\n        # Gut-wrenching\n        from synapse.handlers.typing import RoomMember\n\n        member = RoomMember(ROOM_ID, U_APPLE.to_string())\n        self.handler._member_typing_until[member] = 1002000\n        self.handler._room_typing[ROOM_ID] = {U_APPLE.to_string()}\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.stopped_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n\n        put_json = self.hs.get_federation_http_client().put_json\n        put_json.assert_called_once_with(\n            \"farm\",\n            path=\"/_matrix/federation/v1/send/1000000\",\n            data=_expect_edu_transaction(\n                \"m.typing\",\n                content={\n                    \"room_id\": ROOM_ID,\n                    \"user_id\": U_APPLE.to_string(),\n                    \"typing\": False,\n                },\n            ),\n            json_data_callback=ANY,\n            long_retries=True,\n            backoff_on_404=True,\n            try_trailing_slash_on_400=True,\n        )\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [{\"type\": \"m.typing\", \"room_id\": ROOM_ID, \"content\": {\"user_ids\": []}}],\n        )\n\n    def test_typing_timeout(self):\n        self.room_members = [U_APPLE, U_BANANA]\n\n        self.assertEquals(self.event_source.get_current_key(), 0)\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=10000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 1, rooms=[ROOM_ID])])\n        self.on_new_event.reset_mock()\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )\n\n        self.reactor.pump([16])\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 2, rooms=[ROOM_ID])])\n\n        self.assertEquals(self.event_source.get_current_key(), 2)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=1)\n        )\n        self.assertEquals(\n            events[0],\n            [{\"type\": \"m.typing\", \"room_id\": ROOM_ID, \"content\": {\"user_ids\": []}}],\n        )\n\n        # SYN-230 - see if we can still set after timeout\n\n        self.get_success(\n            self.handler.started_typing(\n                target_user=U_APPLE,\n                requester=create_requester(U_APPLE),\n                room_id=ROOM_ID,\n                timeout=10000,\n            )\n        )\n\n        self.on_new_event.assert_has_calls([call(\"typing_key\", 3, rooms=[ROOM_ID])])\n        self.on_new_event.reset_mock()\n\n        self.assertEquals(self.event_source.get_current_key(), 3)\n        events = self.get_success(\n            self.event_source.get_new_events(room_ids=[ROOM_ID], from_key=0)\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": ROOM_ID,\n                    \"content\": {\"user_ids\": [U_APPLE.to_string()]},\n                }\n            ],\n        )", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fhttp%2Ffederation%2Ftest_matrix_federation_agent.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2019 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\n\nfrom mock import Mock\n\nimport treq\nfrom netaddr import IPSet\nfrom service_identity import VerificationError\nfrom zope.interface import implementer\n\nfrom twisted.internet import defer\nfrom twisted.internet._sslverify import ClientTLSOptions, OpenSSLCertificateOptions\nfrom twisted.internet.protocol import Factory\nfrom twisted.protocols.tls import TLSMemoryBIOFactory\nfrom twisted.web._newclient import ResponseNeverReceived\nfrom twisted.web.client import Agent\nfrom twisted.web.http import HTTPChannel\nfrom twisted.web.http_headers import Headers\nfrom twisted.web.iweb import IPolicyForHTTPS\n\nfrom synapse.config.homeserver import HomeServerConfig\nfrom synapse.crypto.context_factory import FederationPolicyForHTTPS\nfrom synapse.http.federation.matrix_federation_agent import MatrixFederationAgent\nfrom synapse.http.federation.srv_resolver import Server\nfrom synapse.http.federation.well_known_resolver import (\n    WellKnownResolver,\n    _cache_period_from_headers,\n)\nfrom synapse.logging.context import SENTINEL_CONTEXT, LoggingContext, current_context\nfrom synapse.util.caches.ttlcache import TTLCache\n\nfrom tests import unittest\nfrom tests.http import TestServerTLSConnectionFactory, get_test_ca_cert_file\nfrom tests.server import FakeTransport, ThreadedMemoryReactorClock\nfrom tests.utils import default_config\n\nlogger = logging.getLogger(__name__)\n\ntest_server_connection_factory = None\n\n\ndef get_connection_factory():\n    # this needs to happen once, but not until we are ready to run the first test\n    global test_server_connection_factory\n    if test_server_connection_factory is None:\n        test_server_connection_factory = TestServerTLSConnectionFactory(\n            sanlist=[\n                b\"DNS:testserv\",\n                b\"DNS:target-server\",\n                b\"DNS:xn--bcher-kva.com\",\n                b\"IP:1.2.3.4\",\n                b\"IP:::1\",\n            ]\n        )\n    return test_server_connection_factory\n\n\n# Once Async Mocks or lambdas are supported this can go away.\ndef generate_resolve_service(result):\n    async def resolve_service(_):\n        return result\n\n    return resolve_service\n\n\nclass MatrixFederationAgentTests(unittest.TestCase):\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n\n        self.mock_resolver = Mock()\n\n        config_dict = default_config(\"test\", parse=False)\n        config_dict[\"federation_custom_ca_list\"] = [get_test_ca_cert_file()]\n\n        self._config = config = HomeServerConfig()\n        config.parse_config_dict(config_dict, \"\", \"\")\n\n        self.tls_factory = FederationPolicyForHTTPS(config)\n\n        self.well_known_cache = TTLCache(\"test_cache\", timer=self.reactor.seconds)\n        self.had_well_known_cache = TTLCache(\"test_cache\", timer=self.reactor.seconds)\n        self.well_known_resolver = WellKnownResolver(\n            self.reactor,\n            Agent(self.reactor, contextFactory=self.tls_factory),\n            b\"test-agent\",\n            well_known_cache=self.well_known_cache,\n            had_well_known_cache=self.had_well_known_cache,\n        )\n\n        self.agent = MatrixFederationAgent(\n            reactor=self.reactor,\n            tls_client_options_factory=self.tls_factory,\n            user_agent=\"test-agent\",  # Note that this is unused since _well_known_resolver is provided.\n            ip_blacklist=IPSet(),\n            _srv_resolver=self.mock_resolver,\n            _well_known_resolver=self.well_known_resolver,\n        )\n\n    def _make_connection(self, client_factory, expected_sni):\n        \"\"\"Builds a test server, and completes the outgoing client connection\n\n        Returns:\n            HTTPChannel: the test server\n        \"\"\"\n\n        # build the test server\n        server_tls_protocol = _build_test_server(get_connection_factory())\n\n        # now, tell the client protocol factory to build the client protocol (it will be a\n        # _WrappingProtocol, around a TLSMemoryBIOProtocol, around an\n        # HTTP11ClientProtocol) and wire the output of said protocol up to the server via\n        # a FakeTransport.\n        #\n        # Normally this would be done by the TCP socket code in Twisted, but we are\n        # stubbing that out here.\n        client_protocol = client_factory.buildProtocol(None)\n        client_protocol.makeConnection(\n            FakeTransport(server_tls_protocol, self.reactor, client_protocol)\n        )\n\n        # tell the server tls protocol to send its stuff back to the client, too\n        server_tls_protocol.makeConnection(\n            FakeTransport(client_protocol, self.reactor, server_tls_protocol)\n        )\n\n        # grab a hold of the TLS connection, in case it gets torn down\n        server_tls_connection = server_tls_protocol._tlsConnection\n\n        # fish the test server back out of the server-side TLS protocol.\n        http_protocol = server_tls_protocol.wrappedProtocol\n\n        # give the reactor a pump to get the TLS juices flowing.\n        self.reactor.pump((0.1,))\n\n        # check the SNI\n        server_name = server_tls_connection.get_servername()\n        self.assertEqual(\n            server_name,\n            expected_sni,\n            \"Expected SNI %s but got %s\" % (expected_sni, server_name),\n        )\n\n        return http_protocol\n\n    @defer.inlineCallbacks\n    def _make_get_request(self, uri):\n        \"\"\"\n        Sends a simple GET request via the agent, and checks its logcontext management\n        \"\"\"\n        with LoggingContext(\"one\") as context:\n            fetch_d = self.agent.request(b\"GET\", uri)\n\n            # Nothing happened yet\n            self.assertNoResult(fetch_d)\n\n            # should have reset logcontext to the sentinel\n            _check_logcontext(SENTINEL_CONTEXT)\n\n            try:\n                fetch_res = yield fetch_d\n                return fetch_res\n            except Exception as e:\n                logger.info(\"Fetch of %s failed: %s\", uri.decode(\"ascii\"), e)\n                raise\n            finally:\n                _check_logcontext(context)\n\n    def _handle_well_known_connection(\n        self, client_factory, expected_sni, content, response_headers={}\n    ):\n        \"\"\"Handle an outgoing HTTPs connection: wire it up to a server, check that the\n        request is for a .well-known, and send the response.\n\n        Args:\n            client_factory (IProtocolFactory): outgoing connection\n            expected_sni (bytes): SNI that we expect the outgoing connection to send\n            content (bytes): content to send back as the .well-known\n        Returns:\n            HTTPChannel: server impl\n        \"\"\"\n        # make the connection for .well-known\n        well_known_server = self._make_connection(\n            client_factory, expected_sni=expected_sni\n        )\n        # check the .well-known request and send a response\n        self.assertEqual(len(well_known_server.requests), 1)\n        request = well_known_server.requests[0]\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"user-agent\"), [b\"test-agent\"]\n        )\n        self._send_well_known_response(request, content, headers=response_headers)\n        return well_known_server\n\n    def _send_well_known_response(self, request, content, headers={}):\n        \"\"\"Check that an incoming request looks like a valid .well-known request, and\n        send back the response.\n        \"\"\"\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/.well-known/matrix/server\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv\"])\n        # send back a response\n        for k, v in headers.items():\n            request.setHeader(k, v)\n        request.write(content)\n        request.finish()\n\n        self.reactor.pump((0.1,))\n\n    def test_get(self):\n        \"\"\"\n        happy-path test of a GET request with an explicit port\n        \"\"\"\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        test_d = self._make_get_request(b\"matrix://testserv:8448/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv:8448\"]\n        )\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"user-agent\"), [b\"test-agent\"]\n        )\n        content = request.content.read()\n        self.assertEqual(content, b\"\")\n\n        # Deferred is still without a result\n        self.assertNoResult(test_d)\n\n        # send the headers\n        request.responseHeaders.setRawHeaders(b\"Content-Type\", [b\"application/json\"])\n        request.write(\"\")\n\n        self.reactor.pump((0.1,))\n\n        response = self.successResultOf(test_d)\n\n        # that should give us a Response object\n        self.assertEqual(response.code, 200)\n\n        # Send the body\n        request.write('{ \"a\": 1 }'.encode(\"ascii\"))\n        request.finish()\n\n        self.reactor.pump((0.1,))\n\n        # check it can be read\n        json = self.successResultOf(treq.json_content(response))\n        self.assertEqual(json, {\"a\": 1})\n\n    def test_get_ip_address(self):\n        \"\"\"\n        Test the behaviour when the server name contains an explicit IP (with no port)\n        \"\"\"\n        # there will be a getaddrinfo on the IP\n        self.reactor.lookups[\"1.2.3.4\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://1.2.3.4/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=None)\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"1.2.3.4\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_ipv6_address(self):\n        \"\"\"\n        Test the behaviour when the server name contains an explicit IPv6 address\n        (with no port)\n        \"\"\"\n\n        # there will be a getaddrinfo on the IP\n        self.reactor.lookups[\"::1\"] = \"::1\"\n\n        test_d = self._make_get_request(b\"matrix://[::1]/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"::1\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=None)\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"[::1]\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_ipv6_address_with_port(self):\n        \"\"\"\n        Test the behaviour when the server name contains an explicit IPv6 address\n        (with explicit port)\n        \"\"\"\n\n        # there will be a getaddrinfo on the IP\n        self.reactor.lookups[\"::1\"] = \"::1\"\n\n        test_d = self._make_get_request(b\"matrix://[::1]:80/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"::1\")\n        self.assertEqual(port, 80)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=None)\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"[::1]:80\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_hostname_bad_cert(self):\n        \"\"\"\n        Test the behaviour when the certificate on the server doesn't match the hostname\n        \"\"\"\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv1\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://testserv1/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # No SRV record lookup yet\n        self.mock_resolver.resolve_service.assert_not_called()\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        # fonx the connection\n        client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n\n        # attemptdelay on the hostnameendpoint is 0.3, so takes that long before the\n        # .well-known request fails.\n        self.reactor.pump((0.4,))\n\n        # now there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv1\"\n        )\n\n        # we should fall back to a direct connection\n        self.assertEqual(len(clients), 2)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[1]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv1\")\n\n        # there should be no requests\n        self.assertEqual(len(http_server.requests), 0)\n\n        # ... and the request should have failed\n        e = self.failureResultOf(test_d, ResponseNeverReceived)\n        failure_reason = e.value.reasons[0]\n        self.assertIsInstance(failure_reason.value, VerificationError)\n\n    def test_get_ip_address_bad_cert(self):\n        \"\"\"\n        Test the behaviour when the server name contains an explicit IP, but\n        the server cert doesn't cover it\n        \"\"\"\n        # there will be a getaddrinfo on the IP\n        self.reactor.lookups[\"1.2.3.5\"] = \"1.2.3.5\"\n\n        test_d = self._make_get_request(b\"matrix://1.2.3.5/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.5\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=None)\n\n        # there should be no requests\n        self.assertEqual(len(http_server.requests), 0)\n\n        # ... and the request should have failed\n        e = self.failureResultOf(test_d, ResponseNeverReceived)\n        failure_reason = e.value.reasons[0]\n        self.assertIsInstance(failure_reason.value, VerificationError)\n\n    def test_get_no_srv_no_well_known(self):\n        \"\"\"\n        Test the behaviour when the server name has no port, no SRV, and no well-known\n        \"\"\"\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # No SRV record lookup yet\n        self.mock_resolver.resolve_service.assert_not_called()\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        # fonx the connection\n        client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n\n        # attemptdelay on the hostnameendpoint is 0.3, so  takes that long before the\n        # .well-known request fails.\n        self.reactor.pump((0.4,))\n\n        # now there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv\"\n        )\n\n        # we should fall back to a direct connection\n        self.assertEqual(len(clients), 2)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[1]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_well_known(self):\n        \"\"\"Test the behaviour when the .well-known delegates elsewhere\n        \"\"\"\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.reactor.lookups[\"target-server\"] = \"1::f\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        self._handle_well_known_connection(\n            client_factory,\n            expected_sni=b\"testserv\",\n            content=b'{ \"m.server\": \"target-server\" }',\n        )\n\n        # there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.target-server\"\n        )\n\n        # now we should get a connection to the target server\n        self.assertEqual(len(clients), 2)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[1]\n        self.assertEqual(host, \"1::f\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(\n            client_factory, expected_sni=b\"target-server\"\n        )\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"target-server\"]\n        )\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n        self.assertEqual(self.well_known_cache[b\"testserv\"], b\"target-server\")\n\n        # check the cache expires\n        self.reactor.pump((48 * 3600,))\n        self.well_known_cache.expire()\n        self.assertNotIn(b\"testserv\", self.well_known_cache)\n\n    def test_get_well_known_redirect(self):\n        \"\"\"Test the behaviour when the server name has no port and no SRV record, but\n        the .well-known has a 300 redirect\n        \"\"\"\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.reactor.lookups[\"target-server\"] = \"1::f\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        redirect_server = self._make_connection(\n            client_factory, expected_sni=b\"testserv\"\n        )\n\n        # send a 302 redirect\n        self.assertEqual(len(redirect_server.requests), 1)\n        request = redirect_server.requests[0]\n        request.redirect(b\"https://testserv/even_better_known\")\n        request.finish()\n\n        self.reactor.pump((0.1,))\n\n        # now there should be another connection\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        well_known_server = self._make_connection(\n            client_factory, expected_sni=b\"testserv\"\n        )\n\n        self.assertEqual(len(well_known_server.requests), 1, \"No request after 302\")\n        request = well_known_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/even_better_known\")\n        request.write(b'{ \"m.server\": \"target-server\" }')\n        request.finish()\n\n        self.reactor.pump((0.1,))\n\n        # there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.target-server\"\n        )\n\n        # now we should get a connection to the target server\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1::f\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(\n            client_factory, expected_sni=b\"target-server\"\n        )\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"target-server\"]\n        )\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n        self.assertEqual(self.well_known_cache[b\"testserv\"], b\"target-server\")\n\n        # check the cache expires\n        self.reactor.pump((48 * 3600,))\n        self.well_known_cache.expire()\n        self.assertNotIn(b\"testserv\", self.well_known_cache)\n\n    def test_get_invalid_well_known(self):\n        \"\"\"\n        Test the behaviour when the server name has an *invalid* well-known (and no SRV)\n        \"\"\"\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # No SRV record lookup yet\n        self.mock_resolver.resolve_service.assert_not_called()\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        self._handle_well_known_connection(\n            client_factory, expected_sni=b\"testserv\", content=b\"NOT JSON\"\n        )\n\n        # now there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv\"\n        )\n\n        # we should fall back to a direct connection\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_well_known_unsigned_cert(self):\n        \"\"\"Test the behaviour when the .well-known server presents a cert\n        not signed by a CA\n        \"\"\"\n\n        # we use the same test server as the other tests, but use an agent with\n        # the config left to the default, which will not trust it (since the\n        # presented cert is signed by a test CA)\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n\n        config = default_config(\"test\", parse=True)\n\n        # Build a new agent and WellKnownResolver with a different tls factory\n        tls_factory = FederationPolicyForHTTPS(config)\n        agent = MatrixFederationAgent(\n            reactor=self.reactor,\n            tls_client_options_factory=tls_factory,\n            user_agent=b\"test-agent\",  # This is unused since _well_known_resolver is passed below.\n            ip_blacklist=IPSet(),\n            _srv_resolver=self.mock_resolver,\n            _well_known_resolver=WellKnownResolver(\n                self.reactor,\n                Agent(self.reactor, contextFactory=tls_factory),\n                b\"test-agent\",\n                well_known_cache=self.well_known_cache,\n                had_well_known_cache=self.had_well_known_cache,\n            ),\n        )\n\n        test_d = agent.request(b\"GET\", b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        http_proto = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        # there should be no requests\n        self.assertEqual(len(http_proto.requests), 0)\n\n        # and there should be a SRV lookup instead\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv\"\n        )\n\n    def test_get_hostname_srv(self):\n        \"\"\"\n        Test the behaviour when there is a single SRV record\n        \"\"\"\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service(\n            [Server(host=b\"srvtarget\", port=8443)]\n        )\n        self.reactor.lookups[\"srvtarget\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # the request for a .well-known will have failed with a DNS lookup error.\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv\"\n        )\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8443)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_well_known_srv(self):\n        \"\"\"Test the behaviour when the .well-known redirects to a place where there\n        is a SRV.\n        \"\"\"\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.reactor.lookups[\"srvtarget\"] = \"5.6.7.8\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service(\n            [Server(host=b\"srvtarget\", port=8443)]\n        )\n\n        self._handle_well_known_connection(\n            client_factory,\n            expected_sni=b\"testserv\",\n            content=b'{ \"m.server\": \"target-server\" }',\n        )\n\n        # there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.target-server\"\n        )\n\n        # now we should get a connection to the target of the SRV record\n        self.assertEqual(len(clients), 2)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[1]\n        self.assertEqual(host, \"5.6.7.8\")\n        self.assertEqual(port, 8443)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(\n            client_factory, expected_sni=b\"target-server\"\n        )\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"target-server\"]\n        )\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_idna_servername(self):\n        \"\"\"test the behaviour when the server name has idna chars in\"\"\"\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n\n        # the resolver is always called with the IDNA hostname as a native string.\n        self.reactor.lookups[\"xn--bcher-kva.com\"] = \"1.2.3.4\"\n\n        # this is idna for b\u00fccher.com\n        test_d = self._make_get_request(b\"matrix://xn--bcher-kva.com/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # No SRV record lookup yet\n        self.mock_resolver.resolve_service.assert_not_called()\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        # fonx the connection\n        client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n\n        # attemptdelay on the hostnameendpoint is 0.3, so  takes that long before the\n        # .well-known request fails.\n        self.reactor.pump((0.4,))\n\n        # now there should have been a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.xn--bcher-kva.com\"\n        )\n\n        # We should fall back to port 8448\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 2)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[1]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(\n            client_factory, expected_sni=b\"xn--bcher-kva.com\"\n        )\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"xn--bcher-kva.com\"]\n        )\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_idna_srv_target(self):\n        \"\"\"test the behaviour when the target of a SRV record has idna chars\"\"\"\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service(\n            [Server(host=b\"xn--trget-3qa.com\", port=8443)]  # t\u00e2rget.com\n        )\n        self.reactor.lookups[\"xn--trget-3qa.com\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://xn--bcher-kva.com/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.xn--bcher-kva.com\"\n        )\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8443)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(\n            client_factory, expected_sni=b\"xn--bcher-kva.com\"\n        )\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"xn--bcher-kva.com\"]\n        )\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_well_known_cache(self):\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        well_known_server = self._handle_well_known_connection(\n            client_factory,\n            expected_sni=b\"testserv\",\n            response_headers={b\"Cache-Control\": b\"max-age=1000\"},\n            content=b'{ \"m.server\": \"target-server\" }',\n        )\n\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, b\"target-server\")\n\n        # close the tcp connection\n        well_known_server.loseConnection()\n\n        # repeat the request: it should hit the cache\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, b\"target-server\")\n\n        # expire the cache\n        self.reactor.pump((1000.0,))\n\n        # now it should connect again\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        self._handle_well_known_connection(\n            client_factory,\n            expected_sni=b\"testserv\",\n            content=b'{ \"m.server\": \"other-server\" }',\n        )\n\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, b\"other-server\")\n\n    def test_well_known_cache_with_temp_failure(self):\n        \"\"\"Test that we refetch well-known before the cache expires, and that\n        it ignores transient errors.\n        \"\"\"\n\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        well_known_server = self._handle_well_known_connection(\n            client_factory,\n            expected_sni=b\"testserv\",\n            response_headers={b\"Cache-Control\": b\"max-age=1000\"},\n            content=b'{ \"m.server\": \"target-server\" }',\n        )\n\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, b\"target-server\")\n\n        # close the tcp connection\n        well_known_server.loseConnection()\n\n        # Get close to the cache expiry, this will cause the resolver to do\n        # another lookup.\n        self.reactor.pump((900.0,))\n\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n\n        # The resolver may retry a few times, so fonx all requests that come along\n        attempts = 0\n        while self.reactor.tcpClients:\n            clients = self.reactor.tcpClients\n            (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n\n            attempts += 1\n\n            # fonx the connection attempt, this will be treated as a temporary\n            # failure.\n            client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n\n            # There's a few sleeps involved, so we have to pump the reactor a\n            # bit.\n            self.reactor.pump((1.0, 1.0))\n\n        # We expect to see more than one attempt as there was previously a valid\n        # well known.\n        self.assertGreater(attempts, 1)\n\n        # Resolver should return cached value, despite the lookup failing.\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, b\"target-server\")\n\n        # Expire both caches and repeat the request\n        self.reactor.pump((10000.0,))\n\n        # Repated the request, this time it should fail if the lookup fails.\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n\n        clients = self.reactor.tcpClients\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n        self.reactor.pump((0.4,))\n\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, None)\n\n    def test_srv_fallbacks(self):\n        \"\"\"Test that other SRV results are tried if the first one fails.\n        \"\"\"\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service(\n            [\n                Server(host=b\"target.com\", port=8443),\n                Server(host=b\"target.com\", port=8444),\n            ]\n        )\n        self.reactor.lookups[\"target.com\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv\"\n        )\n\n        # We should see an attempt to connect to the first server\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8443)\n\n        # Fonx the connection\n        client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n\n        # There's a 300ms delay in HostnameEndpoint\n        self.reactor.pump((0.4,))\n\n        # Hasn't failed yet\n        self.assertNoResult(test_d)\n\n        # We shouldnow see an attempt to connect to the second server\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8444)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n\nclass TestCachePeriodFromHeaders(unittest.TestCase):\n    def test_cache_control(self):\n        # uppercase\n        self.assertEqual(\n            _cache_period_from_headers(\n                Headers({b\"Cache-Control\": [b\"foo, Max-Age = 100, bar\"]})\n            ),\n            100,\n        )\n\n        # missing value\n        self.assertIsNone(\n            _cache_period_from_headers(Headers({b\"Cache-Control\": [b\"max-age=, bar\"]}))\n        )\n\n        # hackernews: bogus due to semicolon\n        self.assertIsNone(\n            _cache_period_from_headers(\n                Headers({b\"Cache-Control\": [b\"private; max-age=0\"]})\n            )\n        )\n\n        # github\n        self.assertEqual(\n            _cache_period_from_headers(\n                Headers({b\"Cache-Control\": [b\"max-age=0, private, must-revalidate\"]})\n            ),\n            0,\n        )\n\n        # google\n        self.assertEqual(\n            _cache_period_from_headers(\n                Headers({b\"cache-control\": [b\"private, max-age=0\"]})\n            ),\n            0,\n        )\n\n    def test_expires(self):\n        self.assertEqual(\n            _cache_period_from_headers(\n                Headers({b\"Expires\": [b\"Wed, 30 Jan 2019 07:35:33 GMT\"]}),\n                time_now=lambda: 1548833700,\n            ),\n            33,\n        )\n\n        # cache-control overrides expires\n        self.assertEqual(\n            _cache_period_from_headers(\n                Headers(\n                    {\n                        b\"cache-control\": [b\"max-age=10\"],\n                        b\"Expires\": [b\"Wed, 30 Jan 2019 07:35:33 GMT\"],\n                    }\n                ),\n                time_now=lambda: 1548833700,\n            ),\n            10,\n        )\n\n        # invalid expires means immediate expiry\n        self.assertEqual(_cache_period_from_headers(Headers({b\"Expires\": [b\"0\"]})), 0)\n\n\ndef _check_logcontext(context):\n    current = current_context()\n    if current is not context:\n        raise AssertionError(\"Expected logcontext %s but was %s\" % (context, current))\n\n\ndef _build_test_server(connection_creator):\n    \"\"\"Construct a test server\n\n    This builds an HTTP channel, wrapped with a TLSMemoryBIOProtocol\n\n    Args:\n        connection_creator (IOpenSSLServerConnectionCreator): thing to build\n            SSL connections\n        sanlist (list[bytes]): list of the SAN entries for the cert returned\n            by the server\n\n    Returns:\n        TLSMemoryBIOProtocol\n    \"\"\"\n    server_factory = Factory.forProtocol(HTTPChannel)\n    # Request.finish expects the factory to have a 'log' method.\n    server_factory.log = _log_request\n\n    server_tls_factory = TLSMemoryBIOFactory(\n        connection_creator, isClient=False, wrappedFactory=server_factory\n    )\n\n    return server_tls_factory.buildProtocol(None)\n\n\ndef _log_request(request):\n    \"\"\"Implements Factory.log, which is expected by Request.finish\"\"\"\n    logger.info(\"Completed request %s\", request)\n\n\n@implementer(IPolicyForHTTPS)\nclass TrustingTLSPolicyForHTTPS:\n    \"\"\"An IPolicyForHTTPS which checks that the certificate belongs to the\n    right server, but doesn't check the certificate chain.\"\"\"\n\n    def creatorForNetloc(self, hostname, port):\n        certificateOptions = OpenSSLCertificateOptions()\n        return ClientTLSOptions(hostname, certificateOptions.getContext())\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2019 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\n\nfrom mock import Mock\n\nimport treq\nfrom service_identity import VerificationError\nfrom zope.interface import implementer\n\nfrom twisted.internet import defer\nfrom twisted.internet._sslverify import ClientTLSOptions, OpenSSLCertificateOptions\nfrom twisted.internet.protocol import Factory\nfrom twisted.protocols.tls import TLSMemoryBIOFactory\nfrom twisted.web._newclient import ResponseNeverReceived\nfrom twisted.web.client import Agent\nfrom twisted.web.http import HTTPChannel\nfrom twisted.web.http_headers import Headers\nfrom twisted.web.iweb import IPolicyForHTTPS\n\nfrom synapse.config.homeserver import HomeServerConfig\nfrom synapse.crypto.context_factory import FederationPolicyForHTTPS\nfrom synapse.http.federation.matrix_federation_agent import MatrixFederationAgent\nfrom synapse.http.federation.srv_resolver import Server\nfrom synapse.http.federation.well_known_resolver import (\n    WellKnownResolver,\n    _cache_period_from_headers,\n)\nfrom synapse.logging.context import SENTINEL_CONTEXT, LoggingContext, current_context\nfrom synapse.util.caches.ttlcache import TTLCache\n\nfrom tests import unittest\nfrom tests.http import TestServerTLSConnectionFactory, get_test_ca_cert_file\nfrom tests.server import FakeTransport, ThreadedMemoryReactorClock\nfrom tests.utils import default_config\n\nlogger = logging.getLogger(__name__)\n\ntest_server_connection_factory = None\n\n\ndef get_connection_factory():\n    # this needs to happen once, but not until we are ready to run the first test\n    global test_server_connection_factory\n    if test_server_connection_factory is None:\n        test_server_connection_factory = TestServerTLSConnectionFactory(\n            sanlist=[\n                b\"DNS:testserv\",\n                b\"DNS:target-server\",\n                b\"DNS:xn--bcher-kva.com\",\n                b\"IP:1.2.3.4\",\n                b\"IP:::1\",\n            ]\n        )\n    return test_server_connection_factory\n\n\n# Once Async Mocks or lambdas are supported this can go away.\ndef generate_resolve_service(result):\n    async def resolve_service(_):\n        return result\n\n    return resolve_service\n\n\nclass MatrixFederationAgentTests(unittest.TestCase):\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n\n        self.mock_resolver = Mock()\n\n        config_dict = default_config(\"test\", parse=False)\n        config_dict[\"federation_custom_ca_list\"] = [get_test_ca_cert_file()]\n\n        self._config = config = HomeServerConfig()\n        config.parse_config_dict(config_dict, \"\", \"\")\n\n        self.tls_factory = FederationPolicyForHTTPS(config)\n\n        self.well_known_cache = TTLCache(\"test_cache\", timer=self.reactor.seconds)\n        self.had_well_known_cache = TTLCache(\"test_cache\", timer=self.reactor.seconds)\n        self.well_known_resolver = WellKnownResolver(\n            self.reactor,\n            Agent(self.reactor, contextFactory=self.tls_factory),\n            b\"test-agent\",\n            well_known_cache=self.well_known_cache,\n            had_well_known_cache=self.had_well_known_cache,\n        )\n\n        self.agent = MatrixFederationAgent(\n            reactor=self.reactor,\n            tls_client_options_factory=self.tls_factory,\n            user_agent=\"test-agent\",  # Note that this is unused since _well_known_resolver is provided.\n            _srv_resolver=self.mock_resolver,\n            _well_known_resolver=self.well_known_resolver,\n        )\n\n    def _make_connection(self, client_factory, expected_sni):\n        \"\"\"Builds a test server, and completes the outgoing client connection\n\n        Returns:\n            HTTPChannel: the test server\n        \"\"\"\n\n        # build the test server\n        server_tls_protocol = _build_test_server(get_connection_factory())\n\n        # now, tell the client protocol factory to build the client protocol (it will be a\n        # _WrappingProtocol, around a TLSMemoryBIOProtocol, around an\n        # HTTP11ClientProtocol) and wire the output of said protocol up to the server via\n        # a FakeTransport.\n        #\n        # Normally this would be done by the TCP socket code in Twisted, but we are\n        # stubbing that out here.\n        client_protocol = client_factory.buildProtocol(None)\n        client_protocol.makeConnection(\n            FakeTransport(server_tls_protocol, self.reactor, client_protocol)\n        )\n\n        # tell the server tls protocol to send its stuff back to the client, too\n        server_tls_protocol.makeConnection(\n            FakeTransport(client_protocol, self.reactor, server_tls_protocol)\n        )\n\n        # grab a hold of the TLS connection, in case it gets torn down\n        server_tls_connection = server_tls_protocol._tlsConnection\n\n        # fish the test server back out of the server-side TLS protocol.\n        http_protocol = server_tls_protocol.wrappedProtocol\n\n        # give the reactor a pump to get the TLS juices flowing.\n        self.reactor.pump((0.1,))\n\n        # check the SNI\n        server_name = server_tls_connection.get_servername()\n        self.assertEqual(\n            server_name,\n            expected_sni,\n            \"Expected SNI %s but got %s\" % (expected_sni, server_name),\n        )\n\n        return http_protocol\n\n    @defer.inlineCallbacks\n    def _make_get_request(self, uri):\n        \"\"\"\n        Sends a simple GET request via the agent, and checks its logcontext management\n        \"\"\"\n        with LoggingContext(\"one\") as context:\n            fetch_d = self.agent.request(b\"GET\", uri)\n\n            # Nothing happened yet\n            self.assertNoResult(fetch_d)\n\n            # should have reset logcontext to the sentinel\n            _check_logcontext(SENTINEL_CONTEXT)\n\n            try:\n                fetch_res = yield fetch_d\n                return fetch_res\n            except Exception as e:\n                logger.info(\"Fetch of %s failed: %s\", uri.decode(\"ascii\"), e)\n                raise\n            finally:\n                _check_logcontext(context)\n\n    def _handle_well_known_connection(\n        self, client_factory, expected_sni, content, response_headers={}\n    ):\n        \"\"\"Handle an outgoing HTTPs connection: wire it up to a server, check that the\n        request is for a .well-known, and send the response.\n\n        Args:\n            client_factory (IProtocolFactory): outgoing connection\n            expected_sni (bytes): SNI that we expect the outgoing connection to send\n            content (bytes): content to send back as the .well-known\n        Returns:\n            HTTPChannel: server impl\n        \"\"\"\n        # make the connection for .well-known\n        well_known_server = self._make_connection(\n            client_factory, expected_sni=expected_sni\n        )\n        # check the .well-known request and send a response\n        self.assertEqual(len(well_known_server.requests), 1)\n        request = well_known_server.requests[0]\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"user-agent\"), [b\"test-agent\"]\n        )\n        self._send_well_known_response(request, content, headers=response_headers)\n        return well_known_server\n\n    def _send_well_known_response(self, request, content, headers={}):\n        \"\"\"Check that an incoming request looks like a valid .well-known request, and\n        send back the response.\n        \"\"\"\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/.well-known/matrix/server\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv\"])\n        # send back a response\n        for k, v in headers.items():\n            request.setHeader(k, v)\n        request.write(content)\n        request.finish()\n\n        self.reactor.pump((0.1,))\n\n    def test_get(self):\n        \"\"\"\n        happy-path test of a GET request with an explicit port\n        \"\"\"\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        test_d = self._make_get_request(b\"matrix://testserv:8448/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv:8448\"]\n        )\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"user-agent\"), [b\"test-agent\"]\n        )\n        content = request.content.read()\n        self.assertEqual(content, b\"\")\n\n        # Deferred is still without a result\n        self.assertNoResult(test_d)\n\n        # send the headers\n        request.responseHeaders.setRawHeaders(b\"Content-Type\", [b\"application/json\"])\n        request.write(\"\")\n\n        self.reactor.pump((0.1,))\n\n        response = self.successResultOf(test_d)\n\n        # that should give us a Response object\n        self.assertEqual(response.code, 200)\n\n        # Send the body\n        request.write('{ \"a\": 1 }'.encode(\"ascii\"))\n        request.finish()\n\n        self.reactor.pump((0.1,))\n\n        # check it can be read\n        json = self.successResultOf(treq.json_content(response))\n        self.assertEqual(json, {\"a\": 1})\n\n    def test_get_ip_address(self):\n        \"\"\"\n        Test the behaviour when the server name contains an explicit IP (with no port)\n        \"\"\"\n        # there will be a getaddrinfo on the IP\n        self.reactor.lookups[\"1.2.3.4\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://1.2.3.4/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=None)\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"1.2.3.4\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_ipv6_address(self):\n        \"\"\"\n        Test the behaviour when the server name contains an explicit IPv6 address\n        (with no port)\n        \"\"\"\n\n        # there will be a getaddrinfo on the IP\n        self.reactor.lookups[\"::1\"] = \"::1\"\n\n        test_d = self._make_get_request(b\"matrix://[::1]/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"::1\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=None)\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"[::1]\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_ipv6_address_with_port(self):\n        \"\"\"\n        Test the behaviour when the server name contains an explicit IPv6 address\n        (with explicit port)\n        \"\"\"\n\n        # there will be a getaddrinfo on the IP\n        self.reactor.lookups[\"::1\"] = \"::1\"\n\n        test_d = self._make_get_request(b\"matrix://[::1]:80/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"::1\")\n        self.assertEqual(port, 80)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=None)\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"[::1]:80\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_hostname_bad_cert(self):\n        \"\"\"\n        Test the behaviour when the certificate on the server doesn't match the hostname\n        \"\"\"\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv1\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://testserv1/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # No SRV record lookup yet\n        self.mock_resolver.resolve_service.assert_not_called()\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        # fonx the connection\n        client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n\n        # attemptdelay on the hostnameendpoint is 0.3, so takes that long before the\n        # .well-known request fails.\n        self.reactor.pump((0.4,))\n\n        # now there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv1\"\n        )\n\n        # we should fall back to a direct connection\n        self.assertEqual(len(clients), 2)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[1]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv1\")\n\n        # there should be no requests\n        self.assertEqual(len(http_server.requests), 0)\n\n        # ... and the request should have failed\n        e = self.failureResultOf(test_d, ResponseNeverReceived)\n        failure_reason = e.value.reasons[0]\n        self.assertIsInstance(failure_reason.value, VerificationError)\n\n    def test_get_ip_address_bad_cert(self):\n        \"\"\"\n        Test the behaviour when the server name contains an explicit IP, but\n        the server cert doesn't cover it\n        \"\"\"\n        # there will be a getaddrinfo on the IP\n        self.reactor.lookups[\"1.2.3.5\"] = \"1.2.3.5\"\n\n        test_d = self._make_get_request(b\"matrix://1.2.3.5/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.5\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=None)\n\n        # there should be no requests\n        self.assertEqual(len(http_server.requests), 0)\n\n        # ... and the request should have failed\n        e = self.failureResultOf(test_d, ResponseNeverReceived)\n        failure_reason = e.value.reasons[0]\n        self.assertIsInstance(failure_reason.value, VerificationError)\n\n    def test_get_no_srv_no_well_known(self):\n        \"\"\"\n        Test the behaviour when the server name has no port, no SRV, and no well-known\n        \"\"\"\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # No SRV record lookup yet\n        self.mock_resolver.resolve_service.assert_not_called()\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        # fonx the connection\n        client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n\n        # attemptdelay on the hostnameendpoint is 0.3, so  takes that long before the\n        # .well-known request fails.\n        self.reactor.pump((0.4,))\n\n        # now there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv\"\n        )\n\n        # we should fall back to a direct connection\n        self.assertEqual(len(clients), 2)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[1]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_well_known(self):\n        \"\"\"Test the behaviour when the .well-known delegates elsewhere\n        \"\"\"\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.reactor.lookups[\"target-server\"] = \"1::f\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        self._handle_well_known_connection(\n            client_factory,\n            expected_sni=b\"testserv\",\n            content=b'{ \"m.server\": \"target-server\" }',\n        )\n\n        # there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.target-server\"\n        )\n\n        # now we should get a connection to the target server\n        self.assertEqual(len(clients), 2)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[1]\n        self.assertEqual(host, \"1::f\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(\n            client_factory, expected_sni=b\"target-server\"\n        )\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"target-server\"]\n        )\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n        self.assertEqual(self.well_known_cache[b\"testserv\"], b\"target-server\")\n\n        # check the cache expires\n        self.reactor.pump((48 * 3600,))\n        self.well_known_cache.expire()\n        self.assertNotIn(b\"testserv\", self.well_known_cache)\n\n    def test_get_well_known_redirect(self):\n        \"\"\"Test the behaviour when the server name has no port and no SRV record, but\n        the .well-known has a 300 redirect\n        \"\"\"\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.reactor.lookups[\"target-server\"] = \"1::f\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        redirect_server = self._make_connection(\n            client_factory, expected_sni=b\"testserv\"\n        )\n\n        # send a 302 redirect\n        self.assertEqual(len(redirect_server.requests), 1)\n        request = redirect_server.requests[0]\n        request.redirect(b\"https://testserv/even_better_known\")\n        request.finish()\n\n        self.reactor.pump((0.1,))\n\n        # now there should be another connection\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        well_known_server = self._make_connection(\n            client_factory, expected_sni=b\"testserv\"\n        )\n\n        self.assertEqual(len(well_known_server.requests), 1, \"No request after 302\")\n        request = well_known_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/even_better_known\")\n        request.write(b'{ \"m.server\": \"target-server\" }')\n        request.finish()\n\n        self.reactor.pump((0.1,))\n\n        # there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.target-server\"\n        )\n\n        # now we should get a connection to the target server\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1::f\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(\n            client_factory, expected_sni=b\"target-server\"\n        )\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"target-server\"]\n        )\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n        self.assertEqual(self.well_known_cache[b\"testserv\"], b\"target-server\")\n\n        # check the cache expires\n        self.reactor.pump((48 * 3600,))\n        self.well_known_cache.expire()\n        self.assertNotIn(b\"testserv\", self.well_known_cache)\n\n    def test_get_invalid_well_known(self):\n        \"\"\"\n        Test the behaviour when the server name has an *invalid* well-known (and no SRV)\n        \"\"\"\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # No SRV record lookup yet\n        self.mock_resolver.resolve_service.assert_not_called()\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        self._handle_well_known_connection(\n            client_factory, expected_sni=b\"testserv\", content=b\"NOT JSON\"\n        )\n\n        # now there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv\"\n        )\n\n        # we should fall back to a direct connection\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_well_known_unsigned_cert(self):\n        \"\"\"Test the behaviour when the .well-known server presents a cert\n        not signed by a CA\n        \"\"\"\n\n        # we use the same test server as the other tests, but use an agent with\n        # the config left to the default, which will not trust it (since the\n        # presented cert is signed by a test CA)\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n\n        config = default_config(\"test\", parse=True)\n\n        # Build a new agent and WellKnownResolver with a different tls factory\n        tls_factory = FederationPolicyForHTTPS(config)\n        agent = MatrixFederationAgent(\n            reactor=self.reactor,\n            tls_client_options_factory=tls_factory,\n            user_agent=b\"test-agent\",  # This is unused since _well_known_resolver is passed below.\n            _srv_resolver=self.mock_resolver,\n            _well_known_resolver=WellKnownResolver(\n                self.reactor,\n                Agent(self.reactor, contextFactory=tls_factory),\n                b\"test-agent\",\n                well_known_cache=self.well_known_cache,\n                had_well_known_cache=self.had_well_known_cache,\n            ),\n        )\n\n        test_d = agent.request(b\"GET\", b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        http_proto = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        # there should be no requests\n        self.assertEqual(len(http_proto.requests), 0)\n\n        # and there should be a SRV lookup instead\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv\"\n        )\n\n    def test_get_hostname_srv(self):\n        \"\"\"\n        Test the behaviour when there is a single SRV record\n        \"\"\"\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service(\n            [Server(host=b\"srvtarget\", port=8443)]\n        )\n        self.reactor.lookups[\"srvtarget\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # the request for a .well-known will have failed with a DNS lookup error.\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv\"\n        )\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8443)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_get_well_known_srv(self):\n        \"\"\"Test the behaviour when the .well-known redirects to a place where there\n        is a SRV.\n        \"\"\"\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.reactor.lookups[\"srvtarget\"] = \"5.6.7.8\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service(\n            [Server(host=b\"srvtarget\", port=8443)]\n        )\n\n        self._handle_well_known_connection(\n            client_factory,\n            expected_sni=b\"testserv\",\n            content=b'{ \"m.server\": \"target-server\" }',\n        )\n\n        # there should be a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.target-server\"\n        )\n\n        # now we should get a connection to the target of the SRV record\n        self.assertEqual(len(clients), 2)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[1]\n        self.assertEqual(host, \"5.6.7.8\")\n        self.assertEqual(port, 8443)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(\n            client_factory, expected_sni=b\"target-server\"\n        )\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"target-server\"]\n        )\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_idna_servername(self):\n        \"\"\"test the behaviour when the server name has idna chars in\"\"\"\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service([])\n\n        # the resolver is always called with the IDNA hostname as a native string.\n        self.reactor.lookups[\"xn--bcher-kva.com\"] = \"1.2.3.4\"\n\n        # this is idna for b\u00fccher.com\n        test_d = self._make_get_request(b\"matrix://xn--bcher-kva.com/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        # No SRV record lookup yet\n        self.mock_resolver.resolve_service.assert_not_called()\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        # fonx the connection\n        client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n\n        # attemptdelay on the hostnameendpoint is 0.3, so  takes that long before the\n        # .well-known request fails.\n        self.reactor.pump((0.4,))\n\n        # now there should have been a SRV lookup\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.xn--bcher-kva.com\"\n        )\n\n        # We should fall back to port 8448\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 2)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[1]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8448)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(\n            client_factory, expected_sni=b\"xn--bcher-kva.com\"\n        )\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"xn--bcher-kva.com\"]\n        )\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_idna_srv_target(self):\n        \"\"\"test the behaviour when the target of a SRV record has idna chars\"\"\"\n\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service(\n            [Server(host=b\"xn--trget-3qa.com\", port=8443)]  # t\u00e2rget.com\n        )\n        self.reactor.lookups[\"xn--trget-3qa.com\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://xn--bcher-kva.com/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.xn--bcher-kva.com\"\n        )\n\n        # Make sure treq is trying to connect\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients[0]\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8443)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(\n            client_factory, expected_sni=b\"xn--bcher-kva.com\"\n        )\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(\n            request.requestHeaders.getRawHeaders(b\"host\"), [b\"xn--bcher-kva.com\"]\n        )\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n    def test_well_known_cache(self):\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        well_known_server = self._handle_well_known_connection(\n            client_factory,\n            expected_sni=b\"testserv\",\n            response_headers={b\"Cache-Control\": b\"max-age=1000\"},\n            content=b'{ \"m.server\": \"target-server\" }',\n        )\n\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, b\"target-server\")\n\n        # close the tcp connection\n        well_known_server.loseConnection()\n\n        # repeat the request: it should hit the cache\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, b\"target-server\")\n\n        # expire the cache\n        self.reactor.pump((1000.0,))\n\n        # now it should connect again\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        self._handle_well_known_connection(\n            client_factory,\n            expected_sni=b\"testserv\",\n            content=b'{ \"m.server\": \"other-server\" }',\n        )\n\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, b\"other-server\")\n\n    def test_well_known_cache_with_temp_failure(self):\n        \"\"\"Test that we refetch well-known before the cache expires, and that\n        it ignores transient errors.\n        \"\"\"\n\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n\n        # there should be an attempt to connect on port 443 for the .well-known\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 443)\n\n        well_known_server = self._handle_well_known_connection(\n            client_factory,\n            expected_sni=b\"testserv\",\n            response_headers={b\"Cache-Control\": b\"max-age=1000\"},\n            content=b'{ \"m.server\": \"target-server\" }',\n        )\n\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, b\"target-server\")\n\n        # close the tcp connection\n        well_known_server.loseConnection()\n\n        # Get close to the cache expiry, this will cause the resolver to do\n        # another lookup.\n        self.reactor.pump((900.0,))\n\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n\n        # The resolver may retry a few times, so fonx all requests that come along\n        attempts = 0\n        while self.reactor.tcpClients:\n            clients = self.reactor.tcpClients\n            (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n\n            attempts += 1\n\n            # fonx the connection attempt, this will be treated as a temporary\n            # failure.\n            client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n\n            # There's a few sleeps involved, so we have to pump the reactor a\n            # bit.\n            self.reactor.pump((1.0, 1.0))\n\n        # We expect to see more than one attempt as there was previously a valid\n        # well known.\n        self.assertGreater(attempts, 1)\n\n        # Resolver should return cached value, despite the lookup failing.\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, b\"target-server\")\n\n        # Expire both caches and repeat the request\n        self.reactor.pump((10000.0,))\n\n        # Repated the request, this time it should fail if the lookup fails.\n        fetch_d = defer.ensureDeferred(\n            self.well_known_resolver.get_well_known(b\"testserv\")\n        )\n\n        clients = self.reactor.tcpClients\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n        self.reactor.pump((0.4,))\n\n        r = self.successResultOf(fetch_d)\n        self.assertEqual(r.delegated_server, None)\n\n    def test_srv_fallbacks(self):\n        \"\"\"Test that other SRV results are tried if the first one fails.\n        \"\"\"\n        self.mock_resolver.resolve_service.side_effect = generate_resolve_service(\n            [\n                Server(host=b\"target.com\", port=8443),\n                Server(host=b\"target.com\", port=8444),\n            ]\n        )\n        self.reactor.lookups[\"target.com\"] = \"1.2.3.4\"\n\n        test_d = self._make_get_request(b\"matrix://testserv/foo/bar\")\n\n        # Nothing happened yet\n        self.assertNoResult(test_d)\n\n        self.mock_resolver.resolve_service.assert_called_once_with(\n            b\"_matrix._tcp.testserv\"\n        )\n\n        # We should see an attempt to connect to the first server\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8443)\n\n        # Fonx the connection\n        client_factory.clientConnectionFailed(None, Exception(\"nope\"))\n\n        # There's a 300ms delay in HostnameEndpoint\n        self.reactor.pump((0.4,))\n\n        # Hasn't failed yet\n        self.assertNoResult(test_d)\n\n        # We shouldnow see an attempt to connect to the second server\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8444)\n\n        # make a test server, and wire up the client\n        http_server = self._make_connection(client_factory, expected_sni=b\"testserv\")\n\n        self.assertEqual(len(http_server.requests), 1)\n        request = http_server.requests[0]\n        self.assertEqual(request.method, b\"GET\")\n        self.assertEqual(request.path, b\"/foo/bar\")\n        self.assertEqual(request.requestHeaders.getRawHeaders(b\"host\"), [b\"testserv\"])\n\n        # finish the request\n        request.finish()\n        self.reactor.pump((0.1,))\n        self.successResultOf(test_d)\n\n\nclass TestCachePeriodFromHeaders(unittest.TestCase):\n    def test_cache_control(self):\n        # uppercase\n        self.assertEqual(\n            _cache_period_from_headers(\n                Headers({b\"Cache-Control\": [b\"foo, Max-Age = 100, bar\"]})\n            ),\n            100,\n        )\n\n        # missing value\n        self.assertIsNone(\n            _cache_period_from_headers(Headers({b\"Cache-Control\": [b\"max-age=, bar\"]}))\n        )\n\n        # hackernews: bogus due to semicolon\n        self.assertIsNone(\n            _cache_period_from_headers(\n                Headers({b\"Cache-Control\": [b\"private; max-age=0\"]})\n            )\n        )\n\n        # github\n        self.assertEqual(\n            _cache_period_from_headers(\n                Headers({b\"Cache-Control\": [b\"max-age=0, private, must-revalidate\"]})\n            ),\n            0,\n        )\n\n        # google\n        self.assertEqual(\n            _cache_period_from_headers(\n                Headers({b\"cache-control\": [b\"private, max-age=0\"]})\n            ),\n            0,\n        )\n\n    def test_expires(self):\n        self.assertEqual(\n            _cache_period_from_headers(\n                Headers({b\"Expires\": [b\"Wed, 30 Jan 2019 07:35:33 GMT\"]}),\n                time_now=lambda: 1548833700,\n            ),\n            33,\n        )\n\n        # cache-control overrides expires\n        self.assertEqual(\n            _cache_period_from_headers(\n                Headers(\n                    {\n                        b\"cache-control\": [b\"max-age=10\"],\n                        b\"Expires\": [b\"Wed, 30 Jan 2019 07:35:33 GMT\"],\n                    }\n                ),\n                time_now=lambda: 1548833700,\n            ),\n            10,\n        )\n\n        # invalid expires means immediate expiry\n        self.assertEqual(_cache_period_from_headers(Headers({b\"Expires\": [b\"0\"]})), 0)\n\n\ndef _check_logcontext(context):\n    current = current_context()\n    if current is not context:\n        raise AssertionError(\"Expected logcontext %s but was %s\" % (context, current))\n\n\ndef _build_test_server(connection_creator):\n    \"\"\"Construct a test server\n\n    This builds an HTTP channel, wrapped with a TLSMemoryBIOProtocol\n\n    Args:\n        connection_creator (IOpenSSLServerConnectionCreator): thing to build\n            SSL connections\n        sanlist (list[bytes]): list of the SAN entries for the cert returned\n            by the server\n\n    Returns:\n        TLSMemoryBIOProtocol\n    \"\"\"\n    server_factory = Factory.forProtocol(HTTPChannel)\n    # Request.finish expects the factory to have a 'log' method.\n    server_factory.log = _log_request\n\n    server_tls_factory = TLSMemoryBIOFactory(\n        connection_creator, isClient=False, wrappedFactory=server_factory\n    )\n\n    return server_tls_factory.buildProtocol(None)\n\n\ndef _log_request(request):\n    \"\"\"Implements Factory.log, which is expected by Request.finish\"\"\"\n    logger.info(\"Completed request %s\", request)\n\n\n@implementer(IPolicyForHTTPS)\nclass TrustingTLSPolicyForHTTPS:\n    \"\"\"An IPolicyForHTTPS which checks that the certificate belongs to the\n    right server, but doesn't check the certificate chain.\"\"\"\n\n    def creatorForNetloc(self, hostname, port):\n        certificateOptions = OpenSSLCertificateOptions()\n        return ClientTLSOptions(hostname, certificateOptions.getContext())\n", "patch": "@@ -17,6 +17,7 @@\n from mock import Mock\n \n import treq\n+from netaddr import IPSet\n from service_identity import VerificationError\n from zope.interface import implementer\n \n@@ -103,6 +104,7 @@ def setUp(self):\n             reactor=self.reactor,\n             tls_client_options_factory=self.tls_factory,\n             user_agent=\"test-agent\",  # Note that this is unused since _well_known_resolver is provided.\n+            ip_blacklist=IPSet(),\n             _srv_resolver=self.mock_resolver,\n             _well_known_resolver=self.well_known_resolver,\n         )\n@@ -736,6 +738,7 @@ def test_get_well_known_unsigned_cert(self):\n             reactor=self.reactor,\n             tls_client_options_factory=tls_factory,\n             user_agent=b\"test-agent\",  # This is unused since _well_known_resolver is passed below.\n+            ip_blacklist=IPSet(),\n             _srv_resolver=self.mock_resolver,\n             _well_known_resolver=WellKnownResolver(\n                 self.reactor,", "file_path": "files/2021_2/40", "file_language": "py", "file_name": "tests/http/federation/test_matrix_federation_agent.py", "outdated_file_modify": 1, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 1, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": -1, "function_before": [], "function_after": []}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fpush%2Ftest_http.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom mock import Mock\n\nfrom twisted.internet.defer import Deferred\n\nimport synapse.rest.admin\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.rest.client.v1 import login, room\nfrom synapse.rest.client.v2_alpha import receipts\n\nfrom tests.unittest import HomeserverTestCase, override_config\n\n\nclass HTTPPusherTests(HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        receipts.register_servlets,\n    ]\n    user_id = True\n    hijack_auth = False\n\n    def make_homeserver(self, reactor, clock):\n        self.push_attempts = []\n\n        m = Mock()\n\n        def post_json_get_json(url, body):\n            d = Deferred()\n            self.push_attempts.append((d, url, body))\n            return make_deferred_yieldable(d)\n\n        m.post_json_get_json = post_json_get_json\n\n        config = self.default_config()\n        config[\"start_pushers\"] = True\n\n        hs = self.setup_test_homeserver(\n            config=config, proxied_blacklisted_http_client=m\n        )\n\n        return hs\n\n    def test_sends_http(self):\n        \"\"\"\n        The HTTP pusher will send pushes for each message to a HTTP endpoint\n        when configured to do so.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # The other user sends some messages\n        self.helper.send(room, body=\"Hi!\", tok=other_access_token)\n        self.helper.send(room, body=\"There!\", tok=other_access_token)\n\n        # Get the stream ordering before it gets sent\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        last_stream_ordering = pushers[0][\"last_stream_ordering\"]\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # It hasn't succeeded yet, so the stream ordering shouldn't have moved\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertEqual(last_stream_ordering, pushers[0][\"last_stream_ordering\"])\n\n        # One push was attempted to be sent -- it'll be the first message\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(\n            self.push_attempts[0][2][\"notification\"][\"content\"][\"body\"], \"Hi!\"\n        )\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # The stream ordering has increased\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertTrue(pushers[0][\"last_stream_ordering\"] > last_stream_ordering)\n        last_stream_ordering = pushers[0][\"last_stream_ordering\"]\n\n        # Now it'll try and send the second push message, which will be the second one\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n        self.assertEqual(\n            self.push_attempts[1][2][\"notification\"][\"content\"][\"body\"], \"There!\"\n        )\n\n        # Make the second push succeed\n        self.push_attempts[1][0].callback({})\n        self.pump()\n\n        # The stream ordering has increased, again\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertTrue(pushers[0][\"last_stream_ordering\"] > last_stream_ordering)\n\n    def test_sends_high_priority_for_encrypted(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to an encrypted message.\n        This will happen both in 1:1 rooms and larger rooms.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send an encrypted event\n        # I know there'd normally be set-up of an encrypted room first\n        # but this will do for our purposes\n        self.helper.send_event(\n            room,\n            \"m.room.encrypted\",\n            content={\n                \"algorithm\": \"m.megolm.v1.aes-sha2\",\n                \"sender_key\": \"6lImKbzK51MzWLwHh8tUM3UBBSBrLlgup/OOCGTvumM\",\n                \"ciphertext\": \"AwgAErABoRxwpMipdgiwXgu46rHiWQ0DmRj0qUlPrMraBUDk\"\n                \"leTnJRljpuc7IOhsYbLY3uo2WI0ab/ob41sV+3JEIhODJPqH\"\n                \"TK7cEZaIL+/up9e+dT9VGF5kRTWinzjkeqO8FU5kfdRjm+3w\"\n                \"0sy3o1OCpXXCfO+faPhbV/0HuK4ndx1G+myNfK1Nk/CxfMcT\"\n                \"BT+zDS/Df/QePAHVbrr9uuGB7fW8ogW/ulnydgZPRluusFGv\"\n                \"J3+cg9LoPpZPAmv5Me3ec7NtdlfN0oDZ0gk3TiNkkhsxDG9Y\"\n                \"YcNzl78USI0q8+kOV26Bu5dOBpU4WOuojXZHJlP5lMgdzLLl\"\n                \"EQ0\",\n                \"session_id\": \"IigqfNWLL+ez/Is+Duwp2s4HuCZhFG9b9CZKTYHtQ4A\",\n                \"device_id\": \"AHQDUSTAAA\",\n            },\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Add yet another person \u2014 we want to make this room not a 1:1\n        # (as encrypted messages in a 1:1 currently have tweaks applied\n        #  so it doesn't properly exercise the condition of all encrypted\n        #  messages need to be high).\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Check no push notifications are sent regarding the membership changes\n        # (that would confuse the test)\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 1)\n\n        # Send another encrypted event\n        self.helper.send_event(\n            room,\n            \"m.room.encrypted\",\n            content={\n                \"ciphertext\": \"AwgAEoABtEuic/2DF6oIpNH+q/PonzlhXOVho8dTv0tzFr5m\"\n                \"9vTo50yabx3nxsRlP2WxSqa8I07YftP+EKWCWJvTkg6o7zXq\"\n                \"6CK+GVvLQOVgK50SfvjHqJXN+z1VEqj+5mkZVN/cAgJzoxcH\"\n                \"zFHkwDPJC8kQs47IHd8EO9KBUK4v6+NQ1uE/BIak4qAf9aS/\"\n                \"kI+f0gjn9IY9K6LXlah82A/iRyrIrxkCkE/n0VfvLhaWFecC\"\n                \"sAWTcMLoF6fh1Jpke95mljbmFSpsSd/eEQw\",\n                \"device_id\": \"SRCFTWTHXO\",\n                \"session_id\": \"eMA+bhGczuTz1C5cJR1YbmrnnC6Goni4lbvS5vJ1nG4\",\n                \"algorithm\": \"m.megolm.v1.aes-sha2\",\n                \"sender_key\": \"rC/XSIAiYrVGSuaHMop8/pTZbku4sQKBZwRwukgnN1c\",\n            },\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"high\")\n\n    def test_sends_high_priority_for_one_to_one_only(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message in a one-to-one room.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(room, body=\"Hi!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority \u2014 this is a one-to-one room\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Yet another user joins\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Check no push notifications are sent regarding the membership changes\n        # (that would confuse the test)\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 1)\n\n        # Send another event\n        self.helper.send(room, body=\"Welcome!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_sends_high_priority_for_mention(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message containing the user's display name.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other users join\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(room, body=\"Oh, user, hello!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Send another event, this time with no mention\n        self.helper.send(room, body=\"Are you there?\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_sends_high_priority_for_atroom(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message that contains @room.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room (as other_user so the power levels are compatible with\n        # other_user sending @room).\n        room = self.helper.create_room_as(other_user_id, tok=other_access_token)\n\n        # The other users join\n        self.helper.join(room=room, user=user_id, tok=access_token)\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(\n            room,\n            body=\"@room eeek! There's a spider on the table!\",\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Send another event, this time as someone without the power of @room\n        self.helper.send(\n            room, body=\"@room the spider is gone\", tok=yet_another_access_token\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_push_unread_count_group_by_room(self):\n        \"\"\"\n        The HTTP pusher will group unread count by number of unread rooms.\n        \"\"\"\n        # Carry out common push count tests and setup\n        self._test_push_unread_count()\n\n        # Carry out our option-value specific test\n        #\n        # This push should still only contain an unread count of 1 (for 1 unread room)\n        self.assertEqual(\n            self.push_attempts[5][2][\"notification\"][\"counts\"][\"unread\"], 1\n        )\n\n    @override_config({\"push\": {\"group_unread_count_by_room\": False}})\n    def test_push_unread_count_message_count(self):\n        \"\"\"\n        The HTTP pusher will send the total unread message count.\n        \"\"\"\n        # Carry out common push count tests and setup\n        self._test_push_unread_count()\n\n        # Carry out our option-value specific test\n        #\n        # We're counting every unread message, so there should now be 4 since the\n        # last read receipt\n        self.assertEqual(\n            self.push_attempts[5][2][\"notification\"][\"counts\"][\"unread\"], 4\n        )\n\n    def _test_push_unread_count(self):\n        \"\"\"\n        Tests that the correct unread count appears in sent push notifications\n\n        Note that:\n        * Sending messages will cause push notifications to go out to relevant users\n        * Sending a read receipt will cause a \"badge update\" notification to go out to\n          the user that sent the receipt\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"other_user\", \"pass\")\n        other_access_token = self.login(\"other_user\", \"pass\")\n\n        # Create a room (as other_user)\n        room_id = self.helper.create_room_as(other_user_id, tok=other_access_token)\n\n        # The user to get notified joins\n        self.helper.join(room=room_id, user=user_id, tok=access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        response = self.helper.send(\n            room_id, body=\"Hello there!\", tok=other_access_token\n        )\n        # To get an unread count, the user who is getting notified has to have a read\n        # position in the room. We'll set the read position to this event in a moment\n        first_message_event_id = response[\"event_id\"]\n\n        # Advance time a bit (so the pusher will register something has happened) and\n        # make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n\n        # Check that the unread count for the room is 0\n        #\n        # The unread count is zero as the user has no read receipt in the room yet\n        self.assertEqual(\n            self.push_attempts[0][2][\"notification\"][\"counts\"][\"unread\"], 0\n        )\n\n        # Now set the user's read receipt position to the first event\n        #\n        # This will actually trigger a new notification to be sent out so that\n        # even if the user does not receive another message, their unread\n        # count goes down\n        request, channel = self.make_request(\n            \"POST\",\n            \"/rooms/%s/receipt/m.read/%s\" % (room_id, first_message_event_id),\n            {},\n            access_token=access_token,\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        # Advance time and make the push succeed\n        self.push_attempts[1][0].callback({})\n        self.pump()\n\n        # Unread count is still zero as we've read the only message in the room\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(\n            self.push_attempts[1][2][\"notification\"][\"counts\"][\"unread\"], 0\n        )\n\n        # Send another message\n        self.helper.send(\n            room_id, body=\"How's the weather today?\", tok=other_access_token\n        )\n\n        # Advance time and make the push succeed\n        self.push_attempts[2][0].callback({})\n        self.pump()\n\n        # This push should contain an unread count of 1 as there's now been one\n        # message since our last read receipt\n        self.assertEqual(len(self.push_attempts), 3)\n        self.assertEqual(\n            self.push_attempts[2][2][\"notification\"][\"counts\"][\"unread\"], 1\n        )\n\n        # Since we're grouping by room, sending more messages shouldn't increase the\n        # unread count, as they're all being sent in the same room\n        self.helper.send(room_id, body=\"Hello?\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[3][0].callback({})\n\n        self.helper.send(room_id, body=\"Hello??\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[4][0].callback({})\n\n        self.helper.send(room_id, body=\"HELLO???\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[5][0].callback({})\n\n        self.assertEqual(len(self.push_attempts), 6)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom mock import Mock\n\nfrom twisted.internet.defer import Deferred\n\nimport synapse.rest.admin\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.rest.client.v1 import login, room\nfrom synapse.rest.client.v2_alpha import receipts\n\nfrom tests.unittest import HomeserverTestCase, override_config\n\n\nclass HTTPPusherTests(HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        receipts.register_servlets,\n    ]\n    user_id = True\n    hijack_auth = False\n\n    def make_homeserver(self, reactor, clock):\n        self.push_attempts = []\n\n        m = Mock()\n\n        def post_json_get_json(url, body):\n            d = Deferred()\n            self.push_attempts.append((d, url, body))\n            return make_deferred_yieldable(d)\n\n        m.post_json_get_json = post_json_get_json\n\n        config = self.default_config()\n        config[\"start_pushers\"] = True\n\n        hs = self.setup_test_homeserver(config=config, proxied_http_client=m)\n\n        return hs\n\n    def test_sends_http(self):\n        \"\"\"\n        The HTTP pusher will send pushes for each message to a HTTP endpoint\n        when configured to do so.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # The other user sends some messages\n        self.helper.send(room, body=\"Hi!\", tok=other_access_token)\n        self.helper.send(room, body=\"There!\", tok=other_access_token)\n\n        # Get the stream ordering before it gets sent\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        last_stream_ordering = pushers[0][\"last_stream_ordering\"]\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # It hasn't succeeded yet, so the stream ordering shouldn't have moved\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertEqual(last_stream_ordering, pushers[0][\"last_stream_ordering\"])\n\n        # One push was attempted to be sent -- it'll be the first message\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(\n            self.push_attempts[0][2][\"notification\"][\"content\"][\"body\"], \"Hi!\"\n        )\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # The stream ordering has increased\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertTrue(pushers[0][\"last_stream_ordering\"] > last_stream_ordering)\n        last_stream_ordering = pushers[0][\"last_stream_ordering\"]\n\n        # Now it'll try and send the second push message, which will be the second one\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n        self.assertEqual(\n            self.push_attempts[1][2][\"notification\"][\"content\"][\"body\"], \"There!\"\n        )\n\n        # Make the second push succeed\n        self.push_attempts[1][0].callback({})\n        self.pump()\n\n        # The stream ordering has increased, again\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertTrue(pushers[0][\"last_stream_ordering\"] > last_stream_ordering)\n\n    def test_sends_high_priority_for_encrypted(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to an encrypted message.\n        This will happen both in 1:1 rooms and larger rooms.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send an encrypted event\n        # I know there'd normally be set-up of an encrypted room first\n        # but this will do for our purposes\n        self.helper.send_event(\n            room,\n            \"m.room.encrypted\",\n            content={\n                \"algorithm\": \"m.megolm.v1.aes-sha2\",\n                \"sender_key\": \"6lImKbzK51MzWLwHh8tUM3UBBSBrLlgup/OOCGTvumM\",\n                \"ciphertext\": \"AwgAErABoRxwpMipdgiwXgu46rHiWQ0DmRj0qUlPrMraBUDk\"\n                \"leTnJRljpuc7IOhsYbLY3uo2WI0ab/ob41sV+3JEIhODJPqH\"\n                \"TK7cEZaIL+/up9e+dT9VGF5kRTWinzjkeqO8FU5kfdRjm+3w\"\n                \"0sy3o1OCpXXCfO+faPhbV/0HuK4ndx1G+myNfK1Nk/CxfMcT\"\n                \"BT+zDS/Df/QePAHVbrr9uuGB7fW8ogW/ulnydgZPRluusFGv\"\n                \"J3+cg9LoPpZPAmv5Me3ec7NtdlfN0oDZ0gk3TiNkkhsxDG9Y\"\n                \"YcNzl78USI0q8+kOV26Bu5dOBpU4WOuojXZHJlP5lMgdzLLl\"\n                \"EQ0\",\n                \"session_id\": \"IigqfNWLL+ez/Is+Duwp2s4HuCZhFG9b9CZKTYHtQ4A\",\n                \"device_id\": \"AHQDUSTAAA\",\n            },\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Add yet another person \u2014 we want to make this room not a 1:1\n        # (as encrypted messages in a 1:1 currently have tweaks applied\n        #  so it doesn't properly exercise the condition of all encrypted\n        #  messages need to be high).\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Check no push notifications are sent regarding the membership changes\n        # (that would confuse the test)\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 1)\n\n        # Send another encrypted event\n        self.helper.send_event(\n            room,\n            \"m.room.encrypted\",\n            content={\n                \"ciphertext\": \"AwgAEoABtEuic/2DF6oIpNH+q/PonzlhXOVho8dTv0tzFr5m\"\n                \"9vTo50yabx3nxsRlP2WxSqa8I07YftP+EKWCWJvTkg6o7zXq\"\n                \"6CK+GVvLQOVgK50SfvjHqJXN+z1VEqj+5mkZVN/cAgJzoxcH\"\n                \"zFHkwDPJC8kQs47IHd8EO9KBUK4v6+NQ1uE/BIak4qAf9aS/\"\n                \"kI+f0gjn9IY9K6LXlah82A/iRyrIrxkCkE/n0VfvLhaWFecC\"\n                \"sAWTcMLoF6fh1Jpke95mljbmFSpsSd/eEQw\",\n                \"device_id\": \"SRCFTWTHXO\",\n                \"session_id\": \"eMA+bhGczuTz1C5cJR1YbmrnnC6Goni4lbvS5vJ1nG4\",\n                \"algorithm\": \"m.megolm.v1.aes-sha2\",\n                \"sender_key\": \"rC/XSIAiYrVGSuaHMop8/pTZbku4sQKBZwRwukgnN1c\",\n            },\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"high\")\n\n    def test_sends_high_priority_for_one_to_one_only(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message in a one-to-one room.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(room, body=\"Hi!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority \u2014 this is a one-to-one room\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Yet another user joins\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Check no push notifications are sent regarding the membership changes\n        # (that would confuse the test)\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 1)\n\n        # Send another event\n        self.helper.send(room, body=\"Welcome!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_sends_high_priority_for_mention(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message containing the user's display name.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other users join\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(room, body=\"Oh, user, hello!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Send another event, this time with no mention\n        self.helper.send(room, body=\"Are you there?\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_sends_high_priority_for_atroom(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message that contains @room.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room (as other_user so the power levels are compatible with\n        # other_user sending @room).\n        room = self.helper.create_room_as(other_user_id, tok=other_access_token)\n\n        # The other users join\n        self.helper.join(room=room, user=user_id, tok=access_token)\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(\n            room,\n            body=\"@room eeek! There's a spider on the table!\",\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Send another event, this time as someone without the power of @room\n        self.helper.send(\n            room, body=\"@room the spider is gone\", tok=yet_another_access_token\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_push_unread_count_group_by_room(self):\n        \"\"\"\n        The HTTP pusher will group unread count by number of unread rooms.\n        \"\"\"\n        # Carry out common push count tests and setup\n        self._test_push_unread_count()\n\n        # Carry out our option-value specific test\n        #\n        # This push should still only contain an unread count of 1 (for 1 unread room)\n        self.assertEqual(\n            self.push_attempts[5][2][\"notification\"][\"counts\"][\"unread\"], 1\n        )\n\n    @override_config({\"push\": {\"group_unread_count_by_room\": False}})\n    def test_push_unread_count_message_count(self):\n        \"\"\"\n        The HTTP pusher will send the total unread message count.\n        \"\"\"\n        # Carry out common push count tests and setup\n        self._test_push_unread_count()\n\n        # Carry out our option-value specific test\n        #\n        # We're counting every unread message, so there should now be 4 since the\n        # last read receipt\n        self.assertEqual(\n            self.push_attempts[5][2][\"notification\"][\"counts\"][\"unread\"], 4\n        )\n\n    def _test_push_unread_count(self):\n        \"\"\"\n        Tests that the correct unread count appears in sent push notifications\n\n        Note that:\n        * Sending messages will cause push notifications to go out to relevant users\n        * Sending a read receipt will cause a \"badge update\" notification to go out to\n          the user that sent the receipt\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"other_user\", \"pass\")\n        other_access_token = self.login(\"other_user\", \"pass\")\n\n        # Create a room (as other_user)\n        room_id = self.helper.create_room_as(other_user_id, tok=other_access_token)\n\n        # The user to get notified joins\n        self.helper.join(room=room_id, user=user_id, tok=access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        response = self.helper.send(\n            room_id, body=\"Hello there!\", tok=other_access_token\n        )\n        # To get an unread count, the user who is getting notified has to have a read\n        # position in the room. We'll set the read position to this event in a moment\n        first_message_event_id = response[\"event_id\"]\n\n        # Advance time a bit (so the pusher will register something has happened) and\n        # make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n\n        # Check that the unread count for the room is 0\n        #\n        # The unread count is zero as the user has no read receipt in the room yet\n        self.assertEqual(\n            self.push_attempts[0][2][\"notification\"][\"counts\"][\"unread\"], 0\n        )\n\n        # Now set the user's read receipt position to the first event\n        #\n        # This will actually trigger a new notification to be sent out so that\n        # even if the user does not receive another message, their unread\n        # count goes down\n        request, channel = self.make_request(\n            \"POST\",\n            \"/rooms/%s/receipt/m.read/%s\" % (room_id, first_message_event_id),\n            {},\n            access_token=access_token,\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        # Advance time and make the push succeed\n        self.push_attempts[1][0].callback({})\n        self.pump()\n\n        # Unread count is still zero as we've read the only message in the room\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(\n            self.push_attempts[1][2][\"notification\"][\"counts\"][\"unread\"], 0\n        )\n\n        # Send another message\n        self.helper.send(\n            room_id, body=\"How's the weather today?\", tok=other_access_token\n        )\n\n        # Advance time and make the push succeed\n        self.push_attempts[2][0].callback({})\n        self.pump()\n\n        # This push should contain an unread count of 1 as there's now been one\n        # message since our last read receipt\n        self.assertEqual(len(self.push_attempts), 3)\n        self.assertEqual(\n            self.push_attempts[2][2][\"notification\"][\"counts\"][\"unread\"], 1\n        )\n\n        # Since we're grouping by room, sending more messages shouldn't increase the\n        # unread count, as they're all being sent in the same room\n        self.helper.send(room_id, body=\"Hello?\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[3][0].callback({})\n\n        self.helper.send(room_id, body=\"Hello??\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[4][0].callback({})\n\n        self.helper.send(room_id, body=\"HELLO???\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[5][0].callback({})\n\n        self.assertEqual(len(self.push_attempts), 6)\n", "patch": "@@ -49,7 +49,9 @@ def post_json_get_json(url, body):\n         config = self.default_config()\n         config[\"start_pushers\"] = True\n \n-        hs = self.setup_test_homeserver(config=config, proxied_http_client=m)\n+        hs = self.setup_test_homeserver(\n+            config=config, proxied_blacklisted_http_client=m\n+        )\n \n         return hs\n ", "file_path": "files/2021_2/41", "file_language": "py", "file_name": "tests/push/test_http.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class HTTPPusherTests(HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        receipts.register_servlets,\n    ]\n    user_id = True\n    hijack_auth = False\n\n    def make_homeserver(self, reactor, clock):\n        self.push_attempts = []\n\n        m = Mock()\n\n        def post_json_get_json(url, body):\n            d = Deferred()\n            self.push_attempts.append((d, url, body))\n            return make_deferred_yieldable(d)\n\n        m.post_json_get_json = post_json_get_json\n\n        config = self.default_config()\n        config[\"start_pushers\"] = True\n\n        hs = self.setup_test_homeserver(config=config, proxied_http_client=m)\n\n        return hs\n\n    def test_sends_http(self):\n        \"\"\"\n        The HTTP pusher will send pushes for each message to a HTTP endpoint\n        when configured to do so.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # The other user sends some messages\n        self.helper.send(room, body=\"Hi!\", tok=other_access_token)\n        self.helper.send(room, body=\"There!\", tok=other_access_token)\n\n        # Get the stream ordering before it gets sent\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        last_stream_ordering = pushers[0][\"last_stream_ordering\"]\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # It hasn't succeeded yet, so the stream ordering shouldn't have moved\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertEqual(last_stream_ordering, pushers[0][\"last_stream_ordering\"])\n\n        # One push was attempted to be sent -- it'll be the first message\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(\n            self.push_attempts[0][2][\"notification\"][\"content\"][\"body\"], \"Hi!\"\n        )\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # The stream ordering has increased\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertTrue(pushers[0][\"last_stream_ordering\"] > last_stream_ordering)\n        last_stream_ordering = pushers[0][\"last_stream_ordering\"]\n\n        # Now it'll try and send the second push message, which will be the second one\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n        self.assertEqual(\n            self.push_attempts[1][2][\"notification\"][\"content\"][\"body\"], \"There!\"\n        )\n\n        # Make the second push succeed\n        self.push_attempts[1][0].callback({})\n        self.pump()\n\n        # The stream ordering has increased, again\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertTrue(pushers[0][\"last_stream_ordering\"] > last_stream_ordering)\n\n    def test_sends_high_priority_for_encrypted(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to an encrypted message.\n        This will happen both in 1:1 rooms and larger rooms.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send an encrypted event\n        # I know there'd normally be set-up of an encrypted room first\n        # but this will do for our purposes\n        self.helper.send_event(\n            room,\n            \"m.room.encrypted\",\n            content={\n                \"algorithm\": \"m.megolm.v1.aes-sha2\",\n                \"sender_key\": \"6lImKbzK51MzWLwHh8tUM3UBBSBrLlgup/OOCGTvumM\",\n                \"ciphertext\": \"AwgAErABoRxwpMipdgiwXgu46rHiWQ0DmRj0qUlPrMraBUDk\"\n                \"leTnJRljpuc7IOhsYbLY3uo2WI0ab/ob41sV+3JEIhODJPqH\"\n                \"TK7cEZaIL+/up9e+dT9VGF5kRTWinzjkeqO8FU5kfdRjm+3w\"\n                \"0sy3o1OCpXXCfO+faPhbV/0HuK4ndx1G+myNfK1Nk/CxfMcT\"\n                \"BT+zDS/Df/QePAHVbrr9uuGB7fW8ogW/ulnydgZPRluusFGv\"\n                \"J3+cg9LoPpZPAmv5Me3ec7NtdlfN0oDZ0gk3TiNkkhsxDG9Y\"\n                \"YcNzl78USI0q8+kOV26Bu5dOBpU4WOuojXZHJlP5lMgdzLLl\"\n                \"EQ0\",\n                \"session_id\": \"IigqfNWLL+ez/Is+Duwp2s4HuCZhFG9b9CZKTYHtQ4A\",\n                \"device_id\": \"AHQDUSTAAA\",\n            },\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Add yet another person \u2014 we want to make this room not a 1:1\n        # (as encrypted messages in a 1:1 currently have tweaks applied\n        #  so it doesn't properly exercise the condition of all encrypted\n        #  messages need to be high).\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Check no push notifications are sent regarding the membership changes\n        # (that would confuse the test)\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 1)\n\n        # Send another encrypted event\n        self.helper.send_event(\n            room,\n            \"m.room.encrypted\",\n            content={\n                \"ciphertext\": \"AwgAEoABtEuic/2DF6oIpNH+q/PonzlhXOVho8dTv0tzFr5m\"\n                \"9vTo50yabx3nxsRlP2WxSqa8I07YftP+EKWCWJvTkg6o7zXq\"\n                \"6CK+GVvLQOVgK50SfvjHqJXN+z1VEqj+5mkZVN/cAgJzoxcH\"\n                \"zFHkwDPJC8kQs47IHd8EO9KBUK4v6+NQ1uE/BIak4qAf9aS/\"\n                \"kI+f0gjn9IY9K6LXlah82A/iRyrIrxkCkE/n0VfvLhaWFecC\"\n                \"sAWTcMLoF6fh1Jpke95mljbmFSpsSd/eEQw\",\n                \"device_id\": \"SRCFTWTHXO\",\n                \"session_id\": \"eMA+bhGczuTz1C5cJR1YbmrnnC6Goni4lbvS5vJ1nG4\",\n                \"algorithm\": \"m.megolm.v1.aes-sha2\",\n                \"sender_key\": \"rC/XSIAiYrVGSuaHMop8/pTZbku4sQKBZwRwukgnN1c\",\n            },\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"high\")\n\n    def test_sends_high_priority_for_one_to_one_only(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message in a one-to-one room.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(room, body=\"Hi!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority \u2014 this is a one-to-one room\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Yet another user joins\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Check no push notifications are sent regarding the membership changes\n        # (that would confuse the test)\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 1)\n\n        # Send another event\n        self.helper.send(room, body=\"Welcome!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_sends_high_priority_for_mention(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message containing the user's display name.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other users join\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(room, body=\"Oh, user, hello!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Send another event, this time with no mention\n        self.helper.send(room, body=\"Are you there?\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_sends_high_priority_for_atroom(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message that contains @room.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room (as other_user so the power levels are compatible with\n        # other_user sending @room).\n        room = self.helper.create_room_as(other_user_id, tok=other_access_token)\n\n        # The other users join\n        self.helper.join(room=room, user=user_id, tok=access_token)\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(\n            room,\n            body=\"@room eeek! There's a spider on the table!\",\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Send another event, this time as someone without the power of @room\n        self.helper.send(\n            room, body=\"@room the spider is gone\", tok=yet_another_access_token\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_push_unread_count_group_by_room(self):\n        \"\"\"\n        The HTTP pusher will group unread count by number of unread rooms.\n        \"\"\"\n        # Carry out common push count tests and setup\n        self._test_push_unread_count()\n\n        # Carry out our option-value specific test\n        #\n        # This push should still only contain an unread count of 1 (for 1 unread room)\n        self.assertEqual(\n            self.push_attempts[5][2][\"notification\"][\"counts\"][\"unread\"], 1\n        )\n\n    @override_config({\"push\": {\"group_unread_count_by_room\": False}})\n    def test_push_unread_count_message_count(self):\n        \"\"\"\n        The HTTP pusher will send the total unread message count.\n        \"\"\"\n        # Carry out common push count tests and setup\n        self._test_push_unread_count()\n\n        # Carry out our option-value specific test\n        #\n        # We're counting every unread message, so there should now be 4 since the\n        # last read receipt\n        self.assertEqual(\n            self.push_attempts[5][2][\"notification\"][\"counts\"][\"unread\"], 4\n        )\n\n    def _test_push_unread_count(self):\n        \"\"\"\n        Tests that the correct unread count appears in sent push notifications\n\n        Note that:\n        * Sending messages will cause push notifications to go out to relevant users\n        * Sending a read receipt will cause a \"badge update\" notification to go out to\n          the user that sent the receipt\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"other_user\", \"pass\")\n        other_access_token = self.login(\"other_user\", \"pass\")\n\n        # Create a room (as other_user)\n        room_id = self.helper.create_room_as(other_user_id, tok=other_access_token)\n\n        # The user to get notified joins\n        self.helper.join(room=room_id, user=user_id, tok=access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        response = self.helper.send(\n            room_id, body=\"Hello there!\", tok=other_access_token\n        )\n        # To get an unread count, the user who is getting notified has to have a read\n        # position in the room. We'll set the read position to this event in a moment\n        first_message_event_id = response[\"event_id\"]\n\n        # Advance time a bit (so the pusher will register something has happened) and\n        # make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n\n        # Check that the unread count for the room is 0\n        #\n        # The unread count is zero as the user has no read receipt in the room yet\n        self.assertEqual(\n            self.push_attempts[0][2][\"notification\"][\"counts\"][\"unread\"], 0\n        )\n\n        # Now set the user's read receipt position to the first event\n        #\n        # This will actually trigger a new notification to be sent out so that\n        # even if the user does not receive another message, their unread\n        # count goes down\n        request, channel = self.make_request(\n            \"POST\",\n            \"/rooms/%s/receipt/m.read/%s\" % (room_id, first_message_event_id),\n            {},\n            access_token=access_token,\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        # Advance time and make the push succeed\n        self.push_attempts[1][0].callback({})\n        self.pump()\n\n        # Unread count is still zero as we've read the only message in the room\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(\n            self.push_attempts[1][2][\"notification\"][\"counts\"][\"unread\"], 0\n        )\n\n        # Send another message\n        self.helper.send(\n            room_id, body=\"How's the weather today?\", tok=other_access_token\n        )\n\n        # Advance time and make the push succeed\n        self.push_attempts[2][0].callback({})\n        self.pump()\n\n        # This push should contain an unread count of 1 as there's now been one\n        # message since our last read receipt\n        self.assertEqual(len(self.push_attempts), 3)\n        self.assertEqual(\n            self.push_attempts[2][2][\"notification\"][\"counts\"][\"unread\"], 1\n        )\n\n        # Since we're grouping by room, sending more messages shouldn't increase the\n        # unread count, as they're all being sent in the same room\n        self.helper.send(room_id, body=\"Hello?\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[3][0].callback({})\n\n        self.helper.send(room_id, body=\"Hello??\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[4][0].callback({})\n\n        self.helper.send(room_id, body=\"HELLO???\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[5][0].callback({})\n\n        self.assertEqual(len(self.push_attempts), 6)", "target": 0}], "function_after": [{"function": "class HTTPPusherTests(HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        receipts.register_servlets,\n    ]\n    user_id = True\n    hijack_auth = False\n\n    def make_homeserver(self, reactor, clock):\n        self.push_attempts = []\n\n        m = Mock()\n\n        def post_json_get_json(url, body):\n            d = Deferred()\n            self.push_attempts.append((d, url, body))\n            return make_deferred_yieldable(d)\n\n        m.post_json_get_json = post_json_get_json\n\n        config = self.default_config()\n        config[\"start_pushers\"] = True\n\n        hs = self.setup_test_homeserver(\n            config=config, proxied_blacklisted_http_client=m\n        )\n\n        return hs\n\n    def test_sends_http(self):\n        \"\"\"\n        The HTTP pusher will send pushes for each message to a HTTP endpoint\n        when configured to do so.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # The other user sends some messages\n        self.helper.send(room, body=\"Hi!\", tok=other_access_token)\n        self.helper.send(room, body=\"There!\", tok=other_access_token)\n\n        # Get the stream ordering before it gets sent\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        last_stream_ordering = pushers[0][\"last_stream_ordering\"]\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # It hasn't succeeded yet, so the stream ordering shouldn't have moved\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertEqual(last_stream_ordering, pushers[0][\"last_stream_ordering\"])\n\n        # One push was attempted to be sent -- it'll be the first message\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(\n            self.push_attempts[0][2][\"notification\"][\"content\"][\"body\"], \"Hi!\"\n        )\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # The stream ordering has increased\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertTrue(pushers[0][\"last_stream_ordering\"] > last_stream_ordering)\n        last_stream_ordering = pushers[0][\"last_stream_ordering\"]\n\n        # Now it'll try and send the second push message, which will be the second one\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n        self.assertEqual(\n            self.push_attempts[1][2][\"notification\"][\"content\"][\"body\"], \"There!\"\n        )\n\n        # Make the second push succeed\n        self.push_attempts[1][0].callback({})\n        self.pump()\n\n        # The stream ordering has increased, again\n        pushers = self.get_success(\n            self.hs.get_datastore().get_pushers_by({\"user_name\": user_id})\n        )\n        pushers = list(pushers)\n        self.assertEqual(len(pushers), 1)\n        self.assertTrue(pushers[0][\"last_stream_ordering\"] > last_stream_ordering)\n\n    def test_sends_high_priority_for_encrypted(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to an encrypted message.\n        This will happen both in 1:1 rooms and larger rooms.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send an encrypted event\n        # I know there'd normally be set-up of an encrypted room first\n        # but this will do for our purposes\n        self.helper.send_event(\n            room,\n            \"m.room.encrypted\",\n            content={\n                \"algorithm\": \"m.megolm.v1.aes-sha2\",\n                \"sender_key\": \"6lImKbzK51MzWLwHh8tUM3UBBSBrLlgup/OOCGTvumM\",\n                \"ciphertext\": \"AwgAErABoRxwpMipdgiwXgu46rHiWQ0DmRj0qUlPrMraBUDk\"\n                \"leTnJRljpuc7IOhsYbLY3uo2WI0ab/ob41sV+3JEIhODJPqH\"\n                \"TK7cEZaIL+/up9e+dT9VGF5kRTWinzjkeqO8FU5kfdRjm+3w\"\n                \"0sy3o1OCpXXCfO+faPhbV/0HuK4ndx1G+myNfK1Nk/CxfMcT\"\n                \"BT+zDS/Df/QePAHVbrr9uuGB7fW8ogW/ulnydgZPRluusFGv\"\n                \"J3+cg9LoPpZPAmv5Me3ec7NtdlfN0oDZ0gk3TiNkkhsxDG9Y\"\n                \"YcNzl78USI0q8+kOV26Bu5dOBpU4WOuojXZHJlP5lMgdzLLl\"\n                \"EQ0\",\n                \"session_id\": \"IigqfNWLL+ez/Is+Duwp2s4HuCZhFG9b9CZKTYHtQ4A\",\n                \"device_id\": \"AHQDUSTAAA\",\n            },\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Add yet another person \u2014 we want to make this room not a 1:1\n        # (as encrypted messages in a 1:1 currently have tweaks applied\n        #  so it doesn't properly exercise the condition of all encrypted\n        #  messages need to be high).\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Check no push notifications are sent regarding the membership changes\n        # (that would confuse the test)\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 1)\n\n        # Send another encrypted event\n        self.helper.send_event(\n            room,\n            \"m.room.encrypted\",\n            content={\n                \"ciphertext\": \"AwgAEoABtEuic/2DF6oIpNH+q/PonzlhXOVho8dTv0tzFr5m\"\n                \"9vTo50yabx3nxsRlP2WxSqa8I07YftP+EKWCWJvTkg6o7zXq\"\n                \"6CK+GVvLQOVgK50SfvjHqJXN+z1VEqj+5mkZVN/cAgJzoxcH\"\n                \"zFHkwDPJC8kQs47IHd8EO9KBUK4v6+NQ1uE/BIak4qAf9aS/\"\n                \"kI+f0gjn9IY9K6LXlah82A/iRyrIrxkCkE/n0VfvLhaWFecC\"\n                \"sAWTcMLoF6fh1Jpke95mljbmFSpsSd/eEQw\",\n                \"device_id\": \"SRCFTWTHXO\",\n                \"session_id\": \"eMA+bhGczuTz1C5cJR1YbmrnnC6Goni4lbvS5vJ1nG4\",\n                \"algorithm\": \"m.megolm.v1.aes-sha2\",\n                \"sender_key\": \"rC/XSIAiYrVGSuaHMop8/pTZbku4sQKBZwRwukgnN1c\",\n            },\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"high\")\n\n    def test_sends_high_priority_for_one_to_one_only(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message in a one-to-one room.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(room, body=\"Hi!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority \u2014 this is a one-to-one room\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Yet another user joins\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Check no push notifications are sent regarding the membership changes\n        # (that would confuse the test)\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 1)\n\n        # Send another event\n        self.helper.send(room, body=\"Welcome!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_sends_high_priority_for_mention(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message containing the user's display name.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other users join\n        self.helper.join(room=room, user=other_user_id, tok=other_access_token)\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(room, body=\"Oh, user, hello!\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Send another event, this time with no mention\n        self.helper.send(room, body=\"Are you there?\", tok=other_access_token)\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_sends_high_priority_for_atroom(self):\n        \"\"\"\n        The HTTP pusher will send pushes at high priority if they correspond\n        to a message that contains @room.\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"otheruser\", \"pass\")\n        other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Register a third user\n        yet_another_user_id = self.register_user(\"yetanotheruser\", \"pass\")\n        yet_another_access_token = self.login(\"yetanotheruser\", \"pass\")\n\n        # Create a room (as other_user so the power levels are compatible with\n        # other_user sending @room).\n        room = self.helper.create_room_as(other_user_id, tok=other_access_token)\n\n        # The other users join\n        self.helper.join(room=room, user=user_id, tok=access_token)\n        self.helper.join(\n            room=room, user=yet_another_user_id, tok=yet_another_access_token\n        )\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        self.helper.send(\n            room,\n            body=\"@room eeek! There's a spider on the table!\",\n            tok=other_access_token,\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        # Make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it with high priority\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n        self.assertEqual(self.push_attempts[0][2][\"notification\"][\"prio\"], \"high\")\n\n        # Send another event, this time as someone without the power of @room\n        self.helper.send(\n            room, body=\"@room the spider is gone\", tok=yet_another_access_token\n        )\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(self.push_attempts[1][1], \"example.com\")\n\n        # check that this is low-priority\n        self.assertEqual(self.push_attempts[1][2][\"notification\"][\"prio\"], \"low\")\n\n    def test_push_unread_count_group_by_room(self):\n        \"\"\"\n        The HTTP pusher will group unread count by number of unread rooms.\n        \"\"\"\n        # Carry out common push count tests and setup\n        self._test_push_unread_count()\n\n        # Carry out our option-value specific test\n        #\n        # This push should still only contain an unread count of 1 (for 1 unread room)\n        self.assertEqual(\n            self.push_attempts[5][2][\"notification\"][\"counts\"][\"unread\"], 1\n        )\n\n    @override_config({\"push\": {\"group_unread_count_by_room\": False}})\n    def test_push_unread_count_message_count(self):\n        \"\"\"\n        The HTTP pusher will send the total unread message count.\n        \"\"\"\n        # Carry out common push count tests and setup\n        self._test_push_unread_count()\n\n        # Carry out our option-value specific test\n        #\n        # We're counting every unread message, so there should now be 4 since the\n        # last read receipt\n        self.assertEqual(\n            self.push_attempts[5][2][\"notification\"][\"counts\"][\"unread\"], 4\n        )\n\n    def _test_push_unread_count(self):\n        \"\"\"\n        Tests that the correct unread count appears in sent push notifications\n\n        Note that:\n        * Sending messages will cause push notifications to go out to relevant users\n        * Sending a read receipt will cause a \"badge update\" notification to go out to\n          the user that sent the receipt\n        \"\"\"\n        # Register the user who gets notified\n        user_id = self.register_user(\"user\", \"pass\")\n        access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        other_user_id = self.register_user(\"other_user\", \"pass\")\n        other_access_token = self.login(\"other_user\", \"pass\")\n\n        # Create a room (as other_user)\n        room_id = self.helper.create_room_as(other_user_id, tok=other_access_token)\n\n        # The user to get notified joins\n        self.helper.join(room=room_id, user=user_id, tok=access_token)\n\n        # Register the pusher\n        user_tuple = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_tuple.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"example.com\"},\n            )\n        )\n\n        # Send a message\n        response = self.helper.send(\n            room_id, body=\"Hello there!\", tok=other_access_token\n        )\n        # To get an unread count, the user who is getting notified has to have a read\n        # position in the room. We'll set the read position to this event in a moment\n        first_message_event_id = response[\"event_id\"]\n\n        # Advance time a bit (so the pusher will register something has happened) and\n        # make the push succeed\n        self.push_attempts[0][0].callback({})\n        self.pump()\n\n        # Check our push made it\n        self.assertEqual(len(self.push_attempts), 1)\n        self.assertEqual(self.push_attempts[0][1], \"example.com\")\n\n        # Check that the unread count for the room is 0\n        #\n        # The unread count is zero as the user has no read receipt in the room yet\n        self.assertEqual(\n            self.push_attempts[0][2][\"notification\"][\"counts\"][\"unread\"], 0\n        )\n\n        # Now set the user's read receipt position to the first event\n        #\n        # This will actually trigger a new notification to be sent out so that\n        # even if the user does not receive another message, their unread\n        # count goes down\n        request, channel = self.make_request(\n            \"POST\",\n            \"/rooms/%s/receipt/m.read/%s\" % (room_id, first_message_event_id),\n            {},\n            access_token=access_token,\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        # Advance time and make the push succeed\n        self.push_attempts[1][0].callback({})\n        self.pump()\n\n        # Unread count is still zero as we've read the only message in the room\n        self.assertEqual(len(self.push_attempts), 2)\n        self.assertEqual(\n            self.push_attempts[1][2][\"notification\"][\"counts\"][\"unread\"], 0\n        )\n\n        # Send another message\n        self.helper.send(\n            room_id, body=\"How's the weather today?\", tok=other_access_token\n        )\n\n        # Advance time and make the push succeed\n        self.push_attempts[2][0].callback({})\n        self.pump()\n\n        # This push should contain an unread count of 1 as there's now been one\n        # message since our last read receipt\n        self.assertEqual(len(self.push_attempts), 3)\n        self.assertEqual(\n            self.push_attempts[2][2][\"notification\"][\"counts\"][\"unread\"], 1\n        )\n\n        # Since we're grouping by room, sending more messages shouldn't increase the\n        # unread count, as they're all being sent in the same room\n        self.helper.send(room_id, body=\"Hello?\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[3][0].callback({})\n\n        self.helper.send(room_id, body=\"Hello??\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[4][0].callback({})\n\n        self.helper.send(room_id, body=\"HELLO???\", tok=other_access_token)\n\n        # Advance time and make the push succeed\n        self.pump()\n        self.push_attempts[5][0].callback({})\n\n        self.assertEqual(len(self.push_attempts), 6)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Freplication%2F_base.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2019 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nfrom typing import Any, Callable, List, Optional, Tuple\n\nimport attr\n\nfrom twisted.internet.interfaces import IConsumer, IPullProducer, IReactorTime\nfrom twisted.internet.protocol import Protocol\nfrom twisted.internet.task import LoopingCall\nfrom twisted.web.http import HTTPChannel\n\nfrom synapse.app.generic_worker import (\n    GenericWorkerReplicationHandler,\n    GenericWorkerServer,\n)\nfrom synapse.http.server import JsonResource\nfrom synapse.http.site import SynapseRequest, SynapseSite\nfrom synapse.replication.http import ReplicationRestResource, streams\nfrom synapse.replication.tcp.handler import ReplicationCommandHandler\nfrom synapse.replication.tcp.protocol import ClientReplicationStreamProtocol\nfrom synapse.replication.tcp.resource import ReplicationStreamProtocolFactory\nfrom synapse.server import HomeServer\nfrom synapse.util import Clock\n\nfrom tests import unittest\nfrom tests.server import FakeTransport\n\ntry:\n    import hiredis\nexcept ImportError:\n    hiredis = None\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseStreamTestCase(unittest.HomeserverTestCase):\n    \"\"\"Base class for tests of the replication streams\"\"\"\n\n    # hiredis is an optional dependency so we don't want to require it for running\n    # the tests.\n    if not hiredis:\n        skip = \"Requires hiredis\"\n\n    servlets = [\n        streams.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        # build a replication server\n        server_factory = ReplicationStreamProtocolFactory(hs)\n        self.streamer = hs.get_replication_streamer()\n        self.server = server_factory.buildProtocol(None)\n\n        # Make a new HomeServer object for the worker\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.worker_hs = self.setup_test_homeserver(\n            federation_http_client=None,\n            homeserver_to_use=GenericWorkerServer,\n            config=self._get_worker_hs_config(),\n            reactor=self.reactor,\n        )\n\n        # Since we use sqlite in memory databases we need to make sure the\n        # databases objects are the same.\n        self.worker_hs.get_datastore().db_pool = hs.get_datastore().db_pool\n\n        self.test_handler = self._build_replication_data_handler()\n        self.worker_hs._replication_data_handler = self.test_handler\n\n        repl_handler = ReplicationCommandHandler(self.worker_hs)\n        self.client = ClientReplicationStreamProtocol(\n            self.worker_hs, \"client\", \"test\", clock, repl_handler,\n        )\n\n        self._client_transport = None\n        self._server_transport = None\n\n    def _get_worker_hs_config(self) -> dict:\n        config = self.default_config()\n        config[\"worker_app\"] = \"synapse.app.generic_worker\"\n        config[\"worker_replication_host\"] = \"testserv\"\n        config[\"worker_replication_http_port\"] = \"8765\"\n        return config\n\n    def _build_replication_data_handler(self):\n        return TestReplicationDataHandler(self.worker_hs)\n\n    def reconnect(self):\n        if self._client_transport:\n            self.client.close()\n\n        if self._server_transport:\n            self.server.close()\n\n        self._client_transport = FakeTransport(self.server, self.reactor)\n        self.client.makeConnection(self._client_transport)\n\n        self._server_transport = FakeTransport(self.client, self.reactor)\n        self.server.makeConnection(self._server_transport)\n\n    def disconnect(self):\n        if self._client_transport:\n            self._client_transport = None\n            self.client.close()\n\n        if self._server_transport:\n            self._server_transport = None\n            self.server.close()\n\n    def replicate(self):\n        \"\"\"Tell the master side of replication that something has happened, and then\n        wait for the replication to occur.\n        \"\"\"\n        self.streamer.on_notifier_poke()\n        self.pump(0.1)\n\n    def handle_http_replication_attempt(self) -> SynapseRequest:\n        \"\"\"Asserts that a connection attempt was made to the master HS on the\n        HTTP replication port, then proxies it to the master HS object to be\n        handled.\n\n        Returns:\n            The request object received by master HS.\n        \"\"\"\n\n        # We should have an outbound connection attempt.\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8765)\n\n        # Set up client side protocol\n        client_protocol = client_factory.buildProtocol(None)\n\n        request_factory = OneShotRequestFactory()\n\n        # Set up the server side protocol\n        channel = _PushHTTPChannel(self.reactor)\n        channel.requestFactory = request_factory\n        channel.site = self.site\n\n        # Connect client to server and vice versa.\n        client_to_server_transport = FakeTransport(\n            channel, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, channel\n        )\n        channel.makeConnection(server_to_client_transport)\n\n        # The request will now be processed by `self.site` and the response\n        # streamed back.\n        self.reactor.advance(0)\n\n        # We tear down the connection so it doesn't get reused without our\n        # knowledge.\n        server_to_client_transport.loseConnection()\n        client_to_server_transport.loseConnection()\n\n        return request_factory.request\n\n    def assert_request_is_get_repl_stream_updates(\n        self, request: SynapseRequest, stream_name: str\n    ):\n        \"\"\"Asserts that the given request is a HTTP replication request for\n        fetching updates for given stream.\n        \"\"\"\n\n        self.assertRegex(\n            request.path,\n            br\"^/_synapse/replication/get_repl_stream_updates/%s/[^/]+$\"\n            % (stream_name.encode(\"ascii\"),),\n        )\n\n        self.assertEqual(request.method, b\"GET\")\n\n\nclass BaseMultiWorkerStreamTestCase(unittest.HomeserverTestCase):\n    \"\"\"Base class for tests running multiple workers.\n\n    Automatically handle HTTP replication requests from workers to master,\n    unlike `BaseStreamTestCase`.\n    \"\"\"\n\n    servlets = []  # type: List[Callable[[HomeServer, JsonResource], None]]\n\n    def setUp(self):\n        super().setUp()\n\n        # build a replication server\n        self.server_factory = ReplicationStreamProtocolFactory(self.hs)\n        self.streamer = self.hs.get_replication_streamer()\n\n        # Fake in memory Redis server that servers can connect to.\n        self._redis_server = FakeRedisPubSubServer()\n\n        store = self.hs.get_datastore()\n        self.database_pool = store.db_pool\n\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.reactor.lookups[\"localhost\"] = \"127.0.0.1\"\n\n        # A map from a HS instance to the associated HTTP Site to use for\n        # handling inbound HTTP requests to that instance.\n        self._hs_to_site = {self.hs: self.site}\n\n        if self.hs.config.redis.redis_enabled:\n            # Handle attempts to connect to fake redis server.\n            self.reactor.add_tcp_client_callback(\n                \"localhost\", 6379, self.connect_any_redis_attempts,\n            )\n\n            self.hs.get_tcp_replication().start_replication(self.hs)\n\n        # When we see a connection attempt to the master replication listener we\n        # automatically set up the connection. This is so that tests don't\n        # manually have to go and explicitly set it up each time (plus sometimes\n        # it is impossible to write the handling explicitly in the tests).\n        #\n        # Register the master replication listener:\n        self.reactor.add_tcp_client_callback(\n            \"1.2.3.4\",\n            8765,\n            lambda: self._handle_http_replication_attempt(self.hs, 8765),\n        )\n\n    def create_test_resource(self):\n        \"\"\"Overrides `HomeserverTestCase.create_test_resource`.\n        \"\"\"\n        # We override this so that it automatically registers all the HTTP\n        # replication servlets, without having to explicitly do that in all\n        # subclassses.\n\n        resource = ReplicationRestResource(self.hs)\n\n        for servlet in self.servlets:\n            servlet(self.hs, resource)\n\n        return resource\n\n    def make_worker_hs(\n        self, worker_app: str, extra_config: dict = {}, **kwargs\n    ) -> HomeServer:\n        \"\"\"Make a new worker HS instance, correctly connecting replcation\n        stream to the master HS.\n\n        Args:\n            worker_app: Type of worker, e.g. `synapse.app.federation_sender`.\n            extra_config: Any extra config to use for this instances.\n            **kwargs: Options that get passed to `self.setup_test_homeserver`,\n                useful to e.g. pass some mocks for things like `federation_http_client`\n\n        Returns:\n            The new worker HomeServer instance.\n        \"\"\"\n\n        config = self._get_worker_hs_config()\n        config[\"worker_app\"] = worker_app\n        config.update(extra_config)\n\n        worker_hs = self.setup_test_homeserver(\n            homeserver_to_use=GenericWorkerServer,\n            config=config,\n            reactor=self.reactor,\n            **kwargs,\n        )\n\n        # If the instance is in the `instance_map` config then workers may try\n        # and send HTTP requests to it, so we register it with\n        # `_handle_http_replication_attempt` like we do with the master HS.\n        instance_name = worker_hs.get_instance_name()\n        instance_loc = worker_hs.config.worker.instance_map.get(instance_name)\n        if instance_loc:\n            # Ensure the host is one that has a fake DNS entry.\n            if instance_loc.host not in self.reactor.lookups:\n                raise Exception(\n                    \"Host does not have an IP for instance_map[%r].host = %r\"\n                    % (instance_name, instance_loc.host,)\n                )\n\n            self.reactor.add_tcp_client_callback(\n                self.reactor.lookups[instance_loc.host],\n                instance_loc.port,\n                lambda: self._handle_http_replication_attempt(\n                    worker_hs, instance_loc.port\n                ),\n            )\n\n        store = worker_hs.get_datastore()\n        store.db_pool._db_pool = self.database_pool._db_pool\n\n        # Set up TCP replication between master and the new worker if we don't\n        # have Redis support enabled.\n        if not worker_hs.config.redis_enabled:\n            repl_handler = ReplicationCommandHandler(worker_hs)\n            client = ClientReplicationStreamProtocol(\n                worker_hs, \"client\", \"test\", self.clock, repl_handler,\n            )\n            server = self.server_factory.buildProtocol(None)\n\n            client_transport = FakeTransport(server, self.reactor)\n            client.makeConnection(client_transport)\n\n            server_transport = FakeTransport(client, self.reactor)\n            server.makeConnection(server_transport)\n\n        # Set up a resource for the worker\n        resource = ReplicationRestResource(worker_hs)\n\n        for servlet in self.servlets:\n            servlet(worker_hs, resource)\n\n        self._hs_to_site[worker_hs] = SynapseSite(\n            logger_name=\"synapse.access.http.fake\",\n            site_tag=\"{}-{}\".format(\n                worker_hs.config.server.server_name, worker_hs.get_instance_name()\n            ),\n            config=worker_hs.config.server.listeners[0],\n            resource=resource,\n            server_version_string=\"1\",\n        )\n\n        if worker_hs.config.redis.redis_enabled:\n            worker_hs.get_tcp_replication().start_replication(worker_hs)\n\n        return worker_hs\n\n    def _get_worker_hs_config(self) -> dict:\n        config = self.default_config()\n        config[\"worker_replication_host\"] = \"testserv\"\n        config[\"worker_replication_http_port\"] = \"8765\"\n        return config\n\n    def replicate(self):\n        \"\"\"Tell the master side of replication that something has happened, and then\n        wait for the replication to occur.\n        \"\"\"\n        self.streamer.on_notifier_poke()\n        self.pump()\n\n    def _handle_http_replication_attempt(self, hs, repl_port):\n        \"\"\"Handles a connection attempt to the given HS replication HTTP\n        listener on the given port.\n        \"\"\"\n\n        # We should have at least one outbound connection attempt, where the\n        # last is one to the HTTP repication IP/port.\n        clients = self.reactor.tcpClients\n        self.assertGreaterEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, repl_port)\n\n        # Set up client side protocol\n        client_protocol = client_factory.buildProtocol(None)\n\n        request_factory = OneShotRequestFactory()\n\n        # Set up the server side protocol\n        channel = _PushHTTPChannel(self.reactor)\n        channel.requestFactory = request_factory\n        channel.site = self._hs_to_site[hs]\n\n        # Connect client to server and vice versa.\n        client_to_server_transport = FakeTransport(\n            channel, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, channel\n        )\n        channel.makeConnection(server_to_client_transport)\n\n        # Note: at this point we've wired everything up, but we need to return\n        # before the data starts flowing over the connections as this is called\n        # inside `connecTCP` before the connection has been passed back to the\n        # code that requested the TCP connection.\n\n    def connect_any_redis_attempts(self):\n        \"\"\"If redis is enabled we need to deal with workers connecting to a\n        redis server. We don't want to use a real Redis server so we use a\n        fake one.\n        \"\"\"\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"localhost\")\n        self.assertEqual(port, 6379)\n\n        client_protocol = client_factory.buildProtocol(None)\n        server_protocol = self._redis_server.buildProtocol(None)\n\n        client_to_server_transport = FakeTransport(\n            server_protocol, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, server_protocol\n        )\n        server_protocol.makeConnection(server_to_client_transport)\n\n        return client_to_server_transport, server_to_client_transport\n\n\nclass TestReplicationDataHandler(GenericWorkerReplicationHandler):\n    \"\"\"Drop-in for ReplicationDataHandler which just collects RDATA rows\"\"\"\n\n    def __init__(self, hs: HomeServer):\n        super().__init__(hs)\n\n        # list of received (stream_name, token, row) tuples\n        self.received_rdata_rows = []  # type: List[Tuple[str, int, Any]]\n\n    async def on_rdata(self, stream_name, instance_name, token, rows):\n        await super().on_rdata(stream_name, instance_name, token, rows)\n        for r in rows:\n            self.received_rdata_rows.append((stream_name, token, r))\n\n\n@attr.s()\nclass OneShotRequestFactory:\n    \"\"\"A simple request factory that generates a single `SynapseRequest` and\n    stores it for future use. Can only be used once.\n    \"\"\"\n\n    request = attr.ib(default=None)\n\n    def __call__(self, *args, **kwargs):\n        assert self.request is None\n\n        self.request = SynapseRequest(*args, **kwargs)\n        return self.request\n\n\nclass _PushHTTPChannel(HTTPChannel):\n    \"\"\"A HTTPChannel that wraps pull producers to push producers.\n\n    This is a hack to get around the fact that HTTPChannel transparently wraps a\n    pull producer (which is what Synapse uses to reply to requests) with\n    `_PullToPush` to convert it to a push producer. Unfortunately `_PullToPush`\n    uses the standard reactor rather than letting us use our test reactor, which\n    makes it very hard to test.\n    \"\"\"\n\n    def __init__(self, reactor: IReactorTime):\n        super().__init__()\n        self.reactor = reactor\n\n        self._pull_to_push_producer = None  # type: Optional[_PullToPushProducer]\n\n    def registerProducer(self, producer, streaming):\n        # Convert pull producers to push producer.\n        if not streaming:\n            self._pull_to_push_producer = _PullToPushProducer(\n                self.reactor, producer, self\n            )\n            producer = self._pull_to_push_producer\n\n        super().registerProducer(producer, True)\n\n    def unregisterProducer(self):\n        if self._pull_to_push_producer:\n            # We need to manually stop the _PullToPushProducer.\n            self._pull_to_push_producer.stop()\n\n    def checkPersistence(self, request, version):\n        \"\"\"Check whether the connection can be re-used\n        \"\"\"\n        # We hijack this to always say no for ease of wiring stuff up in\n        # `handle_http_replication_attempt`.\n        request.responseHeaders.setRawHeaders(b\"connection\", [b\"close\"])\n        return False\n\n\nclass _PullToPushProducer:\n    \"\"\"A push producer that wraps a pull producer.\n    \"\"\"\n\n    def __init__(\n        self, reactor: IReactorTime, producer: IPullProducer, consumer: IConsumer\n    ):\n        self._clock = Clock(reactor)\n        self._producer = producer\n        self._consumer = consumer\n\n        # While running we use a looping call with a zero delay to call\n        # resumeProducing on given producer.\n        self._looping_call = None  # type: Optional[LoopingCall]\n\n        # We start writing next reactor tick.\n        self._start_loop()\n\n    def _start_loop(self):\n        \"\"\"Start the looping call to\n        \"\"\"\n\n        if not self._looping_call:\n            # Start a looping call which runs every tick.\n            self._looping_call = self._clock.looping_call(self._run_once, 0)\n\n    def stop(self):\n        \"\"\"Stops calling resumeProducing.\n        \"\"\"\n        if self._looping_call:\n            self._looping_call.stop()\n            self._looping_call = None\n\n    def pauseProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self.stop()\n\n    def resumeProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self._start_loop()\n\n    def stopProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self.stop()\n        self._producer.stopProducing()\n\n    def _run_once(self):\n        \"\"\"Calls resumeProducing on producer once.\n        \"\"\"\n\n        try:\n            self._producer.resumeProducing()\n        except Exception:\n            logger.exception(\"Failed to call resumeProducing\")\n            try:\n                self._consumer.unregisterProducer()\n            except Exception:\n                pass\n\n            self.stopProducing()\n\n\nclass FakeRedisPubSubServer:\n    \"\"\"A fake Redis server for pub/sub.\n    \"\"\"\n\n    def __init__(self):\n        self._subscribers = set()\n\n    def add_subscriber(self, conn):\n        \"\"\"A connection has called SUBSCRIBE\n        \"\"\"\n        self._subscribers.add(conn)\n\n    def remove_subscriber(self, conn):\n        \"\"\"A connection has called UNSUBSCRIBE\n        \"\"\"\n        self._subscribers.discard(conn)\n\n    def publish(self, conn, channel, msg) -> int:\n        \"\"\"A connection want to publish a message to subscribers.\n        \"\"\"\n        for sub in self._subscribers:\n            sub.send([\"message\", channel, msg])\n\n        return len(self._subscribers)\n\n    def buildProtocol(self, addr):\n        return FakeRedisPubSubProtocol(self)\n\n\nclass FakeRedisPubSubProtocol(Protocol):\n    \"\"\"A connection from a client talking to the fake Redis server.\n    \"\"\"\n\n    def __init__(self, server: FakeRedisPubSubServer):\n        self._server = server\n        self._reader = hiredis.Reader()\n\n    def dataReceived(self, data):\n        self._reader.feed(data)\n\n        # We might get multiple messages in one packet.\n        while True:\n            msg = self._reader.gets()\n\n            if msg is False:\n                # No more messages.\n                return\n\n            if not isinstance(msg, list):\n                # Inbound commands should always be a list\n                raise Exception(\"Expected redis list\")\n\n            self.handle_command(msg[0], *msg[1:])\n\n    def handle_command(self, command, *args):\n        \"\"\"Received a Redis command from the client.\n        \"\"\"\n\n        # We currently only support pub/sub.\n        if command == b\"PUBLISH\":\n            channel, message = args\n            num_subscribers = self._server.publish(self, channel, message)\n            self.send(num_subscribers)\n        elif command == b\"SUBSCRIBE\":\n            (channel,) = args\n            self._server.add_subscriber(self)\n            self.send([\"subscribe\", channel, 1])\n        else:\n            raise Exception(\"Unknown command\")\n\n    def send(self, msg):\n        \"\"\"Send a message back to the client.\n        \"\"\"\n        raw = self.encode(msg).encode(\"utf-8\")\n\n        self.transport.write(raw)\n        self.transport.flush()\n\n    def encode(self, obj):\n        \"\"\"Encode an object to its Redis format.\n\n        Supports: strings/bytes, integers and list/tuples.\n        \"\"\"\n\n        if isinstance(obj, bytes):\n            # We assume bytes are just unicode strings.\n            obj = obj.decode(\"utf-8\")\n\n        if isinstance(obj, str):\n            return \"${len}\\r\\n{str}\\r\\n\".format(len=len(obj), str=obj)\n        if isinstance(obj, int):\n            return \":{val}\\r\\n\".format(val=obj)\n        if isinstance(obj, (list, tuple)):\n            items = \"\".join(self.encode(a) for a in obj)\n            return \"*{len}\\r\\n{items}\".format(len=len(obj), items=items)\n\n        raise Exception(\"Unrecognized type for encoding redis: %r: %r\", type(obj), obj)\n\n    def connectionLost(self, reason):\n        self._server.remove_subscriber(self)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2019 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nfrom typing import Any, Callable, List, Optional, Tuple\n\nimport attr\n\nfrom twisted.internet.interfaces import IConsumer, IPullProducer, IReactorTime\nfrom twisted.internet.protocol import Protocol\nfrom twisted.internet.task import LoopingCall\nfrom twisted.web.http import HTTPChannel\n\nfrom synapse.app.generic_worker import (\n    GenericWorkerReplicationHandler,\n    GenericWorkerServer,\n)\nfrom synapse.http.server import JsonResource\nfrom synapse.http.site import SynapseRequest, SynapseSite\nfrom synapse.replication.http import ReplicationRestResource, streams\nfrom synapse.replication.tcp.handler import ReplicationCommandHandler\nfrom synapse.replication.tcp.protocol import ClientReplicationStreamProtocol\nfrom synapse.replication.tcp.resource import ReplicationStreamProtocolFactory\nfrom synapse.server import HomeServer\nfrom synapse.util import Clock\n\nfrom tests import unittest\nfrom tests.server import FakeTransport\n\ntry:\n    import hiredis\nexcept ImportError:\n    hiredis = None\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseStreamTestCase(unittest.HomeserverTestCase):\n    \"\"\"Base class for tests of the replication streams\"\"\"\n\n    # hiredis is an optional dependency so we don't want to require it for running\n    # the tests.\n    if not hiredis:\n        skip = \"Requires hiredis\"\n\n    servlets = [\n        streams.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        # build a replication server\n        server_factory = ReplicationStreamProtocolFactory(hs)\n        self.streamer = hs.get_replication_streamer()\n        self.server = server_factory.buildProtocol(None)\n\n        # Make a new HomeServer object for the worker\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.worker_hs = self.setup_test_homeserver(\n            http_client=None,\n            homeserver_to_use=GenericWorkerServer,\n            config=self._get_worker_hs_config(),\n            reactor=self.reactor,\n        )\n\n        # Since we use sqlite in memory databases we need to make sure the\n        # databases objects are the same.\n        self.worker_hs.get_datastore().db_pool = hs.get_datastore().db_pool\n\n        self.test_handler = self._build_replication_data_handler()\n        self.worker_hs._replication_data_handler = self.test_handler\n\n        repl_handler = ReplicationCommandHandler(self.worker_hs)\n        self.client = ClientReplicationStreamProtocol(\n            self.worker_hs, \"client\", \"test\", clock, repl_handler,\n        )\n\n        self._client_transport = None\n        self._server_transport = None\n\n    def _get_worker_hs_config(self) -> dict:\n        config = self.default_config()\n        config[\"worker_app\"] = \"synapse.app.generic_worker\"\n        config[\"worker_replication_host\"] = \"testserv\"\n        config[\"worker_replication_http_port\"] = \"8765\"\n        return config\n\n    def _build_replication_data_handler(self):\n        return TestReplicationDataHandler(self.worker_hs)\n\n    def reconnect(self):\n        if self._client_transport:\n            self.client.close()\n\n        if self._server_transport:\n            self.server.close()\n\n        self._client_transport = FakeTransport(self.server, self.reactor)\n        self.client.makeConnection(self._client_transport)\n\n        self._server_transport = FakeTransport(self.client, self.reactor)\n        self.server.makeConnection(self._server_transport)\n\n    def disconnect(self):\n        if self._client_transport:\n            self._client_transport = None\n            self.client.close()\n\n        if self._server_transport:\n            self._server_transport = None\n            self.server.close()\n\n    def replicate(self):\n        \"\"\"Tell the master side of replication that something has happened, and then\n        wait for the replication to occur.\n        \"\"\"\n        self.streamer.on_notifier_poke()\n        self.pump(0.1)\n\n    def handle_http_replication_attempt(self) -> SynapseRequest:\n        \"\"\"Asserts that a connection attempt was made to the master HS on the\n        HTTP replication port, then proxies it to the master HS object to be\n        handled.\n\n        Returns:\n            The request object received by master HS.\n        \"\"\"\n\n        # We should have an outbound connection attempt.\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8765)\n\n        # Set up client side protocol\n        client_protocol = client_factory.buildProtocol(None)\n\n        request_factory = OneShotRequestFactory()\n\n        # Set up the server side protocol\n        channel = _PushHTTPChannel(self.reactor)\n        channel.requestFactory = request_factory\n        channel.site = self.site\n\n        # Connect client to server and vice versa.\n        client_to_server_transport = FakeTransport(\n            channel, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, channel\n        )\n        channel.makeConnection(server_to_client_transport)\n\n        # The request will now be processed by `self.site` and the response\n        # streamed back.\n        self.reactor.advance(0)\n\n        # We tear down the connection so it doesn't get reused without our\n        # knowledge.\n        server_to_client_transport.loseConnection()\n        client_to_server_transport.loseConnection()\n\n        return request_factory.request\n\n    def assert_request_is_get_repl_stream_updates(\n        self, request: SynapseRequest, stream_name: str\n    ):\n        \"\"\"Asserts that the given request is a HTTP replication request for\n        fetching updates for given stream.\n        \"\"\"\n\n        self.assertRegex(\n            request.path,\n            br\"^/_synapse/replication/get_repl_stream_updates/%s/[^/]+$\"\n            % (stream_name.encode(\"ascii\"),),\n        )\n\n        self.assertEqual(request.method, b\"GET\")\n\n\nclass BaseMultiWorkerStreamTestCase(unittest.HomeserverTestCase):\n    \"\"\"Base class for tests running multiple workers.\n\n    Automatically handle HTTP replication requests from workers to master,\n    unlike `BaseStreamTestCase`.\n    \"\"\"\n\n    servlets = []  # type: List[Callable[[HomeServer, JsonResource], None]]\n\n    def setUp(self):\n        super().setUp()\n\n        # build a replication server\n        self.server_factory = ReplicationStreamProtocolFactory(self.hs)\n        self.streamer = self.hs.get_replication_streamer()\n\n        # Fake in memory Redis server that servers can connect to.\n        self._redis_server = FakeRedisPubSubServer()\n\n        store = self.hs.get_datastore()\n        self.database_pool = store.db_pool\n\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.reactor.lookups[\"localhost\"] = \"127.0.0.1\"\n\n        # A map from a HS instance to the associated HTTP Site to use for\n        # handling inbound HTTP requests to that instance.\n        self._hs_to_site = {self.hs: self.site}\n\n        if self.hs.config.redis.redis_enabled:\n            # Handle attempts to connect to fake redis server.\n            self.reactor.add_tcp_client_callback(\n                \"localhost\", 6379, self.connect_any_redis_attempts,\n            )\n\n            self.hs.get_tcp_replication().start_replication(self.hs)\n\n        # When we see a connection attempt to the master replication listener we\n        # automatically set up the connection. This is so that tests don't\n        # manually have to go and explicitly set it up each time (plus sometimes\n        # it is impossible to write the handling explicitly in the tests).\n        #\n        # Register the master replication listener:\n        self.reactor.add_tcp_client_callback(\n            \"1.2.3.4\",\n            8765,\n            lambda: self._handle_http_replication_attempt(self.hs, 8765),\n        )\n\n    def create_test_resource(self):\n        \"\"\"Overrides `HomeserverTestCase.create_test_resource`.\n        \"\"\"\n        # We override this so that it automatically registers all the HTTP\n        # replication servlets, without having to explicitly do that in all\n        # subclassses.\n\n        resource = ReplicationRestResource(self.hs)\n\n        for servlet in self.servlets:\n            servlet(self.hs, resource)\n\n        return resource\n\n    def make_worker_hs(\n        self, worker_app: str, extra_config: dict = {}, **kwargs\n    ) -> HomeServer:\n        \"\"\"Make a new worker HS instance, correctly connecting replcation\n        stream to the master HS.\n\n        Args:\n            worker_app: Type of worker, e.g. `synapse.app.federation_sender`.\n            extra_config: Any extra config to use for this instances.\n            **kwargs: Options that get passed to `self.setup_test_homeserver`,\n                useful to e.g. pass some mocks for things like `http_client`\n\n        Returns:\n            The new worker HomeServer instance.\n        \"\"\"\n\n        config = self._get_worker_hs_config()\n        config[\"worker_app\"] = worker_app\n        config.update(extra_config)\n\n        worker_hs = self.setup_test_homeserver(\n            homeserver_to_use=GenericWorkerServer,\n            config=config,\n            reactor=self.reactor,\n            **kwargs,\n        )\n\n        # If the instance is in the `instance_map` config then workers may try\n        # and send HTTP requests to it, so we register it with\n        # `_handle_http_replication_attempt` like we do with the master HS.\n        instance_name = worker_hs.get_instance_name()\n        instance_loc = worker_hs.config.worker.instance_map.get(instance_name)\n        if instance_loc:\n            # Ensure the host is one that has a fake DNS entry.\n            if instance_loc.host not in self.reactor.lookups:\n                raise Exception(\n                    \"Host does not have an IP for instance_map[%r].host = %r\"\n                    % (instance_name, instance_loc.host,)\n                )\n\n            self.reactor.add_tcp_client_callback(\n                self.reactor.lookups[instance_loc.host],\n                instance_loc.port,\n                lambda: self._handle_http_replication_attempt(\n                    worker_hs, instance_loc.port\n                ),\n            )\n\n        store = worker_hs.get_datastore()\n        store.db_pool._db_pool = self.database_pool._db_pool\n\n        # Set up TCP replication between master and the new worker if we don't\n        # have Redis support enabled.\n        if not worker_hs.config.redis_enabled:\n            repl_handler = ReplicationCommandHandler(worker_hs)\n            client = ClientReplicationStreamProtocol(\n                worker_hs, \"client\", \"test\", self.clock, repl_handler,\n            )\n            server = self.server_factory.buildProtocol(None)\n\n            client_transport = FakeTransport(server, self.reactor)\n            client.makeConnection(client_transport)\n\n            server_transport = FakeTransport(client, self.reactor)\n            server.makeConnection(server_transport)\n\n        # Set up a resource for the worker\n        resource = ReplicationRestResource(worker_hs)\n\n        for servlet in self.servlets:\n            servlet(worker_hs, resource)\n\n        self._hs_to_site[worker_hs] = SynapseSite(\n            logger_name=\"synapse.access.http.fake\",\n            site_tag=\"{}-{}\".format(\n                worker_hs.config.server.server_name, worker_hs.get_instance_name()\n            ),\n            config=worker_hs.config.server.listeners[0],\n            resource=resource,\n            server_version_string=\"1\",\n        )\n\n        if worker_hs.config.redis.redis_enabled:\n            worker_hs.get_tcp_replication().start_replication(worker_hs)\n\n        return worker_hs\n\n    def _get_worker_hs_config(self) -> dict:\n        config = self.default_config()\n        config[\"worker_replication_host\"] = \"testserv\"\n        config[\"worker_replication_http_port\"] = \"8765\"\n        return config\n\n    def replicate(self):\n        \"\"\"Tell the master side of replication that something has happened, and then\n        wait for the replication to occur.\n        \"\"\"\n        self.streamer.on_notifier_poke()\n        self.pump()\n\n    def _handle_http_replication_attempt(self, hs, repl_port):\n        \"\"\"Handles a connection attempt to the given HS replication HTTP\n        listener on the given port.\n        \"\"\"\n\n        # We should have at least one outbound connection attempt, where the\n        # last is one to the HTTP repication IP/port.\n        clients = self.reactor.tcpClients\n        self.assertGreaterEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, repl_port)\n\n        # Set up client side protocol\n        client_protocol = client_factory.buildProtocol(None)\n\n        request_factory = OneShotRequestFactory()\n\n        # Set up the server side protocol\n        channel = _PushHTTPChannel(self.reactor)\n        channel.requestFactory = request_factory\n        channel.site = self._hs_to_site[hs]\n\n        # Connect client to server and vice versa.\n        client_to_server_transport = FakeTransport(\n            channel, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, channel\n        )\n        channel.makeConnection(server_to_client_transport)\n\n        # Note: at this point we've wired everything up, but we need to return\n        # before the data starts flowing over the connections as this is called\n        # inside `connecTCP` before the connection has been passed back to the\n        # code that requested the TCP connection.\n\n    def connect_any_redis_attempts(self):\n        \"\"\"If redis is enabled we need to deal with workers connecting to a\n        redis server. We don't want to use a real Redis server so we use a\n        fake one.\n        \"\"\"\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"localhost\")\n        self.assertEqual(port, 6379)\n\n        client_protocol = client_factory.buildProtocol(None)\n        server_protocol = self._redis_server.buildProtocol(None)\n\n        client_to_server_transport = FakeTransport(\n            server_protocol, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, server_protocol\n        )\n        server_protocol.makeConnection(server_to_client_transport)\n\n        return client_to_server_transport, server_to_client_transport\n\n\nclass TestReplicationDataHandler(GenericWorkerReplicationHandler):\n    \"\"\"Drop-in for ReplicationDataHandler which just collects RDATA rows\"\"\"\n\n    def __init__(self, hs: HomeServer):\n        super().__init__(hs)\n\n        # list of received (stream_name, token, row) tuples\n        self.received_rdata_rows = []  # type: List[Tuple[str, int, Any]]\n\n    async def on_rdata(self, stream_name, instance_name, token, rows):\n        await super().on_rdata(stream_name, instance_name, token, rows)\n        for r in rows:\n            self.received_rdata_rows.append((stream_name, token, r))\n\n\n@attr.s()\nclass OneShotRequestFactory:\n    \"\"\"A simple request factory that generates a single `SynapseRequest` and\n    stores it for future use. Can only be used once.\n    \"\"\"\n\n    request = attr.ib(default=None)\n\n    def __call__(self, *args, **kwargs):\n        assert self.request is None\n\n        self.request = SynapseRequest(*args, **kwargs)\n        return self.request\n\n\nclass _PushHTTPChannel(HTTPChannel):\n    \"\"\"A HTTPChannel that wraps pull producers to push producers.\n\n    This is a hack to get around the fact that HTTPChannel transparently wraps a\n    pull producer (which is what Synapse uses to reply to requests) with\n    `_PullToPush` to convert it to a push producer. Unfortunately `_PullToPush`\n    uses the standard reactor rather than letting us use our test reactor, which\n    makes it very hard to test.\n    \"\"\"\n\n    def __init__(self, reactor: IReactorTime):\n        super().__init__()\n        self.reactor = reactor\n\n        self._pull_to_push_producer = None  # type: Optional[_PullToPushProducer]\n\n    def registerProducer(self, producer, streaming):\n        # Convert pull producers to push producer.\n        if not streaming:\n            self._pull_to_push_producer = _PullToPushProducer(\n                self.reactor, producer, self\n            )\n            producer = self._pull_to_push_producer\n\n        super().registerProducer(producer, True)\n\n    def unregisterProducer(self):\n        if self._pull_to_push_producer:\n            # We need to manually stop the _PullToPushProducer.\n            self._pull_to_push_producer.stop()\n\n    def checkPersistence(self, request, version):\n        \"\"\"Check whether the connection can be re-used\n        \"\"\"\n        # We hijack this to always say no for ease of wiring stuff up in\n        # `handle_http_replication_attempt`.\n        request.responseHeaders.setRawHeaders(b\"connection\", [b\"close\"])\n        return False\n\n\nclass _PullToPushProducer:\n    \"\"\"A push producer that wraps a pull producer.\n    \"\"\"\n\n    def __init__(\n        self, reactor: IReactorTime, producer: IPullProducer, consumer: IConsumer\n    ):\n        self._clock = Clock(reactor)\n        self._producer = producer\n        self._consumer = consumer\n\n        # While running we use a looping call with a zero delay to call\n        # resumeProducing on given producer.\n        self._looping_call = None  # type: Optional[LoopingCall]\n\n        # We start writing next reactor tick.\n        self._start_loop()\n\n    def _start_loop(self):\n        \"\"\"Start the looping call to\n        \"\"\"\n\n        if not self._looping_call:\n            # Start a looping call which runs every tick.\n            self._looping_call = self._clock.looping_call(self._run_once, 0)\n\n    def stop(self):\n        \"\"\"Stops calling resumeProducing.\n        \"\"\"\n        if self._looping_call:\n            self._looping_call.stop()\n            self._looping_call = None\n\n    def pauseProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self.stop()\n\n    def resumeProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self._start_loop()\n\n    def stopProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self.stop()\n        self._producer.stopProducing()\n\n    def _run_once(self):\n        \"\"\"Calls resumeProducing on producer once.\n        \"\"\"\n\n        try:\n            self._producer.resumeProducing()\n        except Exception:\n            logger.exception(\"Failed to call resumeProducing\")\n            try:\n                self._consumer.unregisterProducer()\n            except Exception:\n                pass\n\n            self.stopProducing()\n\n\nclass FakeRedisPubSubServer:\n    \"\"\"A fake Redis server for pub/sub.\n    \"\"\"\n\n    def __init__(self):\n        self._subscribers = set()\n\n    def add_subscriber(self, conn):\n        \"\"\"A connection has called SUBSCRIBE\n        \"\"\"\n        self._subscribers.add(conn)\n\n    def remove_subscriber(self, conn):\n        \"\"\"A connection has called UNSUBSCRIBE\n        \"\"\"\n        self._subscribers.discard(conn)\n\n    def publish(self, conn, channel, msg) -> int:\n        \"\"\"A connection want to publish a message to subscribers.\n        \"\"\"\n        for sub in self._subscribers:\n            sub.send([\"message\", channel, msg])\n\n        return len(self._subscribers)\n\n    def buildProtocol(self, addr):\n        return FakeRedisPubSubProtocol(self)\n\n\nclass FakeRedisPubSubProtocol(Protocol):\n    \"\"\"A connection from a client talking to the fake Redis server.\n    \"\"\"\n\n    def __init__(self, server: FakeRedisPubSubServer):\n        self._server = server\n        self._reader = hiredis.Reader()\n\n    def dataReceived(self, data):\n        self._reader.feed(data)\n\n        # We might get multiple messages in one packet.\n        while True:\n            msg = self._reader.gets()\n\n            if msg is False:\n                # No more messages.\n                return\n\n            if not isinstance(msg, list):\n                # Inbound commands should always be a list\n                raise Exception(\"Expected redis list\")\n\n            self.handle_command(msg[0], *msg[1:])\n\n    def handle_command(self, command, *args):\n        \"\"\"Received a Redis command from the client.\n        \"\"\"\n\n        # We currently only support pub/sub.\n        if command == b\"PUBLISH\":\n            channel, message = args\n            num_subscribers = self._server.publish(self, channel, message)\n            self.send(num_subscribers)\n        elif command == b\"SUBSCRIBE\":\n            (channel,) = args\n            self._server.add_subscriber(self)\n            self.send([\"subscribe\", channel, 1])\n        else:\n            raise Exception(\"Unknown command\")\n\n    def send(self, msg):\n        \"\"\"Send a message back to the client.\n        \"\"\"\n        raw = self.encode(msg).encode(\"utf-8\")\n\n        self.transport.write(raw)\n        self.transport.flush()\n\n    def encode(self, obj):\n        \"\"\"Encode an object to its Redis format.\n\n        Supports: strings/bytes, integers and list/tuples.\n        \"\"\"\n\n        if isinstance(obj, bytes):\n            # We assume bytes are just unicode strings.\n            obj = obj.decode(\"utf-8\")\n\n        if isinstance(obj, str):\n            return \"${len}\\r\\n{str}\\r\\n\".format(len=len(obj), str=obj)\n        if isinstance(obj, int):\n            return \":{val}\\r\\n\".format(val=obj)\n        if isinstance(obj, (list, tuple)):\n            items = \"\".join(self.encode(a) for a in obj)\n            return \"*{len}\\r\\n{items}\".format(len=len(obj), items=items)\n\n        raise Exception(\"Unrecognized type for encoding redis: %r: %r\", type(obj), obj)\n\n    def connectionLost(self, reason):\n        self._server.remove_subscriber(self)\n", "patch": "@@ -67,7 +67,7 @@ def prepare(self, reactor, clock, hs):\n         # Make a new HomeServer object for the worker\n         self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n         self.worker_hs = self.setup_test_homeserver(\n-            http_client=None,\n+            federation_http_client=None,\n             homeserver_to_use=GenericWorkerServer,\n             config=self._get_worker_hs_config(),\n             reactor=self.reactor,\n@@ -264,7 +264,7 @@ def make_worker_hs(\n             worker_app: Type of worker, e.g. `synapse.app.federation_sender`.\n             extra_config: Any extra config to use for this instances.\n             **kwargs: Options that get passed to `self.setup_test_homeserver`,\n-                useful to e.g. pass some mocks for things like `http_client`\n+                useful to e.g. pass some mocks for things like `federation_http_client`\n \n         Returns:\n             The new worker HomeServer instance.", "file_path": "files/2021_2/42", "file_language": "py", "file_name": "tests/replication/_base.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 1, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class BaseStreamTestCase(unittest.HomeserverTestCase):\n    \"\"\"Base class for tests of the replication streams\"\"\"\n\n    # hiredis is an optional dependency so we don't want to require it for running\n    # the tests.\n    if not hiredis:\n        skip = \"Requires hiredis\"\n\n    servlets = [\n        streams.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        # build a replication server\n        server_factory = ReplicationStreamProtocolFactory(hs)\n        self.streamer = hs.get_replication_streamer()\n        self.server = server_factory.buildProtocol(None)\n\n        # Make a new HomeServer object for the worker\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.worker_hs = self.setup_test_homeserver(\n            http_client=None,\n            homeserver_to_use=GenericWorkerServer,\n            config=self._get_worker_hs_config(),\n            reactor=self.reactor,\n        )\n\n        # Since we use sqlite in memory databases we need to make sure the\n        # databases objects are the same.\n        self.worker_hs.get_datastore().db_pool = hs.get_datastore().db_pool\n\n        self.test_handler = self._build_replication_data_handler()\n        self.worker_hs._replication_data_handler = self.test_handler\n\n        repl_handler = ReplicationCommandHandler(self.worker_hs)\n        self.client = ClientReplicationStreamProtocol(\n            self.worker_hs, \"client\", \"test\", clock, repl_handler,\n        )\n\n        self._client_transport = None\n        self._server_transport = None\n\n    def _get_worker_hs_config(self) -> dict:\n        config = self.default_config()\n        config[\"worker_app\"] = \"synapse.app.generic_worker\"\n        config[\"worker_replication_host\"] = \"testserv\"\n        config[\"worker_replication_http_port\"] = \"8765\"\n        return config\n\n    def _build_replication_data_handler(self):\n        return TestReplicationDataHandler(self.worker_hs)\n\n    def reconnect(self):\n        if self._client_transport:\n            self.client.close()\n\n        if self._server_transport:\n            self.server.close()\n\n        self._client_transport = FakeTransport(self.server, self.reactor)\n        self.client.makeConnection(self._client_transport)\n\n        self._server_transport = FakeTransport(self.client, self.reactor)\n        self.server.makeConnection(self._server_transport)\n\n    def disconnect(self):\n        if self._client_transport:\n            self._client_transport = None\n            self.client.close()\n\n        if self._server_transport:\n            self._server_transport = None\n            self.server.close()\n\n    def replicate(self):\n        \"\"\"Tell the master side of replication that something has happened, and then\n        wait for the replication to occur.\n        \"\"\"\n        self.streamer.on_notifier_poke()\n        self.pump(0.1)\n\n    def handle_http_replication_attempt(self) -> SynapseRequest:\n        \"\"\"Asserts that a connection attempt was made to the master HS on the\n        HTTP replication port, then proxies it to the master HS object to be\n        handled.\n\n        Returns:\n            The request object received by master HS.\n        \"\"\"\n\n        # We should have an outbound connection attempt.\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8765)\n\n        # Set up client side protocol\n        client_protocol = client_factory.buildProtocol(None)\n\n        request_factory = OneShotRequestFactory()\n\n        # Set up the server side protocol\n        channel = _PushHTTPChannel(self.reactor)\n        channel.requestFactory = request_factory\n        channel.site = self.site\n\n        # Connect client to server and vice versa.\n        client_to_server_transport = FakeTransport(\n            channel, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, channel\n        )\n        channel.makeConnection(server_to_client_transport)\n\n        # The request will now be processed by `self.site` and the response\n        # streamed back.\n        self.reactor.advance(0)\n\n        # We tear down the connection so it doesn't get reused without our\n        # knowledge.\n        server_to_client_transport.loseConnection()\n        client_to_server_transport.loseConnection()\n\n        return request_factory.request\n\n    def assert_request_is_get_repl_stream_updates(\n        self, request: SynapseRequest, stream_name: str\n    ):\n        \"\"\"Asserts that the given request is a HTTP replication request for\n        fetching updates for given stream.\n        \"\"\"\n\n        self.assertRegex(\n            request.path,\n            br\"^/_synapse/replication/get_repl_stream_updates/%s/[^/]+$\"\n            % (stream_name.encode(\"ascii\"),),\n        )\n\n        self.assertEqual(request.method, b\"GET\")", "target": 0}, {"function": "class BaseMultiWorkerStreamTestCase(unittest.HomeserverTestCase):\n    \"\"\"Base class for tests running multiple workers.\n\n    Automatically handle HTTP replication requests from workers to master,\n    unlike `BaseStreamTestCase`.\n    \"\"\"\n\n    servlets = []  # type: List[Callable[[HomeServer, JsonResource], None]]\n\n    def setUp(self):\n        super().setUp()\n\n        # build a replication server\n        self.server_factory = ReplicationStreamProtocolFactory(self.hs)\n        self.streamer = self.hs.get_replication_streamer()\n\n        # Fake in memory Redis server that servers can connect to.\n        self._redis_server = FakeRedisPubSubServer()\n\n        store = self.hs.get_datastore()\n        self.database_pool = store.db_pool\n\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.reactor.lookups[\"localhost\"] = \"127.0.0.1\"\n\n        # A map from a HS instance to the associated HTTP Site to use for\n        # handling inbound HTTP requests to that instance.\n        self._hs_to_site = {self.hs: self.site}\n\n        if self.hs.config.redis.redis_enabled:\n            # Handle attempts to connect to fake redis server.\n            self.reactor.add_tcp_client_callback(\n                \"localhost\", 6379, self.connect_any_redis_attempts,\n            )\n\n            self.hs.get_tcp_replication().start_replication(self.hs)\n\n        # When we see a connection attempt to the master replication listener we\n        # automatically set up the connection. This is so that tests don't\n        # manually have to go and explicitly set it up each time (plus sometimes\n        # it is impossible to write the handling explicitly in the tests).\n        #\n        # Register the master replication listener:\n        self.reactor.add_tcp_client_callback(\n            \"1.2.3.4\",\n            8765,\n            lambda: self._handle_http_replication_attempt(self.hs, 8765),\n        )\n\n    def create_test_resource(self):\n        \"\"\"Overrides `HomeserverTestCase.create_test_resource`.\n        \"\"\"\n        # We override this so that it automatically registers all the HTTP\n        # replication servlets, without having to explicitly do that in all\n        # subclassses.\n\n        resource = ReplicationRestResource(self.hs)\n\n        for servlet in self.servlets:\n            servlet(self.hs, resource)\n\n        return resource\n\n    def make_worker_hs(\n        self, worker_app: str, extra_config: dict = {}, **kwargs\n    ) -> HomeServer:\n        \"\"\"Make a new worker HS instance, correctly connecting replcation\n        stream to the master HS.\n\n        Args:\n            worker_app: Type of worker, e.g. `synapse.app.federation_sender`.\n            extra_config: Any extra config to use for this instances.\n            **kwargs: Options that get passed to `self.setup_test_homeserver`,\n                useful to e.g. pass some mocks for things like `http_client`\n\n        Returns:\n            The new worker HomeServer instance.\n        \"\"\"\n\n        config = self._get_worker_hs_config()\n        config[\"worker_app\"] = worker_app\n        config.update(extra_config)\n\n        worker_hs = self.setup_test_homeserver(\n            homeserver_to_use=GenericWorkerServer,\n            config=config,\n            reactor=self.reactor,\n            **kwargs,\n        )\n\n        # If the instance is in the `instance_map` config then workers may try\n        # and send HTTP requests to it, so we register it with\n        # `_handle_http_replication_attempt` like we do with the master HS.\n        instance_name = worker_hs.get_instance_name()\n        instance_loc = worker_hs.config.worker.instance_map.get(instance_name)\n        if instance_loc:\n            # Ensure the host is one that has a fake DNS entry.\n            if instance_loc.host not in self.reactor.lookups:\n                raise Exception(\n                    \"Host does not have an IP for instance_map[%r].host = %r\"\n                    % (instance_name, instance_loc.host,)\n                )\n\n            self.reactor.add_tcp_client_callback(\n                self.reactor.lookups[instance_loc.host],\n                instance_loc.port,\n                lambda: self._handle_http_replication_attempt(\n                    worker_hs, instance_loc.port\n                ),\n            )\n\n        store = worker_hs.get_datastore()\n        store.db_pool._db_pool = self.database_pool._db_pool\n\n        # Set up TCP replication between master and the new worker if we don't\n        # have Redis support enabled.\n        if not worker_hs.config.redis_enabled:\n            repl_handler = ReplicationCommandHandler(worker_hs)\n            client = ClientReplicationStreamProtocol(\n                worker_hs, \"client\", \"test\", self.clock, repl_handler,\n            )\n            server = self.server_factory.buildProtocol(None)\n\n            client_transport = FakeTransport(server, self.reactor)\n            client.makeConnection(client_transport)\n\n            server_transport = FakeTransport(client, self.reactor)\n            server.makeConnection(server_transport)\n\n        # Set up a resource for the worker\n        resource = ReplicationRestResource(worker_hs)\n\n        for servlet in self.servlets:\n            servlet(worker_hs, resource)\n\n        self._hs_to_site[worker_hs] = SynapseSite(\n            logger_name=\"synapse.access.http.fake\",\n            site_tag=\"{}-{}\".format(\n                worker_hs.config.server.server_name, worker_hs.get_instance_name()\n            ),\n            config=worker_hs.config.server.listeners[0],\n            resource=resource,\n            server_version_string=\"1\",\n        )\n\n        if worker_hs.config.redis.redis_enabled:\n            worker_hs.get_tcp_replication().start_replication(worker_hs)\n\n        return worker_hs\n\n    def _get_worker_hs_config(self) -> dict:\n        config = self.default_config()\n        config[\"worker_replication_host\"] = \"testserv\"\n        config[\"worker_replication_http_port\"] = \"8765\"\n        return config\n\n    def replicate(self):\n        \"\"\"Tell the master side of replication that something has happened, and then\n        wait for the replication to occur.\n        \"\"\"\n        self.streamer.on_notifier_poke()\n        self.pump()\n\n    def _handle_http_replication_attempt(self, hs, repl_port):\n        \"\"\"Handles a connection attempt to the given HS replication HTTP\n        listener on the given port.\n        \"\"\"\n\n        # We should have at least one outbound connection attempt, where the\n        # last is one to the HTTP repication IP/port.\n        clients = self.reactor.tcpClients\n        self.assertGreaterEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, repl_port)\n\n        # Set up client side protocol\n        client_protocol = client_factory.buildProtocol(None)\n\n        request_factory = OneShotRequestFactory()\n\n        # Set up the server side protocol\n        channel = _PushHTTPChannel(self.reactor)\n        channel.requestFactory = request_factory\n        channel.site = self._hs_to_site[hs]\n\n        # Connect client to server and vice versa.\n        client_to_server_transport = FakeTransport(\n            channel, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, channel\n        )\n        channel.makeConnection(server_to_client_transport)\n\n        # Note: at this point we've wired everything up, but we need to return\n        # before the data starts flowing over the connections as this is called\n        # inside `connecTCP` before the connection has been passed back to the\n        # code that requested the TCP connection.\n\n    def connect_any_redis_attempts(self):\n        \"\"\"If redis is enabled we need to deal with workers connecting to a\n        redis server. We don't want to use a real Redis server so we use a\n        fake one.\n        \"\"\"\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"localhost\")\n        self.assertEqual(port, 6379)\n\n        client_protocol = client_factory.buildProtocol(None)\n        server_protocol = self._redis_server.buildProtocol(None)\n\n        client_to_server_transport = FakeTransport(\n            server_protocol, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, server_protocol\n        )\n        server_protocol.makeConnection(server_to_client_transport)\n\n        return client_to_server_transport, server_to_client_transport", "target": 0}, {"function": "class TestReplicationDataHandler(GenericWorkerReplicationHandler):\n    \"\"\"Drop-in for ReplicationDataHandler which just collects RDATA rows\"\"\"\n\n    def __init__(self, hs: HomeServer):\n        super().__init__(hs)\n\n        # list of received (stream_name, token, row) tuples\n        self.received_rdata_rows = []  # type: List[Tuple[str, int, Any]]\n\n    async def on_rdata(self, stream_name, instance_name, token, rows):\n        await super().on_rdata(stream_name, instance_name, token, rows)\n        for r in rows:\n            self.received_rdata_rows.append((stream_name, token, r))", "target": 0}, {"function": "class _PushHTTPChannel(HTTPChannel):\n    \"\"\"A HTTPChannel that wraps pull producers to push producers.\n\n    This is a hack to get around the fact that HTTPChannel transparently wraps a\n    pull producer (which is what Synapse uses to reply to requests) with\n    `_PullToPush` to convert it to a push producer. Unfortunately `_PullToPush`\n    uses the standard reactor rather than letting us use our test reactor, which\n    makes it very hard to test.\n    \"\"\"\n\n    def __init__(self, reactor: IReactorTime):\n        super().__init__()\n        self.reactor = reactor\n\n        self._pull_to_push_producer = None  # type: Optional[_PullToPushProducer]\n\n    def registerProducer(self, producer, streaming):\n        # Convert pull producers to push producer.\n        if not streaming:\n            self._pull_to_push_producer = _PullToPushProducer(\n                self.reactor, producer, self\n            )\n            producer = self._pull_to_push_producer\n\n        super().registerProducer(producer, True)\n\n    def unregisterProducer(self):\n        if self._pull_to_push_producer:\n            # We need to manually stop the _PullToPushProducer.\n            self._pull_to_push_producer.stop()\n\n    def checkPersistence(self, request, version):\n        \"\"\"Check whether the connection can be re-used\n        \"\"\"\n        # We hijack this to always say no for ease of wiring stuff up in\n        # `handle_http_replication_attempt`.\n        request.responseHeaders.setRawHeaders(b\"connection\", [b\"close\"])\n        return False", "target": 0}, {"function": "class _PullToPushProducer:\n    \"\"\"A push producer that wraps a pull producer.\n    \"\"\"\n\n    def __init__(\n        self, reactor: IReactorTime, producer: IPullProducer, consumer: IConsumer\n    ):\n        self._clock = Clock(reactor)\n        self._producer = producer\n        self._consumer = consumer\n\n        # While running we use a looping call with a zero delay to call\n        # resumeProducing on given producer.\n        self._looping_call = None  # type: Optional[LoopingCall]\n\n        # We start writing next reactor tick.\n        self._start_loop()\n\n    def _start_loop(self):\n        \"\"\"Start the looping call to\n        \"\"\"\n\n        if not self._looping_call:\n            # Start a looping call which runs every tick.\n            self._looping_call = self._clock.looping_call(self._run_once, 0)\n\n    def stop(self):\n        \"\"\"Stops calling resumeProducing.\n        \"\"\"\n        if self._looping_call:\n            self._looping_call.stop()\n            self._looping_call = None\n\n    def pauseProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self.stop()\n\n    def resumeProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self._start_loop()\n\n    def stopProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self.stop()\n        self._producer.stopProducing()\n\n    def _run_once(self):\n        \"\"\"Calls resumeProducing on producer once.\n        \"\"\"\n\n        try:\n            self._producer.resumeProducing()\n        except Exception:\n            logger.exception(\"Failed to call resumeProducing\")\n            try:\n                self._consumer.unregisterProducer()\n            except Exception:\n                pass\n\n            self.stopProducing()", "target": 0}, {"function": "class FakeRedisPubSubServer:\n    \"\"\"A fake Redis server for pub/sub.\n    \"\"\"\n\n    def __init__(self):\n        self._subscribers = set()\n\n    def add_subscriber(self, conn):\n        \"\"\"A connection has called SUBSCRIBE\n        \"\"\"\n        self._subscribers.add(conn)\n\n    def remove_subscriber(self, conn):\n        \"\"\"A connection has called UNSUBSCRIBE\n        \"\"\"\n        self._subscribers.discard(conn)\n\n    def publish(self, conn, channel, msg) -> int:\n        \"\"\"A connection want to publish a message to subscribers.\n        \"\"\"\n        for sub in self._subscribers:\n            sub.send([\"message\", channel, msg])\n\n        return len(self._subscribers)\n\n    def buildProtocol(self, addr):\n        return FakeRedisPubSubProtocol(self)", "target": 0}, {"function": "class FakeRedisPubSubProtocol(Protocol):\n    \"\"\"A connection from a client talking to the fake Redis server.\n    \"\"\"\n\n    def __init__(self, server: FakeRedisPubSubServer):\n        self._server = server\n        self._reader = hiredis.Reader()\n\n    def dataReceived(self, data):\n        self._reader.feed(data)\n\n        # We might get multiple messages in one packet.\n        while True:\n            msg = self._reader.gets()\n\n            if msg is False:\n                # No more messages.\n                return\n\n            if not isinstance(msg, list):\n                # Inbound commands should always be a list\n                raise Exception(\"Expected redis list\")\n\n            self.handle_command(msg[0], *msg[1:])\n\n    def handle_command(self, command, *args):\n        \"\"\"Received a Redis command from the client.\n        \"\"\"\n\n        # We currently only support pub/sub.\n        if command == b\"PUBLISH\":\n            channel, message = args\n            num_subscribers = self._server.publish(self, channel, message)\n            self.send(num_subscribers)\n        elif command == b\"SUBSCRIBE\":\n            (channel,) = args\n            self._server.add_subscriber(self)\n            self.send([\"subscribe\", channel, 1])\n        else:\n            raise Exception(\"Unknown command\")\n\n    def send(self, msg):\n        \"\"\"Send a message back to the client.\n        \"\"\"\n        raw = self.encode(msg).encode(\"utf-8\")\n\n        self.transport.write(raw)\n        self.transport.flush()\n\n    def encode(self, obj):\n        \"\"\"Encode an object to its Redis format.\n\n        Supports: strings/bytes, integers and list/tuples.\n        \"\"\"\n\n        if isinstance(obj, bytes):\n            # We assume bytes are just unicode strings.\n            obj = obj.decode(\"utf-8\")\n\n        if isinstance(obj, str):\n            return \"${len}\\r\\n{str}\\r\\n\".format(len=len(obj), str=obj)\n        if isinstance(obj, int):\n            return \":{val}\\r\\n\".format(val=obj)\n        if isinstance(obj, (list, tuple)):\n            items = \"\".join(self.encode(a) for a in obj)\n            return \"*{len}\\r\\n{items}\".format(len=len(obj), items=items)\n\n        raise Exception(\"Unrecognized type for encoding redis: %r: %r\", type(obj), obj)\n\n    def connectionLost(self, reason):\n        self._server.remove_subscriber(self)", "target": 0}], "function_after": [{"function": "class BaseStreamTestCase(unittest.HomeserverTestCase):\n    \"\"\"Base class for tests of the replication streams\"\"\"\n\n    # hiredis is an optional dependency so we don't want to require it for running\n    # the tests.\n    if not hiredis:\n        skip = \"Requires hiredis\"\n\n    servlets = [\n        streams.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        # build a replication server\n        server_factory = ReplicationStreamProtocolFactory(hs)\n        self.streamer = hs.get_replication_streamer()\n        self.server = server_factory.buildProtocol(None)\n\n        # Make a new HomeServer object for the worker\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.worker_hs = self.setup_test_homeserver(\n            federation_http_client=None,\n            homeserver_to_use=GenericWorkerServer,\n            config=self._get_worker_hs_config(),\n            reactor=self.reactor,\n        )\n\n        # Since we use sqlite in memory databases we need to make sure the\n        # databases objects are the same.\n        self.worker_hs.get_datastore().db_pool = hs.get_datastore().db_pool\n\n        self.test_handler = self._build_replication_data_handler()\n        self.worker_hs._replication_data_handler = self.test_handler\n\n        repl_handler = ReplicationCommandHandler(self.worker_hs)\n        self.client = ClientReplicationStreamProtocol(\n            self.worker_hs, \"client\", \"test\", clock, repl_handler,\n        )\n\n        self._client_transport = None\n        self._server_transport = None\n\n    def _get_worker_hs_config(self) -> dict:\n        config = self.default_config()\n        config[\"worker_app\"] = \"synapse.app.generic_worker\"\n        config[\"worker_replication_host\"] = \"testserv\"\n        config[\"worker_replication_http_port\"] = \"8765\"\n        return config\n\n    def _build_replication_data_handler(self):\n        return TestReplicationDataHandler(self.worker_hs)\n\n    def reconnect(self):\n        if self._client_transport:\n            self.client.close()\n\n        if self._server_transport:\n            self.server.close()\n\n        self._client_transport = FakeTransport(self.server, self.reactor)\n        self.client.makeConnection(self._client_transport)\n\n        self._server_transport = FakeTransport(self.client, self.reactor)\n        self.server.makeConnection(self._server_transport)\n\n    def disconnect(self):\n        if self._client_transport:\n            self._client_transport = None\n            self.client.close()\n\n        if self._server_transport:\n            self._server_transport = None\n            self.server.close()\n\n    def replicate(self):\n        \"\"\"Tell the master side of replication that something has happened, and then\n        wait for the replication to occur.\n        \"\"\"\n        self.streamer.on_notifier_poke()\n        self.pump(0.1)\n\n    def handle_http_replication_attempt(self) -> SynapseRequest:\n        \"\"\"Asserts that a connection attempt was made to the master HS on the\n        HTTP replication port, then proxies it to the master HS object to be\n        handled.\n\n        Returns:\n            The request object received by master HS.\n        \"\"\"\n\n        # We should have an outbound connection attempt.\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, 8765)\n\n        # Set up client side protocol\n        client_protocol = client_factory.buildProtocol(None)\n\n        request_factory = OneShotRequestFactory()\n\n        # Set up the server side protocol\n        channel = _PushHTTPChannel(self.reactor)\n        channel.requestFactory = request_factory\n        channel.site = self.site\n\n        # Connect client to server and vice versa.\n        client_to_server_transport = FakeTransport(\n            channel, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, channel\n        )\n        channel.makeConnection(server_to_client_transport)\n\n        # The request will now be processed by `self.site` and the response\n        # streamed back.\n        self.reactor.advance(0)\n\n        # We tear down the connection so it doesn't get reused without our\n        # knowledge.\n        server_to_client_transport.loseConnection()\n        client_to_server_transport.loseConnection()\n\n        return request_factory.request\n\n    def assert_request_is_get_repl_stream_updates(\n        self, request: SynapseRequest, stream_name: str\n    ):\n        \"\"\"Asserts that the given request is a HTTP replication request for\n        fetching updates for given stream.\n        \"\"\"\n\n        self.assertRegex(\n            request.path,\n            br\"^/_synapse/replication/get_repl_stream_updates/%s/[^/]+$\"\n            % (stream_name.encode(\"ascii\"),),\n        )\n\n        self.assertEqual(request.method, b\"GET\")", "target": 0}, {"function": "class BaseMultiWorkerStreamTestCase(unittest.HomeserverTestCase):\n    \"\"\"Base class for tests running multiple workers.\n\n    Automatically handle HTTP replication requests from workers to master,\n    unlike `BaseStreamTestCase`.\n    \"\"\"\n\n    servlets = []  # type: List[Callable[[HomeServer, JsonResource], None]]\n\n    def setUp(self):\n        super().setUp()\n\n        # build a replication server\n        self.server_factory = ReplicationStreamProtocolFactory(self.hs)\n        self.streamer = self.hs.get_replication_streamer()\n\n        # Fake in memory Redis server that servers can connect to.\n        self._redis_server = FakeRedisPubSubServer()\n\n        store = self.hs.get_datastore()\n        self.database_pool = store.db_pool\n\n        self.reactor.lookups[\"testserv\"] = \"1.2.3.4\"\n        self.reactor.lookups[\"localhost\"] = \"127.0.0.1\"\n\n        # A map from a HS instance to the associated HTTP Site to use for\n        # handling inbound HTTP requests to that instance.\n        self._hs_to_site = {self.hs: self.site}\n\n        if self.hs.config.redis.redis_enabled:\n            # Handle attempts to connect to fake redis server.\n            self.reactor.add_tcp_client_callback(\n                \"localhost\", 6379, self.connect_any_redis_attempts,\n            )\n\n            self.hs.get_tcp_replication().start_replication(self.hs)\n\n        # When we see a connection attempt to the master replication listener we\n        # automatically set up the connection. This is so that tests don't\n        # manually have to go and explicitly set it up each time (plus sometimes\n        # it is impossible to write the handling explicitly in the tests).\n        #\n        # Register the master replication listener:\n        self.reactor.add_tcp_client_callback(\n            \"1.2.3.4\",\n            8765,\n            lambda: self._handle_http_replication_attempt(self.hs, 8765),\n        )\n\n    def create_test_resource(self):\n        \"\"\"Overrides `HomeserverTestCase.create_test_resource`.\n        \"\"\"\n        # We override this so that it automatically registers all the HTTP\n        # replication servlets, without having to explicitly do that in all\n        # subclassses.\n\n        resource = ReplicationRestResource(self.hs)\n\n        for servlet in self.servlets:\n            servlet(self.hs, resource)\n\n        return resource\n\n    def make_worker_hs(\n        self, worker_app: str, extra_config: dict = {}, **kwargs\n    ) -> HomeServer:\n        \"\"\"Make a new worker HS instance, correctly connecting replcation\n        stream to the master HS.\n\n        Args:\n            worker_app: Type of worker, e.g. `synapse.app.federation_sender`.\n            extra_config: Any extra config to use for this instances.\n            **kwargs: Options that get passed to `self.setup_test_homeserver`,\n                useful to e.g. pass some mocks for things like `federation_http_client`\n\n        Returns:\n            The new worker HomeServer instance.\n        \"\"\"\n\n        config = self._get_worker_hs_config()\n        config[\"worker_app\"] = worker_app\n        config.update(extra_config)\n\n        worker_hs = self.setup_test_homeserver(\n            homeserver_to_use=GenericWorkerServer,\n            config=config,\n            reactor=self.reactor,\n            **kwargs,\n        )\n\n        # If the instance is in the `instance_map` config then workers may try\n        # and send HTTP requests to it, so we register it with\n        # `_handle_http_replication_attempt` like we do with the master HS.\n        instance_name = worker_hs.get_instance_name()\n        instance_loc = worker_hs.config.worker.instance_map.get(instance_name)\n        if instance_loc:\n            # Ensure the host is one that has a fake DNS entry.\n            if instance_loc.host not in self.reactor.lookups:\n                raise Exception(\n                    \"Host does not have an IP for instance_map[%r].host = %r\"\n                    % (instance_name, instance_loc.host,)\n                )\n\n            self.reactor.add_tcp_client_callback(\n                self.reactor.lookups[instance_loc.host],\n                instance_loc.port,\n                lambda: self._handle_http_replication_attempt(\n                    worker_hs, instance_loc.port\n                ),\n            )\n\n        store = worker_hs.get_datastore()\n        store.db_pool._db_pool = self.database_pool._db_pool\n\n        # Set up TCP replication between master and the new worker if we don't\n        # have Redis support enabled.\n        if not worker_hs.config.redis_enabled:\n            repl_handler = ReplicationCommandHandler(worker_hs)\n            client = ClientReplicationStreamProtocol(\n                worker_hs, \"client\", \"test\", self.clock, repl_handler,\n            )\n            server = self.server_factory.buildProtocol(None)\n\n            client_transport = FakeTransport(server, self.reactor)\n            client.makeConnection(client_transport)\n\n            server_transport = FakeTransport(client, self.reactor)\n            server.makeConnection(server_transport)\n\n        # Set up a resource for the worker\n        resource = ReplicationRestResource(worker_hs)\n\n        for servlet in self.servlets:\n            servlet(worker_hs, resource)\n\n        self._hs_to_site[worker_hs] = SynapseSite(\n            logger_name=\"synapse.access.http.fake\",\n            site_tag=\"{}-{}\".format(\n                worker_hs.config.server.server_name, worker_hs.get_instance_name()\n            ),\n            config=worker_hs.config.server.listeners[0],\n            resource=resource,\n            server_version_string=\"1\",\n        )\n\n        if worker_hs.config.redis.redis_enabled:\n            worker_hs.get_tcp_replication().start_replication(worker_hs)\n\n        return worker_hs\n\n    def _get_worker_hs_config(self) -> dict:\n        config = self.default_config()\n        config[\"worker_replication_host\"] = \"testserv\"\n        config[\"worker_replication_http_port\"] = \"8765\"\n        return config\n\n    def replicate(self):\n        \"\"\"Tell the master side of replication that something has happened, and then\n        wait for the replication to occur.\n        \"\"\"\n        self.streamer.on_notifier_poke()\n        self.pump()\n\n    def _handle_http_replication_attempt(self, hs, repl_port):\n        \"\"\"Handles a connection attempt to the given HS replication HTTP\n        listener on the given port.\n        \"\"\"\n\n        # We should have at least one outbound connection attempt, where the\n        # last is one to the HTTP repication IP/port.\n        clients = self.reactor.tcpClients\n        self.assertGreaterEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop()\n        self.assertEqual(host, \"1.2.3.4\")\n        self.assertEqual(port, repl_port)\n\n        # Set up client side protocol\n        client_protocol = client_factory.buildProtocol(None)\n\n        request_factory = OneShotRequestFactory()\n\n        # Set up the server side protocol\n        channel = _PushHTTPChannel(self.reactor)\n        channel.requestFactory = request_factory\n        channel.site = self._hs_to_site[hs]\n\n        # Connect client to server and vice versa.\n        client_to_server_transport = FakeTransport(\n            channel, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, channel\n        )\n        channel.makeConnection(server_to_client_transport)\n\n        # Note: at this point we've wired everything up, but we need to return\n        # before the data starts flowing over the connections as this is called\n        # inside `connecTCP` before the connection has been passed back to the\n        # code that requested the TCP connection.\n\n    def connect_any_redis_attempts(self):\n        \"\"\"If redis is enabled we need to deal with workers connecting to a\n        redis server. We don't want to use a real Redis server so we use a\n        fake one.\n        \"\"\"\n        clients = self.reactor.tcpClients\n        self.assertEqual(len(clients), 1)\n        (host, port, client_factory, _timeout, _bindAddress) = clients.pop(0)\n        self.assertEqual(host, \"localhost\")\n        self.assertEqual(port, 6379)\n\n        client_protocol = client_factory.buildProtocol(None)\n        server_protocol = self._redis_server.buildProtocol(None)\n\n        client_to_server_transport = FakeTransport(\n            server_protocol, self.reactor, client_protocol\n        )\n        client_protocol.makeConnection(client_to_server_transport)\n\n        server_to_client_transport = FakeTransport(\n            client_protocol, self.reactor, server_protocol\n        )\n        server_protocol.makeConnection(server_to_client_transport)\n\n        return client_to_server_transport, server_to_client_transport", "target": 0}, {"function": "class TestReplicationDataHandler(GenericWorkerReplicationHandler):\n    \"\"\"Drop-in for ReplicationDataHandler which just collects RDATA rows\"\"\"\n\n    def __init__(self, hs: HomeServer):\n        super().__init__(hs)\n\n        # list of received (stream_name, token, row) tuples\n        self.received_rdata_rows = []  # type: List[Tuple[str, int, Any]]\n\n    async def on_rdata(self, stream_name, instance_name, token, rows):\n        await super().on_rdata(stream_name, instance_name, token, rows)\n        for r in rows:\n            self.received_rdata_rows.append((stream_name, token, r))", "target": 0}, {"function": "class _PushHTTPChannel(HTTPChannel):\n    \"\"\"A HTTPChannel that wraps pull producers to push producers.\n\n    This is a hack to get around the fact that HTTPChannel transparently wraps a\n    pull producer (which is what Synapse uses to reply to requests) with\n    `_PullToPush` to convert it to a push producer. Unfortunately `_PullToPush`\n    uses the standard reactor rather than letting us use our test reactor, which\n    makes it very hard to test.\n    \"\"\"\n\n    def __init__(self, reactor: IReactorTime):\n        super().__init__()\n        self.reactor = reactor\n\n        self._pull_to_push_producer = None  # type: Optional[_PullToPushProducer]\n\n    def registerProducer(self, producer, streaming):\n        # Convert pull producers to push producer.\n        if not streaming:\n            self._pull_to_push_producer = _PullToPushProducer(\n                self.reactor, producer, self\n            )\n            producer = self._pull_to_push_producer\n\n        super().registerProducer(producer, True)\n\n    def unregisterProducer(self):\n        if self._pull_to_push_producer:\n            # We need to manually stop the _PullToPushProducer.\n            self._pull_to_push_producer.stop()\n\n    def checkPersistence(self, request, version):\n        \"\"\"Check whether the connection can be re-used\n        \"\"\"\n        # We hijack this to always say no for ease of wiring stuff up in\n        # `handle_http_replication_attempt`.\n        request.responseHeaders.setRawHeaders(b\"connection\", [b\"close\"])\n        return False", "target": 0}, {"function": "class _PullToPushProducer:\n    \"\"\"A push producer that wraps a pull producer.\n    \"\"\"\n\n    def __init__(\n        self, reactor: IReactorTime, producer: IPullProducer, consumer: IConsumer\n    ):\n        self._clock = Clock(reactor)\n        self._producer = producer\n        self._consumer = consumer\n\n        # While running we use a looping call with a zero delay to call\n        # resumeProducing on given producer.\n        self._looping_call = None  # type: Optional[LoopingCall]\n\n        # We start writing next reactor tick.\n        self._start_loop()\n\n    def _start_loop(self):\n        \"\"\"Start the looping call to\n        \"\"\"\n\n        if not self._looping_call:\n            # Start a looping call which runs every tick.\n            self._looping_call = self._clock.looping_call(self._run_once, 0)\n\n    def stop(self):\n        \"\"\"Stops calling resumeProducing.\n        \"\"\"\n        if self._looping_call:\n            self._looping_call.stop()\n            self._looping_call = None\n\n    def pauseProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self.stop()\n\n    def resumeProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self._start_loop()\n\n    def stopProducing(self):\n        \"\"\"Implements IPushProducer\n        \"\"\"\n        self.stop()\n        self._producer.stopProducing()\n\n    def _run_once(self):\n        \"\"\"Calls resumeProducing on producer once.\n        \"\"\"\n\n        try:\n            self._producer.resumeProducing()\n        except Exception:\n            logger.exception(\"Failed to call resumeProducing\")\n            try:\n                self._consumer.unregisterProducer()\n            except Exception:\n                pass\n\n            self.stopProducing()", "target": 0}, {"function": "class FakeRedisPubSubServer:\n    \"\"\"A fake Redis server for pub/sub.\n    \"\"\"\n\n    def __init__(self):\n        self._subscribers = set()\n\n    def add_subscriber(self, conn):\n        \"\"\"A connection has called SUBSCRIBE\n        \"\"\"\n        self._subscribers.add(conn)\n\n    def remove_subscriber(self, conn):\n        \"\"\"A connection has called UNSUBSCRIBE\n        \"\"\"\n        self._subscribers.discard(conn)\n\n    def publish(self, conn, channel, msg) -> int:\n        \"\"\"A connection want to publish a message to subscribers.\n        \"\"\"\n        for sub in self._subscribers:\n            sub.send([\"message\", channel, msg])\n\n        return len(self._subscribers)\n\n    def buildProtocol(self, addr):\n        return FakeRedisPubSubProtocol(self)", "target": 0}, {"function": "class FakeRedisPubSubProtocol(Protocol):\n    \"\"\"A connection from a client talking to the fake Redis server.\n    \"\"\"\n\n    def __init__(self, server: FakeRedisPubSubServer):\n        self._server = server\n        self._reader = hiredis.Reader()\n\n    def dataReceived(self, data):\n        self._reader.feed(data)\n\n        # We might get multiple messages in one packet.\n        while True:\n            msg = self._reader.gets()\n\n            if msg is False:\n                # No more messages.\n                return\n\n            if not isinstance(msg, list):\n                # Inbound commands should always be a list\n                raise Exception(\"Expected redis list\")\n\n            self.handle_command(msg[0], *msg[1:])\n\n    def handle_command(self, command, *args):\n        \"\"\"Received a Redis command from the client.\n        \"\"\"\n\n        # We currently only support pub/sub.\n        if command == b\"PUBLISH\":\n            channel, message = args\n            num_subscribers = self._server.publish(self, channel, message)\n            self.send(num_subscribers)\n        elif command == b\"SUBSCRIBE\":\n            (channel,) = args\n            self._server.add_subscriber(self)\n            self.send([\"subscribe\", channel, 1])\n        else:\n            raise Exception(\"Unknown command\")\n\n    def send(self, msg):\n        \"\"\"Send a message back to the client.\n        \"\"\"\n        raw = self.encode(msg).encode(\"utf-8\")\n\n        self.transport.write(raw)\n        self.transport.flush()\n\n    def encode(self, obj):\n        \"\"\"Encode an object to its Redis format.\n\n        Supports: strings/bytes, integers and list/tuples.\n        \"\"\"\n\n        if isinstance(obj, bytes):\n            # We assume bytes are just unicode strings.\n            obj = obj.decode(\"utf-8\")\n\n        if isinstance(obj, str):\n            return \"${len}\\r\\n{str}\\r\\n\".format(len=len(obj), str=obj)\n        if isinstance(obj, int):\n            return \":{val}\\r\\n\".format(val=obj)\n        if isinstance(obj, (list, tuple)):\n            items = \"\".join(self.encode(a) for a in obj)\n            return \"*{len}\\r\\n{items}\".format(len=len(obj), items=items)\n\n        raise Exception(\"Unrecognized type for encoding redis: %r: %r\", type(obj), obj)\n\n    def connectionLost(self, reason):\n        self._server.remove_subscriber(self)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Freplication%2Ftest_federation_sender_shard.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\n\nfrom mock import Mock\n\nfrom synapse.api.constants import EventTypes, Membership\nfrom synapse.events.builder import EventBuilderFactory\nfrom synapse.rest.admin import register_servlets_for_client_rest_resource\nfrom synapse.rest.client.v1 import login, room\nfrom synapse.types import UserID, create_requester\n\nfrom tests.replication._base import BaseMultiWorkerStreamTestCase\nfrom tests.test_utils import make_awaitable\n\nlogger = logging.getLogger(__name__)\n\n\nclass FederationSenderTestCase(BaseMultiWorkerStreamTestCase):\n    servlets = [\n        login.register_servlets,\n        register_servlets_for_client_rest_resource,\n        room.register_servlets,\n    ]\n\n    def default_config(self):\n        conf = super().default_config()\n        conf[\"send_federation\"] = False\n        return conf\n\n    def test_send_event_single_sender(self):\n        \"\"\"Test that using a single federation sender worker correctly sends a\n        new event.\n        \"\"\"\n        mock_client = Mock(spec=[\"put_json\"])\n        mock_client.put_json.return_value = make_awaitable({})\n\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\"send_federation\": True},\n            federation_http_client=mock_client,\n        )\n\n        user = self.register_user(\"user\", \"pass\")\n        token = self.login(\"user\", \"pass\")\n\n        room = self.create_room_with_remote_server(user, token)\n\n        mock_client.put_json.reset_mock()\n\n        self.create_and_send_event(room, UserID.from_string(user))\n        self.replicate()\n\n        # Assert that the event was sent out over federation.\n        mock_client.put_json.assert_called()\n        self.assertEqual(mock_client.put_json.call_args[0][0], \"other_server\")\n        self.assertTrue(mock_client.put_json.call_args[1][\"data\"].get(\"pdus\"))\n\n    def test_send_event_sharded(self):\n        \"\"\"Test that using two federation sender workers correctly sends\n        new events.\n        \"\"\"\n        mock_client1 = Mock(spec=[\"put_json\"])\n        mock_client1.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender1\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            federation_http_client=mock_client1,\n        )\n\n        mock_client2 = Mock(spec=[\"put_json\"])\n        mock_client2.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender2\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            federation_http_client=mock_client2,\n        )\n\n        user = self.register_user(\"user2\", \"pass\")\n        token = self.login(\"user2\", \"pass\")\n\n        sent_on_1 = False\n        sent_on_2 = False\n        for i in range(20):\n            server_name = \"other_server_%d\" % (i,)\n            room = self.create_room_with_remote_server(user, token, server_name)\n            mock_client1.reset_mock()  # type: ignore[attr-defined]\n            mock_client2.reset_mock()  # type: ignore[attr-defined]\n\n            self.create_and_send_event(room, UserID.from_string(user))\n            self.replicate()\n\n            if mock_client1.put_json.called:\n                sent_on_1 = True\n                mock_client2.put_json.assert_not_called()\n                self.assertEqual(mock_client1.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client1.put_json.call_args[1][\"data\"].get(\"pdus\"))\n            elif mock_client2.put_json.called:\n                sent_on_2 = True\n                mock_client1.put_json.assert_not_called()\n                self.assertEqual(mock_client2.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client2.put_json.call_args[1][\"data\"].get(\"pdus\"))\n            else:\n                raise AssertionError(\n                    \"Expected send transaction from one or the other sender\"\n                )\n\n            if sent_on_1 and sent_on_2:\n                break\n\n        self.assertTrue(sent_on_1)\n        self.assertTrue(sent_on_2)\n\n    def test_send_typing_sharded(self):\n        \"\"\"Test that using two federation sender workers correctly sends\n        new typing EDUs.\n        \"\"\"\n        mock_client1 = Mock(spec=[\"put_json\"])\n        mock_client1.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender1\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            federation_http_client=mock_client1,\n        )\n\n        mock_client2 = Mock(spec=[\"put_json\"])\n        mock_client2.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender2\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            federation_http_client=mock_client2,\n        )\n\n        user = self.register_user(\"user3\", \"pass\")\n        token = self.login(\"user3\", \"pass\")\n\n        typing_handler = self.hs.get_typing_handler()\n\n        sent_on_1 = False\n        sent_on_2 = False\n        for i in range(20):\n            server_name = \"other_server_%d\" % (i,)\n            room = self.create_room_with_remote_server(user, token, server_name)\n            mock_client1.reset_mock()  # type: ignore[attr-defined]\n            mock_client2.reset_mock()  # type: ignore[attr-defined]\n\n            self.get_success(\n                typing_handler.started_typing(\n                    target_user=UserID.from_string(user),\n                    requester=create_requester(user),\n                    room_id=room,\n                    timeout=20000,\n                )\n            )\n\n            self.replicate()\n\n            if mock_client1.put_json.called:\n                sent_on_1 = True\n                mock_client2.put_json.assert_not_called()\n                self.assertEqual(mock_client1.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client1.put_json.call_args[1][\"data\"].get(\"edus\"))\n            elif mock_client2.put_json.called:\n                sent_on_2 = True\n                mock_client1.put_json.assert_not_called()\n                self.assertEqual(mock_client2.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client2.put_json.call_args[1][\"data\"].get(\"edus\"))\n            else:\n                raise AssertionError(\n                    \"Expected send transaction from one or the other sender\"\n                )\n\n            if sent_on_1 and sent_on_2:\n                break\n\n        self.assertTrue(sent_on_1)\n        self.assertTrue(sent_on_2)\n\n    def create_room_with_remote_server(self, user, token, remote_server=\"other_server\"):\n        room = self.helper.create_room_as(user, tok=token)\n        store = self.hs.get_datastore()\n        federation = self.hs.get_federation_handler()\n\n        prev_event_ids = self.get_success(store.get_latest_event_ids_in_room(room))\n        room_version = self.get_success(store.get_room_version(room))\n\n        factory = EventBuilderFactory(self.hs)\n        factory.hostname = remote_server\n\n        user_id = UserID(\"user\", remote_server).to_string()\n\n        event_dict = {\n            \"type\": EventTypes.Member,\n            \"state_key\": user_id,\n            \"content\": {\"membership\": Membership.JOIN},\n            \"sender\": user_id,\n            \"room_id\": room,\n        }\n\n        builder = factory.for_room_version(room_version, event_dict)\n        join_event = self.get_success(builder.build(prev_event_ids, None))\n\n        self.get_success(federation.on_send_join_request(remote_server, join_event))\n        self.replicate()\n\n        return room\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\n\nfrom mock import Mock\n\nfrom synapse.api.constants import EventTypes, Membership\nfrom synapse.events.builder import EventBuilderFactory\nfrom synapse.rest.admin import register_servlets_for_client_rest_resource\nfrom synapse.rest.client.v1 import login, room\nfrom synapse.types import UserID, create_requester\n\nfrom tests.replication._base import BaseMultiWorkerStreamTestCase\nfrom tests.test_utils import make_awaitable\n\nlogger = logging.getLogger(__name__)\n\n\nclass FederationSenderTestCase(BaseMultiWorkerStreamTestCase):\n    servlets = [\n        login.register_servlets,\n        register_servlets_for_client_rest_resource,\n        room.register_servlets,\n    ]\n\n    def default_config(self):\n        conf = super().default_config()\n        conf[\"send_federation\"] = False\n        return conf\n\n    def test_send_event_single_sender(self):\n        \"\"\"Test that using a single federation sender worker correctly sends a\n        new event.\n        \"\"\"\n        mock_client = Mock(spec=[\"put_json\"])\n        mock_client.put_json.return_value = make_awaitable({})\n\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\"send_federation\": True},\n            http_client=mock_client,\n        )\n\n        user = self.register_user(\"user\", \"pass\")\n        token = self.login(\"user\", \"pass\")\n\n        room = self.create_room_with_remote_server(user, token)\n\n        mock_client.put_json.reset_mock()\n\n        self.create_and_send_event(room, UserID.from_string(user))\n        self.replicate()\n\n        # Assert that the event was sent out over federation.\n        mock_client.put_json.assert_called()\n        self.assertEqual(mock_client.put_json.call_args[0][0], \"other_server\")\n        self.assertTrue(mock_client.put_json.call_args[1][\"data\"].get(\"pdus\"))\n\n    def test_send_event_sharded(self):\n        \"\"\"Test that using two federation sender workers correctly sends\n        new events.\n        \"\"\"\n        mock_client1 = Mock(spec=[\"put_json\"])\n        mock_client1.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender1\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            http_client=mock_client1,\n        )\n\n        mock_client2 = Mock(spec=[\"put_json\"])\n        mock_client2.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender2\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            http_client=mock_client2,\n        )\n\n        user = self.register_user(\"user2\", \"pass\")\n        token = self.login(\"user2\", \"pass\")\n\n        sent_on_1 = False\n        sent_on_2 = False\n        for i in range(20):\n            server_name = \"other_server_%d\" % (i,)\n            room = self.create_room_with_remote_server(user, token, server_name)\n            mock_client1.reset_mock()  # type: ignore[attr-defined]\n            mock_client2.reset_mock()  # type: ignore[attr-defined]\n\n            self.create_and_send_event(room, UserID.from_string(user))\n            self.replicate()\n\n            if mock_client1.put_json.called:\n                sent_on_1 = True\n                mock_client2.put_json.assert_not_called()\n                self.assertEqual(mock_client1.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client1.put_json.call_args[1][\"data\"].get(\"pdus\"))\n            elif mock_client2.put_json.called:\n                sent_on_2 = True\n                mock_client1.put_json.assert_not_called()\n                self.assertEqual(mock_client2.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client2.put_json.call_args[1][\"data\"].get(\"pdus\"))\n            else:\n                raise AssertionError(\n                    \"Expected send transaction from one or the other sender\"\n                )\n\n            if sent_on_1 and sent_on_2:\n                break\n\n        self.assertTrue(sent_on_1)\n        self.assertTrue(sent_on_2)\n\n    def test_send_typing_sharded(self):\n        \"\"\"Test that using two federation sender workers correctly sends\n        new typing EDUs.\n        \"\"\"\n        mock_client1 = Mock(spec=[\"put_json\"])\n        mock_client1.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender1\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            http_client=mock_client1,\n        )\n\n        mock_client2 = Mock(spec=[\"put_json\"])\n        mock_client2.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender2\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            http_client=mock_client2,\n        )\n\n        user = self.register_user(\"user3\", \"pass\")\n        token = self.login(\"user3\", \"pass\")\n\n        typing_handler = self.hs.get_typing_handler()\n\n        sent_on_1 = False\n        sent_on_2 = False\n        for i in range(20):\n            server_name = \"other_server_%d\" % (i,)\n            room = self.create_room_with_remote_server(user, token, server_name)\n            mock_client1.reset_mock()  # type: ignore[attr-defined]\n            mock_client2.reset_mock()  # type: ignore[attr-defined]\n\n            self.get_success(\n                typing_handler.started_typing(\n                    target_user=UserID.from_string(user),\n                    requester=create_requester(user),\n                    room_id=room,\n                    timeout=20000,\n                )\n            )\n\n            self.replicate()\n\n            if mock_client1.put_json.called:\n                sent_on_1 = True\n                mock_client2.put_json.assert_not_called()\n                self.assertEqual(mock_client1.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client1.put_json.call_args[1][\"data\"].get(\"edus\"))\n            elif mock_client2.put_json.called:\n                sent_on_2 = True\n                mock_client1.put_json.assert_not_called()\n                self.assertEqual(mock_client2.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client2.put_json.call_args[1][\"data\"].get(\"edus\"))\n            else:\n                raise AssertionError(\n                    \"Expected send transaction from one or the other sender\"\n                )\n\n            if sent_on_1 and sent_on_2:\n                break\n\n        self.assertTrue(sent_on_1)\n        self.assertTrue(sent_on_2)\n\n    def create_room_with_remote_server(self, user, token, remote_server=\"other_server\"):\n        room = self.helper.create_room_as(user, tok=token)\n        store = self.hs.get_datastore()\n        federation = self.hs.get_federation_handler()\n\n        prev_event_ids = self.get_success(store.get_latest_event_ids_in_room(room))\n        room_version = self.get_success(store.get_room_version(room))\n\n        factory = EventBuilderFactory(self.hs)\n        factory.hostname = remote_server\n\n        user_id = UserID(\"user\", remote_server).to_string()\n\n        event_dict = {\n            \"type\": EventTypes.Member,\n            \"state_key\": user_id,\n            \"content\": {\"membership\": Membership.JOIN},\n            \"sender\": user_id,\n            \"room_id\": room,\n        }\n\n        builder = factory.for_room_version(room_version, event_dict)\n        join_event = self.get_success(builder.build(prev_event_ids, None))\n\n        self.get_success(federation.on_send_join_request(remote_server, join_event))\n        self.replicate()\n\n        return room\n", "patch": "@@ -50,7 +50,7 @@ def test_send_event_single_sender(self):\n         self.make_worker_hs(\n             \"synapse.app.federation_sender\",\n             {\"send_federation\": True},\n-            http_client=mock_client,\n+            federation_http_client=mock_client,\n         )\n \n         user = self.register_user(\"user\", \"pass\")\n@@ -81,7 +81,7 @@ def test_send_event_sharded(self):\n                 \"worker_name\": \"sender1\",\n                 \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n             },\n-            http_client=mock_client1,\n+            federation_http_client=mock_client1,\n         )\n \n         mock_client2 = Mock(spec=[\"put_json\"])\n@@ -93,7 +93,7 @@ def test_send_event_sharded(self):\n                 \"worker_name\": \"sender2\",\n                 \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n             },\n-            http_client=mock_client2,\n+            federation_http_client=mock_client2,\n         )\n \n         user = self.register_user(\"user2\", \"pass\")\n@@ -144,7 +144,7 @@ def test_send_typing_sharded(self):\n                 \"worker_name\": \"sender1\",\n                 \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n             },\n-            http_client=mock_client1,\n+            federation_http_client=mock_client1,\n         )\n \n         mock_client2 = Mock(spec=[\"put_json\"])\n@@ -156,7 +156,7 @@ def test_send_typing_sharded(self):\n                 \"worker_name\": \"sender2\",\n                 \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n             },\n-            http_client=mock_client2,\n+            federation_http_client=mock_client2,\n         )\n \n         user = self.register_user(\"user3\", \"pass\")", "file_path": "files/2021_2/43", "file_language": "py", "file_name": "tests/replication/test_federation_sender_shard.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class FederationSenderTestCase(BaseMultiWorkerStreamTestCase):\n    servlets = [\n        login.register_servlets,\n        register_servlets_for_client_rest_resource,\n        room.register_servlets,\n    ]\n\n    def default_config(self):\n        conf = super().default_config()\n        conf[\"send_federation\"] = False\n        return conf\n\n    def test_send_event_single_sender(self):\n        \"\"\"Test that using a single federation sender worker correctly sends a\n        new event.\n        \"\"\"\n        mock_client = Mock(spec=[\"put_json\"])\n        mock_client.put_json.return_value = make_awaitable({})\n\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\"send_federation\": True},\n            http_client=mock_client,\n        )\n\n        user = self.register_user(\"user\", \"pass\")\n        token = self.login(\"user\", \"pass\")\n\n        room = self.create_room_with_remote_server(user, token)\n\n        mock_client.put_json.reset_mock()\n\n        self.create_and_send_event(room, UserID.from_string(user))\n        self.replicate()\n\n        # Assert that the event was sent out over federation.\n        mock_client.put_json.assert_called()\n        self.assertEqual(mock_client.put_json.call_args[0][0], \"other_server\")\n        self.assertTrue(mock_client.put_json.call_args[1][\"data\"].get(\"pdus\"))\n\n    def test_send_event_sharded(self):\n        \"\"\"Test that using two federation sender workers correctly sends\n        new events.\n        \"\"\"\n        mock_client1 = Mock(spec=[\"put_json\"])\n        mock_client1.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender1\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            http_client=mock_client1,\n        )\n\n        mock_client2 = Mock(spec=[\"put_json\"])\n        mock_client2.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender2\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            http_client=mock_client2,\n        )\n\n        user = self.register_user(\"user2\", \"pass\")\n        token = self.login(\"user2\", \"pass\")\n\n        sent_on_1 = False\n        sent_on_2 = False\n        for i in range(20):\n            server_name = \"other_server_%d\" % (i,)\n            room = self.create_room_with_remote_server(user, token, server_name)\n            mock_client1.reset_mock()  # type: ignore[attr-defined]\n            mock_client2.reset_mock()  # type: ignore[attr-defined]\n\n            self.create_and_send_event(room, UserID.from_string(user))\n            self.replicate()\n\n            if mock_client1.put_json.called:\n                sent_on_1 = True\n                mock_client2.put_json.assert_not_called()\n                self.assertEqual(mock_client1.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client1.put_json.call_args[1][\"data\"].get(\"pdus\"))\n            elif mock_client2.put_json.called:\n                sent_on_2 = True\n                mock_client1.put_json.assert_not_called()\n                self.assertEqual(mock_client2.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client2.put_json.call_args[1][\"data\"].get(\"pdus\"))\n            else:\n                raise AssertionError(\n                    \"Expected send transaction from one or the other sender\"\n                )\n\n            if sent_on_1 and sent_on_2:\n                break\n\n        self.assertTrue(sent_on_1)\n        self.assertTrue(sent_on_2)\n\n    def test_send_typing_sharded(self):\n        \"\"\"Test that using two federation sender workers correctly sends\n        new typing EDUs.\n        \"\"\"\n        mock_client1 = Mock(spec=[\"put_json\"])\n        mock_client1.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender1\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            http_client=mock_client1,\n        )\n\n        mock_client2 = Mock(spec=[\"put_json\"])\n        mock_client2.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender2\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            http_client=mock_client2,\n        )\n\n        user = self.register_user(\"user3\", \"pass\")\n        token = self.login(\"user3\", \"pass\")\n\n        typing_handler = self.hs.get_typing_handler()\n\n        sent_on_1 = False\n        sent_on_2 = False\n        for i in range(20):\n            server_name = \"other_server_%d\" % (i,)\n            room = self.create_room_with_remote_server(user, token, server_name)\n            mock_client1.reset_mock()  # type: ignore[attr-defined]\n            mock_client2.reset_mock()  # type: ignore[attr-defined]\n\n            self.get_success(\n                typing_handler.started_typing(\n                    target_user=UserID.from_string(user),\n                    requester=create_requester(user),\n                    room_id=room,\n                    timeout=20000,\n                )\n            )\n\n            self.replicate()\n\n            if mock_client1.put_json.called:\n                sent_on_1 = True\n                mock_client2.put_json.assert_not_called()\n                self.assertEqual(mock_client1.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client1.put_json.call_args[1][\"data\"].get(\"edus\"))\n            elif mock_client2.put_json.called:\n                sent_on_2 = True\n                mock_client1.put_json.assert_not_called()\n                self.assertEqual(mock_client2.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client2.put_json.call_args[1][\"data\"].get(\"edus\"))\n            else:\n                raise AssertionError(\n                    \"Expected send transaction from one or the other sender\"\n                )\n\n            if sent_on_1 and sent_on_2:\n                break\n\n        self.assertTrue(sent_on_1)\n        self.assertTrue(sent_on_2)\n\n    def create_room_with_remote_server(self, user, token, remote_server=\"other_server\"):\n        room = self.helper.create_room_as(user, tok=token)\n        store = self.hs.get_datastore()\n        federation = self.hs.get_federation_handler()\n\n        prev_event_ids = self.get_success(store.get_latest_event_ids_in_room(room))\n        room_version = self.get_success(store.get_room_version(room))\n\n        factory = EventBuilderFactory(self.hs)\n        factory.hostname = remote_server\n\n        user_id = UserID(\"user\", remote_server).to_string()\n\n        event_dict = {\n            \"type\": EventTypes.Member,\n            \"state_key\": user_id,\n            \"content\": {\"membership\": Membership.JOIN},\n            \"sender\": user_id,\n            \"room_id\": room,\n        }\n\n        builder = factory.for_room_version(room_version, event_dict)\n        join_event = self.get_success(builder.build(prev_event_ids, None))\n\n        self.get_success(federation.on_send_join_request(remote_server, join_event))\n        self.replicate()\n\n        return room", "target": 0}], "function_after": [{"function": "class FederationSenderTestCase(BaseMultiWorkerStreamTestCase):\n    servlets = [\n        login.register_servlets,\n        register_servlets_for_client_rest_resource,\n        room.register_servlets,\n    ]\n\n    def default_config(self):\n        conf = super().default_config()\n        conf[\"send_federation\"] = False\n        return conf\n\n    def test_send_event_single_sender(self):\n        \"\"\"Test that using a single federation sender worker correctly sends a\n        new event.\n        \"\"\"\n        mock_client = Mock(spec=[\"put_json\"])\n        mock_client.put_json.return_value = make_awaitable({})\n\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\"send_federation\": True},\n            federation_http_client=mock_client,\n        )\n\n        user = self.register_user(\"user\", \"pass\")\n        token = self.login(\"user\", \"pass\")\n\n        room = self.create_room_with_remote_server(user, token)\n\n        mock_client.put_json.reset_mock()\n\n        self.create_and_send_event(room, UserID.from_string(user))\n        self.replicate()\n\n        # Assert that the event was sent out over federation.\n        mock_client.put_json.assert_called()\n        self.assertEqual(mock_client.put_json.call_args[0][0], \"other_server\")\n        self.assertTrue(mock_client.put_json.call_args[1][\"data\"].get(\"pdus\"))\n\n    def test_send_event_sharded(self):\n        \"\"\"Test that using two federation sender workers correctly sends\n        new events.\n        \"\"\"\n        mock_client1 = Mock(spec=[\"put_json\"])\n        mock_client1.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender1\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            federation_http_client=mock_client1,\n        )\n\n        mock_client2 = Mock(spec=[\"put_json\"])\n        mock_client2.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender2\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            federation_http_client=mock_client2,\n        )\n\n        user = self.register_user(\"user2\", \"pass\")\n        token = self.login(\"user2\", \"pass\")\n\n        sent_on_1 = False\n        sent_on_2 = False\n        for i in range(20):\n            server_name = \"other_server_%d\" % (i,)\n            room = self.create_room_with_remote_server(user, token, server_name)\n            mock_client1.reset_mock()  # type: ignore[attr-defined]\n            mock_client2.reset_mock()  # type: ignore[attr-defined]\n\n            self.create_and_send_event(room, UserID.from_string(user))\n            self.replicate()\n\n            if mock_client1.put_json.called:\n                sent_on_1 = True\n                mock_client2.put_json.assert_not_called()\n                self.assertEqual(mock_client1.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client1.put_json.call_args[1][\"data\"].get(\"pdus\"))\n            elif mock_client2.put_json.called:\n                sent_on_2 = True\n                mock_client1.put_json.assert_not_called()\n                self.assertEqual(mock_client2.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client2.put_json.call_args[1][\"data\"].get(\"pdus\"))\n            else:\n                raise AssertionError(\n                    \"Expected send transaction from one or the other sender\"\n                )\n\n            if sent_on_1 and sent_on_2:\n                break\n\n        self.assertTrue(sent_on_1)\n        self.assertTrue(sent_on_2)\n\n    def test_send_typing_sharded(self):\n        \"\"\"Test that using two federation sender workers correctly sends\n        new typing EDUs.\n        \"\"\"\n        mock_client1 = Mock(spec=[\"put_json\"])\n        mock_client1.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender1\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            federation_http_client=mock_client1,\n        )\n\n        mock_client2 = Mock(spec=[\"put_json\"])\n        mock_client2.put_json.return_value = make_awaitable({})\n        self.make_worker_hs(\n            \"synapse.app.federation_sender\",\n            {\n                \"send_federation\": True,\n                \"worker_name\": \"sender2\",\n                \"federation_sender_instances\": [\"sender1\", \"sender2\"],\n            },\n            federation_http_client=mock_client2,\n        )\n\n        user = self.register_user(\"user3\", \"pass\")\n        token = self.login(\"user3\", \"pass\")\n\n        typing_handler = self.hs.get_typing_handler()\n\n        sent_on_1 = False\n        sent_on_2 = False\n        for i in range(20):\n            server_name = \"other_server_%d\" % (i,)\n            room = self.create_room_with_remote_server(user, token, server_name)\n            mock_client1.reset_mock()  # type: ignore[attr-defined]\n            mock_client2.reset_mock()  # type: ignore[attr-defined]\n\n            self.get_success(\n                typing_handler.started_typing(\n                    target_user=UserID.from_string(user),\n                    requester=create_requester(user),\n                    room_id=room,\n                    timeout=20000,\n                )\n            )\n\n            self.replicate()\n\n            if mock_client1.put_json.called:\n                sent_on_1 = True\n                mock_client2.put_json.assert_not_called()\n                self.assertEqual(mock_client1.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client1.put_json.call_args[1][\"data\"].get(\"edus\"))\n            elif mock_client2.put_json.called:\n                sent_on_2 = True\n                mock_client1.put_json.assert_not_called()\n                self.assertEqual(mock_client2.put_json.call_args[0][0], server_name)\n                self.assertTrue(mock_client2.put_json.call_args[1][\"data\"].get(\"edus\"))\n            else:\n                raise AssertionError(\n                    \"Expected send transaction from one or the other sender\"\n                )\n\n            if sent_on_1 and sent_on_2:\n                break\n\n        self.assertTrue(sent_on_1)\n        self.assertTrue(sent_on_2)\n\n    def create_room_with_remote_server(self, user, token, remote_server=\"other_server\"):\n        room = self.helper.create_room_as(user, tok=token)\n        store = self.hs.get_datastore()\n        federation = self.hs.get_federation_handler()\n\n        prev_event_ids = self.get_success(store.get_latest_event_ids_in_room(room))\n        room_version = self.get_success(store.get_room_version(room))\n\n        factory = EventBuilderFactory(self.hs)\n        factory.hostname = remote_server\n\n        user_id = UserID(\"user\", remote_server).to_string()\n\n        event_dict = {\n            \"type\": EventTypes.Member,\n            \"state_key\": user_id,\n            \"content\": {\"membership\": Membership.JOIN},\n            \"sender\": user_id,\n            \"room_id\": room,\n        }\n\n        builder = factory.for_room_version(room_version, event_dict)\n        join_event = self.get_success(builder.build(prev_event_ids, None))\n\n        self.get_success(federation.on_send_join_request(remote_server, join_event))\n        self.replicate()\n\n        return room", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Freplication%2Ftest_pusher_shard.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\n\nfrom mock import Mock\n\nfrom twisted.internet import defer\n\nfrom synapse.rest import admin\nfrom synapse.rest.client.v1 import login, room\n\nfrom tests.replication._base import BaseMultiWorkerStreamTestCase\n\nlogger = logging.getLogger(__name__)\n\n\nclass PusherShardTestCase(BaseMultiWorkerStreamTestCase):\n    \"\"\"Checks pusher sharding works\n    \"\"\"\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        # Register a user who sends a message that we'll get notified about\n        self.other_user_id = self.register_user(\"otheruser\", \"pass\")\n        self.other_access_token = self.login(\"otheruser\", \"pass\")\n\n    def default_config(self):\n        conf = super().default_config()\n        conf[\"start_pushers\"] = False\n        return conf\n\n    def _create_pusher_and_send_msg(self, localpart):\n        # Create a user that will get push notifications\n        user_id = self.register_user(localpart, \"pass\")\n        access_token = self.login(localpart, \"pass\")\n\n        # Register a pusher\n        user_dict = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_dict.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"https://push.example.com/push\"},\n            )\n        )\n\n        self.pump()\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(\n            room=room, user=self.other_user_id, tok=self.other_access_token\n        )\n\n        # The other user sends some messages\n        response = self.helper.send(room, body=\"Hi!\", tok=self.other_access_token)\n        event_id = response[\"event_id\"]\n\n        return event_id\n\n    def test_send_push_single_worker(self):\n        \"\"\"Test that registration works when using a pusher worker.\n        \"\"\"\n        http_client_mock = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\"start_pushers\": True},\n            proxied_blacklisted_http_client=http_client_mock,\n        )\n\n        event_id = self._create_pusher_and_send_msg(\"user\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock.post_json_get_json.assert_called_once()\n        self.assertEqual(\n            http_client_mock.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )\n\n    def test_send_push_multiple_workers(self):\n        \"\"\"Test that registration works when using sharded pusher workers.\n        \"\"\"\n        http_client_mock1 = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock1.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\n                \"start_pushers\": True,\n                \"worker_name\": \"pusher1\",\n                \"pusher_instances\": [\"pusher1\", \"pusher2\"],\n            },\n            proxied_blacklisted_http_client=http_client_mock1,\n        )\n\n        http_client_mock2 = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock2.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\n                \"start_pushers\": True,\n                \"worker_name\": \"pusher2\",\n                \"pusher_instances\": [\"pusher1\", \"pusher2\"],\n            },\n            proxied_blacklisted_http_client=http_client_mock2,\n        )\n\n        # We choose a user name that we know should go to pusher1.\n        event_id = self._create_pusher_and_send_msg(\"user2\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock1.post_json_get_json.assert_called_once()\n        http_client_mock2.post_json_get_json.assert_not_called()\n        self.assertEqual(\n            http_client_mock1.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock1.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )\n\n        http_client_mock1.post_json_get_json.reset_mock()\n        http_client_mock2.post_json_get_json.reset_mock()\n\n        # Now we choose a user name that we know should go to pusher2.\n        event_id = self._create_pusher_and_send_msg(\"user4\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock1.post_json_get_json.assert_not_called()\n        http_client_mock2.post_json_get_json.assert_called_once()\n        self.assertEqual(\n            http_client_mock2.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock2.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\n\nfrom mock import Mock\n\nfrom twisted.internet import defer\n\nfrom synapse.rest import admin\nfrom synapse.rest.client.v1 import login, room\n\nfrom tests.replication._base import BaseMultiWorkerStreamTestCase\n\nlogger = logging.getLogger(__name__)\n\n\nclass PusherShardTestCase(BaseMultiWorkerStreamTestCase):\n    \"\"\"Checks pusher sharding works\n    \"\"\"\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        # Register a user who sends a message that we'll get notified about\n        self.other_user_id = self.register_user(\"otheruser\", \"pass\")\n        self.other_access_token = self.login(\"otheruser\", \"pass\")\n\n    def default_config(self):\n        conf = super().default_config()\n        conf[\"start_pushers\"] = False\n        return conf\n\n    def _create_pusher_and_send_msg(self, localpart):\n        # Create a user that will get push notifications\n        user_id = self.register_user(localpart, \"pass\")\n        access_token = self.login(localpart, \"pass\")\n\n        # Register a pusher\n        user_dict = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_dict.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"https://push.example.com/push\"},\n            )\n        )\n\n        self.pump()\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(\n            room=room, user=self.other_user_id, tok=self.other_access_token\n        )\n\n        # The other user sends some messages\n        response = self.helper.send(room, body=\"Hi!\", tok=self.other_access_token)\n        event_id = response[\"event_id\"]\n\n        return event_id\n\n    def test_send_push_single_worker(self):\n        \"\"\"Test that registration works when using a pusher worker.\n        \"\"\"\n        http_client_mock = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\"start_pushers\": True},\n            proxied_http_client=http_client_mock,\n        )\n\n        event_id = self._create_pusher_and_send_msg(\"user\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock.post_json_get_json.assert_called_once()\n        self.assertEqual(\n            http_client_mock.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )\n\n    def test_send_push_multiple_workers(self):\n        \"\"\"Test that registration works when using sharded pusher workers.\n        \"\"\"\n        http_client_mock1 = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock1.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\n                \"start_pushers\": True,\n                \"worker_name\": \"pusher1\",\n                \"pusher_instances\": [\"pusher1\", \"pusher2\"],\n            },\n            proxied_http_client=http_client_mock1,\n        )\n\n        http_client_mock2 = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock2.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\n                \"start_pushers\": True,\n                \"worker_name\": \"pusher2\",\n                \"pusher_instances\": [\"pusher1\", \"pusher2\"],\n            },\n            proxied_http_client=http_client_mock2,\n        )\n\n        # We choose a user name that we know should go to pusher1.\n        event_id = self._create_pusher_and_send_msg(\"user2\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock1.post_json_get_json.assert_called_once()\n        http_client_mock2.post_json_get_json.assert_not_called()\n        self.assertEqual(\n            http_client_mock1.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock1.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )\n\n        http_client_mock1.post_json_get_json.reset_mock()\n        http_client_mock2.post_json_get_json.reset_mock()\n\n        # Now we choose a user name that we know should go to pusher2.\n        event_id = self._create_pusher_and_send_msg(\"user4\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock1.post_json_get_json.assert_not_called()\n        http_client_mock2.post_json_get_json.assert_called_once()\n        self.assertEqual(\n            http_client_mock2.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock2.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )\n", "patch": "@@ -98,7 +98,7 @@ def test_send_push_single_worker(self):\n         self.make_worker_hs(\n             \"synapse.app.pusher\",\n             {\"start_pushers\": True},\n-            proxied_http_client=http_client_mock,\n+            proxied_blacklisted_http_client=http_client_mock,\n         )\n \n         event_id = self._create_pusher_and_send_msg(\"user\")\n@@ -133,7 +133,7 @@ def test_send_push_multiple_workers(self):\n                 \"worker_name\": \"pusher1\",\n                 \"pusher_instances\": [\"pusher1\", \"pusher2\"],\n             },\n-            proxied_http_client=http_client_mock1,\n+            proxied_blacklisted_http_client=http_client_mock1,\n         )\n \n         http_client_mock2 = Mock(spec_set=[\"post_json_get_json\"])\n@@ -148,7 +148,7 @@ def test_send_push_multiple_workers(self):\n                 \"worker_name\": \"pusher2\",\n                 \"pusher_instances\": [\"pusher1\", \"pusher2\"],\n             },\n-            proxied_http_client=http_client_mock2,\n+            proxied_blacklisted_http_client=http_client_mock2,\n         )\n \n         # We choose a user name that we know should go to pusher1.", "file_path": "files/2021_2/44", "file_language": "py", "file_name": "tests/replication/test_pusher_shard.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class PusherShardTestCase(BaseMultiWorkerStreamTestCase):\n    \"\"\"Checks pusher sharding works\n    \"\"\"\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        # Register a user who sends a message that we'll get notified about\n        self.other_user_id = self.register_user(\"otheruser\", \"pass\")\n        self.other_access_token = self.login(\"otheruser\", \"pass\")\n\n    def default_config(self):\n        conf = super().default_config()\n        conf[\"start_pushers\"] = False\n        return conf\n\n    def _create_pusher_and_send_msg(self, localpart):\n        # Create a user that will get push notifications\n        user_id = self.register_user(localpart, \"pass\")\n        access_token = self.login(localpart, \"pass\")\n\n        # Register a pusher\n        user_dict = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_dict.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"https://push.example.com/push\"},\n            )\n        )\n\n        self.pump()\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(\n            room=room, user=self.other_user_id, tok=self.other_access_token\n        )\n\n        # The other user sends some messages\n        response = self.helper.send(room, body=\"Hi!\", tok=self.other_access_token)\n        event_id = response[\"event_id\"]\n\n        return event_id\n\n    def test_send_push_single_worker(self):\n        \"\"\"Test that registration works when using a pusher worker.\n        \"\"\"\n        http_client_mock = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\"start_pushers\": True},\n            proxied_http_client=http_client_mock,\n        )\n\n        event_id = self._create_pusher_and_send_msg(\"user\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock.post_json_get_json.assert_called_once()\n        self.assertEqual(\n            http_client_mock.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )\n\n    def test_send_push_multiple_workers(self):\n        \"\"\"Test that registration works when using sharded pusher workers.\n        \"\"\"\n        http_client_mock1 = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock1.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\n                \"start_pushers\": True,\n                \"worker_name\": \"pusher1\",\n                \"pusher_instances\": [\"pusher1\", \"pusher2\"],\n            },\n            proxied_http_client=http_client_mock1,\n        )\n\n        http_client_mock2 = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock2.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\n                \"start_pushers\": True,\n                \"worker_name\": \"pusher2\",\n                \"pusher_instances\": [\"pusher1\", \"pusher2\"],\n            },\n            proxied_http_client=http_client_mock2,\n        )\n\n        # We choose a user name that we know should go to pusher1.\n        event_id = self._create_pusher_and_send_msg(\"user2\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock1.post_json_get_json.assert_called_once()\n        http_client_mock2.post_json_get_json.assert_not_called()\n        self.assertEqual(\n            http_client_mock1.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock1.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )\n\n        http_client_mock1.post_json_get_json.reset_mock()\n        http_client_mock2.post_json_get_json.reset_mock()\n\n        # Now we choose a user name that we know should go to pusher2.\n        event_id = self._create_pusher_and_send_msg(\"user4\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock1.post_json_get_json.assert_not_called()\n        http_client_mock2.post_json_get_json.assert_called_once()\n        self.assertEqual(\n            http_client_mock2.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock2.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )", "target": 0}], "function_after": [{"function": "class PusherShardTestCase(BaseMultiWorkerStreamTestCase):\n    \"\"\"Checks pusher sharding works\n    \"\"\"\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        # Register a user who sends a message that we'll get notified about\n        self.other_user_id = self.register_user(\"otheruser\", \"pass\")\n        self.other_access_token = self.login(\"otheruser\", \"pass\")\n\n    def default_config(self):\n        conf = super().default_config()\n        conf[\"start_pushers\"] = False\n        return conf\n\n    def _create_pusher_and_send_msg(self, localpart):\n        # Create a user that will get push notifications\n        user_id = self.register_user(localpart, \"pass\")\n        access_token = self.login(localpart, \"pass\")\n\n        # Register a pusher\n        user_dict = self.get_success(\n            self.hs.get_datastore().get_user_by_access_token(access_token)\n        )\n        token_id = user_dict.token_id\n\n        self.get_success(\n            self.hs.get_pusherpool().add_pusher(\n                user_id=user_id,\n                access_token=token_id,\n                kind=\"http\",\n                app_id=\"m.http\",\n                app_display_name=\"HTTP Push Notifications\",\n                device_display_name=\"pushy push\",\n                pushkey=\"a@example.com\",\n                lang=None,\n                data={\"url\": \"https://push.example.com/push\"},\n            )\n        )\n\n        self.pump()\n\n        # Create a room\n        room = self.helper.create_room_as(user_id, tok=access_token)\n\n        # The other user joins\n        self.helper.join(\n            room=room, user=self.other_user_id, tok=self.other_access_token\n        )\n\n        # The other user sends some messages\n        response = self.helper.send(room, body=\"Hi!\", tok=self.other_access_token)\n        event_id = response[\"event_id\"]\n\n        return event_id\n\n    def test_send_push_single_worker(self):\n        \"\"\"Test that registration works when using a pusher worker.\n        \"\"\"\n        http_client_mock = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\"start_pushers\": True},\n            proxied_blacklisted_http_client=http_client_mock,\n        )\n\n        event_id = self._create_pusher_and_send_msg(\"user\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock.post_json_get_json.assert_called_once()\n        self.assertEqual(\n            http_client_mock.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )\n\n    def test_send_push_multiple_workers(self):\n        \"\"\"Test that registration works when using sharded pusher workers.\n        \"\"\"\n        http_client_mock1 = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock1.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\n                \"start_pushers\": True,\n                \"worker_name\": \"pusher1\",\n                \"pusher_instances\": [\"pusher1\", \"pusher2\"],\n            },\n            proxied_blacklisted_http_client=http_client_mock1,\n        )\n\n        http_client_mock2 = Mock(spec_set=[\"post_json_get_json\"])\n        http_client_mock2.post_json_get_json.side_effect = lambda *_, **__: defer.succeed(\n            {}\n        )\n\n        self.make_worker_hs(\n            \"synapse.app.pusher\",\n            {\n                \"start_pushers\": True,\n                \"worker_name\": \"pusher2\",\n                \"pusher_instances\": [\"pusher1\", \"pusher2\"],\n            },\n            proxied_blacklisted_http_client=http_client_mock2,\n        )\n\n        # We choose a user name that we know should go to pusher1.\n        event_id = self._create_pusher_and_send_msg(\"user2\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock1.post_json_get_json.assert_called_once()\n        http_client_mock2.post_json_get_json.assert_not_called()\n        self.assertEqual(\n            http_client_mock1.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock1.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )\n\n        http_client_mock1.post_json_get_json.reset_mock()\n        http_client_mock2.post_json_get_json.reset_mock()\n\n        # Now we choose a user name that we know should go to pusher2.\n        event_id = self._create_pusher_and_send_msg(\"user4\")\n\n        # Advance time a bit, so the pusher will register something has happened\n        self.pump()\n\n        http_client_mock1.post_json_get_json.assert_not_called()\n        http_client_mock2.post_json_get_json.assert_called_once()\n        self.assertEqual(\n            http_client_mock2.post_json_get_json.call_args[0][0],\n            \"https://push.example.com/push\",\n        )\n        self.assertEqual(\n            event_id,\n            http_client_mock2.post_json_get_json.call_args[0][1][\"notification\"][\n                \"event_id\"\n            ],\n        )", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Frest%2Fadmin%2Ftest_admin.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nimport urllib.parse\nfrom binascii import unhexlify\n\nfrom mock import Mock\n\nfrom twisted.internet.defer import Deferred\n\nimport synapse.rest.admin\nfrom synapse.http.server import JsonResource\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.rest.admin import VersionServlet\nfrom synapse.rest.client.v1 import login, room\nfrom synapse.rest.client.v2_alpha import groups\n\nfrom tests import unittest\nfrom tests.server import FakeSite, make_request\n\n\nclass VersionTestCase(unittest.HomeserverTestCase):\n    url = \"/_synapse/admin/v1/server_version\"\n\n    def create_test_resource(self):\n        resource = JsonResource(self.hs)\n        VersionServlet(self.hs).register(resource)\n        return resource\n\n    def test_version_string(self):\n        request, channel = self.make_request(\"GET\", self.url, shorthand=False)\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            {\"server_version\", \"python_version\"}, set(channel.json_body.keys())\n        )\n\n\nclass DeleteGroupTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        groups.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        self.other_user = self.register_user(\"user\", \"pass\")\n        self.other_user_token = self.login(\"user\", \"pass\")\n\n    def test_delete_group(self):\n        # Create a new group\n        request, channel = self.make_request(\n            \"POST\",\n            \"/create_group\".encode(\"ascii\"),\n            access_token=self.admin_user_tok,\n            content={\"localpart\": \"test\"},\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        group_id = channel.json_body[\"group_id\"]\n\n        self._check_group(group_id, expect_code=200)\n\n        # Invite/join another user\n\n        url = \"/groups/%s/admin/users/invite/%s\" % (group_id, self.other_user)\n        request, channel = self.make_request(\n            \"PUT\", url.encode(\"ascii\"), access_token=self.admin_user_tok, content={}\n        )\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        url = \"/groups/%s/self/accept_invite\" % (group_id,)\n        request, channel = self.make_request(\n            \"PUT\", url.encode(\"ascii\"), access_token=self.other_user_token, content={}\n        )\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        # Check other user knows they're in the group\n        self.assertIn(group_id, self._get_groups_user_is_in(self.admin_user_tok))\n        self.assertIn(group_id, self._get_groups_user_is_in(self.other_user_token))\n\n        # Now delete the group\n        url = \"/_synapse/admin/v1/delete_group/\" + group_id\n        request, channel = self.make_request(\n            \"POST\",\n            url.encode(\"ascii\"),\n            access_token=self.admin_user_tok,\n            content={\"localpart\": \"test\"},\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        # Check group returns 404\n        self._check_group(group_id, expect_code=404)\n\n        # Check users don't think they're in the group\n        self.assertNotIn(group_id, self._get_groups_user_is_in(self.admin_user_tok))\n        self.assertNotIn(group_id, self._get_groups_user_is_in(self.other_user_token))\n\n    def _check_group(self, group_id, expect_code):\n        \"\"\"Assert that trying to fetch the given group results in the given\n        HTTP status code\n        \"\"\"\n\n        url = \"/groups/%s/profile\" % (group_id,)\n        request, channel = self.make_request(\n            \"GET\", url.encode(\"ascii\"), access_token=self.admin_user_tok\n        )\n\n        self.assertEqual(\n            expect_code, int(channel.result[\"code\"]), msg=channel.result[\"body\"]\n        )\n\n    def _get_groups_user_is_in(self, access_token):\n        \"\"\"Returns the list of groups the user is in (given their access token)\n        \"\"\"\n        request, channel = self.make_request(\n            \"GET\", \"/joined_groups\".encode(\"ascii\"), access_token=access_token\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        return channel.json_body[\"groups\"]\n\n\nclass QuarantineMediaTestCase(unittest.HomeserverTestCase):\n    \"\"\"Test /quarantine_media admin API.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        synapse.rest.admin.register_servlets_for_media_repo,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.hs = hs\n\n        # Allow for uploading and downloading to/from the media repo\n        self.media_repo = hs.get_media_repository_resource()\n        self.download_resource = self.media_repo.children[b\"download\"]\n        self.upload_resource = self.media_repo.children[b\"upload\"]\n        self.image_data = unhexlify(\n            b\"89504e470d0a1a0a0000000d4948445200000001000000010806\"\n            b\"0000001f15c4890000000a49444154789c63000100000500010d\"\n            b\"0a2db40000000049454e44ae426082\"\n        )\n\n    def make_homeserver(self, reactor, clock):\n\n        self.fetches = []\n\n        async def get_file(destination, path, output_stream, args=None, max_size=None):\n            \"\"\"\n            Returns tuple[int,dict,str,int] of file length, response headers,\n            absolute URI, and response code.\n            \"\"\"\n\n            def write_to(r):\n                data, response = r\n                output_stream.write(data)\n                return response\n\n            d = Deferred()\n            d.addCallback(write_to)\n            self.fetches.append((d, destination, path, args))\n            return await make_deferred_yieldable(d)\n\n        client = Mock()\n        client.get_file = get_file\n\n        self.storage_path = self.mktemp()\n        self.media_store_path = self.mktemp()\n        os.mkdir(self.storage_path)\n        os.mkdir(self.media_store_path)\n\n        config = self.default_config()\n        config[\"media_store_path\"] = self.media_store_path\n        config[\"thumbnail_requirements\"] = {}\n        config[\"max_image_pixels\"] = 2000000\n\n        provider_config = {\n            \"module\": \"synapse.rest.media.v1.storage_provider.FileStorageProviderBackend\",\n            \"store_local\": True,\n            \"store_synchronous\": False,\n            \"store_remote\": True,\n            \"config\": {\"directory\": self.storage_path},\n        }\n        config[\"media_storage_providers\"] = [provider_config]\n\n        hs = self.setup_test_homeserver(config=config, federation_http_client=client)\n\n        return hs\n\n    def _ensure_quarantined(self, admin_user_tok, server_and_media_id):\n        \"\"\"Ensure a piece of media is quarantined when trying to access it.\"\"\"\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_and_media_id,\n            shorthand=False,\n            access_token=admin_user_tok,\n        )\n\n        # Should be quarantined\n        self.assertEqual(\n            404,\n            int(channel.code),\n            msg=(\n                \"Expected to receive a 404 on accessing quarantined media: %s\"\n                % server_and_media_id\n            ),\n        )\n\n    def test_quarantine_media_requires_admin(self):\n        self.register_user(\"nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"nonadmin\", \"pass\")\n\n        # Attempt quarantine media APIs as non-admin\n        url = \"/_synapse/admin/v1/media/quarantine/example.org/abcde12345\"\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=non_admin_user_tok,\n        )\n\n        # Expect a forbidden error\n        self.assertEqual(\n            403,\n            int(channel.result[\"code\"]),\n            msg=\"Expected forbidden on quarantining media as a non-admin\",\n        )\n\n        # And the roomID/userID endpoint\n        url = \"/_synapse/admin/v1/room/!room%3Aexample.com/media/quarantine\"\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=non_admin_user_tok,\n        )\n\n        # Expect a forbidden error\n        self.assertEqual(\n            403,\n            int(channel.result[\"code\"]),\n            msg=\"Expected forbidden on quarantining media as a non-admin\",\n        )\n\n    def test_quarantine_media_by_id(self):\n        self.register_user(\"id_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"id_admin\", \"pass\")\n\n        self.register_user(\"id_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"id_nonadmin\", \"pass\")\n\n        # Upload some media into the room\n        response = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=admin_user_tok\n        )\n\n        # Extract media ID from the response\n        server_name_and_media_id = response[\"content_uri\"][6:]  # Cut off 'mxc://'\n        server_name, media_id = server_name_and_media_id.split(\"/\")\n\n        # Attempt to access the media\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_name_and_media_id,\n            shorthand=False,\n            access_token=non_admin_user_tok,\n        )\n\n        # Should be successful\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n\n        # Quarantine the media\n        url = \"/_synapse/admin/v1/media/quarantine/%s/%s\" % (\n            urllib.parse.quote(server_name),\n            urllib.parse.quote(media_id),\n        )\n        request, channel = self.make_request(\"POST\", url, access_token=admin_user_tok,)\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n\n        # Attempt to access the media\n        self._ensure_quarantined(admin_user_tok, server_name_and_media_id)\n\n    def test_quarantine_all_media_in_room(self, override_url_template=None):\n        self.register_user(\"room_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"room_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"room_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"room_nonadmin\", \"pass\")\n\n        room_id = self.helper.create_room_as(non_admin_user, tok=admin_user_tok)\n        self.helper.join(room_id, non_admin_user, tok=non_admin_user_tok)\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract mxcs\n        mxc_1 = response_1[\"content_uri\"]\n        mxc_2 = response_2[\"content_uri\"]\n\n        # Send it into the room\n        self.helper.send_event(\n            room_id,\n            \"m.room.message\",\n            content={\"body\": \"image-1\", \"msgtype\": \"m.image\", \"url\": mxc_1},\n            txn_id=\"111\",\n            tok=non_admin_user_tok,\n        )\n        self.helper.send_event(\n            room_id,\n            \"m.room.message\",\n            content={\"body\": \"image-2\", \"msgtype\": \"m.image\", \"url\": mxc_2},\n            txn_id=\"222\",\n            tok=non_admin_user_tok,\n        )\n\n        # Quarantine all media in the room\n        if override_url_template:\n            url = override_url_template % urllib.parse.quote(room_id)\n        else:\n            url = \"/_synapse/admin/v1/room/%s/media/quarantine\" % urllib.parse.quote(\n                room_id\n            )\n        request, channel = self.make_request(\"POST\", url, access_token=admin_user_tok,)\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 2},\n            \"Expected 2 quarantined items\",\n        )\n\n        # Convert mxc URLs to server/media_id strings\n        server_and_media_id_1 = mxc_1[6:]\n        server_and_media_id_2 = mxc_2[6:]\n\n        # Test that we cannot download any of the media anymore\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_2)\n\n    def test_quarantine_all_media_in_room_deprecated_api_path(self):\n        # Perform the above test with the deprecated API path\n        self.test_quarantine_all_media_in_room(\"/_synapse/admin/v1/quarantine_media/%s\")\n\n    def test_quarantine_all_media_by_user(self):\n        self.register_user(\"user_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"user_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"user_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"user_nonadmin\", \"pass\")\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract media IDs\n        server_and_media_id_1 = response_1[\"content_uri\"][6:]\n        server_and_media_id_2 = response_2[\"content_uri\"][6:]\n\n        # Quarantine all media by this user\n        url = \"/_synapse/admin/v1/user/%s/media/quarantine\" % urllib.parse.quote(\n            non_admin_user\n        )\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=admin_user_tok,\n        )\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 2},\n            \"Expected 2 quarantined items\",\n        )\n\n        # Attempt to access each piece of media\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_2)\n\n    def test_cannot_quarantine_safe_media(self):\n        self.register_user(\"user_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"user_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"user_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"user_nonadmin\", \"pass\")\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract media IDs\n        server_and_media_id_1 = response_1[\"content_uri\"][6:]\n        server_and_media_id_2 = response_2[\"content_uri\"][6:]\n\n        # Mark the second item as safe from quarantine.\n        _, media_id_2 = server_and_media_id_2.split(\"/\")\n        self.get_success(self.store.mark_local_media_as_safe(media_id_2))\n\n        # Quarantine all media by this user\n        url = \"/_synapse/admin/v1/user/%s/media/quarantine\" % urllib.parse.quote(\n            non_admin_user\n        )\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=admin_user_tok,\n        )\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 1},\n            \"Expected 1 quarantined item\",\n        )\n\n        # Attempt to access each piece of media, the first should fail, the\n        # second should succeed.\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n\n        # Attempt to access each piece of media\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_and_media_id_2,\n            shorthand=False,\n            access_token=non_admin_user_tok,\n        )\n\n        # Shouldn't be quarantined\n        self.assertEqual(\n            200,\n            int(channel.code),\n            msg=(\n                \"Expected to receive a 200 on accessing not-quarantined media: %s\"\n                % server_and_media_id_2\n            ),\n        )\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nimport urllib.parse\nfrom binascii import unhexlify\n\nfrom mock import Mock\n\nfrom twisted.internet.defer import Deferred\n\nimport synapse.rest.admin\nfrom synapse.http.server import JsonResource\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.rest.admin import VersionServlet\nfrom synapse.rest.client.v1 import login, room\nfrom synapse.rest.client.v2_alpha import groups\n\nfrom tests import unittest\nfrom tests.server import FakeSite, make_request\n\n\nclass VersionTestCase(unittest.HomeserverTestCase):\n    url = \"/_synapse/admin/v1/server_version\"\n\n    def create_test_resource(self):\n        resource = JsonResource(self.hs)\n        VersionServlet(self.hs).register(resource)\n        return resource\n\n    def test_version_string(self):\n        request, channel = self.make_request(\"GET\", self.url, shorthand=False)\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            {\"server_version\", \"python_version\"}, set(channel.json_body.keys())\n        )\n\n\nclass DeleteGroupTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        groups.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        self.other_user = self.register_user(\"user\", \"pass\")\n        self.other_user_token = self.login(\"user\", \"pass\")\n\n    def test_delete_group(self):\n        # Create a new group\n        request, channel = self.make_request(\n            \"POST\",\n            \"/create_group\".encode(\"ascii\"),\n            access_token=self.admin_user_tok,\n            content={\"localpart\": \"test\"},\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        group_id = channel.json_body[\"group_id\"]\n\n        self._check_group(group_id, expect_code=200)\n\n        # Invite/join another user\n\n        url = \"/groups/%s/admin/users/invite/%s\" % (group_id, self.other_user)\n        request, channel = self.make_request(\n            \"PUT\", url.encode(\"ascii\"), access_token=self.admin_user_tok, content={}\n        )\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        url = \"/groups/%s/self/accept_invite\" % (group_id,)\n        request, channel = self.make_request(\n            \"PUT\", url.encode(\"ascii\"), access_token=self.other_user_token, content={}\n        )\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        # Check other user knows they're in the group\n        self.assertIn(group_id, self._get_groups_user_is_in(self.admin_user_tok))\n        self.assertIn(group_id, self._get_groups_user_is_in(self.other_user_token))\n\n        # Now delete the group\n        url = \"/_synapse/admin/v1/delete_group/\" + group_id\n        request, channel = self.make_request(\n            \"POST\",\n            url.encode(\"ascii\"),\n            access_token=self.admin_user_tok,\n            content={\"localpart\": \"test\"},\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        # Check group returns 404\n        self._check_group(group_id, expect_code=404)\n\n        # Check users don't think they're in the group\n        self.assertNotIn(group_id, self._get_groups_user_is_in(self.admin_user_tok))\n        self.assertNotIn(group_id, self._get_groups_user_is_in(self.other_user_token))\n\n    def _check_group(self, group_id, expect_code):\n        \"\"\"Assert that trying to fetch the given group results in the given\n        HTTP status code\n        \"\"\"\n\n        url = \"/groups/%s/profile\" % (group_id,)\n        request, channel = self.make_request(\n            \"GET\", url.encode(\"ascii\"), access_token=self.admin_user_tok\n        )\n\n        self.assertEqual(\n            expect_code, int(channel.result[\"code\"]), msg=channel.result[\"body\"]\n        )\n\n    def _get_groups_user_is_in(self, access_token):\n        \"\"\"Returns the list of groups the user is in (given their access token)\n        \"\"\"\n        request, channel = self.make_request(\n            \"GET\", \"/joined_groups\".encode(\"ascii\"), access_token=access_token\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        return channel.json_body[\"groups\"]\n\n\nclass QuarantineMediaTestCase(unittest.HomeserverTestCase):\n    \"\"\"Test /quarantine_media admin API.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        synapse.rest.admin.register_servlets_for_media_repo,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.hs = hs\n\n        # Allow for uploading and downloading to/from the media repo\n        self.media_repo = hs.get_media_repository_resource()\n        self.download_resource = self.media_repo.children[b\"download\"]\n        self.upload_resource = self.media_repo.children[b\"upload\"]\n        self.image_data = unhexlify(\n            b\"89504e470d0a1a0a0000000d4948445200000001000000010806\"\n            b\"0000001f15c4890000000a49444154789c63000100000500010d\"\n            b\"0a2db40000000049454e44ae426082\"\n        )\n\n    def make_homeserver(self, reactor, clock):\n\n        self.fetches = []\n\n        async def get_file(destination, path, output_stream, args=None, max_size=None):\n            \"\"\"\n            Returns tuple[int,dict,str,int] of file length, response headers,\n            absolute URI, and response code.\n            \"\"\"\n\n            def write_to(r):\n                data, response = r\n                output_stream.write(data)\n                return response\n\n            d = Deferred()\n            d.addCallback(write_to)\n            self.fetches.append((d, destination, path, args))\n            return await make_deferred_yieldable(d)\n\n        client = Mock()\n        client.get_file = get_file\n\n        self.storage_path = self.mktemp()\n        self.media_store_path = self.mktemp()\n        os.mkdir(self.storage_path)\n        os.mkdir(self.media_store_path)\n\n        config = self.default_config()\n        config[\"media_store_path\"] = self.media_store_path\n        config[\"thumbnail_requirements\"] = {}\n        config[\"max_image_pixels\"] = 2000000\n\n        provider_config = {\n            \"module\": \"synapse.rest.media.v1.storage_provider.FileStorageProviderBackend\",\n            \"store_local\": True,\n            \"store_synchronous\": False,\n            \"store_remote\": True,\n            \"config\": {\"directory\": self.storage_path},\n        }\n        config[\"media_storage_providers\"] = [provider_config]\n\n        hs = self.setup_test_homeserver(config=config, http_client=client)\n\n        return hs\n\n    def _ensure_quarantined(self, admin_user_tok, server_and_media_id):\n        \"\"\"Ensure a piece of media is quarantined when trying to access it.\"\"\"\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_and_media_id,\n            shorthand=False,\n            access_token=admin_user_tok,\n        )\n\n        # Should be quarantined\n        self.assertEqual(\n            404,\n            int(channel.code),\n            msg=(\n                \"Expected to receive a 404 on accessing quarantined media: %s\"\n                % server_and_media_id\n            ),\n        )\n\n    def test_quarantine_media_requires_admin(self):\n        self.register_user(\"nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"nonadmin\", \"pass\")\n\n        # Attempt quarantine media APIs as non-admin\n        url = \"/_synapse/admin/v1/media/quarantine/example.org/abcde12345\"\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=non_admin_user_tok,\n        )\n\n        # Expect a forbidden error\n        self.assertEqual(\n            403,\n            int(channel.result[\"code\"]),\n            msg=\"Expected forbidden on quarantining media as a non-admin\",\n        )\n\n        # And the roomID/userID endpoint\n        url = \"/_synapse/admin/v1/room/!room%3Aexample.com/media/quarantine\"\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=non_admin_user_tok,\n        )\n\n        # Expect a forbidden error\n        self.assertEqual(\n            403,\n            int(channel.result[\"code\"]),\n            msg=\"Expected forbidden on quarantining media as a non-admin\",\n        )\n\n    def test_quarantine_media_by_id(self):\n        self.register_user(\"id_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"id_admin\", \"pass\")\n\n        self.register_user(\"id_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"id_nonadmin\", \"pass\")\n\n        # Upload some media into the room\n        response = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=admin_user_tok\n        )\n\n        # Extract media ID from the response\n        server_name_and_media_id = response[\"content_uri\"][6:]  # Cut off 'mxc://'\n        server_name, media_id = server_name_and_media_id.split(\"/\")\n\n        # Attempt to access the media\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_name_and_media_id,\n            shorthand=False,\n            access_token=non_admin_user_tok,\n        )\n\n        # Should be successful\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n\n        # Quarantine the media\n        url = \"/_synapse/admin/v1/media/quarantine/%s/%s\" % (\n            urllib.parse.quote(server_name),\n            urllib.parse.quote(media_id),\n        )\n        request, channel = self.make_request(\"POST\", url, access_token=admin_user_tok,)\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n\n        # Attempt to access the media\n        self._ensure_quarantined(admin_user_tok, server_name_and_media_id)\n\n    def test_quarantine_all_media_in_room(self, override_url_template=None):\n        self.register_user(\"room_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"room_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"room_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"room_nonadmin\", \"pass\")\n\n        room_id = self.helper.create_room_as(non_admin_user, tok=admin_user_tok)\n        self.helper.join(room_id, non_admin_user, tok=non_admin_user_tok)\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract mxcs\n        mxc_1 = response_1[\"content_uri\"]\n        mxc_2 = response_2[\"content_uri\"]\n\n        # Send it into the room\n        self.helper.send_event(\n            room_id,\n            \"m.room.message\",\n            content={\"body\": \"image-1\", \"msgtype\": \"m.image\", \"url\": mxc_1},\n            txn_id=\"111\",\n            tok=non_admin_user_tok,\n        )\n        self.helper.send_event(\n            room_id,\n            \"m.room.message\",\n            content={\"body\": \"image-2\", \"msgtype\": \"m.image\", \"url\": mxc_2},\n            txn_id=\"222\",\n            tok=non_admin_user_tok,\n        )\n\n        # Quarantine all media in the room\n        if override_url_template:\n            url = override_url_template % urllib.parse.quote(room_id)\n        else:\n            url = \"/_synapse/admin/v1/room/%s/media/quarantine\" % urllib.parse.quote(\n                room_id\n            )\n        request, channel = self.make_request(\"POST\", url, access_token=admin_user_tok,)\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 2},\n            \"Expected 2 quarantined items\",\n        )\n\n        # Convert mxc URLs to server/media_id strings\n        server_and_media_id_1 = mxc_1[6:]\n        server_and_media_id_2 = mxc_2[6:]\n\n        # Test that we cannot download any of the media anymore\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_2)\n\n    def test_quarantine_all_media_in_room_deprecated_api_path(self):\n        # Perform the above test with the deprecated API path\n        self.test_quarantine_all_media_in_room(\"/_synapse/admin/v1/quarantine_media/%s\")\n\n    def test_quarantine_all_media_by_user(self):\n        self.register_user(\"user_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"user_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"user_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"user_nonadmin\", \"pass\")\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract media IDs\n        server_and_media_id_1 = response_1[\"content_uri\"][6:]\n        server_and_media_id_2 = response_2[\"content_uri\"][6:]\n\n        # Quarantine all media by this user\n        url = \"/_synapse/admin/v1/user/%s/media/quarantine\" % urllib.parse.quote(\n            non_admin_user\n        )\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=admin_user_tok,\n        )\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 2},\n            \"Expected 2 quarantined items\",\n        )\n\n        # Attempt to access each piece of media\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_2)\n\n    def test_cannot_quarantine_safe_media(self):\n        self.register_user(\"user_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"user_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"user_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"user_nonadmin\", \"pass\")\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract media IDs\n        server_and_media_id_1 = response_1[\"content_uri\"][6:]\n        server_and_media_id_2 = response_2[\"content_uri\"][6:]\n\n        # Mark the second item as safe from quarantine.\n        _, media_id_2 = server_and_media_id_2.split(\"/\")\n        self.get_success(self.store.mark_local_media_as_safe(media_id_2))\n\n        # Quarantine all media by this user\n        url = \"/_synapse/admin/v1/user/%s/media/quarantine\" % urllib.parse.quote(\n            non_admin_user\n        )\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=admin_user_tok,\n        )\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 1},\n            \"Expected 1 quarantined item\",\n        )\n\n        # Attempt to access each piece of media, the first should fail, the\n        # second should succeed.\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n\n        # Attempt to access each piece of media\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_and_media_id_2,\n            shorthand=False,\n            access_token=non_admin_user_tok,\n        )\n\n        # Shouldn't be quarantined\n        self.assertEqual(\n            200,\n            int(channel.code),\n            msg=(\n                \"Expected to receive a 200 on accessing not-quarantined media: %s\"\n                % server_and_media_id_2\n            ),\n        )\n", "patch": "@@ -210,7 +210,7 @@ def write_to(r):\n         }\n         config[\"media_storage_providers\"] = [provider_config]\n \n-        hs = self.setup_test_homeserver(config=config, http_client=client)\n+        hs = self.setup_test_homeserver(config=config, federation_http_client=client)\n \n         return hs\n ", "file_path": "files/2021_2/45", "file_language": "py", "file_name": "tests/rest/admin/test_admin.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class VersionTestCase(unittest.HomeserverTestCase):\n    url = \"/_synapse/admin/v1/server_version\"\n\n    def create_test_resource(self):\n        resource = JsonResource(self.hs)\n        VersionServlet(self.hs).register(resource)\n        return resource\n\n    def test_version_string(self):\n        request, channel = self.make_request(\"GET\", self.url, shorthand=False)\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            {\"server_version\", \"python_version\"}, set(channel.json_body.keys())\n        )", "target": 0}, {"function": "class DeleteGroupTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        groups.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        self.other_user = self.register_user(\"user\", \"pass\")\n        self.other_user_token = self.login(\"user\", \"pass\")\n\n    def test_delete_group(self):\n        # Create a new group\n        request, channel = self.make_request(\n            \"POST\",\n            \"/create_group\".encode(\"ascii\"),\n            access_token=self.admin_user_tok,\n            content={\"localpart\": \"test\"},\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        group_id = channel.json_body[\"group_id\"]\n\n        self._check_group(group_id, expect_code=200)\n\n        # Invite/join another user\n\n        url = \"/groups/%s/admin/users/invite/%s\" % (group_id, self.other_user)\n        request, channel = self.make_request(\n            \"PUT\", url.encode(\"ascii\"), access_token=self.admin_user_tok, content={}\n        )\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        url = \"/groups/%s/self/accept_invite\" % (group_id,)\n        request, channel = self.make_request(\n            \"PUT\", url.encode(\"ascii\"), access_token=self.other_user_token, content={}\n        )\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        # Check other user knows they're in the group\n        self.assertIn(group_id, self._get_groups_user_is_in(self.admin_user_tok))\n        self.assertIn(group_id, self._get_groups_user_is_in(self.other_user_token))\n\n        # Now delete the group\n        url = \"/_synapse/admin/v1/delete_group/\" + group_id\n        request, channel = self.make_request(\n            \"POST\",\n            url.encode(\"ascii\"),\n            access_token=self.admin_user_tok,\n            content={\"localpart\": \"test\"},\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        # Check group returns 404\n        self._check_group(group_id, expect_code=404)\n\n        # Check users don't think they're in the group\n        self.assertNotIn(group_id, self._get_groups_user_is_in(self.admin_user_tok))\n        self.assertNotIn(group_id, self._get_groups_user_is_in(self.other_user_token))\n\n    def _check_group(self, group_id, expect_code):\n        \"\"\"Assert that trying to fetch the given group results in the given\n        HTTP status code\n        \"\"\"\n\n        url = \"/groups/%s/profile\" % (group_id,)\n        request, channel = self.make_request(\n            \"GET\", url.encode(\"ascii\"), access_token=self.admin_user_tok\n        )\n\n        self.assertEqual(\n            expect_code, int(channel.result[\"code\"]), msg=channel.result[\"body\"]\n        )\n\n    def _get_groups_user_is_in(self, access_token):\n        \"\"\"Returns the list of groups the user is in (given their access token)\n        \"\"\"\n        request, channel = self.make_request(\n            \"GET\", \"/joined_groups\".encode(\"ascii\"), access_token=access_token\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        return channel.json_body[\"groups\"]", "target": 0}, {"function": "class QuarantineMediaTestCase(unittest.HomeserverTestCase):\n    \"\"\"Test /quarantine_media admin API.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        synapse.rest.admin.register_servlets_for_media_repo,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.hs = hs\n\n        # Allow for uploading and downloading to/from the media repo\n        self.media_repo = hs.get_media_repository_resource()\n        self.download_resource = self.media_repo.children[b\"download\"]\n        self.upload_resource = self.media_repo.children[b\"upload\"]\n        self.image_data = unhexlify(\n            b\"89504e470d0a1a0a0000000d4948445200000001000000010806\"\n            b\"0000001f15c4890000000a49444154789c63000100000500010d\"\n            b\"0a2db40000000049454e44ae426082\"\n        )\n\n    def make_homeserver(self, reactor, clock):\n\n        self.fetches = []\n\n        async def get_file(destination, path, output_stream, args=None, max_size=None):\n            \"\"\"\n            Returns tuple[int,dict,str,int] of file length, response headers,\n            absolute URI, and response code.\n            \"\"\"\n\n            def write_to(r):\n                data, response = r\n                output_stream.write(data)\n                return response\n\n            d = Deferred()\n            d.addCallback(write_to)\n            self.fetches.append((d, destination, path, args))\n            return await make_deferred_yieldable(d)\n\n        client = Mock()\n        client.get_file = get_file\n\n        self.storage_path = self.mktemp()\n        self.media_store_path = self.mktemp()\n        os.mkdir(self.storage_path)\n        os.mkdir(self.media_store_path)\n\n        config = self.default_config()\n        config[\"media_store_path\"] = self.media_store_path\n        config[\"thumbnail_requirements\"] = {}\n        config[\"max_image_pixels\"] = 2000000\n\n        provider_config = {\n            \"module\": \"synapse.rest.media.v1.storage_provider.FileStorageProviderBackend\",\n            \"store_local\": True,\n            \"store_synchronous\": False,\n            \"store_remote\": True,\n            \"config\": {\"directory\": self.storage_path},\n        }\n        config[\"media_storage_providers\"] = [provider_config]\n\n        hs = self.setup_test_homeserver(config=config, http_client=client)\n\n        return hs\n\n    def _ensure_quarantined(self, admin_user_tok, server_and_media_id):\n        \"\"\"Ensure a piece of media is quarantined when trying to access it.\"\"\"\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_and_media_id,\n            shorthand=False,\n            access_token=admin_user_tok,\n        )\n\n        # Should be quarantined\n        self.assertEqual(\n            404,\n            int(channel.code),\n            msg=(\n                \"Expected to receive a 404 on accessing quarantined media: %s\"\n                % server_and_media_id\n            ),\n        )\n\n    def test_quarantine_media_requires_admin(self):\n        self.register_user(\"nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"nonadmin\", \"pass\")\n\n        # Attempt quarantine media APIs as non-admin\n        url = \"/_synapse/admin/v1/media/quarantine/example.org/abcde12345\"\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=non_admin_user_tok,\n        )\n\n        # Expect a forbidden error\n        self.assertEqual(\n            403,\n            int(channel.result[\"code\"]),\n            msg=\"Expected forbidden on quarantining media as a non-admin\",\n        )\n\n        # And the roomID/userID endpoint\n        url = \"/_synapse/admin/v1/room/!room%3Aexample.com/media/quarantine\"\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=non_admin_user_tok,\n        )\n\n        # Expect a forbidden error\n        self.assertEqual(\n            403,\n            int(channel.result[\"code\"]),\n            msg=\"Expected forbidden on quarantining media as a non-admin\",\n        )\n\n    def test_quarantine_media_by_id(self):\n        self.register_user(\"id_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"id_admin\", \"pass\")\n\n        self.register_user(\"id_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"id_nonadmin\", \"pass\")\n\n        # Upload some media into the room\n        response = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=admin_user_tok\n        )\n\n        # Extract media ID from the response\n        server_name_and_media_id = response[\"content_uri\"][6:]  # Cut off 'mxc://'\n        server_name, media_id = server_name_and_media_id.split(\"/\")\n\n        # Attempt to access the media\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_name_and_media_id,\n            shorthand=False,\n            access_token=non_admin_user_tok,\n        )\n\n        # Should be successful\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n\n        # Quarantine the media\n        url = \"/_synapse/admin/v1/media/quarantine/%s/%s\" % (\n            urllib.parse.quote(server_name),\n            urllib.parse.quote(media_id),\n        )\n        request, channel = self.make_request(\"POST\", url, access_token=admin_user_tok,)\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n\n        # Attempt to access the media\n        self._ensure_quarantined(admin_user_tok, server_name_and_media_id)\n\n    def test_quarantine_all_media_in_room(self, override_url_template=None):\n        self.register_user(\"room_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"room_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"room_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"room_nonadmin\", \"pass\")\n\n        room_id = self.helper.create_room_as(non_admin_user, tok=admin_user_tok)\n        self.helper.join(room_id, non_admin_user, tok=non_admin_user_tok)\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract mxcs\n        mxc_1 = response_1[\"content_uri\"]\n        mxc_2 = response_2[\"content_uri\"]\n\n        # Send it into the room\n        self.helper.send_event(\n            room_id,\n            \"m.room.message\",\n            content={\"body\": \"image-1\", \"msgtype\": \"m.image\", \"url\": mxc_1},\n            txn_id=\"111\",\n            tok=non_admin_user_tok,\n        )\n        self.helper.send_event(\n            room_id,\n            \"m.room.message\",\n            content={\"body\": \"image-2\", \"msgtype\": \"m.image\", \"url\": mxc_2},\n            txn_id=\"222\",\n            tok=non_admin_user_tok,\n        )\n\n        # Quarantine all media in the room\n        if override_url_template:\n            url = override_url_template % urllib.parse.quote(room_id)\n        else:\n            url = \"/_synapse/admin/v1/room/%s/media/quarantine\" % urllib.parse.quote(\n                room_id\n            )\n        request, channel = self.make_request(\"POST\", url, access_token=admin_user_tok,)\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 2},\n            \"Expected 2 quarantined items\",\n        )\n\n        # Convert mxc URLs to server/media_id strings\n        server_and_media_id_1 = mxc_1[6:]\n        server_and_media_id_2 = mxc_2[6:]\n\n        # Test that we cannot download any of the media anymore\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_2)\n\n    def test_quarantine_all_media_in_room_deprecated_api_path(self):\n        # Perform the above test with the deprecated API path\n        self.test_quarantine_all_media_in_room(\"/_synapse/admin/v1/quarantine_media/%s\")\n\n    def test_quarantine_all_media_by_user(self):\n        self.register_user(\"user_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"user_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"user_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"user_nonadmin\", \"pass\")\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract media IDs\n        server_and_media_id_1 = response_1[\"content_uri\"][6:]\n        server_and_media_id_2 = response_2[\"content_uri\"][6:]\n\n        # Quarantine all media by this user\n        url = \"/_synapse/admin/v1/user/%s/media/quarantine\" % urllib.parse.quote(\n            non_admin_user\n        )\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=admin_user_tok,\n        )\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 2},\n            \"Expected 2 quarantined items\",\n        )\n\n        # Attempt to access each piece of media\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_2)\n\n    def test_cannot_quarantine_safe_media(self):\n        self.register_user(\"user_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"user_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"user_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"user_nonadmin\", \"pass\")\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract media IDs\n        server_and_media_id_1 = response_1[\"content_uri\"][6:]\n        server_and_media_id_2 = response_2[\"content_uri\"][6:]\n\n        # Mark the second item as safe from quarantine.\n        _, media_id_2 = server_and_media_id_2.split(\"/\")\n        self.get_success(self.store.mark_local_media_as_safe(media_id_2))\n\n        # Quarantine all media by this user\n        url = \"/_synapse/admin/v1/user/%s/media/quarantine\" % urllib.parse.quote(\n            non_admin_user\n        )\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=admin_user_tok,\n        )\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 1},\n            \"Expected 1 quarantined item\",\n        )\n\n        # Attempt to access each piece of media, the first should fail, the\n        # second should succeed.\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n\n        # Attempt to access each piece of media\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_and_media_id_2,\n            shorthand=False,\n            access_token=non_admin_user_tok,\n        )\n\n        # Shouldn't be quarantined\n        self.assertEqual(\n            200,\n            int(channel.code),\n            msg=(\n                \"Expected to receive a 200 on accessing not-quarantined media: %s\"\n                % server_and_media_id_2\n            ),\n        )", "target": 0}], "function_after": [{"function": "class VersionTestCase(unittest.HomeserverTestCase):\n    url = \"/_synapse/admin/v1/server_version\"\n\n    def create_test_resource(self):\n        resource = JsonResource(self.hs)\n        VersionServlet(self.hs).register(resource)\n        return resource\n\n    def test_version_string(self):\n        request, channel = self.make_request(\"GET\", self.url, shorthand=False)\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            {\"server_version\", \"python_version\"}, set(channel.json_body.keys())\n        )", "target": 0}, {"function": "class DeleteGroupTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        groups.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n\n        self.admin_user = self.register_user(\"admin\", \"pass\", admin=True)\n        self.admin_user_tok = self.login(\"admin\", \"pass\")\n\n        self.other_user = self.register_user(\"user\", \"pass\")\n        self.other_user_token = self.login(\"user\", \"pass\")\n\n    def test_delete_group(self):\n        # Create a new group\n        request, channel = self.make_request(\n            \"POST\",\n            \"/create_group\".encode(\"ascii\"),\n            access_token=self.admin_user_tok,\n            content={\"localpart\": \"test\"},\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        group_id = channel.json_body[\"group_id\"]\n\n        self._check_group(group_id, expect_code=200)\n\n        # Invite/join another user\n\n        url = \"/groups/%s/admin/users/invite/%s\" % (group_id, self.other_user)\n        request, channel = self.make_request(\n            \"PUT\", url.encode(\"ascii\"), access_token=self.admin_user_tok, content={}\n        )\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        url = \"/groups/%s/self/accept_invite\" % (group_id,)\n        request, channel = self.make_request(\n            \"PUT\", url.encode(\"ascii\"), access_token=self.other_user_token, content={}\n        )\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        # Check other user knows they're in the group\n        self.assertIn(group_id, self._get_groups_user_is_in(self.admin_user_tok))\n        self.assertIn(group_id, self._get_groups_user_is_in(self.other_user_token))\n\n        # Now delete the group\n        url = \"/_synapse/admin/v1/delete_group/\" + group_id\n        request, channel = self.make_request(\n            \"POST\",\n            url.encode(\"ascii\"),\n            access_token=self.admin_user_tok,\n            content={\"localpart\": \"test\"},\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        # Check group returns 404\n        self._check_group(group_id, expect_code=404)\n\n        # Check users don't think they're in the group\n        self.assertNotIn(group_id, self._get_groups_user_is_in(self.admin_user_tok))\n        self.assertNotIn(group_id, self._get_groups_user_is_in(self.other_user_token))\n\n    def _check_group(self, group_id, expect_code):\n        \"\"\"Assert that trying to fetch the given group results in the given\n        HTTP status code\n        \"\"\"\n\n        url = \"/groups/%s/profile\" % (group_id,)\n        request, channel = self.make_request(\n            \"GET\", url.encode(\"ascii\"), access_token=self.admin_user_tok\n        )\n\n        self.assertEqual(\n            expect_code, int(channel.result[\"code\"]), msg=channel.result[\"body\"]\n        )\n\n    def _get_groups_user_is_in(self, access_token):\n        \"\"\"Returns the list of groups the user is in (given their access token)\n        \"\"\"\n        request, channel = self.make_request(\n            \"GET\", \"/joined_groups\".encode(\"ascii\"), access_token=access_token\n        )\n\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n\n        return channel.json_body[\"groups\"]", "target": 0}, {"function": "class QuarantineMediaTestCase(unittest.HomeserverTestCase):\n    \"\"\"Test /quarantine_media admin API.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets,\n        synapse.rest.admin.register_servlets_for_media_repo,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.hs = hs\n\n        # Allow for uploading and downloading to/from the media repo\n        self.media_repo = hs.get_media_repository_resource()\n        self.download_resource = self.media_repo.children[b\"download\"]\n        self.upload_resource = self.media_repo.children[b\"upload\"]\n        self.image_data = unhexlify(\n            b\"89504e470d0a1a0a0000000d4948445200000001000000010806\"\n            b\"0000001f15c4890000000a49444154789c63000100000500010d\"\n            b\"0a2db40000000049454e44ae426082\"\n        )\n\n    def make_homeserver(self, reactor, clock):\n\n        self.fetches = []\n\n        async def get_file(destination, path, output_stream, args=None, max_size=None):\n            \"\"\"\n            Returns tuple[int,dict,str,int] of file length, response headers,\n            absolute URI, and response code.\n            \"\"\"\n\n            def write_to(r):\n                data, response = r\n                output_stream.write(data)\n                return response\n\n            d = Deferred()\n            d.addCallback(write_to)\n            self.fetches.append((d, destination, path, args))\n            return await make_deferred_yieldable(d)\n\n        client = Mock()\n        client.get_file = get_file\n\n        self.storage_path = self.mktemp()\n        self.media_store_path = self.mktemp()\n        os.mkdir(self.storage_path)\n        os.mkdir(self.media_store_path)\n\n        config = self.default_config()\n        config[\"media_store_path\"] = self.media_store_path\n        config[\"thumbnail_requirements\"] = {}\n        config[\"max_image_pixels\"] = 2000000\n\n        provider_config = {\n            \"module\": \"synapse.rest.media.v1.storage_provider.FileStorageProviderBackend\",\n            \"store_local\": True,\n            \"store_synchronous\": False,\n            \"store_remote\": True,\n            \"config\": {\"directory\": self.storage_path},\n        }\n        config[\"media_storage_providers\"] = [provider_config]\n\n        hs = self.setup_test_homeserver(config=config, federation_http_client=client)\n\n        return hs\n\n    def _ensure_quarantined(self, admin_user_tok, server_and_media_id):\n        \"\"\"Ensure a piece of media is quarantined when trying to access it.\"\"\"\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_and_media_id,\n            shorthand=False,\n            access_token=admin_user_tok,\n        )\n\n        # Should be quarantined\n        self.assertEqual(\n            404,\n            int(channel.code),\n            msg=(\n                \"Expected to receive a 404 on accessing quarantined media: %s\"\n                % server_and_media_id\n            ),\n        )\n\n    def test_quarantine_media_requires_admin(self):\n        self.register_user(\"nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"nonadmin\", \"pass\")\n\n        # Attempt quarantine media APIs as non-admin\n        url = \"/_synapse/admin/v1/media/quarantine/example.org/abcde12345\"\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=non_admin_user_tok,\n        )\n\n        # Expect a forbidden error\n        self.assertEqual(\n            403,\n            int(channel.result[\"code\"]),\n            msg=\"Expected forbidden on quarantining media as a non-admin\",\n        )\n\n        # And the roomID/userID endpoint\n        url = \"/_synapse/admin/v1/room/!room%3Aexample.com/media/quarantine\"\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=non_admin_user_tok,\n        )\n\n        # Expect a forbidden error\n        self.assertEqual(\n            403,\n            int(channel.result[\"code\"]),\n            msg=\"Expected forbidden on quarantining media as a non-admin\",\n        )\n\n    def test_quarantine_media_by_id(self):\n        self.register_user(\"id_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"id_admin\", \"pass\")\n\n        self.register_user(\"id_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"id_nonadmin\", \"pass\")\n\n        # Upload some media into the room\n        response = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=admin_user_tok\n        )\n\n        # Extract media ID from the response\n        server_name_and_media_id = response[\"content_uri\"][6:]  # Cut off 'mxc://'\n        server_name, media_id = server_name_and_media_id.split(\"/\")\n\n        # Attempt to access the media\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_name_and_media_id,\n            shorthand=False,\n            access_token=non_admin_user_tok,\n        )\n\n        # Should be successful\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n\n        # Quarantine the media\n        url = \"/_synapse/admin/v1/media/quarantine/%s/%s\" % (\n            urllib.parse.quote(server_name),\n            urllib.parse.quote(media_id),\n        )\n        request, channel = self.make_request(\"POST\", url, access_token=admin_user_tok,)\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n\n        # Attempt to access the media\n        self._ensure_quarantined(admin_user_tok, server_name_and_media_id)\n\n    def test_quarantine_all_media_in_room(self, override_url_template=None):\n        self.register_user(\"room_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"room_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"room_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"room_nonadmin\", \"pass\")\n\n        room_id = self.helper.create_room_as(non_admin_user, tok=admin_user_tok)\n        self.helper.join(room_id, non_admin_user, tok=non_admin_user_tok)\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract mxcs\n        mxc_1 = response_1[\"content_uri\"]\n        mxc_2 = response_2[\"content_uri\"]\n\n        # Send it into the room\n        self.helper.send_event(\n            room_id,\n            \"m.room.message\",\n            content={\"body\": \"image-1\", \"msgtype\": \"m.image\", \"url\": mxc_1},\n            txn_id=\"111\",\n            tok=non_admin_user_tok,\n        )\n        self.helper.send_event(\n            room_id,\n            \"m.room.message\",\n            content={\"body\": \"image-2\", \"msgtype\": \"m.image\", \"url\": mxc_2},\n            txn_id=\"222\",\n            tok=non_admin_user_tok,\n        )\n\n        # Quarantine all media in the room\n        if override_url_template:\n            url = override_url_template % urllib.parse.quote(room_id)\n        else:\n            url = \"/_synapse/admin/v1/room/%s/media/quarantine\" % urllib.parse.quote(\n                room_id\n            )\n        request, channel = self.make_request(\"POST\", url, access_token=admin_user_tok,)\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.code), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 2},\n            \"Expected 2 quarantined items\",\n        )\n\n        # Convert mxc URLs to server/media_id strings\n        server_and_media_id_1 = mxc_1[6:]\n        server_and_media_id_2 = mxc_2[6:]\n\n        # Test that we cannot download any of the media anymore\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_2)\n\n    def test_quarantine_all_media_in_room_deprecated_api_path(self):\n        # Perform the above test with the deprecated API path\n        self.test_quarantine_all_media_in_room(\"/_synapse/admin/v1/quarantine_media/%s\")\n\n    def test_quarantine_all_media_by_user(self):\n        self.register_user(\"user_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"user_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"user_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"user_nonadmin\", \"pass\")\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract media IDs\n        server_and_media_id_1 = response_1[\"content_uri\"][6:]\n        server_and_media_id_2 = response_2[\"content_uri\"][6:]\n\n        # Quarantine all media by this user\n        url = \"/_synapse/admin/v1/user/%s/media/quarantine\" % urllib.parse.quote(\n            non_admin_user\n        )\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=admin_user_tok,\n        )\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 2},\n            \"Expected 2 quarantined items\",\n        )\n\n        # Attempt to access each piece of media\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_2)\n\n    def test_cannot_quarantine_safe_media(self):\n        self.register_user(\"user_admin\", \"pass\", admin=True)\n        admin_user_tok = self.login(\"user_admin\", \"pass\")\n\n        non_admin_user = self.register_user(\"user_nonadmin\", \"pass\", admin=False)\n        non_admin_user_tok = self.login(\"user_nonadmin\", \"pass\")\n\n        # Upload some media\n        response_1 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n        response_2 = self.helper.upload_media(\n            self.upload_resource, self.image_data, tok=non_admin_user_tok\n        )\n\n        # Extract media IDs\n        server_and_media_id_1 = response_1[\"content_uri\"][6:]\n        server_and_media_id_2 = response_2[\"content_uri\"][6:]\n\n        # Mark the second item as safe from quarantine.\n        _, media_id_2 = server_and_media_id_2.split(\"/\")\n        self.get_success(self.store.mark_local_media_as_safe(media_id_2))\n\n        # Quarantine all media by this user\n        url = \"/_synapse/admin/v1/user/%s/media/quarantine\" % urllib.parse.quote(\n            non_admin_user\n        )\n        request, channel = self.make_request(\n            \"POST\", url.encode(\"ascii\"), access_token=admin_user_tok,\n        )\n        self.pump(1.0)\n        self.assertEqual(200, int(channel.result[\"code\"]), msg=channel.result[\"body\"])\n        self.assertEqual(\n            json.loads(channel.result[\"body\"].decode(\"utf-8\")),\n            {\"num_quarantined\": 1},\n            \"Expected 1 quarantined item\",\n        )\n\n        # Attempt to access each piece of media, the first should fail, the\n        # second should succeed.\n        self._ensure_quarantined(admin_user_tok, server_and_media_id_1)\n\n        # Attempt to access each piece of media\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            server_and_media_id_2,\n            shorthand=False,\n            access_token=non_admin_user_tok,\n        )\n\n        # Shouldn't be quarantined\n        self.assertEqual(\n            200,\n            int(channel.code),\n            msg=(\n                \"Expected to receive a 200 on accessing not-quarantined media: %s\"\n                % server_and_media_id_2\n            ),\n        )", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Frest%2Fclient%2Fv1%2Ftest_presence.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom mock import Mock\n\nfrom twisted.internet import defer\n\nfrom synapse.rest.client.v1 import presence\nfrom synapse.types import UserID\n\nfrom tests import unittest\n\n\nclass PresenceTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests presence REST API. \"\"\"\n\n    user_id = \"@sid:red\"\n\n    user = UserID.from_string(user_id)\n    servlets = [presence.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        presence_handler = Mock()\n        presence_handler.set_state.return_value = defer.succeed(None)\n\n        hs = self.setup_test_homeserver(\n            \"red\",\n            federation_http_client=None,\n            federation_client=Mock(),\n            presence_handler=presence_handler,\n        )\n\n        return hs\n\n    def test_put_presence(self):\n        \"\"\"\n        PUT to the status endpoint with use_presence enabled will call\n        set_state on the presence handler.\n        \"\"\"\n        self.hs.config.use_presence = True\n\n        body = {\"presence\": \"here\", \"status_msg\": \"beep boop\"}\n        request, channel = self.make_request(\n            \"PUT\", \"/presence/%s/status\" % (self.user_id,), body\n        )\n\n        self.assertEqual(channel.code, 200)\n        self.assertEqual(self.hs.get_presence_handler().set_state.call_count, 1)\n\n    def test_put_presence_disabled(self):\n        \"\"\"\n        PUT to the status endpoint with use_presence disabled will NOT call\n        set_state on the presence handler.\n        \"\"\"\n        self.hs.config.use_presence = False\n\n        body = {\"presence\": \"here\", \"status_msg\": \"beep boop\"}\n        request, channel = self.make_request(\n            \"PUT\", \"/presence/%s/status\" % (self.user_id,), body\n        )\n\n        self.assertEqual(channel.code, 200)\n        self.assertEqual(self.hs.get_presence_handler().set_state.call_count, 0)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom mock import Mock\n\nfrom twisted.internet import defer\n\nfrom synapse.rest.client.v1 import presence\nfrom synapse.types import UserID\n\nfrom tests import unittest\n\n\nclass PresenceTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests presence REST API. \"\"\"\n\n    user_id = \"@sid:red\"\n\n    user = UserID.from_string(user_id)\n    servlets = [presence.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        presence_handler = Mock()\n        presence_handler.set_state.return_value = defer.succeed(None)\n\n        hs = self.setup_test_homeserver(\n            \"red\",\n            http_client=None,\n            federation_client=Mock(),\n            presence_handler=presence_handler,\n        )\n\n        return hs\n\n    def test_put_presence(self):\n        \"\"\"\n        PUT to the status endpoint with use_presence enabled will call\n        set_state on the presence handler.\n        \"\"\"\n        self.hs.config.use_presence = True\n\n        body = {\"presence\": \"here\", \"status_msg\": \"beep boop\"}\n        request, channel = self.make_request(\n            \"PUT\", \"/presence/%s/status\" % (self.user_id,), body\n        )\n\n        self.assertEqual(channel.code, 200)\n        self.assertEqual(self.hs.get_presence_handler().set_state.call_count, 1)\n\n    def test_put_presence_disabled(self):\n        \"\"\"\n        PUT to the status endpoint with use_presence disabled will NOT call\n        set_state on the presence handler.\n        \"\"\"\n        self.hs.config.use_presence = False\n\n        body = {\"presence\": \"here\", \"status_msg\": \"beep boop\"}\n        request, channel = self.make_request(\n            \"PUT\", \"/presence/%s/status\" % (self.user_id,), body\n        )\n\n        self.assertEqual(channel.code, 200)\n        self.assertEqual(self.hs.get_presence_handler().set_state.call_count, 0)\n", "patch": "@@ -38,7 +38,7 @@ def make_homeserver(self, reactor, clock):\n \n         hs = self.setup_test_homeserver(\n             \"red\",\n-            http_client=None,\n+            federation_http_client=None,\n             federation_client=Mock(),\n             presence_handler=presence_handler,\n         )", "file_path": "files/2021_2/46", "file_language": "py", "file_name": "tests/rest/client/v1/test_presence.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class PresenceTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests presence REST API. \"\"\"\n\n    user_id = \"@sid:red\"\n\n    user = UserID.from_string(user_id)\n    servlets = [presence.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        presence_handler = Mock()\n        presence_handler.set_state.return_value = defer.succeed(None)\n\n        hs = self.setup_test_homeserver(\n            \"red\",\n            http_client=None,\n            federation_client=Mock(),\n            presence_handler=presence_handler,\n        )\n\n        return hs\n\n    def test_put_presence(self):\n        \"\"\"\n        PUT to the status endpoint with use_presence enabled will call\n        set_state on the presence handler.\n        \"\"\"\n        self.hs.config.use_presence = True\n\n        body = {\"presence\": \"here\", \"status_msg\": \"beep boop\"}\n        request, channel = self.make_request(\n            \"PUT\", \"/presence/%s/status\" % (self.user_id,), body\n        )\n\n        self.assertEqual(channel.code, 200)\n        self.assertEqual(self.hs.get_presence_handler().set_state.call_count, 1)\n\n    def test_put_presence_disabled(self):\n        \"\"\"\n        PUT to the status endpoint with use_presence disabled will NOT call\n        set_state on the presence handler.\n        \"\"\"\n        self.hs.config.use_presence = False\n\n        body = {\"presence\": \"here\", \"status_msg\": \"beep boop\"}\n        request, channel = self.make_request(\n            \"PUT\", \"/presence/%s/status\" % (self.user_id,), body\n        )\n\n        self.assertEqual(channel.code, 200)\n        self.assertEqual(self.hs.get_presence_handler().set_state.call_count, 0)", "target": 0}], "function_after": [{"function": "class PresenceTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests presence REST API. \"\"\"\n\n    user_id = \"@sid:red\"\n\n    user = UserID.from_string(user_id)\n    servlets = [presence.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        presence_handler = Mock()\n        presence_handler.set_state.return_value = defer.succeed(None)\n\n        hs = self.setup_test_homeserver(\n            \"red\",\n            federation_http_client=None,\n            federation_client=Mock(),\n            presence_handler=presence_handler,\n        )\n\n        return hs\n\n    def test_put_presence(self):\n        \"\"\"\n        PUT to the status endpoint with use_presence enabled will call\n        set_state on the presence handler.\n        \"\"\"\n        self.hs.config.use_presence = True\n\n        body = {\"presence\": \"here\", \"status_msg\": \"beep boop\"}\n        request, channel = self.make_request(\n            \"PUT\", \"/presence/%s/status\" % (self.user_id,), body\n        )\n\n        self.assertEqual(channel.code, 200)\n        self.assertEqual(self.hs.get_presence_handler().set_state.call_count, 1)\n\n    def test_put_presence_disabled(self):\n        \"\"\"\n        PUT to the status endpoint with use_presence disabled will NOT call\n        set_state on the presence handler.\n        \"\"\"\n        self.hs.config.use_presence = False\n\n        body = {\"presence\": \"here\", \"status_msg\": \"beep boop\"}\n        request, channel = self.make_request(\n            \"PUT\", \"/presence/%s/status\" % (self.user_id,), body\n        )\n\n        self.assertEqual(channel.code, 200)\n        self.assertEqual(self.hs.get_presence_handler().set_state.call_count, 0)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Frest%2Fclient%2Fv1%2Ftest_profile.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests REST events for /profile paths.\"\"\"\nimport json\n\nfrom mock import Mock\n\nfrom twisted.internet import defer\n\nimport synapse.types\nfrom synapse.api.errors import AuthError, SynapseError\nfrom synapse.rest import admin\nfrom synapse.rest.client.v1 import login, profile, room\n\nfrom tests import unittest\n\nfrom ....utils import MockHttpResource, setup_test_homeserver\n\nmyid = \"@1234ABCD:test\"\nPATH_PREFIX = \"/_matrix/client/r0\"\n\n\nclass MockHandlerProfileTestCase(unittest.TestCase):\n    \"\"\" Tests rest layer of profile management.\n\n    Todo: move these into ProfileTestCase\n    \"\"\"\n\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_resource = MockHttpResource(prefix=PATH_PREFIX)\n        self.mock_handler = Mock(\n            spec=[\n                \"get_displayname\",\n                \"set_displayname\",\n                \"get_avatar_url\",\n                \"set_avatar_url\",\n                \"check_profile_query_allowed\",\n            ]\n        )\n\n        self.mock_handler.get_displayname.return_value = defer.succeed(Mock())\n        self.mock_handler.set_displayname.return_value = defer.succeed(Mock())\n        self.mock_handler.get_avatar_url.return_value = defer.succeed(Mock())\n        self.mock_handler.set_avatar_url.return_value = defer.succeed(Mock())\n        self.mock_handler.check_profile_query_allowed.return_value = defer.succeed(\n            Mock()\n        )\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup,\n            \"test\",\n            federation_http_client=None,\n            resource_for_client=self.mock_resource,\n            federation=Mock(),\n            federation_client=Mock(),\n            profile_handler=self.mock_handler,\n        )\n\n        async def _get_user_by_req(request=None, allow_guest=False):\n            return synapse.types.create_requester(myid)\n\n        hs.get_auth().get_user_by_req = _get_user_by_req\n\n        profile.register_servlets(hs, self.mock_resource)\n\n    @defer.inlineCallbacks\n    def test_get_my_name(self):\n        mocked_get = self.mock_handler.get_displayname\n        mocked_get.return_value = defer.succeed(\"Frank\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/displayname\" % (myid), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"displayname\": \"Frank\"}, response)\n        self.assertEquals(mocked_get.call_args[0][0].localpart, \"1234ABCD\")\n\n    @defer.inlineCallbacks\n    def test_set_my_name(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.return_value = defer.succeed(())\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\", \"/profile/%s/displayname\" % (myid), b'{\"displayname\": \"Frank Jr.\"}'\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals(mocked_set.call_args[0][0].localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][1].user.localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][2], \"Frank Jr.\")\n\n    @defer.inlineCallbacks\n    def test_set_my_name_noauth(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.side_effect = AuthError(400, \"message\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (\"@4567:test\"),\n            b'{\"displayname\": \"Frank Jr.\"}',\n        )\n\n        self.assertTrue(400 <= code < 499, msg=\"code %d is in the 4xx range\" % (code))\n\n    @defer.inlineCallbacks\n    def test_get_other_name(self):\n        mocked_get = self.mock_handler.get_displayname\n        mocked_get.return_value = defer.succeed(\"Bob\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/displayname\" % (\"@opaque:elsewhere\"), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"displayname\": \"Bob\"}, response)\n\n    @defer.inlineCallbacks\n    def test_set_other_name(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.side_effect = SynapseError(400, \"message\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (\"@opaque:elsewhere\"),\n            b'{\"displayname\":\"bob\"}',\n        )\n\n        self.assertTrue(400 <= code <= 499, msg=\"code %d is in the 4xx range\" % (code))\n\n    @defer.inlineCallbacks\n    def test_get_my_avatar(self):\n        mocked_get = self.mock_handler.get_avatar_url\n        mocked_get.return_value = defer.succeed(\"http://my.server/me.png\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/avatar_url\" % (myid), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"avatar_url\": \"http://my.server/me.png\"}, response)\n        self.assertEquals(mocked_get.call_args[0][0].localpart, \"1234ABCD\")\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar(self):\n        mocked_set = self.mock_handler.set_avatar_url\n        mocked_set.return_value = defer.succeed(())\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/avatar_url\" % (myid),\n            b'{\"avatar_url\": \"http://my.server/pic.gif\"}',\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals(mocked_set.call_args[0][0].localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][1].user.localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][2], \"http://my.server/pic.gif\")\n\n\nclass ProfileTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        self.hs = self.setup_test_homeserver()\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        self.owner = self.register_user(\"owner\", \"pass\")\n        self.owner_tok = self.login(\"owner\", \"pass\")\n\n    def test_set_displayname(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (self.owner,),\n            content=json.dumps({\"displayname\": \"test\"}),\n            access_token=self.owner_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        res = self.get_displayname()\n        self.assertEqual(res, \"test\")\n\n    def test_set_displayname_too_long(self):\n        \"\"\"Attempts to set a stupid displayname should get a 400\"\"\"\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (self.owner,),\n            content=json.dumps({\"displayname\": \"test\" * 100}),\n            access_token=self.owner_tok,\n        )\n        self.assertEqual(channel.code, 400, channel.result)\n\n        res = self.get_displayname()\n        self.assertEqual(res, \"owner\")\n\n    def get_displayname(self):\n        request, channel = self.make_request(\n            \"GET\", \"/profile/%s/displayname\" % (self.owner,)\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n        return channel.json_body[\"displayname\"]\n\n\nclass ProfilesRestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n\n        config = self.default_config()\n        config[\"require_auth_for_profile_requests\"] = True\n        config[\"limit_profile_requests_to_users_who_share_rooms\"] = True\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        # User owning the requested profile.\n        self.owner = self.register_user(\"owner\", \"pass\")\n        self.owner_tok = self.login(\"owner\", \"pass\")\n        self.profile_url = \"/profile/%s\" % (self.owner)\n\n        # User requesting the profile.\n        self.requester = self.register_user(\"requester\", \"pass\")\n        self.requester_tok = self.login(\"requester\", \"pass\")\n\n        self.room_id = self.helper.create_room_as(self.owner, tok=self.owner_tok)\n\n    def test_no_auth(self):\n        self.try_fetch_profile(401)\n\n    def test_not_in_shared_room(self):\n        self.ensure_requester_left_room()\n\n        self.try_fetch_profile(403, access_token=self.requester_tok)\n\n    def test_in_shared_room(self):\n        self.ensure_requester_left_room()\n\n        self.helper.join(room=self.room_id, user=self.requester, tok=self.requester_tok)\n\n        self.try_fetch_profile(200, self.requester_tok)\n\n    def try_fetch_profile(self, expected_code, access_token=None):\n        self.request_profile(expected_code, access_token=access_token)\n\n        self.request_profile(\n            expected_code, url_suffix=\"/displayname\", access_token=access_token\n        )\n\n        self.request_profile(\n            expected_code, url_suffix=\"/avatar_url\", access_token=access_token\n        )\n\n    def request_profile(self, expected_code, url_suffix=\"\", access_token=None):\n        request, channel = self.make_request(\n            \"GET\", self.profile_url + url_suffix, access_token=access_token\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n\n    def ensure_requester_left_room(self):\n        try:\n            self.helper.leave(\n                room=self.room_id, user=self.requester, tok=self.requester_tok\n            )\n        except AssertionError:\n            # We don't care whether the leave request didn't return a 200 (e.g.\n            # if the user isn't already in the room), because we only want to\n            # make sure the user isn't in the room.\n            pass\n\n\nclass OwnProfileUnrestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"require_auth_for_profile_requests\"] = True\n        config[\"limit_profile_requests_to_users_who_share_rooms\"] = True\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        # User requesting the profile.\n        self.requester = self.register_user(\"requester\", \"pass\")\n        self.requester_tok = self.login(\"requester\", \"pass\")\n\n    def test_can_lookup_own_profile(self):\n        \"\"\"Tests that a user can lookup their own profile without having to be in a room\n        if 'require_auth_for_profile_requests' is set to true in the server's config.\n        \"\"\"\n        request, channel = self.make_request(\n            \"GET\", \"/profile/\" + self.requester, access_token=self.requester_tok\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/profile/\" + self.requester + \"/displayname\",\n            access_token=self.requester_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/profile/\" + self.requester + \"/avatar_url\",\n            access_token=self.requester_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests REST events for /profile paths.\"\"\"\nimport json\n\nfrom mock import Mock\n\nfrom twisted.internet import defer\n\nimport synapse.types\nfrom synapse.api.errors import AuthError, SynapseError\nfrom synapse.rest import admin\nfrom synapse.rest.client.v1 import login, profile, room\n\nfrom tests import unittest\n\nfrom ....utils import MockHttpResource, setup_test_homeserver\n\nmyid = \"@1234ABCD:test\"\nPATH_PREFIX = \"/_matrix/client/r0\"\n\n\nclass MockHandlerProfileTestCase(unittest.TestCase):\n    \"\"\" Tests rest layer of profile management.\n\n    Todo: move these into ProfileTestCase\n    \"\"\"\n\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_resource = MockHttpResource(prefix=PATH_PREFIX)\n        self.mock_handler = Mock(\n            spec=[\n                \"get_displayname\",\n                \"set_displayname\",\n                \"get_avatar_url\",\n                \"set_avatar_url\",\n                \"check_profile_query_allowed\",\n            ]\n        )\n\n        self.mock_handler.get_displayname.return_value = defer.succeed(Mock())\n        self.mock_handler.set_displayname.return_value = defer.succeed(Mock())\n        self.mock_handler.get_avatar_url.return_value = defer.succeed(Mock())\n        self.mock_handler.set_avatar_url.return_value = defer.succeed(Mock())\n        self.mock_handler.check_profile_query_allowed.return_value = defer.succeed(\n            Mock()\n        )\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup,\n            \"test\",\n            http_client=None,\n            resource_for_client=self.mock_resource,\n            federation=Mock(),\n            federation_client=Mock(),\n            profile_handler=self.mock_handler,\n        )\n\n        async def _get_user_by_req(request=None, allow_guest=False):\n            return synapse.types.create_requester(myid)\n\n        hs.get_auth().get_user_by_req = _get_user_by_req\n\n        profile.register_servlets(hs, self.mock_resource)\n\n    @defer.inlineCallbacks\n    def test_get_my_name(self):\n        mocked_get = self.mock_handler.get_displayname\n        mocked_get.return_value = defer.succeed(\"Frank\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/displayname\" % (myid), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"displayname\": \"Frank\"}, response)\n        self.assertEquals(mocked_get.call_args[0][0].localpart, \"1234ABCD\")\n\n    @defer.inlineCallbacks\n    def test_set_my_name(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.return_value = defer.succeed(())\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\", \"/profile/%s/displayname\" % (myid), b'{\"displayname\": \"Frank Jr.\"}'\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals(mocked_set.call_args[0][0].localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][1].user.localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][2], \"Frank Jr.\")\n\n    @defer.inlineCallbacks\n    def test_set_my_name_noauth(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.side_effect = AuthError(400, \"message\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (\"@4567:test\"),\n            b'{\"displayname\": \"Frank Jr.\"}',\n        )\n\n        self.assertTrue(400 <= code < 499, msg=\"code %d is in the 4xx range\" % (code))\n\n    @defer.inlineCallbacks\n    def test_get_other_name(self):\n        mocked_get = self.mock_handler.get_displayname\n        mocked_get.return_value = defer.succeed(\"Bob\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/displayname\" % (\"@opaque:elsewhere\"), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"displayname\": \"Bob\"}, response)\n\n    @defer.inlineCallbacks\n    def test_set_other_name(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.side_effect = SynapseError(400, \"message\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (\"@opaque:elsewhere\"),\n            b'{\"displayname\":\"bob\"}',\n        )\n\n        self.assertTrue(400 <= code <= 499, msg=\"code %d is in the 4xx range\" % (code))\n\n    @defer.inlineCallbacks\n    def test_get_my_avatar(self):\n        mocked_get = self.mock_handler.get_avatar_url\n        mocked_get.return_value = defer.succeed(\"http://my.server/me.png\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/avatar_url\" % (myid), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"avatar_url\": \"http://my.server/me.png\"}, response)\n        self.assertEquals(mocked_get.call_args[0][0].localpart, \"1234ABCD\")\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar(self):\n        mocked_set = self.mock_handler.set_avatar_url\n        mocked_set.return_value = defer.succeed(())\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/avatar_url\" % (myid),\n            b'{\"avatar_url\": \"http://my.server/pic.gif\"}',\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals(mocked_set.call_args[0][0].localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][1].user.localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][2], \"http://my.server/pic.gif\")\n\n\nclass ProfileTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        self.hs = self.setup_test_homeserver()\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        self.owner = self.register_user(\"owner\", \"pass\")\n        self.owner_tok = self.login(\"owner\", \"pass\")\n\n    def test_set_displayname(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (self.owner,),\n            content=json.dumps({\"displayname\": \"test\"}),\n            access_token=self.owner_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        res = self.get_displayname()\n        self.assertEqual(res, \"test\")\n\n    def test_set_displayname_too_long(self):\n        \"\"\"Attempts to set a stupid displayname should get a 400\"\"\"\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (self.owner,),\n            content=json.dumps({\"displayname\": \"test\" * 100}),\n            access_token=self.owner_tok,\n        )\n        self.assertEqual(channel.code, 400, channel.result)\n\n        res = self.get_displayname()\n        self.assertEqual(res, \"owner\")\n\n    def get_displayname(self):\n        request, channel = self.make_request(\n            \"GET\", \"/profile/%s/displayname\" % (self.owner,)\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n        return channel.json_body[\"displayname\"]\n\n\nclass ProfilesRestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n\n        config = self.default_config()\n        config[\"require_auth_for_profile_requests\"] = True\n        config[\"limit_profile_requests_to_users_who_share_rooms\"] = True\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        # User owning the requested profile.\n        self.owner = self.register_user(\"owner\", \"pass\")\n        self.owner_tok = self.login(\"owner\", \"pass\")\n        self.profile_url = \"/profile/%s\" % (self.owner)\n\n        # User requesting the profile.\n        self.requester = self.register_user(\"requester\", \"pass\")\n        self.requester_tok = self.login(\"requester\", \"pass\")\n\n        self.room_id = self.helper.create_room_as(self.owner, tok=self.owner_tok)\n\n    def test_no_auth(self):\n        self.try_fetch_profile(401)\n\n    def test_not_in_shared_room(self):\n        self.ensure_requester_left_room()\n\n        self.try_fetch_profile(403, access_token=self.requester_tok)\n\n    def test_in_shared_room(self):\n        self.ensure_requester_left_room()\n\n        self.helper.join(room=self.room_id, user=self.requester, tok=self.requester_tok)\n\n        self.try_fetch_profile(200, self.requester_tok)\n\n    def try_fetch_profile(self, expected_code, access_token=None):\n        self.request_profile(expected_code, access_token=access_token)\n\n        self.request_profile(\n            expected_code, url_suffix=\"/displayname\", access_token=access_token\n        )\n\n        self.request_profile(\n            expected_code, url_suffix=\"/avatar_url\", access_token=access_token\n        )\n\n    def request_profile(self, expected_code, url_suffix=\"\", access_token=None):\n        request, channel = self.make_request(\n            \"GET\", self.profile_url + url_suffix, access_token=access_token\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n\n    def ensure_requester_left_room(self):\n        try:\n            self.helper.leave(\n                room=self.room_id, user=self.requester, tok=self.requester_tok\n            )\n        except AssertionError:\n            # We don't care whether the leave request didn't return a 200 (e.g.\n            # if the user isn't already in the room), because we only want to\n            # make sure the user isn't in the room.\n            pass\n\n\nclass OwnProfileUnrestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"require_auth_for_profile_requests\"] = True\n        config[\"limit_profile_requests_to_users_who_share_rooms\"] = True\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        # User requesting the profile.\n        self.requester = self.register_user(\"requester\", \"pass\")\n        self.requester_tok = self.login(\"requester\", \"pass\")\n\n    def test_can_lookup_own_profile(self):\n        \"\"\"Tests that a user can lookup their own profile without having to be in a room\n        if 'require_auth_for_profile_requests' is set to true in the server's config.\n        \"\"\"\n        request, channel = self.make_request(\n            \"GET\", \"/profile/\" + self.requester, access_token=self.requester_tok\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/profile/\" + self.requester + \"/displayname\",\n            access_token=self.requester_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/profile/\" + self.requester + \"/avatar_url\",\n            access_token=self.requester_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n", "patch": "@@ -63,7 +63,7 @@ def setUp(self):\n         hs = yield setup_test_homeserver(\n             self.addCleanup,\n             \"test\",\n-            http_client=None,\n+            federation_http_client=None,\n             resource_for_client=self.mock_resource,\n             federation=Mock(),\n             federation_client=Mock(),", "file_path": "files/2021_2/47", "file_language": "py", "file_name": "tests/rest/client/v1/test_profile.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class MockHandlerProfileTestCase(unittest.TestCase):\n    \"\"\" Tests rest layer of profile management.\n\n    Todo: move these into ProfileTestCase\n    \"\"\"\n\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_resource = MockHttpResource(prefix=PATH_PREFIX)\n        self.mock_handler = Mock(\n            spec=[\n                \"get_displayname\",\n                \"set_displayname\",\n                \"get_avatar_url\",\n                \"set_avatar_url\",\n                \"check_profile_query_allowed\",\n            ]\n        )\n\n        self.mock_handler.get_displayname.return_value = defer.succeed(Mock())\n        self.mock_handler.set_displayname.return_value = defer.succeed(Mock())\n        self.mock_handler.get_avatar_url.return_value = defer.succeed(Mock())\n        self.mock_handler.set_avatar_url.return_value = defer.succeed(Mock())\n        self.mock_handler.check_profile_query_allowed.return_value = defer.succeed(\n            Mock()\n        )\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup,\n            \"test\",\n            http_client=None,\n            resource_for_client=self.mock_resource,\n            federation=Mock(),\n            federation_client=Mock(),\n            profile_handler=self.mock_handler,\n        )\n\n        async def _get_user_by_req(request=None, allow_guest=False):\n            return synapse.types.create_requester(myid)\n\n        hs.get_auth().get_user_by_req = _get_user_by_req\n\n        profile.register_servlets(hs, self.mock_resource)\n\n    @defer.inlineCallbacks\n    def test_get_my_name(self):\n        mocked_get = self.mock_handler.get_displayname\n        mocked_get.return_value = defer.succeed(\"Frank\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/displayname\" % (myid), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"displayname\": \"Frank\"}, response)\n        self.assertEquals(mocked_get.call_args[0][0].localpart, \"1234ABCD\")\n\n    @defer.inlineCallbacks\n    def test_set_my_name(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.return_value = defer.succeed(())\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\", \"/profile/%s/displayname\" % (myid), b'{\"displayname\": \"Frank Jr.\"}'\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals(mocked_set.call_args[0][0].localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][1].user.localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][2], \"Frank Jr.\")\n\n    @defer.inlineCallbacks\n    def test_set_my_name_noauth(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.side_effect = AuthError(400, \"message\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (\"@4567:test\"),\n            b'{\"displayname\": \"Frank Jr.\"}',\n        )\n\n        self.assertTrue(400 <= code < 499, msg=\"code %d is in the 4xx range\" % (code))\n\n    @defer.inlineCallbacks\n    def test_get_other_name(self):\n        mocked_get = self.mock_handler.get_displayname\n        mocked_get.return_value = defer.succeed(\"Bob\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/displayname\" % (\"@opaque:elsewhere\"), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"displayname\": \"Bob\"}, response)\n\n    @defer.inlineCallbacks\n    def test_set_other_name(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.side_effect = SynapseError(400, \"message\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (\"@opaque:elsewhere\"),\n            b'{\"displayname\":\"bob\"}',\n        )\n\n        self.assertTrue(400 <= code <= 499, msg=\"code %d is in the 4xx range\" % (code))\n\n    @defer.inlineCallbacks\n    def test_get_my_avatar(self):\n        mocked_get = self.mock_handler.get_avatar_url\n        mocked_get.return_value = defer.succeed(\"http://my.server/me.png\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/avatar_url\" % (myid), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"avatar_url\": \"http://my.server/me.png\"}, response)\n        self.assertEquals(mocked_get.call_args[0][0].localpart, \"1234ABCD\")\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar(self):\n        mocked_set = self.mock_handler.set_avatar_url\n        mocked_set.return_value = defer.succeed(())\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/avatar_url\" % (myid),\n            b'{\"avatar_url\": \"http://my.server/pic.gif\"}',\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals(mocked_set.call_args[0][0].localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][1].user.localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][2], \"http://my.server/pic.gif\")", "target": 0}, {"function": "class ProfileTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        self.hs = self.setup_test_homeserver()\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        self.owner = self.register_user(\"owner\", \"pass\")\n        self.owner_tok = self.login(\"owner\", \"pass\")\n\n    def test_set_displayname(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (self.owner,),\n            content=json.dumps({\"displayname\": \"test\"}),\n            access_token=self.owner_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        res = self.get_displayname()\n        self.assertEqual(res, \"test\")\n\n    def test_set_displayname_too_long(self):\n        \"\"\"Attempts to set a stupid displayname should get a 400\"\"\"\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (self.owner,),\n            content=json.dumps({\"displayname\": \"test\" * 100}),\n            access_token=self.owner_tok,\n        )\n        self.assertEqual(channel.code, 400, channel.result)\n\n        res = self.get_displayname()\n        self.assertEqual(res, \"owner\")\n\n    def get_displayname(self):\n        request, channel = self.make_request(\n            \"GET\", \"/profile/%s/displayname\" % (self.owner,)\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n        return channel.json_body[\"displayname\"]", "target": 0}, {"function": "class ProfilesRestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n\n        config = self.default_config()\n        config[\"require_auth_for_profile_requests\"] = True\n        config[\"limit_profile_requests_to_users_who_share_rooms\"] = True\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        # User owning the requested profile.\n        self.owner = self.register_user(\"owner\", \"pass\")\n        self.owner_tok = self.login(\"owner\", \"pass\")\n        self.profile_url = \"/profile/%s\" % (self.owner)\n\n        # User requesting the profile.\n        self.requester = self.register_user(\"requester\", \"pass\")\n        self.requester_tok = self.login(\"requester\", \"pass\")\n\n        self.room_id = self.helper.create_room_as(self.owner, tok=self.owner_tok)\n\n    def test_no_auth(self):\n        self.try_fetch_profile(401)\n\n    def test_not_in_shared_room(self):\n        self.ensure_requester_left_room()\n\n        self.try_fetch_profile(403, access_token=self.requester_tok)\n\n    def test_in_shared_room(self):\n        self.ensure_requester_left_room()\n\n        self.helper.join(room=self.room_id, user=self.requester, tok=self.requester_tok)\n\n        self.try_fetch_profile(200, self.requester_tok)\n\n    def try_fetch_profile(self, expected_code, access_token=None):\n        self.request_profile(expected_code, access_token=access_token)\n\n        self.request_profile(\n            expected_code, url_suffix=\"/displayname\", access_token=access_token\n        )\n\n        self.request_profile(\n            expected_code, url_suffix=\"/avatar_url\", access_token=access_token\n        )\n\n    def request_profile(self, expected_code, url_suffix=\"\", access_token=None):\n        request, channel = self.make_request(\n            \"GET\", self.profile_url + url_suffix, access_token=access_token\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n\n    def ensure_requester_left_room(self):\n        try:\n            self.helper.leave(\n                room=self.room_id, user=self.requester, tok=self.requester_tok\n            )\n        except AssertionError:\n            # We don't care whether the leave request didn't return a 200 (e.g.\n            # if the user isn't already in the room), because we only want to\n            # make sure the user isn't in the room.\n            pass", "target": 0}, {"function": "class OwnProfileUnrestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"require_auth_for_profile_requests\"] = True\n        config[\"limit_profile_requests_to_users_who_share_rooms\"] = True\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        # User requesting the profile.\n        self.requester = self.register_user(\"requester\", \"pass\")\n        self.requester_tok = self.login(\"requester\", \"pass\")\n\n    def test_can_lookup_own_profile(self):\n        \"\"\"Tests that a user can lookup their own profile without having to be in a room\n        if 'require_auth_for_profile_requests' is set to true in the server's config.\n        \"\"\"\n        request, channel = self.make_request(\n            \"GET\", \"/profile/\" + self.requester, access_token=self.requester_tok\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/profile/\" + self.requester + \"/displayname\",\n            access_token=self.requester_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/profile/\" + self.requester + \"/avatar_url\",\n            access_token=self.requester_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)", "target": 0}], "function_after": [{"function": "class MockHandlerProfileTestCase(unittest.TestCase):\n    \"\"\" Tests rest layer of profile management.\n\n    Todo: move these into ProfileTestCase\n    \"\"\"\n\n    @defer.inlineCallbacks\n    def setUp(self):\n        self.mock_resource = MockHttpResource(prefix=PATH_PREFIX)\n        self.mock_handler = Mock(\n            spec=[\n                \"get_displayname\",\n                \"set_displayname\",\n                \"get_avatar_url\",\n                \"set_avatar_url\",\n                \"check_profile_query_allowed\",\n            ]\n        )\n\n        self.mock_handler.get_displayname.return_value = defer.succeed(Mock())\n        self.mock_handler.set_displayname.return_value = defer.succeed(Mock())\n        self.mock_handler.get_avatar_url.return_value = defer.succeed(Mock())\n        self.mock_handler.set_avatar_url.return_value = defer.succeed(Mock())\n        self.mock_handler.check_profile_query_allowed.return_value = defer.succeed(\n            Mock()\n        )\n\n        hs = yield setup_test_homeserver(\n            self.addCleanup,\n            \"test\",\n            federation_http_client=None,\n            resource_for_client=self.mock_resource,\n            federation=Mock(),\n            federation_client=Mock(),\n            profile_handler=self.mock_handler,\n        )\n\n        async def _get_user_by_req(request=None, allow_guest=False):\n            return synapse.types.create_requester(myid)\n\n        hs.get_auth().get_user_by_req = _get_user_by_req\n\n        profile.register_servlets(hs, self.mock_resource)\n\n    @defer.inlineCallbacks\n    def test_get_my_name(self):\n        mocked_get = self.mock_handler.get_displayname\n        mocked_get.return_value = defer.succeed(\"Frank\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/displayname\" % (myid), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"displayname\": \"Frank\"}, response)\n        self.assertEquals(mocked_get.call_args[0][0].localpart, \"1234ABCD\")\n\n    @defer.inlineCallbacks\n    def test_set_my_name(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.return_value = defer.succeed(())\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\", \"/profile/%s/displayname\" % (myid), b'{\"displayname\": \"Frank Jr.\"}'\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals(mocked_set.call_args[0][0].localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][1].user.localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][2], \"Frank Jr.\")\n\n    @defer.inlineCallbacks\n    def test_set_my_name_noauth(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.side_effect = AuthError(400, \"message\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (\"@4567:test\"),\n            b'{\"displayname\": \"Frank Jr.\"}',\n        )\n\n        self.assertTrue(400 <= code < 499, msg=\"code %d is in the 4xx range\" % (code))\n\n    @defer.inlineCallbacks\n    def test_get_other_name(self):\n        mocked_get = self.mock_handler.get_displayname\n        mocked_get.return_value = defer.succeed(\"Bob\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/displayname\" % (\"@opaque:elsewhere\"), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"displayname\": \"Bob\"}, response)\n\n    @defer.inlineCallbacks\n    def test_set_other_name(self):\n        mocked_set = self.mock_handler.set_displayname\n        mocked_set.side_effect = SynapseError(400, \"message\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (\"@opaque:elsewhere\"),\n            b'{\"displayname\":\"bob\"}',\n        )\n\n        self.assertTrue(400 <= code <= 499, msg=\"code %d is in the 4xx range\" % (code))\n\n    @defer.inlineCallbacks\n    def test_get_my_avatar(self):\n        mocked_get = self.mock_handler.get_avatar_url\n        mocked_get.return_value = defer.succeed(\"http://my.server/me.png\")\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"GET\", \"/profile/%s/avatar_url\" % (myid), None\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals({\"avatar_url\": \"http://my.server/me.png\"}, response)\n        self.assertEquals(mocked_get.call_args[0][0].localpart, \"1234ABCD\")\n\n    @defer.inlineCallbacks\n    def test_set_my_avatar(self):\n        mocked_set = self.mock_handler.set_avatar_url\n        mocked_set.return_value = defer.succeed(())\n\n        (code, response) = yield self.mock_resource.trigger(\n            \"PUT\",\n            \"/profile/%s/avatar_url\" % (myid),\n            b'{\"avatar_url\": \"http://my.server/pic.gif\"}',\n        )\n\n        self.assertEquals(200, code)\n        self.assertEquals(mocked_set.call_args[0][0].localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][1].user.localpart, \"1234ABCD\")\n        self.assertEquals(mocked_set.call_args[0][2], \"http://my.server/pic.gif\")", "target": 0}, {"function": "class ProfileTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        self.hs = self.setup_test_homeserver()\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        self.owner = self.register_user(\"owner\", \"pass\")\n        self.owner_tok = self.login(\"owner\", \"pass\")\n\n    def test_set_displayname(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (self.owner,),\n            content=json.dumps({\"displayname\": \"test\"}),\n            access_token=self.owner_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        res = self.get_displayname()\n        self.assertEqual(res, \"test\")\n\n    def test_set_displayname_too_long(self):\n        \"\"\"Attempts to set a stupid displayname should get a 400\"\"\"\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/profile/%s/displayname\" % (self.owner,),\n            content=json.dumps({\"displayname\": \"test\" * 100}),\n            access_token=self.owner_tok,\n        )\n        self.assertEqual(channel.code, 400, channel.result)\n\n        res = self.get_displayname()\n        self.assertEqual(res, \"owner\")\n\n    def get_displayname(self):\n        request, channel = self.make_request(\n            \"GET\", \"/profile/%s/displayname\" % (self.owner,)\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n        return channel.json_body[\"displayname\"]", "target": 0}, {"function": "class ProfilesRestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n\n        config = self.default_config()\n        config[\"require_auth_for_profile_requests\"] = True\n        config[\"limit_profile_requests_to_users_who_share_rooms\"] = True\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        # User owning the requested profile.\n        self.owner = self.register_user(\"owner\", \"pass\")\n        self.owner_tok = self.login(\"owner\", \"pass\")\n        self.profile_url = \"/profile/%s\" % (self.owner)\n\n        # User requesting the profile.\n        self.requester = self.register_user(\"requester\", \"pass\")\n        self.requester_tok = self.login(\"requester\", \"pass\")\n\n        self.room_id = self.helper.create_room_as(self.owner, tok=self.owner_tok)\n\n    def test_no_auth(self):\n        self.try_fetch_profile(401)\n\n    def test_not_in_shared_room(self):\n        self.ensure_requester_left_room()\n\n        self.try_fetch_profile(403, access_token=self.requester_tok)\n\n    def test_in_shared_room(self):\n        self.ensure_requester_left_room()\n\n        self.helper.join(room=self.room_id, user=self.requester, tok=self.requester_tok)\n\n        self.try_fetch_profile(200, self.requester_tok)\n\n    def try_fetch_profile(self, expected_code, access_token=None):\n        self.request_profile(expected_code, access_token=access_token)\n\n        self.request_profile(\n            expected_code, url_suffix=\"/displayname\", access_token=access_token\n        )\n\n        self.request_profile(\n            expected_code, url_suffix=\"/avatar_url\", access_token=access_token\n        )\n\n    def request_profile(self, expected_code, url_suffix=\"\", access_token=None):\n        request, channel = self.make_request(\n            \"GET\", self.profile_url + url_suffix, access_token=access_token\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n\n    def ensure_requester_left_room(self):\n        try:\n            self.helper.leave(\n                room=self.room_id, user=self.requester, tok=self.requester_tok\n            )\n        except AssertionError:\n            # We don't care whether the leave request didn't return a 200 (e.g.\n            # if the user isn't already in the room), because we only want to\n            # make sure the user isn't in the room.\n            pass", "target": 0}, {"function": "class OwnProfileUnrestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        admin.register_servlets_for_client_rest_resource,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"require_auth_for_profile_requests\"] = True\n        config[\"limit_profile_requests_to_users_who_share_rooms\"] = True\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, hs):\n        # User requesting the profile.\n        self.requester = self.register_user(\"requester\", \"pass\")\n        self.requester_tok = self.login(\"requester\", \"pass\")\n\n    def test_can_lookup_own_profile(self):\n        \"\"\"Tests that a user can lookup their own profile without having to be in a room\n        if 'require_auth_for_profile_requests' is set to true in the server's config.\n        \"\"\"\n        request, channel = self.make_request(\n            \"GET\", \"/profile/\" + self.requester, access_token=self.requester_tok\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/profile/\" + self.requester + \"/displayname\",\n            access_token=self.requester_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/profile/\" + self.requester + \"/avatar_url\",\n            access_token=self.requester_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Frest%2Fclient%2Fv1%2Ftest_rooms.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2017 Vector Creations Ltd\n# Copyright 2018-2019 New Vector Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests REST events for /rooms paths.\"\"\"\n\nimport json\nfrom urllib import parse as urlparse\n\nfrom mock import Mock\n\nimport synapse.rest.admin\nfrom synapse.api.constants import EventContentFields, EventTypes, Membership\nfrom synapse.handlers.pagination import PurgeStatus\nfrom synapse.rest.client.v1 import directory, login, profile, room\nfrom synapse.rest.client.v2_alpha import account\nfrom synapse.types import JsonDict, RoomAlias, UserID\nfrom synapse.util.stringutils import random_string\n\nfrom tests import unittest\nfrom tests.test_utils import make_awaitable\n\nPATH_PREFIX = b\"/_matrix/client/api/v1\"\n\n\nclass RoomBase(unittest.HomeserverTestCase):\n    rmcreator_id = None\n\n    servlets = [room.register_servlets, room.register_deprecated_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        self.hs = self.setup_test_homeserver(\n            \"red\", federation_http_client=None, federation_client=Mock(),\n        )\n\n        self.hs.get_federation_handler = Mock()\n        self.hs.get_federation_handler.return_value.maybe_backfill = Mock(\n            return_value=make_awaitable(None)\n        )\n\n        async def _insert_client_ip(*args, **kwargs):\n            return None\n\n        self.hs.get_datastore().insert_client_ip = _insert_client_ip\n\n        return self.hs\n\n\nclass RoomPermissionsTestCase(RoomBase):\n    \"\"\" Tests room permissions. \"\"\"\n\n    user_id = \"@sid1:red\"\n    rmcreator_id = \"@notme:red\"\n\n    def prepare(self, reactor, clock, hs):\n\n        self.helper.auth_user_id = self.rmcreator_id\n        # create some rooms under the name rmcreator_id\n        self.uncreated_rmid = \"!aa:test\"\n        self.created_rmid = self.helper.create_room_as(\n            self.rmcreator_id, is_public=False\n        )\n        self.created_public_rmid = self.helper.create_room_as(\n            self.rmcreator_id, is_public=True\n        )\n\n        # send a message in one of the rooms\n        self.created_rmid_msg_path = (\n            \"rooms/%s/send/m.room.message/a1\" % (self.created_rmid)\n        ).encode(\"ascii\")\n        request, channel = self.make_request(\n            \"PUT\", self.created_rmid_msg_path, b'{\"msgtype\":\"m.text\",\"body\":\"test msg\"}'\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        # set topic for public room\n        request, channel = self.make_request(\n            \"PUT\",\n            (\"rooms/%s/state/m.room.topic\" % self.created_public_rmid).encode(\"ascii\"),\n            b'{\"topic\":\"Public Room Topic\"}',\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        # auth as user_id now\n        self.helper.auth_user_id = self.user_id\n\n    def test_can_do_action(self):\n        msg_content = b'{\"msgtype\":\"m.text\",\"body\":\"hello\"}'\n\n        seq = iter(range(100))\n\n        def send_msg_path():\n            return \"/rooms/%s/send/m.room.message/mid%s\" % (\n                self.created_rmid,\n                str(next(seq)),\n            )\n\n        # send message in uncreated room, expect 403\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/send/m.room.message/mid2\" % (self.uncreated_rmid,),\n            msg_content,\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room not joined (no state), expect 403\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and invited, expect 403\n        self.helper.invite(\n            room=self.created_rmid, src=self.rmcreator_id, targ=self.user_id\n        )\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and joined, expect 200\n        self.helper.join(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and left, expect 403\n        self.helper.leave(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_topic_perms(self):\n        topic_content = b'{\"topic\":\"My Topic Name\"}'\n        topic_path = \"/rooms/%s/state/m.room.topic\" % self.created_rmid\n\n        # set/get topic in uncreated room, expect 403\n        request, channel = self.make_request(\n            \"PUT\", \"/rooms/%s/state/m.room.topic\" % self.uncreated_rmid, topic_content\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/state/m.room.topic\" % self.uncreated_rmid\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set/get topic in created PRIVATE room not joined, expect 403\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set topic in created PRIVATE room and invited, expect 403\n        self.helper.invite(\n            room=self.created_rmid, src=self.rmcreator_id, targ=self.user_id\n        )\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # get topic in created PRIVATE room and invited, expect 403\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set/get topic in created PRIVATE room and joined, expect 200\n        self.helper.join(room=self.created_rmid, user=self.user_id)\n\n        # Only room ops can set topic by default\n        self.helper.auth_user_id = self.rmcreator_id\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.helper.auth_user_id = self.user_id\n\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(topic_content.decode(\"utf8\")), channel.json_body)\n\n        # set/get topic in created PRIVATE room and left, expect 403\n        self.helper.leave(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # get topic in PUBLIC room, not joined, expect 403\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/state/m.room.topic\" % self.created_public_rmid\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set topic in PUBLIC room, not joined, expect 403\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/state/m.room.topic\" % self.created_public_rmid,\n            topic_content,\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def _test_get_membership(self, room=None, members=[], expect_code=None):\n        for member in members:\n            path = \"/rooms/%s/state/m.room.member/%s\" % (room, member)\n            request, channel = self.make_request(\"GET\", path)\n            self.assertEquals(expect_code, channel.code)\n\n    def test_membership_basic_room_perms(self):\n        # === room does not exist ===\n        room = self.uncreated_rmid\n        # get membership of self, get membership of other, uncreated room\n        # expect all 403s\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # trying to invite people to this room should 403\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.rmcreator_id, expect_code=403\n        )\n\n        # set [invite/join/left] of self, set [invite/join/left] of other,\n        # expect all 404s because room doesn't exist on any server\n        for usr in [self.user_id, self.rmcreator_id]:\n            self.helper.join(room=room, user=usr, expect_code=404)\n            self.helper.leave(room=room, user=usr, expect_code=404)\n\n    def test_membership_private_room_perms(self):\n        room = self.created_rmid\n        # get membership of self, get membership of other, private room + invite\n        # expect all 403s\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # get membership of self, get membership of other, private room + joined\n        # expect all 200s\n        self.helper.join(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n        # get membership of self, get membership of other, private room + left\n        # expect all 200s\n        self.helper.leave(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n    def test_membership_public_room_perms(self):\n        room = self.created_public_rmid\n        # get membership of self, get membership of other, public room + invite\n        # expect 403\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # get membership of self, get membership of other, public room + joined\n        # expect all 200s\n        self.helper.join(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n        # get membership of self, get membership of other, public room + left\n        # expect 200.\n        self.helper.leave(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n    def test_invited_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n\n        # set [invite/join/left] of other user, expect 403s\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.rmcreator_id, expect_code=403\n        )\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.JOIN,\n            expect_code=403,\n        )\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )\n\n    def test_joined_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self.helper.join(room=room, user=self.user_id)\n\n        # set invited of self, expect 403\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.user_id, expect_code=403\n        )\n\n        # set joined of self, expect 200 (NOOP)\n        self.helper.join(room=room, user=self.user_id)\n\n        other = \"@burgundy:red\"\n        # set invited of other, expect 200\n        self.helper.invite(room=room, src=self.user_id, targ=other, expect_code=200)\n\n        # set joined of other, expect 403\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=other,\n            membership=Membership.JOIN,\n            expect_code=403,\n        )\n\n        # set left of other, expect 403\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=other,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )\n\n        # set left of self, expect 200\n        self.helper.leave(room=room, user=self.user_id)\n\n    def test_leave_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self.helper.join(room=room, user=self.user_id)\n        self.helper.leave(room=room, user=self.user_id)\n\n        # set [invite/join/left] of self, set [invite/join/left] of other,\n        # expect all 403s\n        for usr in [self.user_id, self.rmcreator_id]:\n            self.helper.change_membership(\n                room=room,\n                src=self.user_id,\n                targ=usr,\n                membership=Membership.INVITE,\n                expect_code=403,\n            )\n\n            self.helper.change_membership(\n                room=room,\n                src=self.user_id,\n                targ=usr,\n                membership=Membership.JOIN,\n                expect_code=403,\n            )\n\n        # It is always valid to LEAVE if you've already left (currently.)\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )\n\n\nclass RoomsMemberListTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/members/list REST events.\"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def test_get_member_list(self):\n        room_id = self.helper.create_room_as(self.user_id)\n        request, channel = self.make_request(\"GET\", \"/rooms/%s/members\" % room_id)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_no_room(self):\n        request, channel = self.make_request(\"GET\", \"/rooms/roomdoesnotexist/members\")\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_no_permission(self):\n        room_id = self.helper.create_room_as(\"@some_other_guy:red\")\n        request, channel = self.make_request(\"GET\", \"/rooms/%s/members\" % room_id)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_mixed_memberships(self):\n        room_creator = \"@some_other_guy:red\"\n        room_id = self.helper.create_room_as(room_creator)\n        room_path = \"/rooms/%s/members\" % room_id\n        self.helper.invite(room=room_id, src=room_creator, targ=self.user_id)\n        # can't see list if you're just invited.\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        self.helper.join(room=room_id, user=self.user_id)\n        # can see list now joined\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        self.helper.leave(room=room_id, user=self.user_id)\n        # can see old list once left\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n\nclass RoomsCreateTestCase(RoomBase):\n    \"\"\" Tests /rooms and /rooms/$room_id REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def test_post_room_no_keys(self):\n        # POST with no config keys, expect new room id\n        request, channel = self.make_request(\"POST\", \"/createRoom\", \"{}\")\n\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_visibility_key(self):\n        # POST with visibility config key, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"visibility\":\"private\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_custom_key(self):\n        # POST with custom config keys, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"custom\":\"stuff\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_known_and_unknown_keys(self):\n        # POST with custom + known config keys, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"visibility\":\"private\",\"custom\":\"things\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_invalid_content(self):\n        # POST with invalid content / paths, expect 400\n        request, channel = self.make_request(\"POST\", \"/createRoom\", b'{\"visibili')\n        self.assertEquals(400, channel.code)\n\n        request, channel = self.make_request(\"POST\", \"/createRoom\", b'[\"hello\"]')\n        self.assertEquals(400, channel.code)\n\n    def test_post_room_invitees_invalid_mxid(self):\n        # POST with invalid invitee, see https://github.com/matrix-org/synapse/issues/4088\n        # Note the trailing space in the MXID here!\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"invite\":[\"@alice:example.com \"]}'\n        )\n        self.assertEquals(400, channel.code)\n\n\nclass RoomTopicTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/topic REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        # create the room\n        self.room_id = self.helper.create_room_as(self.user_id)\n        self.path = \"/rooms/%s/state/m.room.topic\" % (self.room_id,)\n\n    def test_invalid_puts(self):\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", self.path, \"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, '{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, '{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", self.path, '[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, \"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, \"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # valid key, wrong type\n        content = '{\"topic\":[\"Topic name\"]}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_topic(self):\n        # nothing should be there\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(404, channel.code, msg=channel.result[\"body\"])\n\n        # valid put\n        content = '{\"topic\":\"Topic name\"}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # valid get\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(content), channel.json_body)\n\n    def test_rooms_topic_with_extra_keys(self):\n        # valid put with extra keys\n        content = '{\"topic\":\"Seasons\",\"subtopic\":\"Summer\"}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # valid get\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(content), channel.json_body)\n\n\nclass RoomMemberStateTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/members/$user_id/state REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_invalid_puts(self):\n        path = \"/rooms/%s/state/m.room.member/%s\" % (self.room_id, self.user_id)\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", path, \"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, '{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, '{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", path, b'[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, \"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, \"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # valid keys, wrong types\n        content = '{\"membership\":[\"%s\",\"%s\",\"%s\"]}' % (\n            Membership.INVITE,\n            Membership.JOIN,\n            Membership.LEAVE,\n        )\n        request, channel = self.make_request(\"PUT\", path, content.encode(\"ascii\"))\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_members_self(self):\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.user_id,\n        )\n\n        # valid join message (NOOP since we made the room)\n        content = '{\"membership\":\"%s\"}' % Membership.JOIN\n        request, channel = self.make_request(\"PUT\", path, content.encode(\"ascii\"))\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        expected_response = {\"membership\": Membership.JOIN}\n        self.assertEquals(expected_response, channel.json_body)\n\n    def test_rooms_members_other(self):\n        self.other_id = \"@zzsid1:red\"\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.other_id,\n        )\n\n        # valid invite message\n        content = '{\"membership\":\"%s\"}' % Membership.INVITE\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assertEquals(json.loads(content), channel.json_body)\n\n    def test_rooms_members_other_custom_keys(self):\n        self.other_id = \"@zzsid1:red\"\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.other_id,\n        )\n\n        # valid invite message with custom key\n        content = '{\"membership\":\"%s\",\"invite_text\":\"%s\"}' % (\n            Membership.INVITE,\n            \"Join us!\",\n        )\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assertEquals(json.loads(content), channel.json_body)\n\n\nclass RoomJoinRatelimitTestCase(RoomBase):\n    user_id = \"@sid1:red\"\n\n    servlets = [\n        profile.register_servlets,\n        room.register_servlets,\n    ]\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit(self):\n        \"\"\"Tests that local joins are actually rate-limited.\"\"\"\n        for i in range(3):\n            self.helper.create_room_as(self.user_id)\n\n        self.helper.create_room_as(self.user_id, expect_code=429)\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit_profile_change(self):\n        \"\"\"Tests that sending a profile update into all of the user's joined rooms isn't\n        rate-limited by the rate-limiter on joins.\"\"\"\n\n        # Create and join as many rooms as the rate-limiting config allows in a second.\n        room_ids = [\n            self.helper.create_room_as(self.user_id),\n            self.helper.create_room_as(self.user_id),\n            self.helper.create_room_as(self.user_id),\n        ]\n        # Let some time for the rate-limiter to forget about our multi-join.\n        self.reactor.advance(2)\n        # Add one to make sure we're joined to more rooms than the config allows us to\n        # join in a second.\n        room_ids.append(self.helper.create_room_as(self.user_id))\n\n        # Create a profile for the user, since it hasn't been done on registration.\n        store = self.hs.get_datastore()\n        self.get_success(\n            store.create_profile(UserID.from_string(self.user_id).localpart)\n        )\n\n        # Update the display name for the user.\n        path = \"/_matrix/client/r0/profile/%s/displayname\" % self.user_id\n        request, channel = self.make_request(\"PUT\", path, {\"displayname\": \"John Doe\"})\n        self.assertEquals(channel.code, 200, channel.json_body)\n\n        # Check that all the rooms have been sent a profile update into.\n        for room_id in room_ids:\n            path = \"/_matrix/client/r0/rooms/%s/state/m.room.member/%s\" % (\n                room_id,\n                self.user_id,\n            )\n\n            request, channel = self.make_request(\"GET\", path)\n            self.assertEquals(channel.code, 200)\n\n            self.assertIn(\"displayname\", channel.json_body)\n            self.assertEquals(channel.json_body[\"displayname\"], \"John Doe\")\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit_idempotent(self):\n        \"\"\"Tests that the room join endpoints remain idempotent despite rate-limiting\n        on room joins.\"\"\"\n        room_id = self.helper.create_room_as(self.user_id)\n\n        # Let's test both paths to be sure.\n        paths_to_test = [\n            \"/_matrix/client/r0/rooms/%s/join\",\n            \"/_matrix/client/r0/join/%s\",\n        ]\n\n        for path in paths_to_test:\n            # Make sure we send more requests than the rate-limiting config would allow\n            # if all of these requests ended up joining the user to a room.\n            for i in range(4):\n                request, channel = self.make_request(\"POST\", path % room_id, {})\n                self.assertEquals(channel.code, 200)\n\n\nclass RoomMessagesTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/messages/$user_id/$msg_id REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_invalid_puts(self):\n        path = \"/rooms/%s/send/m.room.message/mid1\" % (urlparse.quote(self.room_id))\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", path, b\"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b'{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b'{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", path, b'[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b\"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b\"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_messages_sent(self):\n        path = \"/rooms/%s/send/m.room.message/mid1\" % (urlparse.quote(self.room_id))\n\n        content = b'{\"body\":\"test\",\"msgtype\":{\"type\":\"a\"}}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # custom message types\n        content = b'{\"body\":\"test\",\"msgtype\":\"test.custom.text\"}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # m.text message type\n        path = \"/rooms/%s/send/m.room.message/mid2\" % (urlparse.quote(self.room_id))\n        content = b'{\"body\":\"test2\",\"msgtype\":\"m.text\"}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n\nclass RoomInitialSyncTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/initialSync. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        # create the room\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_initial_sync(self):\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/initialSync\" % self.room_id\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.room_id, channel.json_body[\"room_id\"])\n        self.assertEquals(\"join\", channel.json_body[\"membership\"])\n\n        # Room state is easier to assert on if we unpack it into a dict\n        state = {}\n        for event in channel.json_body[\"state\"]:\n            if \"state_key\" not in event:\n                continue\n            t = event[\"type\"]\n            if t not in state:\n                state[t] = []\n            state[t].append(event)\n\n        self.assertTrue(\"m.room.create\" in state)\n\n        self.assertTrue(\"messages\" in channel.json_body)\n        self.assertTrue(\"chunk\" in channel.json_body[\"messages\"])\n        self.assertTrue(\"end\" in channel.json_body[\"messages\"])\n\n        self.assertTrue(\"presence\" in channel.json_body)\n\n        presence_by_user = {\n            e[\"content\"][\"user_id\"]: e for e in channel.json_body[\"presence\"]\n        }\n        self.assertTrue(self.user_id in presence_by_user)\n        self.assertEquals(\"m.presence\", presence_by_user[self.user_id][\"type\"])\n\n\nclass RoomMessageListTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/messages REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_topo_token_is_accepted(self):\n        token = \"t1-0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/messages?access_token=x&from=%s\" % (self.room_id, token)\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"start\" in channel.json_body)\n        self.assertEquals(token, channel.json_body[\"start\"])\n        self.assertTrue(\"chunk\" in channel.json_body)\n        self.assertTrue(\"end\" in channel.json_body)\n\n    def test_stream_token_is_accepted_for_fwd_pagianation(self):\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/messages?access_token=x&from=%s\" % (self.room_id, token)\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"start\" in channel.json_body)\n        self.assertEquals(token, channel.json_body[\"start\"])\n        self.assertTrue(\"chunk\" in channel.json_body)\n        self.assertTrue(\"end\" in channel.json_body)\n\n    def test_room_messages_purge(self):\n        store = self.hs.get_datastore()\n        pagination_handler = self.hs.get_pagination_handler()\n\n        # Send a first message in the room, which will be removed by the purge.\n        first_event_id = self.helper.send(self.room_id, \"message 1\")[\"event_id\"]\n        first_token = self.get_success(\n            store.get_topological_token_for_event(first_event_id)\n        )\n        first_token_str = self.get_success(first_token.to_string(store))\n\n        # Send a second message in the room, which won't be removed, and which we'll\n        # use as the marker to purge events before.\n        second_event_id = self.helper.send(self.room_id, \"message 2\")[\"event_id\"]\n        second_token = self.get_success(\n            store.get_topological_token_for_event(second_event_id)\n        )\n        second_token_str = self.get_success(second_token.to_string(store))\n\n        # Send a third event in the room to ensure we don't fall under any edge case\n        # due to our marker being the latest forward extremity in the room.\n        self.helper.send(self.room_id, \"message 3\")\n\n        # Check that we get the first and second message when querying /messages.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                second_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 2, [event[\"content\"] for event in chunk])\n\n        # Purge every event before the second event.\n        purge_id = random_string(16)\n        pagination_handler._purges_by_id[purge_id] = PurgeStatus()\n        self.get_success(\n            pagination_handler._purge_history(\n                purge_id=purge_id,\n                room_id=self.room_id,\n                token=second_token_str,\n                delete_local_events=True,\n            )\n        )\n\n        # Check that we only get the second message through /message now that the first\n        # has been purged.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                second_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 1, [event[\"content\"] for event in chunk])\n\n        # Check that we get no event, but also no error, when querying /messages with\n        # the token that was pointing at the first event, because we don't have it\n        # anymore.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                first_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 0, [event[\"content\"] for event in chunk])\n\n\nclass RoomSearchTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n    user_id = True\n    hijack_auth = False\n\n    def prepare(self, reactor, clock, hs):\n\n        # Register the user who does the searching\n        self.user_id = self.register_user(\"user\", \"pass\")\n        self.access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        self.other_user_id = self.register_user(\"otheruser\", \"pass\")\n        self.other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Create a room\n        self.room = self.helper.create_room_as(self.user_id, tok=self.access_token)\n\n        # Invite the other person\n        self.helper.invite(\n            room=self.room,\n            src=self.user_id,\n            tok=self.access_token,\n            targ=self.other_user_id,\n        )\n\n        # The other user joins\n        self.helper.join(\n            room=self.room, user=self.other_user_id, tok=self.other_access_token\n        )\n\n    def test_finds_message(self):\n        \"\"\"\n        The search functionality will search for content in messages if asked to\n        do so.\n        \"\"\"\n        # The other user sends some messages\n        self.helper.send(self.room, body=\"Hi!\", tok=self.other_access_token)\n        self.helper.send(self.room, body=\"There!\", tok=self.other_access_token)\n\n        request, channel = self.make_request(\n            \"POST\",\n            \"/search?access_token=%s\" % (self.access_token,),\n            {\n                \"search_categories\": {\n                    \"room_events\": {\"keys\": [\"content.body\"], \"search_term\": \"Hi\"}\n                }\n            },\n        )\n\n        # Check we get the results we expect -- one search result, of the sent\n        # messages\n        self.assertEqual(channel.code, 200)\n        results = channel.json_body[\"search_categories\"][\"room_events\"]\n        self.assertEqual(results[\"count\"], 1)\n        self.assertEqual(results[\"results\"][0][\"result\"][\"content\"][\"body\"], \"Hi!\")\n\n        # No context was requested, so we should get none.\n        self.assertEqual(results[\"results\"][0][\"context\"], {})\n\n    def test_include_context(self):\n        \"\"\"\n        When event_context includes include_profile, profile information will be\n        included in the search response.\n        \"\"\"\n        # The other user sends some messages\n        self.helper.send(self.room, body=\"Hi!\", tok=self.other_access_token)\n        self.helper.send(self.room, body=\"There!\", tok=self.other_access_token)\n\n        request, channel = self.make_request(\n            \"POST\",\n            \"/search?access_token=%s\" % (self.access_token,),\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"keys\": [\"content.body\"],\n                        \"search_term\": \"Hi\",\n                        \"event_context\": {\"include_profile\": True},\n                    }\n                }\n            },\n        )\n\n        # Check we get the results we expect -- one search result, of the sent\n        # messages\n        self.assertEqual(channel.code, 200)\n        results = channel.json_body[\"search_categories\"][\"room_events\"]\n        self.assertEqual(results[\"count\"], 1)\n        self.assertEqual(results[\"results\"][0][\"result\"][\"content\"][\"body\"], \"Hi!\")\n\n        # We should get context info, like the two users, and the display names.\n        context = results[\"results\"][0][\"context\"]\n        self.assertEqual(len(context[\"profile_info\"].keys()), 2)\n        self.assertEqual(\n            context[\"profile_info\"][self.other_user_id][\"displayname\"], \"otheruser\"\n        )\n\n\nclass PublicRoomsRestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n\n        self.url = b\"/_matrix/client/r0/publicRooms\"\n\n        config = self.default_config()\n        config[\"allow_public_rooms_without_auth\"] = False\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def test_restricted_no_auth(self):\n        request, channel = self.make_request(\"GET\", self.url)\n        self.assertEqual(channel.code, 401, channel.result)\n\n    def test_restricted_auth(self):\n        self.register_user(\"user\", \"pass\")\n        tok = self.login(\"user\", \"pass\")\n\n        request, channel = self.make_request(\"GET\", self.url, access_token=tok)\n        self.assertEqual(channel.code, 200, channel.result)\n\n\nclass PerRoomProfilesForbiddenTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"allow_per_room_profiles\"] = False\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"test\", \"test\")\n        self.tok = self.login(\"test\", \"test\")\n\n        # Set a profile for the test user\n        self.displayname = \"test user\"\n        data = {\"displayname\": self.displayname}\n        request_data = json.dumps(data)\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/_matrix/client/r0/profile/%s/displayname\" % (self.user_id,),\n            request_data,\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self.room_id = self.helper.create_room_as(self.user_id, tok=self.tok)\n\n    def test_per_room_profile_forbidden(self):\n        data = {\"membership\": \"join\", \"displayname\": \"other test user\"}\n        request_data = json.dumps(data)\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/_matrix/client/r0/rooms/%s/state/m.room.member/%s\"\n            % (self.room_id, self.user_id),\n            request_data,\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n        event_id = channel.json_body[\"event_id\"]\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/r0/rooms/%s/event/%s\" % (self.room_id, event_id),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        res_displayname = channel.json_body[\"content\"][\"displayname\"]\n        self.assertEqual(res_displayname, self.displayname, channel.result)\n\n\nclass RoomMembershipReasonTestCase(unittest.HomeserverTestCase):\n    \"\"\"Tests that clients can add a \"reason\" field to membership events and\n    that they get correctly added to the generated events and propagated.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.creator = self.register_user(\"creator\", \"test\")\n        self.creator_tok = self.login(\"creator\", \"test\")\n\n        self.second_user_id = self.register_user(\"second\", \"test\")\n        self.second_tok = self.login(\"second\", \"test\")\n\n        self.room_id = self.helper.create_room_as(self.creator, tok=self.creator_tok)\n\n    def test_join_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/join\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_leave_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/leave\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_kick_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/kick\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_ban_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/ban\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_unban_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/unban\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_invite_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/invite\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_reject_invite_reason(self):\n        self.helper.invite(\n            self.room_id,\n            src=self.creator,\n            targ=self.second_user_id,\n            tok=self.creator_tok,\n        )\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/leave\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def _check_for_reason(self, reason):\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/r0/rooms/{}/state/m.room.member/{}\".format(\n                self.room_id, self.second_user_id\n            ),\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        event_content = channel.json_body\n\n        self.assertEqual(event_content.get(\"reason\"), reason, channel.result)\n\n\nclass LabelsTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    # Filter that should only catch messages with the label \"#fun\".\n    FILTER_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.labels\": [\"#fun\"],\n    }\n    # Filter that should only catch messages without the label \"#fun\".\n    FILTER_NOT_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.not_labels\": [\"#fun\"],\n    }\n    # Filter that should only catch messages with the label \"#work\" but without the label\n    # \"#notfun\".\n    FILTER_LABELS_NOT_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.labels\": [\"#work\"],\n        \"org.matrix.not_labels\": [\"#notfun\"],\n    }\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"test\", \"test\")\n        self.tok = self.login(\"test\", \"test\")\n        self.room_id = self.helper.create_room_as(self.user_id, tok=self.tok)\n\n    def test_context_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /context request.\"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 1, [event[\"content\"] for event in events_before]\n        )\n        self.assertEqual(\n            events_before[0][\"content\"][\"body\"], \"with right label\", events_before[0]\n        )\n\n        events_after = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_after), 1, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with right label\", events_after[0]\n        )\n\n    def test_context_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /context request.\"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_NOT_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 1, [event[\"content\"] for event in events_before]\n        )\n        self.assertEqual(\n            events_before[0][\"content\"][\"body\"], \"without label\", events_before[0]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(\n            len(events_after), 2, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with wrong label\", events_after[0]\n        )\n        self.assertEqual(\n            events_after[1][\"content\"][\"body\"], \"with two wrong labels\", events_after[1]\n        )\n\n    def test_context_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /context request.\n        \"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_LABELS_NOT_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 0, [event[\"content\"] for event in events_before]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(\n            len(events_after), 1, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with wrong label\", events_after[0]\n        )\n\n    def test_messages_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /messages request.\"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (self.room_id, self.tok, token, json.dumps(self.FILTER_LABELS)),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 2, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"with right label\", events[0])\n        self.assertEqual(events[1][\"content\"][\"body\"], \"with right label\", events[1])\n\n    def test_messages_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /messages request.\"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (self.room_id, self.tok, token, json.dumps(self.FILTER_NOT_LABELS)),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 4, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"without label\", events[0])\n        self.assertEqual(events[1][\"content\"][\"body\"], \"without label\", events[1])\n        self.assertEqual(events[2][\"content\"][\"body\"], \"with wrong label\", events[2])\n        self.assertEqual(\n            events[3][\"content\"][\"body\"], \"with two wrong labels\", events[3]\n        )\n\n    def test_messages_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /messages request.\n        \"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (\n                self.room_id,\n                self.tok,\n                token,\n                json.dumps(self.FILTER_LABELS_NOT_LABELS),\n            ),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 1, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"with wrong label\", events[0])\n\n    def test_search_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /search request.\"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 2, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"with right label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[1][\"result\"][\"content\"][\"body\"],\n            \"with right label\",\n            results[1][\"result\"][\"content\"][\"body\"],\n        )\n\n    def test_search_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /search request.\"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_NOT_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 4, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"without label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[1][\"result\"][\"content\"][\"body\"],\n            \"without label\",\n            results[1][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[2][\"result\"][\"content\"][\"body\"],\n            \"with wrong label\",\n            results[2][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[3][\"result\"][\"content\"][\"body\"],\n            \"with two wrong labels\",\n            results[3][\"result\"][\"content\"][\"body\"],\n        )\n\n    def test_search_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /search request.\n        \"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_LABELS_NOT_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 1, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"with wrong label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n\n    def _send_labelled_messages_in_room(self):\n        \"\"\"Sends several messages to a room with different labels (or without any) to test\n        filtering by label.\n        Returns:\n            The ID of the event to use if we're testing filtering on /context.\n        \"\"\"\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with right label\",\n                EventContentFields.LABELS: [\"#fun\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\"msgtype\": \"m.text\", \"body\": \"without label\"},\n            tok=self.tok,\n        )\n\n        res = self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\"msgtype\": \"m.text\", \"body\": \"without label\"},\n            tok=self.tok,\n        )\n        # Return this event's ID when we test filtering in /context requests.\n        event_id = res[\"event_id\"]\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with wrong label\",\n                EventContentFields.LABELS: [\"#work\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with two wrong labels\",\n                EventContentFields.LABELS: [\"#work\", \"#notfun\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with right label\",\n                EventContentFields.LABELS: [\"#fun\"],\n            },\n            tok=self.tok,\n        )\n\n        return event_id\n\n\nclass ContextTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        account.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"user\", \"password\")\n        self.tok = self.login(\"user\", \"password\")\n        self.room_id = self.helper.create_room_as(\n            self.user_id, tok=self.tok, is_public=False\n        )\n\n        self.other_user_id = self.register_user(\"user2\", \"password\")\n        self.other_tok = self.login(\"user2\", \"password\")\n\n        self.helper.invite(self.room_id, self.user_id, self.other_user_id, tok=self.tok)\n        self.helper.join(self.room_id, self.other_user_id, tok=self.other_tok)\n\n    def test_erased_sender(self):\n        \"\"\"Test that an erasure request results in the requester's events being hidden\n        from any new member of the room.\n        \"\"\"\n\n        # Send a bunch of events in the room.\n\n        self.helper.send(self.room_id, \"message 1\", tok=self.tok)\n        self.helper.send(self.room_id, \"message 2\", tok=self.tok)\n        event_id = self.helper.send(self.room_id, \"message 3\", tok=self.tok)[\"event_id\"]\n        self.helper.send(self.room_id, \"message 4\", tok=self.tok)\n        self.helper.send(self.room_id, \"message 5\", tok=self.tok)\n\n        # Check that we can still see the messages before the erasure request.\n\n        request, channel = self.make_request(\n            \"GET\",\n            '/rooms/%s/context/%s?filter={\"types\":[\"m.room.message\"]}'\n            % (self.room_id, event_id),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(len(events_before), 2, events_before)\n        self.assertEqual(\n            events_before[0].get(\"content\", {}).get(\"body\"),\n            \"message 2\",\n            events_before[0],\n        )\n        self.assertEqual(\n            events_before[1].get(\"content\", {}).get(\"body\"),\n            \"message 1\",\n            events_before[1],\n        )\n\n        self.assertEqual(\n            channel.json_body[\"event\"].get(\"content\", {}).get(\"body\"),\n            \"message 3\",\n            channel.json_body[\"event\"],\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(len(events_after), 2, events_after)\n        self.assertEqual(\n            events_after[0].get(\"content\", {}).get(\"body\"),\n            \"message 4\",\n            events_after[0],\n        )\n        self.assertEqual(\n            events_after[1].get(\"content\", {}).get(\"body\"),\n            \"message 5\",\n            events_after[1],\n        )\n\n        # Deactivate the first account and erase the user's data.\n\n        deactivate_account_handler = self.hs.get_deactivate_account_handler()\n        self.get_success(\n            deactivate_account_handler.deactivate_account(self.user_id, erase_data=True)\n        )\n\n        # Invite another user in the room. This is needed because messages will be\n        # pruned only if the user wasn't a member of the room when the messages were\n        # sent.\n\n        invited_user_id = self.register_user(\"user3\", \"password\")\n        invited_tok = self.login(\"user3\", \"password\")\n\n        self.helper.invite(\n            self.room_id, self.other_user_id, invited_user_id, tok=self.other_tok\n        )\n        self.helper.join(self.room_id, invited_user_id, tok=invited_tok)\n\n        # Check that a user that joined the room after the erasure request can't see\n        # the messages anymore.\n\n        request, channel = self.make_request(\n            \"GET\",\n            '/rooms/%s/context/%s?filter={\"types\":[\"m.room.message\"]}'\n            % (self.room_id, event_id),\n            access_token=invited_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(len(events_before), 2, events_before)\n        self.assertDictEqual(events_before[0].get(\"content\"), {}, events_before[0])\n        self.assertDictEqual(events_before[1].get(\"content\"), {}, events_before[1])\n\n        self.assertDictEqual(\n            channel.json_body[\"event\"].get(\"content\"), {}, channel.json_body[\"event\"]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(len(events_after), 2, events_after)\n        self.assertDictEqual(events_after[0].get(\"content\"), {}, events_after[0])\n        self.assertEqual(events_after[1].get(\"content\"), {}, events_after[1])\n\n\nclass RoomAliasListTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        directory.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.room_owner = self.register_user(\"room_owner\", \"test\")\n        self.room_owner_tok = self.login(\"room_owner\", \"test\")\n\n        self.room_id = self.helper.create_room_as(\n            self.room_owner, tok=self.room_owner_tok\n        )\n\n    def test_no_aliases(self):\n        res = self._get_aliases(self.room_owner_tok)\n        self.assertEqual(res[\"aliases\"], [])\n\n    def test_not_in_room(self):\n        self.register_user(\"user\", \"test\")\n        user_tok = self.login(\"user\", \"test\")\n        res = self._get_aliases(user_tok, expected_code=403)\n        self.assertEqual(res[\"errcode\"], \"M_FORBIDDEN\")\n\n    def test_admin_user(self):\n        alias1 = self._random_alias()\n        self._set_alias_via_directory(alias1)\n\n        self.register_user(\"user\", \"test\", admin=True)\n        user_tok = self.login(\"user\", \"test\")\n\n        res = self._get_aliases(user_tok)\n        self.assertEqual(res[\"aliases\"], [alias1])\n\n    def test_with_aliases(self):\n        alias1 = self._random_alias()\n        alias2 = self._random_alias()\n\n        self._set_alias_via_directory(alias1)\n        self._set_alias_via_directory(alias2)\n\n        res = self._get_aliases(self.room_owner_tok)\n        self.assertEqual(set(res[\"aliases\"]), {alias1, alias2})\n\n    def test_peekable_room(self):\n        alias1 = self._random_alias()\n        self._set_alias_via_directory(alias1)\n\n        self.helper.send_state(\n            self.room_id,\n            EventTypes.RoomHistoryVisibility,\n            body={\"history_visibility\": \"world_readable\"},\n            tok=self.room_owner_tok,\n        )\n\n        self.register_user(\"user\", \"test\")\n        user_tok = self.login(\"user\", \"test\")\n\n        res = self._get_aliases(user_tok)\n        self.assertEqual(res[\"aliases\"], [alias1])\n\n    def _get_aliases(self, access_token: str, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/unstable/org.matrix.msc2432/rooms/%s/aliases\"\n            % (self.room_id,),\n            access_token=access_token,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        if expected_code == 200:\n            self.assertIsInstance(res[\"aliases\"], list)\n        return res\n\n    def _random_alias(self) -> str:\n        return RoomAlias(random_string(5), self.hs.hostname).to_string()\n\n    def _set_alias_via_directory(self, alias: str, expected_code: int = 200):\n        url = \"/_matrix/client/r0/directory/room/\" + alias\n        data = {\"room_id\": self.room_id}\n        request_data = json.dumps(data)\n\n        request, channel = self.make_request(\n            \"PUT\", url, request_data, access_token=self.room_owner_tok\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n\n\nclass RoomCanonicalAliasTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        directory.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.room_owner = self.register_user(\"room_owner\", \"test\")\n        self.room_owner_tok = self.login(\"room_owner\", \"test\")\n\n        self.room_id = self.helper.create_room_as(\n            self.room_owner, tok=self.room_owner_tok\n        )\n\n        self.alias = \"#alias:test\"\n        self._set_alias_via_directory(self.alias)\n\n    def _set_alias_via_directory(self, alias: str, expected_code: int = 200):\n        url = \"/_matrix/client/r0/directory/room/\" + alias\n        data = {\"room_id\": self.room_id}\n        request_data = json.dumps(data)\n\n        request, channel = self.make_request(\n            \"PUT\", url, request_data, access_token=self.room_owner_tok\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n\n    def _get_canonical_alias(self, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"rooms/%s/state/m.room.canonical_alias\" % (self.room_id,),\n            access_token=self.room_owner_tok,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        return res\n\n    def _set_canonical_alias(self, content: str, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"PUT\",\n            \"rooms/%s/state/m.room.canonical_alias\" % (self.room_id,),\n            json.dumps(content),\n            access_token=self.room_owner_tok,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        return res\n\n    def test_canonical_alias(self):\n        \"\"\"Test a basic alias message.\"\"\"\n        # There is no canonical alias to start with.\n        self._get_canonical_alias(expected_code=404)\n\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias})\n\n        # Now remove the alias.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_alt_aliases(self):\n        \"\"\"Test a canonical alias message with alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alt_aliases\": [self.alias]})\n\n        # Now remove the alt_aliases.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_alias_alt_aliases(self):\n        \"\"\"Test a canonical alias message with an alias and alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Now remove the alias and alt_aliases.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_partial_modify(self):\n        \"\"\"Test removing only the alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Now remove the alt_aliases.\n        self._set_canonical_alias({\"alias\": self.alias})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias})\n\n    def test_add_alias(self):\n        \"\"\"Test removing only the alt_aliases.\"\"\"\n        # Create an additional alias.\n        second_alias = \"#second:test\"\n        self._set_alias_via_directory(second_alias)\n\n        # Add the canonical alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Then add the second alias.\n        self._set_canonical_alias(\n            {\"alias\": self.alias, \"alt_aliases\": [self.alias, second_alias]}\n        )\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(\n            res, {\"alias\": self.alias, \"alt_aliases\": [self.alias, second_alias]}\n        )\n\n    def test_bad_data(self):\n        \"\"\"Invalid data for alt_aliases should cause errors.\"\"\"\n        self._set_canonical_alias({\"alt_aliases\": \"@bad:test\"}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": None}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": 0}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": 1}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": False}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": True}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": {}}, expected_code=400)\n\n    def test_bad_alias(self):\n        \"\"\"An alias which does not point to the room raises a SynapseError.\"\"\"\n        self._set_canonical_alias({\"alias\": \"@unknown:test\"}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": [\"@unknown:test\"]}, expected_code=400)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2017 Vector Creations Ltd\n# Copyright 2018-2019 New Vector Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests REST events for /rooms paths.\"\"\"\n\nimport json\nfrom urllib import parse as urlparse\n\nfrom mock import Mock\n\nimport synapse.rest.admin\nfrom synapse.api.constants import EventContentFields, EventTypes, Membership\nfrom synapse.handlers.pagination import PurgeStatus\nfrom synapse.rest.client.v1 import directory, login, profile, room\nfrom synapse.rest.client.v2_alpha import account\nfrom synapse.types import JsonDict, RoomAlias, UserID\nfrom synapse.util.stringutils import random_string\n\nfrom tests import unittest\nfrom tests.test_utils import make_awaitable\n\nPATH_PREFIX = b\"/_matrix/client/api/v1\"\n\n\nclass RoomBase(unittest.HomeserverTestCase):\n    rmcreator_id = None\n\n    servlets = [room.register_servlets, room.register_deprecated_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        self.hs = self.setup_test_homeserver(\n            \"red\", http_client=None, federation_client=Mock(),\n        )\n\n        self.hs.get_federation_handler = Mock()\n        self.hs.get_federation_handler.return_value.maybe_backfill = Mock(\n            return_value=make_awaitable(None)\n        )\n\n        async def _insert_client_ip(*args, **kwargs):\n            return None\n\n        self.hs.get_datastore().insert_client_ip = _insert_client_ip\n\n        return self.hs\n\n\nclass RoomPermissionsTestCase(RoomBase):\n    \"\"\" Tests room permissions. \"\"\"\n\n    user_id = \"@sid1:red\"\n    rmcreator_id = \"@notme:red\"\n\n    def prepare(self, reactor, clock, hs):\n\n        self.helper.auth_user_id = self.rmcreator_id\n        # create some rooms under the name rmcreator_id\n        self.uncreated_rmid = \"!aa:test\"\n        self.created_rmid = self.helper.create_room_as(\n            self.rmcreator_id, is_public=False\n        )\n        self.created_public_rmid = self.helper.create_room_as(\n            self.rmcreator_id, is_public=True\n        )\n\n        # send a message in one of the rooms\n        self.created_rmid_msg_path = (\n            \"rooms/%s/send/m.room.message/a1\" % (self.created_rmid)\n        ).encode(\"ascii\")\n        request, channel = self.make_request(\n            \"PUT\", self.created_rmid_msg_path, b'{\"msgtype\":\"m.text\",\"body\":\"test msg\"}'\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        # set topic for public room\n        request, channel = self.make_request(\n            \"PUT\",\n            (\"rooms/%s/state/m.room.topic\" % self.created_public_rmid).encode(\"ascii\"),\n            b'{\"topic\":\"Public Room Topic\"}',\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        # auth as user_id now\n        self.helper.auth_user_id = self.user_id\n\n    def test_can_do_action(self):\n        msg_content = b'{\"msgtype\":\"m.text\",\"body\":\"hello\"}'\n\n        seq = iter(range(100))\n\n        def send_msg_path():\n            return \"/rooms/%s/send/m.room.message/mid%s\" % (\n                self.created_rmid,\n                str(next(seq)),\n            )\n\n        # send message in uncreated room, expect 403\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/send/m.room.message/mid2\" % (self.uncreated_rmid,),\n            msg_content,\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room not joined (no state), expect 403\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and invited, expect 403\n        self.helper.invite(\n            room=self.created_rmid, src=self.rmcreator_id, targ=self.user_id\n        )\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and joined, expect 200\n        self.helper.join(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and left, expect 403\n        self.helper.leave(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_topic_perms(self):\n        topic_content = b'{\"topic\":\"My Topic Name\"}'\n        topic_path = \"/rooms/%s/state/m.room.topic\" % self.created_rmid\n\n        # set/get topic in uncreated room, expect 403\n        request, channel = self.make_request(\n            \"PUT\", \"/rooms/%s/state/m.room.topic\" % self.uncreated_rmid, topic_content\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/state/m.room.topic\" % self.uncreated_rmid\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set/get topic in created PRIVATE room not joined, expect 403\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set topic in created PRIVATE room and invited, expect 403\n        self.helper.invite(\n            room=self.created_rmid, src=self.rmcreator_id, targ=self.user_id\n        )\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # get topic in created PRIVATE room and invited, expect 403\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set/get topic in created PRIVATE room and joined, expect 200\n        self.helper.join(room=self.created_rmid, user=self.user_id)\n\n        # Only room ops can set topic by default\n        self.helper.auth_user_id = self.rmcreator_id\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.helper.auth_user_id = self.user_id\n\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(topic_content.decode(\"utf8\")), channel.json_body)\n\n        # set/get topic in created PRIVATE room and left, expect 403\n        self.helper.leave(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # get topic in PUBLIC room, not joined, expect 403\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/state/m.room.topic\" % self.created_public_rmid\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set topic in PUBLIC room, not joined, expect 403\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/state/m.room.topic\" % self.created_public_rmid,\n            topic_content,\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def _test_get_membership(self, room=None, members=[], expect_code=None):\n        for member in members:\n            path = \"/rooms/%s/state/m.room.member/%s\" % (room, member)\n            request, channel = self.make_request(\"GET\", path)\n            self.assertEquals(expect_code, channel.code)\n\n    def test_membership_basic_room_perms(self):\n        # === room does not exist ===\n        room = self.uncreated_rmid\n        # get membership of self, get membership of other, uncreated room\n        # expect all 403s\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # trying to invite people to this room should 403\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.rmcreator_id, expect_code=403\n        )\n\n        # set [invite/join/left] of self, set [invite/join/left] of other,\n        # expect all 404s because room doesn't exist on any server\n        for usr in [self.user_id, self.rmcreator_id]:\n            self.helper.join(room=room, user=usr, expect_code=404)\n            self.helper.leave(room=room, user=usr, expect_code=404)\n\n    def test_membership_private_room_perms(self):\n        room = self.created_rmid\n        # get membership of self, get membership of other, private room + invite\n        # expect all 403s\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # get membership of self, get membership of other, private room + joined\n        # expect all 200s\n        self.helper.join(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n        # get membership of self, get membership of other, private room + left\n        # expect all 200s\n        self.helper.leave(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n    def test_membership_public_room_perms(self):\n        room = self.created_public_rmid\n        # get membership of self, get membership of other, public room + invite\n        # expect 403\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # get membership of self, get membership of other, public room + joined\n        # expect all 200s\n        self.helper.join(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n        # get membership of self, get membership of other, public room + left\n        # expect 200.\n        self.helper.leave(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n    def test_invited_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n\n        # set [invite/join/left] of other user, expect 403s\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.rmcreator_id, expect_code=403\n        )\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.JOIN,\n            expect_code=403,\n        )\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )\n\n    def test_joined_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self.helper.join(room=room, user=self.user_id)\n\n        # set invited of self, expect 403\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.user_id, expect_code=403\n        )\n\n        # set joined of self, expect 200 (NOOP)\n        self.helper.join(room=room, user=self.user_id)\n\n        other = \"@burgundy:red\"\n        # set invited of other, expect 200\n        self.helper.invite(room=room, src=self.user_id, targ=other, expect_code=200)\n\n        # set joined of other, expect 403\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=other,\n            membership=Membership.JOIN,\n            expect_code=403,\n        )\n\n        # set left of other, expect 403\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=other,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )\n\n        # set left of self, expect 200\n        self.helper.leave(room=room, user=self.user_id)\n\n    def test_leave_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self.helper.join(room=room, user=self.user_id)\n        self.helper.leave(room=room, user=self.user_id)\n\n        # set [invite/join/left] of self, set [invite/join/left] of other,\n        # expect all 403s\n        for usr in [self.user_id, self.rmcreator_id]:\n            self.helper.change_membership(\n                room=room,\n                src=self.user_id,\n                targ=usr,\n                membership=Membership.INVITE,\n                expect_code=403,\n            )\n\n            self.helper.change_membership(\n                room=room,\n                src=self.user_id,\n                targ=usr,\n                membership=Membership.JOIN,\n                expect_code=403,\n            )\n\n        # It is always valid to LEAVE if you've already left (currently.)\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )\n\n\nclass RoomsMemberListTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/members/list REST events.\"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def test_get_member_list(self):\n        room_id = self.helper.create_room_as(self.user_id)\n        request, channel = self.make_request(\"GET\", \"/rooms/%s/members\" % room_id)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_no_room(self):\n        request, channel = self.make_request(\"GET\", \"/rooms/roomdoesnotexist/members\")\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_no_permission(self):\n        room_id = self.helper.create_room_as(\"@some_other_guy:red\")\n        request, channel = self.make_request(\"GET\", \"/rooms/%s/members\" % room_id)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_mixed_memberships(self):\n        room_creator = \"@some_other_guy:red\"\n        room_id = self.helper.create_room_as(room_creator)\n        room_path = \"/rooms/%s/members\" % room_id\n        self.helper.invite(room=room_id, src=room_creator, targ=self.user_id)\n        # can't see list if you're just invited.\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        self.helper.join(room=room_id, user=self.user_id)\n        # can see list now joined\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        self.helper.leave(room=room_id, user=self.user_id)\n        # can see old list once left\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n\nclass RoomsCreateTestCase(RoomBase):\n    \"\"\" Tests /rooms and /rooms/$room_id REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def test_post_room_no_keys(self):\n        # POST with no config keys, expect new room id\n        request, channel = self.make_request(\"POST\", \"/createRoom\", \"{}\")\n\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_visibility_key(self):\n        # POST with visibility config key, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"visibility\":\"private\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_custom_key(self):\n        # POST with custom config keys, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"custom\":\"stuff\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_known_and_unknown_keys(self):\n        # POST with custom + known config keys, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"visibility\":\"private\",\"custom\":\"things\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_invalid_content(self):\n        # POST with invalid content / paths, expect 400\n        request, channel = self.make_request(\"POST\", \"/createRoom\", b'{\"visibili')\n        self.assertEquals(400, channel.code)\n\n        request, channel = self.make_request(\"POST\", \"/createRoom\", b'[\"hello\"]')\n        self.assertEquals(400, channel.code)\n\n    def test_post_room_invitees_invalid_mxid(self):\n        # POST with invalid invitee, see https://github.com/matrix-org/synapse/issues/4088\n        # Note the trailing space in the MXID here!\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"invite\":[\"@alice:example.com \"]}'\n        )\n        self.assertEquals(400, channel.code)\n\n\nclass RoomTopicTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/topic REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        # create the room\n        self.room_id = self.helper.create_room_as(self.user_id)\n        self.path = \"/rooms/%s/state/m.room.topic\" % (self.room_id,)\n\n    def test_invalid_puts(self):\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", self.path, \"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, '{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, '{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", self.path, '[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, \"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, \"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # valid key, wrong type\n        content = '{\"topic\":[\"Topic name\"]}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_topic(self):\n        # nothing should be there\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(404, channel.code, msg=channel.result[\"body\"])\n\n        # valid put\n        content = '{\"topic\":\"Topic name\"}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # valid get\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(content), channel.json_body)\n\n    def test_rooms_topic_with_extra_keys(self):\n        # valid put with extra keys\n        content = '{\"topic\":\"Seasons\",\"subtopic\":\"Summer\"}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # valid get\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(content), channel.json_body)\n\n\nclass RoomMemberStateTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/members/$user_id/state REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_invalid_puts(self):\n        path = \"/rooms/%s/state/m.room.member/%s\" % (self.room_id, self.user_id)\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", path, \"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, '{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, '{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", path, b'[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, \"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, \"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # valid keys, wrong types\n        content = '{\"membership\":[\"%s\",\"%s\",\"%s\"]}' % (\n            Membership.INVITE,\n            Membership.JOIN,\n            Membership.LEAVE,\n        )\n        request, channel = self.make_request(\"PUT\", path, content.encode(\"ascii\"))\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_members_self(self):\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.user_id,\n        )\n\n        # valid join message (NOOP since we made the room)\n        content = '{\"membership\":\"%s\"}' % Membership.JOIN\n        request, channel = self.make_request(\"PUT\", path, content.encode(\"ascii\"))\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        expected_response = {\"membership\": Membership.JOIN}\n        self.assertEquals(expected_response, channel.json_body)\n\n    def test_rooms_members_other(self):\n        self.other_id = \"@zzsid1:red\"\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.other_id,\n        )\n\n        # valid invite message\n        content = '{\"membership\":\"%s\"}' % Membership.INVITE\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assertEquals(json.loads(content), channel.json_body)\n\n    def test_rooms_members_other_custom_keys(self):\n        self.other_id = \"@zzsid1:red\"\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.other_id,\n        )\n\n        # valid invite message with custom key\n        content = '{\"membership\":\"%s\",\"invite_text\":\"%s\"}' % (\n            Membership.INVITE,\n            \"Join us!\",\n        )\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assertEquals(json.loads(content), channel.json_body)\n\n\nclass RoomJoinRatelimitTestCase(RoomBase):\n    user_id = \"@sid1:red\"\n\n    servlets = [\n        profile.register_servlets,\n        room.register_servlets,\n    ]\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit(self):\n        \"\"\"Tests that local joins are actually rate-limited.\"\"\"\n        for i in range(3):\n            self.helper.create_room_as(self.user_id)\n\n        self.helper.create_room_as(self.user_id, expect_code=429)\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit_profile_change(self):\n        \"\"\"Tests that sending a profile update into all of the user's joined rooms isn't\n        rate-limited by the rate-limiter on joins.\"\"\"\n\n        # Create and join as many rooms as the rate-limiting config allows in a second.\n        room_ids = [\n            self.helper.create_room_as(self.user_id),\n            self.helper.create_room_as(self.user_id),\n            self.helper.create_room_as(self.user_id),\n        ]\n        # Let some time for the rate-limiter to forget about our multi-join.\n        self.reactor.advance(2)\n        # Add one to make sure we're joined to more rooms than the config allows us to\n        # join in a second.\n        room_ids.append(self.helper.create_room_as(self.user_id))\n\n        # Create a profile for the user, since it hasn't been done on registration.\n        store = self.hs.get_datastore()\n        self.get_success(\n            store.create_profile(UserID.from_string(self.user_id).localpart)\n        )\n\n        # Update the display name for the user.\n        path = \"/_matrix/client/r0/profile/%s/displayname\" % self.user_id\n        request, channel = self.make_request(\"PUT\", path, {\"displayname\": \"John Doe\"})\n        self.assertEquals(channel.code, 200, channel.json_body)\n\n        # Check that all the rooms have been sent a profile update into.\n        for room_id in room_ids:\n            path = \"/_matrix/client/r0/rooms/%s/state/m.room.member/%s\" % (\n                room_id,\n                self.user_id,\n            )\n\n            request, channel = self.make_request(\"GET\", path)\n            self.assertEquals(channel.code, 200)\n\n            self.assertIn(\"displayname\", channel.json_body)\n            self.assertEquals(channel.json_body[\"displayname\"], \"John Doe\")\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit_idempotent(self):\n        \"\"\"Tests that the room join endpoints remain idempotent despite rate-limiting\n        on room joins.\"\"\"\n        room_id = self.helper.create_room_as(self.user_id)\n\n        # Let's test both paths to be sure.\n        paths_to_test = [\n            \"/_matrix/client/r0/rooms/%s/join\",\n            \"/_matrix/client/r0/join/%s\",\n        ]\n\n        for path in paths_to_test:\n            # Make sure we send more requests than the rate-limiting config would allow\n            # if all of these requests ended up joining the user to a room.\n            for i in range(4):\n                request, channel = self.make_request(\"POST\", path % room_id, {})\n                self.assertEquals(channel.code, 200)\n\n\nclass RoomMessagesTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/messages/$user_id/$msg_id REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_invalid_puts(self):\n        path = \"/rooms/%s/send/m.room.message/mid1\" % (urlparse.quote(self.room_id))\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", path, b\"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b'{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b'{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", path, b'[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b\"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b\"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_messages_sent(self):\n        path = \"/rooms/%s/send/m.room.message/mid1\" % (urlparse.quote(self.room_id))\n\n        content = b'{\"body\":\"test\",\"msgtype\":{\"type\":\"a\"}}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # custom message types\n        content = b'{\"body\":\"test\",\"msgtype\":\"test.custom.text\"}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # m.text message type\n        path = \"/rooms/%s/send/m.room.message/mid2\" % (urlparse.quote(self.room_id))\n        content = b'{\"body\":\"test2\",\"msgtype\":\"m.text\"}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n\nclass RoomInitialSyncTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/initialSync. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        # create the room\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_initial_sync(self):\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/initialSync\" % self.room_id\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.room_id, channel.json_body[\"room_id\"])\n        self.assertEquals(\"join\", channel.json_body[\"membership\"])\n\n        # Room state is easier to assert on if we unpack it into a dict\n        state = {}\n        for event in channel.json_body[\"state\"]:\n            if \"state_key\" not in event:\n                continue\n            t = event[\"type\"]\n            if t not in state:\n                state[t] = []\n            state[t].append(event)\n\n        self.assertTrue(\"m.room.create\" in state)\n\n        self.assertTrue(\"messages\" in channel.json_body)\n        self.assertTrue(\"chunk\" in channel.json_body[\"messages\"])\n        self.assertTrue(\"end\" in channel.json_body[\"messages\"])\n\n        self.assertTrue(\"presence\" in channel.json_body)\n\n        presence_by_user = {\n            e[\"content\"][\"user_id\"]: e for e in channel.json_body[\"presence\"]\n        }\n        self.assertTrue(self.user_id in presence_by_user)\n        self.assertEquals(\"m.presence\", presence_by_user[self.user_id][\"type\"])\n\n\nclass RoomMessageListTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/messages REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_topo_token_is_accepted(self):\n        token = \"t1-0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/messages?access_token=x&from=%s\" % (self.room_id, token)\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"start\" in channel.json_body)\n        self.assertEquals(token, channel.json_body[\"start\"])\n        self.assertTrue(\"chunk\" in channel.json_body)\n        self.assertTrue(\"end\" in channel.json_body)\n\n    def test_stream_token_is_accepted_for_fwd_pagianation(self):\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/messages?access_token=x&from=%s\" % (self.room_id, token)\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"start\" in channel.json_body)\n        self.assertEquals(token, channel.json_body[\"start\"])\n        self.assertTrue(\"chunk\" in channel.json_body)\n        self.assertTrue(\"end\" in channel.json_body)\n\n    def test_room_messages_purge(self):\n        store = self.hs.get_datastore()\n        pagination_handler = self.hs.get_pagination_handler()\n\n        # Send a first message in the room, which will be removed by the purge.\n        first_event_id = self.helper.send(self.room_id, \"message 1\")[\"event_id\"]\n        first_token = self.get_success(\n            store.get_topological_token_for_event(first_event_id)\n        )\n        first_token_str = self.get_success(first_token.to_string(store))\n\n        # Send a second message in the room, which won't be removed, and which we'll\n        # use as the marker to purge events before.\n        second_event_id = self.helper.send(self.room_id, \"message 2\")[\"event_id\"]\n        second_token = self.get_success(\n            store.get_topological_token_for_event(second_event_id)\n        )\n        second_token_str = self.get_success(second_token.to_string(store))\n\n        # Send a third event in the room to ensure we don't fall under any edge case\n        # due to our marker being the latest forward extremity in the room.\n        self.helper.send(self.room_id, \"message 3\")\n\n        # Check that we get the first and second message when querying /messages.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                second_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 2, [event[\"content\"] for event in chunk])\n\n        # Purge every event before the second event.\n        purge_id = random_string(16)\n        pagination_handler._purges_by_id[purge_id] = PurgeStatus()\n        self.get_success(\n            pagination_handler._purge_history(\n                purge_id=purge_id,\n                room_id=self.room_id,\n                token=second_token_str,\n                delete_local_events=True,\n            )\n        )\n\n        # Check that we only get the second message through /message now that the first\n        # has been purged.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                second_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 1, [event[\"content\"] for event in chunk])\n\n        # Check that we get no event, but also no error, when querying /messages with\n        # the token that was pointing at the first event, because we don't have it\n        # anymore.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                first_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 0, [event[\"content\"] for event in chunk])\n\n\nclass RoomSearchTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n    user_id = True\n    hijack_auth = False\n\n    def prepare(self, reactor, clock, hs):\n\n        # Register the user who does the searching\n        self.user_id = self.register_user(\"user\", \"pass\")\n        self.access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        self.other_user_id = self.register_user(\"otheruser\", \"pass\")\n        self.other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Create a room\n        self.room = self.helper.create_room_as(self.user_id, tok=self.access_token)\n\n        # Invite the other person\n        self.helper.invite(\n            room=self.room,\n            src=self.user_id,\n            tok=self.access_token,\n            targ=self.other_user_id,\n        )\n\n        # The other user joins\n        self.helper.join(\n            room=self.room, user=self.other_user_id, tok=self.other_access_token\n        )\n\n    def test_finds_message(self):\n        \"\"\"\n        The search functionality will search for content in messages if asked to\n        do so.\n        \"\"\"\n        # The other user sends some messages\n        self.helper.send(self.room, body=\"Hi!\", tok=self.other_access_token)\n        self.helper.send(self.room, body=\"There!\", tok=self.other_access_token)\n\n        request, channel = self.make_request(\n            \"POST\",\n            \"/search?access_token=%s\" % (self.access_token,),\n            {\n                \"search_categories\": {\n                    \"room_events\": {\"keys\": [\"content.body\"], \"search_term\": \"Hi\"}\n                }\n            },\n        )\n\n        # Check we get the results we expect -- one search result, of the sent\n        # messages\n        self.assertEqual(channel.code, 200)\n        results = channel.json_body[\"search_categories\"][\"room_events\"]\n        self.assertEqual(results[\"count\"], 1)\n        self.assertEqual(results[\"results\"][0][\"result\"][\"content\"][\"body\"], \"Hi!\")\n\n        # No context was requested, so we should get none.\n        self.assertEqual(results[\"results\"][0][\"context\"], {})\n\n    def test_include_context(self):\n        \"\"\"\n        When event_context includes include_profile, profile information will be\n        included in the search response.\n        \"\"\"\n        # The other user sends some messages\n        self.helper.send(self.room, body=\"Hi!\", tok=self.other_access_token)\n        self.helper.send(self.room, body=\"There!\", tok=self.other_access_token)\n\n        request, channel = self.make_request(\n            \"POST\",\n            \"/search?access_token=%s\" % (self.access_token,),\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"keys\": [\"content.body\"],\n                        \"search_term\": \"Hi\",\n                        \"event_context\": {\"include_profile\": True},\n                    }\n                }\n            },\n        )\n\n        # Check we get the results we expect -- one search result, of the sent\n        # messages\n        self.assertEqual(channel.code, 200)\n        results = channel.json_body[\"search_categories\"][\"room_events\"]\n        self.assertEqual(results[\"count\"], 1)\n        self.assertEqual(results[\"results\"][0][\"result\"][\"content\"][\"body\"], \"Hi!\")\n\n        # We should get context info, like the two users, and the display names.\n        context = results[\"results\"][0][\"context\"]\n        self.assertEqual(len(context[\"profile_info\"].keys()), 2)\n        self.assertEqual(\n            context[\"profile_info\"][self.other_user_id][\"displayname\"], \"otheruser\"\n        )\n\n\nclass PublicRoomsRestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n\n        self.url = b\"/_matrix/client/r0/publicRooms\"\n\n        config = self.default_config()\n        config[\"allow_public_rooms_without_auth\"] = False\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def test_restricted_no_auth(self):\n        request, channel = self.make_request(\"GET\", self.url)\n        self.assertEqual(channel.code, 401, channel.result)\n\n    def test_restricted_auth(self):\n        self.register_user(\"user\", \"pass\")\n        tok = self.login(\"user\", \"pass\")\n\n        request, channel = self.make_request(\"GET\", self.url, access_token=tok)\n        self.assertEqual(channel.code, 200, channel.result)\n\n\nclass PerRoomProfilesForbiddenTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"allow_per_room_profiles\"] = False\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"test\", \"test\")\n        self.tok = self.login(\"test\", \"test\")\n\n        # Set a profile for the test user\n        self.displayname = \"test user\"\n        data = {\"displayname\": self.displayname}\n        request_data = json.dumps(data)\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/_matrix/client/r0/profile/%s/displayname\" % (self.user_id,),\n            request_data,\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self.room_id = self.helper.create_room_as(self.user_id, tok=self.tok)\n\n    def test_per_room_profile_forbidden(self):\n        data = {\"membership\": \"join\", \"displayname\": \"other test user\"}\n        request_data = json.dumps(data)\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/_matrix/client/r0/rooms/%s/state/m.room.member/%s\"\n            % (self.room_id, self.user_id),\n            request_data,\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n        event_id = channel.json_body[\"event_id\"]\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/r0/rooms/%s/event/%s\" % (self.room_id, event_id),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        res_displayname = channel.json_body[\"content\"][\"displayname\"]\n        self.assertEqual(res_displayname, self.displayname, channel.result)\n\n\nclass RoomMembershipReasonTestCase(unittest.HomeserverTestCase):\n    \"\"\"Tests that clients can add a \"reason\" field to membership events and\n    that they get correctly added to the generated events and propagated.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.creator = self.register_user(\"creator\", \"test\")\n        self.creator_tok = self.login(\"creator\", \"test\")\n\n        self.second_user_id = self.register_user(\"second\", \"test\")\n        self.second_tok = self.login(\"second\", \"test\")\n\n        self.room_id = self.helper.create_room_as(self.creator, tok=self.creator_tok)\n\n    def test_join_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/join\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_leave_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/leave\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_kick_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/kick\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_ban_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/ban\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_unban_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/unban\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_invite_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/invite\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_reject_invite_reason(self):\n        self.helper.invite(\n            self.room_id,\n            src=self.creator,\n            targ=self.second_user_id,\n            tok=self.creator_tok,\n        )\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/leave\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def _check_for_reason(self, reason):\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/r0/rooms/{}/state/m.room.member/{}\".format(\n                self.room_id, self.second_user_id\n            ),\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        event_content = channel.json_body\n\n        self.assertEqual(event_content.get(\"reason\"), reason, channel.result)\n\n\nclass LabelsTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    # Filter that should only catch messages with the label \"#fun\".\n    FILTER_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.labels\": [\"#fun\"],\n    }\n    # Filter that should only catch messages without the label \"#fun\".\n    FILTER_NOT_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.not_labels\": [\"#fun\"],\n    }\n    # Filter that should only catch messages with the label \"#work\" but without the label\n    # \"#notfun\".\n    FILTER_LABELS_NOT_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.labels\": [\"#work\"],\n        \"org.matrix.not_labels\": [\"#notfun\"],\n    }\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"test\", \"test\")\n        self.tok = self.login(\"test\", \"test\")\n        self.room_id = self.helper.create_room_as(self.user_id, tok=self.tok)\n\n    def test_context_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /context request.\"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 1, [event[\"content\"] for event in events_before]\n        )\n        self.assertEqual(\n            events_before[0][\"content\"][\"body\"], \"with right label\", events_before[0]\n        )\n\n        events_after = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_after), 1, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with right label\", events_after[0]\n        )\n\n    def test_context_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /context request.\"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_NOT_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 1, [event[\"content\"] for event in events_before]\n        )\n        self.assertEqual(\n            events_before[0][\"content\"][\"body\"], \"without label\", events_before[0]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(\n            len(events_after), 2, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with wrong label\", events_after[0]\n        )\n        self.assertEqual(\n            events_after[1][\"content\"][\"body\"], \"with two wrong labels\", events_after[1]\n        )\n\n    def test_context_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /context request.\n        \"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_LABELS_NOT_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 0, [event[\"content\"] for event in events_before]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(\n            len(events_after), 1, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with wrong label\", events_after[0]\n        )\n\n    def test_messages_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /messages request.\"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (self.room_id, self.tok, token, json.dumps(self.FILTER_LABELS)),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 2, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"with right label\", events[0])\n        self.assertEqual(events[1][\"content\"][\"body\"], \"with right label\", events[1])\n\n    def test_messages_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /messages request.\"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (self.room_id, self.tok, token, json.dumps(self.FILTER_NOT_LABELS)),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 4, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"without label\", events[0])\n        self.assertEqual(events[1][\"content\"][\"body\"], \"without label\", events[1])\n        self.assertEqual(events[2][\"content\"][\"body\"], \"with wrong label\", events[2])\n        self.assertEqual(\n            events[3][\"content\"][\"body\"], \"with two wrong labels\", events[3]\n        )\n\n    def test_messages_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /messages request.\n        \"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (\n                self.room_id,\n                self.tok,\n                token,\n                json.dumps(self.FILTER_LABELS_NOT_LABELS),\n            ),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 1, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"with wrong label\", events[0])\n\n    def test_search_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /search request.\"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 2, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"with right label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[1][\"result\"][\"content\"][\"body\"],\n            \"with right label\",\n            results[1][\"result\"][\"content\"][\"body\"],\n        )\n\n    def test_search_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /search request.\"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_NOT_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 4, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"without label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[1][\"result\"][\"content\"][\"body\"],\n            \"without label\",\n            results[1][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[2][\"result\"][\"content\"][\"body\"],\n            \"with wrong label\",\n            results[2][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[3][\"result\"][\"content\"][\"body\"],\n            \"with two wrong labels\",\n            results[3][\"result\"][\"content\"][\"body\"],\n        )\n\n    def test_search_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /search request.\n        \"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_LABELS_NOT_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 1, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"with wrong label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n\n    def _send_labelled_messages_in_room(self):\n        \"\"\"Sends several messages to a room with different labels (or without any) to test\n        filtering by label.\n        Returns:\n            The ID of the event to use if we're testing filtering on /context.\n        \"\"\"\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with right label\",\n                EventContentFields.LABELS: [\"#fun\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\"msgtype\": \"m.text\", \"body\": \"without label\"},\n            tok=self.tok,\n        )\n\n        res = self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\"msgtype\": \"m.text\", \"body\": \"without label\"},\n            tok=self.tok,\n        )\n        # Return this event's ID when we test filtering in /context requests.\n        event_id = res[\"event_id\"]\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with wrong label\",\n                EventContentFields.LABELS: [\"#work\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with two wrong labels\",\n                EventContentFields.LABELS: [\"#work\", \"#notfun\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with right label\",\n                EventContentFields.LABELS: [\"#fun\"],\n            },\n            tok=self.tok,\n        )\n\n        return event_id\n\n\nclass ContextTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        account.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"user\", \"password\")\n        self.tok = self.login(\"user\", \"password\")\n        self.room_id = self.helper.create_room_as(\n            self.user_id, tok=self.tok, is_public=False\n        )\n\n        self.other_user_id = self.register_user(\"user2\", \"password\")\n        self.other_tok = self.login(\"user2\", \"password\")\n\n        self.helper.invite(self.room_id, self.user_id, self.other_user_id, tok=self.tok)\n        self.helper.join(self.room_id, self.other_user_id, tok=self.other_tok)\n\n    def test_erased_sender(self):\n        \"\"\"Test that an erasure request results in the requester's events being hidden\n        from any new member of the room.\n        \"\"\"\n\n        # Send a bunch of events in the room.\n\n        self.helper.send(self.room_id, \"message 1\", tok=self.tok)\n        self.helper.send(self.room_id, \"message 2\", tok=self.tok)\n        event_id = self.helper.send(self.room_id, \"message 3\", tok=self.tok)[\"event_id\"]\n        self.helper.send(self.room_id, \"message 4\", tok=self.tok)\n        self.helper.send(self.room_id, \"message 5\", tok=self.tok)\n\n        # Check that we can still see the messages before the erasure request.\n\n        request, channel = self.make_request(\n            \"GET\",\n            '/rooms/%s/context/%s?filter={\"types\":[\"m.room.message\"]}'\n            % (self.room_id, event_id),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(len(events_before), 2, events_before)\n        self.assertEqual(\n            events_before[0].get(\"content\", {}).get(\"body\"),\n            \"message 2\",\n            events_before[0],\n        )\n        self.assertEqual(\n            events_before[1].get(\"content\", {}).get(\"body\"),\n            \"message 1\",\n            events_before[1],\n        )\n\n        self.assertEqual(\n            channel.json_body[\"event\"].get(\"content\", {}).get(\"body\"),\n            \"message 3\",\n            channel.json_body[\"event\"],\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(len(events_after), 2, events_after)\n        self.assertEqual(\n            events_after[0].get(\"content\", {}).get(\"body\"),\n            \"message 4\",\n            events_after[0],\n        )\n        self.assertEqual(\n            events_after[1].get(\"content\", {}).get(\"body\"),\n            \"message 5\",\n            events_after[1],\n        )\n\n        # Deactivate the first account and erase the user's data.\n\n        deactivate_account_handler = self.hs.get_deactivate_account_handler()\n        self.get_success(\n            deactivate_account_handler.deactivate_account(self.user_id, erase_data=True)\n        )\n\n        # Invite another user in the room. This is needed because messages will be\n        # pruned only if the user wasn't a member of the room when the messages were\n        # sent.\n\n        invited_user_id = self.register_user(\"user3\", \"password\")\n        invited_tok = self.login(\"user3\", \"password\")\n\n        self.helper.invite(\n            self.room_id, self.other_user_id, invited_user_id, tok=self.other_tok\n        )\n        self.helper.join(self.room_id, invited_user_id, tok=invited_tok)\n\n        # Check that a user that joined the room after the erasure request can't see\n        # the messages anymore.\n\n        request, channel = self.make_request(\n            \"GET\",\n            '/rooms/%s/context/%s?filter={\"types\":[\"m.room.message\"]}'\n            % (self.room_id, event_id),\n            access_token=invited_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(len(events_before), 2, events_before)\n        self.assertDictEqual(events_before[0].get(\"content\"), {}, events_before[0])\n        self.assertDictEqual(events_before[1].get(\"content\"), {}, events_before[1])\n\n        self.assertDictEqual(\n            channel.json_body[\"event\"].get(\"content\"), {}, channel.json_body[\"event\"]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(len(events_after), 2, events_after)\n        self.assertDictEqual(events_after[0].get(\"content\"), {}, events_after[0])\n        self.assertEqual(events_after[1].get(\"content\"), {}, events_after[1])\n\n\nclass RoomAliasListTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        directory.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.room_owner = self.register_user(\"room_owner\", \"test\")\n        self.room_owner_tok = self.login(\"room_owner\", \"test\")\n\n        self.room_id = self.helper.create_room_as(\n            self.room_owner, tok=self.room_owner_tok\n        )\n\n    def test_no_aliases(self):\n        res = self._get_aliases(self.room_owner_tok)\n        self.assertEqual(res[\"aliases\"], [])\n\n    def test_not_in_room(self):\n        self.register_user(\"user\", \"test\")\n        user_tok = self.login(\"user\", \"test\")\n        res = self._get_aliases(user_tok, expected_code=403)\n        self.assertEqual(res[\"errcode\"], \"M_FORBIDDEN\")\n\n    def test_admin_user(self):\n        alias1 = self._random_alias()\n        self._set_alias_via_directory(alias1)\n\n        self.register_user(\"user\", \"test\", admin=True)\n        user_tok = self.login(\"user\", \"test\")\n\n        res = self._get_aliases(user_tok)\n        self.assertEqual(res[\"aliases\"], [alias1])\n\n    def test_with_aliases(self):\n        alias1 = self._random_alias()\n        alias2 = self._random_alias()\n\n        self._set_alias_via_directory(alias1)\n        self._set_alias_via_directory(alias2)\n\n        res = self._get_aliases(self.room_owner_tok)\n        self.assertEqual(set(res[\"aliases\"]), {alias1, alias2})\n\n    def test_peekable_room(self):\n        alias1 = self._random_alias()\n        self._set_alias_via_directory(alias1)\n\n        self.helper.send_state(\n            self.room_id,\n            EventTypes.RoomHistoryVisibility,\n            body={\"history_visibility\": \"world_readable\"},\n            tok=self.room_owner_tok,\n        )\n\n        self.register_user(\"user\", \"test\")\n        user_tok = self.login(\"user\", \"test\")\n\n        res = self._get_aliases(user_tok)\n        self.assertEqual(res[\"aliases\"], [alias1])\n\n    def _get_aliases(self, access_token: str, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/unstable/org.matrix.msc2432/rooms/%s/aliases\"\n            % (self.room_id,),\n            access_token=access_token,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        if expected_code == 200:\n            self.assertIsInstance(res[\"aliases\"], list)\n        return res\n\n    def _random_alias(self) -> str:\n        return RoomAlias(random_string(5), self.hs.hostname).to_string()\n\n    def _set_alias_via_directory(self, alias: str, expected_code: int = 200):\n        url = \"/_matrix/client/r0/directory/room/\" + alias\n        data = {\"room_id\": self.room_id}\n        request_data = json.dumps(data)\n\n        request, channel = self.make_request(\n            \"PUT\", url, request_data, access_token=self.room_owner_tok\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n\n\nclass RoomCanonicalAliasTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        directory.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.room_owner = self.register_user(\"room_owner\", \"test\")\n        self.room_owner_tok = self.login(\"room_owner\", \"test\")\n\n        self.room_id = self.helper.create_room_as(\n            self.room_owner, tok=self.room_owner_tok\n        )\n\n        self.alias = \"#alias:test\"\n        self._set_alias_via_directory(self.alias)\n\n    def _set_alias_via_directory(self, alias: str, expected_code: int = 200):\n        url = \"/_matrix/client/r0/directory/room/\" + alias\n        data = {\"room_id\": self.room_id}\n        request_data = json.dumps(data)\n\n        request, channel = self.make_request(\n            \"PUT\", url, request_data, access_token=self.room_owner_tok\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n\n    def _get_canonical_alias(self, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"rooms/%s/state/m.room.canonical_alias\" % (self.room_id,),\n            access_token=self.room_owner_tok,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        return res\n\n    def _set_canonical_alias(self, content: str, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"PUT\",\n            \"rooms/%s/state/m.room.canonical_alias\" % (self.room_id,),\n            json.dumps(content),\n            access_token=self.room_owner_tok,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        return res\n\n    def test_canonical_alias(self):\n        \"\"\"Test a basic alias message.\"\"\"\n        # There is no canonical alias to start with.\n        self._get_canonical_alias(expected_code=404)\n\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias})\n\n        # Now remove the alias.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_alt_aliases(self):\n        \"\"\"Test a canonical alias message with alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alt_aliases\": [self.alias]})\n\n        # Now remove the alt_aliases.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_alias_alt_aliases(self):\n        \"\"\"Test a canonical alias message with an alias and alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Now remove the alias and alt_aliases.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_partial_modify(self):\n        \"\"\"Test removing only the alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Now remove the alt_aliases.\n        self._set_canonical_alias({\"alias\": self.alias})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias})\n\n    def test_add_alias(self):\n        \"\"\"Test removing only the alt_aliases.\"\"\"\n        # Create an additional alias.\n        second_alias = \"#second:test\"\n        self._set_alias_via_directory(second_alias)\n\n        # Add the canonical alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Then add the second alias.\n        self._set_canonical_alias(\n            {\"alias\": self.alias, \"alt_aliases\": [self.alias, second_alias]}\n        )\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(\n            res, {\"alias\": self.alias, \"alt_aliases\": [self.alias, second_alias]}\n        )\n\n    def test_bad_data(self):\n        \"\"\"Invalid data for alt_aliases should cause errors.\"\"\"\n        self._set_canonical_alias({\"alt_aliases\": \"@bad:test\"}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": None}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": 0}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": 1}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": False}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": True}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": {}}, expected_code=400)\n\n    def test_bad_alias(self):\n        \"\"\"An alias which does not point to the room raises a SynapseError.\"\"\"\n        self._set_canonical_alias({\"alias\": \"@unknown:test\"}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": [\"@unknown:test\"]}, expected_code=400)\n", "patch": "@@ -45,7 +45,7 @@ class RoomBase(unittest.HomeserverTestCase):\n     def make_homeserver(self, reactor, clock):\n \n         self.hs = self.setup_test_homeserver(\n-            \"red\", http_client=None, federation_client=Mock(),\n+            \"red\", federation_http_client=None, federation_client=Mock(),\n         )\n \n         self.hs.get_federation_handler = Mock()", "file_path": "files/2021_2/48", "file_language": "py", "file_name": "tests/rest/client/v1/test_rooms.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class RoomBase(unittest.HomeserverTestCase):\n    rmcreator_id = None\n\n    servlets = [room.register_servlets, room.register_deprecated_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        self.hs = self.setup_test_homeserver(\n            \"red\", http_client=None, federation_client=Mock(),\n        )\n\n        self.hs.get_federation_handler = Mock()\n        self.hs.get_federation_handler.return_value.maybe_backfill = Mock(\n            return_value=make_awaitable(None)\n        )\n\n        async def _insert_client_ip(*args, **kwargs):\n            return None\n\n        self.hs.get_datastore().insert_client_ip = _insert_client_ip\n\n        return self.hs", "target": 0}, {"function": "class RoomPermissionsTestCase(RoomBase):\n    \"\"\" Tests room permissions. \"\"\"\n\n    user_id = \"@sid1:red\"\n    rmcreator_id = \"@notme:red\"\n\n    def prepare(self, reactor, clock, hs):\n\n        self.helper.auth_user_id = self.rmcreator_id\n        # create some rooms under the name rmcreator_id\n        self.uncreated_rmid = \"!aa:test\"\n        self.created_rmid = self.helper.create_room_as(\n            self.rmcreator_id, is_public=False\n        )\n        self.created_public_rmid = self.helper.create_room_as(\n            self.rmcreator_id, is_public=True\n        )\n\n        # send a message in one of the rooms\n        self.created_rmid_msg_path = (\n            \"rooms/%s/send/m.room.message/a1\" % (self.created_rmid)\n        ).encode(\"ascii\")\n        request, channel = self.make_request(\n            \"PUT\", self.created_rmid_msg_path, b'{\"msgtype\":\"m.text\",\"body\":\"test msg\"}'\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        # set topic for public room\n        request, channel = self.make_request(\n            \"PUT\",\n            (\"rooms/%s/state/m.room.topic\" % self.created_public_rmid).encode(\"ascii\"),\n            b'{\"topic\":\"Public Room Topic\"}',\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        # auth as user_id now\n        self.helper.auth_user_id = self.user_id\n\n    def test_can_do_action(self):\n        msg_content = b'{\"msgtype\":\"m.text\",\"body\":\"hello\"}'\n\n        seq = iter(range(100))\n\n        def send_msg_path():\n            return \"/rooms/%s/send/m.room.message/mid%s\" % (\n                self.created_rmid,\n                str(next(seq)),\n            )\n\n        # send message in uncreated room, expect 403\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/send/m.room.message/mid2\" % (self.uncreated_rmid,),\n            msg_content,\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room not joined (no state), expect 403\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and invited, expect 403\n        self.helper.invite(\n            room=self.created_rmid, src=self.rmcreator_id, targ=self.user_id\n        )\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and joined, expect 200\n        self.helper.join(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and left, expect 403\n        self.helper.leave(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_topic_perms(self):\n        topic_content = b'{\"topic\":\"My Topic Name\"}'\n        topic_path = \"/rooms/%s/state/m.room.topic\" % self.created_rmid\n\n        # set/get topic in uncreated room, expect 403\n        request, channel = self.make_request(\n            \"PUT\", \"/rooms/%s/state/m.room.topic\" % self.uncreated_rmid, topic_content\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/state/m.room.topic\" % self.uncreated_rmid\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set/get topic in created PRIVATE room not joined, expect 403\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set topic in created PRIVATE room and invited, expect 403\n        self.helper.invite(\n            room=self.created_rmid, src=self.rmcreator_id, targ=self.user_id\n        )\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # get topic in created PRIVATE room and invited, expect 403\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set/get topic in created PRIVATE room and joined, expect 200\n        self.helper.join(room=self.created_rmid, user=self.user_id)\n\n        # Only room ops can set topic by default\n        self.helper.auth_user_id = self.rmcreator_id\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.helper.auth_user_id = self.user_id\n\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(topic_content.decode(\"utf8\")), channel.json_body)\n\n        # set/get topic in created PRIVATE room and left, expect 403\n        self.helper.leave(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # get topic in PUBLIC room, not joined, expect 403\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/state/m.room.topic\" % self.created_public_rmid\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set topic in PUBLIC room, not joined, expect 403\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/state/m.room.topic\" % self.created_public_rmid,\n            topic_content,\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def _test_get_membership(self, room=None, members=[], expect_code=None):\n        for member in members:\n            path = \"/rooms/%s/state/m.room.member/%s\" % (room, member)\n            request, channel = self.make_request(\"GET\", path)\n            self.assertEquals(expect_code, channel.code)\n\n    def test_membership_basic_room_perms(self):\n        # === room does not exist ===\n        room = self.uncreated_rmid\n        # get membership of self, get membership of other, uncreated room\n        # expect all 403s\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # trying to invite people to this room should 403\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.rmcreator_id, expect_code=403\n        )\n\n        # set [invite/join/left] of self, set [invite/join/left] of other,\n        # expect all 404s because room doesn't exist on any server\n        for usr in [self.user_id, self.rmcreator_id]:\n            self.helper.join(room=room, user=usr, expect_code=404)\n            self.helper.leave(room=room, user=usr, expect_code=404)\n\n    def test_membership_private_room_perms(self):\n        room = self.created_rmid\n        # get membership of self, get membership of other, private room + invite\n        # expect all 403s\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # get membership of self, get membership of other, private room + joined\n        # expect all 200s\n        self.helper.join(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n        # get membership of self, get membership of other, private room + left\n        # expect all 200s\n        self.helper.leave(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n    def test_membership_public_room_perms(self):\n        room = self.created_public_rmid\n        # get membership of self, get membership of other, public room + invite\n        # expect 403\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # get membership of self, get membership of other, public room + joined\n        # expect all 200s\n        self.helper.join(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n        # get membership of self, get membership of other, public room + left\n        # expect 200.\n        self.helper.leave(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n    def test_invited_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n\n        # set [invite/join/left] of other user, expect 403s\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.rmcreator_id, expect_code=403\n        )\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.JOIN,\n            expect_code=403,\n        )\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )\n\n    def test_joined_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self.helper.join(room=room, user=self.user_id)\n\n        # set invited of self, expect 403\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.user_id, expect_code=403\n        )\n\n        # set joined of self, expect 200 (NOOP)\n        self.helper.join(room=room, user=self.user_id)\n\n        other = \"@burgundy:red\"\n        # set invited of other, expect 200\n        self.helper.invite(room=room, src=self.user_id, targ=other, expect_code=200)\n\n        # set joined of other, expect 403\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=other,\n            membership=Membership.JOIN,\n            expect_code=403,\n        )\n\n        # set left of other, expect 403\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=other,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )\n\n        # set left of self, expect 200\n        self.helper.leave(room=room, user=self.user_id)\n\n    def test_leave_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self.helper.join(room=room, user=self.user_id)\n        self.helper.leave(room=room, user=self.user_id)\n\n        # set [invite/join/left] of self, set [invite/join/left] of other,\n        # expect all 403s\n        for usr in [self.user_id, self.rmcreator_id]:\n            self.helper.change_membership(\n                room=room,\n                src=self.user_id,\n                targ=usr,\n                membership=Membership.INVITE,\n                expect_code=403,\n            )\n\n            self.helper.change_membership(\n                room=room,\n                src=self.user_id,\n                targ=usr,\n                membership=Membership.JOIN,\n                expect_code=403,\n            )\n\n        # It is always valid to LEAVE if you've already left (currently.)\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )", "target": 0}, {"function": "class RoomsMemberListTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/members/list REST events.\"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def test_get_member_list(self):\n        room_id = self.helper.create_room_as(self.user_id)\n        request, channel = self.make_request(\"GET\", \"/rooms/%s/members\" % room_id)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_no_room(self):\n        request, channel = self.make_request(\"GET\", \"/rooms/roomdoesnotexist/members\")\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_no_permission(self):\n        room_id = self.helper.create_room_as(\"@some_other_guy:red\")\n        request, channel = self.make_request(\"GET\", \"/rooms/%s/members\" % room_id)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_mixed_memberships(self):\n        room_creator = \"@some_other_guy:red\"\n        room_id = self.helper.create_room_as(room_creator)\n        room_path = \"/rooms/%s/members\" % room_id\n        self.helper.invite(room=room_id, src=room_creator, targ=self.user_id)\n        # can't see list if you're just invited.\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        self.helper.join(room=room_id, user=self.user_id)\n        # can see list now joined\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        self.helper.leave(room=room_id, user=self.user_id)\n        # can see old list once left\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])", "target": 0}, {"function": "class RoomsCreateTestCase(RoomBase):\n    \"\"\" Tests /rooms and /rooms/$room_id REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def test_post_room_no_keys(self):\n        # POST with no config keys, expect new room id\n        request, channel = self.make_request(\"POST\", \"/createRoom\", \"{}\")\n\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_visibility_key(self):\n        # POST with visibility config key, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"visibility\":\"private\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_custom_key(self):\n        # POST with custom config keys, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"custom\":\"stuff\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_known_and_unknown_keys(self):\n        # POST with custom + known config keys, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"visibility\":\"private\",\"custom\":\"things\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_invalid_content(self):\n        # POST with invalid content / paths, expect 400\n        request, channel = self.make_request(\"POST\", \"/createRoom\", b'{\"visibili')\n        self.assertEquals(400, channel.code)\n\n        request, channel = self.make_request(\"POST\", \"/createRoom\", b'[\"hello\"]')\n        self.assertEquals(400, channel.code)\n\n    def test_post_room_invitees_invalid_mxid(self):\n        # POST with invalid invitee, see https://github.com/matrix-org/synapse/issues/4088\n        # Note the trailing space in the MXID here!\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"invite\":[\"@alice:example.com \"]}'\n        )\n        self.assertEquals(400, channel.code)", "target": 0}, {"function": "class RoomTopicTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/topic REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        # create the room\n        self.room_id = self.helper.create_room_as(self.user_id)\n        self.path = \"/rooms/%s/state/m.room.topic\" % (self.room_id,)\n\n    def test_invalid_puts(self):\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", self.path, \"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, '{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, '{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", self.path, '[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, \"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, \"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # valid key, wrong type\n        content = '{\"topic\":[\"Topic name\"]}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_topic(self):\n        # nothing should be there\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(404, channel.code, msg=channel.result[\"body\"])\n\n        # valid put\n        content = '{\"topic\":\"Topic name\"}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # valid get\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(content), channel.json_body)\n\n    def test_rooms_topic_with_extra_keys(self):\n        # valid put with extra keys\n        content = '{\"topic\":\"Seasons\",\"subtopic\":\"Summer\"}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # valid get\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(content), channel.json_body)", "target": 0}, {"function": "class RoomMemberStateTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/members/$user_id/state REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_invalid_puts(self):\n        path = \"/rooms/%s/state/m.room.member/%s\" % (self.room_id, self.user_id)\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", path, \"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, '{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, '{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", path, b'[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, \"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, \"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # valid keys, wrong types\n        content = '{\"membership\":[\"%s\",\"%s\",\"%s\"]}' % (\n            Membership.INVITE,\n            Membership.JOIN,\n            Membership.LEAVE,\n        )\n        request, channel = self.make_request(\"PUT\", path, content.encode(\"ascii\"))\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_members_self(self):\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.user_id,\n        )\n\n        # valid join message (NOOP since we made the room)\n        content = '{\"membership\":\"%s\"}' % Membership.JOIN\n        request, channel = self.make_request(\"PUT\", path, content.encode(\"ascii\"))\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        expected_response = {\"membership\": Membership.JOIN}\n        self.assertEquals(expected_response, channel.json_body)\n\n    def test_rooms_members_other(self):\n        self.other_id = \"@zzsid1:red\"\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.other_id,\n        )\n\n        # valid invite message\n        content = '{\"membership\":\"%s\"}' % Membership.INVITE\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assertEquals(json.loads(content), channel.json_body)\n\n    def test_rooms_members_other_custom_keys(self):\n        self.other_id = \"@zzsid1:red\"\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.other_id,\n        )\n\n        # valid invite message with custom key\n        content = '{\"membership\":\"%s\",\"invite_text\":\"%s\"}' % (\n            Membership.INVITE,\n            \"Join us!\",\n        )\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assertEquals(json.loads(content), channel.json_body)", "target": 0}, {"function": "class RoomJoinRatelimitTestCase(RoomBase):\n    user_id = \"@sid1:red\"\n\n    servlets = [\n        profile.register_servlets,\n        room.register_servlets,\n    ]\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit(self):\n        \"\"\"Tests that local joins are actually rate-limited.\"\"\"\n        for i in range(3):\n            self.helper.create_room_as(self.user_id)\n\n        self.helper.create_room_as(self.user_id, expect_code=429)\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit_profile_change(self):\n        \"\"\"Tests that sending a profile update into all of the user's joined rooms isn't\n        rate-limited by the rate-limiter on joins.\"\"\"\n\n        # Create and join as many rooms as the rate-limiting config allows in a second.\n        room_ids = [\n            self.helper.create_room_as(self.user_id),\n            self.helper.create_room_as(self.user_id),\n            self.helper.create_room_as(self.user_id),\n        ]\n        # Let some time for the rate-limiter to forget about our multi-join.\n        self.reactor.advance(2)\n        # Add one to make sure we're joined to more rooms than the config allows us to\n        # join in a second.\n        room_ids.append(self.helper.create_room_as(self.user_id))\n\n        # Create a profile for the user, since it hasn't been done on registration.\n        store = self.hs.get_datastore()\n        self.get_success(\n            store.create_profile(UserID.from_string(self.user_id).localpart)\n        )\n\n        # Update the display name for the user.\n        path = \"/_matrix/client/r0/profile/%s/displayname\" % self.user_id\n        request, channel = self.make_request(\"PUT\", path, {\"displayname\": \"John Doe\"})\n        self.assertEquals(channel.code, 200, channel.json_body)\n\n        # Check that all the rooms have been sent a profile update into.\n        for room_id in room_ids:\n            path = \"/_matrix/client/r0/rooms/%s/state/m.room.member/%s\" % (\n                room_id,\n                self.user_id,\n            )\n\n            request, channel = self.make_request(\"GET\", path)\n            self.assertEquals(channel.code, 200)\n\n            self.assertIn(\"displayname\", channel.json_body)\n            self.assertEquals(channel.json_body[\"displayname\"], \"John Doe\")\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit_idempotent(self):\n        \"\"\"Tests that the room join endpoints remain idempotent despite rate-limiting\n        on room joins.\"\"\"\n        room_id = self.helper.create_room_as(self.user_id)\n\n        # Let's test both paths to be sure.\n        paths_to_test = [\n            \"/_matrix/client/r0/rooms/%s/join\",\n            \"/_matrix/client/r0/join/%s\",\n        ]\n\n        for path in paths_to_test:\n            # Make sure we send more requests than the rate-limiting config would allow\n            # if all of these requests ended up joining the user to a room.\n            for i in range(4):\n                request, channel = self.make_request(\"POST\", path % room_id, {})\n                self.assertEquals(channel.code, 200)", "target": 0}, {"function": "class RoomMessagesTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/messages/$user_id/$msg_id REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_invalid_puts(self):\n        path = \"/rooms/%s/send/m.room.message/mid1\" % (urlparse.quote(self.room_id))\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", path, b\"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b'{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b'{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", path, b'[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b\"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b\"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_messages_sent(self):\n        path = \"/rooms/%s/send/m.room.message/mid1\" % (urlparse.quote(self.room_id))\n\n        content = b'{\"body\":\"test\",\"msgtype\":{\"type\":\"a\"}}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # custom message types\n        content = b'{\"body\":\"test\",\"msgtype\":\"test.custom.text\"}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # m.text message type\n        path = \"/rooms/%s/send/m.room.message/mid2\" % (urlparse.quote(self.room_id))\n        content = b'{\"body\":\"test2\",\"msgtype\":\"m.text\"}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])", "target": 0}, {"function": "class RoomInitialSyncTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/initialSync. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        # create the room\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_initial_sync(self):\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/initialSync\" % self.room_id\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.room_id, channel.json_body[\"room_id\"])\n        self.assertEquals(\"join\", channel.json_body[\"membership\"])\n\n        # Room state is easier to assert on if we unpack it into a dict\n        state = {}\n        for event in channel.json_body[\"state\"]:\n            if \"state_key\" not in event:\n                continue\n            t = event[\"type\"]\n            if t not in state:\n                state[t] = []\n            state[t].append(event)\n\n        self.assertTrue(\"m.room.create\" in state)\n\n        self.assertTrue(\"messages\" in channel.json_body)\n        self.assertTrue(\"chunk\" in channel.json_body[\"messages\"])\n        self.assertTrue(\"end\" in channel.json_body[\"messages\"])\n\n        self.assertTrue(\"presence\" in channel.json_body)\n\n        presence_by_user = {\n            e[\"content\"][\"user_id\"]: e for e in channel.json_body[\"presence\"]\n        }\n        self.assertTrue(self.user_id in presence_by_user)\n        self.assertEquals(\"m.presence\", presence_by_user[self.user_id][\"type\"])", "target": 0}, {"function": "class RoomMessageListTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/messages REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_topo_token_is_accepted(self):\n        token = \"t1-0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/messages?access_token=x&from=%s\" % (self.room_id, token)\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"start\" in channel.json_body)\n        self.assertEquals(token, channel.json_body[\"start\"])\n        self.assertTrue(\"chunk\" in channel.json_body)\n        self.assertTrue(\"end\" in channel.json_body)\n\n    def test_stream_token_is_accepted_for_fwd_pagianation(self):\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/messages?access_token=x&from=%s\" % (self.room_id, token)\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"start\" in channel.json_body)\n        self.assertEquals(token, channel.json_body[\"start\"])\n        self.assertTrue(\"chunk\" in channel.json_body)\n        self.assertTrue(\"end\" in channel.json_body)\n\n    def test_room_messages_purge(self):\n        store = self.hs.get_datastore()\n        pagination_handler = self.hs.get_pagination_handler()\n\n        # Send a first message in the room, which will be removed by the purge.\n        first_event_id = self.helper.send(self.room_id, \"message 1\")[\"event_id\"]\n        first_token = self.get_success(\n            store.get_topological_token_for_event(first_event_id)\n        )\n        first_token_str = self.get_success(first_token.to_string(store))\n\n        # Send a second message in the room, which won't be removed, and which we'll\n        # use as the marker to purge events before.\n        second_event_id = self.helper.send(self.room_id, \"message 2\")[\"event_id\"]\n        second_token = self.get_success(\n            store.get_topological_token_for_event(second_event_id)\n        )\n        second_token_str = self.get_success(second_token.to_string(store))\n\n        # Send a third event in the room to ensure we don't fall under any edge case\n        # due to our marker being the latest forward extremity in the room.\n        self.helper.send(self.room_id, \"message 3\")\n\n        # Check that we get the first and second message when querying /messages.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                second_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 2, [event[\"content\"] for event in chunk])\n\n        # Purge every event before the second event.\n        purge_id = random_string(16)\n        pagination_handler._purges_by_id[purge_id] = PurgeStatus()\n        self.get_success(\n            pagination_handler._purge_history(\n                purge_id=purge_id,\n                room_id=self.room_id,\n                token=second_token_str,\n                delete_local_events=True,\n            )\n        )\n\n        # Check that we only get the second message through /message now that the first\n        # has been purged.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                second_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 1, [event[\"content\"] for event in chunk])\n\n        # Check that we get no event, but also no error, when querying /messages with\n        # the token that was pointing at the first event, because we don't have it\n        # anymore.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                first_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 0, [event[\"content\"] for event in chunk])", "target": 0}, {"function": "class RoomSearchTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n    user_id = True\n    hijack_auth = False\n\n    def prepare(self, reactor, clock, hs):\n\n        # Register the user who does the searching\n        self.user_id = self.register_user(\"user\", \"pass\")\n        self.access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        self.other_user_id = self.register_user(\"otheruser\", \"pass\")\n        self.other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Create a room\n        self.room = self.helper.create_room_as(self.user_id, tok=self.access_token)\n\n        # Invite the other person\n        self.helper.invite(\n            room=self.room,\n            src=self.user_id,\n            tok=self.access_token,\n            targ=self.other_user_id,\n        )\n\n        # The other user joins\n        self.helper.join(\n            room=self.room, user=self.other_user_id, tok=self.other_access_token\n        )\n\n    def test_finds_message(self):\n        \"\"\"\n        The search functionality will search for content in messages if asked to\n        do so.\n        \"\"\"\n        # The other user sends some messages\n        self.helper.send(self.room, body=\"Hi!\", tok=self.other_access_token)\n        self.helper.send(self.room, body=\"There!\", tok=self.other_access_token)\n\n        request, channel = self.make_request(\n            \"POST\",\n            \"/search?access_token=%s\" % (self.access_token,),\n            {\n                \"search_categories\": {\n                    \"room_events\": {\"keys\": [\"content.body\"], \"search_term\": \"Hi\"}\n                }\n            },\n        )\n\n        # Check we get the results we expect -- one search result, of the sent\n        # messages\n        self.assertEqual(channel.code, 200)\n        results = channel.json_body[\"search_categories\"][\"room_events\"]\n        self.assertEqual(results[\"count\"], 1)\n        self.assertEqual(results[\"results\"][0][\"result\"][\"content\"][\"body\"], \"Hi!\")\n\n        # No context was requested, so we should get none.\n        self.assertEqual(results[\"results\"][0][\"context\"], {})\n\n    def test_include_context(self):\n        \"\"\"\n        When event_context includes include_profile, profile information will be\n        included in the search response.\n        \"\"\"\n        # The other user sends some messages\n        self.helper.send(self.room, body=\"Hi!\", tok=self.other_access_token)\n        self.helper.send(self.room, body=\"There!\", tok=self.other_access_token)\n\n        request, channel = self.make_request(\n            \"POST\",\n            \"/search?access_token=%s\" % (self.access_token,),\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"keys\": [\"content.body\"],\n                        \"search_term\": \"Hi\",\n                        \"event_context\": {\"include_profile\": True},\n                    }\n                }\n            },\n        )\n\n        # Check we get the results we expect -- one search result, of the sent\n        # messages\n        self.assertEqual(channel.code, 200)\n        results = channel.json_body[\"search_categories\"][\"room_events\"]\n        self.assertEqual(results[\"count\"], 1)\n        self.assertEqual(results[\"results\"][0][\"result\"][\"content\"][\"body\"], \"Hi!\")\n\n        # We should get context info, like the two users, and the display names.\n        context = results[\"results\"][0][\"context\"]\n        self.assertEqual(len(context[\"profile_info\"].keys()), 2)\n        self.assertEqual(\n            context[\"profile_info\"][self.other_user_id][\"displayname\"], \"otheruser\"\n        )", "target": 0}, {"function": "class PublicRoomsRestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n\n        self.url = b\"/_matrix/client/r0/publicRooms\"\n\n        config = self.default_config()\n        config[\"allow_public_rooms_without_auth\"] = False\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def test_restricted_no_auth(self):\n        request, channel = self.make_request(\"GET\", self.url)\n        self.assertEqual(channel.code, 401, channel.result)\n\n    def test_restricted_auth(self):\n        self.register_user(\"user\", \"pass\")\n        tok = self.login(\"user\", \"pass\")\n\n        request, channel = self.make_request(\"GET\", self.url, access_token=tok)\n        self.assertEqual(channel.code, 200, channel.result)", "target": 0}, {"function": "class PerRoomProfilesForbiddenTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"allow_per_room_profiles\"] = False\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"test\", \"test\")\n        self.tok = self.login(\"test\", \"test\")\n\n        # Set a profile for the test user\n        self.displayname = \"test user\"\n        data = {\"displayname\": self.displayname}\n        request_data = json.dumps(data)\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/_matrix/client/r0/profile/%s/displayname\" % (self.user_id,),\n            request_data,\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self.room_id = self.helper.create_room_as(self.user_id, tok=self.tok)\n\n    def test_per_room_profile_forbidden(self):\n        data = {\"membership\": \"join\", \"displayname\": \"other test user\"}\n        request_data = json.dumps(data)\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/_matrix/client/r0/rooms/%s/state/m.room.member/%s\"\n            % (self.room_id, self.user_id),\n            request_data,\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n        event_id = channel.json_body[\"event_id\"]\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/r0/rooms/%s/event/%s\" % (self.room_id, event_id),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        res_displayname = channel.json_body[\"content\"][\"displayname\"]\n        self.assertEqual(res_displayname, self.displayname, channel.result)", "target": 0}, {"function": "class RoomMembershipReasonTestCase(unittest.HomeserverTestCase):\n    \"\"\"Tests that clients can add a \"reason\" field to membership events and\n    that they get correctly added to the generated events and propagated.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.creator = self.register_user(\"creator\", \"test\")\n        self.creator_tok = self.login(\"creator\", \"test\")\n\n        self.second_user_id = self.register_user(\"second\", \"test\")\n        self.second_tok = self.login(\"second\", \"test\")\n\n        self.room_id = self.helper.create_room_as(self.creator, tok=self.creator_tok)\n\n    def test_join_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/join\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_leave_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/leave\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_kick_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/kick\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_ban_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/ban\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_unban_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/unban\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_invite_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/invite\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_reject_invite_reason(self):\n        self.helper.invite(\n            self.room_id,\n            src=self.creator,\n            targ=self.second_user_id,\n            tok=self.creator_tok,\n        )\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/leave\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def _check_for_reason(self, reason):\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/r0/rooms/{}/state/m.room.member/{}\".format(\n                self.room_id, self.second_user_id\n            ),\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        event_content = channel.json_body\n\n        self.assertEqual(event_content.get(\"reason\"), reason, channel.result)", "target": 0}, {"function": "class LabelsTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    # Filter that should only catch messages with the label \"#fun\".\n    FILTER_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.labels\": [\"#fun\"],\n    }\n    # Filter that should only catch messages without the label \"#fun\".\n    FILTER_NOT_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.not_labels\": [\"#fun\"],\n    }\n    # Filter that should only catch messages with the label \"#work\" but without the label\n    # \"#notfun\".\n    FILTER_LABELS_NOT_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.labels\": [\"#work\"],\n        \"org.matrix.not_labels\": [\"#notfun\"],\n    }\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"test\", \"test\")\n        self.tok = self.login(\"test\", \"test\")\n        self.room_id = self.helper.create_room_as(self.user_id, tok=self.tok)\n\n    def test_context_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /context request.\"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 1, [event[\"content\"] for event in events_before]\n        )\n        self.assertEqual(\n            events_before[0][\"content\"][\"body\"], \"with right label\", events_before[0]\n        )\n\n        events_after = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_after), 1, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with right label\", events_after[0]\n        )\n\n    def test_context_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /context request.\"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_NOT_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 1, [event[\"content\"] for event in events_before]\n        )\n        self.assertEqual(\n            events_before[0][\"content\"][\"body\"], \"without label\", events_before[0]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(\n            len(events_after), 2, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with wrong label\", events_after[0]\n        )\n        self.assertEqual(\n            events_after[1][\"content\"][\"body\"], \"with two wrong labels\", events_after[1]\n        )\n\n    def test_context_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /context request.\n        \"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_LABELS_NOT_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 0, [event[\"content\"] for event in events_before]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(\n            len(events_after), 1, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with wrong label\", events_after[0]\n        )\n\n    def test_messages_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /messages request.\"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (self.room_id, self.tok, token, json.dumps(self.FILTER_LABELS)),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 2, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"with right label\", events[0])\n        self.assertEqual(events[1][\"content\"][\"body\"], \"with right label\", events[1])\n\n    def test_messages_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /messages request.\"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (self.room_id, self.tok, token, json.dumps(self.FILTER_NOT_LABELS)),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 4, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"without label\", events[0])\n        self.assertEqual(events[1][\"content\"][\"body\"], \"without label\", events[1])\n        self.assertEqual(events[2][\"content\"][\"body\"], \"with wrong label\", events[2])\n        self.assertEqual(\n            events[3][\"content\"][\"body\"], \"with two wrong labels\", events[3]\n        )\n\n    def test_messages_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /messages request.\n        \"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (\n                self.room_id,\n                self.tok,\n                token,\n                json.dumps(self.FILTER_LABELS_NOT_LABELS),\n            ),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 1, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"with wrong label\", events[0])\n\n    def test_search_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /search request.\"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 2, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"with right label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[1][\"result\"][\"content\"][\"body\"],\n            \"with right label\",\n            results[1][\"result\"][\"content\"][\"body\"],\n        )\n\n    def test_search_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /search request.\"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_NOT_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 4, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"without label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[1][\"result\"][\"content\"][\"body\"],\n            \"without label\",\n            results[1][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[2][\"result\"][\"content\"][\"body\"],\n            \"with wrong label\",\n            results[2][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[3][\"result\"][\"content\"][\"body\"],\n            \"with two wrong labels\",\n            results[3][\"result\"][\"content\"][\"body\"],\n        )\n\n    def test_search_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /search request.\n        \"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_LABELS_NOT_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 1, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"with wrong label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n\n    def _send_labelled_messages_in_room(self):\n        \"\"\"Sends several messages to a room with different labels (or without any) to test\n        filtering by label.\n        Returns:\n            The ID of the event to use if we're testing filtering on /context.\n        \"\"\"\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with right label\",\n                EventContentFields.LABELS: [\"#fun\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\"msgtype\": \"m.text\", \"body\": \"without label\"},\n            tok=self.tok,\n        )\n\n        res = self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\"msgtype\": \"m.text\", \"body\": \"without label\"},\n            tok=self.tok,\n        )\n        # Return this event's ID when we test filtering in /context requests.\n        event_id = res[\"event_id\"]\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with wrong label\",\n                EventContentFields.LABELS: [\"#work\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with two wrong labels\",\n                EventContentFields.LABELS: [\"#work\", \"#notfun\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with right label\",\n                EventContentFields.LABELS: [\"#fun\"],\n            },\n            tok=self.tok,\n        )\n\n        return event_id", "target": 0}, {"function": "class ContextTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        account.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"user\", \"password\")\n        self.tok = self.login(\"user\", \"password\")\n        self.room_id = self.helper.create_room_as(\n            self.user_id, tok=self.tok, is_public=False\n        )\n\n        self.other_user_id = self.register_user(\"user2\", \"password\")\n        self.other_tok = self.login(\"user2\", \"password\")\n\n        self.helper.invite(self.room_id, self.user_id, self.other_user_id, tok=self.tok)\n        self.helper.join(self.room_id, self.other_user_id, tok=self.other_tok)\n\n    def test_erased_sender(self):\n        \"\"\"Test that an erasure request results in the requester's events being hidden\n        from any new member of the room.\n        \"\"\"\n\n        # Send a bunch of events in the room.\n\n        self.helper.send(self.room_id, \"message 1\", tok=self.tok)\n        self.helper.send(self.room_id, \"message 2\", tok=self.tok)\n        event_id = self.helper.send(self.room_id, \"message 3\", tok=self.tok)[\"event_id\"]\n        self.helper.send(self.room_id, \"message 4\", tok=self.tok)\n        self.helper.send(self.room_id, \"message 5\", tok=self.tok)\n\n        # Check that we can still see the messages before the erasure request.\n\n        request, channel = self.make_request(\n            \"GET\",\n            '/rooms/%s/context/%s?filter={\"types\":[\"m.room.message\"]}'\n            % (self.room_id, event_id),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(len(events_before), 2, events_before)\n        self.assertEqual(\n            events_before[0].get(\"content\", {}).get(\"body\"),\n            \"message 2\",\n            events_before[0],\n        )\n        self.assertEqual(\n            events_before[1].get(\"content\", {}).get(\"body\"),\n            \"message 1\",\n            events_before[1],\n        )\n\n        self.assertEqual(\n            channel.json_body[\"event\"].get(\"content\", {}).get(\"body\"),\n            \"message 3\",\n            channel.json_body[\"event\"],\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(len(events_after), 2, events_after)\n        self.assertEqual(\n            events_after[0].get(\"content\", {}).get(\"body\"),\n            \"message 4\",\n            events_after[0],\n        )\n        self.assertEqual(\n            events_after[1].get(\"content\", {}).get(\"body\"),\n            \"message 5\",\n            events_after[1],\n        )\n\n        # Deactivate the first account and erase the user's data.\n\n        deactivate_account_handler = self.hs.get_deactivate_account_handler()\n        self.get_success(\n            deactivate_account_handler.deactivate_account(self.user_id, erase_data=True)\n        )\n\n        # Invite another user in the room. This is needed because messages will be\n        # pruned only if the user wasn't a member of the room when the messages were\n        # sent.\n\n        invited_user_id = self.register_user(\"user3\", \"password\")\n        invited_tok = self.login(\"user3\", \"password\")\n\n        self.helper.invite(\n            self.room_id, self.other_user_id, invited_user_id, tok=self.other_tok\n        )\n        self.helper.join(self.room_id, invited_user_id, tok=invited_tok)\n\n        # Check that a user that joined the room after the erasure request can't see\n        # the messages anymore.\n\n        request, channel = self.make_request(\n            \"GET\",\n            '/rooms/%s/context/%s?filter={\"types\":[\"m.room.message\"]}'\n            % (self.room_id, event_id),\n            access_token=invited_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(len(events_before), 2, events_before)\n        self.assertDictEqual(events_before[0].get(\"content\"), {}, events_before[0])\n        self.assertDictEqual(events_before[1].get(\"content\"), {}, events_before[1])\n\n        self.assertDictEqual(\n            channel.json_body[\"event\"].get(\"content\"), {}, channel.json_body[\"event\"]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(len(events_after), 2, events_after)\n        self.assertDictEqual(events_after[0].get(\"content\"), {}, events_after[0])\n        self.assertEqual(events_after[1].get(\"content\"), {}, events_after[1])", "target": 0}, {"function": "class RoomAliasListTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        directory.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.room_owner = self.register_user(\"room_owner\", \"test\")\n        self.room_owner_tok = self.login(\"room_owner\", \"test\")\n\n        self.room_id = self.helper.create_room_as(\n            self.room_owner, tok=self.room_owner_tok\n        )\n\n    def test_no_aliases(self):\n        res = self._get_aliases(self.room_owner_tok)\n        self.assertEqual(res[\"aliases\"], [])\n\n    def test_not_in_room(self):\n        self.register_user(\"user\", \"test\")\n        user_tok = self.login(\"user\", \"test\")\n        res = self._get_aliases(user_tok, expected_code=403)\n        self.assertEqual(res[\"errcode\"], \"M_FORBIDDEN\")\n\n    def test_admin_user(self):\n        alias1 = self._random_alias()\n        self._set_alias_via_directory(alias1)\n\n        self.register_user(\"user\", \"test\", admin=True)\n        user_tok = self.login(\"user\", \"test\")\n\n        res = self._get_aliases(user_tok)\n        self.assertEqual(res[\"aliases\"], [alias1])\n\n    def test_with_aliases(self):\n        alias1 = self._random_alias()\n        alias2 = self._random_alias()\n\n        self._set_alias_via_directory(alias1)\n        self._set_alias_via_directory(alias2)\n\n        res = self._get_aliases(self.room_owner_tok)\n        self.assertEqual(set(res[\"aliases\"]), {alias1, alias2})\n\n    def test_peekable_room(self):\n        alias1 = self._random_alias()\n        self._set_alias_via_directory(alias1)\n\n        self.helper.send_state(\n            self.room_id,\n            EventTypes.RoomHistoryVisibility,\n            body={\"history_visibility\": \"world_readable\"},\n            tok=self.room_owner_tok,\n        )\n\n        self.register_user(\"user\", \"test\")\n        user_tok = self.login(\"user\", \"test\")\n\n        res = self._get_aliases(user_tok)\n        self.assertEqual(res[\"aliases\"], [alias1])\n\n    def _get_aliases(self, access_token: str, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/unstable/org.matrix.msc2432/rooms/%s/aliases\"\n            % (self.room_id,),\n            access_token=access_token,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        if expected_code == 200:\n            self.assertIsInstance(res[\"aliases\"], list)\n        return res\n\n    def _random_alias(self) -> str:\n        return RoomAlias(random_string(5), self.hs.hostname).to_string()\n\n    def _set_alias_via_directory(self, alias: str, expected_code: int = 200):\n        url = \"/_matrix/client/r0/directory/room/\" + alias\n        data = {\"room_id\": self.room_id}\n        request_data = json.dumps(data)\n\n        request, channel = self.make_request(\n            \"PUT\", url, request_data, access_token=self.room_owner_tok\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)", "target": 0}, {"function": "class RoomCanonicalAliasTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        directory.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.room_owner = self.register_user(\"room_owner\", \"test\")\n        self.room_owner_tok = self.login(\"room_owner\", \"test\")\n\n        self.room_id = self.helper.create_room_as(\n            self.room_owner, tok=self.room_owner_tok\n        )\n\n        self.alias = \"#alias:test\"\n        self._set_alias_via_directory(self.alias)\n\n    def _set_alias_via_directory(self, alias: str, expected_code: int = 200):\n        url = \"/_matrix/client/r0/directory/room/\" + alias\n        data = {\"room_id\": self.room_id}\n        request_data = json.dumps(data)\n\n        request, channel = self.make_request(\n            \"PUT\", url, request_data, access_token=self.room_owner_tok\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n\n    def _get_canonical_alias(self, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"rooms/%s/state/m.room.canonical_alias\" % (self.room_id,),\n            access_token=self.room_owner_tok,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        return res\n\n    def _set_canonical_alias(self, content: str, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"PUT\",\n            \"rooms/%s/state/m.room.canonical_alias\" % (self.room_id,),\n            json.dumps(content),\n            access_token=self.room_owner_tok,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        return res\n\n    def test_canonical_alias(self):\n        \"\"\"Test a basic alias message.\"\"\"\n        # There is no canonical alias to start with.\n        self._get_canonical_alias(expected_code=404)\n\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias})\n\n        # Now remove the alias.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_alt_aliases(self):\n        \"\"\"Test a canonical alias message with alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alt_aliases\": [self.alias]})\n\n        # Now remove the alt_aliases.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_alias_alt_aliases(self):\n        \"\"\"Test a canonical alias message with an alias and alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Now remove the alias and alt_aliases.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_partial_modify(self):\n        \"\"\"Test removing only the alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Now remove the alt_aliases.\n        self._set_canonical_alias({\"alias\": self.alias})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias})\n\n    def test_add_alias(self):\n        \"\"\"Test removing only the alt_aliases.\"\"\"\n        # Create an additional alias.\n        second_alias = \"#second:test\"\n        self._set_alias_via_directory(second_alias)\n\n        # Add the canonical alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Then add the second alias.\n        self._set_canonical_alias(\n            {\"alias\": self.alias, \"alt_aliases\": [self.alias, second_alias]}\n        )\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(\n            res, {\"alias\": self.alias, \"alt_aliases\": [self.alias, second_alias]}\n        )\n\n    def test_bad_data(self):\n        \"\"\"Invalid data for alt_aliases should cause errors.\"\"\"\n        self._set_canonical_alias({\"alt_aliases\": \"@bad:test\"}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": None}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": 0}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": 1}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": False}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": True}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": {}}, expected_code=400)\n\n    def test_bad_alias(self):\n        \"\"\"An alias which does not point to the room raises a SynapseError.\"\"\"\n        self._set_canonical_alias({\"alias\": \"@unknown:test\"}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": [\"@unknown:test\"]}, expected_code=400)", "target": 0}], "function_after": [{"function": "class RoomBase(unittest.HomeserverTestCase):\n    rmcreator_id = None\n\n    servlets = [room.register_servlets, room.register_deprecated_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        self.hs = self.setup_test_homeserver(\n            \"red\", federation_http_client=None, federation_client=Mock(),\n        )\n\n        self.hs.get_federation_handler = Mock()\n        self.hs.get_federation_handler.return_value.maybe_backfill = Mock(\n            return_value=make_awaitable(None)\n        )\n\n        async def _insert_client_ip(*args, **kwargs):\n            return None\n\n        self.hs.get_datastore().insert_client_ip = _insert_client_ip\n\n        return self.hs", "target": 0}, {"function": "class RoomPermissionsTestCase(RoomBase):\n    \"\"\" Tests room permissions. \"\"\"\n\n    user_id = \"@sid1:red\"\n    rmcreator_id = \"@notme:red\"\n\n    def prepare(self, reactor, clock, hs):\n\n        self.helper.auth_user_id = self.rmcreator_id\n        # create some rooms under the name rmcreator_id\n        self.uncreated_rmid = \"!aa:test\"\n        self.created_rmid = self.helper.create_room_as(\n            self.rmcreator_id, is_public=False\n        )\n        self.created_public_rmid = self.helper.create_room_as(\n            self.rmcreator_id, is_public=True\n        )\n\n        # send a message in one of the rooms\n        self.created_rmid_msg_path = (\n            \"rooms/%s/send/m.room.message/a1\" % (self.created_rmid)\n        ).encode(\"ascii\")\n        request, channel = self.make_request(\n            \"PUT\", self.created_rmid_msg_path, b'{\"msgtype\":\"m.text\",\"body\":\"test msg\"}'\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        # set topic for public room\n        request, channel = self.make_request(\n            \"PUT\",\n            (\"rooms/%s/state/m.room.topic\" % self.created_public_rmid).encode(\"ascii\"),\n            b'{\"topic\":\"Public Room Topic\"}',\n        )\n        self.assertEquals(200, channel.code, channel.result)\n\n        # auth as user_id now\n        self.helper.auth_user_id = self.user_id\n\n    def test_can_do_action(self):\n        msg_content = b'{\"msgtype\":\"m.text\",\"body\":\"hello\"}'\n\n        seq = iter(range(100))\n\n        def send_msg_path():\n            return \"/rooms/%s/send/m.room.message/mid%s\" % (\n                self.created_rmid,\n                str(next(seq)),\n            )\n\n        # send message in uncreated room, expect 403\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/send/m.room.message/mid2\" % (self.uncreated_rmid,),\n            msg_content,\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room not joined (no state), expect 403\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and invited, expect 403\n        self.helper.invite(\n            room=self.created_rmid, src=self.rmcreator_id, targ=self.user_id\n        )\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and joined, expect 200\n        self.helper.join(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # send message in created room and left, expect 403\n        self.helper.leave(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", send_msg_path(), msg_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_topic_perms(self):\n        topic_content = b'{\"topic\":\"My Topic Name\"}'\n        topic_path = \"/rooms/%s/state/m.room.topic\" % self.created_rmid\n\n        # set/get topic in uncreated room, expect 403\n        request, channel = self.make_request(\n            \"PUT\", \"/rooms/%s/state/m.room.topic\" % self.uncreated_rmid, topic_content\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/state/m.room.topic\" % self.uncreated_rmid\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set/get topic in created PRIVATE room not joined, expect 403\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set topic in created PRIVATE room and invited, expect 403\n        self.helper.invite(\n            room=self.created_rmid, src=self.rmcreator_id, targ=self.user_id\n        )\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # get topic in created PRIVATE room and invited, expect 403\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set/get topic in created PRIVATE room and joined, expect 200\n        self.helper.join(room=self.created_rmid, user=self.user_id)\n\n        # Only room ops can set topic by default\n        self.helper.auth_user_id = self.rmcreator_id\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.helper.auth_user_id = self.user_id\n\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(topic_content.decode(\"utf8\")), channel.json_body)\n\n        # set/get topic in created PRIVATE room and left, expect 403\n        self.helper.leave(room=self.created_rmid, user=self.user_id)\n        request, channel = self.make_request(\"PUT\", topic_path, topic_content)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n        request, channel = self.make_request(\"GET\", topic_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # get topic in PUBLIC room, not joined, expect 403\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/state/m.room.topic\" % self.created_public_rmid\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        # set topic in PUBLIC room, not joined, expect 403\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/state/m.room.topic\" % self.created_public_rmid,\n            topic_content,\n        )\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def _test_get_membership(self, room=None, members=[], expect_code=None):\n        for member in members:\n            path = \"/rooms/%s/state/m.room.member/%s\" % (room, member)\n            request, channel = self.make_request(\"GET\", path)\n            self.assertEquals(expect_code, channel.code)\n\n    def test_membership_basic_room_perms(self):\n        # === room does not exist ===\n        room = self.uncreated_rmid\n        # get membership of self, get membership of other, uncreated room\n        # expect all 403s\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # trying to invite people to this room should 403\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.rmcreator_id, expect_code=403\n        )\n\n        # set [invite/join/left] of self, set [invite/join/left] of other,\n        # expect all 404s because room doesn't exist on any server\n        for usr in [self.user_id, self.rmcreator_id]:\n            self.helper.join(room=room, user=usr, expect_code=404)\n            self.helper.leave(room=room, user=usr, expect_code=404)\n\n    def test_membership_private_room_perms(self):\n        room = self.created_rmid\n        # get membership of self, get membership of other, private room + invite\n        # expect all 403s\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # get membership of self, get membership of other, private room + joined\n        # expect all 200s\n        self.helper.join(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n        # get membership of self, get membership of other, private room + left\n        # expect all 200s\n        self.helper.leave(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n    def test_membership_public_room_perms(self):\n        room = self.created_public_rmid\n        # get membership of self, get membership of other, public room + invite\n        # expect 403\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=403\n        )\n\n        # get membership of self, get membership of other, public room + joined\n        # expect all 200s\n        self.helper.join(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n        # get membership of self, get membership of other, public room + left\n        # expect 200.\n        self.helper.leave(room=room, user=self.user_id)\n        self._test_get_membership(\n            members=[self.user_id, self.rmcreator_id], room=room, expect_code=200\n        )\n\n    def test_invited_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n\n        # set [invite/join/left] of other user, expect 403s\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.rmcreator_id, expect_code=403\n        )\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.JOIN,\n            expect_code=403,\n        )\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )\n\n    def test_joined_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self.helper.join(room=room, user=self.user_id)\n\n        # set invited of self, expect 403\n        self.helper.invite(\n            room=room, src=self.user_id, targ=self.user_id, expect_code=403\n        )\n\n        # set joined of self, expect 200 (NOOP)\n        self.helper.join(room=room, user=self.user_id)\n\n        other = \"@burgundy:red\"\n        # set invited of other, expect 200\n        self.helper.invite(room=room, src=self.user_id, targ=other, expect_code=200)\n\n        # set joined of other, expect 403\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=other,\n            membership=Membership.JOIN,\n            expect_code=403,\n        )\n\n        # set left of other, expect 403\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=other,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )\n\n        # set left of self, expect 200\n        self.helper.leave(room=room, user=self.user_id)\n\n    def test_leave_permissions(self):\n        room = self.created_rmid\n        self.helper.invite(room=room, src=self.rmcreator_id, targ=self.user_id)\n        self.helper.join(room=room, user=self.user_id)\n        self.helper.leave(room=room, user=self.user_id)\n\n        # set [invite/join/left] of self, set [invite/join/left] of other,\n        # expect all 403s\n        for usr in [self.user_id, self.rmcreator_id]:\n            self.helper.change_membership(\n                room=room,\n                src=self.user_id,\n                targ=usr,\n                membership=Membership.INVITE,\n                expect_code=403,\n            )\n\n            self.helper.change_membership(\n                room=room,\n                src=self.user_id,\n                targ=usr,\n                membership=Membership.JOIN,\n                expect_code=403,\n            )\n\n        # It is always valid to LEAVE if you've already left (currently.)\n        self.helper.change_membership(\n            room=room,\n            src=self.user_id,\n            targ=self.rmcreator_id,\n            membership=Membership.LEAVE,\n            expect_code=403,\n        )", "target": 0}, {"function": "class RoomsMemberListTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/members/list REST events.\"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def test_get_member_list(self):\n        room_id = self.helper.create_room_as(self.user_id)\n        request, channel = self.make_request(\"GET\", \"/rooms/%s/members\" % room_id)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_no_room(self):\n        request, channel = self.make_request(\"GET\", \"/rooms/roomdoesnotexist/members\")\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_no_permission(self):\n        room_id = self.helper.create_room_as(\"@some_other_guy:red\")\n        request, channel = self.make_request(\"GET\", \"/rooms/%s/members\" % room_id)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n    def test_get_member_list_mixed_memberships(self):\n        room_creator = \"@some_other_guy:red\"\n        room_id = self.helper.create_room_as(room_creator)\n        room_path = \"/rooms/%s/members\" % room_id\n        self.helper.invite(room=room_id, src=room_creator, targ=self.user_id)\n        # can't see list if you're just invited.\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(403, channel.code, msg=channel.result[\"body\"])\n\n        self.helper.join(room=room_id, user=self.user_id)\n        # can see list now joined\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        self.helper.leave(room=room_id, user=self.user_id)\n        # can see old list once left\n        request, channel = self.make_request(\"GET\", room_path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])", "target": 0}, {"function": "class RoomsCreateTestCase(RoomBase):\n    \"\"\" Tests /rooms and /rooms/$room_id REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def test_post_room_no_keys(self):\n        # POST with no config keys, expect new room id\n        request, channel = self.make_request(\"POST\", \"/createRoom\", \"{}\")\n\n        self.assertEquals(200, channel.code, channel.result)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_visibility_key(self):\n        # POST with visibility config key, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"visibility\":\"private\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_custom_key(self):\n        # POST with custom config keys, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"custom\":\"stuff\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_known_and_unknown_keys(self):\n        # POST with custom + known config keys, expect new room id\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"visibility\":\"private\",\"custom\":\"things\"}'\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"room_id\" in channel.json_body)\n\n    def test_post_room_invalid_content(self):\n        # POST with invalid content / paths, expect 400\n        request, channel = self.make_request(\"POST\", \"/createRoom\", b'{\"visibili')\n        self.assertEquals(400, channel.code)\n\n        request, channel = self.make_request(\"POST\", \"/createRoom\", b'[\"hello\"]')\n        self.assertEquals(400, channel.code)\n\n    def test_post_room_invitees_invalid_mxid(self):\n        # POST with invalid invitee, see https://github.com/matrix-org/synapse/issues/4088\n        # Note the trailing space in the MXID here!\n        request, channel = self.make_request(\n            \"POST\", \"/createRoom\", b'{\"invite\":[\"@alice:example.com \"]}'\n        )\n        self.assertEquals(400, channel.code)", "target": 0}, {"function": "class RoomTopicTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/topic REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        # create the room\n        self.room_id = self.helper.create_room_as(self.user_id)\n        self.path = \"/rooms/%s/state/m.room.topic\" % (self.room_id,)\n\n    def test_invalid_puts(self):\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", self.path, \"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, '{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, '{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", self.path, '[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, \"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", self.path, \"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # valid key, wrong type\n        content = '{\"topic\":[\"Topic name\"]}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_topic(self):\n        # nothing should be there\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(404, channel.code, msg=channel.result[\"body\"])\n\n        # valid put\n        content = '{\"topic\":\"Topic name\"}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # valid get\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(content), channel.json_body)\n\n    def test_rooms_topic_with_extra_keys(self):\n        # valid put with extra keys\n        content = '{\"topic\":\"Seasons\",\"subtopic\":\"Summer\"}'\n        request, channel = self.make_request(\"PUT\", self.path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # valid get\n        request, channel = self.make_request(\"GET\", self.path)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assert_dict(json.loads(content), channel.json_body)", "target": 0}, {"function": "class RoomMemberStateTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/members/$user_id/state REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_invalid_puts(self):\n        path = \"/rooms/%s/state/m.room.member/%s\" % (self.room_id, self.user_id)\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", path, \"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, '{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, '{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", path, b'[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, \"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, \"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # valid keys, wrong types\n        content = '{\"membership\":[\"%s\",\"%s\",\"%s\"]}' % (\n            Membership.INVITE,\n            Membership.JOIN,\n            Membership.LEAVE,\n        )\n        request, channel = self.make_request(\"PUT\", path, content.encode(\"ascii\"))\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_members_self(self):\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.user_id,\n        )\n\n        # valid join message (NOOP since we made the room)\n        content = '{\"membership\":\"%s\"}' % Membership.JOIN\n        request, channel = self.make_request(\"PUT\", path, content.encode(\"ascii\"))\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        expected_response = {\"membership\": Membership.JOIN}\n        self.assertEquals(expected_response, channel.json_body)\n\n    def test_rooms_members_other(self):\n        self.other_id = \"@zzsid1:red\"\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.other_id,\n        )\n\n        # valid invite message\n        content = '{\"membership\":\"%s\"}' % Membership.INVITE\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assertEquals(json.loads(content), channel.json_body)\n\n    def test_rooms_members_other_custom_keys(self):\n        self.other_id = \"@zzsid1:red\"\n        path = \"/rooms/%s/state/m.room.member/%s\" % (\n            urlparse.quote(self.room_id),\n            self.other_id,\n        )\n\n        # valid invite message with custom key\n        content = '{\"membership\":\"%s\",\"invite_text\":\"%s\"}' % (\n            Membership.INVITE,\n            \"Join us!\",\n        )\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"GET\", path, None)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n        self.assertEquals(json.loads(content), channel.json_body)", "target": 0}, {"function": "class RoomJoinRatelimitTestCase(RoomBase):\n    user_id = \"@sid1:red\"\n\n    servlets = [\n        profile.register_servlets,\n        room.register_servlets,\n    ]\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit(self):\n        \"\"\"Tests that local joins are actually rate-limited.\"\"\"\n        for i in range(3):\n            self.helper.create_room_as(self.user_id)\n\n        self.helper.create_room_as(self.user_id, expect_code=429)\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit_profile_change(self):\n        \"\"\"Tests that sending a profile update into all of the user's joined rooms isn't\n        rate-limited by the rate-limiter on joins.\"\"\"\n\n        # Create and join as many rooms as the rate-limiting config allows in a second.\n        room_ids = [\n            self.helper.create_room_as(self.user_id),\n            self.helper.create_room_as(self.user_id),\n            self.helper.create_room_as(self.user_id),\n        ]\n        # Let some time for the rate-limiter to forget about our multi-join.\n        self.reactor.advance(2)\n        # Add one to make sure we're joined to more rooms than the config allows us to\n        # join in a second.\n        room_ids.append(self.helper.create_room_as(self.user_id))\n\n        # Create a profile for the user, since it hasn't been done on registration.\n        store = self.hs.get_datastore()\n        self.get_success(\n            store.create_profile(UserID.from_string(self.user_id).localpart)\n        )\n\n        # Update the display name for the user.\n        path = \"/_matrix/client/r0/profile/%s/displayname\" % self.user_id\n        request, channel = self.make_request(\"PUT\", path, {\"displayname\": \"John Doe\"})\n        self.assertEquals(channel.code, 200, channel.json_body)\n\n        # Check that all the rooms have been sent a profile update into.\n        for room_id in room_ids:\n            path = \"/_matrix/client/r0/rooms/%s/state/m.room.member/%s\" % (\n                room_id,\n                self.user_id,\n            )\n\n            request, channel = self.make_request(\"GET\", path)\n            self.assertEquals(channel.code, 200)\n\n            self.assertIn(\"displayname\", channel.json_body)\n            self.assertEquals(channel.json_body[\"displayname\"], \"John Doe\")\n\n    @unittest.override_config(\n        {\"rc_joins\": {\"local\": {\"per_second\": 0.5, \"burst_count\": 3}}}\n    )\n    def test_join_local_ratelimit_idempotent(self):\n        \"\"\"Tests that the room join endpoints remain idempotent despite rate-limiting\n        on room joins.\"\"\"\n        room_id = self.helper.create_room_as(self.user_id)\n\n        # Let's test both paths to be sure.\n        paths_to_test = [\n            \"/_matrix/client/r0/rooms/%s/join\",\n            \"/_matrix/client/r0/join/%s\",\n        ]\n\n        for path in paths_to_test:\n            # Make sure we send more requests than the rate-limiting config would allow\n            # if all of these requests ended up joining the user to a room.\n            for i in range(4):\n                request, channel = self.make_request(\"POST\", path % room_id, {})\n                self.assertEquals(channel.code, 200)", "target": 0}, {"function": "class RoomMessagesTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/messages/$user_id/$msg_id REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_invalid_puts(self):\n        path = \"/rooms/%s/send/m.room.message/mid1\" % (urlparse.quote(self.room_id))\n        # missing keys or invalid json\n        request, channel = self.make_request(\"PUT\", path, b\"{}\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b'{\"_name\":\"bo\"}')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b'{\"nao')\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\n            \"PUT\", path, b'[{\"_name\":\"bo\"},{\"_name\":\"jill\"}]'\n        )\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b\"text only\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        request, channel = self.make_request(\"PUT\", path, b\"\")\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n    def test_rooms_messages_sent(self):\n        path = \"/rooms/%s/send/m.room.message/mid1\" % (urlparse.quote(self.room_id))\n\n        content = b'{\"body\":\"test\",\"msgtype\":{\"type\":\"a\"}}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(400, channel.code, msg=channel.result[\"body\"])\n\n        # custom message types\n        content = b'{\"body\":\"test\",\"msgtype\":\"test.custom.text\"}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])\n\n        # m.text message type\n        path = \"/rooms/%s/send/m.room.message/mid2\" % (urlparse.quote(self.room_id))\n        content = b'{\"body\":\"test2\",\"msgtype\":\"m.text\"}'\n        request, channel = self.make_request(\"PUT\", path, content)\n        self.assertEquals(200, channel.code, msg=channel.result[\"body\"])", "target": 0}, {"function": "class RoomInitialSyncTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/initialSync. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        # create the room\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_initial_sync(self):\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/initialSync\" % self.room_id\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.room_id, channel.json_body[\"room_id\"])\n        self.assertEquals(\"join\", channel.json_body[\"membership\"])\n\n        # Room state is easier to assert on if we unpack it into a dict\n        state = {}\n        for event in channel.json_body[\"state\"]:\n            if \"state_key\" not in event:\n                continue\n            t = event[\"type\"]\n            if t not in state:\n                state[t] = []\n            state[t].append(event)\n\n        self.assertTrue(\"m.room.create\" in state)\n\n        self.assertTrue(\"messages\" in channel.json_body)\n        self.assertTrue(\"chunk\" in channel.json_body[\"messages\"])\n        self.assertTrue(\"end\" in channel.json_body[\"messages\"])\n\n        self.assertTrue(\"presence\" in channel.json_body)\n\n        presence_by_user = {\n            e[\"content\"][\"user_id\"]: e for e in channel.json_body[\"presence\"]\n        }\n        self.assertTrue(self.user_id in presence_by_user)\n        self.assertEquals(\"m.presence\", presence_by_user[self.user_id][\"type\"])", "target": 0}, {"function": "class RoomMessageListTestCase(RoomBase):\n    \"\"\" Tests /rooms/$room_id/messages REST events. \"\"\"\n\n    user_id = \"@sid1:red\"\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_topo_token_is_accepted(self):\n        token = \"t1-0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/messages?access_token=x&from=%s\" % (self.room_id, token)\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"start\" in channel.json_body)\n        self.assertEquals(token, channel.json_body[\"start\"])\n        self.assertTrue(\"chunk\" in channel.json_body)\n        self.assertTrue(\"end\" in channel.json_body)\n\n    def test_stream_token_is_accepted_for_fwd_pagianation(self):\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\", \"/rooms/%s/messages?access_token=x&from=%s\" % (self.room_id, token)\n        )\n        self.assertEquals(200, channel.code)\n        self.assertTrue(\"start\" in channel.json_body)\n        self.assertEquals(token, channel.json_body[\"start\"])\n        self.assertTrue(\"chunk\" in channel.json_body)\n        self.assertTrue(\"end\" in channel.json_body)\n\n    def test_room_messages_purge(self):\n        store = self.hs.get_datastore()\n        pagination_handler = self.hs.get_pagination_handler()\n\n        # Send a first message in the room, which will be removed by the purge.\n        first_event_id = self.helper.send(self.room_id, \"message 1\")[\"event_id\"]\n        first_token = self.get_success(\n            store.get_topological_token_for_event(first_event_id)\n        )\n        first_token_str = self.get_success(first_token.to_string(store))\n\n        # Send a second message in the room, which won't be removed, and which we'll\n        # use as the marker to purge events before.\n        second_event_id = self.helper.send(self.room_id, \"message 2\")[\"event_id\"]\n        second_token = self.get_success(\n            store.get_topological_token_for_event(second_event_id)\n        )\n        second_token_str = self.get_success(second_token.to_string(store))\n\n        # Send a third event in the room to ensure we don't fall under any edge case\n        # due to our marker being the latest forward extremity in the room.\n        self.helper.send(self.room_id, \"message 3\")\n\n        # Check that we get the first and second message when querying /messages.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                second_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 2, [event[\"content\"] for event in chunk])\n\n        # Purge every event before the second event.\n        purge_id = random_string(16)\n        pagination_handler._purges_by_id[purge_id] = PurgeStatus()\n        self.get_success(\n            pagination_handler._purge_history(\n                purge_id=purge_id,\n                room_id=self.room_id,\n                token=second_token_str,\n                delete_local_events=True,\n            )\n        )\n\n        # Check that we only get the second message through /message now that the first\n        # has been purged.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                second_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 1, [event[\"content\"] for event in chunk])\n\n        # Check that we get no event, but also no error, when querying /messages with\n        # the token that was pointing at the first event, because we don't have it\n        # anymore.\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=x&from=%s&dir=b&filter=%s\"\n            % (\n                self.room_id,\n                first_token_str,\n                json.dumps({\"types\": [EventTypes.Message]}),\n            ),\n        )\n        self.assertEqual(channel.code, 200, channel.json_body)\n\n        chunk = channel.json_body[\"chunk\"]\n        self.assertEqual(len(chunk), 0, [event[\"content\"] for event in chunk])", "target": 0}, {"function": "class RoomSearchTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n    user_id = True\n    hijack_auth = False\n\n    def prepare(self, reactor, clock, hs):\n\n        # Register the user who does the searching\n        self.user_id = self.register_user(\"user\", \"pass\")\n        self.access_token = self.login(\"user\", \"pass\")\n\n        # Register the user who sends the message\n        self.other_user_id = self.register_user(\"otheruser\", \"pass\")\n        self.other_access_token = self.login(\"otheruser\", \"pass\")\n\n        # Create a room\n        self.room = self.helper.create_room_as(self.user_id, tok=self.access_token)\n\n        # Invite the other person\n        self.helper.invite(\n            room=self.room,\n            src=self.user_id,\n            tok=self.access_token,\n            targ=self.other_user_id,\n        )\n\n        # The other user joins\n        self.helper.join(\n            room=self.room, user=self.other_user_id, tok=self.other_access_token\n        )\n\n    def test_finds_message(self):\n        \"\"\"\n        The search functionality will search for content in messages if asked to\n        do so.\n        \"\"\"\n        # The other user sends some messages\n        self.helper.send(self.room, body=\"Hi!\", tok=self.other_access_token)\n        self.helper.send(self.room, body=\"There!\", tok=self.other_access_token)\n\n        request, channel = self.make_request(\n            \"POST\",\n            \"/search?access_token=%s\" % (self.access_token,),\n            {\n                \"search_categories\": {\n                    \"room_events\": {\"keys\": [\"content.body\"], \"search_term\": \"Hi\"}\n                }\n            },\n        )\n\n        # Check we get the results we expect -- one search result, of the sent\n        # messages\n        self.assertEqual(channel.code, 200)\n        results = channel.json_body[\"search_categories\"][\"room_events\"]\n        self.assertEqual(results[\"count\"], 1)\n        self.assertEqual(results[\"results\"][0][\"result\"][\"content\"][\"body\"], \"Hi!\")\n\n        # No context was requested, so we should get none.\n        self.assertEqual(results[\"results\"][0][\"context\"], {})\n\n    def test_include_context(self):\n        \"\"\"\n        When event_context includes include_profile, profile information will be\n        included in the search response.\n        \"\"\"\n        # The other user sends some messages\n        self.helper.send(self.room, body=\"Hi!\", tok=self.other_access_token)\n        self.helper.send(self.room, body=\"There!\", tok=self.other_access_token)\n\n        request, channel = self.make_request(\n            \"POST\",\n            \"/search?access_token=%s\" % (self.access_token,),\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"keys\": [\"content.body\"],\n                        \"search_term\": \"Hi\",\n                        \"event_context\": {\"include_profile\": True},\n                    }\n                }\n            },\n        )\n\n        # Check we get the results we expect -- one search result, of the sent\n        # messages\n        self.assertEqual(channel.code, 200)\n        results = channel.json_body[\"search_categories\"][\"room_events\"]\n        self.assertEqual(results[\"count\"], 1)\n        self.assertEqual(results[\"results\"][0][\"result\"][\"content\"][\"body\"], \"Hi!\")\n\n        # We should get context info, like the two users, and the display names.\n        context = results[\"results\"][0][\"context\"]\n        self.assertEqual(len(context[\"profile_info\"].keys()), 2)\n        self.assertEqual(\n            context[\"profile_info\"][self.other_user_id][\"displayname\"], \"otheruser\"\n        )", "target": 0}, {"function": "class PublicRoomsRestrictedTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n\n        self.url = b\"/_matrix/client/r0/publicRooms\"\n\n        config = self.default_config()\n        config[\"allow_public_rooms_without_auth\"] = False\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def test_restricted_no_auth(self):\n        request, channel = self.make_request(\"GET\", self.url)\n        self.assertEqual(channel.code, 401, channel.result)\n\n    def test_restricted_auth(self):\n        self.register_user(\"user\", \"pass\")\n        tok = self.login(\"user\", \"pass\")\n\n        request, channel = self.make_request(\"GET\", self.url, access_token=tok)\n        self.assertEqual(channel.code, 200, channel.result)", "target": 0}, {"function": "class PerRoomProfilesForbiddenTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"allow_per_room_profiles\"] = False\n        self.hs = self.setup_test_homeserver(config=config)\n\n        return self.hs\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"test\", \"test\")\n        self.tok = self.login(\"test\", \"test\")\n\n        # Set a profile for the test user\n        self.displayname = \"test user\"\n        data = {\"displayname\": self.displayname}\n        request_data = json.dumps(data)\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/_matrix/client/r0/profile/%s/displayname\" % (self.user_id,),\n            request_data,\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self.room_id = self.helper.create_room_as(self.user_id, tok=self.tok)\n\n    def test_per_room_profile_forbidden(self):\n        data = {\"membership\": \"join\", \"displayname\": \"other test user\"}\n        request_data = json.dumps(data)\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/_matrix/client/r0/rooms/%s/state/m.room.member/%s\"\n            % (self.room_id, self.user_id),\n            request_data,\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n        event_id = channel.json_body[\"event_id\"]\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/r0/rooms/%s/event/%s\" % (self.room_id, event_id),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        res_displayname = channel.json_body[\"content\"][\"displayname\"]\n        self.assertEqual(res_displayname, self.displayname, channel.result)", "target": 0}, {"function": "class RoomMembershipReasonTestCase(unittest.HomeserverTestCase):\n    \"\"\"Tests that clients can add a \"reason\" field to membership events and\n    that they get correctly added to the generated events and propagated.\n    \"\"\"\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.creator = self.register_user(\"creator\", \"test\")\n        self.creator_tok = self.login(\"creator\", \"test\")\n\n        self.second_user_id = self.register_user(\"second\", \"test\")\n        self.second_tok = self.login(\"second\", \"test\")\n\n        self.room_id = self.helper.create_room_as(self.creator, tok=self.creator_tok)\n\n    def test_join_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/join\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_leave_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/leave\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_kick_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/kick\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_ban_reason(self):\n        self.helper.join(self.room_id, user=self.second_user_id, tok=self.second_tok)\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/ban\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_unban_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/unban\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_invite_reason(self):\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/invite\".format(self.room_id),\n            content={\"reason\": reason, \"user_id\": self.second_user_id},\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def test_reject_invite_reason(self):\n        self.helper.invite(\n            self.room_id,\n            src=self.creator,\n            targ=self.second_user_id,\n            tok=self.creator_tok,\n        )\n\n        reason = \"hello\"\n        request, channel = self.make_request(\n            \"POST\",\n            \"/_matrix/client/r0/rooms/{}/leave\".format(self.room_id),\n            content={\"reason\": reason},\n            access_token=self.second_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        self._check_for_reason(reason)\n\n    def _check_for_reason(self, reason):\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/r0/rooms/{}/state/m.room.member/{}\".format(\n                self.room_id, self.second_user_id\n            ),\n            access_token=self.creator_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        event_content = channel.json_body\n\n        self.assertEqual(event_content.get(\"reason\"), reason, channel.result)", "target": 0}, {"function": "class LabelsTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        profile.register_servlets,\n    ]\n\n    # Filter that should only catch messages with the label \"#fun\".\n    FILTER_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.labels\": [\"#fun\"],\n    }\n    # Filter that should only catch messages without the label \"#fun\".\n    FILTER_NOT_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.not_labels\": [\"#fun\"],\n    }\n    # Filter that should only catch messages with the label \"#work\" but without the label\n    # \"#notfun\".\n    FILTER_LABELS_NOT_LABELS = {\n        \"types\": [EventTypes.Message],\n        \"org.matrix.labels\": [\"#work\"],\n        \"org.matrix.not_labels\": [\"#notfun\"],\n    }\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"test\", \"test\")\n        self.tok = self.login(\"test\", \"test\")\n        self.room_id = self.helper.create_room_as(self.user_id, tok=self.tok)\n\n    def test_context_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /context request.\"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 1, [event[\"content\"] for event in events_before]\n        )\n        self.assertEqual(\n            events_before[0][\"content\"][\"body\"], \"with right label\", events_before[0]\n        )\n\n        events_after = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_after), 1, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with right label\", events_after[0]\n        )\n\n    def test_context_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /context request.\"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_NOT_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 1, [event[\"content\"] for event in events_before]\n        )\n        self.assertEqual(\n            events_before[0][\"content\"][\"body\"], \"without label\", events_before[0]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(\n            len(events_after), 2, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with wrong label\", events_after[0]\n        )\n        self.assertEqual(\n            events_after[1][\"content\"][\"body\"], \"with two wrong labels\", events_after[1]\n        )\n\n    def test_context_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /context request.\n        \"\"\"\n        event_id = self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/context/%s?filter=%s\"\n            % (self.room_id, event_id, json.dumps(self.FILTER_LABELS_NOT_LABELS)),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(\n            len(events_before), 0, [event[\"content\"] for event in events_before]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(\n            len(events_after), 1, [event[\"content\"] for event in events_after]\n        )\n        self.assertEqual(\n            events_after[0][\"content\"][\"body\"], \"with wrong label\", events_after[0]\n        )\n\n    def test_messages_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /messages request.\"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (self.room_id, self.tok, token, json.dumps(self.FILTER_LABELS)),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 2, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"with right label\", events[0])\n        self.assertEqual(events[1][\"content\"][\"body\"], \"with right label\", events[1])\n\n    def test_messages_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /messages request.\"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (self.room_id, self.tok, token, json.dumps(self.FILTER_NOT_LABELS)),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 4, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"without label\", events[0])\n        self.assertEqual(events[1][\"content\"][\"body\"], \"without label\", events[1])\n        self.assertEqual(events[2][\"content\"][\"body\"], \"with wrong label\", events[2])\n        self.assertEqual(\n            events[3][\"content\"][\"body\"], \"with two wrong labels\", events[3]\n        )\n\n    def test_messages_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /messages request.\n        \"\"\"\n        self._send_labelled_messages_in_room()\n\n        token = \"s0_0_0_0_0_0_0_0_0\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/rooms/%s/messages?access_token=%s&from=%s&filter=%s\"\n            % (\n                self.room_id,\n                self.tok,\n                token,\n                json.dumps(self.FILTER_LABELS_NOT_LABELS),\n            ),\n        )\n\n        events = channel.json_body[\"chunk\"]\n\n        self.assertEqual(len(events), 1, [event[\"content\"] for event in events])\n        self.assertEqual(events[0][\"content\"][\"body\"], \"with wrong label\", events[0])\n\n    def test_search_filter_labels(self):\n        \"\"\"Test that we can filter by a label on a /search request.\"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 2, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"with right label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[1][\"result\"][\"content\"][\"body\"],\n            \"with right label\",\n            results[1][\"result\"][\"content\"][\"body\"],\n        )\n\n    def test_search_filter_not_labels(self):\n        \"\"\"Test that we can filter by the absence of a label on a /search request.\"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_NOT_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 4, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"without label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[1][\"result\"][\"content\"][\"body\"],\n            \"without label\",\n            results[1][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[2][\"result\"][\"content\"][\"body\"],\n            \"with wrong label\",\n            results[2][\"result\"][\"content\"][\"body\"],\n        )\n        self.assertEqual(\n            results[3][\"result\"][\"content\"][\"body\"],\n            \"with two wrong labels\",\n            results[3][\"result\"][\"content\"][\"body\"],\n        )\n\n    def test_search_filter_labels_not_labels(self):\n        \"\"\"Test that we can filter by both a label and the absence of another label on a\n        /search request.\n        \"\"\"\n        request_data = json.dumps(\n            {\n                \"search_categories\": {\n                    \"room_events\": {\n                        \"search_term\": \"label\",\n                        \"filter\": self.FILTER_LABELS_NOT_LABELS,\n                    }\n                }\n            }\n        )\n\n        self._send_labelled_messages_in_room()\n\n        request, channel = self.make_request(\n            \"POST\", \"/search?access_token=%s\" % self.tok, request_data\n        )\n\n        results = channel.json_body[\"search_categories\"][\"room_events\"][\"results\"]\n\n        self.assertEqual(\n            len(results), 1, [result[\"result\"][\"content\"] for result in results],\n        )\n        self.assertEqual(\n            results[0][\"result\"][\"content\"][\"body\"],\n            \"with wrong label\",\n            results[0][\"result\"][\"content\"][\"body\"],\n        )\n\n    def _send_labelled_messages_in_room(self):\n        \"\"\"Sends several messages to a room with different labels (or without any) to test\n        filtering by label.\n        Returns:\n            The ID of the event to use if we're testing filtering on /context.\n        \"\"\"\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with right label\",\n                EventContentFields.LABELS: [\"#fun\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\"msgtype\": \"m.text\", \"body\": \"without label\"},\n            tok=self.tok,\n        )\n\n        res = self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\"msgtype\": \"m.text\", \"body\": \"without label\"},\n            tok=self.tok,\n        )\n        # Return this event's ID when we test filtering in /context requests.\n        event_id = res[\"event_id\"]\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with wrong label\",\n                EventContentFields.LABELS: [\"#work\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with two wrong labels\",\n                EventContentFields.LABELS: [\"#work\", \"#notfun\"],\n            },\n            tok=self.tok,\n        )\n\n        self.helper.send_event(\n            room_id=self.room_id,\n            type=EventTypes.Message,\n            content={\n                \"msgtype\": \"m.text\",\n                \"body\": \"with right label\",\n                EventContentFields.LABELS: [\"#fun\"],\n            },\n            tok=self.tok,\n        )\n\n        return event_id", "target": 0}, {"function": "class ContextTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        room.register_servlets,\n        login.register_servlets,\n        account.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.user_id = self.register_user(\"user\", \"password\")\n        self.tok = self.login(\"user\", \"password\")\n        self.room_id = self.helper.create_room_as(\n            self.user_id, tok=self.tok, is_public=False\n        )\n\n        self.other_user_id = self.register_user(\"user2\", \"password\")\n        self.other_tok = self.login(\"user2\", \"password\")\n\n        self.helper.invite(self.room_id, self.user_id, self.other_user_id, tok=self.tok)\n        self.helper.join(self.room_id, self.other_user_id, tok=self.other_tok)\n\n    def test_erased_sender(self):\n        \"\"\"Test that an erasure request results in the requester's events being hidden\n        from any new member of the room.\n        \"\"\"\n\n        # Send a bunch of events in the room.\n\n        self.helper.send(self.room_id, \"message 1\", tok=self.tok)\n        self.helper.send(self.room_id, \"message 2\", tok=self.tok)\n        event_id = self.helper.send(self.room_id, \"message 3\", tok=self.tok)[\"event_id\"]\n        self.helper.send(self.room_id, \"message 4\", tok=self.tok)\n        self.helper.send(self.room_id, \"message 5\", tok=self.tok)\n\n        # Check that we can still see the messages before the erasure request.\n\n        request, channel = self.make_request(\n            \"GET\",\n            '/rooms/%s/context/%s?filter={\"types\":[\"m.room.message\"]}'\n            % (self.room_id, event_id),\n            access_token=self.tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(len(events_before), 2, events_before)\n        self.assertEqual(\n            events_before[0].get(\"content\", {}).get(\"body\"),\n            \"message 2\",\n            events_before[0],\n        )\n        self.assertEqual(\n            events_before[1].get(\"content\", {}).get(\"body\"),\n            \"message 1\",\n            events_before[1],\n        )\n\n        self.assertEqual(\n            channel.json_body[\"event\"].get(\"content\", {}).get(\"body\"),\n            \"message 3\",\n            channel.json_body[\"event\"],\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(len(events_after), 2, events_after)\n        self.assertEqual(\n            events_after[0].get(\"content\", {}).get(\"body\"),\n            \"message 4\",\n            events_after[0],\n        )\n        self.assertEqual(\n            events_after[1].get(\"content\", {}).get(\"body\"),\n            \"message 5\",\n            events_after[1],\n        )\n\n        # Deactivate the first account and erase the user's data.\n\n        deactivate_account_handler = self.hs.get_deactivate_account_handler()\n        self.get_success(\n            deactivate_account_handler.deactivate_account(self.user_id, erase_data=True)\n        )\n\n        # Invite another user in the room. This is needed because messages will be\n        # pruned only if the user wasn't a member of the room when the messages were\n        # sent.\n\n        invited_user_id = self.register_user(\"user3\", \"password\")\n        invited_tok = self.login(\"user3\", \"password\")\n\n        self.helper.invite(\n            self.room_id, self.other_user_id, invited_user_id, tok=self.other_tok\n        )\n        self.helper.join(self.room_id, invited_user_id, tok=invited_tok)\n\n        # Check that a user that joined the room after the erasure request can't see\n        # the messages anymore.\n\n        request, channel = self.make_request(\n            \"GET\",\n            '/rooms/%s/context/%s?filter={\"types\":[\"m.room.message\"]}'\n            % (self.room_id, event_id),\n            access_token=invited_tok,\n        )\n        self.assertEqual(channel.code, 200, channel.result)\n\n        events_before = channel.json_body[\"events_before\"]\n\n        self.assertEqual(len(events_before), 2, events_before)\n        self.assertDictEqual(events_before[0].get(\"content\"), {}, events_before[0])\n        self.assertDictEqual(events_before[1].get(\"content\"), {}, events_before[1])\n\n        self.assertDictEqual(\n            channel.json_body[\"event\"].get(\"content\"), {}, channel.json_body[\"event\"]\n        )\n\n        events_after = channel.json_body[\"events_after\"]\n\n        self.assertEqual(len(events_after), 2, events_after)\n        self.assertDictEqual(events_after[0].get(\"content\"), {}, events_after[0])\n        self.assertEqual(events_after[1].get(\"content\"), {}, events_after[1])", "target": 0}, {"function": "class RoomAliasListTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        directory.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.room_owner = self.register_user(\"room_owner\", \"test\")\n        self.room_owner_tok = self.login(\"room_owner\", \"test\")\n\n        self.room_id = self.helper.create_room_as(\n            self.room_owner, tok=self.room_owner_tok\n        )\n\n    def test_no_aliases(self):\n        res = self._get_aliases(self.room_owner_tok)\n        self.assertEqual(res[\"aliases\"], [])\n\n    def test_not_in_room(self):\n        self.register_user(\"user\", \"test\")\n        user_tok = self.login(\"user\", \"test\")\n        res = self._get_aliases(user_tok, expected_code=403)\n        self.assertEqual(res[\"errcode\"], \"M_FORBIDDEN\")\n\n    def test_admin_user(self):\n        alias1 = self._random_alias()\n        self._set_alias_via_directory(alias1)\n\n        self.register_user(\"user\", \"test\", admin=True)\n        user_tok = self.login(\"user\", \"test\")\n\n        res = self._get_aliases(user_tok)\n        self.assertEqual(res[\"aliases\"], [alias1])\n\n    def test_with_aliases(self):\n        alias1 = self._random_alias()\n        alias2 = self._random_alias()\n\n        self._set_alias_via_directory(alias1)\n        self._set_alias_via_directory(alias2)\n\n        res = self._get_aliases(self.room_owner_tok)\n        self.assertEqual(set(res[\"aliases\"]), {alias1, alias2})\n\n    def test_peekable_room(self):\n        alias1 = self._random_alias()\n        self._set_alias_via_directory(alias1)\n\n        self.helper.send_state(\n            self.room_id,\n            EventTypes.RoomHistoryVisibility,\n            body={\"history_visibility\": \"world_readable\"},\n            tok=self.room_owner_tok,\n        )\n\n        self.register_user(\"user\", \"test\")\n        user_tok = self.login(\"user\", \"test\")\n\n        res = self._get_aliases(user_tok)\n        self.assertEqual(res[\"aliases\"], [alias1])\n\n    def _get_aliases(self, access_token: str, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"/_matrix/client/unstable/org.matrix.msc2432/rooms/%s/aliases\"\n            % (self.room_id,),\n            access_token=access_token,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        if expected_code == 200:\n            self.assertIsInstance(res[\"aliases\"], list)\n        return res\n\n    def _random_alias(self) -> str:\n        return RoomAlias(random_string(5), self.hs.hostname).to_string()\n\n    def _set_alias_via_directory(self, alias: str, expected_code: int = 200):\n        url = \"/_matrix/client/r0/directory/room/\" + alias\n        data = {\"room_id\": self.room_id}\n        request_data = json.dumps(data)\n\n        request, channel = self.make_request(\n            \"PUT\", url, request_data, access_token=self.room_owner_tok\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)", "target": 0}, {"function": "class RoomCanonicalAliasTestCase(unittest.HomeserverTestCase):\n    servlets = [\n        synapse.rest.admin.register_servlets_for_client_rest_resource,\n        directory.register_servlets,\n        login.register_servlets,\n        room.register_servlets,\n    ]\n\n    def prepare(self, reactor, clock, homeserver):\n        self.room_owner = self.register_user(\"room_owner\", \"test\")\n        self.room_owner_tok = self.login(\"room_owner\", \"test\")\n\n        self.room_id = self.helper.create_room_as(\n            self.room_owner, tok=self.room_owner_tok\n        )\n\n        self.alias = \"#alias:test\"\n        self._set_alias_via_directory(self.alias)\n\n    def _set_alias_via_directory(self, alias: str, expected_code: int = 200):\n        url = \"/_matrix/client/r0/directory/room/\" + alias\n        data = {\"room_id\": self.room_id}\n        request_data = json.dumps(data)\n\n        request, channel = self.make_request(\n            \"PUT\", url, request_data, access_token=self.room_owner_tok\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n\n    def _get_canonical_alias(self, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"GET\",\n            \"rooms/%s/state/m.room.canonical_alias\" % (self.room_id,),\n            access_token=self.room_owner_tok,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        return res\n\n    def _set_canonical_alias(self, content: str, expected_code: int = 200) -> JsonDict:\n        \"\"\"Calls the endpoint under test. returns the json response object.\"\"\"\n        request, channel = self.make_request(\n            \"PUT\",\n            \"rooms/%s/state/m.room.canonical_alias\" % (self.room_id,),\n            json.dumps(content),\n            access_token=self.room_owner_tok,\n        )\n        self.assertEqual(channel.code, expected_code, channel.result)\n        res = channel.json_body\n        self.assertIsInstance(res, dict)\n        return res\n\n    def test_canonical_alias(self):\n        \"\"\"Test a basic alias message.\"\"\"\n        # There is no canonical alias to start with.\n        self._get_canonical_alias(expected_code=404)\n\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias})\n\n        # Now remove the alias.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_alt_aliases(self):\n        \"\"\"Test a canonical alias message with alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alt_aliases\": [self.alias]})\n\n        # Now remove the alt_aliases.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_alias_alt_aliases(self):\n        \"\"\"Test a canonical alias message with an alias and alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Now remove the alias and alt_aliases.\n        self._set_canonical_alias({})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {})\n\n    def test_partial_modify(self):\n        \"\"\"Test removing only the alt_aliases.\"\"\"\n        # Create an alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Now remove the alt_aliases.\n        self._set_canonical_alias({\"alias\": self.alias})\n\n        # There is an alias event, but it is empty.\n        res = self._get_canonical_alias()\n        self.assertEqual(res, {\"alias\": self.alias})\n\n    def test_add_alias(self):\n        \"\"\"Test removing only the alt_aliases.\"\"\"\n        # Create an additional alias.\n        second_alias = \"#second:test\"\n        self._set_alias_via_directory(second_alias)\n\n        # Add the canonical alias.\n        self._set_canonical_alias({\"alias\": self.alias, \"alt_aliases\": [self.alias]})\n\n        # Then add the second alias.\n        self._set_canonical_alias(\n            {\"alias\": self.alias, \"alt_aliases\": [self.alias, second_alias]}\n        )\n\n        # Canonical alias now exists!\n        res = self._get_canonical_alias()\n        self.assertEqual(\n            res, {\"alias\": self.alias, \"alt_aliases\": [self.alias, second_alias]}\n        )\n\n    def test_bad_data(self):\n        \"\"\"Invalid data for alt_aliases should cause errors.\"\"\"\n        self._set_canonical_alias({\"alt_aliases\": \"@bad:test\"}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": None}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": 0}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": 1}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": False}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": True}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": {}}, expected_code=400)\n\n    def test_bad_alias(self):\n        \"\"\"An alias which does not point to the room raises a SynapseError.\"\"\"\n        self._set_canonical_alias({\"alias\": \"@unknown:test\"}, expected_code=400)\n        self._set_canonical_alias({\"alt_aliases\": [\"@unknown:test\"]}, expected_code=400)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Frest%2Fclient%2Fv1%2Ftest_typing.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests REST events for /rooms paths.\"\"\"\n\nfrom mock import Mock\n\nfrom twisted.internet import defer\n\nfrom synapse.rest.client.v1 import room\nfrom synapse.types import UserID\n\nfrom tests import unittest\n\nPATH_PREFIX = \"/_matrix/client/api/v1\"\n\n\nclass RoomTypingTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests /rooms/$room_id/typing/$user_id REST API. \"\"\"\n\n    user_id = \"@sid:red\"\n\n    user = UserID.from_string(user_id)\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        hs = self.setup_test_homeserver(\n            \"red\", federation_http_client=None, federation_client=Mock(),\n        )\n\n        self.event_source = hs.get_event_sources().sources[\"typing\"]\n\n        hs.get_federation_handler = Mock()\n\n        async def get_user_by_access_token(token=None, allow_guest=False):\n            return {\n                \"user\": UserID.from_string(self.auth_user_id),\n                \"token_id\": 1,\n                \"is_guest\": False,\n            }\n\n        hs.get_auth().get_user_by_access_token = get_user_by_access_token\n\n        async def _insert_client_ip(*args, **kwargs):\n            return None\n\n        hs.get_datastore().insert_client_ip = _insert_client_ip\n\n        def get_room_members(room_id):\n            if room_id == self.room_id:\n                return defer.succeed([self.user])\n            else:\n                return defer.succeed([])\n\n        @defer.inlineCallbacks\n        def fetch_room_distributions_into(\n            room_id, localusers=None, remotedomains=None, ignore_user=None\n        ):\n            members = yield get_room_members(room_id)\n            for member in members:\n                if ignore_user is not None and member == ignore_user:\n                    continue\n\n                if hs.is_mine(member):\n                    if localusers is not None:\n                        localusers.add(member)\n                else:\n                    if remotedomains is not None:\n                        remotedomains.add(member.domain)\n\n        hs.get_room_member_handler().fetch_room_distributions_into = (\n            fetch_room_distributions_into\n        )\n\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n        # Need another user to make notifications actually work\n        self.helper.join(self.room_id, user=\"@jim:red\")\n\n    def test_set_typing(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(from_key=0, room_ids=[self.room_id])\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": self.room_id,\n                    \"content\": {\"user_ids\": [self.user_id]},\n                }\n            ],\n        )\n\n    def test_set_not_typing(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": false}',\n        )\n        self.assertEquals(200, channel.code)\n\n    def test_typing_timeout(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n\n        self.reactor.advance(36)\n\n        self.assertEquals(self.event_source.get_current_key(), 2)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 3)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests REST events for /rooms paths.\"\"\"\n\nfrom mock import Mock\n\nfrom twisted.internet import defer\n\nfrom synapse.rest.client.v1 import room\nfrom synapse.types import UserID\n\nfrom tests import unittest\n\nPATH_PREFIX = \"/_matrix/client/api/v1\"\n\n\nclass RoomTypingTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests /rooms/$room_id/typing/$user_id REST API. \"\"\"\n\n    user_id = \"@sid:red\"\n\n    user = UserID.from_string(user_id)\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        hs = self.setup_test_homeserver(\n            \"red\", http_client=None, federation_client=Mock(),\n        )\n\n        self.event_source = hs.get_event_sources().sources[\"typing\"]\n\n        hs.get_federation_handler = Mock()\n\n        async def get_user_by_access_token(token=None, allow_guest=False):\n            return {\n                \"user\": UserID.from_string(self.auth_user_id),\n                \"token_id\": 1,\n                \"is_guest\": False,\n            }\n\n        hs.get_auth().get_user_by_access_token = get_user_by_access_token\n\n        async def _insert_client_ip(*args, **kwargs):\n            return None\n\n        hs.get_datastore().insert_client_ip = _insert_client_ip\n\n        def get_room_members(room_id):\n            if room_id == self.room_id:\n                return defer.succeed([self.user])\n            else:\n                return defer.succeed([])\n\n        @defer.inlineCallbacks\n        def fetch_room_distributions_into(\n            room_id, localusers=None, remotedomains=None, ignore_user=None\n        ):\n            members = yield get_room_members(room_id)\n            for member in members:\n                if ignore_user is not None and member == ignore_user:\n                    continue\n\n                if hs.is_mine(member):\n                    if localusers is not None:\n                        localusers.add(member)\n                else:\n                    if remotedomains is not None:\n                        remotedomains.add(member.domain)\n\n        hs.get_room_member_handler().fetch_room_distributions_into = (\n            fetch_room_distributions_into\n        )\n\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n        # Need another user to make notifications actually work\n        self.helper.join(self.room_id, user=\"@jim:red\")\n\n    def test_set_typing(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(from_key=0, room_ids=[self.room_id])\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": self.room_id,\n                    \"content\": {\"user_ids\": [self.user_id]},\n                }\n            ],\n        )\n\n    def test_set_not_typing(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": false}',\n        )\n        self.assertEquals(200, channel.code)\n\n    def test_typing_timeout(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n\n        self.reactor.advance(36)\n\n        self.assertEquals(self.event_source.get_current_key(), 2)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 3)\n", "patch": "@@ -39,7 +39,7 @@ class RoomTypingTestCase(unittest.HomeserverTestCase):\n     def make_homeserver(self, reactor, clock):\n \n         hs = self.setup_test_homeserver(\n-            \"red\", http_client=None, federation_client=Mock(),\n+            \"red\", federation_http_client=None, federation_client=Mock(),\n         )\n \n         self.event_source = hs.get_event_sources().sources[\"typing\"]", "file_path": "files/2021_2/49", "file_language": "py", "file_name": "tests/rest/client/v1/test_typing.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class RoomTypingTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests /rooms/$room_id/typing/$user_id REST API. \"\"\"\n\n    user_id = \"@sid:red\"\n\n    user = UserID.from_string(user_id)\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        hs = self.setup_test_homeserver(\n            \"red\", http_client=None, federation_client=Mock(),\n        )\n\n        self.event_source = hs.get_event_sources().sources[\"typing\"]\n\n        hs.get_federation_handler = Mock()\n\n        async def get_user_by_access_token(token=None, allow_guest=False):\n            return {\n                \"user\": UserID.from_string(self.auth_user_id),\n                \"token_id\": 1,\n                \"is_guest\": False,\n            }\n\n        hs.get_auth().get_user_by_access_token = get_user_by_access_token\n\n        async def _insert_client_ip(*args, **kwargs):\n            return None\n\n        hs.get_datastore().insert_client_ip = _insert_client_ip\n\n        def get_room_members(room_id):\n            if room_id == self.room_id:\n                return defer.succeed([self.user])\n            else:\n                return defer.succeed([])\n\n        @defer.inlineCallbacks\n        def fetch_room_distributions_into(\n            room_id, localusers=None, remotedomains=None, ignore_user=None\n        ):\n            members = yield get_room_members(room_id)\n            for member in members:\n                if ignore_user is not None and member == ignore_user:\n                    continue\n\n                if hs.is_mine(member):\n                    if localusers is not None:\n                        localusers.add(member)\n                else:\n                    if remotedomains is not None:\n                        remotedomains.add(member.domain)\n\n        hs.get_room_member_handler().fetch_room_distributions_into = (\n            fetch_room_distributions_into\n        )\n\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n        # Need another user to make notifications actually work\n        self.helper.join(self.room_id, user=\"@jim:red\")\n\n    def test_set_typing(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(from_key=0, room_ids=[self.room_id])\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": self.room_id,\n                    \"content\": {\"user_ids\": [self.user_id]},\n                }\n            ],\n        )\n\n    def test_set_not_typing(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": false}',\n        )\n        self.assertEquals(200, channel.code)\n\n    def test_typing_timeout(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n\n        self.reactor.advance(36)\n\n        self.assertEquals(self.event_source.get_current_key(), 2)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 3)", "target": 0}], "function_after": [{"function": "class RoomTypingTestCase(unittest.HomeserverTestCase):\n    \"\"\" Tests /rooms/$room_id/typing/$user_id REST API. \"\"\"\n\n    user_id = \"@sid:red\"\n\n    user = UserID.from_string(user_id)\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n\n        hs = self.setup_test_homeserver(\n            \"red\", federation_http_client=None, federation_client=Mock(),\n        )\n\n        self.event_source = hs.get_event_sources().sources[\"typing\"]\n\n        hs.get_federation_handler = Mock()\n\n        async def get_user_by_access_token(token=None, allow_guest=False):\n            return {\n                \"user\": UserID.from_string(self.auth_user_id),\n                \"token_id\": 1,\n                \"is_guest\": False,\n            }\n\n        hs.get_auth().get_user_by_access_token = get_user_by_access_token\n\n        async def _insert_client_ip(*args, **kwargs):\n            return None\n\n        hs.get_datastore().insert_client_ip = _insert_client_ip\n\n        def get_room_members(room_id):\n            if room_id == self.room_id:\n                return defer.succeed([self.user])\n            else:\n                return defer.succeed([])\n\n        @defer.inlineCallbacks\n        def fetch_room_distributions_into(\n            room_id, localusers=None, remotedomains=None, ignore_user=None\n        ):\n            members = yield get_room_members(room_id)\n            for member in members:\n                if ignore_user is not None and member == ignore_user:\n                    continue\n\n                if hs.is_mine(member):\n                    if localusers is not None:\n                        localusers.add(member)\n                else:\n                    if remotedomains is not None:\n                        remotedomains.add(member.domain)\n\n        hs.get_room_member_handler().fetch_room_distributions_into = (\n            fetch_room_distributions_into\n        )\n\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n        # Need another user to make notifications actually work\n        self.helper.join(self.room_id, user=\"@jim:red\")\n\n    def test_set_typing(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n        events = self.get_success(\n            self.event_source.get_new_events(from_key=0, room_ids=[self.room_id])\n        )\n        self.assertEquals(\n            events[0],\n            [\n                {\n                    \"type\": \"m.typing\",\n                    \"room_id\": self.room_id,\n                    \"content\": {\"user_ids\": [self.user_id]},\n                }\n            ],\n        )\n\n    def test_set_not_typing(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": false}',\n        )\n        self.assertEquals(200, channel.code)\n\n    def test_typing_timeout(self):\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 1)\n\n        self.reactor.advance(36)\n\n        self.assertEquals(self.event_source.get_current_key(), 2)\n\n        request, channel = self.make_request(\n            \"PUT\",\n            \"/rooms/%s/typing/%s\" % (self.room_id, self.user_id),\n            b'{\"typing\": true, \"timeout\": 30000}',\n        )\n        self.assertEquals(200, channel.code)\n\n        self.assertEquals(self.event_source.get_current_key(), 3)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Frest%2Fkey%2Fv2%2Ftest_remote_key_resource.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport urllib.parse\nfrom io import BytesIO, StringIO\n\nfrom mock import Mock\n\nimport signedjson.key\nfrom canonicaljson import encode_canonical_json\nfrom nacl.signing import SigningKey\nfrom signedjson.sign import sign_json\n\nfrom twisted.web.resource import NoResource\n\nfrom synapse.crypto.keyring import PerspectivesKeyFetcher\nfrom synapse.http.site import SynapseRequest\nfrom synapse.rest.key.v2 import KeyApiV2Resource\nfrom synapse.storage.keys import FetchKeyResult\nfrom synapse.util.httpresourcetree import create_resource_tree\nfrom synapse.util.stringutils import random_string\n\nfrom tests import unittest\nfrom tests.server import FakeChannel\nfrom tests.utils import default_config\n\n\nclass BaseRemoteKeyResourceTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        self.http_client = Mock()\n        return self.setup_test_homeserver(federation_http_client=self.http_client)\n\n    def create_test_resource(self):\n        return create_resource_tree(\n            {\"/_matrix/key/v2\": KeyApiV2Resource(self.hs)}, root_resource=NoResource()\n        )\n\n    def expect_outgoing_key_request(\n        self, server_name: str, signing_key: SigningKey\n    ) -> None:\n        \"\"\"\n        Tell the mock http client to expect an outgoing GET request for the given key\n        \"\"\"\n\n        async def get_json(destination, path, ignore_backoff=False, **kwargs):\n            self.assertTrue(ignore_backoff)\n            self.assertEqual(destination, server_name)\n            key_id = \"%s:%s\" % (signing_key.alg, signing_key.version)\n            self.assertEqual(\n                path, \"/_matrix/key/v2/server/%s\" % (urllib.parse.quote(key_id),)\n            )\n\n            response = {\n                \"server_name\": server_name,\n                \"old_verify_keys\": {},\n                \"valid_until_ts\": 200 * 1000,\n                \"verify_keys\": {\n                    key_id: {\n                        \"key\": signedjson.key.encode_verify_key_base64(\n                            signing_key.verify_key\n                        )\n                    }\n                },\n            }\n            sign_json(response, server_name, signing_key)\n            return response\n\n        self.http_client.get_json.side_effect = get_json\n\n\nclass RemoteKeyResourceTestCase(BaseRemoteKeyResourceTestCase):\n    def make_notary_request(self, server_name: str, key_id: str) -> dict:\n        \"\"\"Send a GET request to the test server requesting the given key.\n\n        Checks that the response is a 200 and returns the decoded json body.\n        \"\"\"\n        channel = FakeChannel(self.site, self.reactor)\n        req = SynapseRequest(channel)\n        req.content = BytesIO(b\"\")\n        req.requestReceived(\n            b\"GET\",\n            b\"/_matrix/key/v2/query/%s/%s\"\n            % (server_name.encode(\"utf-8\"), key_id.encode(\"utf-8\")),\n            b\"1.1\",\n        )\n        channel.await_result()\n        self.assertEqual(channel.code, 200)\n        resp = channel.json_body\n        return resp\n\n    def test_get_key(self):\n        \"\"\"Fetch a remote key\"\"\"\n        SERVER_NAME = \"remote.server\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        self.expect_outgoing_key_request(SERVER_NAME, testkey)\n\n        resp = self.make_notary_request(SERVER_NAME, \"ed25519:ver1\")\n        keys = resp[\"server_keys\"]\n        self.assertEqual(len(keys), 1)\n\n        self.assertIn(\"ed25519:ver1\", keys[0][\"verify_keys\"])\n        self.assertEqual(len(keys[0][\"verify_keys\"]), 1)\n\n        # it should be signed by both the origin server and the notary\n        self.assertIn(SERVER_NAME, keys[0][\"signatures\"])\n        self.assertIn(self.hs.hostname, keys[0][\"signatures\"])\n\n    def test_get_own_key(self):\n        \"\"\"Fetch our own key\"\"\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        self.expect_outgoing_key_request(self.hs.hostname, testkey)\n\n        resp = self.make_notary_request(self.hs.hostname, \"ed25519:ver1\")\n        keys = resp[\"server_keys\"]\n        self.assertEqual(len(keys), 1)\n\n        # it should be signed by both itself, and the notary signing key\n        sigs = keys[0][\"signatures\"]\n        self.assertEqual(len(sigs), 1)\n        self.assertIn(self.hs.hostname, sigs)\n        oursigs = sigs[self.hs.hostname]\n        self.assertEqual(len(oursigs), 2)\n\n        # the requested key should be present in the verify_keys section\n        self.assertIn(\"ed25519:ver1\", keys[0][\"verify_keys\"])\n\n\nclass EndToEndPerspectivesTests(BaseRemoteKeyResourceTestCase):\n    \"\"\"End-to-end tests of the perspectives fetch case\n\n    The idea here is to actually wire up a PerspectivesKeyFetcher to the notary\n    endpoint, to check that the two implementations are compatible.\n    \"\"\"\n\n    def default_config(self):\n        config = super().default_config()\n\n        # replace the signing key with our own\n        self.hs_signing_key = signedjson.key.generate_signing_key(\"kssk\")\n        strm = StringIO()\n        signedjson.key.write_signing_keys(strm, [self.hs_signing_key])\n        config[\"signing_key\"] = strm.getvalue()\n\n        return config\n\n    def prepare(self, reactor, clock, homeserver):\n        # make a second homeserver, configured to use the first one as a key notary\n        self.http_client2 = Mock()\n        config = default_config(name=\"keyclient\")\n        config[\"trusted_key_servers\"] = [\n            {\n                \"server_name\": self.hs.hostname,\n                \"verify_keys\": {\n                    \"ed25519:%s\"\n                    % (\n                        self.hs_signing_key.version,\n                    ): signedjson.key.encode_verify_key_base64(\n                        self.hs_signing_key.verify_key\n                    )\n                },\n            }\n        ]\n        self.hs2 = self.setup_test_homeserver(\n            federation_http_client=self.http_client2, config=config\n        )\n\n        # wire up outbound POST /key/v2/query requests from hs2 so that they\n        # will be forwarded to hs1\n        async def post_json(destination, path, data):\n            self.assertEqual(destination, self.hs.hostname)\n            self.assertEqual(\n                path, \"/_matrix/key/v2/query\",\n            )\n\n            channel = FakeChannel(self.site, self.reactor)\n            req = SynapseRequest(channel)\n            req.content = BytesIO(encode_canonical_json(data))\n\n            req.requestReceived(\n                b\"POST\", path.encode(\"utf-8\"), b\"1.1\",\n            )\n            channel.await_result()\n            self.assertEqual(channel.code, 200)\n            resp = channel.json_body\n            return resp\n\n        self.http_client2.post_json.side_effect = post_json\n\n    def test_get_key(self):\n        \"\"\"Fetch a key belonging to a random server\"\"\"\n        # make up a key to be fetched.\n        testkey = signedjson.key.generate_signing_key(\"abc\")\n\n        # we expect hs1 to make a regular key request to the target server\n        self.expect_outgoing_key_request(\"targetserver\", testkey)\n        keyid = \"ed25519:%s\" % (testkey.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({\"targetserver\": {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(\"targetserver\", res)\n        keyres = res[\"targetserver\"][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(testkey.verify_key),\n        )\n\n    def test_get_notary_key(self):\n        \"\"\"Fetch a key belonging to the notary server\"\"\"\n        # make up a key to be fetched. We randomise the keyid to try to get it to\n        # appear before the key server signing key sometimes (otherwise we bail out\n        # before fetching its signature)\n        testkey = signedjson.key.generate_signing_key(random_string(5))\n\n        # we expect hs1 to make a regular key request to itself\n        self.expect_outgoing_key_request(self.hs.hostname, testkey)\n        keyid = \"ed25519:%s\" % (testkey.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({self.hs.hostname: {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(self.hs.hostname, res)\n        keyres = res[self.hs.hostname][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(testkey.verify_key),\n        )\n\n    def test_get_notary_keyserver_key(self):\n        \"\"\"Fetch the notary's keyserver key\"\"\"\n        # we expect hs1 to make a regular key request to itself\n        self.expect_outgoing_key_request(self.hs.hostname, self.hs_signing_key)\n        keyid = \"ed25519:%s\" % (self.hs_signing_key.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({self.hs.hostname: {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(self.hs.hostname, res)\n        keyres = res[self.hs.hostname][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(self.hs_signing_key.verify_key),\n        )\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport urllib.parse\nfrom io import BytesIO, StringIO\n\nfrom mock import Mock\n\nimport signedjson.key\nfrom canonicaljson import encode_canonical_json\nfrom nacl.signing import SigningKey\nfrom signedjson.sign import sign_json\n\nfrom twisted.web.resource import NoResource\n\nfrom synapse.crypto.keyring import PerspectivesKeyFetcher\nfrom synapse.http.site import SynapseRequest\nfrom synapse.rest.key.v2 import KeyApiV2Resource\nfrom synapse.storage.keys import FetchKeyResult\nfrom synapse.util.httpresourcetree import create_resource_tree\nfrom synapse.util.stringutils import random_string\n\nfrom tests import unittest\nfrom tests.server import FakeChannel\nfrom tests.utils import default_config\n\n\nclass BaseRemoteKeyResourceTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        self.http_client = Mock()\n        return self.setup_test_homeserver(http_client=self.http_client)\n\n    def create_test_resource(self):\n        return create_resource_tree(\n            {\"/_matrix/key/v2\": KeyApiV2Resource(self.hs)}, root_resource=NoResource()\n        )\n\n    def expect_outgoing_key_request(\n        self, server_name: str, signing_key: SigningKey\n    ) -> None:\n        \"\"\"\n        Tell the mock http client to expect an outgoing GET request for the given key\n        \"\"\"\n\n        async def get_json(destination, path, ignore_backoff=False, **kwargs):\n            self.assertTrue(ignore_backoff)\n            self.assertEqual(destination, server_name)\n            key_id = \"%s:%s\" % (signing_key.alg, signing_key.version)\n            self.assertEqual(\n                path, \"/_matrix/key/v2/server/%s\" % (urllib.parse.quote(key_id),)\n            )\n\n            response = {\n                \"server_name\": server_name,\n                \"old_verify_keys\": {},\n                \"valid_until_ts\": 200 * 1000,\n                \"verify_keys\": {\n                    key_id: {\n                        \"key\": signedjson.key.encode_verify_key_base64(\n                            signing_key.verify_key\n                        )\n                    }\n                },\n            }\n            sign_json(response, server_name, signing_key)\n            return response\n\n        self.http_client.get_json.side_effect = get_json\n\n\nclass RemoteKeyResourceTestCase(BaseRemoteKeyResourceTestCase):\n    def make_notary_request(self, server_name: str, key_id: str) -> dict:\n        \"\"\"Send a GET request to the test server requesting the given key.\n\n        Checks that the response is a 200 and returns the decoded json body.\n        \"\"\"\n        channel = FakeChannel(self.site, self.reactor)\n        req = SynapseRequest(channel)\n        req.content = BytesIO(b\"\")\n        req.requestReceived(\n            b\"GET\",\n            b\"/_matrix/key/v2/query/%s/%s\"\n            % (server_name.encode(\"utf-8\"), key_id.encode(\"utf-8\")),\n            b\"1.1\",\n        )\n        channel.await_result()\n        self.assertEqual(channel.code, 200)\n        resp = channel.json_body\n        return resp\n\n    def test_get_key(self):\n        \"\"\"Fetch a remote key\"\"\"\n        SERVER_NAME = \"remote.server\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        self.expect_outgoing_key_request(SERVER_NAME, testkey)\n\n        resp = self.make_notary_request(SERVER_NAME, \"ed25519:ver1\")\n        keys = resp[\"server_keys\"]\n        self.assertEqual(len(keys), 1)\n\n        self.assertIn(\"ed25519:ver1\", keys[0][\"verify_keys\"])\n        self.assertEqual(len(keys[0][\"verify_keys\"]), 1)\n\n        # it should be signed by both the origin server and the notary\n        self.assertIn(SERVER_NAME, keys[0][\"signatures\"])\n        self.assertIn(self.hs.hostname, keys[0][\"signatures\"])\n\n    def test_get_own_key(self):\n        \"\"\"Fetch our own key\"\"\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        self.expect_outgoing_key_request(self.hs.hostname, testkey)\n\n        resp = self.make_notary_request(self.hs.hostname, \"ed25519:ver1\")\n        keys = resp[\"server_keys\"]\n        self.assertEqual(len(keys), 1)\n\n        # it should be signed by both itself, and the notary signing key\n        sigs = keys[0][\"signatures\"]\n        self.assertEqual(len(sigs), 1)\n        self.assertIn(self.hs.hostname, sigs)\n        oursigs = sigs[self.hs.hostname]\n        self.assertEqual(len(oursigs), 2)\n\n        # the requested key should be present in the verify_keys section\n        self.assertIn(\"ed25519:ver1\", keys[0][\"verify_keys\"])\n\n\nclass EndToEndPerspectivesTests(BaseRemoteKeyResourceTestCase):\n    \"\"\"End-to-end tests of the perspectives fetch case\n\n    The idea here is to actually wire up a PerspectivesKeyFetcher to the notary\n    endpoint, to check that the two implementations are compatible.\n    \"\"\"\n\n    def default_config(self):\n        config = super().default_config()\n\n        # replace the signing key with our own\n        self.hs_signing_key = signedjson.key.generate_signing_key(\"kssk\")\n        strm = StringIO()\n        signedjson.key.write_signing_keys(strm, [self.hs_signing_key])\n        config[\"signing_key\"] = strm.getvalue()\n\n        return config\n\n    def prepare(self, reactor, clock, homeserver):\n        # make a second homeserver, configured to use the first one as a key notary\n        self.http_client2 = Mock()\n        config = default_config(name=\"keyclient\")\n        config[\"trusted_key_servers\"] = [\n            {\n                \"server_name\": self.hs.hostname,\n                \"verify_keys\": {\n                    \"ed25519:%s\"\n                    % (\n                        self.hs_signing_key.version,\n                    ): signedjson.key.encode_verify_key_base64(\n                        self.hs_signing_key.verify_key\n                    )\n                },\n            }\n        ]\n        self.hs2 = self.setup_test_homeserver(\n            http_client=self.http_client2, config=config\n        )\n\n        # wire up outbound POST /key/v2/query requests from hs2 so that they\n        # will be forwarded to hs1\n        async def post_json(destination, path, data):\n            self.assertEqual(destination, self.hs.hostname)\n            self.assertEqual(\n                path, \"/_matrix/key/v2/query\",\n            )\n\n            channel = FakeChannel(self.site, self.reactor)\n            req = SynapseRequest(channel)\n            req.content = BytesIO(encode_canonical_json(data))\n\n            req.requestReceived(\n                b\"POST\", path.encode(\"utf-8\"), b\"1.1\",\n            )\n            channel.await_result()\n            self.assertEqual(channel.code, 200)\n            resp = channel.json_body\n            return resp\n\n        self.http_client2.post_json.side_effect = post_json\n\n    def test_get_key(self):\n        \"\"\"Fetch a key belonging to a random server\"\"\"\n        # make up a key to be fetched.\n        testkey = signedjson.key.generate_signing_key(\"abc\")\n\n        # we expect hs1 to make a regular key request to the target server\n        self.expect_outgoing_key_request(\"targetserver\", testkey)\n        keyid = \"ed25519:%s\" % (testkey.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({\"targetserver\": {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(\"targetserver\", res)\n        keyres = res[\"targetserver\"][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(testkey.verify_key),\n        )\n\n    def test_get_notary_key(self):\n        \"\"\"Fetch a key belonging to the notary server\"\"\"\n        # make up a key to be fetched. We randomise the keyid to try to get it to\n        # appear before the key server signing key sometimes (otherwise we bail out\n        # before fetching its signature)\n        testkey = signedjson.key.generate_signing_key(random_string(5))\n\n        # we expect hs1 to make a regular key request to itself\n        self.expect_outgoing_key_request(self.hs.hostname, testkey)\n        keyid = \"ed25519:%s\" % (testkey.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({self.hs.hostname: {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(self.hs.hostname, res)\n        keyres = res[self.hs.hostname][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(testkey.verify_key),\n        )\n\n    def test_get_notary_keyserver_key(self):\n        \"\"\"Fetch the notary's keyserver key\"\"\"\n        # we expect hs1 to make a regular key request to itself\n        self.expect_outgoing_key_request(self.hs.hostname, self.hs_signing_key)\n        keyid = \"ed25519:%s\" % (self.hs_signing_key.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({self.hs.hostname: {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(self.hs.hostname, res)\n        keyres = res[self.hs.hostname][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(self.hs_signing_key.verify_key),\n        )\n", "patch": "@@ -39,7 +39,7 @@\n class BaseRemoteKeyResourceTestCase(unittest.HomeserverTestCase):\n     def make_homeserver(self, reactor, clock):\n         self.http_client = Mock()\n-        return self.setup_test_homeserver(http_client=self.http_client)\n+        return self.setup_test_homeserver(federation_http_client=self.http_client)\n \n     def create_test_resource(self):\n         return create_resource_tree(\n@@ -172,7 +172,7 @@ def prepare(self, reactor, clock, homeserver):\n             }\n         ]\n         self.hs2 = self.setup_test_homeserver(\n-            http_client=self.http_client2, config=config\n+            federation_http_client=self.http_client2, config=config\n         )\n \n         # wire up outbound POST /key/v2/query requests from hs2 so that they", "file_path": "files/2021_2/50", "file_language": "py", "file_name": "tests/rest/key/v2/test_remote_key_resource.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class BaseRemoteKeyResourceTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        self.http_client = Mock()\n        return self.setup_test_homeserver(http_client=self.http_client)\n\n    def create_test_resource(self):\n        return create_resource_tree(\n            {\"/_matrix/key/v2\": KeyApiV2Resource(self.hs)}, root_resource=NoResource()\n        )\n\n    def expect_outgoing_key_request(\n        self, server_name: str, signing_key: SigningKey\n    ) -> None:\n        \"\"\"\n        Tell the mock http client to expect an outgoing GET request for the given key\n        \"\"\"\n\n        async def get_json(destination, path, ignore_backoff=False, **kwargs):\n            self.assertTrue(ignore_backoff)\n            self.assertEqual(destination, server_name)\n            key_id = \"%s:%s\" % (signing_key.alg, signing_key.version)\n            self.assertEqual(\n                path, \"/_matrix/key/v2/server/%s\" % (urllib.parse.quote(key_id),)\n            )\n\n            response = {\n                \"server_name\": server_name,\n                \"old_verify_keys\": {},\n                \"valid_until_ts\": 200 * 1000,\n                \"verify_keys\": {\n                    key_id: {\n                        \"key\": signedjson.key.encode_verify_key_base64(\n                            signing_key.verify_key\n                        )\n                    }\n                },\n            }\n            sign_json(response, server_name, signing_key)\n            return response\n\n        self.http_client.get_json.side_effect = get_json", "target": 0}, {"function": "class RemoteKeyResourceTestCase(BaseRemoteKeyResourceTestCase):\n    def make_notary_request(self, server_name: str, key_id: str) -> dict:\n        \"\"\"Send a GET request to the test server requesting the given key.\n\n        Checks that the response is a 200 and returns the decoded json body.\n        \"\"\"\n        channel = FakeChannel(self.site, self.reactor)\n        req = SynapseRequest(channel)\n        req.content = BytesIO(b\"\")\n        req.requestReceived(\n            b\"GET\",\n            b\"/_matrix/key/v2/query/%s/%s\"\n            % (server_name.encode(\"utf-8\"), key_id.encode(\"utf-8\")),\n            b\"1.1\",\n        )\n        channel.await_result()\n        self.assertEqual(channel.code, 200)\n        resp = channel.json_body\n        return resp\n\n    def test_get_key(self):\n        \"\"\"Fetch a remote key\"\"\"\n        SERVER_NAME = \"remote.server\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        self.expect_outgoing_key_request(SERVER_NAME, testkey)\n\n        resp = self.make_notary_request(SERVER_NAME, \"ed25519:ver1\")\n        keys = resp[\"server_keys\"]\n        self.assertEqual(len(keys), 1)\n\n        self.assertIn(\"ed25519:ver1\", keys[0][\"verify_keys\"])\n        self.assertEqual(len(keys[0][\"verify_keys\"]), 1)\n\n        # it should be signed by both the origin server and the notary\n        self.assertIn(SERVER_NAME, keys[0][\"signatures\"])\n        self.assertIn(self.hs.hostname, keys[0][\"signatures\"])\n\n    def test_get_own_key(self):\n        \"\"\"Fetch our own key\"\"\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        self.expect_outgoing_key_request(self.hs.hostname, testkey)\n\n        resp = self.make_notary_request(self.hs.hostname, \"ed25519:ver1\")\n        keys = resp[\"server_keys\"]\n        self.assertEqual(len(keys), 1)\n\n        # it should be signed by both itself, and the notary signing key\n        sigs = keys[0][\"signatures\"]\n        self.assertEqual(len(sigs), 1)\n        self.assertIn(self.hs.hostname, sigs)\n        oursigs = sigs[self.hs.hostname]\n        self.assertEqual(len(oursigs), 2)\n\n        # the requested key should be present in the verify_keys section\n        self.assertIn(\"ed25519:ver1\", keys[0][\"verify_keys\"])", "target": 0}, {"function": "class EndToEndPerspectivesTests(BaseRemoteKeyResourceTestCase):\n    \"\"\"End-to-end tests of the perspectives fetch case\n\n    The idea here is to actually wire up a PerspectivesKeyFetcher to the notary\n    endpoint, to check that the two implementations are compatible.\n    \"\"\"\n\n    def default_config(self):\n        config = super().default_config()\n\n        # replace the signing key with our own\n        self.hs_signing_key = signedjson.key.generate_signing_key(\"kssk\")\n        strm = StringIO()\n        signedjson.key.write_signing_keys(strm, [self.hs_signing_key])\n        config[\"signing_key\"] = strm.getvalue()\n\n        return config\n\n    def prepare(self, reactor, clock, homeserver):\n        # make a second homeserver, configured to use the first one as a key notary\n        self.http_client2 = Mock()\n        config = default_config(name=\"keyclient\")\n        config[\"trusted_key_servers\"] = [\n            {\n                \"server_name\": self.hs.hostname,\n                \"verify_keys\": {\n                    \"ed25519:%s\"\n                    % (\n                        self.hs_signing_key.version,\n                    ): signedjson.key.encode_verify_key_base64(\n                        self.hs_signing_key.verify_key\n                    )\n                },\n            }\n        ]\n        self.hs2 = self.setup_test_homeserver(\n            http_client=self.http_client2, config=config\n        )\n\n        # wire up outbound POST /key/v2/query requests from hs2 so that they\n        # will be forwarded to hs1\n        async def post_json(destination, path, data):\n            self.assertEqual(destination, self.hs.hostname)\n            self.assertEqual(\n                path, \"/_matrix/key/v2/query\",\n            )\n\n            channel = FakeChannel(self.site, self.reactor)\n            req = SynapseRequest(channel)\n            req.content = BytesIO(encode_canonical_json(data))\n\n            req.requestReceived(\n                b\"POST\", path.encode(\"utf-8\"), b\"1.1\",\n            )\n            channel.await_result()\n            self.assertEqual(channel.code, 200)\n            resp = channel.json_body\n            return resp\n\n        self.http_client2.post_json.side_effect = post_json\n\n    def test_get_key(self):\n        \"\"\"Fetch a key belonging to a random server\"\"\"\n        # make up a key to be fetched.\n        testkey = signedjson.key.generate_signing_key(\"abc\")\n\n        # we expect hs1 to make a regular key request to the target server\n        self.expect_outgoing_key_request(\"targetserver\", testkey)\n        keyid = \"ed25519:%s\" % (testkey.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({\"targetserver\": {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(\"targetserver\", res)\n        keyres = res[\"targetserver\"][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(testkey.verify_key),\n        )\n\n    def test_get_notary_key(self):\n        \"\"\"Fetch a key belonging to the notary server\"\"\"\n        # make up a key to be fetched. We randomise the keyid to try to get it to\n        # appear before the key server signing key sometimes (otherwise we bail out\n        # before fetching its signature)\n        testkey = signedjson.key.generate_signing_key(random_string(5))\n\n        # we expect hs1 to make a regular key request to itself\n        self.expect_outgoing_key_request(self.hs.hostname, testkey)\n        keyid = \"ed25519:%s\" % (testkey.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({self.hs.hostname: {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(self.hs.hostname, res)\n        keyres = res[self.hs.hostname][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(testkey.verify_key),\n        )\n\n    def test_get_notary_keyserver_key(self):\n        \"\"\"Fetch the notary's keyserver key\"\"\"\n        # we expect hs1 to make a regular key request to itself\n        self.expect_outgoing_key_request(self.hs.hostname, self.hs_signing_key)\n        keyid = \"ed25519:%s\" % (self.hs_signing_key.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({self.hs.hostname: {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(self.hs.hostname, res)\n        keyres = res[self.hs.hostname][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(self.hs_signing_key.verify_key),\n        )", "target": 0}], "function_after": [{"function": "class BaseRemoteKeyResourceTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        self.http_client = Mock()\n        return self.setup_test_homeserver(federation_http_client=self.http_client)\n\n    def create_test_resource(self):\n        return create_resource_tree(\n            {\"/_matrix/key/v2\": KeyApiV2Resource(self.hs)}, root_resource=NoResource()\n        )\n\n    def expect_outgoing_key_request(\n        self, server_name: str, signing_key: SigningKey\n    ) -> None:\n        \"\"\"\n        Tell the mock http client to expect an outgoing GET request for the given key\n        \"\"\"\n\n        async def get_json(destination, path, ignore_backoff=False, **kwargs):\n            self.assertTrue(ignore_backoff)\n            self.assertEqual(destination, server_name)\n            key_id = \"%s:%s\" % (signing_key.alg, signing_key.version)\n            self.assertEqual(\n                path, \"/_matrix/key/v2/server/%s\" % (urllib.parse.quote(key_id),)\n            )\n\n            response = {\n                \"server_name\": server_name,\n                \"old_verify_keys\": {},\n                \"valid_until_ts\": 200 * 1000,\n                \"verify_keys\": {\n                    key_id: {\n                        \"key\": signedjson.key.encode_verify_key_base64(\n                            signing_key.verify_key\n                        )\n                    }\n                },\n            }\n            sign_json(response, server_name, signing_key)\n            return response\n\n        self.http_client.get_json.side_effect = get_json", "target": 0}, {"function": "class RemoteKeyResourceTestCase(BaseRemoteKeyResourceTestCase):\n    def make_notary_request(self, server_name: str, key_id: str) -> dict:\n        \"\"\"Send a GET request to the test server requesting the given key.\n\n        Checks that the response is a 200 and returns the decoded json body.\n        \"\"\"\n        channel = FakeChannel(self.site, self.reactor)\n        req = SynapseRequest(channel)\n        req.content = BytesIO(b\"\")\n        req.requestReceived(\n            b\"GET\",\n            b\"/_matrix/key/v2/query/%s/%s\"\n            % (server_name.encode(\"utf-8\"), key_id.encode(\"utf-8\")),\n            b\"1.1\",\n        )\n        channel.await_result()\n        self.assertEqual(channel.code, 200)\n        resp = channel.json_body\n        return resp\n\n    def test_get_key(self):\n        \"\"\"Fetch a remote key\"\"\"\n        SERVER_NAME = \"remote.server\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        self.expect_outgoing_key_request(SERVER_NAME, testkey)\n\n        resp = self.make_notary_request(SERVER_NAME, \"ed25519:ver1\")\n        keys = resp[\"server_keys\"]\n        self.assertEqual(len(keys), 1)\n\n        self.assertIn(\"ed25519:ver1\", keys[0][\"verify_keys\"])\n        self.assertEqual(len(keys[0][\"verify_keys\"]), 1)\n\n        # it should be signed by both the origin server and the notary\n        self.assertIn(SERVER_NAME, keys[0][\"signatures\"])\n        self.assertIn(self.hs.hostname, keys[0][\"signatures\"])\n\n    def test_get_own_key(self):\n        \"\"\"Fetch our own key\"\"\"\n        testkey = signedjson.key.generate_signing_key(\"ver1\")\n        self.expect_outgoing_key_request(self.hs.hostname, testkey)\n\n        resp = self.make_notary_request(self.hs.hostname, \"ed25519:ver1\")\n        keys = resp[\"server_keys\"]\n        self.assertEqual(len(keys), 1)\n\n        # it should be signed by both itself, and the notary signing key\n        sigs = keys[0][\"signatures\"]\n        self.assertEqual(len(sigs), 1)\n        self.assertIn(self.hs.hostname, sigs)\n        oursigs = sigs[self.hs.hostname]\n        self.assertEqual(len(oursigs), 2)\n\n        # the requested key should be present in the verify_keys section\n        self.assertIn(\"ed25519:ver1\", keys[0][\"verify_keys\"])", "target": 0}, {"function": "class EndToEndPerspectivesTests(BaseRemoteKeyResourceTestCase):\n    \"\"\"End-to-end tests of the perspectives fetch case\n\n    The idea here is to actually wire up a PerspectivesKeyFetcher to the notary\n    endpoint, to check that the two implementations are compatible.\n    \"\"\"\n\n    def default_config(self):\n        config = super().default_config()\n\n        # replace the signing key with our own\n        self.hs_signing_key = signedjson.key.generate_signing_key(\"kssk\")\n        strm = StringIO()\n        signedjson.key.write_signing_keys(strm, [self.hs_signing_key])\n        config[\"signing_key\"] = strm.getvalue()\n\n        return config\n\n    def prepare(self, reactor, clock, homeserver):\n        # make a second homeserver, configured to use the first one as a key notary\n        self.http_client2 = Mock()\n        config = default_config(name=\"keyclient\")\n        config[\"trusted_key_servers\"] = [\n            {\n                \"server_name\": self.hs.hostname,\n                \"verify_keys\": {\n                    \"ed25519:%s\"\n                    % (\n                        self.hs_signing_key.version,\n                    ): signedjson.key.encode_verify_key_base64(\n                        self.hs_signing_key.verify_key\n                    )\n                },\n            }\n        ]\n        self.hs2 = self.setup_test_homeserver(\n            federation_http_client=self.http_client2, config=config\n        )\n\n        # wire up outbound POST /key/v2/query requests from hs2 so that they\n        # will be forwarded to hs1\n        async def post_json(destination, path, data):\n            self.assertEqual(destination, self.hs.hostname)\n            self.assertEqual(\n                path, \"/_matrix/key/v2/query\",\n            )\n\n            channel = FakeChannel(self.site, self.reactor)\n            req = SynapseRequest(channel)\n            req.content = BytesIO(encode_canonical_json(data))\n\n            req.requestReceived(\n                b\"POST\", path.encode(\"utf-8\"), b\"1.1\",\n            )\n            channel.await_result()\n            self.assertEqual(channel.code, 200)\n            resp = channel.json_body\n            return resp\n\n        self.http_client2.post_json.side_effect = post_json\n\n    def test_get_key(self):\n        \"\"\"Fetch a key belonging to a random server\"\"\"\n        # make up a key to be fetched.\n        testkey = signedjson.key.generate_signing_key(\"abc\")\n\n        # we expect hs1 to make a regular key request to the target server\n        self.expect_outgoing_key_request(\"targetserver\", testkey)\n        keyid = \"ed25519:%s\" % (testkey.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({\"targetserver\": {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(\"targetserver\", res)\n        keyres = res[\"targetserver\"][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(testkey.verify_key),\n        )\n\n    def test_get_notary_key(self):\n        \"\"\"Fetch a key belonging to the notary server\"\"\"\n        # make up a key to be fetched. We randomise the keyid to try to get it to\n        # appear before the key server signing key sometimes (otherwise we bail out\n        # before fetching its signature)\n        testkey = signedjson.key.generate_signing_key(random_string(5))\n\n        # we expect hs1 to make a regular key request to itself\n        self.expect_outgoing_key_request(self.hs.hostname, testkey)\n        keyid = \"ed25519:%s\" % (testkey.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({self.hs.hostname: {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(self.hs.hostname, res)\n        keyres = res[self.hs.hostname][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(testkey.verify_key),\n        )\n\n    def test_get_notary_keyserver_key(self):\n        \"\"\"Fetch the notary's keyserver key\"\"\"\n        # we expect hs1 to make a regular key request to itself\n        self.expect_outgoing_key_request(self.hs.hostname, self.hs_signing_key)\n        keyid = \"ed25519:%s\" % (self.hs_signing_key.version,)\n\n        fetcher = PerspectivesKeyFetcher(self.hs2)\n        d = fetcher.get_keys({self.hs.hostname: {keyid: 1000}})\n        res = self.get_success(d)\n        self.assertIn(self.hs.hostname, res)\n        keyres = res[self.hs.hostname][keyid]\n        assert isinstance(keyres, FetchKeyResult)\n        self.assertEqual(\n            signedjson.key.encode_verify_key_base64(keyres.verify_key),\n            signedjson.key.encode_verify_key_base64(self.hs_signing_key.verify_key),\n        )", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Frest%2Fmedia%2Fv1%2Ftest_media_storage.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport shutil\nimport tempfile\nfrom binascii import unhexlify\nfrom io import BytesIO\nfrom typing import Optional\nfrom urllib import parse\n\nfrom mock import Mock\n\nimport attr\nfrom parameterized import parameterized_class\nfrom PIL import Image as Image\n\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred\n\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.rest.media.v1._base import FileInfo\nfrom synapse.rest.media.v1.filepath import MediaFilePaths\nfrom synapse.rest.media.v1.media_storage import MediaStorage\nfrom synapse.rest.media.v1.storage_provider import FileStorageProviderBackend\n\nfrom tests import unittest\nfrom tests.server import FakeSite, make_request\n\n\nclass MediaStorageTests(unittest.HomeserverTestCase):\n\n    needs_threadpool = True\n\n    def prepare(self, reactor, clock, hs):\n        self.test_dir = tempfile.mkdtemp(prefix=\"synapse-tests-\")\n        self.addCleanup(shutil.rmtree, self.test_dir)\n\n        self.primary_base_path = os.path.join(self.test_dir, \"primary\")\n        self.secondary_base_path = os.path.join(self.test_dir, \"secondary\")\n\n        hs.config.media_store_path = self.primary_base_path\n\n        storage_providers = [FileStorageProviderBackend(hs, self.secondary_base_path)]\n\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n        self.media_storage = MediaStorage(\n            hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n\n    def test_ensure_media_is_in_local_cache(self):\n        media_id = \"some_media_id\"\n        test_body = \"Test\\n\"\n\n        # First we create a file that is in a storage provider but not in the\n        # local primary media store\n        rel_path = self.filepaths.local_media_filepath_rel(media_id)\n        secondary_path = os.path.join(self.secondary_base_path, rel_path)\n\n        os.makedirs(os.path.dirname(secondary_path))\n\n        with open(secondary_path, \"w\") as f:\n            f.write(test_body)\n\n        # Now we run ensure_media_is_in_local_cache, which should copy the file\n        # to the local cache.\n        file_info = FileInfo(None, media_id)\n\n        # This uses a real blocking threadpool so we have to wait for it to be\n        # actually done :/\n        x = defer.ensureDeferred(\n            self.media_storage.ensure_media_is_in_local_cache(file_info)\n        )\n\n        # Hotloop until the threadpool does its job...\n        self.wait_on_thread(x)\n\n        local_path = self.get_success(x)\n\n        self.assertTrue(os.path.exists(local_path))\n\n        # Asserts the file is under the expected local cache directory\n        self.assertEquals(\n            os.path.commonprefix([self.primary_base_path, local_path]),\n            self.primary_base_path,\n        )\n\n        with open(local_path) as f:\n            body = f.read()\n\n        self.assertEqual(test_body, body)\n\n\n@attr.s\nclass _TestImage:\n    \"\"\"An image for testing thumbnailing with the expected results\n\n    Attributes:\n        data: The raw image to thumbnail\n        content_type: The type of the image as a content type, e.g. \"image/png\"\n        extension: The extension associated with the format, e.g. \".png\"\n        expected_cropped: The expected bytes from cropped thumbnailing, or None if\n            test should just check for success.\n        expected_scaled: The expected bytes from scaled thumbnailing, or None if\n            test should just check for a valid image returned.\n    \"\"\"\n\n    data = attr.ib(type=bytes)\n    content_type = attr.ib(type=bytes)\n    extension = attr.ib(type=bytes)\n    expected_cropped = attr.ib(type=Optional[bytes])\n    expected_scaled = attr.ib(type=Optional[bytes])\n    expected_found = attr.ib(default=True, type=bool)\n\n\n@parameterized_class(\n    (\"test_image\",),\n    [\n        # smoll png\n        (\n            _TestImage(\n                unhexlify(\n                    b\"89504e470d0a1a0a0000000d4948445200000001000000010806\"\n                    b\"0000001f15c4890000000a49444154789c63000100000500010d\"\n                    b\"0a2db40000000049454e44ae426082\"\n                ),\n                b\"image/png\",\n                b\".png\",\n                unhexlify(\n                    b\"89504e470d0a1a0a0000000d4948445200000020000000200806\"\n                    b\"000000737a7af40000001a49444154789cedc101010000008220\"\n                    b\"ffaf6e484001000000ef0610200001194334ee0000000049454e\"\n                    b\"44ae426082\"\n                ),\n                unhexlify(\n                    b\"89504e470d0a1a0a0000000d4948445200000001000000010806\"\n                    b\"0000001f15c4890000000d49444154789c636060606000000005\"\n                    b\"0001a5f645400000000049454e44ae426082\"\n                ),\n            ),\n        ),\n        # small lossless webp\n        (\n            _TestImage(\n                unhexlify(\n                    b\"524946461a000000574542505650384c0d0000002f0000001007\"\n                    b\"1011118888fe0700\"\n                ),\n                b\"image/webp\",\n                b\".webp\",\n                None,\n                None,\n            ),\n        ),\n        # an empty file\n        (_TestImage(b\"\", b\"image/gif\", b\".gif\", None, None, False,),),\n    ],\n)\nclass MediaRepoTests(unittest.HomeserverTestCase):\n\n    hijack_auth = True\n    user_id = \"@test:user\"\n\n    def make_homeserver(self, reactor, clock):\n\n        self.fetches = []\n\n        def get_file(destination, path, output_stream, args=None, max_size=None):\n            \"\"\"\n            Returns tuple[int,dict,str,int] of file length, response headers,\n            absolute URI, and response code.\n            \"\"\"\n\n            def write_to(r):\n                data, response = r\n                output_stream.write(data)\n                return response\n\n            d = Deferred()\n            d.addCallback(write_to)\n            self.fetches.append((d, destination, path, args))\n            return make_deferred_yieldable(d)\n\n        client = Mock()\n        client.get_file = get_file\n\n        self.storage_path = self.mktemp()\n        self.media_store_path = self.mktemp()\n        os.mkdir(self.storage_path)\n        os.mkdir(self.media_store_path)\n\n        config = self.default_config()\n        config[\"media_store_path\"] = self.media_store_path\n        config[\"thumbnail_requirements\"] = {}\n        config[\"max_image_pixels\"] = 2000000\n\n        provider_config = {\n            \"module\": \"synapse.rest.media.v1.storage_provider.FileStorageProviderBackend\",\n            \"store_local\": True,\n            \"store_synchronous\": False,\n            \"store_remote\": True,\n            \"config\": {\"directory\": self.storage_path},\n        }\n        config[\"media_storage_providers\"] = [provider_config]\n\n        hs = self.setup_test_homeserver(config=config, federation_http_client=client)\n\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n\n        self.media_repo = hs.get_media_repository_resource()\n        self.download_resource = self.media_repo.children[b\"download\"]\n        self.thumbnail_resource = self.media_repo.children[b\"thumbnail\"]\n\n        self.media_id = \"example.com/12345\"\n\n    def _req(self, content_disposition):\n\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            self.media_id,\n            shorthand=False,\n            await_result=False,\n        )\n        self.pump()\n\n        # We've made one fetch, to example.com, using the media URL, and asking\n        # the other server not to do a remote fetch\n        self.assertEqual(len(self.fetches), 1)\n        self.assertEqual(self.fetches[0][1], \"example.com\")\n        self.assertEqual(\n            self.fetches[0][2], \"/_matrix/media/r0/download/\" + self.media_id\n        )\n        self.assertEqual(self.fetches[0][3], {\"allow_remote\": \"false\"})\n\n        headers = {\n            b\"Content-Length\": [b\"%d\" % (len(self.test_image.data))],\n            b\"Content-Type\": [self.test_image.content_type],\n        }\n        if content_disposition:\n            headers[b\"Content-Disposition\"] = [content_disposition]\n\n        self.fetches[0][0].callback(\n            (self.test_image.data, (len(self.test_image.data), headers))\n        )\n\n        self.pump()\n        self.assertEqual(channel.code, 200)\n\n        return channel\n\n    def test_disposition_filename_ascii(self):\n        \"\"\"\n        If the filename is filename=<ascii> then Synapse will decode it as an\n        ASCII string, and use filename= in the response.\n        \"\"\"\n        channel = self._req(b\"inline; filename=out\" + self.test_image.extension)\n\n        headers = channel.headers\n        self.assertEqual(\n            headers.getRawHeaders(b\"Content-Type\"), [self.test_image.content_type]\n        )\n        self.assertEqual(\n            headers.getRawHeaders(b\"Content-Disposition\"),\n            [b\"inline; filename=out\" + self.test_image.extension],\n        )\n\n    def test_disposition_filenamestar_utf8escaped(self):\n        \"\"\"\n        If the filename is filename=*utf8''<utf8 escaped> then Synapse will\n        correctly decode it as the UTF-8 string, and use filename* in the\n        response.\n        \"\"\"\n        filename = parse.quote(\"\\u2603\".encode(\"utf8\")).encode(\"ascii\")\n        channel = self._req(\n            b\"inline; filename*=utf-8''\" + filename + self.test_image.extension\n        )\n\n        headers = channel.headers\n        self.assertEqual(\n            headers.getRawHeaders(b\"Content-Type\"), [self.test_image.content_type]\n        )\n        self.assertEqual(\n            headers.getRawHeaders(b\"Content-Disposition\"),\n            [b\"inline; filename*=utf-8''\" + filename + self.test_image.extension],\n        )\n\n    def test_disposition_none(self):\n        \"\"\"\n        If there is no filename, one isn't passed on in the Content-Disposition\n        of the request.\n        \"\"\"\n        channel = self._req(None)\n\n        headers = channel.headers\n        self.assertEqual(\n            headers.getRawHeaders(b\"Content-Type\"), [self.test_image.content_type]\n        )\n        self.assertEqual(headers.getRawHeaders(b\"Content-Disposition\"), None)\n\n    def test_thumbnail_crop(self):\n        self._test_thumbnail(\n            \"crop\", self.test_image.expected_cropped, self.test_image.expected_found\n        )\n\n    def test_thumbnail_scale(self):\n        self._test_thumbnail(\n            \"scale\", self.test_image.expected_scaled, self.test_image.expected_found\n        )\n\n    def _test_thumbnail(self, method, expected_body, expected_found):\n        params = \"?width=32&height=32&method=\" + method\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.thumbnail_resource),\n            \"GET\",\n            self.media_id + params,\n            shorthand=False,\n            await_result=False,\n        )\n        self.pump()\n\n        headers = {\n            b\"Content-Length\": [b\"%d\" % (len(self.test_image.data))],\n            b\"Content-Type\": [self.test_image.content_type],\n        }\n        self.fetches[0][0].callback(\n            (self.test_image.data, (len(self.test_image.data), headers))\n        )\n        self.pump()\n\n        if expected_found:\n            self.assertEqual(channel.code, 200)\n            if expected_body is not None:\n                self.assertEqual(\n                    channel.result[\"body\"], expected_body, channel.result[\"body\"]\n                )\n            else:\n                # ensure that the result is at least some valid image\n                Image.open(BytesIO(channel.result[\"body\"]))\n        else:\n            # A 404 with a JSON body.\n            self.assertEqual(channel.code, 404)\n            self.assertEqual(\n                channel.json_body,\n                {\n                    \"errcode\": \"M_NOT_FOUND\",\n                    \"error\": \"Not found [b'example.com', b'12345']\",\n                },\n            )\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport shutil\nimport tempfile\nfrom binascii import unhexlify\nfrom io import BytesIO\nfrom typing import Optional\nfrom urllib import parse\n\nfrom mock import Mock\n\nimport attr\nfrom parameterized import parameterized_class\nfrom PIL import Image as Image\n\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred\n\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.rest.media.v1._base import FileInfo\nfrom synapse.rest.media.v1.filepath import MediaFilePaths\nfrom synapse.rest.media.v1.media_storage import MediaStorage\nfrom synapse.rest.media.v1.storage_provider import FileStorageProviderBackend\n\nfrom tests import unittest\nfrom tests.server import FakeSite, make_request\n\n\nclass MediaStorageTests(unittest.HomeserverTestCase):\n\n    needs_threadpool = True\n\n    def prepare(self, reactor, clock, hs):\n        self.test_dir = tempfile.mkdtemp(prefix=\"synapse-tests-\")\n        self.addCleanup(shutil.rmtree, self.test_dir)\n\n        self.primary_base_path = os.path.join(self.test_dir, \"primary\")\n        self.secondary_base_path = os.path.join(self.test_dir, \"secondary\")\n\n        hs.config.media_store_path = self.primary_base_path\n\n        storage_providers = [FileStorageProviderBackend(hs, self.secondary_base_path)]\n\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n        self.media_storage = MediaStorage(\n            hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n\n    def test_ensure_media_is_in_local_cache(self):\n        media_id = \"some_media_id\"\n        test_body = \"Test\\n\"\n\n        # First we create a file that is in a storage provider but not in the\n        # local primary media store\n        rel_path = self.filepaths.local_media_filepath_rel(media_id)\n        secondary_path = os.path.join(self.secondary_base_path, rel_path)\n\n        os.makedirs(os.path.dirname(secondary_path))\n\n        with open(secondary_path, \"w\") as f:\n            f.write(test_body)\n\n        # Now we run ensure_media_is_in_local_cache, which should copy the file\n        # to the local cache.\n        file_info = FileInfo(None, media_id)\n\n        # This uses a real blocking threadpool so we have to wait for it to be\n        # actually done :/\n        x = defer.ensureDeferred(\n            self.media_storage.ensure_media_is_in_local_cache(file_info)\n        )\n\n        # Hotloop until the threadpool does its job...\n        self.wait_on_thread(x)\n\n        local_path = self.get_success(x)\n\n        self.assertTrue(os.path.exists(local_path))\n\n        # Asserts the file is under the expected local cache directory\n        self.assertEquals(\n            os.path.commonprefix([self.primary_base_path, local_path]),\n            self.primary_base_path,\n        )\n\n        with open(local_path) as f:\n            body = f.read()\n\n        self.assertEqual(test_body, body)\n\n\n@attr.s\nclass _TestImage:\n    \"\"\"An image for testing thumbnailing with the expected results\n\n    Attributes:\n        data: The raw image to thumbnail\n        content_type: The type of the image as a content type, e.g. \"image/png\"\n        extension: The extension associated with the format, e.g. \".png\"\n        expected_cropped: The expected bytes from cropped thumbnailing, or None if\n            test should just check for success.\n        expected_scaled: The expected bytes from scaled thumbnailing, or None if\n            test should just check for a valid image returned.\n    \"\"\"\n\n    data = attr.ib(type=bytes)\n    content_type = attr.ib(type=bytes)\n    extension = attr.ib(type=bytes)\n    expected_cropped = attr.ib(type=Optional[bytes])\n    expected_scaled = attr.ib(type=Optional[bytes])\n    expected_found = attr.ib(default=True, type=bool)\n\n\n@parameterized_class(\n    (\"test_image\",),\n    [\n        # smoll png\n        (\n            _TestImage(\n                unhexlify(\n                    b\"89504e470d0a1a0a0000000d4948445200000001000000010806\"\n                    b\"0000001f15c4890000000a49444154789c63000100000500010d\"\n                    b\"0a2db40000000049454e44ae426082\"\n                ),\n                b\"image/png\",\n                b\".png\",\n                unhexlify(\n                    b\"89504e470d0a1a0a0000000d4948445200000020000000200806\"\n                    b\"000000737a7af40000001a49444154789cedc101010000008220\"\n                    b\"ffaf6e484001000000ef0610200001194334ee0000000049454e\"\n                    b\"44ae426082\"\n                ),\n                unhexlify(\n                    b\"89504e470d0a1a0a0000000d4948445200000001000000010806\"\n                    b\"0000001f15c4890000000d49444154789c636060606000000005\"\n                    b\"0001a5f645400000000049454e44ae426082\"\n                ),\n            ),\n        ),\n        # small lossless webp\n        (\n            _TestImage(\n                unhexlify(\n                    b\"524946461a000000574542505650384c0d0000002f0000001007\"\n                    b\"1011118888fe0700\"\n                ),\n                b\"image/webp\",\n                b\".webp\",\n                None,\n                None,\n            ),\n        ),\n        # an empty file\n        (_TestImage(b\"\", b\"image/gif\", b\".gif\", None, None, False,),),\n    ],\n)\nclass MediaRepoTests(unittest.HomeserverTestCase):\n\n    hijack_auth = True\n    user_id = \"@test:user\"\n\n    def make_homeserver(self, reactor, clock):\n\n        self.fetches = []\n\n        def get_file(destination, path, output_stream, args=None, max_size=None):\n            \"\"\"\n            Returns tuple[int,dict,str,int] of file length, response headers,\n            absolute URI, and response code.\n            \"\"\"\n\n            def write_to(r):\n                data, response = r\n                output_stream.write(data)\n                return response\n\n            d = Deferred()\n            d.addCallback(write_to)\n            self.fetches.append((d, destination, path, args))\n            return make_deferred_yieldable(d)\n\n        client = Mock()\n        client.get_file = get_file\n\n        self.storage_path = self.mktemp()\n        self.media_store_path = self.mktemp()\n        os.mkdir(self.storage_path)\n        os.mkdir(self.media_store_path)\n\n        config = self.default_config()\n        config[\"media_store_path\"] = self.media_store_path\n        config[\"thumbnail_requirements\"] = {}\n        config[\"max_image_pixels\"] = 2000000\n\n        provider_config = {\n            \"module\": \"synapse.rest.media.v1.storage_provider.FileStorageProviderBackend\",\n            \"store_local\": True,\n            \"store_synchronous\": False,\n            \"store_remote\": True,\n            \"config\": {\"directory\": self.storage_path},\n        }\n        config[\"media_storage_providers\"] = [provider_config]\n\n        hs = self.setup_test_homeserver(config=config, http_client=client)\n\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n\n        self.media_repo = hs.get_media_repository_resource()\n        self.download_resource = self.media_repo.children[b\"download\"]\n        self.thumbnail_resource = self.media_repo.children[b\"thumbnail\"]\n\n        self.media_id = \"example.com/12345\"\n\n    def _req(self, content_disposition):\n\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.download_resource),\n            \"GET\",\n            self.media_id,\n            shorthand=False,\n            await_result=False,\n        )\n        self.pump()\n\n        # We've made one fetch, to example.com, using the media URL, and asking\n        # the other server not to do a remote fetch\n        self.assertEqual(len(self.fetches), 1)\n        self.assertEqual(self.fetches[0][1], \"example.com\")\n        self.assertEqual(\n            self.fetches[0][2], \"/_matrix/media/r0/download/\" + self.media_id\n        )\n        self.assertEqual(self.fetches[0][3], {\"allow_remote\": \"false\"})\n\n        headers = {\n            b\"Content-Length\": [b\"%d\" % (len(self.test_image.data))],\n            b\"Content-Type\": [self.test_image.content_type],\n        }\n        if content_disposition:\n            headers[b\"Content-Disposition\"] = [content_disposition]\n\n        self.fetches[0][0].callback(\n            (self.test_image.data, (len(self.test_image.data), headers))\n        )\n\n        self.pump()\n        self.assertEqual(channel.code, 200)\n\n        return channel\n\n    def test_disposition_filename_ascii(self):\n        \"\"\"\n        If the filename is filename=<ascii> then Synapse will decode it as an\n        ASCII string, and use filename= in the response.\n        \"\"\"\n        channel = self._req(b\"inline; filename=out\" + self.test_image.extension)\n\n        headers = channel.headers\n        self.assertEqual(\n            headers.getRawHeaders(b\"Content-Type\"), [self.test_image.content_type]\n        )\n        self.assertEqual(\n            headers.getRawHeaders(b\"Content-Disposition\"),\n            [b\"inline; filename=out\" + self.test_image.extension],\n        )\n\n    def test_disposition_filenamestar_utf8escaped(self):\n        \"\"\"\n        If the filename is filename=*utf8''<utf8 escaped> then Synapse will\n        correctly decode it as the UTF-8 string, and use filename* in the\n        response.\n        \"\"\"\n        filename = parse.quote(\"\\u2603\".encode(\"utf8\")).encode(\"ascii\")\n        channel = self._req(\n            b\"inline; filename*=utf-8''\" + filename + self.test_image.extension\n        )\n\n        headers = channel.headers\n        self.assertEqual(\n            headers.getRawHeaders(b\"Content-Type\"), [self.test_image.content_type]\n        )\n        self.assertEqual(\n            headers.getRawHeaders(b\"Content-Disposition\"),\n            [b\"inline; filename*=utf-8''\" + filename + self.test_image.extension],\n        )\n\n    def test_disposition_none(self):\n        \"\"\"\n        If there is no filename, one isn't passed on in the Content-Disposition\n        of the request.\n        \"\"\"\n        channel = self._req(None)\n\n        headers = channel.headers\n        self.assertEqual(\n            headers.getRawHeaders(b\"Content-Type\"), [self.test_image.content_type]\n        )\n        self.assertEqual(headers.getRawHeaders(b\"Content-Disposition\"), None)\n\n    def test_thumbnail_crop(self):\n        self._test_thumbnail(\n            \"crop\", self.test_image.expected_cropped, self.test_image.expected_found\n        )\n\n    def test_thumbnail_scale(self):\n        self._test_thumbnail(\n            \"scale\", self.test_image.expected_scaled, self.test_image.expected_found\n        )\n\n    def _test_thumbnail(self, method, expected_body, expected_found):\n        params = \"?width=32&height=32&method=\" + method\n        request, channel = make_request(\n            self.reactor,\n            FakeSite(self.thumbnail_resource),\n            \"GET\",\n            self.media_id + params,\n            shorthand=False,\n            await_result=False,\n        )\n        self.pump()\n\n        headers = {\n            b\"Content-Length\": [b\"%d\" % (len(self.test_image.data))],\n            b\"Content-Type\": [self.test_image.content_type],\n        }\n        self.fetches[0][0].callback(\n            (self.test_image.data, (len(self.test_image.data), headers))\n        )\n        self.pump()\n\n        if expected_found:\n            self.assertEqual(channel.code, 200)\n            if expected_body is not None:\n                self.assertEqual(\n                    channel.result[\"body\"], expected_body, channel.result[\"body\"]\n                )\n            else:\n                # ensure that the result is at least some valid image\n                Image.open(BytesIO(channel.result[\"body\"]))\n        else:\n            # A 404 with a JSON body.\n            self.assertEqual(channel.code, 404)\n            self.assertEqual(\n                channel.json_body,\n                {\n                    \"errcode\": \"M_NOT_FOUND\",\n                    \"error\": \"Not found [b'example.com', b'12345']\",\n                },\n            )\n", "patch": "@@ -214,7 +214,7 @@ def write_to(r):\n         }\n         config[\"media_storage_providers\"] = [provider_config]\n \n-        hs = self.setup_test_homeserver(config=config, http_client=client)\n+        hs = self.setup_test_homeserver(config=config, federation_http_client=client)\n \n         return hs\n ", "file_path": "files/2021_2/51", "file_language": "py", "file_name": "tests/rest/media/v1/test_media_storage.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class MediaStorageTests(unittest.HomeserverTestCase):\n\n    needs_threadpool = True\n\n    def prepare(self, reactor, clock, hs):\n        self.test_dir = tempfile.mkdtemp(prefix=\"synapse-tests-\")\n        self.addCleanup(shutil.rmtree, self.test_dir)\n\n        self.primary_base_path = os.path.join(self.test_dir, \"primary\")\n        self.secondary_base_path = os.path.join(self.test_dir, \"secondary\")\n\n        hs.config.media_store_path = self.primary_base_path\n\n        storage_providers = [FileStorageProviderBackend(hs, self.secondary_base_path)]\n\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n        self.media_storage = MediaStorage(\n            hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n\n    def test_ensure_media_is_in_local_cache(self):\n        media_id = \"some_media_id\"\n        test_body = \"Test\\n\"\n\n        # First we create a file that is in a storage provider but not in the\n        # local primary media store\n        rel_path = self.filepaths.local_media_filepath_rel(media_id)\n        secondary_path = os.path.join(self.secondary_base_path, rel_path)\n\n        os.makedirs(os.path.dirname(secondary_path))\n\n        with open(secondary_path, \"w\") as f:\n            f.write(test_body)\n\n        # Now we run ensure_media_is_in_local_cache, which should copy the file\n        # to the local cache.\n        file_info = FileInfo(None, media_id)\n\n        # This uses a real blocking threadpool so we have to wait for it to be\n        # actually done :/\n        x = defer.ensureDeferred(\n            self.media_storage.ensure_media_is_in_local_cache(file_info)\n        )\n\n        # Hotloop until the threadpool does its job...\n        self.wait_on_thread(x)\n\n        local_path = self.get_success(x)\n\n        self.assertTrue(os.path.exists(local_path))\n\n        # Asserts the file is under the expected local cache directory\n        self.assertEquals(\n            os.path.commonprefix([self.primary_base_path, local_path]),\n            self.primary_base_path,\n        )\n\n        with open(local_path) as f:\n            body = f.read()\n\n        self.assertEqual(test_body, body)", "target": 0}], "function_after": [{"function": "class MediaStorageTests(unittest.HomeserverTestCase):\n\n    needs_threadpool = True\n\n    def prepare(self, reactor, clock, hs):\n        self.test_dir = tempfile.mkdtemp(prefix=\"synapse-tests-\")\n        self.addCleanup(shutil.rmtree, self.test_dir)\n\n        self.primary_base_path = os.path.join(self.test_dir, \"primary\")\n        self.secondary_base_path = os.path.join(self.test_dir, \"secondary\")\n\n        hs.config.media_store_path = self.primary_base_path\n\n        storage_providers = [FileStorageProviderBackend(hs, self.secondary_base_path)]\n\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n        self.media_storage = MediaStorage(\n            hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n\n    def test_ensure_media_is_in_local_cache(self):\n        media_id = \"some_media_id\"\n        test_body = \"Test\\n\"\n\n        # First we create a file that is in a storage provider but not in the\n        # local primary media store\n        rel_path = self.filepaths.local_media_filepath_rel(media_id)\n        secondary_path = os.path.join(self.secondary_base_path, rel_path)\n\n        os.makedirs(os.path.dirname(secondary_path))\n\n        with open(secondary_path, \"w\") as f:\n            f.write(test_body)\n\n        # Now we run ensure_media_is_in_local_cache, which should copy the file\n        # to the local cache.\n        file_info = FileInfo(None, media_id)\n\n        # This uses a real blocking threadpool so we have to wait for it to be\n        # actually done :/\n        x = defer.ensureDeferred(\n            self.media_storage.ensure_media_is_in_local_cache(file_info)\n        )\n\n        # Hotloop until the threadpool does its job...\n        self.wait_on_thread(x)\n\n        local_path = self.get_success(x)\n\n        self.assertTrue(os.path.exists(local_path))\n\n        # Asserts the file is under the expected local cache directory\n        self.assertEquals(\n            os.path.commonprefix([self.primary_base_path, local_path]),\n            self.primary_base_path,\n        )\n\n        with open(local_path) as f:\n            body = f.read()\n\n        self.assertEqual(test_body, body)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fstorage%2Ftest_e2e_room_keys.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom tests import unittest\n\n# sample room_key data for use in the tests\nroom_key = {\n    \"first_message_index\": 1,\n    \"forwarded_count\": 1,\n    \"is_verified\": False,\n    \"session_data\": \"SSBBTSBBIEZJU0gK\",\n}\n\n\nclass E2eRoomKeysHandlerTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_room_keys_version_delete(self):\n        # test that deleting a room key backup deletes the keys\n        version1 = self.get_success(\n            self.store.create_e2e_room_keys_version(\n                \"user_id\", {\"algorithm\": \"rot13\", \"auth_data\": {}}\n            )\n        )\n\n        self.get_success(\n            self.store.add_e2e_room_keys(\n                \"user_id\", version1, [(\"room\", \"session\", room_key)]\n            )\n        )\n\n        version2 = self.get_success(\n            self.store.create_e2e_room_keys_version(\n                \"user_id\", {\"algorithm\": \"rot13\", \"auth_data\": {}}\n            )\n        )\n\n        self.get_success(\n            self.store.add_e2e_room_keys(\n                \"user_id\", version2, [(\"room\", \"session\", room_key)]\n            )\n        )\n\n        # make sure the keys were stored properly\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version1))\n        self.assertEqual(len(keys[\"rooms\"]), 1)\n\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version2))\n        self.assertEqual(len(keys[\"rooms\"]), 1)\n\n        # delete version1\n        self.get_success(self.store.delete_e2e_room_keys_version(\"user_id\", version1))\n\n        # make sure the key from version1 is gone, and the key from version2 is\n        # still there\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version1))\n        self.assertEqual(len(keys[\"rooms\"]), 0)\n\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version2))\n        self.assertEqual(len(keys[\"rooms\"]), 1)\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom tests import unittest\n\n# sample room_key data for use in the tests\nroom_key = {\n    \"first_message_index\": 1,\n    \"forwarded_count\": 1,\n    \"is_verified\": False,\n    \"session_data\": \"SSBBTSBBIEZJU0gK\",\n}\n\n\nclass E2eRoomKeysHandlerTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", http_client=None)\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_room_keys_version_delete(self):\n        # test that deleting a room key backup deletes the keys\n        version1 = self.get_success(\n            self.store.create_e2e_room_keys_version(\n                \"user_id\", {\"algorithm\": \"rot13\", \"auth_data\": {}}\n            )\n        )\n\n        self.get_success(\n            self.store.add_e2e_room_keys(\n                \"user_id\", version1, [(\"room\", \"session\", room_key)]\n            )\n        )\n\n        version2 = self.get_success(\n            self.store.create_e2e_room_keys_version(\n                \"user_id\", {\"algorithm\": \"rot13\", \"auth_data\": {}}\n            )\n        )\n\n        self.get_success(\n            self.store.add_e2e_room_keys(\n                \"user_id\", version2, [(\"room\", \"session\", room_key)]\n            )\n        )\n\n        # make sure the keys were stored properly\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version1))\n        self.assertEqual(len(keys[\"rooms\"]), 1)\n\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version2))\n        self.assertEqual(len(keys[\"rooms\"]), 1)\n\n        # delete version1\n        self.get_success(self.store.delete_e2e_room_keys_version(\"user_id\", version1))\n\n        # make sure the key from version1 is gone, and the key from version2 is\n        # still there\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version1))\n        self.assertEqual(len(keys[\"rooms\"]), 0)\n\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version2))\n        self.assertEqual(len(keys[\"rooms\"]), 1)\n", "patch": "@@ -26,7 +26,7 @@\n \n class E2eRoomKeysHandlerTestCase(unittest.HomeserverTestCase):\n     def make_homeserver(self, reactor, clock):\n-        hs = self.setup_test_homeserver(\"server\", http_client=None)\n+        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n         self.store = hs.get_datastore()\n         return hs\n ", "file_path": "files/2021_2/52", "file_language": "py", "file_name": "tests/storage/test_e2e_room_keys.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class E2eRoomKeysHandlerTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", http_client=None)\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_room_keys_version_delete(self):\n        # test that deleting a room key backup deletes the keys\n        version1 = self.get_success(\n            self.store.create_e2e_room_keys_version(\n                \"user_id\", {\"algorithm\": \"rot13\", \"auth_data\": {}}\n            )\n        )\n\n        self.get_success(\n            self.store.add_e2e_room_keys(\n                \"user_id\", version1, [(\"room\", \"session\", room_key)]\n            )\n        )\n\n        version2 = self.get_success(\n            self.store.create_e2e_room_keys_version(\n                \"user_id\", {\"algorithm\": \"rot13\", \"auth_data\": {}}\n            )\n        )\n\n        self.get_success(\n            self.store.add_e2e_room_keys(\n                \"user_id\", version2, [(\"room\", \"session\", room_key)]\n            )\n        )\n\n        # make sure the keys were stored properly\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version1))\n        self.assertEqual(len(keys[\"rooms\"]), 1)\n\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version2))\n        self.assertEqual(len(keys[\"rooms\"]), 1)\n\n        # delete version1\n        self.get_success(self.store.delete_e2e_room_keys_version(\"user_id\", version1))\n\n        # make sure the key from version1 is gone, and the key from version2 is\n        # still there\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version1))\n        self.assertEqual(len(keys[\"rooms\"]), 0)\n\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version2))\n        self.assertEqual(len(keys[\"rooms\"]), 1)", "target": 0}], "function_after": [{"function": "class E2eRoomKeysHandlerTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n        self.store = hs.get_datastore()\n        return hs\n\n    def test_room_keys_version_delete(self):\n        # test that deleting a room key backup deletes the keys\n        version1 = self.get_success(\n            self.store.create_e2e_room_keys_version(\n                \"user_id\", {\"algorithm\": \"rot13\", \"auth_data\": {}}\n            )\n        )\n\n        self.get_success(\n            self.store.add_e2e_room_keys(\n                \"user_id\", version1, [(\"room\", \"session\", room_key)]\n            )\n        )\n\n        version2 = self.get_success(\n            self.store.create_e2e_room_keys_version(\n                \"user_id\", {\"algorithm\": \"rot13\", \"auth_data\": {}}\n            )\n        )\n\n        self.get_success(\n            self.store.add_e2e_room_keys(\n                \"user_id\", version2, [(\"room\", \"session\", room_key)]\n            )\n        )\n\n        # make sure the keys were stored properly\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version1))\n        self.assertEqual(len(keys[\"rooms\"]), 1)\n\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version2))\n        self.assertEqual(len(keys[\"rooms\"]), 1)\n\n        # delete version1\n        self.get_success(self.store.delete_e2e_room_keys_version(\"user_id\", version1))\n\n        # make sure the key from version1 is gone, and the key from version2 is\n        # still there\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version1))\n        self.assertEqual(len(keys[\"rooms\"]), 0)\n\n        keys = self.get_success(self.store.get_e2e_room_keys(\"user_id\", version2))\n        self.assertEqual(len(keys[\"rooms\"]), 1)", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fstorage%2Ftest_purge.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom twisted.internet import defer\n\nfrom synapse.api.errors import NotFoundError\nfrom synapse.rest.client.v1 import room\n\nfrom tests.unittest import HomeserverTestCase\n\n\nclass PurgeTests(HomeserverTestCase):\n\n    user_id = \"@red:server\"\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_purge(self):\n        \"\"\"\n        Purging a room will delete everything before the topological point.\n        \"\"\"\n        # Send four messages to the room\n        first = self.helper.send(self.room_id, body=\"test1\")\n        second = self.helper.send(self.room_id, body=\"test2\")\n        third = self.helper.send(self.room_id, body=\"test3\")\n        last = self.helper.send(self.room_id, body=\"test4\")\n\n        store = self.hs.get_datastore()\n        storage = self.hs.get_storage()\n\n        # Get the topological token\n        token = self.get_success(\n            store.get_topological_token_for_event(last[\"event_id\"])\n        )\n        token_str = self.get_success(token.to_string(self.hs.get_datastore()))\n\n        # Purge everything before this topological token\n        self.get_success(\n            storage.purge_events.purge_history(self.room_id, token_str, True)\n        )\n\n        # 1-3 should fail and last will succeed, meaning that 1-3 are deleted\n        # and last is not.\n        self.get_failure(store.get_event(first[\"event_id\"]), NotFoundError)\n        self.get_failure(store.get_event(second[\"event_id\"]), NotFoundError)\n        self.get_failure(store.get_event(third[\"event_id\"]), NotFoundError)\n        self.get_success(store.get_event(last[\"event_id\"]))\n\n    def test_purge_wont_delete_extrems(self):\n        \"\"\"\n        Purging a room will delete everything before the topological point.\n        \"\"\"\n        # Send four messages to the room\n        first = self.helper.send(self.room_id, body=\"test1\")\n        second = self.helper.send(self.room_id, body=\"test2\")\n        third = self.helper.send(self.room_id, body=\"test3\")\n        last = self.helper.send(self.room_id, body=\"test4\")\n\n        storage = self.hs.get_datastore()\n\n        # Set the topological token higher than it should be\n        token = self.get_success(\n            storage.get_topological_token_for_event(last[\"event_id\"])\n        )\n        event = \"t{}-{}\".format(token.topological + 1, token.stream + 1)\n\n        # Purge everything before this topological token\n        purge = defer.ensureDeferred(storage.purge_history(self.room_id, event, True))\n        self.pump()\n        f = self.failureResultOf(purge)\n        self.assertIn(\"greater than forward\", f.value.args[0])\n\n        # Try and get the events\n        self.get_success(storage.get_event(first[\"event_id\"]))\n        self.get_success(storage.get_event(second[\"event_id\"]))\n        self.get_success(storage.get_event(third[\"event_id\"]))\n        self.get_success(storage.get_event(last[\"event_id\"]))\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom twisted.internet import defer\n\nfrom synapse.api.errors import NotFoundError\nfrom synapse.rest.client.v1 import room\n\nfrom tests.unittest import HomeserverTestCase\n\n\nclass PurgeTests(HomeserverTestCase):\n\n    user_id = \"@red:server\"\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", http_client=None)\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_purge(self):\n        \"\"\"\n        Purging a room will delete everything before the topological point.\n        \"\"\"\n        # Send four messages to the room\n        first = self.helper.send(self.room_id, body=\"test1\")\n        second = self.helper.send(self.room_id, body=\"test2\")\n        third = self.helper.send(self.room_id, body=\"test3\")\n        last = self.helper.send(self.room_id, body=\"test4\")\n\n        store = self.hs.get_datastore()\n        storage = self.hs.get_storage()\n\n        # Get the topological token\n        token = self.get_success(\n            store.get_topological_token_for_event(last[\"event_id\"])\n        )\n        token_str = self.get_success(token.to_string(self.hs.get_datastore()))\n\n        # Purge everything before this topological token\n        self.get_success(\n            storage.purge_events.purge_history(self.room_id, token_str, True)\n        )\n\n        # 1-3 should fail and last will succeed, meaning that 1-3 are deleted\n        # and last is not.\n        self.get_failure(store.get_event(first[\"event_id\"]), NotFoundError)\n        self.get_failure(store.get_event(second[\"event_id\"]), NotFoundError)\n        self.get_failure(store.get_event(third[\"event_id\"]), NotFoundError)\n        self.get_success(store.get_event(last[\"event_id\"]))\n\n    def test_purge_wont_delete_extrems(self):\n        \"\"\"\n        Purging a room will delete everything before the topological point.\n        \"\"\"\n        # Send four messages to the room\n        first = self.helper.send(self.room_id, body=\"test1\")\n        second = self.helper.send(self.room_id, body=\"test2\")\n        third = self.helper.send(self.room_id, body=\"test3\")\n        last = self.helper.send(self.room_id, body=\"test4\")\n\n        storage = self.hs.get_datastore()\n\n        # Set the topological token higher than it should be\n        token = self.get_success(\n            storage.get_topological_token_for_event(last[\"event_id\"])\n        )\n        event = \"t{}-{}\".format(token.topological + 1, token.stream + 1)\n\n        # Purge everything before this topological token\n        purge = defer.ensureDeferred(storage.purge_history(self.room_id, event, True))\n        self.pump()\n        f = self.failureResultOf(purge)\n        self.assertIn(\"greater than forward\", f.value.args[0])\n\n        # Try and get the events\n        self.get_success(storage.get_event(first[\"event_id\"]))\n        self.get_success(storage.get_event(second[\"event_id\"]))\n        self.get_success(storage.get_event(third[\"event_id\"]))\n        self.get_success(storage.get_event(last[\"event_id\"]))\n", "patch": "@@ -27,7 +27,7 @@ class PurgeTests(HomeserverTestCase):\n     servlets = [room.register_servlets]\n \n     def make_homeserver(self, reactor, clock):\n-        hs = self.setup_test_homeserver(\"server\", http_client=None)\n+        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n         return hs\n \n     def prepare(self, reactor, clock, hs):", "file_path": "files/2021_2/53", "file_language": "py", "file_name": "tests/storage/test_purge.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class PurgeTests(HomeserverTestCase):\n\n    user_id = \"@red:server\"\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", http_client=None)\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_purge(self):\n        \"\"\"\n        Purging a room will delete everything before the topological point.\n        \"\"\"\n        # Send four messages to the room\n        first = self.helper.send(self.room_id, body=\"test1\")\n        second = self.helper.send(self.room_id, body=\"test2\")\n        third = self.helper.send(self.room_id, body=\"test3\")\n        last = self.helper.send(self.room_id, body=\"test4\")\n\n        store = self.hs.get_datastore()\n        storage = self.hs.get_storage()\n\n        # Get the topological token\n        token = self.get_success(\n            store.get_topological_token_for_event(last[\"event_id\"])\n        )\n        token_str = self.get_success(token.to_string(self.hs.get_datastore()))\n\n        # Purge everything before this topological token\n        self.get_success(\n            storage.purge_events.purge_history(self.room_id, token_str, True)\n        )\n\n        # 1-3 should fail and last will succeed, meaning that 1-3 are deleted\n        # and last is not.\n        self.get_failure(store.get_event(first[\"event_id\"]), NotFoundError)\n        self.get_failure(store.get_event(second[\"event_id\"]), NotFoundError)\n        self.get_failure(store.get_event(third[\"event_id\"]), NotFoundError)\n        self.get_success(store.get_event(last[\"event_id\"]))\n\n    def test_purge_wont_delete_extrems(self):\n        \"\"\"\n        Purging a room will delete everything before the topological point.\n        \"\"\"\n        # Send four messages to the room\n        first = self.helper.send(self.room_id, body=\"test1\")\n        second = self.helper.send(self.room_id, body=\"test2\")\n        third = self.helper.send(self.room_id, body=\"test3\")\n        last = self.helper.send(self.room_id, body=\"test4\")\n\n        storage = self.hs.get_datastore()\n\n        # Set the topological token higher than it should be\n        token = self.get_success(\n            storage.get_topological_token_for_event(last[\"event_id\"])\n        )\n        event = \"t{}-{}\".format(token.topological + 1, token.stream + 1)\n\n        # Purge everything before this topological token\n        purge = defer.ensureDeferred(storage.purge_history(self.room_id, event, True))\n        self.pump()\n        f = self.failureResultOf(purge)\n        self.assertIn(\"greater than forward\", f.value.args[0])\n\n        # Try and get the events\n        self.get_success(storage.get_event(first[\"event_id\"]))\n        self.get_success(storage.get_event(second[\"event_id\"]))\n        self.get_success(storage.get_event(third[\"event_id\"]))\n        self.get_success(storage.get_event(last[\"event_id\"]))", "target": 0}], "function_after": [{"function": "class PurgeTests(HomeserverTestCase):\n\n    user_id = \"@red:server\"\n    servlets = [room.register_servlets]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\"server\", federation_http_client=None)\n        return hs\n\n    def prepare(self, reactor, clock, hs):\n        self.room_id = self.helper.create_room_as(self.user_id)\n\n    def test_purge(self):\n        \"\"\"\n        Purging a room will delete everything before the topological point.\n        \"\"\"\n        # Send four messages to the room\n        first = self.helper.send(self.room_id, body=\"test1\")\n        second = self.helper.send(self.room_id, body=\"test2\")\n        third = self.helper.send(self.room_id, body=\"test3\")\n        last = self.helper.send(self.room_id, body=\"test4\")\n\n        store = self.hs.get_datastore()\n        storage = self.hs.get_storage()\n\n        # Get the topological token\n        token = self.get_success(\n            store.get_topological_token_for_event(last[\"event_id\"])\n        )\n        token_str = self.get_success(token.to_string(self.hs.get_datastore()))\n\n        # Purge everything before this topological token\n        self.get_success(\n            storage.purge_events.purge_history(self.room_id, token_str, True)\n        )\n\n        # 1-3 should fail and last will succeed, meaning that 1-3 are deleted\n        # and last is not.\n        self.get_failure(store.get_event(first[\"event_id\"]), NotFoundError)\n        self.get_failure(store.get_event(second[\"event_id\"]), NotFoundError)\n        self.get_failure(store.get_event(third[\"event_id\"]), NotFoundError)\n        self.get_success(store.get_event(last[\"event_id\"]))\n\n    def test_purge_wont_delete_extrems(self):\n        \"\"\"\n        Purging a room will delete everything before the topological point.\n        \"\"\"\n        # Send four messages to the room\n        first = self.helper.send(self.room_id, body=\"test1\")\n        second = self.helper.send(self.room_id, body=\"test2\")\n        third = self.helper.send(self.room_id, body=\"test3\")\n        last = self.helper.send(self.room_id, body=\"test4\")\n\n        storage = self.hs.get_datastore()\n\n        # Set the topological token higher than it should be\n        token = self.get_success(\n            storage.get_topological_token_for_event(last[\"event_id\"])\n        )\n        event = \"t{}-{}\".format(token.topological + 1, token.stream + 1)\n\n        # Purge everything before this topological token\n        purge = defer.ensureDeferred(storage.purge_history(self.room_id, event, True))\n        self.pump()\n        f = self.failureResultOf(purge)\n        self.assertIn(\"greater than forward\", f.value.args[0])\n\n        # Try and get the events\n        self.get_success(storage.get_event(first[\"event_id\"]))\n        self.get_success(storage.get_event(second[\"event_id\"]))\n        self.get_success(storage.get_event(third[\"event_id\"]))\n        self.get_success(storage.get_event(last[\"event_id\"]))", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fstorage%2Ftest_redaction.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom mock import Mock\n\nfrom canonicaljson import json\n\nfrom twisted.internet import defer\n\nfrom synapse.api.constants import EventTypes, Membership\nfrom synapse.api.room_versions import RoomVersions\nfrom synapse.types import RoomID, UserID\n\nfrom tests import unittest\nfrom tests.utils import create_room\n\n\nclass RedactionTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"redaction_retention_period\"] = \"30d\"\n        return self.setup_test_homeserver(\n            resource_for_federation=Mock(), federation_http_client=None, config=config\n        )\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.event_builder_factory = hs.get_event_builder_factory()\n        self.event_creation_handler = hs.get_event_creation_handler()\n\n        self.u_alice = UserID.from_string(\"@alice:test\")\n        self.u_bob = UserID.from_string(\"@bob:test\")\n\n        self.room1 = RoomID.from_string(\"!abc123:test\")\n\n        self.get_success(\n            create_room(hs, self.room1.to_string(), self.u_alice.to_string())\n        )\n\n        self.depth = 1\n\n    def inject_room_member(\n        self, room, user, membership, replaces_state=None, extra_content={}\n    ):\n        content = {\"membership\": membership}\n        content.update(extra_content)\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Member,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": content,\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def inject_message(self, room, user, body):\n        self.depth += 1\n\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Message,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": {\"body\": body, \"msgtype\": \"message\"},\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def inject_redaction(self, room, event_id, user, reason):\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Redaction,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": {\"reason\": reason},\n                \"redacts\": event_id,\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def test_redact(self):\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        # Check event has not been redacted:\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"body\": \"t\", \"msgtype\": \"message\"},\n            },\n            event,\n        )\n\n        self.assertFalse(\"redacted_because\" in event.unsigned)\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertEqual(msg_event.event_id, event.event_id)\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {},\n            },\n            event,\n        )\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Redaction,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"reason\": reason},\n            },\n            event.unsigned[\"redacted_because\"],\n        )\n\n    def test_redact_join(self):\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(\n            self.inject_room_member(\n                self.room1, self.u_bob, Membership.JOIN, extra_content={\"blue\": \"red\"}\n            )\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Member,\n                \"user_id\": self.u_bob.to_string(),\n                \"content\": {\"membership\": Membership.JOIN, \"blue\": \"red\"},\n            },\n            event,\n        )\n\n        self.assertFalse(hasattr(event, \"redacted_because\"))\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        # Check redaction\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Member,\n                \"user_id\": self.u_bob.to_string(),\n                \"content\": {\"membership\": Membership.JOIN},\n            },\n            event,\n        )\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Redaction,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"reason\": reason},\n            },\n            event.unsigned[\"redacted_because\"],\n        )\n\n    def test_circular_redaction(self):\n        redaction_event_id1 = \"$redaction1_id:test\"\n        redaction_event_id2 = \"$redaction2_id:test\"\n\n        class EventIdManglingBuilder:\n            def __init__(self, base_builder, event_id):\n                self._base_builder = base_builder\n                self._event_id = event_id\n\n            @defer.inlineCallbacks\n            def build(self, prev_event_ids, auth_event_ids):\n                built_event = yield defer.ensureDeferred(\n                    self._base_builder.build(prev_event_ids, auth_event_ids)\n                )\n\n                built_event._event_id = self._event_id\n                built_event._dict[\"event_id\"] = self._event_id\n                assert built_event.event_id == self._event_id\n\n                return built_event\n\n            @property\n            def room_id(self):\n                return self._base_builder.room_id\n\n            @property\n            def type(self):\n                return self._base_builder.type\n\n        event_1, context_1 = self.get_success(\n            self.event_creation_handler.create_new_client_event(\n                EventIdManglingBuilder(\n                    self.event_builder_factory.for_room_version(\n                        RoomVersions.V1,\n                        {\n                            \"type\": EventTypes.Redaction,\n                            \"sender\": self.u_alice.to_string(),\n                            \"room_id\": self.room1.to_string(),\n                            \"content\": {\"reason\": \"test\"},\n                            \"redacts\": redaction_event_id2,\n                        },\n                    ),\n                    redaction_event_id1,\n                )\n            )\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event_1, context_1))\n\n        event_2, context_2 = self.get_success(\n            self.event_creation_handler.create_new_client_event(\n                EventIdManglingBuilder(\n                    self.event_builder_factory.for_room_version(\n                        RoomVersions.V1,\n                        {\n                            \"type\": EventTypes.Redaction,\n                            \"sender\": self.u_alice.to_string(),\n                            \"room_id\": self.room1.to_string(),\n                            \"content\": {\"reason\": \"test\"},\n                            \"redacts\": redaction_event_id1,\n                        },\n                    ),\n                    redaction_event_id2,\n                )\n            )\n        )\n        self.get_success(self.storage.persistence.persist_event(event_2, context_2))\n\n        # fetch one of the redactions\n        fetched = self.get_success(self.store.get_event(redaction_event_id1))\n\n        # it should have been redacted\n        self.assertEqual(fetched.unsigned[\"redacted_by\"], redaction_event_id2)\n        self.assertEqual(\n            fetched.unsigned[\"redacted_because\"].event_id, redaction_event_id2\n        )\n\n    def test_redact_censor(self):\n        \"\"\"Test that a redacted event gets censored in the DB after a month\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        # Check event has not been redacted:\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"body\": \"t\", \"msgtype\": \"message\"},\n            },\n            event,\n        )\n\n        self.assertFalse(\"redacted_because\" in event.unsigned)\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {},\n            },\n            event,\n        )\n\n        event_json = self.get_success(\n            self.store.db_pool.simple_select_one_onecol(\n                table=\"event_json\",\n                keyvalues={\"event_id\": msg_event.event_id},\n                retcol=\"json\",\n            )\n        )\n\n        self.assert_dict(\n            {\"content\": {\"body\": \"t\", \"msgtype\": \"message\"}}, json.loads(event_json)\n        )\n\n        # Advance by 30 days, then advance again to ensure that the looping call\n        # for updating the stream position gets called and then the looping call\n        # for the censoring gets called.\n        self.reactor.advance(60 * 60 * 24 * 31)\n        self.reactor.advance(60 * 60 * 2)\n\n        event_json = self.get_success(\n            self.store.db_pool.simple_select_one_onecol(\n                table=\"event_json\",\n                keyvalues={\"event_id\": msg_event.event_id},\n                retcol=\"json\",\n            )\n        )\n\n        self.assert_dict({\"content\": {}}, json.loads(event_json))\n\n    def test_redact_redaction(self):\n        \"\"\"Tests that we can redact a redaction and can fetch it again.\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        first_redact_event = self.get_success(\n            self.inject_redaction(\n                self.room1, msg_event.event_id, self.u_alice, \"Redacting message\"\n            )\n        )\n\n        self.get_success(\n            self.inject_redaction(\n                self.room1,\n                first_redact_event.event_id,\n                self.u_alice,\n                \"Redacting redaction\",\n            )\n        )\n\n        # Now lets jump to the future where we have censored the redaction event\n        # in the DB.\n        self.reactor.advance(60 * 60 * 24 * 31)\n\n        # We just want to check that fetching the event doesn't raise an exception.\n        self.get_success(\n            self.store.get_event(first_redact_event.event_id, allow_none=True)\n        )\n\n    def test_store_redacted_redaction(self):\n        \"\"\"Tests that we can store a redacted redaction.\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Redaction,\n                \"sender\": self.u_alice.to_string(),\n                \"room_id\": self.room1.to_string(),\n                \"content\": {\"reason\": \"foo\"},\n            },\n        )\n\n        redaction_event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(\n            self.storage.persistence.persist_event(redaction_event, context)\n        )\n\n        # Now lets jump to the future where we have censored the redaction event\n        # in the DB.\n        self.reactor.advance(60 * 60 * 24 * 31)\n\n        # We just want to check that fetching the event doesn't raise an exception.\n        self.get_success(\n            self.store.get_event(redaction_event.event_id, allow_none=True)\n        )\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom mock import Mock\n\nfrom canonicaljson import json\n\nfrom twisted.internet import defer\n\nfrom synapse.api.constants import EventTypes, Membership\nfrom synapse.api.room_versions import RoomVersions\nfrom synapse.types import RoomID, UserID\n\nfrom tests import unittest\nfrom tests.utils import create_room\n\n\nclass RedactionTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"redaction_retention_period\"] = \"30d\"\n        return self.setup_test_homeserver(\n            resource_for_federation=Mock(), http_client=None, config=config\n        )\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.event_builder_factory = hs.get_event_builder_factory()\n        self.event_creation_handler = hs.get_event_creation_handler()\n\n        self.u_alice = UserID.from_string(\"@alice:test\")\n        self.u_bob = UserID.from_string(\"@bob:test\")\n\n        self.room1 = RoomID.from_string(\"!abc123:test\")\n\n        self.get_success(\n            create_room(hs, self.room1.to_string(), self.u_alice.to_string())\n        )\n\n        self.depth = 1\n\n    def inject_room_member(\n        self, room, user, membership, replaces_state=None, extra_content={}\n    ):\n        content = {\"membership\": membership}\n        content.update(extra_content)\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Member,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": content,\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def inject_message(self, room, user, body):\n        self.depth += 1\n\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Message,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": {\"body\": body, \"msgtype\": \"message\"},\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def inject_redaction(self, room, event_id, user, reason):\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Redaction,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": {\"reason\": reason},\n                \"redacts\": event_id,\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def test_redact(self):\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        # Check event has not been redacted:\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"body\": \"t\", \"msgtype\": \"message\"},\n            },\n            event,\n        )\n\n        self.assertFalse(\"redacted_because\" in event.unsigned)\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertEqual(msg_event.event_id, event.event_id)\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {},\n            },\n            event,\n        )\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Redaction,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"reason\": reason},\n            },\n            event.unsigned[\"redacted_because\"],\n        )\n\n    def test_redact_join(self):\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(\n            self.inject_room_member(\n                self.room1, self.u_bob, Membership.JOIN, extra_content={\"blue\": \"red\"}\n            )\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Member,\n                \"user_id\": self.u_bob.to_string(),\n                \"content\": {\"membership\": Membership.JOIN, \"blue\": \"red\"},\n            },\n            event,\n        )\n\n        self.assertFalse(hasattr(event, \"redacted_because\"))\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        # Check redaction\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Member,\n                \"user_id\": self.u_bob.to_string(),\n                \"content\": {\"membership\": Membership.JOIN},\n            },\n            event,\n        )\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Redaction,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"reason\": reason},\n            },\n            event.unsigned[\"redacted_because\"],\n        )\n\n    def test_circular_redaction(self):\n        redaction_event_id1 = \"$redaction1_id:test\"\n        redaction_event_id2 = \"$redaction2_id:test\"\n\n        class EventIdManglingBuilder:\n            def __init__(self, base_builder, event_id):\n                self._base_builder = base_builder\n                self._event_id = event_id\n\n            @defer.inlineCallbacks\n            def build(self, prev_event_ids, auth_event_ids):\n                built_event = yield defer.ensureDeferred(\n                    self._base_builder.build(prev_event_ids, auth_event_ids)\n                )\n\n                built_event._event_id = self._event_id\n                built_event._dict[\"event_id\"] = self._event_id\n                assert built_event.event_id == self._event_id\n\n                return built_event\n\n            @property\n            def room_id(self):\n                return self._base_builder.room_id\n\n            @property\n            def type(self):\n                return self._base_builder.type\n\n        event_1, context_1 = self.get_success(\n            self.event_creation_handler.create_new_client_event(\n                EventIdManglingBuilder(\n                    self.event_builder_factory.for_room_version(\n                        RoomVersions.V1,\n                        {\n                            \"type\": EventTypes.Redaction,\n                            \"sender\": self.u_alice.to_string(),\n                            \"room_id\": self.room1.to_string(),\n                            \"content\": {\"reason\": \"test\"},\n                            \"redacts\": redaction_event_id2,\n                        },\n                    ),\n                    redaction_event_id1,\n                )\n            )\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event_1, context_1))\n\n        event_2, context_2 = self.get_success(\n            self.event_creation_handler.create_new_client_event(\n                EventIdManglingBuilder(\n                    self.event_builder_factory.for_room_version(\n                        RoomVersions.V1,\n                        {\n                            \"type\": EventTypes.Redaction,\n                            \"sender\": self.u_alice.to_string(),\n                            \"room_id\": self.room1.to_string(),\n                            \"content\": {\"reason\": \"test\"},\n                            \"redacts\": redaction_event_id1,\n                        },\n                    ),\n                    redaction_event_id2,\n                )\n            )\n        )\n        self.get_success(self.storage.persistence.persist_event(event_2, context_2))\n\n        # fetch one of the redactions\n        fetched = self.get_success(self.store.get_event(redaction_event_id1))\n\n        # it should have been redacted\n        self.assertEqual(fetched.unsigned[\"redacted_by\"], redaction_event_id2)\n        self.assertEqual(\n            fetched.unsigned[\"redacted_because\"].event_id, redaction_event_id2\n        )\n\n    def test_redact_censor(self):\n        \"\"\"Test that a redacted event gets censored in the DB after a month\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        # Check event has not been redacted:\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"body\": \"t\", \"msgtype\": \"message\"},\n            },\n            event,\n        )\n\n        self.assertFalse(\"redacted_because\" in event.unsigned)\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {},\n            },\n            event,\n        )\n\n        event_json = self.get_success(\n            self.store.db_pool.simple_select_one_onecol(\n                table=\"event_json\",\n                keyvalues={\"event_id\": msg_event.event_id},\n                retcol=\"json\",\n            )\n        )\n\n        self.assert_dict(\n            {\"content\": {\"body\": \"t\", \"msgtype\": \"message\"}}, json.loads(event_json)\n        )\n\n        # Advance by 30 days, then advance again to ensure that the looping call\n        # for updating the stream position gets called and then the looping call\n        # for the censoring gets called.\n        self.reactor.advance(60 * 60 * 24 * 31)\n        self.reactor.advance(60 * 60 * 2)\n\n        event_json = self.get_success(\n            self.store.db_pool.simple_select_one_onecol(\n                table=\"event_json\",\n                keyvalues={\"event_id\": msg_event.event_id},\n                retcol=\"json\",\n            )\n        )\n\n        self.assert_dict({\"content\": {}}, json.loads(event_json))\n\n    def test_redact_redaction(self):\n        \"\"\"Tests that we can redact a redaction and can fetch it again.\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        first_redact_event = self.get_success(\n            self.inject_redaction(\n                self.room1, msg_event.event_id, self.u_alice, \"Redacting message\"\n            )\n        )\n\n        self.get_success(\n            self.inject_redaction(\n                self.room1,\n                first_redact_event.event_id,\n                self.u_alice,\n                \"Redacting redaction\",\n            )\n        )\n\n        # Now lets jump to the future where we have censored the redaction event\n        # in the DB.\n        self.reactor.advance(60 * 60 * 24 * 31)\n\n        # We just want to check that fetching the event doesn't raise an exception.\n        self.get_success(\n            self.store.get_event(first_redact_event.event_id, allow_none=True)\n        )\n\n    def test_store_redacted_redaction(self):\n        \"\"\"Tests that we can store a redacted redaction.\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Redaction,\n                \"sender\": self.u_alice.to_string(),\n                \"room_id\": self.room1.to_string(),\n                \"content\": {\"reason\": \"foo\"},\n            },\n        )\n\n        redaction_event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(\n            self.storage.persistence.persist_event(redaction_event, context)\n        )\n\n        # Now lets jump to the future where we have censored the redaction event\n        # in the DB.\n        self.reactor.advance(60 * 60 * 24 * 31)\n\n        # We just want to check that fetching the event doesn't raise an exception.\n        self.get_success(\n            self.store.get_event(redaction_event.event_id, allow_none=True)\n        )\n", "patch": "@@ -34,7 +34,7 @@ def make_homeserver(self, reactor, clock):\n         config = self.default_config()\n         config[\"redaction_retention_period\"] = \"30d\"\n         return self.setup_test_homeserver(\n-            resource_for_federation=Mock(), http_client=None, config=config\n+            resource_for_federation=Mock(), federation_http_client=None, config=config\n         )\n \n     def prepare(self, reactor, clock, hs):", "file_path": "files/2021_2/54", "file_language": "py", "file_name": "tests/storage/test_redaction.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class RedactionTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"redaction_retention_period\"] = \"30d\"\n        return self.setup_test_homeserver(\n            resource_for_federation=Mock(), http_client=None, config=config\n        )\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.event_builder_factory = hs.get_event_builder_factory()\n        self.event_creation_handler = hs.get_event_creation_handler()\n\n        self.u_alice = UserID.from_string(\"@alice:test\")\n        self.u_bob = UserID.from_string(\"@bob:test\")\n\n        self.room1 = RoomID.from_string(\"!abc123:test\")\n\n        self.get_success(\n            create_room(hs, self.room1.to_string(), self.u_alice.to_string())\n        )\n\n        self.depth = 1\n\n    def inject_room_member(\n        self, room, user, membership, replaces_state=None, extra_content={}\n    ):\n        content = {\"membership\": membership}\n        content.update(extra_content)\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Member,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": content,\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def inject_message(self, room, user, body):\n        self.depth += 1\n\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Message,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": {\"body\": body, \"msgtype\": \"message\"},\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def inject_redaction(self, room, event_id, user, reason):\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Redaction,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": {\"reason\": reason},\n                \"redacts\": event_id,\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def test_redact(self):\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        # Check event has not been redacted:\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"body\": \"t\", \"msgtype\": \"message\"},\n            },\n            event,\n        )\n\n        self.assertFalse(\"redacted_because\" in event.unsigned)\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertEqual(msg_event.event_id, event.event_id)\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {},\n            },\n            event,\n        )\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Redaction,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"reason\": reason},\n            },\n            event.unsigned[\"redacted_because\"],\n        )\n\n    def test_redact_join(self):\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(\n            self.inject_room_member(\n                self.room1, self.u_bob, Membership.JOIN, extra_content={\"blue\": \"red\"}\n            )\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Member,\n                \"user_id\": self.u_bob.to_string(),\n                \"content\": {\"membership\": Membership.JOIN, \"blue\": \"red\"},\n            },\n            event,\n        )\n\n        self.assertFalse(hasattr(event, \"redacted_because\"))\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        # Check redaction\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Member,\n                \"user_id\": self.u_bob.to_string(),\n                \"content\": {\"membership\": Membership.JOIN},\n            },\n            event,\n        )\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Redaction,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"reason\": reason},\n            },\n            event.unsigned[\"redacted_because\"],\n        )\n\n    def test_circular_redaction(self):\n        redaction_event_id1 = \"$redaction1_id:test\"\n        redaction_event_id2 = \"$redaction2_id:test\"\n\n        class EventIdManglingBuilder:\n            def __init__(self, base_builder, event_id):\n                self._base_builder = base_builder\n                self._event_id = event_id\n\n            @defer.inlineCallbacks\n            def build(self, prev_event_ids, auth_event_ids):\n                built_event = yield defer.ensureDeferred(\n                    self._base_builder.build(prev_event_ids, auth_event_ids)\n                )\n\n                built_event._event_id = self._event_id\n                built_event._dict[\"event_id\"] = self._event_id\n                assert built_event.event_id == self._event_id\n\n                return built_event\n\n            @property\n            def room_id(self):\n                return self._base_builder.room_id\n\n            @property\n            def type(self):\n                return self._base_builder.type\n\n        event_1, context_1 = self.get_success(\n            self.event_creation_handler.create_new_client_event(\n                EventIdManglingBuilder(\n                    self.event_builder_factory.for_room_version(\n                        RoomVersions.V1,\n                        {\n                            \"type\": EventTypes.Redaction,\n                            \"sender\": self.u_alice.to_string(),\n                            \"room_id\": self.room1.to_string(),\n                            \"content\": {\"reason\": \"test\"},\n                            \"redacts\": redaction_event_id2,\n                        },\n                    ),\n                    redaction_event_id1,\n                )\n            )\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event_1, context_1))\n\n        event_2, context_2 = self.get_success(\n            self.event_creation_handler.create_new_client_event(\n                EventIdManglingBuilder(\n                    self.event_builder_factory.for_room_version(\n                        RoomVersions.V1,\n                        {\n                            \"type\": EventTypes.Redaction,\n                            \"sender\": self.u_alice.to_string(),\n                            \"room_id\": self.room1.to_string(),\n                            \"content\": {\"reason\": \"test\"},\n                            \"redacts\": redaction_event_id1,\n                        },\n                    ),\n                    redaction_event_id2,\n                )\n            )\n        )\n        self.get_success(self.storage.persistence.persist_event(event_2, context_2))\n\n        # fetch one of the redactions\n        fetched = self.get_success(self.store.get_event(redaction_event_id1))\n\n        # it should have been redacted\n        self.assertEqual(fetched.unsigned[\"redacted_by\"], redaction_event_id2)\n        self.assertEqual(\n            fetched.unsigned[\"redacted_because\"].event_id, redaction_event_id2\n        )\n\n    def test_redact_censor(self):\n        \"\"\"Test that a redacted event gets censored in the DB after a month\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        # Check event has not been redacted:\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"body\": \"t\", \"msgtype\": \"message\"},\n            },\n            event,\n        )\n\n        self.assertFalse(\"redacted_because\" in event.unsigned)\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {},\n            },\n            event,\n        )\n\n        event_json = self.get_success(\n            self.store.db_pool.simple_select_one_onecol(\n                table=\"event_json\",\n                keyvalues={\"event_id\": msg_event.event_id},\n                retcol=\"json\",\n            )\n        )\n\n        self.assert_dict(\n            {\"content\": {\"body\": \"t\", \"msgtype\": \"message\"}}, json.loads(event_json)\n        )\n\n        # Advance by 30 days, then advance again to ensure that the looping call\n        # for updating the stream position gets called and then the looping call\n        # for the censoring gets called.\n        self.reactor.advance(60 * 60 * 24 * 31)\n        self.reactor.advance(60 * 60 * 2)\n\n        event_json = self.get_success(\n            self.store.db_pool.simple_select_one_onecol(\n                table=\"event_json\",\n                keyvalues={\"event_id\": msg_event.event_id},\n                retcol=\"json\",\n            )\n        )\n\n        self.assert_dict({\"content\": {}}, json.loads(event_json))\n\n    def test_redact_redaction(self):\n        \"\"\"Tests that we can redact a redaction and can fetch it again.\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        first_redact_event = self.get_success(\n            self.inject_redaction(\n                self.room1, msg_event.event_id, self.u_alice, \"Redacting message\"\n            )\n        )\n\n        self.get_success(\n            self.inject_redaction(\n                self.room1,\n                first_redact_event.event_id,\n                self.u_alice,\n                \"Redacting redaction\",\n            )\n        )\n\n        # Now lets jump to the future where we have censored the redaction event\n        # in the DB.\n        self.reactor.advance(60 * 60 * 24 * 31)\n\n        # We just want to check that fetching the event doesn't raise an exception.\n        self.get_success(\n            self.store.get_event(first_redact_event.event_id, allow_none=True)\n        )\n\n    def test_store_redacted_redaction(self):\n        \"\"\"Tests that we can store a redacted redaction.\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Redaction,\n                \"sender\": self.u_alice.to_string(),\n                \"room_id\": self.room1.to_string(),\n                \"content\": {\"reason\": \"foo\"},\n            },\n        )\n\n        redaction_event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(\n            self.storage.persistence.persist_event(redaction_event, context)\n        )\n\n        # Now lets jump to the future where we have censored the redaction event\n        # in the DB.\n        self.reactor.advance(60 * 60 * 24 * 31)\n\n        # We just want to check that fetching the event doesn't raise an exception.\n        self.get_success(\n            self.store.get_event(redaction_event.event_id, allow_none=True)\n        )", "target": 0}], "function_after": [{"function": "class RedactionTestCase(unittest.HomeserverTestCase):\n    def make_homeserver(self, reactor, clock):\n        config = self.default_config()\n        config[\"redaction_retention_period\"] = \"30d\"\n        return self.setup_test_homeserver(\n            resource_for_federation=Mock(), federation_http_client=None, config=config\n        )\n\n    def prepare(self, reactor, clock, hs):\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.event_builder_factory = hs.get_event_builder_factory()\n        self.event_creation_handler = hs.get_event_creation_handler()\n\n        self.u_alice = UserID.from_string(\"@alice:test\")\n        self.u_bob = UserID.from_string(\"@bob:test\")\n\n        self.room1 = RoomID.from_string(\"!abc123:test\")\n\n        self.get_success(\n            create_room(hs, self.room1.to_string(), self.u_alice.to_string())\n        )\n\n        self.depth = 1\n\n    def inject_room_member(\n        self, room, user, membership, replaces_state=None, extra_content={}\n    ):\n        content = {\"membership\": membership}\n        content.update(extra_content)\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Member,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": content,\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def inject_message(self, room, user, body):\n        self.depth += 1\n\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Message,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": {\"body\": body, \"msgtype\": \"message\"},\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def inject_redaction(self, room, event_id, user, reason):\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Redaction,\n                \"sender\": user.to_string(),\n                \"state_key\": user.to_string(),\n                \"room_id\": room.to_string(),\n                \"content\": {\"reason\": reason},\n                \"redacts\": event_id,\n            },\n        )\n\n        event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event, context))\n\n        return event\n\n    def test_redact(self):\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        # Check event has not been redacted:\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"body\": \"t\", \"msgtype\": \"message\"},\n            },\n            event,\n        )\n\n        self.assertFalse(\"redacted_because\" in event.unsigned)\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertEqual(msg_event.event_id, event.event_id)\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {},\n            },\n            event,\n        )\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Redaction,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"reason\": reason},\n            },\n            event.unsigned[\"redacted_because\"],\n        )\n\n    def test_redact_join(self):\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(\n            self.inject_room_member(\n                self.room1, self.u_bob, Membership.JOIN, extra_content={\"blue\": \"red\"}\n            )\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Member,\n                \"user_id\": self.u_bob.to_string(),\n                \"content\": {\"membership\": Membership.JOIN, \"blue\": \"red\"},\n            },\n            event,\n        )\n\n        self.assertFalse(hasattr(event, \"redacted_because\"))\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        # Check redaction\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Member,\n                \"user_id\": self.u_bob.to_string(),\n                \"content\": {\"membership\": Membership.JOIN},\n            },\n            event,\n        )\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Redaction,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"reason\": reason},\n            },\n            event.unsigned[\"redacted_because\"],\n        )\n\n    def test_circular_redaction(self):\n        redaction_event_id1 = \"$redaction1_id:test\"\n        redaction_event_id2 = \"$redaction2_id:test\"\n\n        class EventIdManglingBuilder:\n            def __init__(self, base_builder, event_id):\n                self._base_builder = base_builder\n                self._event_id = event_id\n\n            @defer.inlineCallbacks\n            def build(self, prev_event_ids, auth_event_ids):\n                built_event = yield defer.ensureDeferred(\n                    self._base_builder.build(prev_event_ids, auth_event_ids)\n                )\n\n                built_event._event_id = self._event_id\n                built_event._dict[\"event_id\"] = self._event_id\n                assert built_event.event_id == self._event_id\n\n                return built_event\n\n            @property\n            def room_id(self):\n                return self._base_builder.room_id\n\n            @property\n            def type(self):\n                return self._base_builder.type\n\n        event_1, context_1 = self.get_success(\n            self.event_creation_handler.create_new_client_event(\n                EventIdManglingBuilder(\n                    self.event_builder_factory.for_room_version(\n                        RoomVersions.V1,\n                        {\n                            \"type\": EventTypes.Redaction,\n                            \"sender\": self.u_alice.to_string(),\n                            \"room_id\": self.room1.to_string(),\n                            \"content\": {\"reason\": \"test\"},\n                            \"redacts\": redaction_event_id2,\n                        },\n                    ),\n                    redaction_event_id1,\n                )\n            )\n        )\n\n        self.get_success(self.storage.persistence.persist_event(event_1, context_1))\n\n        event_2, context_2 = self.get_success(\n            self.event_creation_handler.create_new_client_event(\n                EventIdManglingBuilder(\n                    self.event_builder_factory.for_room_version(\n                        RoomVersions.V1,\n                        {\n                            \"type\": EventTypes.Redaction,\n                            \"sender\": self.u_alice.to_string(),\n                            \"room_id\": self.room1.to_string(),\n                            \"content\": {\"reason\": \"test\"},\n                            \"redacts\": redaction_event_id1,\n                        },\n                    ),\n                    redaction_event_id2,\n                )\n            )\n        )\n        self.get_success(self.storage.persistence.persist_event(event_2, context_2))\n\n        # fetch one of the redactions\n        fetched = self.get_success(self.store.get_event(redaction_event_id1))\n\n        # it should have been redacted\n        self.assertEqual(fetched.unsigned[\"redacted_by\"], redaction_event_id2)\n        self.assertEqual(\n            fetched.unsigned[\"redacted_because\"].event_id, redaction_event_id2\n        )\n\n    def test_redact_censor(self):\n        \"\"\"Test that a redacted event gets censored in the DB after a month\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        # Check event has not been redacted:\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {\"body\": \"t\", \"msgtype\": \"message\"},\n            },\n            event,\n        )\n\n        self.assertFalse(\"redacted_because\" in event.unsigned)\n\n        # Redact event\n        reason = \"Because I said so\"\n        self.get_success(\n            self.inject_redaction(self.room1, msg_event.event_id, self.u_alice, reason)\n        )\n\n        event = self.get_success(self.store.get_event(msg_event.event_id))\n\n        self.assertTrue(\"redacted_because\" in event.unsigned)\n\n        self.assertObjectHasAttributes(\n            {\n                \"type\": EventTypes.Message,\n                \"user_id\": self.u_alice.to_string(),\n                \"content\": {},\n            },\n            event,\n        )\n\n        event_json = self.get_success(\n            self.store.db_pool.simple_select_one_onecol(\n                table=\"event_json\",\n                keyvalues={\"event_id\": msg_event.event_id},\n                retcol=\"json\",\n            )\n        )\n\n        self.assert_dict(\n            {\"content\": {\"body\": \"t\", \"msgtype\": \"message\"}}, json.loads(event_json)\n        )\n\n        # Advance by 30 days, then advance again to ensure that the looping call\n        # for updating the stream position gets called and then the looping call\n        # for the censoring gets called.\n        self.reactor.advance(60 * 60 * 24 * 31)\n        self.reactor.advance(60 * 60 * 2)\n\n        event_json = self.get_success(\n            self.store.db_pool.simple_select_one_onecol(\n                table=\"event_json\",\n                keyvalues={\"event_id\": msg_event.event_id},\n                retcol=\"json\",\n            )\n        )\n\n        self.assert_dict({\"content\": {}}, json.loads(event_json))\n\n    def test_redact_redaction(self):\n        \"\"\"Tests that we can redact a redaction and can fetch it again.\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        msg_event = self.get_success(self.inject_message(self.room1, self.u_alice, \"t\"))\n\n        first_redact_event = self.get_success(\n            self.inject_redaction(\n                self.room1, msg_event.event_id, self.u_alice, \"Redacting message\"\n            )\n        )\n\n        self.get_success(\n            self.inject_redaction(\n                self.room1,\n                first_redact_event.event_id,\n                self.u_alice,\n                \"Redacting redaction\",\n            )\n        )\n\n        # Now lets jump to the future where we have censored the redaction event\n        # in the DB.\n        self.reactor.advance(60 * 60 * 24 * 31)\n\n        # We just want to check that fetching the event doesn't raise an exception.\n        self.get_success(\n            self.store.get_event(first_redact_event.event_id, allow_none=True)\n        )\n\n    def test_store_redacted_redaction(self):\n        \"\"\"Tests that we can store a redacted redaction.\n        \"\"\"\n\n        self.get_success(\n            self.inject_room_member(self.room1, self.u_alice, Membership.JOIN)\n        )\n\n        builder = self.event_builder_factory.for_room_version(\n            RoomVersions.V1,\n            {\n                \"type\": EventTypes.Redaction,\n                \"sender\": self.u_alice.to_string(),\n                \"room_id\": self.room1.to_string(),\n                \"content\": {\"reason\": \"foo\"},\n            },\n        )\n\n        redaction_event, context = self.get_success(\n            self.event_creation_handler.create_new_client_event(builder)\n        )\n\n        self.get_success(\n            self.storage.persistence.persist_event(redaction_event, context)\n        )\n\n        # Now lets jump to the future where we have censored the redaction event\n        # in the DB.\n        self.reactor.advance(60 * 60 * 24 * 31)\n\n        # We just want to check that fetching the event doesn't raise an exception.\n        self.get_success(\n            self.store.get_event(redaction_event.event_id, allow_none=True)\n        )", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Fstorage%2Ftest_roommember.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom unittest.mock import Mock\n\nfrom synapse.api.constants import Membership\nfrom synapse.rest.admin import register_servlets_for_client_rest_resource\nfrom synapse.rest.client.v1 import login, room\nfrom synapse.types import UserID, create_requester\n\nfrom tests import unittest\nfrom tests.test_utils import event_injection\nfrom tests.utils import TestHomeServer\n\n\nclass RoomMemberStoreTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        login.register_servlets,\n        register_servlets_for_client_rest_resource,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            resource_for_federation=Mock(), federation_http_client=None\n        )\n        return hs\n\n    def prepare(self, reactor, clock, hs: TestHomeServer):\n\n        # We can't test the RoomMemberStore on its own without the other event\n        # storage logic\n        self.store = hs.get_datastore()\n\n        self.u_alice = self.register_user(\"alice\", \"pass\")\n        self.t_alice = self.login(\"alice\", \"pass\")\n        self.u_bob = self.register_user(\"bob\", \"pass\")\n\n        # User elsewhere on another host\n        self.u_charlie = UserID.from_string(\"@charlie:elsewhere\")\n\n    def test_one_member(self):\n\n        # Alice creates the room, and is automatically joined\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n\n        rooms_for_user = self.get_success(\n            self.store.get_rooms_for_local_user_where_membership_is(\n                self.u_alice, [Membership.JOIN]\n            )\n        )\n\n        self.assertEquals([self.room], [m.room_id for m in rooms_for_user])\n\n    def test_count_known_servers(self):\n        \"\"\"\n        _count_known_servers will calculate how many servers are in a room.\n        \"\"\"\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        servers = self.get_success(self.store._count_known_servers())\n        self.assertEqual(servers, 2)\n\n    def test_count_known_servers_stat_counter_disabled(self):\n        \"\"\"\n        If enabled, the metrics for how many servers are known will be counted.\n        \"\"\"\n        self.assertTrue(\"_known_servers_count\" not in self.store.__dict__.keys())\n\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        self.pump()\n\n        self.assertTrue(\"_known_servers_count\" not in self.store.__dict__.keys())\n\n    @unittest.override_config(\n        {\"enable_metrics\": True, \"metrics_flags\": {\"known_servers\": True}}\n    )\n    def test_count_known_servers_stat_counter_enabled(self):\n        \"\"\"\n        If enabled, the metrics for how many servers are known will be counted.\n        \"\"\"\n        # Initialises to 1 -- itself\n        self.assertEqual(self.store._known_servers_count, 1)\n\n        self.pump()\n\n        # No rooms have been joined, so technically the SQL returns 0, but it\n        # will still say it knows about itself.\n        self.assertEqual(self.store._known_servers_count, 1)\n\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        self.pump(1)\n\n        # It now knows about Charlie's server.\n        self.assertEqual(self.store._known_servers_count, 2)\n\n    def test_get_joined_users_from_context(self):\n        room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        bob_event = self.get_success(\n            event_injection.inject_member_event(\n                self.hs, room, self.u_bob, Membership.JOIN\n            )\n        )\n\n        # first, create a regular event\n        event, context = self.get_success(\n            event_injection.create_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_alice,\n                prev_event_ids=[bob_event.event_id],\n                type=\"m.test.1\",\n                content={},\n            )\n        )\n\n        users = self.get_success(\n            self.store.get_joined_users_from_context(event, context)\n        )\n        self.assertEqual(users.keys(), {self.u_alice, self.u_bob})\n\n        # Regression test for #7376: create a state event whose key matches bob's\n        # user_id, but which is *not* a membership event, and persist that; then check\n        # that `get_joined_users_from_context` returns the correct users for the next event.\n        non_member_event = self.get_success(\n            event_injection.inject_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_bob,\n                prev_event_ids=[bob_event.event_id],\n                type=\"m.test.2\",\n                state_key=self.u_bob,\n                content={},\n            )\n        )\n        event, context = self.get_success(\n            event_injection.create_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_alice,\n                prev_event_ids=[non_member_event.event_id],\n                type=\"m.test.3\",\n                content={},\n            )\n        )\n        users = self.get_success(\n            self.store.get_joined_users_from_context(event, context)\n        )\n        self.assertEqual(users.keys(), {self.u_alice, self.u_bob})\n\n\nclass CurrentStateMembershipUpdateTestCase(unittest.HomeserverTestCase):\n    def prepare(self, reactor, clock, homeserver):\n        self.store = homeserver.get_datastore()\n        self.room_creator = homeserver.get_room_creation_handler()\n\n    def test_can_rerun_update(self):\n        # First make sure we have completed all updates.\n        while not self.get_success(\n            self.store.db_pool.updates.has_completed_background_updates()\n        ):\n            self.get_success(\n                self.store.db_pool.updates.do_next_background_update(100), by=0.1\n            )\n\n        # Now let's create a room, which will insert a membership\n        user = UserID(\"alice\", \"test\")\n        requester = create_requester(user)\n        self.get_success(self.room_creator.create_room(requester, {}))\n\n        # Register the background update to run again.\n        self.get_success(\n            self.store.db_pool.simple_insert(\n                table=\"background_updates\",\n                values={\n                    \"update_name\": \"current_state_events_membership\",\n                    \"progress_json\": \"{}\",\n                    \"depends_on\": None,\n                },\n            )\n        )\n\n        # ... and tell the DataStore that it hasn't finished all updates yet\n        self.store.db_pool.updates._all_done = False\n\n        # Now let's actually drive the updates to completion\n        while not self.get_success(\n            self.store.db_pool.updates.has_completed_background_updates()\n        ):\n            self.get_success(\n                self.store.db_pool.updates.do_next_background_update(100), by=0.1\n            )\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2019 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom unittest.mock import Mock\n\nfrom synapse.api.constants import Membership\nfrom synapse.rest.admin import register_servlets_for_client_rest_resource\nfrom synapse.rest.client.v1 import login, room\nfrom synapse.types import UserID, create_requester\n\nfrom tests import unittest\nfrom tests.test_utils import event_injection\nfrom tests.utils import TestHomeServer\n\n\nclass RoomMemberStoreTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        login.register_servlets,\n        register_servlets_for_client_rest_resource,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            resource_for_federation=Mock(), http_client=None\n        )\n        return hs\n\n    def prepare(self, reactor, clock, hs: TestHomeServer):\n\n        # We can't test the RoomMemberStore on its own without the other event\n        # storage logic\n        self.store = hs.get_datastore()\n\n        self.u_alice = self.register_user(\"alice\", \"pass\")\n        self.t_alice = self.login(\"alice\", \"pass\")\n        self.u_bob = self.register_user(\"bob\", \"pass\")\n\n        # User elsewhere on another host\n        self.u_charlie = UserID.from_string(\"@charlie:elsewhere\")\n\n    def test_one_member(self):\n\n        # Alice creates the room, and is automatically joined\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n\n        rooms_for_user = self.get_success(\n            self.store.get_rooms_for_local_user_where_membership_is(\n                self.u_alice, [Membership.JOIN]\n            )\n        )\n\n        self.assertEquals([self.room], [m.room_id for m in rooms_for_user])\n\n    def test_count_known_servers(self):\n        \"\"\"\n        _count_known_servers will calculate how many servers are in a room.\n        \"\"\"\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        servers = self.get_success(self.store._count_known_servers())\n        self.assertEqual(servers, 2)\n\n    def test_count_known_servers_stat_counter_disabled(self):\n        \"\"\"\n        If enabled, the metrics for how many servers are known will be counted.\n        \"\"\"\n        self.assertTrue(\"_known_servers_count\" not in self.store.__dict__.keys())\n\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        self.pump()\n\n        self.assertTrue(\"_known_servers_count\" not in self.store.__dict__.keys())\n\n    @unittest.override_config(\n        {\"enable_metrics\": True, \"metrics_flags\": {\"known_servers\": True}}\n    )\n    def test_count_known_servers_stat_counter_enabled(self):\n        \"\"\"\n        If enabled, the metrics for how many servers are known will be counted.\n        \"\"\"\n        # Initialises to 1 -- itself\n        self.assertEqual(self.store._known_servers_count, 1)\n\n        self.pump()\n\n        # No rooms have been joined, so technically the SQL returns 0, but it\n        # will still say it knows about itself.\n        self.assertEqual(self.store._known_servers_count, 1)\n\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        self.pump(1)\n\n        # It now knows about Charlie's server.\n        self.assertEqual(self.store._known_servers_count, 2)\n\n    def test_get_joined_users_from_context(self):\n        room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        bob_event = self.get_success(\n            event_injection.inject_member_event(\n                self.hs, room, self.u_bob, Membership.JOIN\n            )\n        )\n\n        # first, create a regular event\n        event, context = self.get_success(\n            event_injection.create_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_alice,\n                prev_event_ids=[bob_event.event_id],\n                type=\"m.test.1\",\n                content={},\n            )\n        )\n\n        users = self.get_success(\n            self.store.get_joined_users_from_context(event, context)\n        )\n        self.assertEqual(users.keys(), {self.u_alice, self.u_bob})\n\n        # Regression test for #7376: create a state event whose key matches bob's\n        # user_id, but which is *not* a membership event, and persist that; then check\n        # that `get_joined_users_from_context` returns the correct users for the next event.\n        non_member_event = self.get_success(\n            event_injection.inject_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_bob,\n                prev_event_ids=[bob_event.event_id],\n                type=\"m.test.2\",\n                state_key=self.u_bob,\n                content={},\n            )\n        )\n        event, context = self.get_success(\n            event_injection.create_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_alice,\n                prev_event_ids=[non_member_event.event_id],\n                type=\"m.test.3\",\n                content={},\n            )\n        )\n        users = self.get_success(\n            self.store.get_joined_users_from_context(event, context)\n        )\n        self.assertEqual(users.keys(), {self.u_alice, self.u_bob})\n\n\nclass CurrentStateMembershipUpdateTestCase(unittest.HomeserverTestCase):\n    def prepare(self, reactor, clock, homeserver):\n        self.store = homeserver.get_datastore()\n        self.room_creator = homeserver.get_room_creation_handler()\n\n    def test_can_rerun_update(self):\n        # First make sure we have completed all updates.\n        while not self.get_success(\n            self.store.db_pool.updates.has_completed_background_updates()\n        ):\n            self.get_success(\n                self.store.db_pool.updates.do_next_background_update(100), by=0.1\n            )\n\n        # Now let's create a room, which will insert a membership\n        user = UserID(\"alice\", \"test\")\n        requester = create_requester(user)\n        self.get_success(self.room_creator.create_room(requester, {}))\n\n        # Register the background update to run again.\n        self.get_success(\n            self.store.db_pool.simple_insert(\n                table=\"background_updates\",\n                values={\n                    \"update_name\": \"current_state_events_membership\",\n                    \"progress_json\": \"{}\",\n                    \"depends_on\": None,\n                },\n            )\n        )\n\n        # ... and tell the DataStore that it hasn't finished all updates yet\n        self.store.db_pool.updates._all_done = False\n\n        # Now let's actually drive the updates to completion\n        while not self.get_success(\n            self.store.db_pool.updates.has_completed_background_updates()\n        ):\n            self.get_success(\n                self.store.db_pool.updates.do_next_background_update(100), by=0.1\n            )\n", "patch": "@@ -36,7 +36,7 @@ class RoomMemberStoreTestCase(unittest.HomeserverTestCase):\n \n     def make_homeserver(self, reactor, clock):\n         hs = self.setup_test_homeserver(\n-            resource_for_federation=Mock(), http_client=None\n+            resource_for_federation=Mock(), federation_http_client=None\n         )\n         return hs\n ", "file_path": "files/2021_2/55", "file_language": "py", "file_name": "tests/storage/test_roommember.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class RoomMemberStoreTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        login.register_servlets,\n        register_servlets_for_client_rest_resource,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            resource_for_federation=Mock(), http_client=None\n        )\n        return hs\n\n    def prepare(self, reactor, clock, hs: TestHomeServer):\n\n        # We can't test the RoomMemberStore on its own without the other event\n        # storage logic\n        self.store = hs.get_datastore()\n\n        self.u_alice = self.register_user(\"alice\", \"pass\")\n        self.t_alice = self.login(\"alice\", \"pass\")\n        self.u_bob = self.register_user(\"bob\", \"pass\")\n\n        # User elsewhere on another host\n        self.u_charlie = UserID.from_string(\"@charlie:elsewhere\")\n\n    def test_one_member(self):\n\n        # Alice creates the room, and is automatically joined\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n\n        rooms_for_user = self.get_success(\n            self.store.get_rooms_for_local_user_where_membership_is(\n                self.u_alice, [Membership.JOIN]\n            )\n        )\n\n        self.assertEquals([self.room], [m.room_id for m in rooms_for_user])\n\n    def test_count_known_servers(self):\n        \"\"\"\n        _count_known_servers will calculate how many servers are in a room.\n        \"\"\"\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        servers = self.get_success(self.store._count_known_servers())\n        self.assertEqual(servers, 2)\n\n    def test_count_known_servers_stat_counter_disabled(self):\n        \"\"\"\n        If enabled, the metrics for how many servers are known will be counted.\n        \"\"\"\n        self.assertTrue(\"_known_servers_count\" not in self.store.__dict__.keys())\n\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        self.pump()\n\n        self.assertTrue(\"_known_servers_count\" not in self.store.__dict__.keys())\n\n    @unittest.override_config(\n        {\"enable_metrics\": True, \"metrics_flags\": {\"known_servers\": True}}\n    )\n    def test_count_known_servers_stat_counter_enabled(self):\n        \"\"\"\n        If enabled, the metrics for how many servers are known will be counted.\n        \"\"\"\n        # Initialises to 1 -- itself\n        self.assertEqual(self.store._known_servers_count, 1)\n\n        self.pump()\n\n        # No rooms have been joined, so technically the SQL returns 0, but it\n        # will still say it knows about itself.\n        self.assertEqual(self.store._known_servers_count, 1)\n\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        self.pump(1)\n\n        # It now knows about Charlie's server.\n        self.assertEqual(self.store._known_servers_count, 2)\n\n    def test_get_joined_users_from_context(self):\n        room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        bob_event = self.get_success(\n            event_injection.inject_member_event(\n                self.hs, room, self.u_bob, Membership.JOIN\n            )\n        )\n\n        # first, create a regular event\n        event, context = self.get_success(\n            event_injection.create_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_alice,\n                prev_event_ids=[bob_event.event_id],\n                type=\"m.test.1\",\n                content={},\n            )\n        )\n\n        users = self.get_success(\n            self.store.get_joined_users_from_context(event, context)\n        )\n        self.assertEqual(users.keys(), {self.u_alice, self.u_bob})\n\n        # Regression test for #7376: create a state event whose key matches bob's\n        # user_id, but which is *not* a membership event, and persist that; then check\n        # that `get_joined_users_from_context` returns the correct users for the next event.\n        non_member_event = self.get_success(\n            event_injection.inject_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_bob,\n                prev_event_ids=[bob_event.event_id],\n                type=\"m.test.2\",\n                state_key=self.u_bob,\n                content={},\n            )\n        )\n        event, context = self.get_success(\n            event_injection.create_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_alice,\n                prev_event_ids=[non_member_event.event_id],\n                type=\"m.test.3\",\n                content={},\n            )\n        )\n        users = self.get_success(\n            self.store.get_joined_users_from_context(event, context)\n        )\n        self.assertEqual(users.keys(), {self.u_alice, self.u_bob})", "target": 0}, {"function": "class CurrentStateMembershipUpdateTestCase(unittest.HomeserverTestCase):\n    def prepare(self, reactor, clock, homeserver):\n        self.store = homeserver.get_datastore()\n        self.room_creator = homeserver.get_room_creation_handler()\n\n    def test_can_rerun_update(self):\n        # First make sure we have completed all updates.\n        while not self.get_success(\n            self.store.db_pool.updates.has_completed_background_updates()\n        ):\n            self.get_success(\n                self.store.db_pool.updates.do_next_background_update(100), by=0.1\n            )\n\n        # Now let's create a room, which will insert a membership\n        user = UserID(\"alice\", \"test\")\n        requester = create_requester(user)\n        self.get_success(self.room_creator.create_room(requester, {}))\n\n        # Register the background update to run again.\n        self.get_success(\n            self.store.db_pool.simple_insert(\n                table=\"background_updates\",\n                values={\n                    \"update_name\": \"current_state_events_membership\",\n                    \"progress_json\": \"{}\",\n                    \"depends_on\": None,\n                },\n            )\n        )\n\n        # ... and tell the DataStore that it hasn't finished all updates yet\n        self.store.db_pool.updates._all_done = False\n\n        # Now let's actually drive the updates to completion\n        while not self.get_success(\n            self.store.db_pool.updates.has_completed_background_updates()\n        ):\n            self.get_success(\n                self.store.db_pool.updates.do_next_background_update(100), by=0.1\n            )", "target": 0}], "function_after": [{"function": "class RoomMemberStoreTestCase(unittest.HomeserverTestCase):\n\n    servlets = [\n        login.register_servlets,\n        register_servlets_for_client_rest_resource,\n        room.register_servlets,\n    ]\n\n    def make_homeserver(self, reactor, clock):\n        hs = self.setup_test_homeserver(\n            resource_for_federation=Mock(), federation_http_client=None\n        )\n        return hs\n\n    def prepare(self, reactor, clock, hs: TestHomeServer):\n\n        # We can't test the RoomMemberStore on its own without the other event\n        # storage logic\n        self.store = hs.get_datastore()\n\n        self.u_alice = self.register_user(\"alice\", \"pass\")\n        self.t_alice = self.login(\"alice\", \"pass\")\n        self.u_bob = self.register_user(\"bob\", \"pass\")\n\n        # User elsewhere on another host\n        self.u_charlie = UserID.from_string(\"@charlie:elsewhere\")\n\n    def test_one_member(self):\n\n        # Alice creates the room, and is automatically joined\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n\n        rooms_for_user = self.get_success(\n            self.store.get_rooms_for_local_user_where_membership_is(\n                self.u_alice, [Membership.JOIN]\n            )\n        )\n\n        self.assertEquals([self.room], [m.room_id for m in rooms_for_user])\n\n    def test_count_known_servers(self):\n        \"\"\"\n        _count_known_servers will calculate how many servers are in a room.\n        \"\"\"\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        servers = self.get_success(self.store._count_known_servers())\n        self.assertEqual(servers, 2)\n\n    def test_count_known_servers_stat_counter_disabled(self):\n        \"\"\"\n        If enabled, the metrics for how many servers are known will be counted.\n        \"\"\"\n        self.assertTrue(\"_known_servers_count\" not in self.store.__dict__.keys())\n\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        self.pump()\n\n        self.assertTrue(\"_known_servers_count\" not in self.store.__dict__.keys())\n\n    @unittest.override_config(\n        {\"enable_metrics\": True, \"metrics_flags\": {\"known_servers\": True}}\n    )\n    def test_count_known_servers_stat_counter_enabled(self):\n        \"\"\"\n        If enabled, the metrics for how many servers are known will be counted.\n        \"\"\"\n        # Initialises to 1 -- itself\n        self.assertEqual(self.store._known_servers_count, 1)\n\n        self.pump()\n\n        # No rooms have been joined, so technically the SQL returns 0, but it\n        # will still say it knows about itself.\n        self.assertEqual(self.store._known_servers_count, 1)\n\n        self.room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        self.inject_room_member(self.room, self.u_bob, Membership.JOIN)\n        self.inject_room_member(self.room, self.u_charlie.to_string(), Membership.JOIN)\n\n        self.pump(1)\n\n        # It now knows about Charlie's server.\n        self.assertEqual(self.store._known_servers_count, 2)\n\n    def test_get_joined_users_from_context(self):\n        room = self.helper.create_room_as(self.u_alice, tok=self.t_alice)\n        bob_event = self.get_success(\n            event_injection.inject_member_event(\n                self.hs, room, self.u_bob, Membership.JOIN\n            )\n        )\n\n        # first, create a regular event\n        event, context = self.get_success(\n            event_injection.create_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_alice,\n                prev_event_ids=[bob_event.event_id],\n                type=\"m.test.1\",\n                content={},\n            )\n        )\n\n        users = self.get_success(\n            self.store.get_joined_users_from_context(event, context)\n        )\n        self.assertEqual(users.keys(), {self.u_alice, self.u_bob})\n\n        # Regression test for #7376: create a state event whose key matches bob's\n        # user_id, but which is *not* a membership event, and persist that; then check\n        # that `get_joined_users_from_context` returns the correct users for the next event.\n        non_member_event = self.get_success(\n            event_injection.inject_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_bob,\n                prev_event_ids=[bob_event.event_id],\n                type=\"m.test.2\",\n                state_key=self.u_bob,\n                content={},\n            )\n        )\n        event, context = self.get_success(\n            event_injection.create_event(\n                self.hs,\n                room_id=room,\n                sender=self.u_alice,\n                prev_event_ids=[non_member_event.event_id],\n                type=\"m.test.3\",\n                content={},\n            )\n        )\n        users = self.get_success(\n            self.store.get_joined_users_from_context(event, context)\n        )\n        self.assertEqual(users.keys(), {self.u_alice, self.u_bob})", "target": 0}, {"function": "class CurrentStateMembershipUpdateTestCase(unittest.HomeserverTestCase):\n    def prepare(self, reactor, clock, homeserver):\n        self.store = homeserver.get_datastore()\n        self.room_creator = homeserver.get_room_creation_handler()\n\n    def test_can_rerun_update(self):\n        # First make sure we have completed all updates.\n        while not self.get_success(\n            self.store.db_pool.updates.has_completed_background_updates()\n        ):\n            self.get_success(\n                self.store.db_pool.updates.do_next_background_update(100), by=0.1\n            )\n\n        # Now let's create a room, which will insert a membership\n        user = UserID(\"alice\", \"test\")\n        requester = create_requester(user)\n        self.get_success(self.room_creator.create_room(requester, {}))\n\n        # Register the background update to run again.\n        self.get_success(\n            self.store.db_pool.simple_insert(\n                table=\"background_updates\",\n                values={\n                    \"update_name\": \"current_state_events_membership\",\n                    \"progress_json\": \"{}\",\n                    \"depends_on\": None,\n                },\n            )\n        )\n\n        # ... and tell the DataStore that it hasn't finished all updates yet\n        self.store.db_pool.updates._all_done = False\n\n        # Now let's actually drive the updates to completion\n        while not self.get_success(\n            self.store.db_pool.updates.has_completed_background_updates()\n        ):\n            self.get_success(\n                self.store.db_pool.updates.do_next_background_update(100), by=0.1\n            )", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Ftest_federation.py", "code": "# -*- coding: utf-8 -*-\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom mock import Mock\n\nfrom twisted.internet.defer import succeed\n\nfrom synapse.api.errors import FederationError\nfrom synapse.events import make_event_from_dict\nfrom synapse.logging.context import LoggingContext\nfrom synapse.types import UserID, create_requester\nfrom synapse.util import Clock\nfrom synapse.util.retryutils import NotRetryingDestination\n\nfrom tests import unittest\nfrom tests.server import ThreadedMemoryReactorClock, setup_test_homeserver\nfrom tests.test_utils import make_awaitable\n\n\nclass MessageAcceptTests(unittest.HomeserverTestCase):\n    def setUp(self):\n\n        self.http_client = Mock()\n        self.reactor = ThreadedMemoryReactorClock()\n        self.hs_clock = Clock(self.reactor)\n        self.homeserver = setup_test_homeserver(\n            self.addCleanup,\n            federation_http_client=self.http_client,\n            clock=self.hs_clock,\n            reactor=self.reactor,\n        )\n\n        user_id = UserID(\"us\", \"test\")\n        our_user = create_requester(user_id)\n        room_creator = self.homeserver.get_room_creation_handler()\n        self.room_id = self.get_success(\n            room_creator.create_room(\n                our_user, room_creator._presets_dict[\"public_chat\"], ratelimit=False\n            )\n        )[0][\"room_id\"]\n\n        self.store = self.homeserver.get_datastore()\n\n        # Figure out what the most recent event is\n        most_recent = self.get_success(\n            self.homeserver.get_datastore().get_latest_event_ids_in_room(self.room_id)\n        )[0]\n\n        join_event = make_event_from_dict(\n            {\n                \"room_id\": self.room_id,\n                \"sender\": \"@baduser:test.serv\",\n                \"state_key\": \"@baduser:test.serv\",\n                \"event_id\": \"$join:test.serv\",\n                \"depth\": 1000,\n                \"origin_server_ts\": 1,\n                \"type\": \"m.room.member\",\n                \"origin\": \"test.servx\",\n                \"content\": {\"membership\": \"join\"},\n                \"auth_events\": [],\n                \"prev_state\": [(most_recent, {})],\n                \"prev_events\": [(most_recent, {})],\n            }\n        )\n\n        self.handler = self.homeserver.get_federation_handler()\n        self.handler.do_auth = lambda origin, event, context, auth_events: succeed(\n            context\n        )\n        self.client = self.homeserver.get_federation_client()\n        self.client._check_sigs_and_hash_and_fetch = lambda dest, pdus, **k: succeed(\n            pdus\n        )\n\n        # Send the join, it should return None (which is not an error)\n        self.assertEqual(\n            self.get_success(\n                self.handler.on_receive_pdu(\n                    \"test.serv\", join_event, sent_to_us_directly=True\n                )\n            ),\n            None,\n        )\n\n        # Make sure we actually joined the room\n        self.assertEqual(\n            self.get_success(self.store.get_latest_event_ids_in_room(self.room_id))[0],\n            \"$join:test.serv\",\n        )\n\n    def test_cant_hide_direct_ancestors(self):\n        \"\"\"\n        If you send a message, you must be able to provide the direct\n        prev_events that said event references.\n        \"\"\"\n\n        async def post_json(destination, path, data, headers=None, timeout=0):\n            # If it asks us for new missing events, give them NOTHING\n            if path.startswith(\"/_matrix/federation/v1/get_missing_events/\"):\n                return {\"events\": []}\n\n        self.http_client.post_json = post_json\n\n        # Figure out what the most recent event is\n        most_recent = self.get_success(\n            self.store.get_latest_event_ids_in_room(self.room_id)\n        )[0]\n\n        # Now lie about an event\n        lying_event = make_event_from_dict(\n            {\n                \"room_id\": self.room_id,\n                \"sender\": \"@baduser:test.serv\",\n                \"event_id\": \"one:test.serv\",\n                \"depth\": 1000,\n                \"origin_server_ts\": 1,\n                \"type\": \"m.room.message\",\n                \"origin\": \"test.serv\",\n                \"content\": {\"body\": \"hewwo?\"},\n                \"auth_events\": [],\n                \"prev_events\": [(\"two:test.serv\", {}), (most_recent, {})],\n            }\n        )\n\n        with LoggingContext(request=\"lying_event\"):\n            failure = self.get_failure(\n                self.handler.on_receive_pdu(\n                    \"test.serv\", lying_event, sent_to_us_directly=True\n                ),\n                FederationError,\n            )\n\n        # on_receive_pdu should throw an error\n        self.assertEqual(\n            failure.value.args[0],\n            (\n                \"ERROR 403: Your server isn't divulging details about prev_events \"\n                \"referenced in this event.\"\n            ),\n        )\n\n        # Make sure the invalid event isn't there\n        extrem = self.get_success(self.store.get_latest_event_ids_in_room(self.room_id))\n        self.assertEqual(extrem[0], \"$join:test.serv\")\n\n    def test_retry_device_list_resync(self):\n        \"\"\"Tests that device lists are marked as stale if they couldn't be synced, and\n        that stale device lists are retried periodically.\n        \"\"\"\n        remote_user_id = \"@john:test_remote\"\n        remote_origin = \"test_remote\"\n\n        # Track the number of attempts to resync the user's device list.\n        self.resync_attempts = 0\n\n        # When this function is called, increment the number of resync attempts (only if\n        # we're querying devices for the right user ID), then raise a\n        # NotRetryingDestination error to fail the resync gracefully.\n        def query_user_devices(destination, user_id):\n            if user_id == remote_user_id:\n                self.resync_attempts += 1\n\n            raise NotRetryingDestination(0, 0, destination)\n\n        # Register the mock on the federation client.\n        federation_client = self.homeserver.get_federation_client()\n        federation_client.query_user_devices = Mock(side_effect=query_user_devices)\n\n        # Register a mock on the store so that the incoming update doesn't fail because\n        # we don't share a room with the user.\n        store = self.homeserver.get_datastore()\n        store.get_rooms_for_user = Mock(return_value=make_awaitable([\"!someroom:test\"]))\n\n        # Manually inject a fake device list update. We need this update to include at\n        # least one prev_id so that the user's device list will need to be retried.\n        device_list_updater = self.homeserver.get_device_handler().device_list_updater\n        self.get_success(\n            device_list_updater.incoming_device_list_update(\n                origin=remote_origin,\n                edu_content={\n                    \"deleted\": False,\n                    \"device_display_name\": \"Mobile\",\n                    \"device_id\": \"QBUAZIFURK\",\n                    \"prev_id\": [5],\n                    \"stream_id\": 6,\n                    \"user_id\": remote_user_id,\n                },\n            )\n        )\n\n        # Check that there was one resync attempt.\n        self.assertEqual(self.resync_attempts, 1)\n\n        # Check that the resync attempt failed and caused the user's device list to be\n        # marked as stale.\n        need_resync = self.get_success(\n            store.get_user_ids_requiring_device_list_resync()\n        )\n        self.assertIn(remote_user_id, need_resync)\n\n        # Check that waiting for 30 seconds caused Synapse to retry resyncing the device\n        # list.\n        self.reactor.advance(30)\n        self.assertEqual(self.resync_attempts, 2)\n\n    def test_cross_signing_keys_retry(self):\n        \"\"\"Tests that resyncing a device list correctly processes cross-signing keys from\n        the remote server.\n        \"\"\"\n        remote_user_id = \"@john:test_remote\"\n        remote_master_key = \"85T7JXPFBAySB/jwby4S3lBPTqY3+Zg53nYuGmu1ggY\"\n        remote_self_signing_key = \"QeIiFEjluPBtI7WQdG365QKZcFs9kqmHir6RBD0//nQ\"\n\n        # Register mock device list retrieval on the federation client.\n        federation_client = self.homeserver.get_federation_client()\n        federation_client.query_user_devices = Mock(\n            return_value=succeed(\n                {\n                    \"user_id\": remote_user_id,\n                    \"stream_id\": 1,\n                    \"devices\": [],\n                    \"master_key\": {\n                        \"user_id\": remote_user_id,\n                        \"usage\": [\"master\"],\n                        \"keys\": {\"ed25519:\" + remote_master_key: remote_master_key},\n                    },\n                    \"self_signing_key\": {\n                        \"user_id\": remote_user_id,\n                        \"usage\": [\"self_signing\"],\n                        \"keys\": {\n                            \"ed25519:\"\n                            + remote_self_signing_key: remote_self_signing_key\n                        },\n                    },\n                }\n            )\n        )\n\n        # Resync the device list.\n        device_handler = self.homeserver.get_device_handler()\n        self.get_success(\n            device_handler.device_list_updater.user_device_resync(remote_user_id),\n        )\n\n        # Retrieve the cross-signing keys for this user.\n        keys = self.get_success(\n            self.store.get_e2e_cross_signing_keys_bulk(user_ids=[remote_user_id]),\n        )\n        self.assertTrue(remote_user_id in keys)\n\n        # Check that the master key is the one returned by the mock.\n        master_key = keys[remote_user_id][\"master\"]\n        self.assertEqual(len(master_key[\"keys\"]), 1)\n        self.assertTrue(\"ed25519:\" + remote_master_key in master_key[\"keys\"].keys())\n        self.assertTrue(remote_master_key in master_key[\"keys\"].values())\n\n        # Check that the self-signing key is the one returned by the mock.\n        self_signing_key = keys[remote_user_id][\"self_signing\"]\n        self.assertEqual(len(self_signing_key[\"keys\"]), 1)\n        self.assertTrue(\n            \"ed25519:\" + remote_self_signing_key in self_signing_key[\"keys\"].keys(),\n        )\n        self.assertTrue(remote_self_signing_key in self_signing_key[\"keys\"].values())\n", "code_before": "# -*- coding: utf-8 -*-\n# Copyright 2020 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom mock import Mock\n\nfrom twisted.internet.defer import succeed\n\nfrom synapse.api.errors import FederationError\nfrom synapse.events import make_event_from_dict\nfrom synapse.logging.context import LoggingContext\nfrom synapse.types import UserID, create_requester\nfrom synapse.util import Clock\nfrom synapse.util.retryutils import NotRetryingDestination\n\nfrom tests import unittest\nfrom tests.server import ThreadedMemoryReactorClock, setup_test_homeserver\nfrom tests.test_utils import make_awaitable\n\n\nclass MessageAcceptTests(unittest.HomeserverTestCase):\n    def setUp(self):\n\n        self.http_client = Mock()\n        self.reactor = ThreadedMemoryReactorClock()\n        self.hs_clock = Clock(self.reactor)\n        self.homeserver = setup_test_homeserver(\n            self.addCleanup,\n            http_client=self.http_client,\n            clock=self.hs_clock,\n            reactor=self.reactor,\n        )\n\n        user_id = UserID(\"us\", \"test\")\n        our_user = create_requester(user_id)\n        room_creator = self.homeserver.get_room_creation_handler()\n        self.room_id = self.get_success(\n            room_creator.create_room(\n                our_user, room_creator._presets_dict[\"public_chat\"], ratelimit=False\n            )\n        )[0][\"room_id\"]\n\n        self.store = self.homeserver.get_datastore()\n\n        # Figure out what the most recent event is\n        most_recent = self.get_success(\n            self.homeserver.get_datastore().get_latest_event_ids_in_room(self.room_id)\n        )[0]\n\n        join_event = make_event_from_dict(\n            {\n                \"room_id\": self.room_id,\n                \"sender\": \"@baduser:test.serv\",\n                \"state_key\": \"@baduser:test.serv\",\n                \"event_id\": \"$join:test.serv\",\n                \"depth\": 1000,\n                \"origin_server_ts\": 1,\n                \"type\": \"m.room.member\",\n                \"origin\": \"test.servx\",\n                \"content\": {\"membership\": \"join\"},\n                \"auth_events\": [],\n                \"prev_state\": [(most_recent, {})],\n                \"prev_events\": [(most_recent, {})],\n            }\n        )\n\n        self.handler = self.homeserver.get_federation_handler()\n        self.handler.do_auth = lambda origin, event, context, auth_events: succeed(\n            context\n        )\n        self.client = self.homeserver.get_federation_client()\n        self.client._check_sigs_and_hash_and_fetch = lambda dest, pdus, **k: succeed(\n            pdus\n        )\n\n        # Send the join, it should return None (which is not an error)\n        self.assertEqual(\n            self.get_success(\n                self.handler.on_receive_pdu(\n                    \"test.serv\", join_event, sent_to_us_directly=True\n                )\n            ),\n            None,\n        )\n\n        # Make sure we actually joined the room\n        self.assertEqual(\n            self.get_success(self.store.get_latest_event_ids_in_room(self.room_id))[0],\n            \"$join:test.serv\",\n        )\n\n    def test_cant_hide_direct_ancestors(self):\n        \"\"\"\n        If you send a message, you must be able to provide the direct\n        prev_events that said event references.\n        \"\"\"\n\n        async def post_json(destination, path, data, headers=None, timeout=0):\n            # If it asks us for new missing events, give them NOTHING\n            if path.startswith(\"/_matrix/federation/v1/get_missing_events/\"):\n                return {\"events\": []}\n\n        self.http_client.post_json = post_json\n\n        # Figure out what the most recent event is\n        most_recent = self.get_success(\n            self.store.get_latest_event_ids_in_room(self.room_id)\n        )[0]\n\n        # Now lie about an event\n        lying_event = make_event_from_dict(\n            {\n                \"room_id\": self.room_id,\n                \"sender\": \"@baduser:test.serv\",\n                \"event_id\": \"one:test.serv\",\n                \"depth\": 1000,\n                \"origin_server_ts\": 1,\n                \"type\": \"m.room.message\",\n                \"origin\": \"test.serv\",\n                \"content\": {\"body\": \"hewwo?\"},\n                \"auth_events\": [],\n                \"prev_events\": [(\"two:test.serv\", {}), (most_recent, {})],\n            }\n        )\n\n        with LoggingContext(request=\"lying_event\"):\n            failure = self.get_failure(\n                self.handler.on_receive_pdu(\n                    \"test.serv\", lying_event, sent_to_us_directly=True\n                ),\n                FederationError,\n            )\n\n        # on_receive_pdu should throw an error\n        self.assertEqual(\n            failure.value.args[0],\n            (\n                \"ERROR 403: Your server isn't divulging details about prev_events \"\n                \"referenced in this event.\"\n            ),\n        )\n\n        # Make sure the invalid event isn't there\n        extrem = self.get_success(self.store.get_latest_event_ids_in_room(self.room_id))\n        self.assertEqual(extrem[0], \"$join:test.serv\")\n\n    def test_retry_device_list_resync(self):\n        \"\"\"Tests that device lists are marked as stale if they couldn't be synced, and\n        that stale device lists are retried periodically.\n        \"\"\"\n        remote_user_id = \"@john:test_remote\"\n        remote_origin = \"test_remote\"\n\n        # Track the number of attempts to resync the user's device list.\n        self.resync_attempts = 0\n\n        # When this function is called, increment the number of resync attempts (only if\n        # we're querying devices for the right user ID), then raise a\n        # NotRetryingDestination error to fail the resync gracefully.\n        def query_user_devices(destination, user_id):\n            if user_id == remote_user_id:\n                self.resync_attempts += 1\n\n            raise NotRetryingDestination(0, 0, destination)\n\n        # Register the mock on the federation client.\n        federation_client = self.homeserver.get_federation_client()\n        federation_client.query_user_devices = Mock(side_effect=query_user_devices)\n\n        # Register a mock on the store so that the incoming update doesn't fail because\n        # we don't share a room with the user.\n        store = self.homeserver.get_datastore()\n        store.get_rooms_for_user = Mock(return_value=make_awaitable([\"!someroom:test\"]))\n\n        # Manually inject a fake device list update. We need this update to include at\n        # least one prev_id so that the user's device list will need to be retried.\n        device_list_updater = self.homeserver.get_device_handler().device_list_updater\n        self.get_success(\n            device_list_updater.incoming_device_list_update(\n                origin=remote_origin,\n                edu_content={\n                    \"deleted\": False,\n                    \"device_display_name\": \"Mobile\",\n                    \"device_id\": \"QBUAZIFURK\",\n                    \"prev_id\": [5],\n                    \"stream_id\": 6,\n                    \"user_id\": remote_user_id,\n                },\n            )\n        )\n\n        # Check that there was one resync attempt.\n        self.assertEqual(self.resync_attempts, 1)\n\n        # Check that the resync attempt failed and caused the user's device list to be\n        # marked as stale.\n        need_resync = self.get_success(\n            store.get_user_ids_requiring_device_list_resync()\n        )\n        self.assertIn(remote_user_id, need_resync)\n\n        # Check that waiting for 30 seconds caused Synapse to retry resyncing the device\n        # list.\n        self.reactor.advance(30)\n        self.assertEqual(self.resync_attempts, 2)\n\n    def test_cross_signing_keys_retry(self):\n        \"\"\"Tests that resyncing a device list correctly processes cross-signing keys from\n        the remote server.\n        \"\"\"\n        remote_user_id = \"@john:test_remote\"\n        remote_master_key = \"85T7JXPFBAySB/jwby4S3lBPTqY3+Zg53nYuGmu1ggY\"\n        remote_self_signing_key = \"QeIiFEjluPBtI7WQdG365QKZcFs9kqmHir6RBD0//nQ\"\n\n        # Register mock device list retrieval on the federation client.\n        federation_client = self.homeserver.get_federation_client()\n        federation_client.query_user_devices = Mock(\n            return_value=succeed(\n                {\n                    \"user_id\": remote_user_id,\n                    \"stream_id\": 1,\n                    \"devices\": [],\n                    \"master_key\": {\n                        \"user_id\": remote_user_id,\n                        \"usage\": [\"master\"],\n                        \"keys\": {\"ed25519:\" + remote_master_key: remote_master_key},\n                    },\n                    \"self_signing_key\": {\n                        \"user_id\": remote_user_id,\n                        \"usage\": [\"self_signing\"],\n                        \"keys\": {\n                            \"ed25519:\"\n                            + remote_self_signing_key: remote_self_signing_key\n                        },\n                    },\n                }\n            )\n        )\n\n        # Resync the device list.\n        device_handler = self.homeserver.get_device_handler()\n        self.get_success(\n            device_handler.device_list_updater.user_device_resync(remote_user_id),\n        )\n\n        # Retrieve the cross-signing keys for this user.\n        keys = self.get_success(\n            self.store.get_e2e_cross_signing_keys_bulk(user_ids=[remote_user_id]),\n        )\n        self.assertTrue(remote_user_id in keys)\n\n        # Check that the master key is the one returned by the mock.\n        master_key = keys[remote_user_id][\"master\"]\n        self.assertEqual(len(master_key[\"keys\"]), 1)\n        self.assertTrue(\"ed25519:\" + remote_master_key in master_key[\"keys\"].keys())\n        self.assertTrue(remote_master_key in master_key[\"keys\"].values())\n\n        # Check that the self-signing key is the one returned by the mock.\n        self_signing_key = keys[remote_user_id][\"self_signing\"]\n        self.assertEqual(len(self_signing_key[\"keys\"]), 1)\n        self.assertTrue(\n            \"ed25519:\" + remote_self_signing_key in self_signing_key[\"keys\"].keys(),\n        )\n        self.assertTrue(remote_self_signing_key in self_signing_key[\"keys\"].values())\n", "patch": "@@ -37,7 +37,7 @@ def setUp(self):\n         self.hs_clock = Clock(self.reactor)\n         self.homeserver = setup_test_homeserver(\n             self.addCleanup,\n-            http_client=self.http_client,\n+            federation_http_client=self.http_client,\n             clock=self.hs_clock,\n             reactor=self.reactor,\n         )", "file_path": "files/2021_2/56", "file_language": "py", "file_name": "tests/test_federation.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class MessageAcceptTests(unittest.HomeserverTestCase):\n    def setUp(self):\n\n        self.http_client = Mock()\n        self.reactor = ThreadedMemoryReactorClock()\n        self.hs_clock = Clock(self.reactor)\n        self.homeserver = setup_test_homeserver(\n            self.addCleanup,\n            http_client=self.http_client,\n            clock=self.hs_clock,\n            reactor=self.reactor,\n        )\n\n        user_id = UserID(\"us\", \"test\")\n        our_user = create_requester(user_id)\n        room_creator = self.homeserver.get_room_creation_handler()\n        self.room_id = self.get_success(\n            room_creator.create_room(\n                our_user, room_creator._presets_dict[\"public_chat\"], ratelimit=False\n            )\n        )[0][\"room_id\"]\n\n        self.store = self.homeserver.get_datastore()\n\n        # Figure out what the most recent event is\n        most_recent = self.get_success(\n            self.homeserver.get_datastore().get_latest_event_ids_in_room(self.room_id)\n        )[0]\n\n        join_event = make_event_from_dict(\n            {\n                \"room_id\": self.room_id,\n                \"sender\": \"@baduser:test.serv\",\n                \"state_key\": \"@baduser:test.serv\",\n                \"event_id\": \"$join:test.serv\",\n                \"depth\": 1000,\n                \"origin_server_ts\": 1,\n                \"type\": \"m.room.member\",\n                \"origin\": \"test.servx\",\n                \"content\": {\"membership\": \"join\"},\n                \"auth_events\": [],\n                \"prev_state\": [(most_recent, {})],\n                \"prev_events\": [(most_recent, {})],\n            }\n        )\n\n        self.handler = self.homeserver.get_federation_handler()\n        self.handler.do_auth = lambda origin, event, context, auth_events: succeed(\n            context\n        )\n        self.client = self.homeserver.get_federation_client()\n        self.client._check_sigs_and_hash_and_fetch = lambda dest, pdus, **k: succeed(\n            pdus\n        )\n\n        # Send the join, it should return None (which is not an error)\n        self.assertEqual(\n            self.get_success(\n                self.handler.on_receive_pdu(\n                    \"test.serv\", join_event, sent_to_us_directly=True\n                )\n            ),\n            None,\n        )\n\n        # Make sure we actually joined the room\n        self.assertEqual(\n            self.get_success(self.store.get_latest_event_ids_in_room(self.room_id))[0],\n            \"$join:test.serv\",\n        )\n\n    def test_cant_hide_direct_ancestors(self):\n        \"\"\"\n        If you send a message, you must be able to provide the direct\n        prev_events that said event references.\n        \"\"\"\n\n        async def post_json(destination, path, data, headers=None, timeout=0):\n            # If it asks us for new missing events, give them NOTHING\n            if path.startswith(\"/_matrix/federation/v1/get_missing_events/\"):\n                return {\"events\": []}\n\n        self.http_client.post_json = post_json\n\n        # Figure out what the most recent event is\n        most_recent = self.get_success(\n            self.store.get_latest_event_ids_in_room(self.room_id)\n        )[0]\n\n        # Now lie about an event\n        lying_event = make_event_from_dict(\n            {\n                \"room_id\": self.room_id,\n                \"sender\": \"@baduser:test.serv\",\n                \"event_id\": \"one:test.serv\",\n                \"depth\": 1000,\n                \"origin_server_ts\": 1,\n                \"type\": \"m.room.message\",\n                \"origin\": \"test.serv\",\n                \"content\": {\"body\": \"hewwo?\"},\n                \"auth_events\": [],\n                \"prev_events\": [(\"two:test.serv\", {}), (most_recent, {})],\n            }\n        )\n\n        with LoggingContext(request=\"lying_event\"):\n            failure = self.get_failure(\n                self.handler.on_receive_pdu(\n                    \"test.serv\", lying_event, sent_to_us_directly=True\n                ),\n                FederationError,\n            )\n\n        # on_receive_pdu should throw an error\n        self.assertEqual(\n            failure.value.args[0],\n            (\n                \"ERROR 403: Your server isn't divulging details about prev_events \"\n                \"referenced in this event.\"\n            ),\n        )\n\n        # Make sure the invalid event isn't there\n        extrem = self.get_success(self.store.get_latest_event_ids_in_room(self.room_id))\n        self.assertEqual(extrem[0], \"$join:test.serv\")\n\n    def test_retry_device_list_resync(self):\n        \"\"\"Tests that device lists are marked as stale if they couldn't be synced, and\n        that stale device lists are retried periodically.\n        \"\"\"\n        remote_user_id = \"@john:test_remote\"\n        remote_origin = \"test_remote\"\n\n        # Track the number of attempts to resync the user's device list.\n        self.resync_attempts = 0\n\n        # When this function is called, increment the number of resync attempts (only if\n        # we're querying devices for the right user ID), then raise a\n        # NotRetryingDestination error to fail the resync gracefully.\n        def query_user_devices(destination, user_id):\n            if user_id == remote_user_id:\n                self.resync_attempts += 1\n\n            raise NotRetryingDestination(0, 0, destination)\n\n        # Register the mock on the federation client.\n        federation_client = self.homeserver.get_federation_client()\n        federation_client.query_user_devices = Mock(side_effect=query_user_devices)\n\n        # Register a mock on the store so that the incoming update doesn't fail because\n        # we don't share a room with the user.\n        store = self.homeserver.get_datastore()\n        store.get_rooms_for_user = Mock(return_value=make_awaitable([\"!someroom:test\"]))\n\n        # Manually inject a fake device list update. We need this update to include at\n        # least one prev_id so that the user's device list will need to be retried.\n        device_list_updater = self.homeserver.get_device_handler().device_list_updater\n        self.get_success(\n            device_list_updater.incoming_device_list_update(\n                origin=remote_origin,\n                edu_content={\n                    \"deleted\": False,\n                    \"device_display_name\": \"Mobile\",\n                    \"device_id\": \"QBUAZIFURK\",\n                    \"prev_id\": [5],\n                    \"stream_id\": 6,\n                    \"user_id\": remote_user_id,\n                },\n            )\n        )\n\n        # Check that there was one resync attempt.\n        self.assertEqual(self.resync_attempts, 1)\n\n        # Check that the resync attempt failed and caused the user's device list to be\n        # marked as stale.\n        need_resync = self.get_success(\n            store.get_user_ids_requiring_device_list_resync()\n        )\n        self.assertIn(remote_user_id, need_resync)\n\n        # Check that waiting for 30 seconds caused Synapse to retry resyncing the device\n        # list.\n        self.reactor.advance(30)\n        self.assertEqual(self.resync_attempts, 2)\n\n    def test_cross_signing_keys_retry(self):\n        \"\"\"Tests that resyncing a device list correctly processes cross-signing keys from\n        the remote server.\n        \"\"\"\n        remote_user_id = \"@john:test_remote\"\n        remote_master_key = \"85T7JXPFBAySB/jwby4S3lBPTqY3+Zg53nYuGmu1ggY\"\n        remote_self_signing_key = \"QeIiFEjluPBtI7WQdG365QKZcFs9kqmHir6RBD0//nQ\"\n\n        # Register mock device list retrieval on the federation client.\n        federation_client = self.homeserver.get_federation_client()\n        federation_client.query_user_devices = Mock(\n            return_value=succeed(\n                {\n                    \"user_id\": remote_user_id,\n                    \"stream_id\": 1,\n                    \"devices\": [],\n                    \"master_key\": {\n                        \"user_id\": remote_user_id,\n                        \"usage\": [\"master\"],\n                        \"keys\": {\"ed25519:\" + remote_master_key: remote_master_key},\n                    },\n                    \"self_signing_key\": {\n                        \"user_id\": remote_user_id,\n                        \"usage\": [\"self_signing\"],\n                        \"keys\": {\n                            \"ed25519:\"\n                            + remote_self_signing_key: remote_self_signing_key\n                        },\n                    },\n                }\n            )\n        )\n\n        # Resync the device list.\n        device_handler = self.homeserver.get_device_handler()\n        self.get_success(\n            device_handler.device_list_updater.user_device_resync(remote_user_id),\n        )\n\n        # Retrieve the cross-signing keys for this user.\n        keys = self.get_success(\n            self.store.get_e2e_cross_signing_keys_bulk(user_ids=[remote_user_id]),\n        )\n        self.assertTrue(remote_user_id in keys)\n\n        # Check that the master key is the one returned by the mock.\n        master_key = keys[remote_user_id][\"master\"]\n        self.assertEqual(len(master_key[\"keys\"]), 1)\n        self.assertTrue(\"ed25519:\" + remote_master_key in master_key[\"keys\"].keys())\n        self.assertTrue(remote_master_key in master_key[\"keys\"].values())\n\n        # Check that the self-signing key is the one returned by the mock.\n        self_signing_key = keys[remote_user_id][\"self_signing\"]\n        self.assertEqual(len(self_signing_key[\"keys\"]), 1)\n        self.assertTrue(\n            \"ed25519:\" + remote_self_signing_key in self_signing_key[\"keys\"].keys(),\n        )\n        self.assertTrue(remote_self_signing_key in self_signing_key[\"keys\"].values())", "target": 0}], "function_after": [{"function": "class MessageAcceptTests(unittest.HomeserverTestCase):\n    def setUp(self):\n\n        self.http_client = Mock()\n        self.reactor = ThreadedMemoryReactorClock()\n        self.hs_clock = Clock(self.reactor)\n        self.homeserver = setup_test_homeserver(\n            self.addCleanup,\n            federation_http_client=self.http_client,\n            clock=self.hs_clock,\n            reactor=self.reactor,\n        )\n\n        user_id = UserID(\"us\", \"test\")\n        our_user = create_requester(user_id)\n        room_creator = self.homeserver.get_room_creation_handler()\n        self.room_id = self.get_success(\n            room_creator.create_room(\n                our_user, room_creator._presets_dict[\"public_chat\"], ratelimit=False\n            )\n        )[0][\"room_id\"]\n\n        self.store = self.homeserver.get_datastore()\n\n        # Figure out what the most recent event is\n        most_recent = self.get_success(\n            self.homeserver.get_datastore().get_latest_event_ids_in_room(self.room_id)\n        )[0]\n\n        join_event = make_event_from_dict(\n            {\n                \"room_id\": self.room_id,\n                \"sender\": \"@baduser:test.serv\",\n                \"state_key\": \"@baduser:test.serv\",\n                \"event_id\": \"$join:test.serv\",\n                \"depth\": 1000,\n                \"origin_server_ts\": 1,\n                \"type\": \"m.room.member\",\n                \"origin\": \"test.servx\",\n                \"content\": {\"membership\": \"join\"},\n                \"auth_events\": [],\n                \"prev_state\": [(most_recent, {})],\n                \"prev_events\": [(most_recent, {})],\n            }\n        )\n\n        self.handler = self.homeserver.get_federation_handler()\n        self.handler.do_auth = lambda origin, event, context, auth_events: succeed(\n            context\n        )\n        self.client = self.homeserver.get_federation_client()\n        self.client._check_sigs_and_hash_and_fetch = lambda dest, pdus, **k: succeed(\n            pdus\n        )\n\n        # Send the join, it should return None (which is not an error)\n        self.assertEqual(\n            self.get_success(\n                self.handler.on_receive_pdu(\n                    \"test.serv\", join_event, sent_to_us_directly=True\n                )\n            ),\n            None,\n        )\n\n        # Make sure we actually joined the room\n        self.assertEqual(\n            self.get_success(self.store.get_latest_event_ids_in_room(self.room_id))[0],\n            \"$join:test.serv\",\n        )\n\n    def test_cant_hide_direct_ancestors(self):\n        \"\"\"\n        If you send a message, you must be able to provide the direct\n        prev_events that said event references.\n        \"\"\"\n\n        async def post_json(destination, path, data, headers=None, timeout=0):\n            # If it asks us for new missing events, give them NOTHING\n            if path.startswith(\"/_matrix/federation/v1/get_missing_events/\"):\n                return {\"events\": []}\n\n        self.http_client.post_json = post_json\n\n        # Figure out what the most recent event is\n        most_recent = self.get_success(\n            self.store.get_latest_event_ids_in_room(self.room_id)\n        )[0]\n\n        # Now lie about an event\n        lying_event = make_event_from_dict(\n            {\n                \"room_id\": self.room_id,\n                \"sender\": \"@baduser:test.serv\",\n                \"event_id\": \"one:test.serv\",\n                \"depth\": 1000,\n                \"origin_server_ts\": 1,\n                \"type\": \"m.room.message\",\n                \"origin\": \"test.serv\",\n                \"content\": {\"body\": \"hewwo?\"},\n                \"auth_events\": [],\n                \"prev_events\": [(\"two:test.serv\", {}), (most_recent, {})],\n            }\n        )\n\n        with LoggingContext(request=\"lying_event\"):\n            failure = self.get_failure(\n                self.handler.on_receive_pdu(\n                    \"test.serv\", lying_event, sent_to_us_directly=True\n                ),\n                FederationError,\n            )\n\n        # on_receive_pdu should throw an error\n        self.assertEqual(\n            failure.value.args[0],\n            (\n                \"ERROR 403: Your server isn't divulging details about prev_events \"\n                \"referenced in this event.\"\n            ),\n        )\n\n        # Make sure the invalid event isn't there\n        extrem = self.get_success(self.store.get_latest_event_ids_in_room(self.room_id))\n        self.assertEqual(extrem[0], \"$join:test.serv\")\n\n    def test_retry_device_list_resync(self):\n        \"\"\"Tests that device lists are marked as stale if they couldn't be synced, and\n        that stale device lists are retried periodically.\n        \"\"\"\n        remote_user_id = \"@john:test_remote\"\n        remote_origin = \"test_remote\"\n\n        # Track the number of attempts to resync the user's device list.\n        self.resync_attempts = 0\n\n        # When this function is called, increment the number of resync attempts (only if\n        # we're querying devices for the right user ID), then raise a\n        # NotRetryingDestination error to fail the resync gracefully.\n        def query_user_devices(destination, user_id):\n            if user_id == remote_user_id:\n                self.resync_attempts += 1\n\n            raise NotRetryingDestination(0, 0, destination)\n\n        # Register the mock on the federation client.\n        federation_client = self.homeserver.get_federation_client()\n        federation_client.query_user_devices = Mock(side_effect=query_user_devices)\n\n        # Register a mock on the store so that the incoming update doesn't fail because\n        # we don't share a room with the user.\n        store = self.homeserver.get_datastore()\n        store.get_rooms_for_user = Mock(return_value=make_awaitable([\"!someroom:test\"]))\n\n        # Manually inject a fake device list update. We need this update to include at\n        # least one prev_id so that the user's device list will need to be retried.\n        device_list_updater = self.homeserver.get_device_handler().device_list_updater\n        self.get_success(\n            device_list_updater.incoming_device_list_update(\n                origin=remote_origin,\n                edu_content={\n                    \"deleted\": False,\n                    \"device_display_name\": \"Mobile\",\n                    \"device_id\": \"QBUAZIFURK\",\n                    \"prev_id\": [5],\n                    \"stream_id\": 6,\n                    \"user_id\": remote_user_id,\n                },\n            )\n        )\n\n        # Check that there was one resync attempt.\n        self.assertEqual(self.resync_attempts, 1)\n\n        # Check that the resync attempt failed and caused the user's device list to be\n        # marked as stale.\n        need_resync = self.get_success(\n            store.get_user_ids_requiring_device_list_resync()\n        )\n        self.assertIn(remote_user_id, need_resync)\n\n        # Check that waiting for 30 seconds caused Synapse to retry resyncing the device\n        # list.\n        self.reactor.advance(30)\n        self.assertEqual(self.resync_attempts, 2)\n\n    def test_cross_signing_keys_retry(self):\n        \"\"\"Tests that resyncing a device list correctly processes cross-signing keys from\n        the remote server.\n        \"\"\"\n        remote_user_id = \"@john:test_remote\"\n        remote_master_key = \"85T7JXPFBAySB/jwby4S3lBPTqY3+Zg53nYuGmu1ggY\"\n        remote_self_signing_key = \"QeIiFEjluPBtI7WQdG365QKZcFs9kqmHir6RBD0//nQ\"\n\n        # Register mock device list retrieval on the federation client.\n        federation_client = self.homeserver.get_federation_client()\n        federation_client.query_user_devices = Mock(\n            return_value=succeed(\n                {\n                    \"user_id\": remote_user_id,\n                    \"stream_id\": 1,\n                    \"devices\": [],\n                    \"master_key\": {\n                        \"user_id\": remote_user_id,\n                        \"usage\": [\"master\"],\n                        \"keys\": {\"ed25519:\" + remote_master_key: remote_master_key},\n                    },\n                    \"self_signing_key\": {\n                        \"user_id\": remote_user_id,\n                        \"usage\": [\"self_signing\"],\n                        \"keys\": {\n                            \"ed25519:\"\n                            + remote_self_signing_key: remote_self_signing_key\n                        },\n                    },\n                }\n            )\n        )\n\n        # Resync the device list.\n        device_handler = self.homeserver.get_device_handler()\n        self.get_success(\n            device_handler.device_list_updater.user_device_resync(remote_user_id),\n        )\n\n        # Retrieve the cross-signing keys for this user.\n        keys = self.get_success(\n            self.store.get_e2e_cross_signing_keys_bulk(user_ids=[remote_user_id]),\n        )\n        self.assertTrue(remote_user_id in keys)\n\n        # Check that the master key is the one returned by the mock.\n        master_key = keys[remote_user_id][\"master\"]\n        self.assertEqual(len(master_key[\"keys\"]), 1)\n        self.assertTrue(\"ed25519:\" + remote_master_key in master_key[\"keys\"].keys())\n        self.assertTrue(remote_master_key in master_key[\"keys\"].values())\n\n        # Check that the self-signing key is the one returned by the mock.\n        self_signing_key = keys[remote_user_id][\"self_signing\"]\n        self.assertEqual(len(self_signing_key[\"keys\"]), 1)\n        self.assertTrue(\n            \"ed25519:\" + remote_self_signing_key in self_signing_key[\"keys\"].keys(),\n        )\n        self.assertTrue(remote_self_signing_key in self_signing_key[\"keys\"].values())", "target": 0}]}, {"raw_url": "https://github.com/matrix-org/synapse/raw/30fba6210834a4ecd91badf0c8f3eb278b72e746/tests%2Ftest_server.py", "code": "# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\n\nfrom twisted.internet.defer import Deferred\nfrom twisted.web.resource import Resource\n\nfrom synapse.api.errors import Codes, RedirectException, SynapseError\nfrom synapse.config.server import parse_listener_def\nfrom synapse.http.server import DirectServeHtmlResource, JsonResource, OptionsResource\nfrom synapse.http.site import SynapseSite\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.util import Clock\n\nfrom tests import unittest\nfrom tests.server import (\n    FakeSite,\n    ThreadedMemoryReactorClock,\n    make_request,\n    setup_test_homeserver,\n)\n\n\nclass JsonResourceTests(unittest.TestCase):\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n        self.hs_clock = Clock(self.reactor)\n        self.homeserver = setup_test_homeserver(\n            self.addCleanup,\n            federation_http_client=None,\n            clock=self.hs_clock,\n            reactor=self.reactor,\n        )\n\n    def test_handler_for_request(self):\n        \"\"\"\n        JsonResource.handler_for_request gives correctly decoded URL args to\n        the callback, while Twisted will give the raw bytes of URL query\n        arguments.\n        \"\"\"\n        got_kwargs = {}\n\n        def _callback(request, **kwargs):\n            got_kwargs.update(kwargs)\n            return 200, kwargs\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\",\n            [re.compile(\"^/_matrix/foo/(?P<room_id>[^/]*)$\")],\n            _callback,\n            \"test_servlet\",\n        )\n\n        request, channel = make_request(\n            self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo/%E2%98%83?a=%E2%98%83\"\n        )\n\n        self.assertEqual(request.args, {b\"a\": [\"\\N{SNOWMAN}\".encode(\"utf8\")]})\n        self.assertEqual(got_kwargs, {\"room_id\": \"\\N{SNOWMAN}\"})\n\n    def test_callback_direct_exception(self):\n        \"\"\"\n        If the web callback raises an uncaught exception, it will be translated\n        into a 500.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            raise Exception(\"boo\")\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"500\")\n\n    def test_callback_indirect_exception(self):\n        \"\"\"\n        If the web callback raises an uncaught exception in a Deferred, it will\n        be translated into a 500.\n        \"\"\"\n\n        def _throw(*args):\n            raise Exception(\"boo\")\n\n        def _callback(request, **kwargs):\n            d = Deferred()\n            d.addCallback(_throw)\n            self.reactor.callLater(1, d.callback, True)\n            return make_deferred_yieldable(d)\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"500\")\n\n    def test_callback_synapseerror(self):\n        \"\"\"\n        If the web callback raises a SynapseError, it returns the appropriate\n        status code and message set in it.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            raise SynapseError(403, \"Forbidden!!one!\", Codes.FORBIDDEN)\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"403\")\n        self.assertEqual(channel.json_body[\"error\"], \"Forbidden!!one!\")\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_FORBIDDEN\")\n\n    def test_no_handler(self):\n        \"\"\"\n        If there is no handler to process the request, Synapse will return 400.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            \"\"\"\n            Not ever actually called!\n            \"\"\"\n            self.fail(\"shouldn't ever get here\")\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(\n            self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foobar\"\n        )\n\n        self.assertEqual(channel.result[\"code\"], b\"400\")\n        self.assertEqual(channel.json_body[\"error\"], \"Unrecognized request\")\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_UNRECOGNIZED\")\n\n    def test_head_request(self):\n        \"\"\"\n        JsonResource.handler_for_request gives correctly decoded URL args to\n        the callback, while Twisted will give the raw bytes of URL query\n        arguments.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            return 200, {\"result\": True}\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\",\n        )\n\n        # The path was registered as GET, but this is a HEAD request.\n        _, channel = make_request(self.reactor, FakeSite(res), b\"HEAD\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertNotIn(\"body\", channel.result)\n\n\nclass OptionsResourceTests(unittest.TestCase):\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n\n        class DummyResource(Resource):\n            isLeaf = True\n\n            def render(self, request):\n                return request.path\n\n        # Setup a resource with some children.\n        self.resource = OptionsResource()\n        self.resource.putChild(b\"res\", DummyResource())\n\n    def _make_request(self, method, path):\n        \"\"\"Create a request from the method/path and return a channel with the response.\"\"\"\n        # Create a site and query for the resource.\n        site = SynapseSite(\n            \"test\",\n            \"site_tag\",\n            parse_listener_def({\"type\": \"http\", \"port\": 0}),\n            self.resource,\n            \"1.0\",\n        )\n\n        # render the request and return the channel\n        _, channel = make_request(self.reactor, site, method, path, shorthand=False)\n        return channel\n\n    def test_unknown_options_request(self):\n        \"\"\"An OPTIONS requests to an unknown URL still returns 204 No Content.\"\"\"\n        channel = self._make_request(b\"OPTIONS\", b\"/foo/\")\n        self.assertEqual(channel.result[\"code\"], b\"204\")\n        self.assertNotIn(\"body\", channel.result)\n\n        # Ensure the correct CORS headers have been added\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Origin\"),\n            \"has CORS Origin header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Methods\"),\n            \"has CORS Methods header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Headers\"),\n            \"has CORS Headers header\",\n        )\n\n    def test_known_options_request(self):\n        \"\"\"An OPTIONS requests to an known URL still returns 204 No Content.\"\"\"\n        channel = self._make_request(b\"OPTIONS\", b\"/res/\")\n        self.assertEqual(channel.result[\"code\"], b\"204\")\n        self.assertNotIn(\"body\", channel.result)\n\n        # Ensure the correct CORS headers have been added\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Origin\"),\n            \"has CORS Origin header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Methods\"),\n            \"has CORS Methods header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Headers\"),\n            \"has CORS Headers header\",\n        )\n\n    def test_unknown_request(self):\n        \"\"\"A non-OPTIONS request to an unknown URL should 404.\"\"\"\n        channel = self._make_request(b\"GET\", b\"/foo/\")\n        self.assertEqual(channel.result[\"code\"], b\"404\")\n\n    def test_known_request(self):\n        \"\"\"A non-OPTIONS request to an known URL should query the proper resource.\"\"\"\n        channel = self._make_request(b\"GET\", b\"/res/\")\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertEqual(channel.result[\"body\"], b\"/res/\")\n\n\nclass WrapHtmlRequestHandlerTests(unittest.TestCase):\n    class TestResource(DirectServeHtmlResource):\n        callback = None\n\n        async def _async_render_GET(self, request):\n            await self.callback(request)\n\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n\n    def test_good_response(self):\n        async def callback(request):\n            request.write(b\"response\")\n            request.finish()\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        body = channel.result[\"body\"]\n        self.assertEqual(body, b\"response\")\n\n    def test_redirect_exception(self):\n        \"\"\"\n        If the callback raises a RedirectException, it is turned into a 30x\n        with the right location.\n        \"\"\"\n\n        async def callback(request, **kwargs):\n            raise RedirectException(b\"/look/an/eagle\", 301)\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"301\")\n        headers = channel.result[\"headers\"]\n        location_headers = [v for k, v in headers if k == b\"Location\"]\n        self.assertEqual(location_headers, [b\"/look/an/eagle\"])\n\n    def test_redirect_exception_with_cookie(self):\n        \"\"\"\n        If the callback raises a RedirectException which sets a cookie, that is\n        returned too\n        \"\"\"\n\n        async def callback(request, **kwargs):\n            e = RedirectException(b\"/no/over/there\", 304)\n            e.cookies.append(b\"session=yespls\")\n            raise e\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"304\")\n        headers = channel.result[\"headers\"]\n        location_headers = [v for k, v in headers if k == b\"Location\"]\n        self.assertEqual(location_headers, [b\"/no/over/there\"])\n        cookies_headers = [v for k, v in headers if k == b\"Set-Cookie\"]\n        self.assertEqual(cookies_headers, [b\"session=yespls\"])\n\n    def test_head_request(self):\n        \"\"\"A head request should work by being turned into a GET request.\"\"\"\n\n        async def callback(request):\n            request.write(b\"response\")\n            request.finish()\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"HEAD\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertNotIn(\"body\", channel.result)\n", "code_before": "# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\n\nfrom twisted.internet.defer import Deferred\nfrom twisted.web.resource import Resource\n\nfrom synapse.api.errors import Codes, RedirectException, SynapseError\nfrom synapse.config.server import parse_listener_def\nfrom synapse.http.server import DirectServeHtmlResource, JsonResource, OptionsResource\nfrom synapse.http.site import SynapseSite\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.util import Clock\n\nfrom tests import unittest\nfrom tests.server import (\n    FakeSite,\n    ThreadedMemoryReactorClock,\n    make_request,\n    setup_test_homeserver,\n)\n\n\nclass JsonResourceTests(unittest.TestCase):\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n        self.hs_clock = Clock(self.reactor)\n        self.homeserver = setup_test_homeserver(\n            self.addCleanup, http_client=None, clock=self.hs_clock, reactor=self.reactor\n        )\n\n    def test_handler_for_request(self):\n        \"\"\"\n        JsonResource.handler_for_request gives correctly decoded URL args to\n        the callback, while Twisted will give the raw bytes of URL query\n        arguments.\n        \"\"\"\n        got_kwargs = {}\n\n        def _callback(request, **kwargs):\n            got_kwargs.update(kwargs)\n            return 200, kwargs\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\",\n            [re.compile(\"^/_matrix/foo/(?P<room_id>[^/]*)$\")],\n            _callback,\n            \"test_servlet\",\n        )\n\n        request, channel = make_request(\n            self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo/%E2%98%83?a=%E2%98%83\"\n        )\n\n        self.assertEqual(request.args, {b\"a\": [\"\\N{SNOWMAN}\".encode(\"utf8\")]})\n        self.assertEqual(got_kwargs, {\"room_id\": \"\\N{SNOWMAN}\"})\n\n    def test_callback_direct_exception(self):\n        \"\"\"\n        If the web callback raises an uncaught exception, it will be translated\n        into a 500.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            raise Exception(\"boo\")\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"500\")\n\n    def test_callback_indirect_exception(self):\n        \"\"\"\n        If the web callback raises an uncaught exception in a Deferred, it will\n        be translated into a 500.\n        \"\"\"\n\n        def _throw(*args):\n            raise Exception(\"boo\")\n\n        def _callback(request, **kwargs):\n            d = Deferred()\n            d.addCallback(_throw)\n            self.reactor.callLater(1, d.callback, True)\n            return make_deferred_yieldable(d)\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"500\")\n\n    def test_callback_synapseerror(self):\n        \"\"\"\n        If the web callback raises a SynapseError, it returns the appropriate\n        status code and message set in it.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            raise SynapseError(403, \"Forbidden!!one!\", Codes.FORBIDDEN)\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"403\")\n        self.assertEqual(channel.json_body[\"error\"], \"Forbidden!!one!\")\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_FORBIDDEN\")\n\n    def test_no_handler(self):\n        \"\"\"\n        If there is no handler to process the request, Synapse will return 400.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            \"\"\"\n            Not ever actually called!\n            \"\"\"\n            self.fail(\"shouldn't ever get here\")\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(\n            self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foobar\"\n        )\n\n        self.assertEqual(channel.result[\"code\"], b\"400\")\n        self.assertEqual(channel.json_body[\"error\"], \"Unrecognized request\")\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_UNRECOGNIZED\")\n\n    def test_head_request(self):\n        \"\"\"\n        JsonResource.handler_for_request gives correctly decoded URL args to\n        the callback, while Twisted will give the raw bytes of URL query\n        arguments.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            return 200, {\"result\": True}\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\",\n        )\n\n        # The path was registered as GET, but this is a HEAD request.\n        _, channel = make_request(self.reactor, FakeSite(res), b\"HEAD\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertNotIn(\"body\", channel.result)\n\n\nclass OptionsResourceTests(unittest.TestCase):\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n\n        class DummyResource(Resource):\n            isLeaf = True\n\n            def render(self, request):\n                return request.path\n\n        # Setup a resource with some children.\n        self.resource = OptionsResource()\n        self.resource.putChild(b\"res\", DummyResource())\n\n    def _make_request(self, method, path):\n        \"\"\"Create a request from the method/path and return a channel with the response.\"\"\"\n        # Create a site and query for the resource.\n        site = SynapseSite(\n            \"test\",\n            \"site_tag\",\n            parse_listener_def({\"type\": \"http\", \"port\": 0}),\n            self.resource,\n            \"1.0\",\n        )\n\n        # render the request and return the channel\n        _, channel = make_request(self.reactor, site, method, path, shorthand=False)\n        return channel\n\n    def test_unknown_options_request(self):\n        \"\"\"An OPTIONS requests to an unknown URL still returns 204 No Content.\"\"\"\n        channel = self._make_request(b\"OPTIONS\", b\"/foo/\")\n        self.assertEqual(channel.result[\"code\"], b\"204\")\n        self.assertNotIn(\"body\", channel.result)\n\n        # Ensure the correct CORS headers have been added\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Origin\"),\n            \"has CORS Origin header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Methods\"),\n            \"has CORS Methods header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Headers\"),\n            \"has CORS Headers header\",\n        )\n\n    def test_known_options_request(self):\n        \"\"\"An OPTIONS requests to an known URL still returns 204 No Content.\"\"\"\n        channel = self._make_request(b\"OPTIONS\", b\"/res/\")\n        self.assertEqual(channel.result[\"code\"], b\"204\")\n        self.assertNotIn(\"body\", channel.result)\n\n        # Ensure the correct CORS headers have been added\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Origin\"),\n            \"has CORS Origin header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Methods\"),\n            \"has CORS Methods header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Headers\"),\n            \"has CORS Headers header\",\n        )\n\n    def test_unknown_request(self):\n        \"\"\"A non-OPTIONS request to an unknown URL should 404.\"\"\"\n        channel = self._make_request(b\"GET\", b\"/foo/\")\n        self.assertEqual(channel.result[\"code\"], b\"404\")\n\n    def test_known_request(self):\n        \"\"\"A non-OPTIONS request to an known URL should query the proper resource.\"\"\"\n        channel = self._make_request(b\"GET\", b\"/res/\")\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertEqual(channel.result[\"body\"], b\"/res/\")\n\n\nclass WrapHtmlRequestHandlerTests(unittest.TestCase):\n    class TestResource(DirectServeHtmlResource):\n        callback = None\n\n        async def _async_render_GET(self, request):\n            await self.callback(request)\n\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n\n    def test_good_response(self):\n        async def callback(request):\n            request.write(b\"response\")\n            request.finish()\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        body = channel.result[\"body\"]\n        self.assertEqual(body, b\"response\")\n\n    def test_redirect_exception(self):\n        \"\"\"\n        If the callback raises a RedirectException, it is turned into a 30x\n        with the right location.\n        \"\"\"\n\n        async def callback(request, **kwargs):\n            raise RedirectException(b\"/look/an/eagle\", 301)\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"301\")\n        headers = channel.result[\"headers\"]\n        location_headers = [v for k, v in headers if k == b\"Location\"]\n        self.assertEqual(location_headers, [b\"/look/an/eagle\"])\n\n    def test_redirect_exception_with_cookie(self):\n        \"\"\"\n        If the callback raises a RedirectException which sets a cookie, that is\n        returned too\n        \"\"\"\n\n        async def callback(request, **kwargs):\n            e = RedirectException(b\"/no/over/there\", 304)\n            e.cookies.append(b\"session=yespls\")\n            raise e\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"304\")\n        headers = channel.result[\"headers\"]\n        location_headers = [v for k, v in headers if k == b\"Location\"]\n        self.assertEqual(location_headers, [b\"/no/over/there\"])\n        cookies_headers = [v for k, v in headers if k == b\"Set-Cookie\"]\n        self.assertEqual(cookies_headers, [b\"session=yespls\"])\n\n    def test_head_request(self):\n        \"\"\"A head request should work by being turned into a GET request.\"\"\"\n\n        async def callback(request):\n            request.write(b\"response\")\n            request.finish()\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"HEAD\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertNotIn(\"body\", channel.result)\n", "patch": "@@ -38,7 +38,10 @@ def setUp(self):\n         self.reactor = ThreadedMemoryReactorClock()\n         self.hs_clock = Clock(self.reactor)\n         self.homeserver = setup_test_homeserver(\n-            self.addCleanup, http_client=None, clock=self.hs_clock, reactor=self.reactor\n+            self.addCleanup,\n+            federation_http_client=None,\n+            clock=self.hs_clock,\n+            reactor=self.reactor,\n         )\n \n     def test_handler_for_request(self):", "file_path": "files/2021_2/57", "file_language": "py", "file_name": "tests/test_server.py", "outdated_file_modify": 0, "outdated_file_before": 0, "outdated_file_after": 0, "llm_check": 0, "static_check": 0, "static": {"rats": [false, []], "semgrep": [false, []]}, "target": 0, "function_before": [{"function": "class JsonResourceTests(unittest.TestCase):\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n        self.hs_clock = Clock(self.reactor)\n        self.homeserver = setup_test_homeserver(\n            self.addCleanup, http_client=None, clock=self.hs_clock, reactor=self.reactor\n        )\n\n    def test_handler_for_request(self):\n        \"\"\"\n        JsonResource.handler_for_request gives correctly decoded URL args to\n        the callback, while Twisted will give the raw bytes of URL query\n        arguments.\n        \"\"\"\n        got_kwargs = {}\n\n        def _callback(request, **kwargs):\n            got_kwargs.update(kwargs)\n            return 200, kwargs\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\",\n            [re.compile(\"^/_matrix/foo/(?P<room_id>[^/]*)$\")],\n            _callback,\n            \"test_servlet\",\n        )\n\n        request, channel = make_request(\n            self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo/%E2%98%83?a=%E2%98%83\"\n        )\n\n        self.assertEqual(request.args, {b\"a\": [\"\\N{SNOWMAN}\".encode(\"utf8\")]})\n        self.assertEqual(got_kwargs, {\"room_id\": \"\\N{SNOWMAN}\"})\n\n    def test_callback_direct_exception(self):\n        \"\"\"\n        If the web callback raises an uncaught exception, it will be translated\n        into a 500.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            raise Exception(\"boo\")\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"500\")\n\n    def test_callback_indirect_exception(self):\n        \"\"\"\n        If the web callback raises an uncaught exception in a Deferred, it will\n        be translated into a 500.\n        \"\"\"\n\n        def _throw(*args):\n            raise Exception(\"boo\")\n\n        def _callback(request, **kwargs):\n            d = Deferred()\n            d.addCallback(_throw)\n            self.reactor.callLater(1, d.callback, True)\n            return make_deferred_yieldable(d)\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"500\")\n\n    def test_callback_synapseerror(self):\n        \"\"\"\n        If the web callback raises a SynapseError, it returns the appropriate\n        status code and message set in it.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            raise SynapseError(403, \"Forbidden!!one!\", Codes.FORBIDDEN)\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"403\")\n        self.assertEqual(channel.json_body[\"error\"], \"Forbidden!!one!\")\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_FORBIDDEN\")\n\n    def test_no_handler(self):\n        \"\"\"\n        If there is no handler to process the request, Synapse will return 400.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            \"\"\"\n            Not ever actually called!\n            \"\"\"\n            self.fail(\"shouldn't ever get here\")\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(\n            self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foobar\"\n        )\n\n        self.assertEqual(channel.result[\"code\"], b\"400\")\n        self.assertEqual(channel.json_body[\"error\"], \"Unrecognized request\")\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_UNRECOGNIZED\")\n\n    def test_head_request(self):\n        \"\"\"\n        JsonResource.handler_for_request gives correctly decoded URL args to\n        the callback, while Twisted will give the raw bytes of URL query\n        arguments.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            return 200, {\"result\": True}\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\",\n        )\n\n        # The path was registered as GET, but this is a HEAD request.\n        _, channel = make_request(self.reactor, FakeSite(res), b\"HEAD\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertNotIn(\"body\", channel.result)", "target": 0}, {"function": "class OptionsResourceTests(unittest.TestCase):\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n\n        class DummyResource(Resource):\n            isLeaf = True\n\n            def render(self, request):\n                return request.path\n\n        # Setup a resource with some children.\n        self.resource = OptionsResource()\n        self.resource.putChild(b\"res\", DummyResource())\n\n    def _make_request(self, method, path):\n        \"\"\"Create a request from the method/path and return a channel with the response.\"\"\"\n        # Create a site and query for the resource.\n        site = SynapseSite(\n            \"test\",\n            \"site_tag\",\n            parse_listener_def({\"type\": \"http\", \"port\": 0}),\n            self.resource,\n            \"1.0\",\n        )\n\n        # render the request and return the channel\n        _, channel = make_request(self.reactor, site, method, path, shorthand=False)\n        return channel\n\n    def test_unknown_options_request(self):\n        \"\"\"An OPTIONS requests to an unknown URL still returns 204 No Content.\"\"\"\n        channel = self._make_request(b\"OPTIONS\", b\"/foo/\")\n        self.assertEqual(channel.result[\"code\"], b\"204\")\n        self.assertNotIn(\"body\", channel.result)\n\n        # Ensure the correct CORS headers have been added\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Origin\"),\n            \"has CORS Origin header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Methods\"),\n            \"has CORS Methods header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Headers\"),\n            \"has CORS Headers header\",\n        )\n\n    def test_known_options_request(self):\n        \"\"\"An OPTIONS requests to an known URL still returns 204 No Content.\"\"\"\n        channel = self._make_request(b\"OPTIONS\", b\"/res/\")\n        self.assertEqual(channel.result[\"code\"], b\"204\")\n        self.assertNotIn(\"body\", channel.result)\n\n        # Ensure the correct CORS headers have been added\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Origin\"),\n            \"has CORS Origin header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Methods\"),\n            \"has CORS Methods header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Headers\"),\n            \"has CORS Headers header\",\n        )\n\n    def test_unknown_request(self):\n        \"\"\"A non-OPTIONS request to an unknown URL should 404.\"\"\"\n        channel = self._make_request(b\"GET\", b\"/foo/\")\n        self.assertEqual(channel.result[\"code\"], b\"404\")\n\n    def test_known_request(self):\n        \"\"\"A non-OPTIONS request to an known URL should query the proper resource.\"\"\"\n        channel = self._make_request(b\"GET\", b\"/res/\")\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertEqual(channel.result[\"body\"], b\"/res/\")", "target": 0}, {"function": "class WrapHtmlRequestHandlerTests(unittest.TestCase):\n    class TestResource(DirectServeHtmlResource):\n        callback = None\n\n        async def _async_render_GET(self, request):\n            await self.callback(request)\n\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n\n    def test_good_response(self):\n        async def callback(request):\n            request.write(b\"response\")\n            request.finish()\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        body = channel.result[\"body\"]\n        self.assertEqual(body, b\"response\")\n\n    def test_redirect_exception(self):\n        \"\"\"\n        If the callback raises a RedirectException, it is turned into a 30x\n        with the right location.\n        \"\"\"\n\n        async def callback(request, **kwargs):\n            raise RedirectException(b\"/look/an/eagle\", 301)\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"301\")\n        headers = channel.result[\"headers\"]\n        location_headers = [v for k, v in headers if k == b\"Location\"]\n        self.assertEqual(location_headers, [b\"/look/an/eagle\"])\n\n    def test_redirect_exception_with_cookie(self):\n        \"\"\"\n        If the callback raises a RedirectException which sets a cookie, that is\n        returned too\n        \"\"\"\n\n        async def callback(request, **kwargs):\n            e = RedirectException(b\"/no/over/there\", 304)\n            e.cookies.append(b\"session=yespls\")\n            raise e\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"304\")\n        headers = channel.result[\"headers\"]\n        location_headers = [v for k, v in headers if k == b\"Location\"]\n        self.assertEqual(location_headers, [b\"/no/over/there\"])\n        cookies_headers = [v for k, v in headers if k == b\"Set-Cookie\"]\n        self.assertEqual(cookies_headers, [b\"session=yespls\"])\n\n    def test_head_request(self):\n        \"\"\"A head request should work by being turned into a GET request.\"\"\"\n\n        async def callback(request):\n            request.write(b\"response\")\n            request.finish()\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"HEAD\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertNotIn(\"body\", channel.result)", "target": 0}], "function_after": [{"function": "class JsonResourceTests(unittest.TestCase):\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n        self.hs_clock = Clock(self.reactor)\n        self.homeserver = setup_test_homeserver(\n            self.addCleanup,\n            federation_http_client=None,\n            clock=self.hs_clock,\n            reactor=self.reactor,\n        )\n\n    def test_handler_for_request(self):\n        \"\"\"\n        JsonResource.handler_for_request gives correctly decoded URL args to\n        the callback, while Twisted will give the raw bytes of URL query\n        arguments.\n        \"\"\"\n        got_kwargs = {}\n\n        def _callback(request, **kwargs):\n            got_kwargs.update(kwargs)\n            return 200, kwargs\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\",\n            [re.compile(\"^/_matrix/foo/(?P<room_id>[^/]*)$\")],\n            _callback,\n            \"test_servlet\",\n        )\n\n        request, channel = make_request(\n            self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo/%E2%98%83?a=%E2%98%83\"\n        )\n\n        self.assertEqual(request.args, {b\"a\": [\"\\N{SNOWMAN}\".encode(\"utf8\")]})\n        self.assertEqual(got_kwargs, {\"room_id\": \"\\N{SNOWMAN}\"})\n\n    def test_callback_direct_exception(self):\n        \"\"\"\n        If the web callback raises an uncaught exception, it will be translated\n        into a 500.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            raise Exception(\"boo\")\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"500\")\n\n    def test_callback_indirect_exception(self):\n        \"\"\"\n        If the web callback raises an uncaught exception in a Deferred, it will\n        be translated into a 500.\n        \"\"\"\n\n        def _throw(*args):\n            raise Exception(\"boo\")\n\n        def _callback(request, **kwargs):\n            d = Deferred()\n            d.addCallback(_throw)\n            self.reactor.callLater(1, d.callback, True)\n            return make_deferred_yieldable(d)\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"500\")\n\n    def test_callback_synapseerror(self):\n        \"\"\"\n        If the web callback raises a SynapseError, it returns the appropriate\n        status code and message set in it.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            raise SynapseError(403, \"Forbidden!!one!\", Codes.FORBIDDEN)\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"403\")\n        self.assertEqual(channel.json_body[\"error\"], \"Forbidden!!one!\")\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_FORBIDDEN\")\n\n    def test_no_handler(self):\n        \"\"\"\n        If there is no handler to process the request, Synapse will return 400.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            \"\"\"\n            Not ever actually called!\n            \"\"\"\n            self.fail(\"shouldn't ever get here\")\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\"\n        )\n\n        _, channel = make_request(\n            self.reactor, FakeSite(res), b\"GET\", b\"/_matrix/foobar\"\n        )\n\n        self.assertEqual(channel.result[\"code\"], b\"400\")\n        self.assertEqual(channel.json_body[\"error\"], \"Unrecognized request\")\n        self.assertEqual(channel.json_body[\"errcode\"], \"M_UNRECOGNIZED\")\n\n    def test_head_request(self):\n        \"\"\"\n        JsonResource.handler_for_request gives correctly decoded URL args to\n        the callback, while Twisted will give the raw bytes of URL query\n        arguments.\n        \"\"\"\n\n        def _callback(request, **kwargs):\n            return 200, {\"result\": True}\n\n        res = JsonResource(self.homeserver)\n        res.register_paths(\n            \"GET\", [re.compile(\"^/_matrix/foo$\")], _callback, \"test_servlet\",\n        )\n\n        # The path was registered as GET, but this is a HEAD request.\n        _, channel = make_request(self.reactor, FakeSite(res), b\"HEAD\", b\"/_matrix/foo\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertNotIn(\"body\", channel.result)", "target": 0}, {"function": "class OptionsResourceTests(unittest.TestCase):\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n\n        class DummyResource(Resource):\n            isLeaf = True\n\n            def render(self, request):\n                return request.path\n\n        # Setup a resource with some children.\n        self.resource = OptionsResource()\n        self.resource.putChild(b\"res\", DummyResource())\n\n    def _make_request(self, method, path):\n        \"\"\"Create a request from the method/path and return a channel with the response.\"\"\"\n        # Create a site and query for the resource.\n        site = SynapseSite(\n            \"test\",\n            \"site_tag\",\n            parse_listener_def({\"type\": \"http\", \"port\": 0}),\n            self.resource,\n            \"1.0\",\n        )\n\n        # render the request and return the channel\n        _, channel = make_request(self.reactor, site, method, path, shorthand=False)\n        return channel\n\n    def test_unknown_options_request(self):\n        \"\"\"An OPTIONS requests to an unknown URL still returns 204 No Content.\"\"\"\n        channel = self._make_request(b\"OPTIONS\", b\"/foo/\")\n        self.assertEqual(channel.result[\"code\"], b\"204\")\n        self.assertNotIn(\"body\", channel.result)\n\n        # Ensure the correct CORS headers have been added\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Origin\"),\n            \"has CORS Origin header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Methods\"),\n            \"has CORS Methods header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Headers\"),\n            \"has CORS Headers header\",\n        )\n\n    def test_known_options_request(self):\n        \"\"\"An OPTIONS requests to an known URL still returns 204 No Content.\"\"\"\n        channel = self._make_request(b\"OPTIONS\", b\"/res/\")\n        self.assertEqual(channel.result[\"code\"], b\"204\")\n        self.assertNotIn(\"body\", channel.result)\n\n        # Ensure the correct CORS headers have been added\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Origin\"),\n            \"has CORS Origin header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Methods\"),\n            \"has CORS Methods header\",\n        )\n        self.assertTrue(\n            channel.headers.hasHeader(b\"Access-Control-Allow-Headers\"),\n            \"has CORS Headers header\",\n        )\n\n    def test_unknown_request(self):\n        \"\"\"A non-OPTIONS request to an unknown URL should 404.\"\"\"\n        channel = self._make_request(b\"GET\", b\"/foo/\")\n        self.assertEqual(channel.result[\"code\"], b\"404\")\n\n    def test_known_request(self):\n        \"\"\"A non-OPTIONS request to an known URL should query the proper resource.\"\"\"\n        channel = self._make_request(b\"GET\", b\"/res/\")\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertEqual(channel.result[\"body\"], b\"/res/\")", "target": 0}, {"function": "class WrapHtmlRequestHandlerTests(unittest.TestCase):\n    class TestResource(DirectServeHtmlResource):\n        callback = None\n\n        async def _async_render_GET(self, request):\n            await self.callback(request)\n\n    def setUp(self):\n        self.reactor = ThreadedMemoryReactorClock()\n\n    def test_good_response(self):\n        async def callback(request):\n            request.write(b\"response\")\n            request.finish()\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        body = channel.result[\"body\"]\n        self.assertEqual(body, b\"response\")\n\n    def test_redirect_exception(self):\n        \"\"\"\n        If the callback raises a RedirectException, it is turned into a 30x\n        with the right location.\n        \"\"\"\n\n        async def callback(request, **kwargs):\n            raise RedirectException(b\"/look/an/eagle\", 301)\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"301\")\n        headers = channel.result[\"headers\"]\n        location_headers = [v for k, v in headers if k == b\"Location\"]\n        self.assertEqual(location_headers, [b\"/look/an/eagle\"])\n\n    def test_redirect_exception_with_cookie(self):\n        \"\"\"\n        If the callback raises a RedirectException which sets a cookie, that is\n        returned too\n        \"\"\"\n\n        async def callback(request, **kwargs):\n            e = RedirectException(b\"/no/over/there\", 304)\n            e.cookies.append(b\"session=yespls\")\n            raise e\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"GET\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"304\")\n        headers = channel.result[\"headers\"]\n        location_headers = [v for k, v in headers if k == b\"Location\"]\n        self.assertEqual(location_headers, [b\"/no/over/there\"])\n        cookies_headers = [v for k, v in headers if k == b\"Set-Cookie\"]\n        self.assertEqual(cookies_headers, [b\"session=yespls\"])\n\n    def test_head_request(self):\n        \"\"\"A head request should work by being turned into a GET request.\"\"\"\n\n        async def callback(request):\n            request.write(b\"response\")\n            request.finish()\n\n        res = WrapHtmlRequestHandlerTests.TestResource()\n        res.callback = callback\n\n        _, channel = make_request(self.reactor, FakeSite(res), b\"HEAD\", b\"/path\")\n\n        self.assertEqual(channel.result[\"code\"], b\"200\")\n        self.assertNotIn(\"body\", channel.result)", "target": 0}]}], "outdated": 0, "cwe_descripiton": "", "cwe_consequence": "", "cwe_method": "", "cwe_solution": ""}
